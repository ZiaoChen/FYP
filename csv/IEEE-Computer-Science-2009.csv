Title,Abstract,Keywords
From PID to Active Disturbance Rejection Control,"Active disturbance rejection control (ADRC) can be summarized as follows: it inherits from proportional-integral-derivative (PID) the quality that makes it such a success: the error driven, rather than model-based, control law; it takes from modern control theory its best offering: the state observer; it embraces the power of nonlinear feedback and puts it to full use; it is a useful digital control technology developed out of an experimental platform rooted in computer simulations. ADRC is made possible only when control is taken as an experimental science, instead of a mathematical one. It is motivated by the ever increasing demands from industry that requires the control technology to move beyond PID, which has dominated the practice for over 80 years. Specifically, there are four areas of weakness in PID that we strive to address: 1) the error computation; 2) noise degradation in the derivative control; 3) oversimplification and the loss of performance in the control law in the form of a linear weighted sum; and 4) complications brought by the integral control. Correspondingly, we propose four distinct measures: 1) a simple differential equation as a transient trajectory generator; 2) a noise-tolerant tracking differentiator; 3) the nonlinear control laws; and 4) the concept and method of total disturbance estimation and rejection. Together, they form a new set of tools and a new way of control design. Times and again in experiments and on factory floors, ADRC proves to be a capable replacement of PID with unmistakable advantage in performance and practicality, providing solutions to pressing engineering problems of today. With the new outlook and possibilities that ADRC represents, we further believe that control engineering may very well break the hold of classical PID and enter a new era, an era that brings back the spirit of innovations.",
Frequency-tuned salient region detection,"Detection of visually salient image regions is useful for applications like object segmentation, adaptive compression, and object recognition. In this paper, we introduce a method for salient region detection that outputs full resolution saliency maps with well-defined boundaries of salient objects. These boundaries are preserved by retaining substantially more frequency content from the original image than other existing techniques. Our method exploits features of color and luminance, is simple to implement, and is computationally efficient. We compare our algorithm to five state-of-the-art salient region detection methods with a frequency domain analysis, ground truth, and a salient object segmentation application. Our method outperforms the five algorithms both on the ground-truth evaluation and on the segmentation task by achieving both higher precision and better recall.","Biology computing,
Object detection,
Object segmentation,
Biological system modeling,
Image coding,
Object recognition,
Frequency domain analysis,
Image segmentation,
Image analysis,
Frequency estimation"
"A Survey of Affect Recognition Methods: Audio, Visual, and Spontaneous Expressions","Automated analysis of human affective behavior has attracted increasing attention from researchers in psychology, computer science, linguistics, neuroscience, and related disciplines. However, the existing methods typically handle only deliberately displayed and exaggerated expressions of prototypical emotions despite the fact that deliberate behaviour differs in visual appearance, audio profile, and timing from spontaneously occurring behaviour. To address this problem, efforts to develop algorithms that can process naturally occurring human affective behaviour have recently emerged. Moreover, an increasing number of efforts are reported toward multimodal fusion for human affect analysis including audiovisual fusion, linguistic and paralinguistic fusion, and multi-cue visual fusion based on facial expressions, head movements, and body gestures. This paper introduces and surveys these recent advances. We first discuss human emotion perception from a psychological perspective. Next we examine available approaches to solving the problem of machine understanding of human affective behavior, and discuss important issues like the collection and availability of training and test data. We finally outline some of the scientific and engineering challenges to advancing human affect sensing technology.",
Rodinia: A benchmark suite for heterogeneous computing,"This paper presents and characterizes Rodinia, a benchmark suite for heterogeneous computing. To help architects study emerging platforms such as GPUs (Graphics Processing Units), Rodinia includes applications and kernels which target multi-core CPU and GPU platforms. The choice of applications is inspired by Berkeley's dwarf taxonomy. Our characterization shows that the Rodinia benchmarks cover a wide range of parallel communication patterns, synchronization techniques and power consumption, and has led to some important architectural insight, such as the growing importance of memory-bandwidth limitations and the consequent importance of data layout.","Kernel,
Multicore processing,
Parallel processing,
Application software,
Yarn,
Benchmark testing,
Central Processing Unit,
Energy consumption,
Microprocessors,
Computer architecture"
Learning to predict where humans look,"For many applications in graphics, design, and human computer interaction, it is essential to understand where humans look in a scene. Where eye tracking devices are not a viable option, models of saliency can be used to predict fixation locations. Most saliency approaches are based on bottom-up computation that does not consider top-down image semantics and often does not match actual eye movements. To address this problem, we collected eye tracking data of 15 viewers on 1003 images and use this database as training and testing examples to learn a model of saliency based on low, middle and high-level image features. This large database of eye tracking data is publicly available with this paper.",
Super-resolution from a single image,"Methods for super-resolution can be broadly classified into two families of methods: (i) The classical multi-image super-resolution (combining images obtained at subpixel misalignments), and (ii) Example-Based super-resolution (learning correspondence between low and high resolution image patches from a database). In this paper we propose a unified framework for combining these two families of methods. We further show how this combined approach can be applied to obtain super resolution from as little as a single image (with no database or prior examples). Our approach is based on the observation that patches in a natural image tend to redundantly recur many times inside the image, both within the same scale, as well as across different scales. Recurrence of patches within the same image scale (at subpixel misalignments) gives rise to the classical super-resolution, whereas recurrence of patches across different scales of the same image gives rise to example-based super-resolution. Our approach attempts to recover at each pixel its best possible resolution increase based on its patch redundancy within and across scales.","Image resolution,
Strontium,
Image databases,
Computer science,
Mathematics,
Layout,
Equations,
Image reconstruction,
Computer vision,
Frequency"
ImageNet: A large-scale hierarchical image database,"The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.","Large-scale systems,
Image databases,
Explosions,
Internet,
Robustness,
Information retrieval,
Image retrieval,
Multimedia databases,
Ontologies,
Spine"
Describing objects by their attributes,"We propose to shift the goal of recognition from naming to describing. Doing so allows us not only to name familiar objects, but also: to report unusual aspects of a familiar object (“spotty dog”, not just “dog”); to say something about unfamiliar objects (“hairy and four-legged”, not just “unknown”); and to learn how to recognize new objects with few or no visual examples. Rather than focusing on identity assignment, we make inferring attributes the core problem of recognition. These attributes can be semantic (“spotty”) or discriminative (“dogs have it but sheep do not”). Learning attributes presents a major new challenge: generalization across object categories, not just across instances within a category. In this paper, we also introduce a novel feature selection method for learning attributes that generalize well across categories. We support our claims by thorough evaluation that provides insights into the limitations of the standard recognition paradigm of naming and demonstrates the new abilities provided by our attribute-based framework.",
Abnormal crowd behavior detection using social force model,"In this paper we introduce a novel method to detect and localize abnormal behaviors in crowd videos using Social Force model. For this purpose, a grid of particles is placed over the image and it is advected with the space-time average of optical flow. By treating the moving particles as individuals, their interaction forces are estimated using social force model. The interaction force is then mapped into the image plane to obtain Force Flow for every pixel in every frame. Randomly selected spatio-temporal volumes of Force Flow are used to model the normal behavior of the crowd. We classify frames as normal and abnormal by using a bag of words approach. The regions of anomalies in the abnormal frames are localized using interaction forces. The experiments are conducted on a publicly available dataset from University of Minnesota for escape panic scenarios and a challenging dataset of crowd videos taken from the web. The experiments show that the proposed method captures the dynamics of the crowd behavior successfully. In addition, we have shown that the social force approach outperforms similar approaches based on pure optical flow.","Videos,
Image motion analysis,
Pixel"
What is the best multi-stage architecture for object recognition?,"In many recent object recognition systems, feature extraction stages are generally composed of a filter bank, a non-linear transformation, and some sort of feature pooling layer. Most systems use only one stage of feature extraction in which the filters are hard-wired, or two stages where the filters in one or both stages are learned in supervised or unsupervised mode. This paper addresses three questions: 1. How does the non-linearities that follow the filter banks influence the recognition accuracy? 2. does learning the filter banks in an unsupervised or supervised manner improve the performance over random filters or hardwired filters? 3. Is there any advantage to using an architecture with two stages of feature extraction, rather than one? We show that using non-linearities that include rectification and local contrast normalization is the single most important ingredient for good accuracy on object recognition benchmarks. We show that two stages of feature extraction yield better accuracy than one. Most surprisingly, we show that a two-stage system with random filters can yield almost 63% recognition rate on Caltech-101, provided that the proper non-linearities and pooling layers are used. Finally, we show that with supervised refinement, the system achieves state-of-the-art performance on NORB dataset (5.6%) and unsupervised pre-training followed by supervised refinement produces good accuracy on Caltech-101 (≫ 65%), and the lowest known error rate on the undistorted, unprocessed MNIST dataset (0.53%).","Object recognition,
Filter bank,
Feature extraction,
Refining,
Brain modeling,
Gabor filters,
Learning systems,
Image edge detection,
Error analysis,
Histograms"
Sparse subspace clustering,"We propose a method based on sparse representation (SR) to cluster data drawn from multiple low-dimensional linear or affine subspaces embedded in a high-dimensional space. Our method is based on the fact that each point in a union of subspaces has a SR with respect to a dictionary formed by all other data points. In general, finding such a SR is NP hard. Our key contribution is to show that, under mild assumptions, the SR can be obtained `exactly' by using l1 optimization. The segmentation of the data is obtained by applying spectral clustering to a similarity matrix built from this SR. Our method can handle noise, outliers as well as missing data. We apply our subspace clustering algorithm to the problem of segmenting multiple motions in video. Experiments on 167 video sequences show that our approach significantly outperforms state-of-the-art methods.","Strontium,
Principal component analysis,
Image coding,
Image segmentation,
Polynomials,
Iterative methods,
Dictionaries,
Clustering algorithms,
Video sequences,
Information theory"
A Novel Connectionist System for Unconstrained Handwriting Recognition,"Recognizing lines of unconstrained handwritten text is a challenging task. The difficulty of segmenting cursive or overlapping characters, combined with the need to exploit surrounding context, has led to low recognition rates for even the best current recognizers. Most recent progress in the field has been made either through improved preprocessing or through advances in language modeling. Relatively little work has been done on the basic recognition algorithms. Indeed, most systems rely on the same hidden Markov models that have been used for decades in speech and handwriting recognition, despite their well-known shortcomings. This paper proposes an alternative approach based on a novel type of recurrent neural network, specifically designed for sequence labeling tasks where the data is hard to segment and contains long-range bidirectional interdependencies. In experiments on two large unconstrained handwriting databases, our approach achieves word recognition accuracies of 79.7 percent on online data and 74.1 percent on offline data, significantly outperforming a state-of-the-art HMM-based system. In addition, we demonstrate the network's robustness to lexicon size, measure the individual influence of its hidden layers, and analyze its use of context. Last, we provide an in-depth discussion of the differences between the network and HMMs, suggesting reasons for the network's superior performance.","Handwriting recognition,
Hidden Markov models,
Character recognition,
Text recognition,
Speech,
Recurrent neural networks,
Labeling,
Databases,
Robustness,
Size measurement"
Comparison and Evaluation of Methods for Liver Segmentation From CT Datasets,"This paper presents a comparison study between 10 automatic and six interactive methods for liver segmentation from contrast-enhanced CT images. It is based on results from the ""MICCAI 2007 Grand Challenge"" workshop, where 16 teams evaluated their algorithms on a common database. A collection of 20 clinical images with reference segmentations was provided to train and tune algorithms in advance. Participants were also allowed to use additional proprietary training data for that purpose. All teams then had to apply their methods to 10 test datasets and submit the obtained results. Employed algorithms include statistical shape models, atlas registration, level-sets, graph-cuts and rule-based systems. All results were compared to reference segmentations five error measures that highlight different aspects of segmentation accuracy. All measures were combined according to a specific scoring system relating the obtained values to human expert variability. In general, interactive methods reached higher average scores than automatic approaches and featured a better consistency of segmentation quality. However, the best automatic methods (mainly based on statistical shape models with some additional free deformation) could compete well on the majority of test images. The study provides an insight in performance of different segmentation approaches under real-world conditions and highlights achievements and limitations of current image analysis techniques.","Liver,
Image segmentation,
Computed tomography,
Shape,
Image databases,
Training data,
Testing,
Knowledge based systems,
Humans,
Deformable models"
Virtual Network Embedding with Coordinated Node and Link Mapping,"Recently network virtualization has been proposed as a promising way to overcome the current ossification of the Internet by allowing multiple heterogeneous virtual networks (VNs) to coexist on a shared infrastructure. A major challenge in this respect is the VN embedding problem that deals with efficient mapping of virtual nodes and virtual links onto the substrate network resources. Since this problem is known to be NP-hard, previous research focused on designing heuristic-based algorithms which had clear separation between the node mapping and the link mapping phases. This paper proposes VN embedding algorithms with better coordination between the two phases. We formulate the VN embedding problem as a mixed integer program through substrate network augmentation. We then relax the integer constraints to obtain a linear program, and devise two VN embedding algorithms D-ViNE and R-ViNE using deterministic and randomized rounding techniques, respectively. Simulation experiments show that the proposed algorithms increase the acceptance ratio and the revenue while decreasing the cost incurred by the substrate network in the long run.","Peer to peer computing,
Computer science,
IP networks,
Heuristic algorithms,
Indium phosphide,
Bandwidth,
Communications Society,
Algorithm design and analysis,
Costs,
Resource virtualization"
Pedestrian detection: A benchmark,"Pedestrian detection is a key problem in computer vision, with several applications including robotics, surveillance and automotive safety. Much of the progress of the past few years has been driven by the availability of challenging public datasets. To continue the rapid rate of innovation, we introduce the Caltech Pedestrian Dataset, which is two orders of magnitude larger than existing datasets. The dataset contains richly annotated video, recorded from a moving vehicle, with challenging images of low resolution and frequently occluded people. We propose improved evaluation metrics, demonstrating that commonly used per-window measures are flawed and can fail to predict performance on full images. We also benchmark several promising detection systems, providing an overview of state-of-the-art performance and a direct, unbiased comparison of existing methods. Finally, by analyzing common failure cases, we help identify future research directions for the field.",
Multiple kernels for object detection,"Our objective is to obtain a state-of-the art object category detector by employing a state-of-the-art image classifier to search for the object in all possible image sub-windows. We use multiple kernel learning of Varma and Ray (ICCV 2007) to learn an optimal combination of exponential χ2 kernels, each of which captures a different feature channel. Our features include the distribution of edges, dense and sparse visual words, and feature descriptors at different levels of spatial organization.","Kernel,
Object detection,
Art,
Detectors,
Testing,
Costs,
Computational complexity,
Optimization methods,
Pipelines,
Performance evaluation"
A detailed analysis of the KDD CUP 99 data set,"During the last decade, anomaly detection has attracted the attention of many researchers to overcome the weakness of signature-based IDSs in detecting novel attacks, and KDDCUP'99 is the mostly widely used data set for the evaluation of these systems. Having conducted a statistical analysis on this data set, we found two important issues which highly affects the performance of evaluated systems, and results in a very poor evaluation of anomaly detection approaches. To solve these issues, we have proposed a new data set, NSL-KDD, which consists of selected records of the complete KDD data set and does not suffer from any of mentioned shortcomings.",
Deterministic and Probabilistic Tractography Based on Complex Fibre Orientation Distributions,"We propose an integral concept for tractography to describe crossing and splitting fibre bundles based on the fibre orientation distribution function (ODF) estimated from high angular resolution diffusion imaging (HARDI). We show that in order to perform accurate probabilistic tractography, one needs to use a fibre ODF estimation and not the diffusion ODF. We use a new fibre ODF estimation obtained from a sharpening deconvolution transform (SDT) of the diffusion ODF reconstructed from q-ball imaging (QBI). This SDT provides new insight into the relationship between the HARDI signal, the diffusion ODF, and the fibre ODF. We demonstrate that the SDT agrees with classical spherical deconvolution and improves the angular resolution of QBI. Another important contribution of this paper is the development of new deterministic and new probabilistic tractography algorithms using the full multidirectional information obtained through use of the fibre ODF. An extensive comparison study is performed on human brain datasets comparing our new deterministic and probabilistic tracking algorithms in complex fibre crossing regions. Finally, as an application of our new probabilistic tracking, we quantify the reconstruction of transcallosal fibres intersecting with the corona radiata and the superior longitudinal fasciculus in a group of eight subjects. Most current diffusion tensor imaging (DTI)-based methods neglect these fibres, which might lead to incorrect interpretations of brain functions.","High-resolution imaging,
Diffusion tensor imaging,
Magnetic resonance imaging,
Image resolution,
Deconvolution,
Humans,
Distribution functions,
Image reconstruction,
Tensile stress,
Signal resolution"
Is that you? Metric learning approaches for face identification,"Face identification is the problem of determining whether two face images depict the same person or not. This is difficult due to variations in scale, pose, lighting, background, expression, hairstyle, and glasses. In this paper we present two methods for learning robust distance measures: (a) a logistic discriminant approach which learns the metric from a set of labelled image pairs (LDML) and (b) a nearest neighbour approach which computes the probability for two images to belong to the same class (MkNN). We evaluate our approaches on the Labeled Faces in the Wild data set, a large and very challenging data set of faces from Yahoo! News. The evaluation protocol for this data set defines a restricted setting, where a fixed set of positive and negative image pairs is given, as well as an unrestricted one, where faces are labelled by their identity. We are the first to present results for the unrestricted setting, and show that our methods benefit from this richer training data, much more so than the current state-of-the-art method. Our results of 79.3% and 87.5% correct for the restricted and unrestricted setting respectively, significantly improve over the current state-of-the-art result of 78.5%. Confidence scores obtained for face identification can be used for many applications e.g. clustering or recognition from a single training example. We show that our learned metrics also improve performance for these tasks.","Level set,
Image segmentation,
Kernel,
Active contours,
Computational complexity,
Pixel,
Biomedical computing,
Computer science,
Graph theory,
Optimization methods"
Kernelized locality-sensitive hashing for scalable image search,"Fast retrieval methods are critical for large-scale and data-driven vision applications. Recent work has explored ways to embed high-dimensional features or complex distance functions into a low-dimensional Hamming space where items can be efficiently searched. However, existing methods do not apply for high-dimensional kernelized data when the underlying feature embedding for the kernel is unknown. We show how to generalize locality-sensitive hashing to accommodate arbitrary kernel functions, making it possible to preserve the algorithm's sub-linear time similarity search guarantees for a wide class of useful similarity functions. Since a number of successful image-based kernels have unknown or incomputable embeddings, this is especially valuable for image retrieval tasks. We validate our technique on several large-scale datasets, and show that it enables accurate and fast performance for example-based object classification, feature matching, and content-based retrieval.","Kernel,
Image databases,
Spatial databases,
Information retrieval,
Large-scale systems,
Image retrieval,
Content based retrieval,
Visual databases,
Indexing,
Object recognition"
Pictorial structures revisited: People detection and articulated pose estimation,"Non-rigid object detection and articulated pose estimation are two related and challenging problems in computer vision. Numerous models have been proposed over the years and often address different special cases, such as pedestrian detection or upper body pose estimation in TV footage. This paper shows that such specialization may not be necessary, and proposes a generic approach based on the pictorial structures framework. We show that the right selection of components for both appearance and spatial modeling is crucial for general applicability and overall performance of the model. The appearance of body parts is modeled using densely sampled shape context descriptors and discriminatively trained AdaBoost classifiers. Furthermore, we interpret the normalized margin of each classifier as likelihood in a generative model. Non-Gaussian relationships between parts are represented as Gaussians in the coordinate system of the joint between parts. The marginal posterior of each part is inferred using belief propagation. We demonstrate that such a model is equally suitable for both detection and pose estimation tasks, outperforming the state of the art on three recently proposed datasets.","Object detection,
Detectors,
Humans,
Biological system modeling,
Layout,
Gaussian processes,
State estimation,
Indexing,
Iterative methods,
Image edge detection"
Combination Strategies in Multi-Atlas Image Segmentation: Application to Brain MR Data,"It has been shown that employing multiple atlas images improves segmentation accuracy in atlas-based medical image segmentation. Each atlas image is registered to the target image independently and the calculated transformation is applied to the segmentation of the atlas image to obtain a segmented version of the target image. Several independent candidate segmentations result from the process, which must be somehow combined into a single final segmentation. Majority voting is the generally used rule to fuse the segmentations, but more sophisticated methods have also been proposed. In this paper, we show that the use of global weights to ponderate candidate segmentations has a major limitation. As a means to improve segmentation accuracy, we propose the generalized local weighting voting method. Namely, the fusion weights adapt voxel-by-voxel according to a local estimation of segmentation performance. Using digital phantoms and MR images of the human brain, we demonstrate that the performance of each combination technique depends on the gray level contrast characteristics of the segmented region, and that no fusion method yields better results than the others for all the regions. In particular, we show that local combination strategies outperform global methods in segmenting high-contrast structures, while global techniques are less sensitive to noise when contrast between neighboring structures is low. We conclude that, in order to achieve the highest overall segmentation accuracy, the best combination method for each particular structure must be selected.",
Modeling and simulation of scalable Cloud computing environments and the CloudSim toolkit: Challenges and opportunities,"Cloud computing aims to power the next generation data centers and enables application service providers to lease data center capabilities for deploying applications depending on user QoS (Quality of Service) requirements. Cloud applications have different composition, configuration, and deployment requirements. Quantifying the performance of resource allocation policies and application scheduling algorithms at finer details in Cloud computing environments for different application and service models under varying load, energy performance (power consumption, heat dissipation), and system size is a challenging problem to tackle. To simplify this process, in this paper we propose CloudSim: an extensible simulation toolkit that enables modelling and simulation of Cloud computing environments. The CloudSim toolkit supports modelling and creation of one or more virtual machines (VMs) on a simulated node of a Data Center, jobs, and their mapping to suitable VMs. It also allows simulation of multiple Data Centers to enable a study on federation and associated policies for migration of VMs for reliability and automatic scaling of applications.","Computational modeling,
Cloud computing,
Power system modeling,
Voice mail,
Quality of service,
Resource management,
Scheduling algorithm,
Load modeling,
Energy consumption,
Virtual machining"
TRUST: A General Framework for Truthful Double Spectrum Auctions,"We design truthful double spectrum auctions where multiple parties can trade spectrum based on their individual needs. Open, market-based spectrum trading motivates existing spectrum owners (as sellers) to lease their selected idle spectrum to new spectrum users, and provides new users (as buyers) the spectrum they desperately need. The most significant challenge is how to make the auction economic-robust (truthful in particular) while enabling spectrum reuse to improve spectrum utilization. Unfortunately, existing designs either do not consider spectrum reuse or become untruthful when applied to double spectrum auctions. We address this challenge by proposing TRUST, a general framework for truthful double spectrum auctions. TRUST takes as input any reusability-driven spectrum allocation algorithm, and applies a novel winner determination and pricing mechanism to achieve truthfulness and other economic properties while significantly improving spectrum utilization. To our best knowledge, TRUST is the first solution for truthful double spectrum auctions that enable spectrum reuse. Our results show that economic factors introduce a tradeoff between spectrum efficiency and economic robustness. TRUST makes an important contribution on enabling spectrum reuse to minimize such tradeoff.","Economics,
Pricing,
FCC,
Communications Society,
Computer science,
Mechanical factors,
Robustness,
Equal opportunities,
Supply and demand,
Wireless networks"
Decomposing a scene into geometric and semantically consistent regions,"High-level, or holistic, scene understanding involves reasoning about objects, regions, and the 3D relationships between them. This requires a representation above the level of pixels that can be endowed with high-level attributes such as class of object/region, its orientation, and (rough 3D) location within the scene. Towards this goal, we propose a region-based model which combines appearance and scene geometry to automatically decompose a scene into semantically meaningful regions. Our model is defined in terms of a unified energy function over scene appearance and structure. We show how this energy function can be learned from data and present an efficient inference technique that makes use of multiple over-segmentations of the image to propose moves in the energy-space. We show, experimentally, that our method achieves state-of-the-art performance on the tasks of both multi-class image segmentation and geometric reasoning. Finally, by understanding region classes and geometry, we show how our model can be used as the basis for 3D reconstruction of the scene.","Layout,
Image segmentation,
Solid modeling,
Geometry,
Image reconstruction,
Computer science,
Pixel,
Roads,
Image decomposition,
Robustness"
Highly Undersampled Magnetic Resonance Image Reconstruction via Homotopic \ell_{0} -Minimization,"In clinical magnetic resonance imaging (MRI), any reduction in scan time offers a number of potential benefits ranging from high-temporal-rate observation of physiological processes to improvements in patient comfort. Following recent developments in compressive sensing (CS) theory, several authors have demonstrated that certain classes of MR images which possess sparse representations in some transform domain can be accurately reconstructed from very highly undersampled K-space data by solving a convex lscr1-minimization problem. Although lscr1-based techniques are extremely powerful, they inherently require a degree of over-sampling above the theoretical minimum sampling rate to guarantee that exact reconstruction can be achieved. In this paper, we propose a generalization of the CS paradigm based on homotopic approximation of the lscr0 quasi-norm and show how MR image reconstruction can be pushed even further below the Nyquist limit and significantly closer to the theoretical bound. Following a brief review of standard CS methods and the developed theoretical extensions, several example MRI reconstructions from highly undersampled K-space data are presented.","Magnetic resonance,
Image reconstruction,
Magnetic resonance imaging,
Image sampling,
Image coding,
Biomedical imaging,
Signal sampling,
Standards development,
Biological tissues,
Ionizing radiation"
Current Trends in Remote Laboratories,"Remote laboratories have been introduced during the last few decades into engineering education processes as well as integrated within e-learning frameworks offered to engineering and science students. Remote laboratories are also being used to support life-long learning and student's autonomous learning activities. In this paper, after a brief overview of state-of-the-art technologies in the development of remote laboratories and presentation of recent and interesting examples of remote laboratories in several areas related with industrial electronics education, some current trends and challenges are also identified and discussed.",
Anomaly detection in extremely crowded scenes using spatio-temporal motion pattern models,"Extremely crowded scenes present unique challenges to video analysis that cannot be addressed with conventional approaches. We present a novel statistical framework for modeling the local spatio-temporal motion pattern behavior of extremely crowded scenes. Our key insight is to exploit the dense activity of the crowded scene by modeling the rich motion patterns in local areas, effectively capturing the underlying intrinsic structure they form in the video. In other words, we model the motion variation of local space-time volumes and their spatial-temporal statistical behaviors to characterize the overall behavior of the scene. We demonstrate that by capturing the steady-state motion behavior with these spatio-temporal motion pattern models, we can naturally detect unusual activity as statistical deviations. Our experiments show that local spatio-temporal motion pattern modeling offers promising results in real-world scenes with complex activities that are hard for even human observers to analyze.","Motion detection,
Layout,
Event detection,
Hidden Markov models,
Humans,
Motion analysis,
Ultraviolet sources,
Video sequences,
Pattern analysis,
Image motion analysis"
Multi-user diversity in a spectrum sharing system,"We investigate the effects of multi-user diversity in a spectrum sharing system where secondary users restrictively utilize a spectrum licensed to primary users only if interference perceived at primary users is regulated below a predetermined level. This interference regulation affects the characteristics of multiuser diversity gains previously known in non-spectrum sharing systems. Our numerical and analytical results show that the multiuser diversity gain in a spectrum sharing system increases differently according to conditions given by the transmit power of secondary users, P, and a predetermined interference temperature, Q - if P is sufficiently larger than Q, the multiuser diversity gain in terms of capacity scales like log2 (W (Ns)) similarly to a previously known scaling law in the non-spectrum sharing systems, where W(middot) and Ns denote a Lambert W function and the number of secondary transmitters, respectively. However, the scaling law of multiuser diversity gain becomes log2(Ns) as P becomes sufficiently larger such that P Gt QNs.","Interference,
Diversity methods,
Temperature,
Rayleigh channels,
Radio transmitters,
Cognitive radio,
Chromium,
Information technology,
Frequency measurement,
Time measurement"
Dynamic association for load balancing and interference avoidance in multi-cell networks,"Next-generation cellular networks will provide higher cell capacity by adopting advanced physical layer techniques and broader bandwidth. Even in such networks, boundary users would suffer from low throughput due to severe intercell interference and unbalanced user distributions among cells, unless additional schemes to mitigate this problem are employed. In this paper, we tackle this problem by jointly optimizing partial frequency reuse and load-balancing schemes in a multicell network. We formulate this problem as a network-wide utility maximization problem and propose optimal offline and practical online algorithms to solve this. Our online algorithm turns out to be a simple mixture of inter- and intra-cell handover mechanisms for existing users and user association control and cell-site selection mechanisms for newly arriving users. A remarkable feature of the proposed algorithm is that it uses a notion of expected throughput as the decision making metric, as opposed to signal strength in conventional systems. Extensive simulations demonstrate that our online algorithm can not only closely approximate network-wide proportional fairness but also provide two types of gain, interference avoidance gain and load balancing gain, which yield 20~100% throughput improvement of boundary users (depending on traffic load distribution), while not penalizing total system throughput.We also demonstrate that this improvement cannot be achieved by conventional systems using universal frequency reuse and signal strength as the decision making metric.","Load management,
Interference,
Throughput,
Frequency,
Decision making,
Next generation networking,
Land mobile radio cellular systems,
Physical layer,
Bandwidth,
Telecommunication traffic"
You'll never walk alone: Modeling social behavior for multi-target tracking,"Object tracking typically relies on a dynamic model to predict the object's location from its past trajectory. In crowded scenarios a strong dynamic model is particularly important, because more accurate predictions allow for smaller search regions, which greatly simplifies data association. Traditional dynamic models predict the location for each target solely based on its own history, without taking into account the remaining scene objects. Collisions are resolved only when they happen. Such an approach ignores important aspects of human behavior: people are driven by their future destination, take into account their environment, anticipate collisions, and adjust their trajectories at an early stage in order to avoid them. In this work, we introduce a model of dynamic social behavior, inspired by models developed for crowd simulation. The model is trained with videos recorded from birds-eye view at busy locations, and applied as a motion model for multi-people tracking from a vehicle-mounted camera. Experiments on real sequences show that accounting for social interactions and scene knowledge improves tracking performance, especially during occlusions.","Predictive models,
Vehicle dynamics,
Layout,
Humans,
Computer vision,
Trajectory,
Cameras,
Legged locomotion,
Path planning,
Computer science"
Fractional order control - A tutorial,"Many real dynamic systems are better characterized using a non-integer order dynamic model based on fractional calculus or, differentiation or integration of non-integer order. Traditional calculus is based on integer order differentiation and integration. The concept of fractional calculus has tremendous potential to change the way we see, model, and control the nature around us. Denying fractional derivatives is like saying that zero, fractional, or irrational numbers do not exist. In this paper, we offer a tutorial on fractional calculus in controls. Basic definitions of fractional calculus, fractional order dynamic systems and controls are presented first. Then, fractional order PID controllers are introduced which may make fractional order controllers ubiquitous in industry. Additionally, several typical known fractional order controllers are introduced and commented. Numerical methods for simulating fractional order systems are given in detail so that a beginner can get started quickly. Discretization techniques for fractional order operators are introduced in some details too. Both digital and analog realization methods of fractional order operators are introduced. Finally, remarks on future research efforts in fractional order control are given.","Tutorial,
Fractional calculus,
Control systems,
Three-term control,
Electrical equipment industry,
Industrial control,
Numerical simulation,
Voltage,
Propagation losses,
Transmission line theory"
Class segmentation and object localization with superpixel neighborhoods,"We propose a method to identify and localize object classes in images. Instead of operating at the pixel level, we advocate the use of superpixels as the basic unit of a class segmentation or pixel localization scheme. To this end, we construct a classifier on the histogram of local features found in each superpixel. We regularize this classifier by aggregating histograms in the neighborhood of each superpixel and then refine our results further by using the classifier in a conditional random field operating on the superpixel graph. Our proposed method exceeds the previously published state-of-the-art on two challenging datasets: Graz-02 and the PASCAL VOC 2007 Segmentation Challenge.","Image segmentation,
Histograms,
Pixel,
Object detection,
Computer science,
Detectors,
Grid computing,
Merging,
Time measurement,
Statistics"
Observing Human-Object Interactions: Using Spatial and Functional Compatibility for Recognition,"Interpretation of images and videos containing humans interacting with different objects is a daunting task. It involves understanding scene or event, analyzing human movements, recognizing manipulable objects, and observing the effect of the human movement on those objects. While each of these perceptual tasks can be conducted independently, recognition rate improves when interactions between them are considered. Motivated by psychological studies of human perception, we present a Bayesian approach which integrates various perceptual tasks involved in understanding human-object interactions. Previous approaches to object and action recognition rely on static shape or appearance feature matching and motion analysis, respectively. Our approach goes beyond these traditional approaches and applies spatial and functional constraints on each of the perceptual elements for coherent semantic interpretation. Such constraints allow us to recognize objects and actions when the appearances are not discriminative enough. We also demonstrate the use of such constraints in recognition of actions from static images without using any motion information.","Humans,
Image recognition,
Object recognition,
Pattern recognition,
Videos,
Bayesian methods,
Shape,
Motion analysis,
Information analysis,
Layout"
Communities and Emerging Semantics in Semantic Link Network: Discovery and Learning,"The World Wide Web provides plentiful contents for Web-based learning, but its hyperlink-based architecture connects Web resources for browsing freely rather than for effective learning. To support effective learning, an e-learning system should be able to discover and make use of the semantic communities and the emerging semantic relations in a dynamic complex network of learning resources. Previous graph-based community discovery approaches are limited in ability to discover semantic communities. This paper first suggests the semantic link network (SLN), a loosely coupled semantic data model that can semantically link resources and derive out implicit semantic links according to a set of relational reasoning rules. By studying the intrinsic relationship between semantic communities and the semantic space of SLN, approaches to discovering reasoning-constraint, rule-constraint, and classification-constraint semantic communities are proposed. Further, the approaches, principles, and strategies for discovering emerging semantics in dynamic SLNs are studied. The basic laws of the semantic link network motion are revealed for the first time. An e-learning environment incorporating the proposed approaches, principles, and strategies to support effective discovery and learning is suggested.","Electronic learning,
Data models,
Web sites,
Service oriented architecture,
Organizing,
Complex networks,
Image resolution,
Videos,
Relational databases"
Parallel Tracking and Mapping on a camera phone,"Camera phones are a promising platform for hand-held augmented reality. As their computational resources grow, they are becoming increasingly suitable for visual tracking tasks. At the same time, they still offer considerable challenges: Their cameras offer a narrow field-of-view not best suitable for robust tracking; images are often received at less than 15Hz; long exposure times result in significant motion blur; and finally, a rolling shutter causes severe smearing effects. This paper describes an attempt to implement a keyframe-based SLAMsystem on a camera phone (specifically, the Apple iPhone 3G). We describe a series of adaptations to the Parallel Tracking and Mapping system to mitigate the impact of the device's imaging deficiencies. Early results demonstrate a system capable of generating and augmenting small maps, albeit with reduced accuracy and robustness compared to SLAM on a PC.",
A Fuzzy Locally Adaptive Bayesian Segmentation Approach for Volume Determination in PET,"Accurate volume estimation in positron emission tomography (PET) is crucial for different oncology applications. The objective of our study was to develop a new fuzzy locally adaptive Bayesian (FLAB) segmentation for automatic lesion volume delineation. FLAB was compared with a threshold approach as well as the previously proposed fuzzy hidden Markov chains (FHMC) and the fuzzy C-Means (FCM) algorithms. The performance of the algorithms was assessed on acquired datasets of the IEC phantom, covering a range of spherical lesion sizes (10-37 mm), contrast ratios (4:1 and 8:1), noise levels (1, 2, and 5 min acquisitions), and voxel sizes (8 and 64 mm3). In addition, the performance of the FLAB model was assessed on realistic nonuniform and nonspherical volumes simulated from patient lesions. Results show that FLAB performs better than the other methodologies, particularly for smaller objects. The volume error was 5%-15% for the different sphere sizes (down to 13 mm), contrast and image qualities considered, with a high reproducibility (variation < 4%). By comparison, the thresholding results were greatly dependent on image contrast and noise, whereas FCM results were less dependent on noise but consistently failed to segment lesions < 2 cm. In addition, FLAB performed consistently better for lesions < 2 cm in comparison to the FHMC algorithm. Finally the FLAB model provided errors less than 10% for nonspherical lesions with inhomogeneous activity distributions. Future developments will concentrate on an extension of FLAB in order to allow the segmentation of separate activity distribution regions within the same functional volume as well as a robustness study with respect to different scanners and reconstruction algorithms.",
Designing efficient sorting algorithms for manycore GPUs,"We describe the design of high-performance parallel radix sort and merge sort routines for manycore GPUs, taking advantage of the full programmability offered by CUDA. Our radix sort is the fastest GPU sort and our merge sort is the fastest comparison-based sort reported in the literature. Our radix sort is up to 4 times faster than the graphics-based GPUSort and greater than 2 times faster than other CUDA-based radix sorts. It is also 23% faster, on average, than even a very carefully optimized multicore CPU sorting routine. To achieve this performance, we carefully design our algorithms to expose substantial fine-grained parallelism and decompose the computation into independent tasks that perform minimal global communication. We exploit the high-speed onchip shared memory provided by NVIDIA's GPU architecture and efficient data-parallel primitives, particularly parallel scan. While targeted at GPUs, these algorithms should also be well-suited for other manycore processors.",
Histograms of oriented optical flow and Binet-Cauchy kernels on nonlinear dynamical systems for the recognition of human actions,"System theoretic approaches to action recognition model the dynamics of a scene with linear dynamical systems (LDSs) and perform classification using metrics on the space of LDSs, e.g. Binet-Cauchy kernels. However, such approaches are only applicable to time series data living in a Euclidean space, e.g. joint trajectories extracted from motion capture data or feature point trajectories extracted from video. Much of the success of recent object recognition techniques relies on the use of more complex feature descriptors, such as SIFT descriptors or HOG descriptors, which are essentially histograms. Since histograms live in a non-Euclidean space, we can no longer model their temporal evolution with LDSs, nor can we classify them using a metric for LDSs. In this paper, we propose to represent each frame of a video using a histogram of oriented optical flow (HOOF) and to recognize human actions by classifying HOOF time-series. For this purpose, we propose a generalization of the Binet-Cauchy kernels to nonlinear dynamical systems (NLDS) whose output lives in a non-Euclidean space, e.g. the space of histograms. This can be achieved by using kernels defined on the original non-Euclidean space, leading to a well-defined metric for NLDSs. We use these kernels for the classification of actions in video sequences using (HOOF) as the output of the NLDS. We evaluate our approach to recognition of human actions in several scenarios and achieve encouraging results.",
An adaptive geometry-based stochastic model for non-isotropic MIMO mobile-to-mobile channels,"In this paper, a generic and adaptive geometrybased stochastic model (GBSM) is proposed for non-isotropic multiple-input multiple-output (MIMO) mobile-to-mobile (M2M) Ricean fading channels. The proposed model employs a combined two-ring model and ellipse model, where the received signal is constructed as a sum of the line-of-sight, single-, and doublebounced rays with different energies. This makes the model sufficiently generic and adaptable to a variety of M2M scenarios (macro-, micro-, and pico-cells). More importantly, our model is the first GBSM that has the ability to study the impact of the vehicular traffic density on channel characteristics. From the proposed model, the space-time-frequency correlation function and the corresponding space-Doppler-frequency power spectral density (PSD) of any two sub-channels are derived for a non-isotropic scattering environment. Based on the detailed investigation of correlations and PSDs, some interesting observations and useful conclusions are obtained. These observations and conclusions can be considered as a guidance for setting important parameters of our model appropriately and building up more purposeful measurement campaigns in the future. Finally, close agreement is achieved between the theoretical results and measured data, demonstrating the utility of the proposed model.",
"Characterizing flash memory: Anomalies, observations, and applications","Despite flash memory's promise, it suffers from many idiosyncrasies such as limited durability, data integrity problems, and asymmetry in operation granularity. As architects, we aim to find ways to overcome these idiosyncrasies while exploiting flash memory's useful characteristics. To be successful, we must understand the trade-offs between the performance, cost (in both power and dollars), and reliability of flash memory. In addition, we must understand how different usage patterns affect these characteristics. Flash manufacturers provide conservative guidelines about these metrics, and this lack of detail makes it difficult to design systems that fully exploit flash memory's capabilities. We have empirically characterized flash memory technology from five manufacturers by directly measuring the performance, power, and reliability. We demonstrate that performance varies significantly between vendors, devices, and from publicly available datasheets. We also demonstrate and quantify some unexpected device characteristics and show how we can use them to improve responsiveness and energy consumption of solid state disks by 44% and 13%, respectively, as well as increase flash device lifetime by 5.2x.",
An Active Contour Model for Segmenting and Measuring Retinal Vessels,"This paper presents an algorithm for segmenting and measuring retinal vessels, by growing a ldquoRibbon of Twinsrdquo active contour model, which uses two pairs of contours to capture each vessel edge, while maintaining width consistency. The algorithm is initialized using a generalized morphological order filter to identify approximate vessels centerlines. Once the vessel segments are identified the network topology is determined using an implicit neural cost function to resolve junction configurations. The algorithm is robust, and can accurately locate vessel edges under difficult conditions, including noisy blurred edges, closely parallel vessels, light reflex phenomena, and very fine vessels. It yields precise vessel width measurements, with subpixel average width errors. We compare the algorithm with several benchmarks from the literature, demonstrating higher segmentation sensitivity and more accurate width measurement.",
What is the right model for wireless channel interference?,"In wireless communications, the desired wireless signal is typically decoded by treating the sum of all the other ongoing signal transmissions as noise. In the networking literature, this phenomenon is typically abstracted using a wireless channel interference model. The level of detail in the interference model, evidently determines the accuracy of the results based upon the model. Several works in the networking literature have made use of simplistic interference models, e.g., fixed ranges for communication and interference, the capture threshold model (used in the ns2 network simulator), the protocol model, and so on. At the same time, fairly complex interference models such as those based on the SINR (signal-to-interference-and-noise ratio) have also been proposed and used. We investigate the impact of the choice of the interference model, on the conclusions that can be drawn regarding the performance of wireless networks, by comparing different wireless interference models. We find that both in the case of random access networks, as well as in the case of scheduled networks (where node transmissions are scheduled to be completely conflict-free), different interference models can produce significantly different results. Therefore, a lot of caution should be exercised before accepting or interpreting results based on simplified interference models. Further, we feel that an SINR-based model is the minimum level of detail that should be employed to model wireless channel interference in a networking context.",
Determination of Electric Conductivity and Local SAR Via B1 Mapping,"The electric conductivity can potentially be used as an additional diagnostic parameter, e.g., in tumor diagnosis. Moreover, the electric conductivity, in connection with the electric field, can be used to estimate the local SAR distribution during MR measurements. In this study, a new approach, called electric properties tomography (EPT) is presented. It derives the patient's electric conductivity, along with the corresponding electric fields, from the spatial sensitivity distributions of the applied RF coils, which are measured via MRI. Corresponding numerical simulations and initial experiments on a standard clinical MRI system underline the principal feasibility of EPT to determine the electric conductivity and the local SAR. In contrast to previous methods to measure the patient's electric properties, EPT does not apply externally mounted electrodes, currents, or RF probes, thus enhancing the practicality of the approach. Furthermore, in contrast to previous methods, EPT circumvents the solution of an inverse problem, which might lead to significantly higher spatial image resolution.","Electric variables measurement,
Conductivity measurement,
Radio frequency,
Magnetic resonance imaging,
Neoplasms,
Tomography,
Coils,
Numerical simulation,
Current measurement,
Electrodes"
Indoor broadcasting via white LEDs and OFDM,"Recently, visible light communication (VLC) technology has been gaining attention in both academia and industry. This is driven by the progress of white light emitting diode (LED) technology for solid-state lighting (SSL) and the potential of simultaneously using such LEDs for illumination and indoor wireless data transmission. This paper provides an overview about the technology and describes the physical layer implementation of a VLC system based on a modified version of the classical orthogonal frequency division multiplexing (OFDM) modulation technique. Besides, the paper presents a hardware prototype for short-range broadcasting using a white LED lamp. The OFDM system runs on DSP development boards. Off-the-shelf 9 LEDs and a single photodiode (PD) are utilized to build the analog frontends. The prototype allows investigating the influence of the electrical signal-to-noise ratio (SNR), constellation order, and channel coding on the bit-error performance. Theoretical and experimental results on optical path loss show close match. In this context, the influence of the LED beam angle on the horizontal coverage is highlighted.",
Toward Accurate and Fast Iris Segmentation for Iris Biometrics,"Iris segmentation is an essential module in iris recognition because it defines the effective image region used for subsequent processing such as feature extraction. Traditional iris segmentation methods often involve an exhaustive search of a large parameter space, which is time consuming and sensitive to noise. To address these problems, this paper presents a novel algorithm for accurate and fast iris segmentation. After efficient reflection removal, an Adaboost-cascade iris detector is first built to extract a rough position of the iris center. Edge points of iris boundaries are then detected, and an elastic model named pulling and pushing is established. Under this model, the center and radius of the circular iris boundaries are iteratively refined in a way driven by the restoring forces of Hooke's law. Furthermore, a smoothing spline-based edge fitting scheme is presented to deal with noncircular iris boundaries. After that, eyelids are localized via edge detection followed by curve fitting. The novelty here is the adoption of a rank filter for noise elimination and a histogram filter for tackling the shape irregularity of eyelids. Finally, eyelashes and shadows are detected via a learned prediction model. This model provides an adaptive threshold for eyelash and shadow detection by analyzing the intensity distributions of different iris regions. Experimental results on three challenging iris image databases demonstrate that the proposed algorithm outperforms state-of-the-art methods in both accuracy and speed.","Biometrics,
Image segmentation,
Image edge detection,
Eyelids,
Filters,
Eyelashes,
Iris recognition,
Feature extraction,
Acoustic reflection,
Detectors"
High-Performance Cloud Computing: A View of Scientific Applications,"Scientific computing often requires the availability of a massive number of computers for performing large scale experiments. Traditionally, these needs have been addressed by using high-performance computing solutions and installed facilities such as clusters and super computers, which are difficult to setup, maintain, and operate. Cloud computing provides scientists with a completely new model of utilizing the computing infrastructure. Compute resources, storage resources, as well as applications, can be dynamically provisioned (and integrated within the existing infrastructure) on a pay per use basis. These resources can be released when they are no more needed. Such services are often offered within the context of a Service Level Agreement (SLA), which ensure the desired Quality of Service (QoS). Aneka, an enterprise Cloud computing solution, harnesses the power of compute resources by relying on private and public Clouds and delivers to users the desired QoS. Its flexible and service based infrastructure supports multiple programming paradigms that make Aneka address a variety of different scenarios: from finance applications to computational science. As examples of scientific computing in the Cloud, we present a preliminary case study on using Aneka for the classification of gene expression data and the execution of fMRI brain imaging workflow.",
Automated 3-D Intraretinal Layer Segmentation of Macular Spectral-Domain Optical Coherence Tomography Images,"With the introduction of spectral-domain optical coherence tomography (OCT), much larger image datasets are routinely acquired compared to what was possible using the previous generation of time-domain OCT. Thus, the need for 3-D segmentation methods for processing such data is becoming increasingly important. We report a graph-theoretic segmentation method for the simultaneous segmentation of multiple 3-D surfaces that is guaranteed to be optimal with respect to the cost function and that is directly applicable to the segmentation of 3-D spectral OCT image data. We present two extensions to the general layered graph segmentation method: the ability to incorporate varying feasibility constraints and the ability to incorporate true regional information. Appropriate feasibility constraints and cost functions were learned from a training set of 13 spectral-domain OCT images from 13 subjects. After training, our approach was tested on a test set of 28 images from 14 subjects. An overall mean unsigned border positioning error of 5.69 plusmn 2.41 mum was achieved when segmenting seven surfaces (six layers) and using the average of the manual tracings of two ophthalmologists as the reference standard. This result is very comparable to the measured interobserver variability of 5.71 plusmn 1.98 mum.","Image segmentation,
Tomography,
Cities and towns,
Optical sensors,
Biomedical optical imaging,
Time domain analysis,
Cost function,
Testing,
Biomedical imaging,
Oncology"
PDRAM: A hybrid PRAM and DRAM main memory system,"In this paper, we propose PDRAM, a novel energy efficient main memory architecture based on phase change random access memory (PRAM) and DRAM. The paper explores the challenges involved in incorporating PRAM into the main memory hierarchy of computing systems, and proposes a low overhead hybrid hardware-software solution for managing it. Our experimental results indicate that our solution is able to achieve average energy savings of 30% at negligible overhead over conventional memory architectures.","Phase change random access memory,
Random access memory,
Memory management,
Power system management,
Energy management,
Hardware,
Energy efficiency,
Memory architecture,
Energy consumption,
Costs"
Multi-cue onboard pedestrian detection,"Various powerful people detection methods exist. Surprisingly, most approaches rely on static image features only despite the obvious potential of motion information for people detection. This paper systematically evaluates different features and classifiers in a sliding-window framework. First, our experiments indicate that incorporating motion information improves detection performance significantly. Second, the combination of multiple and complementary feature types can also help improve performance. And third, the choice of the classifier-feature combination and several implementation details are crucial to reach best performance. In contrast to many recent papers experimental results are reported for four different datasets rather than using a single one. Three of them are taken from the literature allowing for direct comparison. The fourth dataset is newly recorded using an onboard camera driving through urban environment. Consequently this dataset is more realistic and more challenging than any currently available dataset.","Detectors,
Cameras,
Motion detection,
Computer vision,
Robot vision systems,
Humans,
Image motion analysis,
Object detection,
Boosting,
Histograms"
Recognising action as clouds of space-time interest points,"Much of recent action recognition research is based on space-time interest points extracted from video using a Bag of Words (BOW) representation. It mainly relies on the discriminative power of individual local space-time descriptors, whilst ignoring potentially valuable information about the global spatio-temporal distribution of interest points. In this paper, we propose a novel action recognition approach which differs significantly from previous interest points based approaches in that only the global spatiotemporal distribution of the interest points are exploited. This is achieved through extracting holistic features from clouds of interest points accumulated over multiple temporal scales followed by automatic feature selection. Our approach avoids the non-trivial problems of selecting the optimal space-time descriptor, clustering algorithm for constructing a codebook, and selecting codebook size faced by previous interest points based methods. Our model is able to capture smooth motions, robust to view changes and occlusions at a low computation cost. Experiments using the KTH and WEIZMANN datasets demonstrate that our approach outperforms most existing methods.","Clouds,
Data mining,
Shape,
Clustering algorithms,
Robustness,
Computational efficiency,
Cameras,
Noise shaping,
Power engineering and energy,
Computer science"
Discriminative models for multi-class object layout,"Many state-of-the-art approaches for object recognition reduce the problem to a 0-1 classification task. Such reductions allow one to leverage sophisticated classifiers for learning. These models are typically trained independently for each class using positive and negative examples cropped from images. At test-time, various post-processing heuristics such as non-maxima suppression (NMS) are required to reconcile multiple detections within and between different classes for each image. Though crucial to good performance on benchmarks, this post-processing is usually defined heuristically.","Object detection,
Object recognition,
Detectors,
Benchmark testing,
Predictive models,
Statistics,
Face detection,
Image edge detection,
Computer science,
Labeling"
Graph Twiddling in a MapReduce World,"As the size of graphs for analysis continues to grow, methods of graph processing that scale well have become increasingly important. One way to handle large datasets is to disperse them across an array of networked computers, each of which implements simple sorting and accumulating, or MapReduce, operations. This cloud computing approach offers many attractive features. If decomposing useful graph operations in terms of MapReduce cycles is possible, it provides incentive for seriously considering cloud computing. Moreover, it offers a way to handle a large graph on a single machine that can't hold the entire graph as well as enables streaming graph processing. This article examines this possibility.","Distributed computing,
Cloud computing,
Sorting,
Packaging,
Humans,
Distributed processing,
Computer networks,
Hardware,
Robustness,
National security"
On the study of network coding with diversity,"Recently proposed physical-layer network coding (PNC) has demonstrated the promise to significantly improve the throughput of wireless networks whose links can be modeled as additive white Gaussian noise (AWGN) channels. However, the extension to multipath channels is problematic, since the technique would then require both amplitude and phase compensation at each transmitter. Phase compensation requires accurate distributed phase tracking, whereas the required amplitude compensation is even more troubling, as it leads to an inefficient system that yields no diversity even in the presence of perfect channel estimates. Here, a system that avoids these limitations is obtained by reaching up one level higher in the network hierarchy and performing distributed relay selection with cognizance of the PNC technique that we will employ at the physical layer. Since the resulting scheme will achieve a form of selection diversity, we term it ldquonetwork coding with diversityrdquo (NCD). To facilitate performance evaluation, two information-theoretic metrics, the outage and ergodic capacity, are studied. Our analytical and simulation results show that the proposed protocol achieves more robust performance and higher system throughput than comparable schemes. Finally, the proposed network coding is extended to the context of cooperative multiple access channels, which yields a new cooperative protocol with larger outage and ergodic capacity compared with existing transmission schemes.","Network coding,
Throughput,
AWGN,
Access protocols,
Wireless networks,
Additive white noise,
Multipath channels,
Transmitters,
Amplitude estimation,
Phase estimation"
Encounter-Based Routing in DTNs,"Current work in routing protocols for delay and disruption tolerant networks leverage epidemic-style algorithms that trade off injecting many copies of messages into the network for increased probability of message delivery. However, such techniques can cause a large amount of contention in the network, increase overall delays, and drain each mobile node's limited battery supply. We present a new DTN routing algorithm, called Encounter-Based Routing (EBR), which maximizes delivery ratios while minimizing overhead and delay. Furthermore, we present a means of securing EBR against black hole denial- of-service attacks. EBR achieves up to a 40% improvement in message delivery over the current state-of-the-art, as well as achieving up to a 145% increase in goodput. Also, we further show how EBR outperforms other protocols by introduce three new composite metrics that better characterize DTN routing performance.","Disruption tolerant networking,
Routing protocols,
Peer to peer computing,
Delay,
Computer crime,
Bandwidth,
Taxonomy,
Communications Society,
Computer science,
Batteries"
Single image haze removal using dark channel prior,"In this paper, we propose a simple but effective image prior - dark channel prior to remove haze from a single input image. The dark channel prior is a kind of statistics of the haze-free outdoor images. It is based on a key observation - most local patches in haze-free outdoor images contain some pixels which have very low intensities in at least one color channel. Using this prior with the haze imaging model, we can directly estimate the thickness of the haze and recover a high quality haze-free image. Results on a variety of outdoor haze images demonstrate the power of the proposed prior. Moreover, a high quality depth map can also be obtained as a by-product of haze removal.","Statistics,
Pixel"
Recovering the spatial layout of cluttered rooms,"In this paper, we consider the problem of recovering the spatial layout of indoor scenes from monocular images. The presence of clutter is a major problem for existing single-view 3D reconstruction algorithms, most of which rely on finding the ground-wall boundary. In most rooms, this boundary is partially or entirely occluded. We gain robustness to clutter by modeling the global room space with a parameteric 3D “box” and by iteratively localizing clutter and refitting the box. To fit the box, we introduce a structured learning algorithm that chooses the set of parameters to minimize error, based on global perspective cues. On a dataset of 308 images, we demonstrate the ability of our algorithm to recover spatial layout in cluttered rooms and show several examples of estimated free space.","Layout,
Robustness,
Parameter estimation,
Image segmentation,
Iterative algorithms,
Labeling,
Floors,
Computer science,
Reconstruction algorithms,
Humans"
Minimum Mean Squared Error interference alignment,"To achieve the full multiplexing gain of MIMO interference networks at high SNRs, the interference from different transmitters must be aligned in lower-dimensional subspaces at the receivers. Recently a distributed ¿max-SINR¿ algorithm for precoder optimization has been proposed that achieves interference alignment for sufficiently high SNRs. We show that this algorithm can be interpreted as a variation of an algorithm that minimizes the sum Mean Squared Error (MSE). To maximize sum utility, where the utility depends on rate or SINR, a weighted sum MSE objective is used to compute the beams, where the weights are updated according to the sum utility objective. We specify a class of utility functions for which convergence of the sum utility to a local optimum is guaranteed with asynchronous updates of beams, receiver filters, and utility weights. Numerical results are presented, which show that this method achieves interference alignment at high SNRs, and can achieve different points on the boundary of the achievable rate region by adjusting the MSE weights.",
Local Trinary Patterns for human action recognition,"We present a novel action recognition method which is based on combining the effective description properties of Local Binary Patterns with the appearance invariance and adaptability of patch matching based methods. The resulting method is extremely efficient, and thus is suitable for real-time uses of simultaneous recovery of human action of several lengths and starting points. Tested on all publicity available datasets in the literature known to us, our system repeatedly achieves state of the art performance. Lastly, we present a new benchmark that focuses on uncut motion recognition in broadcast sports video.",
Manhattan-world stereo,"Multi-view stereo (MVS) algorithms now produce reconstructions that rival laser range scanner accuracy. However, stereo algorithms require textured surfaces, and therefore work poorly for many architectural scenes (e.g., building interiors with textureless, painted walls). This paper presents a novel MVS approach to overcome these limitations for Manhattan World scenes, i.e., scenes that consists of piece-wise planar surfaces with dominant directions. Given a set of calibrated photographs, we first reconstruct textured regions using an existing MVS algorithm, then extract dominant plane directions, generate plane hypotheses, and recover per-view depth maps using Markov random fields. We have tested our algorithm on several datasets ranging from office interiors to outdoor buildings, and demonstrate results that outperform the current state of the art for such texture-poor scenes.",
The performance of a new version of MOEA/D on CEC09 unconstrained MOP test instances,This paper describes the idea of MOEA/D and proposes a strategy for allocating the computational resource to different subproblems in MOEA/D. The new version of MOEA/D has been tested on all the CEC09 unconstrained MOP test instances.,"Testing,
Evolutionary computation,
Computer science,
Pareto optimization,
Resource management,
Tellurium"
"WiMAX femtocell: requirements, challenges, and solutions","Femtocells are low-power base stations operating in licensed spectrum, which are typically installed indoors in a house, small office, or home office by end users to provide exclusive or preferential access to a designated group of users as configured by the femtocell subscriber and/or the access provider. Femtocells have gained a lot of attention recently due to their advantages in terms of infrastructure cost saving and improved user experience in indoor environments. Several standards bodies, such as 3GPP, WiMAX Forum, and IEEE 802.16, have started to develop standard solutions to enable and optimize femtocells operation. However, there are some technical challenges to the success of femtocell technologies that need to be overcome before they can be deployed on large scales. This article presents an overview of WiMAX femtocell requirements, deployment models, and solutions in the near and long terms.","WiMAX,
Femtocell networks,
Macrocell networks,
Costs,
Indoor environments,
Quality of service,
Frequency synchronization,
Interference,
Base stations,
Standards organizations"
On the limits of communication with low-precision analog-to-digital conversion at the receiver,"As communication systems scale up in speed and bandwidth, the cost and power consumption of high-precision (e.g., 8-12 bits) analog-to-digital conversion (ADC) becomes the limiting factor in modern transceiver architectures based on digital signal processing. In this work, we explore the impact of lowering the precision of the ADC on the performance of the communication link. Specifically, we evaluate the communication limits imposed by low-precision ADC (e.g., 1-3 bits) for transmission over the real discrete-time additive white Gaussian noise (AWGN) channel, under an average power constraint on the input. For an ADC with K quantization bins (i.e., a precision of log2 K bits), we show that the input distribution need not have any more than K+1 mass points to achieve the channel capacity. For 2-bin (1-bit) symmetric quantization, this result is tightened to show that binary antipodal signaling is optimum for any signal-to- noise ratio (SNR). For multi-bit quantization, a dual formulation of the channel capacity problem is used to obtain tight upper bounds on the capacity. The cutting-plane algorithm is employed to compute the capacity numerically, and the results obtained are used to make the following encouraging observations : (a) up to a moderately high SNR of 20 dB, 2-3 bit quantization results in only 10-20% reduction of spectral efficiency compared to unquantized observations, (b) standard equiprobable pulse amplitude modulated input with quantizer thresholds set to implement maximum likelihood hard decisions is asymptotically optimum at high SNR, and works well at low to moderate SNRs as well.","Analog-digital conversion,
Quantization,
AWGN,
Additive white noise,
Channel capacity,
Signal to noise ratio,
Pulse modulation,
Bandwidth,
Costs,
Energy consumption"
Towards a navigation system for autonomous indoor flying,"Recently there has been increasing research on the development of autonomous flying vehicles.Whereas most of the proposed approaches are suitable for outdoor operation, only a few techniques have been designed for indoor environments. In this paper we present a general system consisting of sensors and algorithms which enables a small sized flying vehicle to operate indoors. This is done by adapting techniques which have been successfully applied on ground robots. We released our system as open-source with the intention to provide the community with a new framework for building applications for indoor flying robots. We present a set of experiments to validate our system on an open source quadrotor.","Robot sensing systems,
Mobile robots,
Remotely operated vehicles,
Land vehicles,
Helicopters,
Indoor environments,
Payloads,
Global Positioning System,
Aircraft navigation,
Simultaneous localization and mapping"
Sensible Organizations: Technology and Methodology for Automatically Measuring Organizational Behavior,"We present the design, implementation, and deployment of a wearable computing platform for measuring and analyzing human behavior in organizational settings. We propose the use of wearable electronic badges capable of automatically measuring the amount of face-to-face interaction, conversational time, physical proximity to other people, and physical activity levels in order to capture individual and collective patterns of behavior. Our goal is to be able to understand how patterns of behavior shape individuals and organizations. By using on-body sensors in large groups of people for extended periods of time in naturalistic settings, we have been able to identify, measure, and quantify social interactions, group behavior, and organizational dynamics. We deployed this wearable computing platform in a group of 22 employees working in a real organization over a period of one month. Using these automatic measurements, we were able to predict employees' self-assessments of job satisfaction and their own perceptions of group interaction quality by combining data collected with our platform and e-mail communication data. In particular, the total amount of communication was predictive of both of these assessments, and betweenness in the social network exhibited a high negative correlation with group interaction satisfaction. We also found that physical proximity and e-mail exchange had a negative correlation of r = -0.55&nbsp;(p 0.01), which has far-reaching implications for past and future research on social networks.","Humans,
Wearable computers,
Electronic mail,
Social network services,
Laboratories,
Anthropometry,
Time measurement,
Shape,
Wearable sensors"
A highly resilient routing algorithm for fault-tolerant NoCs,"Current trends in technology scaling foreshadow worsening transistor reliability as well as greater numbers of transistors in each system. The combination of these factors will soon make long-term product reliability extremely difficult in complex modern systems such as systems on a chip (SoC) and chip multiprocessor (CMP) designs, where even a single device failure can cause fatal system errors. Resiliency to device failure will be a necessary condition at future technology nodes. In this work, we present a network-on-chip (NoC) routing algorithm to boost the robustness in interconnect networks, by reconfiguring them to avoid faulty components while maintaining connectivity and correct operation. This distributed algorithm can be implemented in hardware with less than 300 gates per network router. Experimental results over a broad range of 2D-mesh and 2D-torus networks demonstrate 99.99% reliability on average when 10% of the interconnect links have failed.","Routing,
Fault tolerance,
Network-on-a-chip,
Hardware,
System-on-a-chip,
Maintenance,
Joining processes,
Fault tolerant systems,
Monte Carlo methods,
Computer network reliability"
Comparing anomaly-detection algorithms for keystroke dynamics,"Keystroke dynamics-the analysis of typing rhythms to discriminate among users-has been proposed for detecting impostors (i.e., both insiders and external attackers). Since many anomaly-detection algorithms have been proposed for this task, it is natural to ask which are the top performers (e.g., to identify promising research directions). Unfortunately, we cannot conduct a sound comparison of detectors using the results in the literature because evaluation conditions are inconsistent across studies. Our objective is to collect a keystroke-dynamics data set, to develop a repeatable evaluation procedure, and to measure the performance of a range of detectors so that the results can be compared soundly. We collected data from 51 subjects typing 400 passwords each, and we implemented and evaluated 14 detectors from the keystroke-dynamics and pattern-recognition literature. The three top-performing detectors achieve equal-error rates between 9.6% and 10.2%. The results-along with the shared data and evaluation methodology-constitute a benchmark for comparing detectors and measuring progress.",
On capacity of cognitive radio networks with average interference power constraints,"Cognitive radio (CR) has been considered as a promising technology to improve the spectrum utilization. In this paper we analyze the capacity of a CR network with average received interference power constraints. Under the assumptions of uniform node placements and a simple power control scheme, the maximum transmit power of a target CR transmitter is characterized by its cumulative distribution function (CDF). We study two CR scenarios for future applications. The first scenario is called the CR based central access network, which aims at providing broadband access to CR devices. In the second scenario, the so-called CR assisted virtual multiple-input multiple-output (MIMO) network, CR is used to improve the access capability of a cellular system. The uplink ergodic channel capacities of both scenarios are derived and analyzed with an emphasis on understanding the impact of numbers of primary users and CR users on the capacity. Numerical and simulation results suggest that the CR based central access network is more suitable for less-populated rural areas where a relatively low density of primary receivers is expected; while the CR assisted virtual MIMO network performs better in urban environments with a dense population of mobile CR users.","Cognitive radio,
Interference constraints,
Chromium,
MIMO,
Power control,
Radio transmitters,
Distribution functions,
Cellular networks,
Channel capacity,
Numerical simulation"
The Application of Compressed Sensing for Photo-Acoustic Tomography,"Photo-acoustic (PA) imaging has been developed for different purposes, but recently, the modality has gained interest with applications to small animal imaging. As a technique it is sensitive to endogenous optical contrast present in tissues and, contrary to diffuse optical imaging, it promises to bring high resolution imaging for in vivo studies at midrange depths (3-10 mm). Because of the limited amount of radiation tissues can be exposed to, existing reconstruction algorithms for circular tomography require a great number of measurements and averaging, implying long acquisition times. Time-resolved PA imaging is therefore possible only at the cost of complex and expensive electronics. This paper suggests a new reconstruction strategy using the compressed sensing formalism which states that a small number of linear projections of a compressible image contain enough information for reconstruction. By directly sampling the image to recover in a sparse representation, it is possible to dramatically reduce the number of measurements needed for a given quality of reconstruction.","Compressed sensing,
Tomography,
Optical imaging,
High-resolution imaging,
Image reconstruction,
Animals,
Optical sensors,
Reconstruction algorithms,
Costs,
Image coding"
Learning invariant features through topographic filter maps,"Several recently-proposed architectures for high-performance object recognition are composed of two main stages: a feature extraction stage that extracts locally-invariant feature vectors from regularly spaced image patches, and a somewhat generic supervised classifier. The first stage is often composed of three main modules: (1) a bank of filters (often oriented edge detectors); (2) a non-linear transform, such as a point-wise squashing functions, quantization, or normalization; (3) a spatial pooling operation which combines the outputs of similar filters over neighboring regions. We propose a method that automatically learns such feature extractors in an unsupervised fashion by simultaneously learning the filters and the pooling units that combine multiple filter outputs together. The method automatically generates topographic maps of similar filters that extract features of orientations, scales, and positions. These similar filters are pooled together, producing locally-invariant outputs. The learned feature descriptors give comparable results as SIFT on image recognition tasks for which SIFT is well suited, and better results than SIFT on tasks for which SIFT is less well suited.","Feature extraction,
Filter bank,
Image edge detection,
Image recognition,
Object recognition,
Detectors,
Quantization,
Proposals,
Robustness,
Brain modeling"
Service Performance and Analysis in Cloud Computing,"Cloud computing is a new cost-efficient computing paradigm in which information and computer power can be accessed from aWeb browser by customers. Understanding the characteristics of computer service performance has become critical for service applications in cloud computing. For the commercial success of this new computing paradigm, the ability to deliver Quality of Services (QoS) guaranteed services is crucial. In this paper, we present an approach for studying computer service performance in cloud computing. Specifically, in an effort to deliver QoS guaranteed services in such a computing environment, we find the relationship among the maximal number of customers, the minimal service resources and the highest level of services. The obtained results provide the guidelines of computer service performance in cloud computing that would be greatly useful in the design of this new computing paradigm.","Performance analysis,
Cloud computing,
Grid computing,
Quality of service,
Computer science,
Web and internet services,
Web server,
Information analysis,
Business,
Application software"
Object detection using a max-margin Hough transform,"We present a discriminative Hough transform based object detector where each local part casts a weighted vote for the possible locations of the object center. We show that the weights can be learned in a max-margin framework which directly optimizes the classification performance. The discriminative training takes into account both the codebook appearance and the spatial distribution of its position with respect to the object center to derive its importance. On various datasets we show that the discriminative training improves the Hough detector. Combined with a verification step using a SVM based classifier, our approach achieves a detection rate of 91.9% at 0.3 false positives per image on the ETHZ shape dataset, a significant improvement over the state of the art, while running the verification step on at least an order of magnitude fewer windows than in a sliding window approach.","Object detection,
Voting,
Shape,
Detectors,
Face detection,
Computer science,
Support vector machines,
Support vector machine classification,
Monte Carlo methods,
Packaging"
A Framework for Geometric Analysis of Vascular Structures: Application to Cerebral Aneurysms,"There is well-documented evidence that vascular geometry has a major impact in blood flow dynamics and consequently in the development of vascular diseases, like atherosclerosis and cerebral aneurysmal disease. The study of vascular geometry and the identification of geometric features associated with a specific pathological condition can therefore shed light into the mechanisms involved in the pathogenesis and progression of the disease. Although the development of medical imaging technologies is providing increasing amounts of data on the three-dimensional morphology of the in vivo vasculature, robust and objective tools for quantitative analysis of vascular geometry are still lacking. In this paper, we present a framework for the geometric analysis of vascular structures, in particular for the quantification of the geometric relationships between the elements of a vascular network based on the definition of centerlines. The framework is founded upon solid computational geometry criteria, which confer robustness of the analysis with respect to the high variability of in vivo vascular geometry. The techniques presented are readily available as part of the VMTK, an open source framework for image segmentation, geometric characterization, mesh generation and computational hemodynamics specifically developed for the analysis of vascular structures. As part of the Aneurisk project, we present the application of the present framework to the characterization of the geometric relationships between cerebral aneurysms and their parent vasculature.","Aneurysm,
Diseases,
Robustness,
Image analysis,
Computational geometry,
Blood flow,
Atherosclerosis,
Pathology,
Pathogens,
Biomedical imaging"
A Fully Quantum Asymptotic Equipartition Property,"The classical asymptotic equipartition property is the statement that, in the limit of a large number of identical repetitions of a random experiment, the output sequence is virtually certain to come from the typical set, each member of which is almost equally likely. In this paper, a fully quantum generalization of this property is shown, where both the output of the experiment and side information are quantum. An explicit bound on the convergence is given, which is independent of the dimensionality of the side information. This naturally leads to a family of REacutenyi-like quantum conditional entropies, for which the von Neumann entropy emerges as a special case.",
Fixed SINR solutions for the MIMO wiretap channel,"This paper studies the use of artificial interference in reducing the likelihood that a message transmitted between two multi-antenna nodes is intercepted by an undetected eavesdropper. Unlike previous work that assumes some prior knowledge of the eavesdropper's channel and focuses on the information theoretic concept of secrecy capacity, we also consider the case where no information regarding the eavesdropper is present, and we use the relative signal-to-interference-plus-noise-ratio (SINR) of a single transmitted data stream as our performance metric. A portion of the transmit power is used to broadcast the information signal with just enough power to guarantee a certain SINR at the desired receiver, and the remainder of the power is used to broadcast artificial noise in order to mask the desired signal from a potential eavesdropper. The interference is designed to be orthogonal to the information signal when it reaches the desired receiver, and we study the resulting relative SINR of the desired receiver and the eavesdropper assuming both employ optimal beamformers.",
SMPSO: A new PSO-based metaheuristic for multi-objective optimization,"In this work, we present a new multi-objective particle swarm optimization algorithm (PSO) characterized by the use of a strategy to limit the velocity of the particles. The proposed approach, called Speed-constrained Multi-objective PSO (SMPSO) allows to produce new effective particle positions in those cases in which the velocity becomes too high. Other features of SMPSO include the use of polynomial mutation as a turbulence factor and an external archive to store the non-dominated solutions found during the search. Our proposed approach is compared with respect to five multi-objective metaheuristics representative of the state-of-the-art in the area. For the comparison, two different criteria are adopted: the quality of the resulting approximation sets and the convergence speed to the Pareto front. The experiments carried out indicate that SMPSO obtains remarkable results in terms of both, accuracy and speed.",
"Live virtual machine migration with adaptive, memory compression","Live migration of virtual machines has been a powerful tool to facilitate system maintenance, load balancing, fault tolerance, and power-saving, especially in clusters or data centers. Although pre-copy is a predominantly used approach in the state of the art, it is difficult to provide quick migration with low network overhead, due to a great amount of transferred data during migration, leading to large performance degradation of virtual machine services. This paper presents the design and implementation of a novel memory-compression-based VM migration approach (MECOM) that first uses memory compression to provide fast, stable virtual machine migration, while guaranteeing the virtual machine services to be slightly affected. Based on memory page characteristics, we design an adaptive zero-aware compression algorithm for balancing the performance and the cost of virtual machine migration. Pages are quickly compressed in batches on the source and exactly recovered on the target. Experiment demonstrates that compared with Xen, our system can significantly reduce 27.1% of downtime, 32% of total migration time and 68.8% of total transferred data on average.","Virtual machining,
Virtual manufacturing,
Degradation,
Voice mail,
Power system management,
Load management,
Fault tolerant systems,
Energy management,
Chromium,
Computers"
A Markov Clustering Topic Model for mining behaviour in video,"This paper addresses the problem of fully automated mining of public space video data. A novel Markov Clustering Topic Model (MCTM) is introduced which builds on existing Dynamic Bayesian Network models (e.g. HMMs) and Bayesian topic models (e.g. Latent Dirichlet Allocation), and overcomes their drawbacks on accuracy, robustness and computational efficiency. Specifically, our model profiles complex dynamic scenes by robustly clustering visual events into activities and these activities into global behaviours, and correlates behaviours over time. A collapsed Gibbs sampler is derived for offline learning with unlabeled training data, and significantly, a new approximation to online Bayesian inference is formulated to enable dynamic scene understanding and behaviour mining in new video data online in real-time. The strength of this model is demonstrated by unsupervised learning of dynamic scene models, mining behaviours and detecting salient events in three complex and crowded public scenes.","Layout,
Hidden Markov models,
Bayesian methods,
Robustness,
Event detection,
Humans,
Traffic control,
Data engineering,
Computer science,
Computational efficiency"
Hierarchical spatio-temporal context modeling for action recognition,"The problem of recognizing actions in realistic videos is challenging yet absorbing owing to its great potentials in many practical applications. Most previous research is limited due to the use of simplified action databases under controlled environments or focus on excessively localized features without sufficiently encapsulating the spatio-temporal context. In this paper, we propose to model the spatio-temporal context information in a hierarchical way, where three levels of context are exploited in ascending order of abstraction: 1) point-level context (SIFT average descriptor), 2) intra-trajectory context (trajectory transition descriptor), and 3) inter-trajectory context (trajectory proximity descriptor). To obtain efficient and compact representations for the latter two levels, we encode the spatiotemporal context information into the transition matrix of a Markov process, and then extract its stationary distribution as the final context descriptor. Building on the multichannel nonlinear SVMs, we validate this proposed hierarchical framework on the realistic action (HOHA) and event (LSCOM) recognition databases, and achieve 27% and 66% relative performance improvements over the state-of-the-art results, respectively. We further propose to employ the Multiple Kernel Learning (MKL) technique to prune the kernels towards speedup in algorithm evaluation.",
CHIPIC: An Efficient Code for Electromagnetic PIC Modeling and Simulation,"Computer-aided highly efficient electromagnetic particle-in-cell (PIC) (CHIPIC) code, developed at the University of Electronic Science and Technology of China, is a fully computer-aided code which combines modeling, calculations, and analysis with an integrated environment. CHIPIC is specifically designed for efficient modeling and simulation. CHIPIC includes a computer-aided design (CAD) system and a physical kernel. The CAD system is designed based on the common flowchart of PIC simulation to provide efficient modeling and analysis, and the physical kernel is developed to provide fast and accurate calculations. The physical kernel can run on a single processor or in parallel mode. When it runs in parallel mode, the message-pass interface and open specifications for multiprocessing are adopted. The validity of this electromagnetic PIC code is proved by simulating a magnetically insulated transmission-line-oscillator tube.","Electromagnetic modeling,
Design automation,
Kernel,
Computational modeling,
Insulation,
Analytical models,
Flowcharts,
Physics,
Electromagnetic fields,
Computer simulation"
Hardware Trojan: Threats and emerging solutions,"Malicious modification of hardware during design or fabrication has emerged as a major security concern. Such tampering (also referred to as Hardware Trojan) causes an integrated circuit (IC) to have altered functional behavior, potentially with disastrous consequences in safety-critical applications. Conventional design-time verification and post-manufacturing testing cannot be readily extended to detect hardware Trojans due to their stealthy nature, inordinately large number of possible instances and large variety in structure and operating mode. In this paper, we analyze the threat posed by hardware Trojans and the methods of deterring them. We present a Trojan taxonomy, models of Trojan operations and a review of the state-of-the-art Trojan prevention and detection techniques. Next, we discuss the major challenges associated with this security concern and future research needs to address them.","Hardware,
Security,
Integrated circuit modeling,
Circuit testing,
Fabrication,
Application specific integrated circuits,
Taxonomy,
Manufacturing,
Integrated circuit testing,
Electronic design automation and methodology"
Protovis: A Graphical Toolkit for Visualization,"Despite myriad tools for visualizing data, there remains a gap between the notational efficiency of high-level visualization systems and the expressiveness and accessibility of low-level graphical systems. Powerful visualization systems may be inflexible or impose abstractions foreign to visual thinking, while graphical systems such as rendering APIs and vector-based drawing programs are tedious for complex work. We argue that an easy-to-use graphical system tailored for visualization is needed. In response, we contribute Protovis, an extensible toolkit for constructing visualizations by composing simple graphical primitives. In Protovis, designers specify visualizations as a hierarchy of marks with visual properties defined as functions of data. This representation achieves a level of expressiveness comparable to low-level graphics systems, while improving efficiency - the effort required to specify a visualization - and accessibility - the effort required to learn and modify the representation. We substantiate this claim through a diverse collection of examples and comparative analysis with popular visualization tools.","Data visualization,
Graphics,
User interfaces,
Rendering (computer graphics),
Software tools,
Costs,
Computer science,
Data processing,
Encoding,
Domain specific languages"
Unsupervised spoken keyword spotting via segmental DTW on Gaussian posteriorgrams,"In this paper, we present an unsupervised learning framework to address the problem of detecting spoken keywords. Without any transcription information, a Gaussian Mixture Model is trained to label speech frames with a Gaussian posteriorgram. Given one or more spoken examples of a keyword, we use segmental dynamic time warping to compare the Gaussian posteriorgrams between keyword samples and test utterances. The keyword detection result is then obtained by ranking the distortion scores of all the test utterances. We examine the TIMIT corpus as a development set to tune the parameters in our system, and the MIT Lecture corpus for more substantial evaluation. The results demonstrate the viability and effectiveness of our unsupervised learning framework on the keyword spotting task.",
Primary-prioritized Markov approach for dynamic spectrum allocation,"Dynamic spectrum access has become a promising approach to fully utilize the scarce spectrum resources. In a dynamically changing spectrum environment, it is very important to consider the statistics of different users' spectrum access so as to achieve more efficient spectrum allocation. In this paper, we propose a primary-prioritized Markov approach for dynamic spectrum access through modeling the interactions between the primary and the secondary users as continuous-time Markov chains (CTMC). Based on the CTMC models, to compensate the throughput degradation due to the interference among secondary users, we derive the optimal access probabilities for the secondary users, by which the spectrum access of the secondary users is optimally coordinated, and the spectrum dynamics are clearly captured. Therefore, a good tradeoff can be achieved between the spectrum efficiency and fairness. The simulation results show that the proposed primary-prioritized dynamic spectrum access approach under proportional fairness criterion achieves much higher throughput than the CSMA-based random access approaches and the approach achieving max-min fairness. Moreover, it provides fair spectrum sharing among secondary users with only small performance degradation compared to the approach maximizing the overall average throughput.","FCC,
Throughput,
Degradation,
Interference,
Pricing,
Resource management,
Statistics,
Radio network,
Radio spectrum management"
Distributed maximum a posteriori estimation for multi-robot cooperative localization,"This paper presents a distributed Maximum A Posteriori (MAP) estimator for multi-robot Cooperative Localization (CL). As opposed to centralized MAP-based CL, the proposed algorithm reduces the memory and processing requirements by distributing data and computations amongst the robots. Specifically, a distributed data-allocation scheme is presented that enables robots to simultaneously process and update their local data. Additionally, a distributed Conjugate Gradient algorithm is employed that reduces the cost of computing the MAP estimates, while utilizing all available resources in the team and increasing robustness to single-point failures. Finally, a computationally efficient distributed marginalization of past robot poses is introduced for limiting the size of the optimization problem. The communication and computational complexity of the proposed algorithm is described in detail, while extensive simulation studies are presented for validating the performance of the distributed MAP estimator and comparing its accuracy to that of existing approaches.",
Barrier Coverage of Line-Based Deployed Wireless Sensor Networks,"Barrier coverage of wireless sensor networks has been studied intensively in recent years under the assumption that sensors are deployed uniformly at random in a large area (Poisson point process model). However, when sensors are deployed along a line (e.g., sensors are dropped from an aircraft along a given path), they would be distributed along the line with random offsets due to wind and other environmental factors. It is important to study the barrier coverage of such line- based deployment strategy as it represents a more realistic sensor placement model than the Poisson point process model. This paper presents the first set of results in this direction. In particular, we establish a tight lower-bound for the existence of barrier coverage under line-based deployments. Our results show that the barrier coverage of the line-based deployments significantly outperforms that of the Poisson model when the random offsets are relatively small compared to the sensor's sensing range. We then study sensor deployments along multiple lines and show how barrier coverage is affected by the distance between adjacent lines and the random offsets of sensors. These results demonstrate that sensor deployment strategies have direct impact on the barrier coverage of wireless sensor networks. Different deployment strategies may result in significantly different barrier coverage. Therefore, in the planning and deployment of wireless sensor networks, the coverage goal and possible sensor deployment strategies must be carefully and jointly considered. The results obtained in this paper will provide important guidelines to the deployment and performance of wireless sensor networks for barrier coverage.","Wireless sensor networks,
Aircraft,
Sensor phenomena and characterization,
Communications Society,
Environmental factors,
Strategic planning,
Guidelines,
Mechanical sensors,
Computer science,
USA Councils"
Real-Time Near-Duplicate Elimination for Web Video Search With Content and Context,"With the exponential growth of social media, there exist huge numbers of near-duplicate web videos, ranging from simple formatting to complex mixture of different editing effects. In addition to the abundant video content, the social Web provides rich sets of context information associated with web videos, such as thumbnail image, time duration and so on. At the same time, the popularity of Web 2.0 demands for timely response to user queries. To balance the speed and accuracy aspects, in this paper, we combine the contextual information from time duration, number of views, and thumbnail images with the content analysis derived from color and local points to achieve real-time near-duplicate elimination. The results of 24 popular queries retrieved from YouTube show that the proposed approach integrating content and context can reach real-time novelty re-ranking of web videos with extremely high efficiency, where the majority of duplicates can be rapidly detected and removed from the top rankings. The speedup of the proposed approach can reach 164 times faster than the effective hierarchical method proposed in , with just a slight loss of performance.",
Design and evaluation of a hierarchical on-chip interconnect for next-generation CMPs,"Performance and power consumption of an on-chip interconnect that forms the backbone of Chip Multiprocessors (CMPs), are directly influenced by the underlying network topology. Both these parameters can also be optimized by application induced communication locality since applications mapped on a large CMP system will benefit from clustered communication, where data is placed in cache banks closer to the cores accessing it. Thus, in this paper, we design a hierarchical network topology that takes advantage of such communication locality. The two-tier hierarchical topology consists of local networks that are connected via a global network. The local network is a simple, high-bandwidth, low-power shared bus fabric, and the global network is a low-radix mesh. The key insight that enables the hybrid topology is that most communication in CMP applications can be limited to the local network, and thus, using a fast, low-power bus to handle local communication will improve both packet latency and power-efficiency. The proposed hierarchical topology provides up to 63% reduction in energy-delay-product over mesh, 47% over flattened butterfly, and 33% with respect to concentrated mesh across network sizes with uniform and non-uniform synthetic traffic. For real parallel workloads, the hybrid topology provides up to 14% improvement in system performance (IPC) and in terms of energy-delay-product, improvements of 70%, 22%, 30% over the mesh, flattened butterfly, and concentrated mesh, respectively, for a 32-way CMP. Although the hybrid topology scales in a power- and bandwidth-efficient manner with network size, while keeping the average packet latency low in comparison to high radix topologies, it has lower throughput due to high concentration. To improve the throughput of the hybrid topology, we propose a novel router micro-architecture, called XShare, which exploits data value locality and bimodal traffic characteristics of CMP applications to transfer multiple small flits over a single channel. This helps in enhancing the network throughput by 35%, providing a latency reduction of 14% with synthetic traffic, and improving IPC on an average 4% with application workloads.","Network topology,
Delay,
Telecommunication traffic,
Throughput,
Power system interconnection,
Energy consumption,
Network-on-a-chip,
Spine,
Fabrics,
System performance"
Coregistered FDG PET/CT-Based Textural Characterization of Head and Neck Cancer for Radiation Treatment Planning,"Coregistered fluoro-deoxy-glucose (FDG) positron emission tomography/computed tomography (PET/CT) has shown potential to improve the accuracy of radiation targeting of head and neck cancer (HNC) when compared to the use of CT simulation alone. The objective of this study was to identify textural features useful in distinguishing tumor from normal tissue in head and neck via quantitative texture analysis of coregistered 18 F-FDG PET and CT images. Abnormal and typical normal tissues were manually segmented from PET/CT images of 20 patients with HNC and 20 patients with lung cancer. Texture features including some derived from spatial grey-level dependence matrices (SGLDM) and neighborhood gray-tone-difference matrices (NGTDM) were selected for characterization of these segmented regions of interest (ROIs). Both K nearest neighbors (KNNs) and decision tree (DT)-based KNN classifiers were employed to discriminate images of abnormal and normal tissues. The area under the curve (AZ) of receiver operating characteristics (ROC) was used to evaluate the discrimination performance of features in comparison to an expert observer. The leave-one-out and bootstrap techniques were used to validate the results. The AZ of DT-based KNN classifier was 0.95. Sensitivity and specificity for normal and abnormal tissue classification were 89% and 99%, respectively. In summary, NGTDM features such as PET coarseness, PET contrast, and CT coarseness extracted from FDG PET/CT images provided good discrimination performance. The clinical use of such features may lead to improvement in the accuracy of radiation targeting of HNC.",
High-Resolution Quantitative Imaging of Cornea Elasticity Using Supersonic Shear Imaging,"The noninvasive estimation of in vivo mechanical properties of cornea is envisioned to find several applications in ophthalmology. Such high-resolution measurements of local cornea stiffness could lead to a better anticipation and understanding of corneal pathologies such as Keratoconus. It could also provide a quantitative evaluation of corneal biomechanical response after corneal refractive surgeries and a tool for evaluating the efficacy of new cornea treatments such as cornea transplant using femtosecond laser or therapy based on Riboflavin/UltraViolet-A Corneal Cross Linking (UVA CXL). In the very important issue of glaucoma diagnosis and management, the fine tuning corneal elasticity measurement could also succeed to strongly correlate the applanation tonometry with the ""true"" intra-ocular pressure (IOP). This initial investigation evaluates the ability of ultrafast and high-resolution ultrasonic systems to provide a real-time and quantitative mapping of corneal viscoelasticity. Quantitative elasticity maps were acquired ex vivo on porcine cornea using the supersonic shear imaging (SSI) technique. A conventional 15 MHz linear probe was used to perform conventional ultrasonic imaging of the cornea. A dedicated ultrasonic sequence combines the generation of a remote palpation in the cornea and ultrafast (20 000 frames/s) ultrasonic imaging of the resulting corneal displacements that evolve into a shear wave propagation whose local speed was directly linked to local elasticity. A quantitative high-resolution map (150 mum resolution) of local corneal elasticity can be provided by this dedicated sequence of ultrasonic insonifications. Quantitative maps of corneal elasticity were obtained on ex vivo freshly enucleated porcine corneas. In the cornea, a quite homogenous stiffness map was found with a 190 kPa +/ - 32 kPa mean elasticity. The influence of photodynamic Riboflavin/UVA induced CXL was measured. A significant Young's modulus increase was obtained with a mean 890 kPa + / - 250 kPa posttreatment Young's modulus (460% increase), located in the anterior part of the cornea. Simulations based on 3-D time domain finite differences simulation were also performed and found to be in good agreement with ex vivo experiments. The SSI technique can perform real-time, noninvasive, high-resolution, and quantitative maps of the whole corneal elasticity. This technique could be real time and straightforward adapted for a very wide field of in vivo investigations.","High-resolution imaging,
Cornea,
Elasticity,
Ultrasonic imaging,
In vivo,
Laser tuning,
Ultrasonic variables measurement,
Mechanical factors,
Pathology,
Optical refraction"
MOPS: Providing Content-Based Service in Disruption-Tolerant Networks,"Content-based service, which dynamically routes and delivers events from sources to interested users, is extremely important to network services. However, existing content-based protocols for static networks will incur unaffordable maintenance costs if they are applied directly to the highly mobile environment that is featured in disruption-tolerant networks (DTNs). In this paper, we propose a unique publish/subscribe scheme that utilizes the long-term social network properties, which are observed in many DTNs, to facilitate content-based services in DTNs. We distributively construct communities based on the neighboring relationships from nodes' encounter histories. Brokers are deployed to bridge the communities, and they adopt a locally prioritized pub/sub scheme which combines the structural importance with subscription interests, to decide what events they should collect, store, and propagate. Different trade-offs for content-based service can be achieved by tuning the closeness threshold in community formation or by adjusting the broker-to-broker communication scheme. Extensive real-trace and synthetic-trace driven simulation results are presented to support the effectiveness of our scheme.",
Real-time correlative scan matching,"Scan matching, the problem of registering two laser scans in order to determine the relative positions from which the scans were obtained, is one of the most heavily relied-upon tools for mobile robots. Current algorithms, in a trade-off for computational performance, employ heuristics in order to quickly compute an answer. Of course, these heuristics are imperfect: existing methods can produce poor results, particularly when the prior is weak. The computational power available to modern robots warrants a re-examination of these quality vs. complexity trade-offs. In this paper, we advocate a probabilistically-motivated scan-matching algorithm that produces higher quality and more robust results at the cost of additional computation time. We describe several novel implementations of this approach that achieve real-time performance on modern hardware, including a multi-resolution approach for conventional CPUs, and a parallel approach for graphics processing units (GPUs). We also provide an empirical evaluation of our methods and several contemporary methods, illustrating the benefits of our approach. The robustness of the methods make them especially useful for global loop-closing.","Robustness,
Laser radar,
Iterative closest point algorithm,
Mobile robots,
Cost function,
Graphics,
Navigation,
Wheels,
Motion estimation,
Central Processing Unit"
Distributed resource allocation schemes,"In this article, we discuss distributed resource allocation schemes in which each transmitter determines its allocation autonomously, based on the exchange of interference prices. These schemes have been primarily motivated by the common model for spectrum sharing in which a user or service provider may transmit in a designated band provided that they abide by certain rules (e.g., a standard such as 802.11). An attractive property of these schemes is that they are scalable, i.e., the information exchange and overhead can be adapted according to the size of the network.",
An experimental investigation into the influence of user state and environment on fading characteristics in wireless body area networks at 2.45 GHz,"Using seven strategically placed, time-synchronized body worn receivers covering the head, upper front and back torso, and the limbs, we have investigated the effect of user state: stationary or mobile and local environment: anechoic chamber, open office area and hallway upon first and second order statistics for on-body fading channels. Three candidate models were considered: Nakagami, Rice and lognormal. Using maximum likelihood estimation and the Akaike information criterion it was established that the Nakagami-m distribution best described small-scale fading for the majority of on-body channels over all the measurement scenarios. When the user was stationary, Nakagami-m parameters were found to be much greater than 1, irrespective of local surroundings. For mobile channels, Nakagami-m parameters significantly decreased, with channels in the open office area and hallway experiencing the worst fading conditions.","Fading,
Body sensor networks,
Humans,
Wireless sensor networks,
Anechoic chambers,
Radio propagation,
Biomedical monitoring,
Communication standards,
Wireless communication,
Cotton"
System-level cost analysis and design exploration for three-dimensional integrated circuits (3D ICs),"Three-dimensional integrated circuit (3D IC) is emerging as an attractive option for overcoming the barriers in interconnect scaling. The majority of the existing 3D IC research is focused on how to take advantage of the performance, power, smaller form-factor, and heterogeneous integration benefits that offered by 3D integration. However, all such advantages ultimately have to translate into cost savings when a design strategy has to be decided: Is 3D integration a cost effective technology for a particular IC design? Consequently, system-level cost analysis at the early design stage is imperative to help the decision making on whether 3D integration should be adopted. In this paper, we study the design estimation method for 3D ICs at the early design stage, and propose a cost analysis model to study the cost implication for 3D ICs, and address the following cost-related problems related to 3D IC design: (1) Do all the benefits of 3D IC design come with a much higher cost? (2) How can 3D integration be achieved in a cost-effective way? (3) Are there any design options to compensate the extra 3D bonding cost? A cost-driven 3D IC design flow is also proposed to guide the design space exploration for 3D ICs toward a cost-effective direction.","Costs,
Three-dimensional integrated circuits,
Integrated circuit interconnections,
Design methodology,
Bonding,
Decision making,
Through-silicon vias,
CMOS technology,
Computer science,
Design engineering"
Three-Dimensional Microwave Breast Imaging: Dispersive Dielectric Properties Estimation Using Patient-Specific Basis Functions,"Breast imaging via microwave tomography involves estimating the distribution of dielectric properties within the patient's breast on a discrete mesh. The number of unknowns in the discrete mesh can be very large for 3-D imaging, and this results in computational challenges. We propose a new approach where the discrete mesh is replaced with a relatively small number of smooth basis functions. The dimension of the tomography problem is reduced by estimating the coefficients of the basis functions instead of the dielectric properties at each element in the discrete mesh. The basis functions are constructed using knowledge of the location of the breast surface. The number of functions used in the basis can be varied to balance resolution and computational complexity. The reduced dimension of the inverse problem enables application of a computationally efficient, multiple-frequency inverse scattering algorithm in 3-D. The efficacy of the proposed approach is verified using two 3-D anatomically realistic numerical breast phantoms. It is shown for the case of single-frequency microwave tomography that the imaging accuracy is comparable to that obtained when the original discrete mesh is used, despite the reduction of the dimension of the inverse problem. Results are also shown for a multiple-frequency algorithm where it is computationally challenging to use the original discrete mesh.","Microwave imaging,
Dispersion,
Dielectrics,
Tomography,
Inverse problems,
Breast cancer,
Finite difference methods,
Time domain analysis,
Patient monitoring,
Microwave antenna arrays"
Factorizing Scene Albedo and Depth from a Single Foggy Image,"Atmospheric conditions induced by suspended particles, such as fog and haze, severely degrade image quality. Restoring the true scene colors (clear day image) from a single image of a weather-degraded scene remains a challenging task due to the inherent ambiguity between scene albedo and depth. In this paper, we introduce a novel probabilistic method that fully leverages natural statistics of both the albedo and depth of the scene to resolve this ambiguity. Our key idea is to model the image with a factorial Markov random field in which the. scene albedo and depth are. two statistically independent latent layers. We. show that we may exploit natural image and depth statistics as priors on these hidden layers and factorize a single foggy image via a canonical Expectation Maximization algorithm with alternating minimization. Experimental results show that the proposed method achieves more accurate restoration compared to state-of-the-art methods that focus on only recovering scene albedo or depth individually.",Layout
Experiences in Hardware Trojan design and implementation,"We report our experiences in designing and implementing several hardware Trojans within the framework of the Embedded System Challenge competition that was held as part of the Cyber Security Awareness Week (CSAW) at the Polytechnic Institute of New York University in October 2008. Due to the globalization of the Integrated Circuit (IC) manufacturing industry, hardware Trojans constitute an increasingly probable threat to both commercial and military applications. With traditional testing methods falling short in the quest of finding hardware Trojans, several specialized detection methods have surfaced. To facilitate research in this area, a better understanding of what Hardware Trojans would look like and what impact they would incur to an IC is required. To this end, we present eight distinct attack techniques employing Register Transfer Level (RTL) hardware Trojans to compromise the security of an Alpha encryption module implemented on a Digilent BASYS Spartan-3 FPGA board. Our work, which earned second place in the aforementioned competition, demonstrates that current RTL designs are, indeed, quite vulnerable to hardware Trojan attacks.","Circuit testing,
Cryptography,
Hardware design languages,
Foundries,
Embedded system,
Application specific integrated circuits,
Field programmable gate arrays,
Manufacturing,
Computer science,
Computer security"
Regularized Extreme Learning Machine,"Extreme Learning Machine proposed by Huang G-B has attracted many attentions for its extremely fast training speed and good generalization performance. But it still can be considered as empirical risk minimization theme and tends to generate over-fitting model. Additionally, since ELM doesn't considering heteroskedasticity in real applications, its performance will be affected seriously when outliers exist in the dataset. In order to address these drawbacks, we propose a novel algorithm called Regularized Extreme Learning Machine based on structural risk minimization principle and weighted least square. The generalization performance of the proposed algorithm was improved significantly in most cases without increasing training time.","Machine learning,
Risk management,
Least squares methods,
Neural networks,
Joining processes,
Computer science,
Mathematical model,
Multi-layer neural network,
Feedforward neural networks,
Neurons"
Scale invariance and noise in natural images,"Natural images are known to have scale invariant statistics. While some eariler studies have reported the kurtosis of marginal bandpass filter response distributions to be constant throughout scales, other studies have reported that the kurtosis values are lower for high frequency filters than for lower frequency ones. In this work we propose a resolution for this discrepancy and suggest that this change in kurtosis values is due to noise present in the image. We suggest that this effect is consistent with a clean, natural image corrupted by white noise. We propose a model for this effect, and use it to estimate noise standard deviation in corrupted natural images. In particular, our results suggest that classical benchmark images used in low-level vision are actually noisy and can be cleaned up. Our results on noise estimation on two sets of 50 and a 100 natural images are significantly better than the state-of-the-art.","Frequency,
Shape,
Statistical distributions,
Band pass filters,
Noise shaping,
Layout,
Gaussian distribution,
Computer science,
Image resolution,
White noise"
Vicis: A reliable network for unreliable silicon,"Process scaling has given designers billions of transistors to work with. As feature sizes near the atomic scale, extensive variation and wearout inevitably make margining uneconomical or impossible. The ElastIC project seeks to address this by creating a large-scale chip-multiprocessor that can self-diagnose, adapt, and heal. Creating large, flexible designs in this environment naturally lends itself to the repetitive nature of network-on-chip (NoC), but the loss of a single link or router will result in complete network failure. In this work we present Vicis, an ElastIC-style NoC that can tolerate the loss of many network components due to wearout induced hard faults. Vicis uses the inherent redundancy in the network and its routers in order to maintain correct operation while incurring a much lower area overhead than previously proposed N-modular redundancy (NMR) based solutions. Each router has a built-in-self-test (BIST) that diagnoses the locations of hard fault and runs a number of algorithms to best use ECC, port swapping, and a crossbar bypass bus to mitigate them. The routers work together to run distributed algorithms to solve network-wide problems as well, protecting the networking against critical failures in individual routers. In this work we show that with stuck-at fault rates as high as 1 in 2000 gates, Vicis will continue to operate with approximately half of its routers still functional and communicating.","Silicon,
Network-on-a-chip,
Redundancy,
System recovery,
Fault diagnosis,
Power system management,
Testing,
Telecommunication traffic,
Fault tolerance,
Electric breakdown"
Half-integrality based algorithms for cosegmentation of images,"We study the cosegmentation problem where the objective is to segment the same object (i.e., region) from a pair of images. The segmentation for each image can be cast using a partitioning/segmentation function with an additional constraint that seeks to make the histograms of the segmented regions (based on intensity and texture features) similar. Using Markov random field (MRF) energy terms for the simultaneous segmentation of the images together with histogram consistency requirements using the squared L2 (rather than L1) distance, after linearization and adjustments, yields an optimization model with some interesting combinatorial properties. We discuss these properties which are closely related to certain relaxation strategies recently introduced in computer vision. Finally, we show experimental results of the proposed approach.","Image segmentation,
Histograms,
Biomedical imaging,
Computer vision,
Pathology,
Mathematics,
Computer science,
Markov random fields,
Brain,
Videos"
"Breadcrumbs: Efficient, Best-Effort Content Location in Cache Networks","For several years, web caching has been used to meet the ever-increasing Web access loads. A fundamental capability of all such systems is that of inter-cache coordination, which can be divided into two main types: explicit and implicit coordination. While the former allows for greater control over resource allocation, the latter does not suffer from the additional communication overhead needed for coordination. In this paper, we consider a network in which each router has a local cache that caches files passing through it. By additionally storing minimal information regarding caching history, we develop a simple content caching, location, and routing systems that adopts an implicit, transparent, and best-effort approach towards caching. Though only best effort, the policy outperforms classic policies that allow explicit coordination between caches.","Routing,
Computer science,
Resource management,
Communications Society,
Communication system control,
History,
Costs,
Protocols,
Topology,
Content based retrieval"
"Data, Information, and Knowledge in Visualization","In visualization, we use the terms data, information and knowledge extensively, often in an interrelated context. In many cases, they indicate different levels of abstraction, understanding, or truthfulness. For example, ""visualization is concerned with exploring data and information,"" ""the primary objective in data visualization is to gain insight into an information space,"" and ""information visualization"" is for ""data mining and knowledge discovery."" In other cases, these three terms indicate data types, for instance, as adjectives in noun phrases, such as data visualization, information visualization, and knowledge visualization. These examples suggest that data, information, and knowledge could serve as both the input and output of a visualization process, raising questions about their exact role in visualization.","Data visualization,
Space technology,
Information management,
Taxonomy,
Humans,
Silver,
Computer science,
Data engineering,
Knowledge engineering,
Psychology"
Segmentation in Ultrasonic B-Mode Images of Healthy Carotid Arteries Using Mixtures of Nakagami Distributions and Stochastic Optimization,"The goal of this work is to perform a segmentation of the intimamedia thickness (IMT) of carotid arteries in view of computing various dynamical properties of that tissue, such as the elasticity distribution (elastogram). The echogenicity of a region of interest comprising the intima-media layers, the lumen, and the adventitia in an ultrasonic B-mode image is modeled by a mixture of three Nakagami distributions. In a first step, we compute the maximum a posteriori estimator of the proposed model, using the expectation maximization (EM) algorithm. We then compute the optimal segmentation based on the estimated distributions as well as a statistical prior for disease-free IMT using a variant of the exploration/selection (ES) algorithm. Convergence of the ES algorithm to the optimal solution is assured asymptotically and is independent of the initial solution. In particular, our method is well suited to a semi-automatic context that requires minimal manual initialization. Tests of the proposed method on 30 sequences of ultrasonic B-mode images of presumably disease-free control subjects are reported. They suggest that the semi-automatic segmentations obtained by the proposed method are within the variability of the manual segmentations of two experts.",
Noninvasive Carotid Strain Imaging Using Angular Compounding at Large Beam Steered Angles: Validation in Vessel Phantoms,"Stroke and myocardial infarction are initiated by rupturing vulnerable atherosclerotic plaques. With noninvasive ultrasound elastography, these plaques might be detected in carotid arteries. However, since the ultrasound beam is generally not aligned with the radial direction in which the artery pulsates, radial and circumferential strains need to be derived from axial and lateral data. Conventional techniques to perform this conversion have the disadvantage that lateral strain is required. Since the lateral strain has relatively poor accuracy, the quality of the radial and circumferential strains is reduced. In this study, the radial and circumferential strain estimates are improved by combining axial strain data acquired at multiple insonification angles. Adaptive techniques to correct for grating lobe interference and other artifacts that occur when performing beam steering at large angles are introduced. Acquisitions at multiple angles are performed with a beam steered linear array. For each beam steered angle, there are two spatially restricted regions of the circular vessel cross section where the axial strain is closely aligned with the radial strain and two spatially restricted regions (different from the radial strain regions) where the axial strain is closely aligned with the circumferential strain. These segments with high quality strain estimates are compounded to form radial or circumferential strain images. Compound radial and circumferential strain images were constructed for a homogeneous vessel phantom with a concentric lumen subjected to different intraluminal pressures. Comparison of the elastographic signal-to-noise ratio (SNRe) and contrast-to-noise ratio ( CNRe) revealed that compounding increases the image quality considerably compared to images from 0deg information only. SNRe and CNRe increase up to 2.7 and 6.6 dB, respectively. The highest image quality was achieved by projecting axial data, completed with a small segment determined by either principal component analysis or by application of a rotation matrix.","Capacitive sensors,
Imaging phantoms,
Ultrasonic imaging,
Image segmentation,
Image quality,
Myocardium,
Carotid arteries,
Gratings,
Interference,
Beam steering"
Beyond pairwise energies: Efficient optimization for higher-order MRFs,"In this paper, we introduce a higher-order MRF optimization framework. On the one hand, it is very general; we thus use it to derive a generic optimizer that can be applied to almost any higher-order MRF and that provably optimizes a dual relaxation related to the input MRF problem. On the other hand, it is also extremely flexible and thus can be easily adapted to yield far more powerful algorithms when dealing with subclasses of high-order MRFs. We thus introduce a new powerful class of high-order potentials, which are shown to offer enough expressive power and to be useful for many vision tasks. To address them, we derive, based on the same framework, a novel and extremely efficient message-passing algorithm, which goes beyond the aforementioned generic optimizer and is able to deliver almost optimal solutions of very high quality. Experimental results on vision problems demonstrate the extreme effectiveness of our approach. For instance, we show that in some cases we are even able to compute the global optimum for NP-hard higher-order MRFs in a very efficient manner.","Computer vision,
Inference algorithms,
Computer science,
Computational efficiency,
Master-slave,
Graphical models,
Optimization methods"
An Adaptive Mean-Shift Framework for MRI Brain Segmentation,"An automated scheme for magnetic resonance imaging (MRI) brain segmentation is proposed. An adaptive mean-shift methodology is utilized in order to classify brain voxels into one of three main tissue types: gray matter, white matter, and cerebro-spinal fluid. The MRI image space is represented by a high-dimensional feature space that includes multimodal intensity features as well as spatial features. An adaptive mean-shift algorithm clusters the joint spatial-intensity feature space, thus extracting a representative set of high-density points within the feature space, otherwise known as modes. Tissue segmentation is obtained by a follow-up phase of intensity-based mode clustering into the three tissue categories. By its nonparametric nature, adaptive mean-shift can deal successfully with nonconvex clusters and produce convergence modes that are better candidates for intensity based classification than the initial voxels. The proposed method is validated on 3-D single and multimodal datasets, for both simulated and real MRI data. It is shown to perform well in comparison to other state-of-the-art methods without the use of a preregistered statistical brain atlas.","Magnetic resonance imaging,
Image segmentation,
Prototypes,
Alzheimer's disease,
Humans,
Biomedical image processing,
Clustering algorithms,
Feature extraction,
Convergence,
Brain modeling"
Stateful implication logic with memristors,"In this paper computation with memristors is studied in terms of how many memristors are needed to perform a given logic operation. It has been shown that memristors are naturally suited for performing implication logic (combination of implication and false operation) instead of Boolean logic. Also, it should be noted that a memristor can be used as both a logic gate and a latch (stateful logic). Being functionally complete, implication logic can be used to compute any Boolean function. However, by performing implication logic with stateful devices, storage of intermediate results requires additional memristors to keep data yet to be used from being written over. This paper describes an effective way to compute any Boolean function with a small number of memristors. Also, the length of the corresponding computing sequence is considered.",
Artificial neural network-polar coordinated fuzzy controller based maximum power point tracking control under partially shaded conditions,"The one of main causes of reducing energy yield of photovoltaic systems is partially shaded conditions. Although the conventional maximum power point tracking (MPPT) control algorithms operate well under uniform insolation, they do not operate well in non-uniform insolation. The non-uniform conditions cause multiple local maximum power points on the power-voltage curve. The conventional MPPT methods cannot distinguish between the global and local peaks. Since the global maximum power point (MPP) may change within a large voltage window and also its position depends on shading patterns, it is very difficult to recognise the global operating point under partially shaded conditions. In this paper, a novel MPPT system is proposed for partially shaded PV array using artificial neural network (ANN) and fuzzy logic with polar information controller. The ANN with three layer feed-forward is trained once for several partially shaded conditions to determine the global MPP voltage. The fuzzy logic with polar information controller uses the global MPP voltage as a reference voltage to generate the required control signal for the power converter. Another objective of this study is to determine the estimated maximum power and energy generation of PV system through the same ANN structure. The effectiveness of the proposed method is demonstrated under the experimental real-time simulation technique based dSPACE real-time interface system for different interconnected PV arrays such as series-parallel, bridge link and total cross tied configurations.","power system control,
feedforward neural nets,
fuzzy control,
fuzzy logic,
photovoltaic power systems,
power engineering computing"
Automatic Temporal Segment Detection and Affect Recognition From Face and Body Display,"Psychologists have long explored mechanisms with which humans recognize other humans' affective states from modalities, such as voice and face display. This exploration has led to the identification of the main mechanisms, including the important role played in the recognition process by the modalities' dynamics. Constrained by the human physiology, the temporal evolution of a modality appears to be well approximated by a sequence of temporal segments called onset, apex, and offset. Stemming from these findings, computer scientists, over the past 15 years, have proposed various methodologies to automate the recognition process. We note, however, two main limitations to date. The first is that much of the past research has focused on affect recognition from single modalities. The second is that even the few multimodal systems have not paid sufficient attention to the modalities' dynamics: The automatic determination of their temporal segments, their synchronization to the purpose of modality fusion, and their role in affect recognition are yet to be adequately explored. To address this issue, this paper focuses on affective face and body display, proposes a method to automatically detect their temporal segments or phases, explores whether the detection of the temporal phases can effectively support recognition of affective states, and recognizes affective states based on phase synchronization/alignment. The experimental results obtained show the following: 1) affective face and body displays are simultaneous but not strictly synchronous; 2) explicit detection of the temporal phases can improve the accuracy of affect recognition; 3) recognition from fused face and body modalities performs better than that from the face or the body modality alone; and 4) synchronized feature-level fusion achieves better performance than decision-level fusion.","Face detection,
Face recognition,
Displays,
Humans,
Phase detection,
Emotion recognition,
Speech recognition,
Automatic speech recognition,
Physiology,
Mood"
The BoneXpert Method for Automated Determination of Skeletal Maturity,"Bone age rating is associated with a considerable variability from the human interpretation, and this is the motivation for presenting a new method for automated determination of bone age (skeletal maturity). The method, called BoneXpert, reconstructs, from radiographs of the hand, the borders of 15 bones automatically and then computes ldquointrinsicrdquo bone ages for each of 13 bones (radius, ulna, and 11 short bones). Finally, it transforms the intrinsic bone ages into Greulich Pyle (GP) or Tanner Whitehouse (TW) bone age. The bone reconstruction method automatically rejects images with abnormal bone morphology or very poor image quality. From the methodological point of view, BoneXpert contains the following innovations: 1) a generative model (active appearance model) for the bone reconstruction; 2) the prediction of bone age from shape, intensity, and texture scores derived from principal component analysis; 3) the consensus bone age concept that defines bone age of each bone as the best estimate of the bone age of the other bones in the hand; 4) a common bone age model for males and females; and 5) the unified modelling of TW and GP bone age. BoneXpert is developed on 1559 images. It is validated on the Greulich Pyle atlas in the age range 2-17 years yielding an SD of 0.42 years [0.37; 0.47] 95% conf, and on 84 clinical TW-rated images yielding an SD of 0.80 years [0.68; 0.93] 95% conf. The precision of the GP bone age determination (its ability to yield the same result on a repeated radiograph) is inferred under suitable assumptions from six longitudinal series of radiographs. The result is an SD on a single determination of 0.17 years [0.13; 0.21] 95% conf.","Bones,
Radiography,
Predictive models,
Image reconstruction,
Humans,
Reconstruction algorithms,
Morphology,
Image quality,
Technological innovation,
Active shape model"
A scalable collusion-resistant multi-winner cognitive spectrum auction game,"Dynamic spectrum access (DSA), enabled by cognitive radio technologies, has become a promising approach to improve efficiency in spectrum utilization, and the spectrum auction is one important DSA approach, in which secondary users lease some unused bands from primary users. However, spectrum auctions are different from existing auctions studied by economists, because spectrum resources are interference-limited rather than quantity-limited, and it is possible to award one band to multiple secondary users with negligible mutual interference. To accommodate this special feature in wireless communications, in this paper, we present a novel multi-winner spectrum auction game not existing in auction literature. As secondary users may be selfish in nature and tend to be dishonest in pursuit of higher profits, we develop effective mechanisms to suppress their dishonest/collusive behaviors when secondary users distort their valuations about spectrum resources and interference relationships. Moreover, in order to make the proposed game scalable when the size of problem grows, the semi-definite programming (SDP) relaxation is applied to reduce the complexity significantly. Finally, simulation results are presented to evaluate the proposed auction mechanisms, and demonstrate the complexity reduction as well.","Cognitive radio,
FCC,
Wireless communication,
Cost accounting,
Interference suppression,
Communications Society,
Laboratories,
Object detection,
Data communication,
Mobile ad hoc networks"
A multi-channel token ring protocol for QoS provisioning in inter-vehicle communications,"This paper proposes a multi-channel token ring media access control (MAC) protocol (MCTRP) for inter-vehicle communications (IVC). Through adaptive ring coordination and channel scheduling, vehicles are autonomously organized into multiple rings operating on different service channels. Based on the multi-channel ring structure, emergency messages can be disseminated with a low delay. With the token based data exchange protocol, the network throughput is further improved for non-safety multimedia applications. An analytical model is developed to evaluate the performance of MCTRP in terms of the average full ring delay, emergency message delay, and ring throughput. Extensive simulations with ns-2 are conducted to validate the analytical model and demonstrate the efficiency and effectiveness of the proposed MCTRP.",
Control and protection of power electronics interfaced distributed generation systems in a customer-driven microgrid,"This paper discusses control and protection of power electronics interfaced distributed generation (DG) systems in a customer-driven microgrid (CDM). Particularly, the following topics will be addressed: microgrid system configurations and features, DG interfacing converter topologies and control, power flow control in grid-connected operation, islanding detection, autonomous islanding operation with load shedding and load demand sharing among DG units, and system/DG protection. Most of the above mentioned control and protection issues should be embedded into the DG interfacing converter control scheme. Some case study results are also shown in this paper to further illustrate the above mentioned issues.","Control systems,
Power system protection,
Power electronics,
Distributed control,
Power system reliability,
Cogeneration,
Communication system control,
Power generation,
Voltage control,
Power system management"
Robust visual tracking using &#x2113;1 minimization,"In this paper we propose a robust visual tracking method by casting tracking as a sparse approximation problem in a particle filter framework. In this framework, occlusion, corruption and other challenging issues are addressed seamlessly through a set of trivial templates. Specifically, to find the tracking target at a new frame, each target candidate is sparsely represented in the space spanned by target templates and trivial templates. The sparsity is achieved by solving an ℓ1-regularized least squares problem. Then the candidate with the smallest projection error is taken as the tracking target. After that, tracking is continued using a Bayesian state inference framework in which a particle filter is used for propagating sample distributions over time. Two additional components further improve the robustness of our approach: 1) the nonnegativity constraints that help filter out clutter that is similar to tracked targets in reversed intensity patterns, and 2) a dynamic template update scheme that keeps track of the most representative templates throughout the tracking procedure. We test the proposed approach on five challenging sequences involving heavy occlusions, drastic illumination changes, and large pose variations. The proposed approach shows excellent performance in comparison with three previously proposed trackers.",
Rethinking Enterprise Network Control,"This paper presents Ethane, a new network architecture for the enterprise. Ethane allows managers to define a single network-wide fine-grain policy and then enforces it directly. Ethane couples extremely simple flow-based Ethernet switches with a centralized controller that manages the admittance and routing of flows. While radical, this design is backwards-compatible with existing hosts and switches. We have implemented Ethane in both hardware and software, supporting both wired and wireless hosts. We also show that it is compatible with existing high-fanout switches by porting it to popular commodity switching chipsets. We have deployed and managed two operational Ethane networks, one in the Stanford University Computer Science Department supporting over 300 hosts, and another within a small business of 30 hosts. Our deployment experiences have significantly affected Ethane's design.","Switches,
Routing,
Computer network management,
Middleboxes,
Ethernet networks,
Centralized control,
Admittance,
Hardware,
Computer science,
Application software"
"Joint learning of visual attributes, object classes and visual saliency","We present a method to learn visual attributes (eg.“red”, “metal”, “spotted”) and object classes (eg. “car”, “dress”, “umbrella”) together. We assume images are labeled with category, but not location, of an instance. We estimate models with an iterative procedure: the current model is used to produce a saliency score, which, together with a homogeneity cue, identifies likely locations for the object (resp. attribute); then those locations are used to produce better models with multiple instance learning. Crucially, the object and attribute models must agree on the potential locations of an object. This means that the more accurate of the two models can guide the improvement of the less accurate model. Our method is evaluated on two data sets of images of real scenes, one in which the attribute is color and the other in which it is material. We show that our joint learning produces improved detectors. We demonstrate generalization by detecting attribute-object pairs which do not appear in our training data. The iteration gives significant improvement in performance.","Detectors,
Object detection,
Training data,
Computer science,
Layout,
Learning systems,
Computer vision"
Reproducible Research in Computational Harmonic Analysis,"Scientific computation is emerging as absolutely central to the scientific method. Unfortunately, it's error-prone and currently immature-traditional scientific publication is incapable of finding and rooting out errors in scientific computation-which must be recognized as a crisis. An important recent development and a necessary response to the crisis is reproducible computational research in which researchers publish the article along with the full computational environment that produces the results. In this article, the authors review their approach and how it has evolved over time, discussing the arguments for and against working reproducibly.","Harmonic analysis,
Computational modeling,
Computer errors,
Large-scale systems,
Logic testing,
Data mining,
Laboratories,
Image recognition,
Information management,
Scientific computing"
Error-Correcting Codes in Projective Spaces Via Rank-Metric Codes and Ferrers Diagrams,"Coding in the projective space has received recently a lot of attention due to its application in network coding. Reduced row echelon form of the linear subspaces and Ferrers diagram can play a key role for solving coding problems in the projective space. In this paper, we propose a method to design error-correcting codes in the projective space. We use a multilevel approach to design our codes. First, we select a constant-weight code. Each codeword defines a skeleton of a basis for a subspace in reduced row echelon form. This skeleton contains a Ferrers diagram on which we design a rank-metric code. Each such rank-metric code is lifted to a constant-dimension code. The union of these codes is our final constant-dimension code. In particular, the codes constructed recently by Koetter and Kschischang are a subset of our codes. The rank-metric codes used for this construction form a new class of rank-metric codes. We present a decoding algorithm to the constructed codes in the projective space. The efficiency of the decoding depends on the efficiency of the decoding for the constant-weight codes and the rank-metric codes. Finally, we use puncturing on our final constant-dimension codes to obtain large codes in the projective space which are not constant-dimension.","Error correction codes,
Space technology,
Network coding,
Decoding,
Extraterrestrial measurements,
Skeleton,
Design methodology,
Galois fields,
Computer science"
Nonparametric scene parsing: Label transfer via dense scene alignment,"In this paper we propose a novel nonparametric approach for object recognition and scene parsing using dense scene alignment. Given an input image, we retrieve its best matches from a large database with annotated images using our modified, coarse-to-fine SIFT flow algorithm that aligns the structures within two images. Based on the dense scene correspondence obtained from the SIFT flow, our system warps the existing annotations, and integrates multiple cues in a Markov random field framework to segment and recognize the query image. Promising experimental results have been achieved by our nonparametric scene parsing system on a challenging database. Compared to existing object recognition approaches that require training for each object category, our system is easy to implement, has few parameters, and embeds contextual information naturally in the retrieval/alignment procedure.","Layout,
Image databases,
Object recognition,
Information retrieval,
Image retrieval,
Image segmentation,
Image recognition,
Computer vision,
Computer science,
Artificial intelligence"
DiffQ: Practical Differential Backlog Congestion Control for Wireless Networks,"Congestion control in wireless multi-hop networks is challenging and complicated because of two reasons. First, interference is ubiquitous and causes loss in the shared medium. Second, wireless multihop networks are characterized by the use of diverse and dynamically changing routing paths. Traditional end point based congestion control protocols are ineffective in such a setting resulting in unfairness and starvation. This paper adapts the optimal theoretical work of Tassiulas and Ephremedes on cross-layer optimization of wireless networks involving congestion control, routing and scheduling, for practical solutions to congestion control in multi-hop wireless networks. This work is the first that implements in real off-shelf radios, a differential backlog based MAC scheduling and router-assisted backpressure congestion control for multi-hop wireless networks. Our adaptation, called DiffQ, is implemented between transport and IP and supports legacy TCP and UDP applications. In a network of 46 IEEE 802.11 wireless nodes, we demonstrate that DiffQ far outperforms many previously proposed ""practical"" solutions for congestion control.","Wireless networks,
Spread spectrum communication,
Routing,
Interference,
Communication system control,
Optimal control,
TCPIP,
Delay estimation,
Communications Society,
Computer science"
Saliency detection for content-aware image resizing,"Content aware image re-targeting methods aim to arbitrarily change image aspect ratios while preserving visually prominent features. To determine visual importance of pixels, existing re-targeting schemes mostly rely on grayscale intensity gradient maps. These maps show higher energy only at edges of objects, are sensitive to noise, and may result in deforming salient objects. In this paper, we present a computationally efficient, noise robust re-targeting scheme based on seam carving by using saliency maps that assign higher importance to visually prominent whole regions (and not just edges). This is achieved by computing global saliency of pixels using intensity as well as color features. Our saliency maps easily avoid artifacts that conventional seam carving generates and are more robust in the presence of noise. Also, unlike gradient maps, which may have to be recomputed several times during a seam carving based re-targeting operation, our saliency maps are computed only once independent of the number of seams added or removed.",
On the energy efficiency of graphics processing units for scientific computing,"The graphics processing unit (GPU) has emerged as a computational accelerator that dramatically reduces the time to discovery in high-end computing (HEC). However, while today's state-of-the-art GPU can easily reduce the execution time of a parallel code by many orders of magnitude, it arguably comes at the expense of significant power and energy consumption. For example, the NVIDIA GTX 280 video card is rated at 236 watts, which is as much as the rest of a compute node, thus requiring a 500-W power supply. As a consequence, the GPU has been viewed as a “non-green” computing solution. This paper seeks to characterize, and perhaps debunk, the notion of a “power-hungry GPU” via an empirical study of the performance, power, and energy characteristics of GPUs for scientific computing. Specifically, we take an important biological code that runs in a traditional CPU environment and transform and map it to a hybrid CPU+GPU environment. The end result is that our hybrid CPU+GPU environment, hereafter referred to simply as GPU environment, delivers an energy-delay product that is multiple orders of magnitude better than a traditional CPU environment, whether unicore or multicore.","Energy efficiency,
Scientific computing,
Energy consumption,
Computer graphics,
Acceleration,
Biology computing,
Biological information theory,
Central Processing Unit,
Electrostatics,
Space exploration"
Robust Speaker-Adaptive HMM-Based Text-to-Speech Synthesis,"This paper describes a speaker-adaptive HMM-based speech synthesis system. The new system, called ldquoHTS-2007,rdquo employs speaker adaptation (CSMAPLR+MAP), feature-space adaptive training, mixed-gender modeling, and full-covariance modeling using CSMAPLR transforms, in addition to several other techniques that have proved effective in our previous systems. Subjective evaluation results show that the new system generates significantly better quality synthetic speech than speaker-dependent approaches with realistic amounts of speech data, and that it bears comparison with speaker-dependent approaches even when large amounts of speech data are available. In addition, a comparison study with several speech synthesis techniques shows the new system is very robust: It is able to build voices from less-than-ideal speech data and synthesize good-quality speech even for out-of-domain sentences.","Robustness,
Speech synthesis,
Hidden Markov models,
Speech analysis,
Nose,
High temperature superconductors,
Councils,
Continuous-stirred tank reactor,
Computer science,
Information science"
Repeated open spectrum sharing game with cheat-proof strategies,"Dynamic spectrum access has become a promising approach to improve spectrum efficiency by adaptively coordinating different users' access according to spectrum dynamics. However, users who are competing with each other for spectrum may have no incentive to cooperate, and they may even exchange false private information about their channel conditions in order to get more access to the spectrum. In this paper, we propose a repeated spectrum sharing game with cheat-proof strategies. By using the punishment-based repeated game, users get the incentive to share the spectrum in a cooperative way; and through mechanism-design-based and statistics-based approaches, user honesty is further enforced. Specific cooperation rules have been proposed based on the maximum total throughput and proportional fairness criteria. Simulation results show that the proposed scheme can greatly improve the spectrum efficiency by alleviating mutual interference.","FCC,
Interference,
Access protocols,
Game theory,
Throughput,
US Government,
Cognitive radio,
Wireless LAN,
Bluetooth,
Frequency"
Comparison of Several Cloud Computing Platforms,"Cloud computing is the development of parallel computing, distributed computing and grid computing. It has been one of the most hot research topics. Now many corporations have involved in the cloud computing related techniques and many cloud computing platforms have been put forward. This is a favorable situation to study and application of cloud computing related techniques. Though interesting, there are also some problems for so many flatforms. For to a novice or user with little knowledge about cloud computing, it is still very hard to make a reasonable choice. What differences are there for different cloud computing platforms and what characteristics and advantages each has? To answer these problems, the characteristics, architectures and applications of several popular cloud computing platforms are analyzed and discussed in detail. From the comparison of these platforms, users can better understand the different cloud platforms and more reasonablely choose what they want.","Cloud computing,
Distributed computing,
Grid computing,
Concurrent computing,
Computer science,
High performance computing,
Parallel processing,
Platform virtualization,
Computer industry,
Network servers"
A survey on spectrum sensing techniques for cognitive radio,"Spectrum sensing is an important functionality of cognitive radio (CR). Accuracy and speed of estimation are the key indicators to select the appropriate spectrum sensing technique. Conventional spectrum estimation techniques which are based on short time Fourier transform (STFT) suffer from familiar problems such as low frequency resolution, high variance of estimated power spectrum and high side lobes/leakages. Methods such as multitaper spectrum estimation successfully alleviate these infarctions but exact a high price in terms of complexity. On these accounts, it appears that the filter bank spectrum estimation formulated by F. Boroujeny and wavelet based spectrum estimates are the most promising and pragmatic approaches for CR applications. This article surveys and appraises available literature on various spectrum sensing techniques and discusses spectrum sensing as a key element of CR system design.","Cognitive radio,
Frequency estimation,
Chromium,
Spectral analysis,
Filter bank,
Wireless sensor networks,
Radiofrequency identification,
Signal processing,
Autocorrelation,
Radar"
Express Cube Topologies for on-Chip Interconnects,"Driven by continuing scaling of Moore's law, chip multi-processors and systems-on-a-chip are expected to grow the core count from dozens today to hundreds in the near future. Scalability of on-chip interconnect topologies is critical to meeting these demands. In this work, we seek to develop a better understanding of how network topologies scale with regard to cost, performance, and energy considering the advantages and limitations afforded on a die. Our contributions are three-fold. First, we propose a new topology, called Multidrop Express Channels (MECS), that uses a one-to-many communication model enabling a high degree of connectivity in a bandwidth-efficient manner. In a 64-terminal network, MECS enjoys a 9% latency advantage over other topologies at low network loads, which extends to over 20% in a 256-terminal network. Second, we demonstrate that partitioning the available wires among multiple networks and channels enables new opportunities for trading-off performance, area, and energy-efficiency that depend on the partitioning scheme. Third, we introduce Generalized Express Cubes - a framework for expressing the space of on-chip interconnects - and demonstrate how existing and proposed topologies can be mapped to it.","Network topology,
Scalability,
Delay,
Moore's Law,
Network-on-a-chip,
Silicon,
System-on-a-chip,
Costs,
Wires,
Energy efficiency"
Sets of Frequency Hopping Sequences: Bounds and Optimal Constructions,"Frequency hopping spread spectrum and direct sequence spread spectrum are two main spread coding technologies in communication systems. Frequency hopping sequences are needed in frequency hopping code-division multiple-access (FH-CDMA) systems. In this paper, four algebraic and a combinatorial constructions of optimal sets of frequency hopping sequences with new parameters are presented, and a number of bounds on sets of frequency hopping sequences are described.","Frequency,
Spread spectrum communication,
Communication systems,
Information science,
Communications technology,
Multiaccess communication,
Councils,
Codes,
Computer science,
Materials science and technology"
Combined optical imaging and mammography of the healthy breast: Optical contrast derived from breast structure and compression,"In this paper, we report new progress in developing the instrument and software platform of a combined X-ray mammography/diffuse optical breast imaging system. Particularly, we focus on system validation using a series of balloon phantom experiments and the optical image analysis of 49 healthy patients. Using the finite-element method for forward modeling and a regularized Gauss-Newton method for parameter reconstruction, we recovered the inclusions inside the phantom and the hemoglobin images of the human breasts. An enhanced coupling coefficient estimation scheme was also incorporated to improve the accuracy and robustness of the reconstructions. The recovered average total hemoglobin concentration (HbT) and oxygen saturation (SO<sub>2</sub>) from 68 breast measurements are 16.2 mum and 71%, respectively, where the HbT presents a linear trend with breast density. The low HbT value compared to literature is likely due to the associated mammographic compression. From the spatially co-registered optical/X-ray images, we can identify the chest-wall muscle, fatty tissue, and fibroglandular regions with an average HbT of 20.1plusmn6.1 &nbsp;mum for fibroglandular tissue, 15.4plusmn5.0&nbsp;mum for adipose, and 22.2plusmn7.3&nbsp;mum for muscle tissue. The differences between fibroglandular tissue and the orresponding adipose tissue are significant. At the same time, we recognize that the optical images are influenced, to a certain extent, by mammographical compression. The optical images from a subset of patients show composite features from both tissue structure and pressure distribution. We present mechanical simulations which further confirm this hypothesis.","Breast,
Optical imaging,
Mammography,
Optical saturation,
Image coding,
X-ray imaging,
Imaging phantoms,
Image reconstruction,
Muscles,
Instruments"
Weather effects on hybrid FSO/RF communication link,"Free space optics (FSO) or optical wireless systems provide high data rate solution for bandwidth hungry communication applications. Carrier class availability is a necessity for wide scale acceptability which is extremely difficult to achieve in the case of optical wireless links. FSO links are highly weather-dependent and different weather effects reduce the link availability. Employing a hybrid network consisting of an FSO link and a back up link in the GHz frequency range renders high availability besides providing comparable data rates. In this paper effects of fog, rain and snow on FSO/GHz hybrid network are studied so that GHz frequencies with best complementary behaviour can be selected as a back up link. As a prime conclusion of the article, it is suggested that free space optical links can be supplemented with 40 GHz RF links to achieve near carrier class availability.","Radio frequency,
Availability,
Optical attenuators,
High speed optical techniques,
Biomedical optical imaging,
Back,
Rain,
Snow,
Video on demand,
Artificial satellites"
Joint cross-layer scheduling and spectrum sensing for OFDMA cognitive radio systems,"In most of the existing works on cognitive radio (CR) systems, the spectrum sensing and the cross-layer scheduling are designed separately. Specifically, the sensing module first determines whether or not a channel resource is available for the CR system based on the sensing information. The scheduling module then schedules the data transmission of different users on the available channels based on the hard-decision sensing information (HSI). In this paper, we shall propose a joint crosslayer and sensing design and study its performance advantages over the aforementioned traditional decoupled approaches. We shall consider the downlink transmission of an OFDMA-based secondary system sharing the spectrum with primary users using cognitive radio technology. We shall rely on the joint design framework to optimize a system utility, which adapts the power allocation and the subcarrier assignment across the secondary users (under a average interference constraint to the primary users) based on both the channel state information (CSI) and the raw sensing information (RSI). In addition, we shall also propose a distributed implementation for the cross-layer sensing and scheduling design using primal-dual decomposition approach. Simulation results reveals the substantial performance gain of the proposed joint design over the conventional CR systems.","Cognitive radio,
Chromium,
Radio transmitters,
Channel state information,
Wireless sensor networks,
Base stations,
Data communication,
Downlink,
Constraint optimization,
Design optimization"
Implementing WebGIS on Hadoop: A case study of improving small file I/O performance on HDFS,"Hadoop framework has been widely used in various clusters to build large scale, high performance systems. However, Hadoop distributed file system (HDFS) is designed to manage large files and suffers performance penalty while managing a large amount of small files. As a consequence, many web applications, like WebGIS, may not take benefits from Hadoop. In this paper, we propose an approach to optimize I/O performance of small files on HDFS. The basic idea is to combine small files into large ones to reduce the file number and build index for each file. Furthermore, some novel features such as grouping neighboring files and reserving several latest version of data are considered to meet the characteristics of WebGIS access patterns. Preliminary experiment results show that our approach achieves better performance.","Large-scale systems,
File systems,
Web and internet services,
Geographic Information Systems,
High performance computing,
File servers,
Web server,
Scalability,
Open source software,
Middleware"
On the construction of 2-connected virtual backbone in wireless networks,"Virtual backbone has been proposed as the routing infrastructure to alleviate the broadcasting storm problem in ad hoc networks. Since the nodes in the virtual backbone need to carry other node's traffic, and node and link failure are inherent in wireless networks, it is desirable that the virtual backbone is fault tolerant. In this paper, we propose a new algorithm called Connecting Dominating Set Augmentation (CDSA) to construct a 2-connected virtual backbone which can resist the failure of one wireless node. We show that CDSA has guaranteed quality by proving that the size of the CDSA constructed 2-connected backbone is within a constant factor of the optimal 2-connected virtual backbone size. Through extensive simulations, we demonstrate that in practice, CDSA can build a 2-connected virtual backbone with only small overhead.","Spine,
Wireless networks,
Routing,
Broadcasting,
Storms,
Ad hoc networks,
Telecommunication traffic,
Fault tolerance,
Joining processes,
Resists"
A Nonlocal Maximum Likelihood Estimation Method for Rician Noise Reduction in MR Images,"Postacquisition denoising of magnetic resonance (MR) images is of importance for clinical diagnosis and computerized analysis, such as tissue classification and segmentation. It has been shown that the noise in MR magnitude images follows a Rician distribution, which is signal-dependent when signal-to-noise ratio (SNR) is low. It is particularly difficult to remove the random fluctuations and bias introduced by Rician noise. The objective of this paper is to estimate the noise free signal from MR magnitude images. We model images as random fields and assume that pixels which have similar neighborhoods come from the same distribution. We propose a nonlocal maximum likelihood (NLML) estimation method for Rician noise reduction. Our method yields an optimal estimation result that is more accurate in recovering the true signal from Rician noise than NL means algorithm in the sense of SNR, contrast, and method error. We demonstrate that NLML performs better than the conventional local maximum likelihood (LML) estimation method in preserving and defining sharp tissue boundaries in terms of a well-defined sharpness metric while also having superior performance in method error.","Maximum likelihood estimation,
Rician channels,
Noise reduction,
Signal to noise ratio,
Magnetic resonance,
Clinical diagnosis,
Image analysis,
Magnetic analysis,
Image segmentation,
Magnetic noise"
Cloud Computing: New Wine or Just a New Bottle?,"Cloud computing has evolved from previous computing paradigms going back as far to the days of mainframes, but is it really different? Do the explosive new capabilities from cloud computing solve any of the problems left unsolved from three decades ago? The authors in this issue discuss their views on what cloud computing is leaving the reader to decide for themselves.","Cloud computing,
Grid computing,
Portable computers,
Business,
Physics computing,
Internet,
User interfaces,
Computer interfaces,
Computer applications,
Computer architecture"
A Constant Bound on Throughput Improvement of Multicast Network Coding in Undirected Networks,"Recent research in network coding shows that, joint consideration of both coding and routing strategies may lead to higher information transmission rates than routing only. A fundamental question in the field of network coding is: how large can the throughput improvement due to network coding be? In this paper, we prove that in undirected networks, the ratio of achievable multicast throughput with network coding to that without network coding is bounded by a constant ratio of 2, i.e., network coding can at most double the throughput. This result holds for any undirected network topology, any link capacity configuration, any multicast group size, and any source information rate. This constant bound 2 represents the tightest bound that has been proved so far in general undirected settings, and is to be contrasted with the unbounded potential of network coding in improving multicast throughput in directed networks.","Throughput,
Network coding,
Routing,
Network topology,
Encoding,
Decoding,
Computer science,
Spread spectrum communication,
Communication networks,
Information rates"
Agent-based modeling and simulation,"Agent-based modeling and simulation (ABMS) is a new approach to modeling systems comprised of autonomous, interacting agents. Computational advances have made possible a growing number of agent-based models across a variety of application domains. Applications range from modeling agent behavior in the stock market, supply chains, and consumer markets, to predicting the spread of epidemics, mitigating the threat of bio-warfare, and understanding the factors that may be responsible for the fall of ancient civilizations. Such progress suggests the potential of ABMS to have far-reaching effects on the way that businesses use computers to support decision-making and researchers use agent-based models as electronic laboratories. Some contend that ABMS “is a third way of doing science” and could augment traditional deductive and inductive reasoning as discovery methods. This brief tutorial introduces agent-based modeling by describing the foundations of ABMS, discussing some illustrative applications, and addressing toolkits and methods for developing agent-based models.","Power system modeling,
Laboratories,
Computational modeling,
Adaptive systems,
Application software,
Power system economics,
Power generation economics,
Environmental economics,
Predictive models,
Stock markets"
Attack-proof collaborative spectrum sensing in cognitive radio networks,"Collaborative sensing in cognitive radio networks can significantly improve the probability of detecting the transmission of primary users. In current collaborative sensing schemes, all collaborative secondary users are assumed to be honest. As a consequence, the system is vulnerable to attacks in which malicious secondary users report false detection results. In this paper, we investigate how to improve the security of collaborative sensing. Particularly, we develop a malicious user detection algorithm that calculates the suspicious level of secondary users based on their past reports. Then, we calculate trust values as well as consistency values that are used to eliminate the malicious users' influence on the primary user detection results. Through simulations, we show that even a single malicious user can significantly degrade the performance of collaborative sensing. The proposed trust value indicator can effectively differentiate honest and malicious secondary users. The receiver operating characteristic (ROC) curves for the primary user detection demonstrate the improvement in the security of collaborative sensing.","Collaboration,
Cognitive radio,
Security,
Power system modeling,
Detection algorithms,
Degradation,
Biomedical computing,
Computer networks,
Biomedical engineering,
Receivers"
A DTI-Derived Measure of Cortico-Cortical Connectivity,"We arm researchers with a simple method to chart a macroscopic cortico-cortical connectivity network in living human subjects. The researcher provides a diffusion-magnetic resonance imaging (MRI) data set and N cortical regions of interest. In return, we provide an N times N structural adjacency matrix (SAM) quantifying the relative connectivity between all cortical region pairs. We also return a connectivity map for each pair to enable visualization of interconnecting fiber bundles. The measure of connectivity we devise is: 1) free of length bias, 2) proportional to fiber bundle cross-sectional area, and 3) invariant to an exchange of seed and target. We construct a 3-D lattice scaffolding (graph) for white-matter by drawing a link between each pair of voxels in a 26-voxel neighborhood for which their two respective principal eigenvectors form a sufficiently small angle. The connectivity between a cortical region pair is then measured as the maximum number of link-disjoint paths that can be established between them in the white-matter graph. We devise an efficient Edmonds-Karp-like algorithm to compute a conservative bound on the maximum number of link-disjoint paths. Using both simulated and authentic diffusion-tensor imaging data, we demonstrate that the number of link-disjoint paths as a measure of connectivity satisfies properties 1)-3), unlike the fraction of intersecting streamlines-the measure intrinsic to most existing probabilistic tracking algorithms. Finally, we present connectivity maps of some notoriously difficult to track longitudinal and contralateral fasciculi.","Humans,
Magnetic resonance imaging,
Diffusion tensor imaging,
Neurons,
Councils,
Nerve fibers,
Microscopy,
Data visualization,
Integrated circuit interconnections,
Area measurement"
DMTCP: Transparent checkpointing for cluster computations and the desktop,"DMTCP (distributed multithreaded checkpointing) is a transparent user-level checkpointing package for distributed applications. Checkpointing and restart is demonstrated for a wide range of over 20 well known applications, including MATLAB, Python, TightVNC, MPICH2, OpenMPI, and runCMS. RunCMS runs as a 680 MB image in memory that includes 540 dynamic libraries, and is used for the CMS experiment of the Large Hadron Collider at CERN. DMTCP transparently checkpoints general cluster computations consisting of many nodes, processes, and threads; as well as typical desktop applications. On 128 distributed cores (32 nodes), checkpoint and restart times are typically 2 seconds, with negligible run-time overhead. Typical checkpoint times are reduced to 0.2 seconds when using forked checkpointing. Experimental results show that checkpoint time remains nearly constant as the number of nodes increases on a medium-size cluster. DMTCP automatically accounts for fork, exec, ssh, mutexes/ semaphores, TCP/IP sockets, UNIX domain sockets, pipes, ptys (pseudo-terminals), terminal modes, ownership of controlling terminals, signal handlers, open file descriptors, shared open file descriptors, I/O (including the readline library), shared memory (via mmap), parent-child process relationships, pid virtualization, and other operating system artifacts. By emphasizing an unprivileged, user-space approach, compatibility is maintained across Linux kernels from 2.6.9 through the current 2.6.28. Since DMTCP is unprivileged and does not require special kernel modules or kernel patches, DMTCP can be incorporated and distributed as a checkpoint-restart module within some larger package.","Checkpointing,
Kernel,
Packaging,
Libraries,
Sockets,
MATLAB,
Collision mitigation,
Large Hadron Collider,
Yarn,
Runtime"
Semantics-based code search,"Our goal is to use the vast repositories of available open source code to generate specific functions or classes that meet a user's specifications. The key words here are specifications and generate. We let users specify what they are looking for as precisely as possible using keywords, class or method signatures, test cases, contracts, and security constraints. Our system then uses an open set of program transformations to map retrieved code into what the user asked for. This approach is implemented in a prototype system for Java with a web interface.","Programming profession,
Search engines,
Security,
Writing,
Open source software,
Computer science,
Testing,
Contracts,
Prototypes,
Java"
SWIM: A Simple Model to Generate Small Mobile Worlds,"This paper presents small world in motion (SWIM), a new mobility model for ad-hoc networking. SWIM is relatively simple, is easily tuned by setting just a few parameters, and generates traces that look real-synthetic traces have the same statistical properties of real traces. SWIM shows experimentally and theoretically the presence of the power law and exponential decay dichotomy of inter-contact time, and, most importantly, our experiments show that it can predict very accurately the performance of forwarding protocols.","Protocols,
Humans,
Peer to peer computing,
Mobile computing,
Power generation,
Communications Society,
Computer science,
Computational modeling,
Computer simulation,
Probability distribution"
Adaptive Random Test Case Prioritization,"Regression testing assures changed programs against unintended amendments. Rearranging the execution order of test cases is a key idea to improve their effectiveness. Paradoxically, many test case prioritization techniques resolve tie cases using the random selection approach, and yet random ordering of test cases has been considered as ineffective. Existing unit testing research unveils that adaptive random testing (ART) is a promising candidate that may replace random testing (RT). In this paper, we not only propose a new family of coverage-based ART techniques, but also show empirically that they are statistically superior to the RT-based technique in detecting faults. Furthermore, one of the ART prioritization techniques is consistently comparable to some of the best coverage-based prioritization techniques (namely, the ""additional"" techniques) and yet involves much less time cost.","Software testing,
Subspace constraints,
Fault detection,
Automatic testing,
Programming,
Greedy algorithms,
Software engineering,
Costs,
Australia Council,
Computer science"
Semantics of Ranking Queries for Probabilistic Data and Expected Ranks,"When dealing with massive quantities of data, top-k queries are a powerful technique for returning only the k most relevant tuples for inspection, based on a scoring function. The problem of efficiently answering such ranking queries has been studied and analyzed extensively within traditional database settings. The importance of the top-k is perhaps even greater in probabilistic databases, where a relation can encode exponentially many possible worlds. There have been several recent attempts to propose definitions and algorithms for ranking queries over probabilistic data. However, these all lack many of the intuitive properties of a top-k over deterministic data. Specifically, we define a number of fundamental properties, including exact-k, containment, unique-rank, value-invariance, and stability, which are all satisfied by ranking queries on certain data. We argue that all these conditions should also be fulfilled by any reasonable definition for ranking uncertain data. Unfortunately, none of the existing definitions is able to achieve this. To remedy this shortcoming, this work proposes an intuitive new approach of expected rank. This uses the well-founded notion of the expected rank of each tuple across all possible worlds as the basis of the ranking. We are able to prove that, in contrast to all existing approaches, the expected rank satisfies all the required properties for a ranking query. We provide efficient solutions to compute this ranking across the major models of uncertain data, such as attribute-level and tuple-level uncertainty. For an uncertain relation of N tuples, the processing cost is O(N logN)—no worse than simply sorting the relation. In settings where there is a high cost for generating each tuple in turn, we provide pruning techniques based on probabilistic tail bounds that can terminate the search early and guarantee that the top-k has been found. Finally, a comprehensive experimental study confirms the effectiveness of our approach.","Streaming media,
Multimedia databases,
Data engineering,
USA Councils,
Computer science,
Inspection,
Uncertainty,
Costs,
Relational databases,
Power engineering and energy"
Learning-Based Vertebra Detection and Iterative Normalized-Cut Segmentation for Spinal MRI,"Automatic extraction of vertebra regions from a spinal magnetic resonance (MR) image is normally required as the first step to an intelligent spinal MR image diagnosis system. In this work, we develop a fully automatic vertebra detection and segmentation system, which consists of three stages; namely, AdaBoost-based vertebra detection, detection refinement via robust curve fitting, and vertebra segmentation by an iterative normalized cut algorithm. In order to produce an efficient and effective vertebra detector, a statistical learning approach based on an improved AdaBoost algorithm is proposed. A robust estimation procedure is applied on the detected vertebra locations to fit a spine curve, thus refining the above vertebra detection results. This refinement process involves removing the false detections and recovering the miss-detected vertebrae. Finally, an iterative normalized-cut segmentation algorithm is proposed to segment the precise vertebra regions from the detected vertebra locations. In our implementation, the proposed AdaBoost-based detector is trained from 22 spinal MR volume images. The experimental results show that the proposed vertebra detection and segmentation system can achieve nearly 98% vertebra detection rate and 96% segmentation accuracy on a variety of testing spinal MR images. Our experiments also show the vertebra detection and segmentation accuracies by using the proposed algorithm are superior to those of the previous representative methods. The proposed vertebra detection and segmentation system is proved to be robust and accurate so that it can be used for advanced research and application on spinal MR images.","Magnetic resonance imaging,
Image segmentation,
Robustness,
Iterative algorithms,
Detectors,
Magnetic resonance,
Curve fitting,
Statistical learning,
Spine,
System testing"
Towards physical mashups in the Web of Things,"Wireless Sensor Networks (WSNs) have promising industrial applications, since they reduce the gap between traditional enterprise systems and the real world. However, every particular application requires complex integration work, and therefore technical expertise, effort and time which prevents users from creating small tactical, ad-hoc applications using sensor networks. Following the success of Web 2.0 “mashups”, we propose a similar lightweight approach for combining enterprise services (e.g. ERPs) with WSNs. Specifically, we discuss the traditional integration solutions, propose and implement an alternative architecture where sensor nodes are accessible according to the REST principles. With this approach, the nodes become part of a “Web of Things” and interacting with them as well as composing their services with existing ones, becomes almost as easy as browsing the web.","Mashups,
Application software,
Wireless sensor networks,
Enterprise resource planning,
Computer architecture,
Temperature sensors,
Pervasive computing,
Information technology,
Sensor systems and applications,
Computer industry"
Biologically-inspired dynamical systems for movement generation: Automatic real-time goal adaptation and obstacle avoidance,"Dynamical systems can generate movement trajectories that are robust against perturbations. This article presents an improved modification of the original dynamic movement primitive (DMP) framework by Ijspeert et al [1], [2]. The new equations can generalize movements to new targets without singularities and large accelerations. Furthermore, the new equations can represent a movement in 3D task space without depending on the choice of coordinate system (invariance under invertible affine transformations). Our modified DMP is motivated from biological data (spinal-cord stimulation in frogs) and human behavioral experiments. We further extend the formalism to obstacle avoidance by exploiting the robustness against perturbations: an additional term is added to the differential equations to make the robot steer around an obstacle. This additional term empirically describes human obstacle avoidance. We demonstrate the feasibility of our approach using the Sarcos Slave robot arm: after learning a single placing movement, the robot placed a cup between two arbitrarily given positions and avoided approaching obstacles.",
Periocular biometrics in the visible spectrum: A feasibility study,"Periocular biometric refers to the facial region in the immediate vicinity of the eye. Acquisition of the periocular biometric does not require high user cooperation and close capture distance unlike other ocular biometrics (e.g., iris, retina, and sclera). We study the feasibility of using periocular images of an individual as a biometric trait. Global and local information are extracted from the periocular region using texture and point operators resulting in a feature set that can be used for matching. The effect of fusing these feature sets is also studied. The experimental results show a 77% rank-1 recognition accuracy using 958 images captured from 30 different subjects.","Biometrics,
Iris,
Retina,
Humans,
Eyebrows,
Blood vessels,
Biomedical imaging,
Computer science,
Data mining,
Image recognition"
Prostate Cancer Segmentation With Simultaneous Estimation of Markov Random Field Parameters and Class,"Prostate cancer is one of the leading causes of death from cancer among men in the United States. Currently, high-resolution magnetic resonance imaging (MRI) has been shown to have higher accuracy than trans-rectal ultrasound (TRUS) when used to ascertain the presence of prostate cancer. As MRI can provide both morphological and functional images for a tissue of interest, some researchers are exploring the uses of multispectral MRI to guide prostate biopsies and radiation therapy. However, success with prostate cancer localization based on current imaging methods has been limited due to overlap in feature space of benign and malignant tissues using any one MRI method and the interobserver variability. In this paper, we present a new unsupervised segmentation method for prostate cancer detection, using fuzzy Markov random fields (fuzzy MRFs) for the segmentation of multispectral MR prostate images. Typically, both hard and fuzzy MRF models have two groups of parameters to be estimated: the MRF parameters and class parameters for each pixel in the image. To date, these two parameters have been treated separately, and estimated in an alternating fashion. In this paper, we develop a new method to estimate the parameters defining the Markovian distribution of the measured data, while performing the data clustering simultaneously. We perform computer simulations on synthetic test images and multispectral MR prostate datasets to demonstrate the efficacy and efficiency of the proposed method and also provide a comparison with some of the commonly used methods.","Prostate cancer,
Markov random fields,
Magnetic resonance imaging,
Image segmentation,
Ultrasonic imaging,
Parameter estimation,
Performance evaluation,
Biopsy,
Biomedical applications of radiation,
Cancer detection"
Password Cracking Using Probabilistic Context-Free Grammars,"Choosing the most effective word-mangling rules to use when performing a dictionary-based password cracking attack can be a difficult task. In this paper we discuss a new method that generates password structures in highest probability order. We first automatically create a probabilistic context-free grammar based upon a training set of previously disclosed passwords. This grammar then allows us to generate word-mangling rules, and from them, password guesses to be used in password cracking. We will also show that this approach seems to provide a more effective way to crack passwords as compared to traditional methods by testing our tools and techniques on real password sets. In one series of experiments, training on a set of disclosed passwords, our approach was able to crack 28% to 129% more passwords than John the Ripper, a publicly available standard password cracking program.",
Convergence and Parameter Choice for Monte-Carlo Simulations of Diffusion MRI,"This paper describes a general and flexible Monte- Carlo simulation framework for diffusing spins that generates realistic synthetic data for diffusion magnetic resonance imaging. Similar systems in the literature consider only simple substrates and their authors do not consider convergence and parameter optimization. We show how to run Monte-Carlo simulations within complex irregular substrates. We compare the results of the Monte-Carlo simulation to an analytical model of restricted diffusion to assess precision and accuracy of the generated results. We obtain an optimal combination of spins and updates for a given run time by trading off number of updates in favor of number of spins such that precision and accuracy of sythesized data are both optimized. Further experiments demonstrate the system using a tissue environment that current analytic models cannot capture. This tissue model incorporates swelling, abutting, and deformation. Swelling-induced restriction in the extracellular space due to the effects of abutting cylinders leads to large departures from the predictions of the analytical model, which does not capture these effects. This swelling-induced restriction may be an important mechanism in explaining the changes in apparent diffusion constant observed in the aftermath of acute ischemic stroke.","Convergence,
Magnetic resonance imaging,
Analytical models,
Microstructure,
Diffusion tensor imaging,
Extracellular,
Magnetic field measurement,
In vivo,
Tensile stress,
Biomedical imaging"
Test architecture design and optimization for three-dimensional SoCs,"Core-based system-on-chips (SoCs) fabricated on three-dimensional (3D) technology are emerging for better integration capabilities. Effective test architecture design and optimization techniques are essential to minimize the manufacturing cost for such giga-scale integrated circuits. In this paper, we propose novel test solutions for 3D SoCs manufactured with die-to-wafer and die-to-die bonding techniques. Both testing time and routing cost associated with the test access mechanisms in 3D SoCs are considered in our simulated annealing-based technique. Experimental results on ITC'02 SoC benchmark circuits are compared to those obtained with two baseline solutions, which show the effectiveness of the proposed technique.","Design optimization,
Circuit testing,
System-on-a-chip,
Integrated circuit technology,
Integrated circuit testing,
Integrated circuit manufacture,
Cost function,
Pulp manufacturing,
Bonding,
Routing"
RELAX: Incorporating Uncertainty into the Specification of Self-Adaptive Systems,"Self-adaptive systems have the capability to autonomously modify their behaviour at run-time in response to changes in their environment. Self-adaptation is particularly necessary for applications that must run continuously, even under adverse conditions and changing requirements; sample domains include automotive systems, telecommunications, and environmental monitoring systems. While a few techniques have been developed to support the monitoring and analysis of requirements for adaptive systems, limited attention has been paid to the actual creation and specification of requirements of self-adaptive systems. As a result, self-adaptivity is often constructed in an ad-hoc manner. In this paper, we argue that a more rigorous treatment of requirements explicitly relating to self-adaptivity is needed and that, in particular, requirements languages for self-adaptive systems should include explicit constructs for specifying and dealing with the uncertainty inherent in self-adaptive systems. We present RELAX, a new requirements language for self-adaptive systems and illustrate it using examples from the smart home domain.","Uncertainty,
Smart homes,
Runtime environment,
Automotive engineering,
Condition monitoring,
Adaptive systems,
Humans,
Biomedical monitoring,
Computer science,
USA Councils"
Modeling Dominance in Group Conversations Using Nonverbal Activity Cues,"Dominance - a behavioral expression of power - is a fundamental mechanism of social interaction, expressed and perceived in conversations through spoken words and audiovisual nonverbal cues. The automatic modeling of dominance patterns from sensor data represents a relevant problem in social computing. In this paper, we present a systematic study on dominance modeling in group meetings from fully automatic nonverbal activity cues, in a multi-camera, multi-microphone setting. We investigate efficient audio and visual activity cues for the characterization of dominant behavior, analyzing single and joint modalities. Unsupervised and supervised approaches for dominance modeling are also investigated. Activity cues and models are objectively evaluated on a set of dominance-related classification tasks, derived from an analysis of the variability of human judgment of perceived dominance in group discussions. Our investigation highlights the power of relatively simple yet efficient approaches and the challenges of audiovisual integration. This constitutes the most detailed study on automatic dominance modeling in meetings to date.",
Consistent cooperative localization,"In cooperative navigation, teams of mobile robots obtain range and/or angle measurements to each other and dead-reckoning information to help each other navigate more accurately. One typical approach is moving baseline navigation, in which multiple Autonomous Underwater Vehicles (AUVs) exchange range measurements using acoustic modems to perform mobile trilateration. While the sharing of information between vehicles can be highly beneficial, exchanging measurements and state estimates can also be dangerous because of the risk of measurements being used by a vehicle more than once; such data re-use leads to inconsistent (overconfident) estimates, making data association and outlier rejection more difficult and divergence more likely. In this paper, we present a technique for the consistent cooperative localization of multiple AUVs performing mobile trilateration. Each AUV establishes a bank of filters, performing careful bookkeeping to track the origins of measurements and prevent the use any of the measurements more than once. The multiple estimates are combined in a consistent manner, yielding conservative covariance estimates. The technique is illustrated using simulation results. The new method is compared side-by-side with a naive approach that does not keep track of the origins of measurements, illustrating that the new method keeps conservative covariance bounds whereas state estimates obtained with the naive approach become overconfident and diverge.","Acoustic measurements,
Navigation,
State estimation,
Performance evaluation,
Remotely operated vehicles,
Yield estimation,
Mobile robots,
Goniometers,
Underwater vehicles,
Underwater acoustics"
Dynamic thermal management in 3D multicore architectures,"Technology scaling has caused the feature sizes to shrink continuously, whereas interconnects, unlike transistors, have not followed the same trend. Designing 3D stack architectures is a recently proposed approach to overcome the power consumption and delay problems associated with the interconnects by reducing the length of the wires going across the chip. However, 3D integration introduces serious thermal challenges due to the high power density resulting from placing computational units on top of each other. In this work, we first investigate how the existing thermal management, power management and job scheduling policies affect the thermal behavior in 3D chips. We then propose a dynamic thermally-aware job scheduling technique for 3D systems to reduce the thermal problems at very low performance cost. Our approach can also be integrated with power management policies to reduce energy consumption while avoiding the thermal hot spots and large temperature variations.","Thermal management,
Multicore processing,
Energy management,
Power system management,
Energy consumption,
Processor scheduling,
Transistors,
Computer architecture,
Delay,
Wires"
Reliable relay assisted wireless multicast using network coding,"We first consider a topology consisting of one source, two destinations and one relay. For such a topology, it is shown that a network coding based cooperative (NCBC) multicast scheme can achieve a diversity order of two. In this paper, we discuss and analyze NCBC in a systematic way as well as compare its performance with two other multicast protocols. The throughput, delay and queue length for each protocol are evaluated. In addition, we present an optimal scheme to maximize throughput subject to delay and queue length constraints. Numerical results will demonstrate that network coding can bring significant gains in terms of throughput.","Relays,
Network coding,
Throughput,
Delay,
Wireless networks,
Multicast protocols,
Network topology,
Mobile ad hoc networks,
Broadcasting,
Fading"
Requirements for QoS-Based Web Service Description and Discovery,"The goal of service oriented architectures (SOAs) is to enable the creation of business applications through the automatic discovery and composition of independently developed and deployed (Web) services. Automatic discovery of Web services (WSs) can be achieved by incorporating semantics into a richer WS description model (WSDM) and by the use of semantic Web (SW) technologies in the WS matchmaking and selection (i.e., discovery) process. A sufficiently rich WSDM should encompass not only functional but also nonfunctional aspects like quality of service (QoS). QoS is a set of performance and domain-dependent attributes that has a substantial impact on WS requesters' expectations. Thus, it can be used for distinguishing between many functionally equivalent WSs that are available nowadays. This paper starts by defining QoS in the context of WSs. Its main contribution is the analysis of the requirements for a semantically rich QoS-based WSDM and an accurate, effective QoS-based WS Discovery (WSDi) process. In addition, a road map of extending current WS standard technologies for realizing semantic, functional, and QoS-based WSDi, respecting the above requirements, is presented.","Quality of service,
Monitoring,
Availability,
Security,
Delay,
Web services"
The secret life of bugs: Going past the errors and omissions in software repositories,"Every bug has a story behind it. The people that discover and resolve it need to coordinate, to get information from documents, tools, or other people, and to navigate through issues of accountability, ownership, and organizational structure. This paper reports on a field study of coordination activities around bug fixing that used a combination of case study research and a survey of software professionals. Results show that the histories of even simple bugs are strongly dependent on social, organizational, and technical knowledge that cannot be solely extracted through automation of electronic repositories, and that such automation provides incomplete and often erroneous accounts of coordination. The paper uses rich bug histories and survey results to identify common bug fixing coordination patterns and to provide implications for tool designers and researchers of coordination in software development.",
In vitro and in vivo studies on wireless powering of medical sensors and implantable devices,This paper investigates wireless electricity (witricity) and its application to medical sensors and implantable devices. Several coupling scenarios of resonators are analyzed theoretically. In vitro experiments are conducted in open air and through an agar phantom of the human head. An in vivo animal experiment is also carried out. Our studies indicate that witricity is a suitable tool for providing wireless power to a variety of medical sensors and implanted devices.,
A method for normalizing histology slides for quantitative analysis,"Inconsistencies in the preparation of histology slides make it difficult to perform quantitative analysis on their results. In this paper we provide two mechanisms for overcoming many of the known inconsistencies in the staining process, thereby bringing slides that were processed or stored under very different conditions into a common, normalized space to enable improved quantitative analysis.","Fluorescence,
Microscopy,
Information retrieval,
Deconvolution,
Image retrieval,
Diffraction,
Detectors,
Biological cells,
Maximum likelihood estimation,
Testing"
Max-margin hidden conditional random fields for human action recognition,"We present a new method for classification with structured latent variables. Our model is formulated using the max-margin formalism in the discriminative learning literature. We propose an efficient learning algorithm based on the cutting plane method and decomposed dual optimization. We apply our model to the problem of recognizing human actions from video sequences, where we model a human action as a global root template and a constellation of several “parts”. We show that our model outperforms another similar method that uses hidden conditional random fields, and is comparable to other state-of-the-art approaches. More importantly, our proposed work is quite general and can potentially be applied in a wide variety of vision problems that involve various complex, interdependent latent structures.","Humans,
Object recognition,
Optimization methods,
Video sequences,
Computer vision,
Torso,
Labeling,
Pixel,
Training data"
"Fast, Accurate and Shift-Varying Line Projections for Iterative Reconstruction Using the GPU","List-mode processing provides an efficient way to deal with sparse projections in iterative image reconstruction for emission tomography. An issue often reported is the tremendous amount of computation required by such algorithm. Each recorded event requires several back- and forward line projections. We investigated the use of the programmable graphics processing unit (GPU) to accelerate the line-projection operations and implement fully-3D list-mode ordered-subsets expectation-maximization for positron emission tomography (PET). We designed a reconstruction approach that incorporates resolution kernels, which model the spatially-varying physical processes associated with photon emission, transport and detection. Our development is particularly suitable for applications where the projection data is sparse, such as high-resolution, dynamic, and time-of-flight PET reconstruction. The GPU approach runs more than 50 times faster than an equivalent CPU implementation while image quality and accuracy are virtually identical. This paper describes in details how the GPU can be used to accelerate the line projection operations, even when the lines-of-response have arbitrary endpoint locations and shift-varying resolution kernels are used. A quantitative evaluation is included to validate the correctness of this new approach.","Image reconstruction,
Positron emission tomography,
Detectors,
Iterative algorithms,
Acceleration,
Spatial resolution,
Kernel,
Photonic crystals,
Solids,
Radiology"
Markov chain existence and Hidden Markov models in spectrum sensing,"The primary function of a cognitive radio is to detect idle frequencies or sub-bands, not used by the primary users (PUs), and allocate these frequencies to secondary users. The state of the sub-band at any time point is either free (unoccupied by a PU) or busy (occupied by a PU). The states of a sub-band are monitored over L consecutive time periods, where each period is of a given time interval. Existing research assume the presence of a Markov chain for sub-band utilization by PUs over time, but this assumption has not been validated. Therefore, in this paper we validate existence of a Markov chain for sub-band utilization using real-time measurements collected in the paging band (928–948 MHz). Furthermore, since the detection of idle sub-bands by a cognitive radio is prone to errors, we probabilistically model the errors and then formulate a spectrum sensing paradigm as a Hidden Markov model that predicts the true states of a sub-band. The accuracy of our proposed method in predicting the true states of the sub-band is substantiated using extensive simulations.","Hidden Markov models,
Chromium,
Cognitive radio,
Viterbi algorithm,
Predictive models,
Wireless sensor networks,
Power system modeling,
Radio spectrum management,
Monitoring,
Bandwidth"
FCUDA: Enabling efficient compilation of CUDA kernels onto FPGAs,"As growing power dissipation and thermal effects disrupted the rising clock frequency trend and threatened to annul Moore's law, the computing industry has switched its route to higher performance through parallel processing. The rise of multi-core systems in all domains of computing has opened the door to heterogeneous multi-processors, where processors of different compute characteristics can be combined to effectively boost the performance per watt of different application kernels. GPUs and FPGAs are becoming very popular in PC-based heterogeneous systems for speeding up compute intensive kernels of scientific, imaging and simulation applications. GPUs can execute hundreds of concurrent threads, while FPGAs provide customized concurrency for highly parallel kernels. However, exploiting the parallelism available in these applications is currently not a push-button task. Often the programmer has to expose the application's fine and coarse grained parallelism by using special APIs. CUDA is such a parallel-computing API that is driven by the GPU industry and is gaining significant popularity. In this work, we adapt the CUDA programming model into a new FPGA design flow called FCUDA, which efficiently maps the coarse and fine grained parallelism exposed in CUDA onto the reconfigurable fabric. Our CUDA-to-FPGA flow employs AutoPilot, an advanced high-level synthesis tool which enables high-abstraction FPGA programming. FCUDA is based on a source-to-source compilation that transforms the SPMD CUDA thread blocks into parallel C code for AutoPilot. We describe the details of our CUDA-to-FPGA flow and demonstrate the highly competitive performance of the resulting customized FPGA multi-core accelerators. To the best of our knowledge, this is the first CUDA-to-FPGA flow to demonstrate the applicability and potential advantage of using the CUDA programming model for high-performance computing in FPGAs.",
Introduction for Freshmen to Embedded Systems Using LEGO Mindstorms,"The purpose of the course presented here is to introduce freshmen to embedded systems using LEGO Mindstorms, under an ANSI-C programming environment. The students build their own LEGO robots, make programs for them using ANSI-C, and operate them. By creating these LEGO robots, the students become more motivated, learning the basic concepts of embedded systems and how these concepts can be applied to real world problems.","Embedded system,
Robots,
Robot sensing systems,
Tactile sensors,
Software,
Programming,
Hardware"
Priority Assignment for Global Fixed Priority Pre-Emptive Scheduling in Multiprocessor Real-Time Systems,"This paper addresses the problem of priority assignment in multiprocessor real-time systems using global fixed task-priority pre-emptive scheduling. In this paper, we prove that Audsley's Optimal Priority Assignment (OPA) algorithm, originally devised for uniprocessor scheduling, is applicable to the multiprocessor case, provided that three conditions hold with respect to the schedulability tests used. Our empirical investigations show that the combination of optimal priority assignment policy and a simple compatible schedulability test is highly effective, in terms of the number of tasksets deemed to be schedulable. We also examine the performance of heuristic priority assignment policies such as Deadline Monotonic, and an extension of the TkC priority assignment policy called DkC that can be used with any schedulability test. Here we find that Deadline Monotonic priority assignment has relatively poor performance in the multiprocessor case, while DkC priority assignment is highly effective.","Real time systems,
Processor scheduling,
Testing,
Scheduling algorithm,
Aerospace electronics,
Space technology,
Computer science,
Runtime,
Embedded system,
Automotive electronics"
A survey and taxonomy of infrastructure as a service and web hosting cloud providers,"With an increasing number of providers claiming to offer Cloud infrastructures, there is a lack in the community for a common terminology, accompanied by a clear definition and classification of Cloud features. We conduct in this paper a survey on a selection of Cloud providers, and propose a taxonomy of eight important Cloud computing elements covering service type, resource deployment, hardware, runtime tuning, business model, middleware, and performance. We conclude that the provisioning of Service Level Agreements as utilities, of open and interoperable middleware solutions, as well as of sustained performance metrics for high-performance computing applications are three elements with the highest need of further community research.","Taxonomy,
Cloud computing,
Hardware,
Companies,
Software maintenance,
Terminology,
Middleware,
Web and internet services,
Computer science,
Runtime"
GPU clusters for high-performance computing,"Large-scale GPU clusters are gaining popularity in the scientific computing community. However, their deployment and production use are associated with a number of new challenges. In this paper, we present our efforts to address some of the challenges with building and running GPU clusters in HPC environments. We touch upon such issues as balanced cluster architecture, resource sharing in a cluster environment, programming models, and applications for GPU clusters.","Production,
Computer architecture,
Resource management,
Bandwidth,
Hardware,
Space cooling,
Data security,
Parallel programming,
Quadratic programming,
Computational biophysics"
Minimally Redundant 2-D Array Designs for 3-D Medical Ultrasound Imaging,"In real-time ultrasonic 3-D imaging, in addition to difficulties in fabricating and interconnecting 2-D transducer arrays with hundreds of elements, there are also challenges in acquiring and processing data from a large number of ultrasound channels. The coarray (spatial convolution of the transmit and receive arrays) can be used to find efficient array designs that capture all of the spatial frequency content (a transmit-receive element combination corresponds to a spatial frequency) with a reduced number of active channels and firing events. Eliminating the redundancies in the transmit-receive element combinations and firing events reduces the overall system complexity and improves the frame rate. Here we explore four reduced redundancy 2-D array configurations for miniature 3-D ultrasonic imaging systems. Our approach is based on 1) coarray design with reduced redundancy using different subsets of linear arrays constituting the 2-D transducer array, and 2) 3-D scanning using fan-beams (narrow in one dimension and broad in the other dimension) generated by the transmit linear arrays. We form the overall array response through coherent summation of the individual responses of each transmit-receive array pairs. We present theoretical and simulated point spread functions of the array configurations along with quantitative comparison in terms of the front-end complexity and image quality.","Ultrasonic imaging,
Biomedical imaging,
Phased arrays,
Ultrasonic transducer arrays,
Ultrasonic transducers,
Array signal processing,
Data acquisition,
Biomedical transducers,
Frequency,
Image quality"
A Composite Semisupervised SVM for Classification of Hyperspectral Images,"This letter presents a novel composite semisupervised support vector machine (SVM) for the spectral-spatial classification of hyperspectral images. In particular, the proposed technique exploits the following: 1) unlabeled data for increasing the reliability of the training phase when few training samples are available and 2) composite kernel functions for simultaneously taking into account spectral and spatial information included in the considered image. Experiments carried out on a hyperspectral image pointed out the effectiveness of the presented technique, which resulted in a significant increase of the classification accuracy with respect to both supervised SVMs and progressive semisupervised SVMs with single kernels, as well as supervised SVMs with composite kernels.",
Multirate Anypath Routing in Wireless Mesh Networks,"In this paper, we present a new routing paradigm that generalizes opportunistic routing in wireless mesh networks. In multirate anypath routing, each node uses both a set of next hops and a selected transmission rate to reach a destination. Using this rate, a packet is broadcast to the nodes in the set and one of them forwards the packet on to the destination. To date, there is no theory capable of jointly optimizing both the set of next hops and the transmission rate used by each node. We bridge this gap by introducing a polynomial-time algorithm to this problem and provide the proof of its optimality. The proposed algorithm runs in the same running time as regular shortest-path algorithms and is therefore suitable for deployment in link-state routing protocols. We conducted experiments in a 802.11b testbed network, and our results show that multirate anypath routing performs on average 80% and up to 6.4 times better than anypath routing with a fixed rate of 11 Mbps. If the rate is fixed at 1 Mbps instead, performance improves by up to one order of magnitude.","Wireless mesh networks,
Peer to peer computing,
Polynomials,
Testing,
Bit rate,
Computer science,
Broadcasting,
Routing protocols,
Costs,
Performance gain"
Decentralized power control scheme in femtocell networks: A game theoretic approach,"We propose a new distributed power control scheme for closed access femtocell networks. Since femto APs can be deployed unpredictably in a macro cell area, interference to macro users and interference between femto APs in down-link (DL) cause a severe problem. In this paper, we describe the system model using a noncooperative game model. We formulate a payoff function to provide fairness and to minimize interference. At the same time, we prove that this function can be appeared as a supermodular type. Our power control algorithm based on the game model can be applied to the decentralized environment. This novel power control algorithm leads transmission power to reach a steady state condition, i.e. Nash Equilibrium (NE). Through mathematical analysis and numerical results, we show that our proposed power control algorithm has several good characteristics.","Power control,
Femtocell networks,
Game theory,
Interference,
Power system modeling,
Macrocell networks,
Computer science,
Electronic mail,
Steady-state,
Nash equilibrium"
Higher-order clique reduction in binary graph cut,"We introduce a new technique that can reduce any higher-order Markov random field with binary labels into a first-order one that has the same minima as the original. Moreover, we combine the reduction with the fusion-move and QPBO algorithms to optimize higher-order multi-label problems. While many vision problems today are formulated as energy minimization problems, they have mostly been limited to using first-order energies, which consist of unary and pairwise clique potentials, with a few exceptions that consider triples. This is because of the lack of efficient algorithms to optimize energies with higher-order interactions. Our algorithm challenges this restriction that limits the representational power of the models, so that higher-order energies can be used to capture the rich statistics of natural scenes. To demonstrate the algorithm, we minimize a third-order energy, which allows clique potentials with up to four pixels, in an image restoration problem. The problem uses the fields of experts model, a learned spatial prior of natural images that has been used to test two belief propagation algorithms capable of optimizing higher-order energies. The results show that the algorithm exceeds the BP algorithms in both optimization performance and speed.","Markov random fields,
Higher order statistics,
Labeling,
Layout,
Image restoration,
Belief propagation,
Optimization methods,
Iterative algorithms,
Biology,
Energy capture"
In Vivo High-ResolutionConductivity Imaging of the Human Leg Using MREIT: The First Human Experiment,"We present the first in vivo cross-sectional conductivity image of the human leg with 1.7 mm pixel size using the magnetic resonance electrical impedance tomography (MREIT) technique. After a review of its experimental protocol by an Institutional Review Board (IRB), we performed MREIT imaging experiments of four human subjects using a 3 T MRI scanner. Adopting thin and flexible carbon-hydrogel electrodes with a large surface area and good contact, we could inject as much as 9 mA current in a form of 15 ms pulse into the leg without producing a painful sensation and motion artifact. Sequentially injecting two imaging currents in two different directions, we collected induced magnetic flux density data inside the leg. Scaled conductivity images reconstructed by using the single-step harmonic B z algorithm well distinguished different parts of the subcutaneous adipose tissue, muscle, crural fascia, intermuscular septum and bone inside the leg. We could observe spurious noise spikes in the outer layer of the bone primarily due to the MR signal void phenomenon there. Around the fat, the chemical shift of about two pixels occurred obscuring the boundary of the fat region. Future work should include a fat correction method incorporated in the MREIT pulse sequence and improvements in radio-frequency coils and image reconstruction algorithms. Further human imaging experiments are planned and being conducted to produce conductivity images from different parts of the human body.","In vivo,
Humans,
Leg,
Conductivity,
Magnetic resonance imaging,
Image reconstruction,
Bones,
Pixel,
Magnetic resonance,
Surface impedance"
GPU-based parallel particle swarm optimization,"A novel parallel approach to run standard particle swarm optimization (SPSO) on Graphic Processing Unit (GPU) is presented in this paper. By using the general-purpose computing ability of GPU and based on the software platform of Compute Unified Device Architecture (CUDA) from NVIDIA, SPSO can be executed in parallel on GPU. Experiments are conducted by running SPSO both on GPU and CPU, respectively, to optimize four benchmark test functions. The running time of the SPSO based on GPU (GPU-SPSO) is greatly shortened compared to that of the SPSO on CPU (CPU-SPSO). Running speed of GPU-SPSO can be more than 11 times as fast as that of CPU-SPSO, with the same performance. compared to CPU-SPSO, GPU-SPSO shows special speed advantages on large swarm population applications and hign dimensional problems, which can be widely used in real optimizing problems.","Particle swarm optimization,
Central Processing Unit,
Graphics,
Concurrent computing,
Computer architecture,
Ant colony optimization,
Application software,
Machine intelligence,
Programming profession,
Benchmark testing"
A Survey of Botnet and Botnet Detection,"Among the various forms of malware, botnets are emerging as the most serious threat against cyber-security as they provide a distributed platform for several illegal activities such as launching distributed denial of service attacks against critical targets, malware dissemination, phishing, and click fraud. The defining characteristic of botnets is the use of command and control channels through which they can be updated and directed. Recently, botnet detection has been an interesting research topic related to cyber-threat and cyber-crime prevention. This paper is a survey of botnet and botnet detection. The survey clarifies botnet phenomenon and discusses botnet detection techniques. This survey classifies botnet detection techniques into four classes: signature-based, anomaly-based, DNS-based, and mining-base. It summarizes botnet detection techniques in each class and provides a brief comparison of botnet detection techniques.",
Godson-3: A Scalable Multicore RISC Processor with x86 Emulation,"The Godson-3 microprocessor aims at high-throughput server applications, high-performance scientific computing, and high-end embedded applications. It offers a scalable network on chip, hardware support for x86 emulation, and a reconfigurable architecture. The four-core Godson-3 chip is fabricated with 65-nm CMOS technology. Eight- and 16-core Godson-3 chips are in development.","Multicore processing,
Reduced instruction set computing,
Emulation,
CMOS technology,
Microprocessors,
Network servers,
Scientific computing,
Network-on-a-chip,
Hardware,
Reconfigurable architectures"
SD-VBS: The San Diego Vision Benchmark Suite,"In the era of multi-core, computer vision has emerged as an exciting application area which promises to continue to drive the demand for both more powerful and more energy efficient processors. Although there is still a long way to go, vision has matured significantly over the last few decades, and the list of applications that are useful to end users continues to grow. The parallelism inherent in vision applications makes them a promising workload for multi-core and many-core processors.","Application software,
Computer vision,
Parallel processing,
MATLAB,
Drives,
Energy efficiency,
Portable computers,
Computer architecture,
Computational modeling,
Runtime"
An accurate and scalable analytical model for IEEE 802.15.4 slotted CSMA/CA networks,"In this paper a Markov chain based analytical model is proposed to evaluate the slotted CSMA/CA algorithm specified in the MAC layer of IEEE 802.15.4 standard. The analytical model consists of two two-dimensional Markov chains, used to model the state transition of an 802.15.4 device, during the periods of a transmission and between two consecutive frame transmissions, respectively. By introducing the two Markov chains a small number of Markov states are required and the scalability of the analytical model is improved. The analytical model is used to investigate the impact of the CSMA/CA parameters, the number of contending devices, and the data frame size on the network performance in terms of throughput and energy efficiency. It is shown by simulations that the proposed analytical model can accurately predict the performance of slotted CSMA/CA algorithm for uplink, downlink and bi-direction traffic, with both acknowledgement and non-acknowledgement modes.","Analytical models,
Multiaccess communication,
Scalability,
Throughput,
Energy efficiency,
Predictive models,
Downlink,
Bidirectional control,
Telecommunication traffic,
Traffic control"
On optimal precoding in linear vector Gaussian channels with arbitrary input distribution,"The design of the precoder the maximizes the mutual information in linear vector Gaussian channels with an arbitrary input distribution is studied. Precisely, the precoder optimal left singular vectors and singular values are derived. The characterization of the right singular vectors is left, in general, as an open problem whose computational complexity is then studied in three cases: Gaussian signaling, low SNR, and high SNR. For the Gaussian signaling case and the low SNR regime, the dependence of the mutual information on the right singular vectors vanishes, making the optimal precoder design problem easy to solve. In the high SNR regime, however, the dependence on the right singular vectors cannot be avoided and we show the difficulty of computing the optimal precoder through an NP-hardness analysis.","Vectors,
Gaussian channels,
Mutual information,
Jacobian matrices,
Symmetric matrices,
Computational complexity,
Signal design,
Covariance matrix,
Transmitters,
AWGN"
Domestic energy management methodology for optimizing efficiency in Smart Grids,"Increasing energy prices and the greenhouse effect lead to more awareness of energy efficiency of electricity supply. During the last years, a lot of domestic technologies have been developed to improve this efficiency. These technologies on their own already improve the efficiency, but more can be gained by a combined management. Multiple optimization objectives can be used to improve the efficiency, from peak shaving and Virtual Power Plant (VPP) to adapting to fluctuating generation of wind turbines. In this paper a generic management methology is proposed applicable for most domestic technologies, scenarios and optimization objectives. Both local scale optimization objectives (a single house) and global scale optimization objectives (multiple houses) can be used. Simulations of different scenarios show that both local and global objectives can be reached.","Energy management,
Optimization methods,
Smart grids,
Power generation,
Wind energy generation,
Home appliances,
Load management,
Distributed power generation,
Energy efficiency,
Technology management"
System design of an Insertable Robotic Effector Platform for Single Port Access (SPA) Surgery,"This paper presents a novel design and preliminary kinematic analysis of an insertable robotic effector platform (IREP) for single port access (SPA) surgery. The IREP robot can be deployed into body cavity through a Ø15 mm skin incision to perform SPA procedures. It consists of two snake-like continuum robots as slave surgical assistants for tissue manipulation, two parallelogram mechanisms for the continuum robots' placement, and one controllable stereo vision module with integrated light source for depth perception and tool tracking. Design considerations and alternatives, calculations and preliminary simulations of this 17-DoF surgical robotic system are presented in this paper. The overall control system hierarchy for tele-manipulation using the IREP robot is also presented.","Surgery,
Parallel robots,
Robot vision systems,
System analysis and design,
Kinematics,
Skin,
Lighting control,
Stereo vision,
Light sources,
Control systems"
Agent Mining: The Synergy of Agents and Data Mining,"Autonomous agents and multiagent systems (or agents) and data mining and knowledge discovery (or data mining) are two of the most active areas in information technology. Ongoing research has revealed a number of intrinsic challenges and problems facing each area, which can't be addressed solely within the confines of the respective discipline. A profound insight of bringing these two communities together has unveiled a tremendous potential for new opportunities and wider applications through the synergy of agents and data mining. With increasing interest in this synergy, agent mining is emerging as a new research field studying the interaction and integration of agents and data mining. In this paper, we give an overall perspective of the driving forces, theoretical underpinnings, main research issues, and application domains of this field, while addressing the state-of-the-art of agent mining research and development. Our review is divided into three key research topics: agent-driven data mining, data mining-driven agents, and joint issues in the synergy of agents and data mining. This new and promising field exhibits a great potential for groundbreaking work from foundational, technological and practical perspectives.","Data mining,
Intelligent agent,
Delta modulation,
Humans,
Intelligent systems,
Logic,
Enterprise resource planning,
Communities,
Multiagent systems,
Peer to peer computing"
Narrowband Magnetic Particle Imaging,"The magnetic particle imaging (MPI) method directly images the magnetization of super-paramagnetic iron oxide (SPIO) nanoparticles, which are contrast agents commonly used in magnetic resonance imaging (MRI). MPI, as originally envisioned, requires a high-bandwidth receiver coil and preamplifier, which are difficult to optimally noise match. This paper introduces Narrowband MPI, which dramatically reduces bandwidth requirements and increases the signal-to-noise ratio for a fixed specific absorption rate. We employ a two-tone excitation (called intermodulation) that can be tailored for a high-Q, narrowband receiver coil. We then demonstrate a new MPI instrument capable of full 3-D tomographic imaging of SPIO particles by imaging acrylic and tissue phantoms.","Narrowband,
Magnetic particles,
Magnetic resonance imaging,
Coils,
Magnetization,
Iron,
Nanoparticles,
Preamplifiers,
Magnetic noise,
Bandwidth"
4-terminal relay technology for complementary logic,"A 4-terminal (4T) relay technology is proposed for complementary logic circuit applications. The advantage of the 4T relay design is that it provides a means for electrically adjusting the switching voltage; as a result, a 4T relay can mimic the operation of either an n-channel or p-channel MOSFET. Fabricated 4T relays exhibit good on-state current (Ion ≫ 700µA for VDS = 1V) and zero off-state leakage current. Low-voltage switching (≪ 2V) and low switching delay (100ns) are demonstrated by appropriately biasing the body terminal. Endurance exceeds 109 on/off cycles without stiction or wear issues. Complementary operation is demonstrated in a functional relay inverter circuit.",
Model-Based Iterative Reconstruction for Radial Fast Spin-Echo MRI,"In radial fast spin-echo magnetic resonance imaging (MRI), a set of overlapping spokes with an inconsistent T2 weighting is acquired, which results in an averaged image contrast when employing conventional image reconstruction techniques. This work demonstrates that the problem may be overcome with the use of a dedicated reconstruction method that further allows for T2 quantification by extracting the embedded relaxation information. Thus, the proposed reconstruction method directly yields a spin-density and relaxivity map from only a single radial data set. The method is based on an inverse formulation of the problem and involves a modeling of the received MRI signal. Because the solution is found by numerical optimization, the approach exploits all data acquired. Further, it handles multicoil data and optionally allows for the incorporation of additional prior knowledge. Simulations and experimental results for a phantom and human brain in vivo demonstrate that the method yields spin-density and relaxivity maps that are neither affected by the typical artifacts from TE mixing, nor by streaking artifacts from the incomplete k-space coverage at individual echo times.","Magnetic resonance imaging,
Image sampling,
Image reconstruction,
Reconstruction algorithms,
Tellurium,
Biomedical measurements,
Time measurement,
Frequency,
Nuclear magnetic resonance,
Data mining"
I/O performance challenges at leadership scale,"Today's top high performance computing systems run applications with hundreds of thousands of processes, contain hundreds of storage nodes, and must meet massive I/O requirements for capacity and performance. These leadership-class systems face daunting challenges to deploying scalable I/O systems. In this paper we present a case study of the I/O challenges to performance and scalability on Intrepid, the IBM Blue Gene/P system at the Argonne Leadership Computing Facility. Listed in the top 5 fastest supercomputers of 2008, Intrepid runs computational science applications with intensive demands on the I/O system. We show that Intrepid's file and storage system sustain high performance under varying workloads as the applications scale with the number of processes.","storage management,
IBM computers,
parallel machines"
An Investigation into the Functional Form of the Size-Defect Relationship for Software Modules,"The importance of the relationship between size and defect proneness of software modules is well recognized. Understanding the nature of that relationship can facilitate various development decisions related to prioritization of quality assurance activities. Overall, the previous research only drew a general conclusion that there was a monotonically increasing relationship between module size and defect proneness. In this study, we analyzed class-level size and defect data in order to increase our understanding of this crucial relationship. In order to obtain validated and more generalizable results, we studied four large-scale object-oriented products, Mozilla, Cn3d, JBoss, and Eclipse. Our results consistently revealed a significant effect of size on defect proneness; however, contrary to common intuition, the size-defect relationship took a logarithmic form, indicating that smaller classes were proportionally more problematic than larger classes. Therefore, practitioners should consider giving higher priority to smaller modules when planning focused quality assurance activities with limited resources. For example, in Mozilla and Eclipse, an inspection strategy investing 80% of available resources on 100-LOC classes and the rest on 1,000-LOC classes would be more than twice as cost effective as the opposite strategy. These results should be immediately useful to guide focused quality assurance activities in large-scale software projects.","Open source software,
Software quality,
Size measurement,
Inspection,
Object oriented modeling,
Predictive models,
Quality assurance,
Large-scale systems,
Software measurement,
Density measurement"
Combined Volumetric and Surface Registration,"In this paper, we propose a novel method for the registration of volumetric images of the brain that optimizes the alignment of both cortical and subcortical structures. In order to achieve this, relevant geometrical information is extracted from a surface-based morph and diffused into the volume using the Navier operator of elasticity, resulting in a volumetric warp that aligns cortical folding patterns. This warp field is then refined with an intensity driven optical flow procedure that registers noncortical regions, while preserving the cortical alignment. The result is a combined surface and volume morph (CVS) that accurately registers both cortical and subcortical regions, establishing a single coordinate system suitable for the entire brain.","Biomedical imaging,
Data mining,
Magnetic resonance imaging,
Hospitals,
Optimization methods,
Elasticity,
Biomedical optical imaging,
Geometrical optics,
Image motion analysis,
Brain"
Electrical treeing characteristics in XLPE power cable insulation in frequency range between 20 and 500 Hz,"Electrical treeing is one of the main reasons for long term degradation of polymeric materials used in high voltage AC applications. In this paper we report on an investigation of electrical tree growth characteristics in XLPE samples from a commercial XLPE power cable. Electrical trees have been grown over a frequency range from 20 Hz to 500 Hz and images of trees were taken using CCD camera without interrupting the application of voltage. The fractal dimension of electric tree is obtained using a simple box-counting technique. Contrary to our expectation it has been found that the fractal dimension prior to the breakdown shows no significant change when frequency of the applied voltage increases. Instead, the frequency accelerates tree growth rate and reduces the time to breakdown. A new approach for investigating the frequency effect on trees has been devised. In addition to looking into the fractal analysis of tree as a whole, regions of growth are being sectioned to reveal differences in terms of growth rate, accumulated damage and fractal dimension.","Trees - insulation,
Power cable insulation,
Frequency,
Fractals,
Breakdown voltage,
Thermal degradation,
Polymers,
Power cables,
Charge coupled devices,
Charge-coupled image sensors"
Differential modulation for two-way wireless communications: a perspective of differential network coding at the physical layer,"This work considers two-way relay channels (TWRC), where two terminals transmit simultaneously to each other with the help of a relay node. For single antenna systems, we propose several new transmission schemes for both amplify-and-forward (AF) protocol and decode-and-forward (DF) protocol where the channel state information is not required. These new schemes are the counterpart of the traditional noncoherent detection or differential detection in point-to-point communications. Differential modulation design for TWRC is challenging because the received signal is a mixture of the signals from both source terminals. We derive maximum likelihood (ML) detectors for both AF and DF protocols, where the latter can be considered as performing differential network coding at the physical layer. As the exact ML detector is prohibitively complex, we propose several suboptimal alternatives including decision feedback detectors and prediction-based detectors. All these strategies work well as evidenced by the simulation results. The proposed protocols are especially useful when the required average data rate is high. In addition, we extend the protocols to the multiple-antenna case and provide the design criterion of the differential unitary space time modulation (DUSTM) for TWRC.","Modulation coding,
Wireless communication,
Network coding,
Physical layer,
Protocols,
Detectors,
Relays,
Maximum likelihood detection,
Transmitting antennas,
Maximum likelihood decoding"
Mining source code to automatically split identifiers for software analysis,"Automated software engineering tools (e.g., program search, concern location, code reuse, quality assessment, etc.) increasingly rely on natural language information from comments and identifiers in code. The first step in analyzing words from identifiers requires splitting identifiers into their constituent words. Unlike natural languages, where space and punctuation are used to delineate words, identifiers cannot contain spaces. One common way to split identifiers is to follow programming language naming conventions. For example, Java programmers often use camel case, where words are delineated by uppercase letters or non-alphabetic characters. However, programmers also create identifiers by concatenating sequences of words together with no discernible delineation, which poses challenges to automatic identifier splitting. In this paper, we present an algorithm to automatically split identifiers into sequences of words by mining word frequencies in source code. With these word frequencies, our identifier splitter uses a scoring technique to automatically select the most appropriate partitioning for an identifier. In an evaluation of over 8000 identifiers from open source Java programs, our Samurai approach outperforms the existing state of the art techniques.","Software maintenance,
Natural languages,
Programming profession,
Software tools,
Java,
Frequency,
Open source software,
Software quality,
Information analysis,
Quality assessment"
Cross-layer optimization for wireless multihop networks with pairwise intersession network coding,"For wireless multi-hop networks with unicast sessions, most coding opportunities involve only two or three sessions as coding across many sessions requires greater transmission power to broadcast the coded symbol to many receivers, which enhances interference. This work shows that with a new flow-based characterization of pairwise intersession network coding (coding across two unicast sessions), an optimal joint coding, scheduling, and rate-control scheme can be devised and implemented using only the binary XOR operation. The new scheduling/rate-control scheme demonstrates provably graceful throughput degradation with imperfect scheduling, which facilitates the design tradeoff between the throughput optimality and computational complexity of different scheduling schemes. Our results show that pairwise intersession network coding improves the throughput of non-coding solutions regardless of whether perfect/imperfect scheduling is used. Both the deterministic and stochastic packet arrivals and departures are considered. This work shows a striking resemblance between pairwise intersession network coding and non-coded solutions, and thus advocates extensions of non-coding wisdoms to their network coding counterpart.","Spread spectrum communication,
Network coding,
Processor scheduling,
Throughput,
Unicast,
Broadcasting,
Interference,
Degradation,
Computational complexity,
Stochastic processes"
Discovering Homogeneous Web Service Community in the User-Centric Web Environment,"The Web has undergone a tremendous change toward a highly user-centric environment. Millions of users can participate and collaborate for their own interests and benefits. Services computing paradigm together with the proliferation of Web services have created great potential opportunities for the users, also known as service consumers, to produce value-added services by means of service discovery and composition. In this paper, we propose an efficient approach to facilitating the service consumer on discovering Web services. First, we analyze the service discovery requirements from the service consumer's perspective and outline a conceptual model of homogeneous Web service communities. The homogeneous service community contains two types of discovery: the search of similar operations and that of composible operations. Second, we describe a similarity measurement model for Web services by leveraging the metadata from WSDL, and design a graph-based algorithm to support both of the two discovery types. Finally, adopting the popular atom feeds, we design a prototype to facilitate the consumers to discover while subscribing Web services in an easy-of-use manner. With the experimental evaluation and prototype demonstration, our approach not only alleviates the consumers from time-consuming discovery tasks but also lowers their entry barrier in the user-centric Web environment.",
Nonlinear Dimensionality Reduction with Local Spline Embedding,"This paper presents a new algorithm for nonlinear dimensionality reduction (NLDR). Our algorithm is developed under the conceptual framework of compatible mapping. Each such mapping is a compound of a tangent space projection and a group of splines. Tangent space projection is estimated at each data point on the manifold, through which the data point itself and its neighbors are represented in tangent space with local coordinates. Splines are then constructed to guarantee that each of the local coordinates can be mapped to its own single global coordinate with respect to the underlying manifold. Thus, the compatibility between local alignments is ensured. In such a work setting, we develop an optimization framework based on reconstruction error analysis, which can yield a global optimum. The proposed algorithm is also extended to embed out of samples via spline interpolation. Experiments on toy data sets and real-world data sets illustrate the validity of our method.","Spline,
Principal component analysis,
Image reconstruction,
Data visualization,
Error analysis,
Interpolation,
Artificial intelligence,
Pattern recognition,
Computer vision,
Data mining"
Nonlinear Index Coding Outperforming the Linear Optimum,"The following source coding problem was introduced by Birk and Kol: a sender holds a word x isin {0, 1}n, and wishes to broadcast a codeword to n receivers, Rn,..., Rn. The receiver Ri is interested in xi, and has prior side information comprising some subset of the n bits. This corresponds to a directed graph G on n vertices, where i j is an edge Ri Ri knows the bit xj. An index code for G is an encoding scheme which enables each Ri to always reconstruct xi, given his side information. The minimal word length of an index code was studied by Bar-Yossef, Birk, Jayram, and Kol (FOCS'06). They introduced a graph parameter, minrk2(G), which completely characterizes the length of an optimal linear index code for G. They showed that in various cases linear codes attain the optimal word length, and conjectured that linear index coding is in fact always optimal. In this work, we disprove the main conjecture of Bar-Yossef, Birk, Jayram, and Kol in the following strong sense: for any epsiv > 0 and sufficiently large n, there is an n-vertex graph G so that every linear index code for G requires codewords of length at least nepsiv and yet a nonlinear index code for G has a word length of ne. This is achieved by an explicit construction, which extends Alon's variant of the celebrated Ramsey construction of Frankl and Wilson. In addition, we study optimal index codes in various, less restricted, natural models, and prove several related properties of the graph parameter minrk(G).","Source coding,
Broadcasting,
Computer science,
Linear code,
IEEE Foundation,
Binary codes"
Modeling RFID signal strength and tag detection for localization and mapping,"In recent years, there has been an increasing interest within the robotics community in investigating whether Radio Frequency Identification (RFID) technology can be utilized to solve localization and mapping problems in the context of mobile robots. We present a novel sensor model which can be utilized for localizing RFID tags and for tracking a mobile agent moving through an RFID-equipped environment. The proposed probabilistic sensor model characterizes the received signal strength indication (RSSI) information as well as the tag detection events to achieve a higher modeling accuracy compared to state-of-the-art models which deal with one of these aspects only. We furthermore propose a method that is able to bootstrap such a sensor model in a fully unsupervised fashion. Real-world experiments demonstrate the effectiveness of our approach also in comparison to existing techniques.","Radiofrequency identification,
Signal mapping,
Sensor phenomena and characterization,
RFID tags,
Context modeling,
Event detection,
RF signals,
Signal processing,
Mobile robots,
Loaded antennas"
An overview of peak-to-average power ratio reduction schemes for OFDM signals,"Orthogonal frequency division multiplexing (OFDM) has been adopted as a standard for various high data rate wireless communication systems due to the spectral bandwidth efficiency, robustness to frequency selective fading channels, etc. However, implementation of the OFDM system entails several difficulties. One of the major drawbacks is the high peak-to-average power ratio (PAPR), which results in intercarrier interference, high out-of-band radiation, and bit error rate performance degradation, mainly due to the nonlinearity of the high power amplifier. This paper reviews the conventional PAPR reduction schemes and their modifications for achieving the low computational complexity required for practical implementation in wireless communication systems.","Peak to average power ratio,
Vectors,
Baseband,
Frequency domain analysis,
Partial transmit sequences,
Bit error rate"
Reactive grasping using optical proximity sensors,"We propose a system for improving grasping using fingertip optical proximity sensors that allows us to perform online grasp adjustments to an initial grasp point without requiring premature object contact or regrasping strategies. We present novel optical proximity sensors that fit inside the fingertips of a Barrett Hand, and demonstrate their use alongside a probabilistic model for robustly combining sensor readings and a hierarchical reactive controller for improving grasps online. This system can be used to complement existing grasp planning algorithms, or be used in more interactive settings where a human indicates the location of objects. Finally, we perform a series of experiments using a Barrett hand equipped with our sensors to grasp a variety of common objects with mixed geometries and surface textures.","Optical sensors,
Robot sensing systems,
Computational geometry,
Fingers,
Geometrical optics,
Tactile sensors,
Computer science,
Sensor systems,
Grasping,
Surface texture"
Reasoning about edits to feature models,"Features express the variabilities and commonalities among programs in a software product line (SPL). A feature model defines the valid combinations of features, where each combination corresponds to a program in an SPL. SPLs and their feature models evolve over time. We classify the evolution of a feature model via modifications as refactorings, specializations, generalizations, or arbitrary edits. We present an algorithm to reason about feature model edits to help designers determine how the program membership of an SPL has changed. Our algorithm takes two feature models as input (before and after edit versions), where the set of features in both models are not necessarily the same, and it automatically computes the change classification. Our algorithm is able to give examples of added or deleted products and efficiently classifies edits to even large models that have thousands of features.","Computer science,
Logic,
Algorithm design and analysis,
Classification algorithms,
Software algorithms,
Assembly,
Law,
Legal factors,
Feedback,
Sun"
The One-Shot similarity kernel,"The One-Shot similarity measure has recently been introduced in the context of face recognition where it was used to produce state-of-the-art results. Given two vectors, their One-Shot similarity score reflects the likelihood of each vector belonging in the same class as the other vector and not in a class defined by a fixed set of “negative” examples. The potential of this approach has thus far been largely unexplored. In this paper we analyze the One-Shot score and show that: (1) when using a version of LDA as the underlying classifier, this score is a Conditionally Positive Definite kernel and may be used within kernel-methods (e.g., SVM), (2) it can be efficiently computed, and (3) that it is effective as an underlying mechanism for image representation. We further demonstrate the effectiveness of the One-Shot similarity score in a number of applications including multiclass identification and descriptor generation.","Kernel,
Computer science,
Linear discriminant analysis,
Support vector machines,
Support vector machine classification,
Image representation,
Training data,
Computer vision,
Face recognition,
Image analysis"
Automatic registration of LIDAR and optical images of urban scenes,"Fusion of 3D laser radar (LIDAR) imagery and aerial optical imagery is an efficient method for constructing 3D virtual reality models. One difficult aspect of creating such models is registering the optical image with the LIDAR point cloud, which is characterized as a camera pose estimation problem. We propose a novel application of mutual information registration methods, which exploits the statistical dependency in urban scenes of optical appearance with measured LIDAR elevation. We utilize the well known downhill simplex optimization to infer camera pose parameters. We discuss three methods for measuring mutual information between LIDAR imagery and optical imagery. Utilization of OpenGL and graphics hardware in the optimization process yields registration times dramatically lower than previous methods. Using an initial registration comparable to GPS/INS accuracy, we demonstrate the utility of our algorithm with a collection of urban images and present 3D models created with the fused imagery.","Laser radar,
Layout,
Cameras,
Mutual information,
Laser fusion,
Laser modes,
Radar imaging,
Virtual reality,
Clouds,
Graphics"
"MDCSim: A multi-tier data center simulation, platform","Performance and power issues are becoming increasingly important in the design of large, cluster-based multitier data centers for supporting a multitude of services. The design and analysis of such large/complex distributed systems often suffer from the lack of availability of an adequate physical infrastructure. This paper presents a comprehensive, flexible, and scalable simulation platform for in-depth analysis of multi-tier data centers. Designed as a pluggable three-level architecture, our simulator captures all the important design specifics of the underlying communication paradigm, kernel level scheduling artifacts, and the application level interactions among the tiers of a three-tier data center. The flexibility of the simulator is attributed to its ability in experimenting with different design alternatives in the three layers, and in analyzing both the performance and power consumption with realistic workloads. The scalability of the simulator is demonstrated with analyses of different data center configurations. In addition, we have designed a prototype three-tier data center on an Infiniband Architecture (IBA) connected Linux cluster to validate the simulator. Using RUBiS benchmark workload, it is shown that the simulator is quite accurate in estimating the throughput, response time, and power consumption. We then demonstrate the applicability of the simulator in conducting three different types of studies. First, we conduct a comparative analysis of the IBA and 10 Gigabit Ethernet (10GigE) under different traffic conditions and with varying size clusters for understanding their relative merits in designing cluster-based servers. Second, measurement and characterization of power consumption across the servers of a three-tier data center is done. Third, we perform a configuration analysis of the Web server (WS), Application Server (AS), and Database Server (DB) for performance optimization. We believe that such a comprehensive simulation infrastructure is critical for providing guidelines in designing efficient and cost-effective multi-tier data centers.","Analytical models,
Energy consumption,
Data analysis,
Performance analysis,
Availability,
Kernel,
Scalability,
Virtual prototyping,
Linux,
Throughput"
Discovering and representing systematic code changes,"Software engineers often inspect program differences when reviewing others' code changes, when writing check-in comments, or when determining why a program behaves differently from expected behavior after modification. Program differencing tools that support these tasks are limited in their ability to group related code changes or to detect potential inconsistencies in those changes. To overcome these limitations and to complement existing approaches, we built Logical Structural Diff (LSdiff), a tool that infers systematic structural differences as logic rules. LSdiff notes anomalies from systematic changes as exceptions to the logic rules. We conducted a focus group study with professional software engineers in a large E-commerce company; we also compared LSdiff's results with textual differences and with structural differences without rules. Our evaluation suggests that LSdiff complements existing differencing tools by grouping code changes that form systematic change patterns regardless of their distribution throughout the code, and its ability to discover anomalies shows promise in detecting inconsistent changes.","Programming profession,
Logic,
Computer science,
Pervasive computing,
Tree graphs,
Flow graphs,
Packaging,
Writing,
Software tools,
Computer bugs"
Exploring the solution space of beaconing in VANETs,"Vehicular networking is an enabling technology for Intelligent Transportation Systems (ITS). Different types of vehicular traffic applications are currently being investigated. In this paper we briefly introduce the communication requirements of a Co-operative Adaptive Cruise Control (C-ACC) vehicular traffic efficiency application. Furthermore, we propose a Channel Busy Time model to evaluate the solution space of a vehicular beaconing system designed to communicate information both vital and sufficient for vehicular traffic applications and in particular for C-ACC. We identify that the solution space is three-dimensional. These dimensions being based on the number of nodes (or vehicles), the beacon generation rate of the nodes and the size (or duration) of a beacon message. Based on the Channel Busy Time model we derive boundaries and ranges of parameters within which the beaconing system can be adapted to meet the requirements of the C-ACC application.","Space exploration,
Communication system traffic control,
Telecommunication traffic,
Intelligent transportation systems,
Traffic control,
Space technology,
Intelligent networks,
Intelligent vehicles,
Programmable control,
Adaptive control"
On the use of relevance feedback in IR-based concept location,"Concept location is a critical activity during software evolution as it produces the location where a change is to start in response to a modification request, such as, a bug report or a new feature request. Lexical-based concept location techniques rely on matching the text embedded in the source code to queries formulated by the developers. The efficiency of such techniques is strongly dependent on the ability of the developer to write good queries. We propose an approach to augment information retrieval (IR) based concept location via an explicit relevance feedback (RF) mechanism. RF is a two-part process in which the developer judges existing results returned by a search and the IR system uses this information to perform a new search, returning more relevant information to the user. A set of case studies performed on open source software systems reveals the impact of RF on IR based concept location.","Radio frequency,
Software maintenance,
Computer science,
Information retrieval,
Humans,
State feedback,
Open source software,
Search engines,
Internet,
Software systems"
"Observe locally, infer globally: A space-time MRF for detecting abnormal activities with incremental updates","We propose a space-time Markov random field (MRF) model to detect abnormal activities in video. The nodes in the MRF graph correspond to a grid of local regions in the video frames, and neighboring nodes in both space and time are associated with links. To learn normal patterns of activity at each local node, we capture the distribution of its typical optical flow with a mixture of probabilistic principal component analyzers. For any new optical flow patterns detected in incoming video clips, we use the learned model and MRF graph to compute a maximum a posteriori estimate of the degree of normality at each local node. Further, we show how to incrementally update the current model parameters as new video observations stream in, so that the model can efficiently adapt to visual context changes over a long period of time. Experimental results on surveillance videos show that our space-time MRF model robustly detects abnormal activities both in a local and global sense: not only does it accurately localize the atomic abnormal activities in a crowded video, but at the same time it captures the global-level abnormalities caused by irregular interactions between local activities.","Optical sensors,
Image motion analysis,
Context modeling,
Markov random fields,
Optical devices,
Pattern analysis,
Optical detectors,
Optical computing,
Maximum a posteriori estimation,
Streaming media"
Interactive visual clustering of large collections of trajectories,"One of the most common operations in exploration and analysis of various kinds of data is clustering, i.e. discovery and interpretation of groups of objects having similar properties and/or behaviors. In clustering, objects are often treated as points in multi-dimensional space of properties. However, structurally complex objects, such as trajectories of moving entities and other kinds of spatio-temporal data, cannot be adequately represented in this manner. Such data require sophisticated and computationally intensive clustering algorithms, which are very hard to scale effectively to large datasets not fitting in the computer main memory. We propose an approach to extracting meaningful clusters from large databases by combining clustering and classification, which are driven by a human analyst through an interactive visual interface.","Humans,
Clustering algorithms,
Data visualization,
Information analysis,
Functional analysis,
Information systems,
Clustering methods,
Scalability,
Joining processes,
Spatiotemporal phenomena"
Diversity analysis on imbalanced data sets by using ensemble models,"Many real-world applications have problems when learning from imbalanced data sets, such as medical diagnosis, fraud detection, and text classification. Very few minority class instances cannot provide sufficient information and result in performance degrading greatly. As a good way to improve the classification performance of weak learner, some ensemble-based algorithms have been proposed to solve class imbalance problem. However, it is still not clear that how diversity affects classification performance especially on minority classes, since diversity is one influential factor of ensemble. This paper explores the impact of diversity on each class and overall performance. As the other influential factor, accuracy is also discussed because of the trade-off between diversity and accuracy. Firstly, three popular re-sampling methods are combined into our ensemble model and evaluated for diversity analysis, which includes under-sampling, over-sampling, and SMOTE [1] - a data generation algorithm. Secondly, we experiment not only on two-class tasks, but also those with multiple classes. Thirdly, we improve SMOTE in a novel way for solving multi-class data sets in ensemble model - SMOTEBagging.","Data analysis,
Voting,
Bagging,
Costs,
Medical diagnosis,
Text categorization,
Intrusion detection,
Semisupervised learning,
Predictive models,
Boosting"
Color image dehazing using the near-infrared,"In landscape photography, distant objects often appear blurred with a blue color cast, a degradation caused by atmospheric haze. To enhance image contrast, pleasantness and information content, dehazing can be performed.","Color,
Light scattering,
Rayleigh scattering,
Particle scattering,
Mie scattering,
Layout,
Filters,
Image fusion,
Cameras,
Degradation"
Low-cost router microarchitecture for on-chip networks,"On-chip networks are critical to the scaling of future multicore processors. The challenge for on-chip network is to reduce the cost including power consumption and area while providing high performance such as low latency and high bandwidth. Although much research in on-chip network have focused on improving the performance of on-chip networks, they have often relied on a router microarchitecture adopted from off-chip networks. As a result, the on-chip network architecture will not scale properly because of design complexity. In this paper, we propose a low-cost, on-chip network router microarchitecture which is different from the commonly assumed baseline router microarchitecture. We reduce the cost of on-chip networks by partitioning the crossbar, prioritizing packets in flight to simplify arbitration, and reducing the amount of buffers. We show that by introducing intermediate buffers to decouple the routing in the x and the y dimensions, high performance can be achieved with the proposed, low-cost router microarchitecture. By removing the complexity of a baseline router microarchitecture, the low-cost router microarchitecture can also approach the ideal latency in on-chip networks. However, the prioritized switch arbitration simplifies the router but creates starvation for some nodes. We show how delaying the rate credits are returned upstream can be used to implement a distributed, starvation avoidance mechanism to provide fairness. Our evaluations show that the proposed low-cost router can reduce the area by 37% and the power consumption by 45% compared with a baseline router microarchitecture that achieves a similar throughput.","Microarchitecture,
Network-on-a-chip,
Delay,
Costs,
Energy consumption,
Switches,
Multicore processing,
Bandwidth,
Routing,
Throughput"
Dynamic clustering for tracking multiple transceiver-free objects,"RF-based transceiver-free object tracking, originally proposed by the authors, allows real-time tracking of a moving object, where the object does not have to be equipped with an RF transceiver. Our previous algorithm, the best cover algorithm, suffers from a drawback, i.e., it does not work well when there are multiple objects in the tracking area. In this paper, we propose a localization model of distance, transmission power and the signal dynamics caused by the objects. The signal dynamics are derived from the measured Radio Signal Strength Indication (RSSI). Using this new model, we propose the “probabilistic cover algorithm” which is based on distributed dynamic clustering thus it can dramatically improve the localization accuracy when multiple objects are present. Moreover, the probabilistic cover algorithm can reduce the tracking latency in the system. We argue that the small overhead of the proposed algorithm makes it scalable for large deployment. Experimental results show that in addition to its ability to identify multiple objects, the tracking accuracy is improved at a rate of 10% to 20%.","Clustering algorithms,
Vehicle dynamics,
Target tracking,
Wireless sensor networks,
Computer science,
Transceivers,
Delay,
Object detection,
Global Positioning System,
Infrared sensors"
Hole filling method using depth based in-painting for view synthesis in free viewpoint television and 3-D video,"Depth image-based rendering (DIBR) is generally used to synthesize virtual view images in free viewpoint television (FTV) and three-dimensional (3-D) video. One of the main problems in DIBR is how to fill the holes caused by disocclusion regions and inaccurate depth values. In this paper, we propose a new hole filling method using a depth based in-painting technique. Experimental results show that the proposed hole filling method provides improved rendering quality both objectively and subjectively.","Filling,
TV,
Rendering (computer graphics),
Information geometry,
Displays,
Layout,
Transform coding,
Laboratories,
Navigation,
Digital multimedia broadcasting"
Monotonic convergence of distributed interference pricing in wireless networks,"We study distributed algorithms for allocating powers and/or adjusting beamforming vectors in a peer-to-peer wireless network which may have multiple-input-single-output (MISO) links. The objective is to maximize the total utility summed over all users, where each user's utility is a function of the received signal-to-interference-plus-noise ratio (SINR). Each user (receiver) announces an interference price, representing the marginal cost of interference from other users. A particular user (transmitter) then updates its power and beamforming vector to maximize its utility minus the interference cost to other users, which is determined from their announced interference prices. We show that if each transmitter update is based on a current set of interference prices and the utility functions satisfy certain concavity conditions, then the total utility is non-decreasing with each update. The proof is based on the convexity of the utility functions with respect to received interference, and applies to rate utility functions, and an arbitrary number of interfering MISO links. The extension to multi-carrier links is discussed as well as algorithmic variations in which the prices are not immediately updated after power or beam updates.","Convergence,
Interference,
Pricing,
Wireless networks,
Array signal processing,
Costs,
Transmitters,
Distributed algorithms,
Peer to peer computing,
Signal to noise ratio"
Generalized Algorithms for Direct Reconstruction of Parametric Images From Dynamic PET Data,"Indirect and direct methods have been developed for reconstructing parametric images from dynamic positron emission tomography (PET) data. Indirect methods are simple and easy to implement because reconstruction and kinetic modeling are performed in two separate steps. Direct methods estimate parametric images directly from dynamic PET sinograms and, in theory, can be statistically more efficient, but the algorithms are often difficult to implement and are very specific to the kinetic model being used. This paper presents a class of generalized algorithms for direct reconstruction of parametric images that are relatively easy to implement and can be adapted to different kinetic models. The proposed algorithms use optimization transfer principle to convert the maximization of a penalized likelihood into a pixel-wise weighted least squares (WLS) kinetic fitting problem at each iteration. Thus, it can employ existing WLS algorithms developed for kinetic models. The proposed algorithms resemble the empirical iterative implementation of the indirect approach, but converge to a solution of the direct formulation. Computer simulations showed that the proposed direct reconstruction algorithms are flexible and achieve a better bias-variance tradeoff than indirect reconstruction methods.","Image reconstruction,
Positron emission tomography,
Kinetic theory,
Biomedical engineering,
Iterative algorithms,
Reconstruction algorithms,
Least squares methods,
Iterative methods,
Biological system modeling,
Pixel"
Pulse Wave Imaging of Normal and Aneurysmal Abdominal Aortas In Vivo,"The abdominal aortic aneurysm (AAA) is a common vascular disease. The current clinical criterion for treating AAAs is an increased diameter above a critical value. However, the maximum diameter does not correlate well with aortic rupture, the main cause of death from AAA disease. AAA disease leads to changes in the aortic wall mechanical properties. The pulse-wave velocity (PWV) may indicate such a change. Because of limitations in temporal and spatial resolution, the widely used foot-to-foot method measures the global, instead of regional, PWV between two points at a certain distance in the circulation. However, mechanical properties are nonuniform along the normal and pathological (e.g., the AAA and atherosclerosis) arteries; thus, such changes are typically regional. Pulse-wave imaging (PWI) has been developed by our group to map the pulse-wave propagation along the abdominal aorta in mice in vivo. By using a retrospective electrocardiogram (ECG) gating technique, the radio-frequency (RF) signals over one cardiac cycle were obtained in murine aortas at the extremely high frame rate of 8 kHz and with a field-of-view (FOV) of 12 times 12 mm2. The velocities of the aortic wall were estimated using an RF-based speckle tracking method. An Angiotensin II (AngII) infusion-based AAA model was used to simulate the human AAA case. Sequences of wall velocity images can noninvasively and quantitatively map the propagation of the pulse wave along the aortic wall. In the normal and sham aortas, the propagation of the pulse wave was relatively uniform along the wall, while in the AngII-treated aortas, the propagation was shown to be nonuniform. There was no significant difference (p > 0.05) in the PWV between sham (4.67 plusmn 1.15 m/s, n = 5) and AngII-treated (4.34 plusmn 1.48 m/s, n = 17) aortas. The correlation coefficient of the linear regression was significantly higher (p < 0.005) in the sham aortas (0.89 plusmn 0.03, n = 5) than in the AngII-treated ones (0.61 plusmn 0.15, n = 17). The wall velocities induced by the pulse wave were lower and the pulse wave moved nonuniformly along the AngII-treated aorta (p < 0.005), with the lowest velocities at the aneurysmal regions. The discrepancy in the regional wall velocity and the nonuniform pulse-wave propagation along the AngII-treated aorta indicated the inhomogeneities in the aortic wall properties, and the reduced wall velocities indicated stiffening of the aneurysmal wall. This novel technique may thus constitute an early detection tool of vascular degeneration as well as serve as a suitable predictor of AAA rupture, complementary to the current clinical screening practice.","Abdomen,
Biomedical imaging,
In vivo,
Diseases,
Mechanical factors,
Radio frequency,
Aneurysm,
Spatial resolution,
Pathology,
Atherosclerosis"
BAT: A robust signature scheme for vehicular networks using Binary Authentication Tree,"In this paper, we propose a robust and efficient signature scheme for vehicle-to-infrastructure communications, called binary authentication tree (BAT). The BAT scheme can effectively eliminate the performance bottleneck when verifying a mass of signatures within a rigorously required interval, even under adverse scenarios with bogus messages. Given any n received messages with k ges 1 bogus ones, the computation cost to verify all these messages only requires approximately (k + 1) ldr log(n/k) + 4k - 2 time-consuming pairing operations. The BAT scheme can also be gracefully transplanted to other similar batch signature schemes. In addition, it offers the other conventional security for vehicular networks, such as identity privacy and traceability. Theoretical analysis and simulation results demonstrate the validity and practicality of the BAT scheme.","Robustness,
Authentication,
Road vehicles,
Communication system security,
Wireless communication,
Data privacy,
Protocols,
Safety,
Computational efficiency,
Analytical models"
Compact modelling of Through-Silicon Vias (TSVs) in three-dimensional (3-D) integrated circuits,"Modeling parasitic parameters of Through-Silicon-Via (TSV) structures is essential in exploring electrical characteristics such as delay and signal integrity (SI) of circuits and interconnections in three-dimensional (3-D) Integrated Circuits (ICs). This paper presents a complete set of self-consistent equations including self and coupling terms for resistance, capacitance and inductance of various TSV structures. Further, a reduced-order electrical circuit model is proposed for isolated TSVs as well as bundled structures for delay and SI analysis, and extracted TSV parasitics are employed in Spectre simulations for performance evaluations. Critical issues in the performance modeling for design space exploration of 3-D ICs such as cross-talk induced switching pattern dependent delay variation and cross-talk on noise are discussed. The error in these metrics when using the proposed models as compared to a field solver is contained to a few percentage points.","Integrated circuit modeling,
Through-silicon vias,
Delay,
Electric variables,
Integrated circuit interconnections,
Equations,
Coupling circuits,
Electric resistance,
Parasitic capacitance,
Inductance"
Performance Analysis of OLSR and BATMAN Protocols Considering Link Quality Parameter,"In this paper, we present the implementation and analysis of our testbed considering the Link Quality Window Size (LQWS) parameter of Optimized Link State Routing (OLSR) and Better Approach To Mobile Ad-hoc Networking (B.A.T.M.A.N.) protocols. We investigate the effect of mobility in the throughput of a Mobile Ad-hoc Network (MANET). The mobile nodes move toward the destination at a regular speed. When the mobile nodes arrive at the corner, they stop for about three seconds. In our experiments, we consider two cases: only one node is moving (mobile node)and two nodes (intermediate nodes) are moving at the same time. We assess the performance of our testbed in terms of throughput, round trip time, jitter and packet loss. From our experiments, we found that throughput of TCP was improved by reducing LQWS.","Performance analysis,
Routing protocols,
Throughput,
Mobile ad hoc networks,
System testing,
Ad hoc networks,
Performance loss,
Computer networks,
Information science,
USA Councils"
Multi-service load sharing for resource management in the cellular/WLAN integrated network,"With the interworking between a cellular network and wireless local area networks (WLANs), an essential aspect of resource management is taking advantage of the overlay network structure to efficiently share the multi-service traffic load between the interworked systems. In this study, we propose a new load sharing scheme for voice and elastic data services in a cellular/WLAN integrated network. Admission control and dynamic vertical handoff are applied to pool the free bandwidths of the two systems to effectively serve elastic data traffic and improve the multiplexing gain. To further combat the cell bandwidth limitation, data calls in the cell are served under an efficient service discipline, referred to as shortest remaining processing time (SRPT). The SRPT can well exploit the heavy-tailedness of data call size to improve the resource utilization. An accurate analytical model is developed to determine an appropriate size threshold so that data calls are properly distributed to the integrated cell and WLAN, taking into account the load conditions and traffic characteristics. It is observed from extensive simulation and numerical analysis that the new scheme significantly improves the overall system performance.",
A scalable auto-tuning framework for compiler optimization,We describe a scalable and general-purpose framework for auto-tuning compiler-generated code. We combine Active Harmony's parallel search backend with the CHiLL compiler transformation framework to generate in parallel a set of alternative implementations of computation kernels and automatically select the one with the best-performing implementation. The resulting system achieves performance of compiler-generated code comparable to the fully automated version of the ATLAS library for the tested kernels. Performance for various kernels is 1.4 to 3.6 times faster than the native Intel compiler without search. Our search algorithm simultaneously evaluates different combinations of compiler optimizations and converges to solutions in only a few tens of search-steps.,"Optimizing compilers,
Programming profession,
Program processors,
Application software,
Kernel,
Tuning,
Parallel architectures,
Costs,
Computer architecture,
Software libraries"
"Temporal Summaries: Supporting Temporal Categorical Searching, Aggregation and Comparison","When analyzing thousands of event histories, analysts often want to see the events as an aggregate to detect insights and generate new hypotheses about the data. An analysis tool must emphasize both the prevalence and the temporal ordering of these events. Additionally, the analysis tool must also support flexible comparisons to allow analysts to gather visual evidence. In a previous work, we introduced align, rank, and filter (ARF) to accentuate temporal ordering. In this paper, we present temporal summaries, an interactive visualization technique that highlights the prevalence of event occurrences. Temporal summaries dynamically aggregate events in multiple granularities (year, month, week, day, hour, etc.) for the purpose of spotting trends over time and comparing several groups of records. They provide affordances for analysts to perform temporal range filters. We demonstrate the applicability of this approach in two extensive case studies with analysts who applied temporal summaries to search, filter, and look for patterns in electronic health records and academic records.","Data visualization,
Aggregates,
Filters,
Data analysis,
Displays,
Collaborative work,
Springs,
History,
Event detection,
Performance analysis"
Accelerated Gradient Method for Multi-task Sparse Learning Problem,"Many real world learning problems can be recast as multi-task learning problems which utilize correlations among different tasks to obtain better generalization performance than learning each task individually. The feature selection problem in multi-task setting has many applications in fields of computer vision, text classification and bio-informatics. Generally, it can be realized by solving a L-1-infinity regularized optimization problem. And the solution automatically yields the joint sparsity among different tasks. However, due to the nonsmooth nature of the L-1-infinity norm, there lacks an efficient training algorithm for solving such problem with general convex loss functions. In this paper, we propose an accelerated gradient method based on an ``optimal'' first order black-box method named after Nesterov and provide the convergence rate for smooth convex loss functions. For nonsmooth convex loss functions, such as hinge loss, our method still has fast convergence rate empirically. Moreover, by exploiting the structure of the L-1-infinity ball, we solve the black-box oracle in Nesterov's method by a simple sorting scheme. Our method is suitable for large-scale multi-task learning problem since it only utilizes the first order information and is very easy to implement. Experimental results show that our method significantly outperforms the most state-of-the-art methods in both convergence speed and learning accuracy.","Acceleration,
Gradient methods,
Convergence,
Large-scale systems,
Computer science,
Machine learning,
Optimization methods,
Data mining,
Data engineering,
Application software"
A layered architecture for Vehicular Delay-Tolerant Networks,"Vehicular Delay-Tolerant Network (VDTN) is a new network architecture based on the concept of Delay Tolerant Networks (DTN). It aims to be an architecture that handles non-real time applications at low cost, under unreliable conditions, enabling connectivity in diverse scenarios, using vehicles to carry data between terminal nodes. For example, it can be applied in rural and remote areas, or in emergency scenarios. This paper proposes a layered architecture for VDTNs, using out-of-band signaling, based on the separation of the control plane and data plane. It presents the layers interactions and the envisioned protocols required at each layer. The paper provides a deep understanding of the characteristics of VDTN and reveal some design issues in its modeling, leading to insights for future theoretic study and protocol design.",
Large-scale attribute selection using wrappers,"Scheme-specific attribute selection with the wrapper and variants of forward selection is a popular attribute selection technique for classification that yields good results. However, it can run the risk of overfitting because of the extent of the search and the extensive use of internal cross-validation. Moreover, although wrapper evaluators tend to achieve superior accuracy compared to filters, they face a high computational cost. The problems of overfitting and high runtime occur in particular on high-dimensional datasets, like microarray data. We investigate Linear Forward Selection, a technique to reduce the number of attributes expansions in each forward selection step. Our experiments demonstrate that this approach is faster, finds smaller subsets and can even increase the accuracy compared to standard forward selection. We also investigate a variant that applies explicit subset size determination in forward selection to combat overfitting, where the search is forced to stop at a precomputed ldquooptimalrdquo subset size. We show that this technique reduces subset size while maintaining comparable accuracy.",
Lasing in Cs at 894 nm pumped by the dissociation of CsAr and CsKr excimers,We describe the first demonstration of an atomic laser (Cs) pumped by photoexciting CsKr or CsAr excimers which subsequently dissociate. Photopumping atomic gas lasers with broadband diode lasers is now possible.,"Laser excitation,
Pump lasers,
Atomic beams,
Satellites,
Atom lasers,
Gas lasers,
Laser transitions,
Diode lasers,
Absorption,
Resonance"
Context-Based Matching and Ranking of Web Services for Composition,"In this work, we propose a two-step, context-based semantic approach to the problem of matching and ranking Web services for possible service composition. We present an analysis of different methods for classifying Web services for possible composition and supply a context-based semantic matching method for ranking these possibilities. Semantic understanding of Web services may provide added value by identifying new possibilities for compositions of services. The semantic matching ranking approach is unique since it provides the Web service designer with an explicit numeric estimation of the extent to which a possible composition ldquomakes sense.rdquo First, we analyze two common methods for text processing, TF/IDF and context analysis; and two types of service description, free text and WSDL. Second, we present a method for evaluating the proximity of services for possible compositions. Each Web service WSDL context descriptor is evaluated according to its proximity to other services' free text context descriptors. The methods were tested on a large repository of real-world Web services. The experimental results indicate that context analysis is more useful than TF/IDF. Furthermore, the method evaluating the proximity of the WSDL description to the textual description of other services provides high recall and precision results.","Context,
Web services,
Ontologies,
Data mining,
Internet,
Probability density function,
Distance measurement"
RoleNet: Movie Analysis from the Perspective of Social Networks,"With the idea of social network analysis, we propose a novel way to analyze movie videos from the perspective of social relationships rather than audiovisual features. To appropriately describe role's relationships in movies, we devise a method to quantify relations and construct role's social networks, called RoleNet. Based on RoleNet, we are able to perform semantic analysis that goes beyond conventional feature-based approaches. In this work, social relations between roles are used to be the context information of video scenes, and leading roles and the corresponding communities can be automatically determined. The results of community identification provide new alternatives in media management and browsing. Moreover, by describing video scenes with role's context, social-relation-based story segmentation method is developed to pave a new way for this widely-studied topic. Experimental results show the effectiveness of leading role determination and community identification. We also demonstrate that the social-based story segmentation approach works much better than the conventional tempo-based method. Finally, we give extensive discussions and state that the proposed ideas provide insights into context-based video analysis.","Motion pictures,
Communities,
Videos,
Social network services,
Postal services,
Biological system modeling,
Media"
Mining Individual Life Pattern Based on Location History,"The increasing pervasiveness of location-acquisition technologies (GPS, GSM networks, etc.) enables people to conveniently log their location history into spatial-temporal data, thus giving rise to the necessity as well as opportunity to discovery valuable knowledge from this type of data. In this paper, we propose the novel notion of individual life pattern, which captures individual's general life style and regularity. Concretely, we propose the life pattern normal form (the LP-normal form) to formally describe which kind of life regularity can be discovered from location history; then we propose the LP-Mine framework to effectively retrieve life patterns from raw individual GPS data. Our definition of life pattern focuses on significant places of individual life and considers diverse properties to combine the significant places. LP-Mine is comprised of two phases: the modelling phase and the mining phase. The modelling phase pre-processes GPS data into an available format as the input of the mining phase. The mining phase applies separate strategies to discover different types of pattern. Finally, we conduct extensive experiments using GPS data collected by volunteers in the real world to verify the effectiveness of the framework.",
HPMR: Prefetching and pre-shuffling in shared MapReduce computation environment,"MapReduce is a programming model that supports distributed and parallel processing for large-scale data-intensive applications such as machine learning, data mining, and scientific simulation. Hadoop is an open-source implementation of the MapReduce programming model. Hadoop is used by many companies including Yahoo!, Amazon, and Facebook to perform various data mining on large-scale data sets such as user search logs and visit logs. In these cases, it is very common to share the same computing resources by multiple users due to practical considerations about cost, system utilization, and manageability. However, Hadoop assumes that all cluster nodes are dedicated to a single user, failing to guarantee high performance in the shared MapReduce computation environment. In this paper, we propose two optimization schemes, prefetching and pre-shuffling, which improve the overall performance under the shared environment while retaining compatibility with the native Hadoop. The proposed schemes are implemented in the native Hadoop-0.18.3 as a plug-in component called HPMR (High Performance MapReduce Engine). Our evaluation on the Yahoo!Grid platform with three different workloads and seven types of test sets from Yahoo! shows that HPMR reduces the execution time by up to 73%.","Prefetching,
Large-scale systems,
Data mining,
Parallel programming,
Parallel processing,
Machine learning,
Computational modeling,
Open source software,
Facebook,
Costs"
24/7 Characterization of petascale I/O workloads,"Developing and tuning computational science applications to run on extreme scale systems are increasingly complicated processes. Challenges such as managing memory access and tuning message-passing behavior are made easier by tools designed specifically to aid in these processes. Tools that can help users better understand the behavior of their application with respect to I/O have not yet reached the level of utility necessary to play a central role in application development and tuning. This deficiency in the tool set means that we have a poor understanding of how specific applications interact with storage. Worse, the community has little knowledge of what sorts of access patterns are common in today's applications, leading to confusion in the storage research community as to the pressing needs of the computational science community. This paper describes the Darshan I/O characterization tool. Darshan is designed to capture an accurate picture of application I/O behavior, including properties such as patterns of access within files, with the minimum possible overhead. This characterization can shed important light on the I/O behavior of applications at extreme scale. Darshan also can enable researchers to gain greater insight into the overall patterns of access exhibited by such applications, helping the storage community to understand how to best serve current computational science applications and better predict the needs of future applications. In this work we demonstrate Darshan's ability to characterize the I/O behavior of four scientific applications and show that it induces negligible overhead for I/O intensive jobs with as many as 65,536 processes.","Computer applications,
Laboratories,
Memory management,
Reflection,
File systems,
Petascale computing,
Mathematics,
Computer science,
Application software,
Pressing"
Recognizing Visual Focus of Attention From Head Pose in Natural Meetings,"We address the problem of recognizing the visual focus of attention (VFOA) of meeting participants based on their head pose. To this end, the head pose observations are modeled using a Gaussian mixture model (GMM) or a hidden Markov model (HMM) whose hidden states correspond to the VFOA. The novelties of this paper are threefold. First, contrary to previous studies on the topic, in our setup, the potential VFOA of a person is not restricted to other participants only. It includes environmental targets as well (a table and a projection screen), which increases the complexity of the task, with more VFOA targets spread in the pan as well as tilt gaze space. Second, we propose a geometric model to set the GMM or HMM parameters by exploiting results from cognitive science on saccadic eye motion, which allows the prediction of the head pose given a gaze target. Third, an unsupervised parameter adaptation step not using any labeled data is proposed, which accounts for the specific gazing behavior of each participant. Using a publicly available corpus of eight meetings featuring four persons, we analyze the above methods by evaluating, through objective performance measures, the recognition of the VFOA from head pose information obtained either using a magnetic sensor device or a vision-based tracking system. The results clearly show that in such complex but realistic situations, the VFOA recognition performance is highly dependent on how well the visual targets are separated for a given meeting participant. In addition, the results show that the use of a geometric model with unsupervised adaptation achieves better results than the use of training data to set the HMM parameters.","Hidden Markov models,
Magnetic heads,
Solid modeling,
Predictive models,
Cognitive science,
Information analysis,
Magnetic analysis,
Performance analysis,
Magnetic sensors,
Target tracking"
Fitness-guided path exploration in dynamic symbolic execution,"Dynamic symbolic execution is a structural testing technique that systematically explores feasible paths of the program under test by running the program with different test inputs to improve code coverage. To address the space-explosion issue in path exploration, we propose a novel approach called Fitnex, a search strategy that uses state-dependent fitness values (computed through a fitness function) to guide path exploration. The fitness function measures how close an already discovered feasible path is to a particular test target (e.g., covering a not-yet-covered branch). Our new fitness-guided search strategy is integrated with other strategies that are effective for exploration problems where the fitness heuristic fails. We implemented the new approach in Pex, an automated structural testing tool developed at Microsoft Research. We evaluated our new approach by comparing it with existing search strategies. The empirical results show that our approach is effective since it consistently achieves high code coverage faster than existing search strategies.","System testing,
Software testing,
Automatic testing,
Computer science,
Particle measurements,
Concrete,
Performance evaluation,
Computer bugs,
Contracts,
Buildings"
"A comparative analysis of network dependability, fault-tolerance, reliability, security, and survivability","A number of qualitative and quantitative terms are used to describe the performance of what has come to be known as information systems, networks or infrastructures. However, some of these terms either have overlapping meanings or contain ambiguities in their definitions presenting problems to those who attempt a rigorous evaluation of the performance of such systems. The phenomenon arises because the wide range of disciplines covered by the term information technology have developed their own distinct terminologies. This paper presents a systematic approach for determining common and complementary characteristics of five widely-used concepts, dependability, fault-tolerance, reliability, security, and survivability. The approach consists of comparing definitions, attributes, and evaluation measures for each of the five concepts and developing corresponding relations. Removing redundancies and clarifying ambiguities will help the mapping of broad user-specified requirements into objective performance parameters for analyzing and designing information infrastructures.","Fault tolerance,
Information security,
Hardware,
Fault tolerant systems,
Terminology,
Information analysis,
Performance analysis,
Computer science,
Humans,
Reliability engineering"
The Legal Framework for Reproducible Scientific Research: Licensing and Copyright,"As computational researchers increasingly make their results available in a reproducible way, and often outside the traditional journal publishing mechanism, questions naturally arise with regard to copyright, subsequent use and citation, and ownership rights in general. The growing number of scientists who release their research publicly face a gap in the current licensing and copyright structure, particularly on the Internet. Scientific research produces more than the final paper: The code, data structures, experimental design and parameters, documentation, and figures are all important for scholarship communication and result replication. The author proposes the reproducible research standard for scientific researchers to use for all components of their scholarship that should encourage reproducible scientific investigation through attribution, facilitate greater collaboration, and promote engagement of the larger community in scientific learning and discovery.","Law,
Legal factors,
Licenses,
Scholarships,
Publishing,
Internet,
Data structures,
Design for experiments,
Documentation,
Collaboration"
Improving metadata management for small files in HDFS,"Scientific applications are adapting HDFS/MapReduce to perform large scale data analytics. One of the major challenges is that an overabundance of small files is common in these applications, and HDFS manages all its files through a single server, the Namenode. It is anticipated that small files can significantly impact the performance of Namenode. In this work we propose a mechanism to store small files in HDFS efficiently and improve the space utilization for metadata. Our scheme is based on the assumption that each client is assigned a quota in the file system, for both the space and number of files. In our approach, we utilize the compression method ‘harballing', provided by Hadoop, to better utilize the HDFS. We provide for new job functionality to allow for in-job archival of directories and files so that running MapReduce programs may complete without being killed by the JobTracker due to quota policies. This approach leads to better functionality of metadata operations and more efficient usage of the HDFS. Our analysis results show that we can reduce the metadata footprint in main memory by a factor of 42.",
Tour the world: Building a web-scale landmark recognition engine,"Modeling and recognizing landmarks at world-scale is a useful yet challenging task. There exists no readily available list of worldwide landmarks. Obtaining reliable visual models for each landmark can also pose problems, and efficiency is another challenge for such a large scale system. This paper leverages the vast amount of multimedia data on the Web, the availability of an Internet image search engine, and advances in object recognition and clustering techniques, to address these issues. First, a comprehensive list of landmarks is mined from two sources: (1) ~20 million GPS-tagged photos and (2) online tour guide Web pages. Candidate images for each landmark are then obtained from photo sharing Websites or by querying an image search engine. Second, landmark visual models are built by pruning candidate images using efficient image matching and unsupervised clustering techniques. Finally, the landmarks and their visual models are validated by checking authorship of their member images. The resulting landmark recognition engine incorporates 5312 landmarks from 1259 cities in 144 countries. The experiments demonstrate that the engine can deliver satisfactory recognition performance with high efficiency.","Search engines,
Internet,
Image recognition,
Large-scale systems,
Object recognition,
Image matching,
Cultural differences,
Data mining,
Reliability engineering,
Availability"
Computational Intelligence in Gait Research: A Perspective on Current Applications and Future Challenges,"Our mobility is an important daily requirement so much so that any disruption to it severely degrades our perceived quality of life. Studies in gait and human movement sciences, therefore, play a significant role in maintaining the well-being of our mobility. Current gait analysis involves numerous interdependent gait parameters that are difficult to adequately interpret due to the large volume of recorded data and lengthy assessment times in gait laboratories. A proposed solution to these problems is computational intelligence (CI), which is an emerging paradigm in biomedical engineering most notably in pathology detection and prosthesis design. The integration of CI technology in gait systems facilitates studies in disorders caused by lower limb defects, cerebral disorders, and aging effects by learning data relationships through a combination of signal processing and machine learning techniques. Learning paradigms, such as supervised learning, unsupervised learning, and fuzzy and evolutionary algorithms, provide advanced modeling capabilities for biomechanical systems that in the past have relied heavily on statistical analysis. CI offers the ability to investigate nonlinear data relationships, enhance data interpretation, design more efficient diagnostic methods, and extrapolate model functionality. These are envisioned to result in more cost-effective, efficient, and easy-to-use systems, which would address global shortages in medical personnel and rising medical costs. This paper surveys current signal processing and CI methodologies followed by gait applications ranging from normal gait studies and disorder detection to artificial gait simulation. We review recent systems focusing on the existing challenges and issues involved in making them successful. We also examine new research in sensor technologies for gait that could be combined with these intelligent systems to develop more effective healthcare solutions.",
Predicting Multiple Metrics for Queries: Better Decisions Enabled by Machine Learning,"One of the most challenging aspects of managing a very large data warehouse is identifying how queries will behave before they start executing. Yet knowing their performance characteristics --- their runtimes and resource usage --- can solve two important problems. First, every database vendor struggles with managing unexpectedly long-running queries. When these long-running queries can be identified before they start, they can be rejected or scheduled when they will not cause extreme resource contention for the other queries in the system. Second, deciding whether a system can complete a given workload in a given time period (or a bigger system is necessary) depends on knowing the resource requirements of the queries in that workload. We have developed a system that uses machine learning to accurately predict the performance metrics of database queries whose execution times range from milliseconds to hours. For training and testing our system, we used both real customer queries and queries generated from an extended set of TPC-DS templates. The extensions mimic queries that caused customer problems. We used these queries to compare how accurately different techniques predict metrics such as elapsed time, records used, disk I/Os, and message bytes. The most promising technique was not only the most accurate, but also predicted these metrics simultaneously and using only information available prior to query execution. We validated the accuracy of this machine learning technique on a number of HP Neoview configurations. We were able to predict individual query elapsed time within 20% of its actual time for 85% of the test queries. Most importantly, we were able to correctly identify both the short and long-running (up to two hour) queries to inform workload management and capacity planning.","Machine learning,
Capacity planning,
USA Councils,
Data warehouses,
Databases,
Measurement,
System testing,
Data engineering,
Computer science,
Milling machines"
Flexible design of cognitive radio wireless systems,"The goal of this article is to show how many challenging unsolved resource allocation problems in the emerging field of cognitive radio (CR) networks fit naturally either in the game theoretical paradigm or in the more general theory of VI. This provides us with all the mathematical tools necessary to analyze the proposed equilibrium problems for CR systems (e.g., existence and uniqueness of the solution) and to devise distributed algorithms along with their convergence properties.","Cognitive radio,
Game theory,
Chromium,
Resource management,
FCC,
Quality of service,
Multidimensional signal processing,
Frequency,
Interference,
Signal processing algorithms"
High-Throughput Automated Injection of Individual Biological Cells,"The ability of efficiently delivering soluable/insoluable drug compounds or biomolecules into individual biological cells and quantifying their cellular responses is important for genetics, proteomics, and drug discovery. This paper presents a fully automated system for zebrafish embryo injection, which overcomes the problems inherent in manual injection, such as human fatigue and large variations in success rates due to poor reproducibility. Based on ldquolooking-then-movingrdquo control, the microrobotic system performs injection at a speed of 15 zebrafish embryos (chorion unremoved) per minute. Besides a high injection speed that compares favorably with that of a highly proficient injection technician, a vacuum-based embryo holding device enables fast immobilization of a large number of zebrafish embryos, shortening the embryo patterning process from minutes to seconds. The recognition of embryo structures from image processing identifies a desired destination inside the embryo for material deposition, together with precise motion control resulting in a success rate of 100%. Carefully tuning suction pressure levels as well as injection and retraction speeds produced a high survival rate of 98%. The quantitative performance evaluation of the automated system was based on the continuous injection of 250 zebrafish embryos. The technologies can be extended to other biological injection applications such as the injection of mouse embryos, Drosophila embryos, and C. elegans to enable high-throughput biological and pharmaceutical research.",
Semi-partitioned Scheduling of Sporadic Task Systems on Multiprocessors,"This paper presents a new algorithm for scheduling of sporadic task systems with arbitrary deadlines on identical multiprocessor platforms. The algorithm is based on the concept of semi-partitioned scheduling, in which most tasks are fixed to specific processors, while a few tasks migrate across processors. Particularly, we design the algorithm so that tasks are qualified to migrate only if a task set cannot be partitioned any more, and such migratory tasks migrate from one processor to another processor only once in each period. The scheduling policy is then subject to Earliest Deadline First. Simulation results show that the algorithm delivers competitive scheduling performance to the state-of-the-art, with a smaller number of context switches.",
The evolution and impact of code smells: A case study of two open source systems,"Code smells are design flaws in object-oriented designs that may lead to maintainability issues in the further evolution of the software system. This study focuses on the evolution of code smells within a system and their impact on the change behavior (change frequency and size). The study investigates two code smells, God Class and Shotgun Surgery, by analyzing the historical data over several years of development of two large scale open source systems. The detection of code smells in the evolution of those systems was performed by the application of an automated approach using detection strategies. The results show that we can identify different phases in the evolution of code smells during the system development and that code smell infected components exhibit a different change behavior. This information is useful for the identification of risk areas within a software system that need refactoring to assure a future positive evolution.",
Performance Bounds for Two-Way Amplify-and-Forward Relaying Based on Relay Path Selection,"In this paper, we consider two-way amplify-and-forward relaying communication over a multiple relays network in which the best relay node is selected among others. Our proposed scheme picks for duplex communication between the source and the destination only the relay which provides the best end-to-end performance based on the instantaneous channel information. Similar to the relay selection method proposed, our scheme also does not require any explicit communications among the relays. Based on the proposed relay selection criterion, we offer the performance bounds on the average sum-rate, average symbol error rate, outage probability over identically, independent distributed Rayleigh fading channels. Further, our numerical examples show that our proposed scheme provides considerable sum-rate gains and offers an increasing a diversity order as the number of relay candidates increases.","Relays,
Protocols,
Electronic mail,
Error analysis,
Fading,
Protective relaying,
Computer networks,
Computer science education,
Educational programs,
Cities and towns"
Performance analysis of RFID Generation-2 protocol,"This paper investigates the performance of EPC-gloabl Generation-2 (Gen-2) ultra high frequency (UHF) radio frequency identification (RFID) protocol. Gen-2 protocol has a critical parameter Q that plays an important role in resolving tag collisions. Gen-2 protocol proposes an adaptive slot-count algorithm to adjust Q dynamically based on the type of replies from tags. In this paper, we define two performance metrics for tag identification: Query Success Rate (QSR) and tag identification speed (TIS). We establish a Discrete-Time Markov Chain (DTMC) model for the Gen-2 and accordingly obtain quantitative results of QSR and TIS. Extensive simulations validate our theoretical analysis and demonstrate that the number of tags has little impact on the performance. In other words, QSR and TIS do not nearly decrease even though the number of tags may be increasing. Our model for Gen-2 protocol is also useful to study the performance of other RFID protocols.","Performance analysis,
Radiofrequency identification,
Protocols,
Frequency,
RFID tags,
Bit error rate,
Measurement,
Analytical models,
Radio communication,
Databases"
Ultrasonic guided-waves characterization with warped frequency transforms,"Guided wave (GW) dispersion curves can be extracted from a time-transient measurement by means of time-frequency representations (TFRs). Unfortunately, any TFR is subject to the time-frequency uncertainty principle. This, in general, limits the capability of TFRs to characterize closely spaced guided modes over a wide frequency range. To overcome this limitation, we implemented a new warped frequency transform that presents enhanced mode extraction capabilities because of a more flexible tiling of the time-frequency domain. The tiling is designed to match the dispersive spectro-temporal structure of a GW by selecting an appropriate map of the time-frequency plane. The proposed transformation is fast, invertible, and covariant to group delay shifts. An application to Lamb waves propagating in an aluminum plate is presented. Time-transient GWs propagation events obtained both numerically and experimentally are considered. The results show that the proposed warped frequency transform limits the interference patterns which appear with other TFRs and produces a sparse representation of the Lamb wave pattern that can be suitable for identification and characterization purposes.","Time frequency analysis,
Dispersion,
Ultrasonic variables measurement,
Delay,
Aluminum,
Interference"
Clustered Synopsis of Surveillance Video,"Millions of surveillance cameras record video around the clock, producing huge video archives. Even when a video archive is known to include critical activities, finding them is like finding a needle in a haystack, making the archive almost worthless. Two main approaches were proposed to address this problem: action recognition and video summarization. Methods for automatic detection of activities still face problems in many scenarios. The video synopsis approach to video summarization is very effective, but may produce confusing summaries by the simultaneous display of multiple activities.A new methodology for the generation of short and coherent video summaries is presented, based on clustering of similar activities. Objects with similar activities are easy to watch simultaneously, and outliers can be spotted instantly. Clustered synopsis is also suitable for efficient creation of ground truth data.",
Scaling Genetic Algorithms Using MapReduce,"Genetic algorithms(GAs) are increasingly being applied to large scale problems. The traditional MPI-based parallel GAs require detailed knowledge about machine architecture. On the other hand, MapReduce is a powerful abstraction proposed by Google for making scalable and fault tolerant applications. In this paper, we show how genetic algorithms can be modeled into the MapReduce model. We describe the algorithm design and implementation of GAs on Hadoop, an open source implementation of MapReduce. Our experiments demonstrate the convergence and scalability up to 10^5 variable problems. Adding more resources would enable us to solve even larger problems without any changes in the algorithms and implementation since we do not introduce any performance bottlenecks.",
Multiple Human Tracking and Identification With Wireless Distributed Pyroelectric Sensor Systems,"This paper presents a wireless distributed pyroelectric sensor system for tracking and identifying multiple humans based on their body heat radiation. This study aims to make pyroelectric sensors a low-cost alternative to infrared video sensors in thermal gait biometric applications. In this system, the sensor field of view (FOV) is specifically modulated with Fresnel lens arrays for functionality of tracking or identification, and the sensor deployment is chosen to facilitate the process of data-object-association. An Expectation-Maximization-Bayesian tracking scheme is proposed and implemented among slave, master, and host modules of a prototype system. Information fusion schemes are developed to improve the system identification performance for both individuals and multiple subjects. The fusion of thermal gait biometric information measured by multiple nodes is tested at four levels: sample, feature, score, and decision. Experimentally, the prototype system is able to simultaneously track two individuals in both follow-up and crossover scenarios with average tracking errors less than 0.5 m. The experimental results also demonstrate system's potential to be a reliable biometric system for the verification/identification of a small group of human subjects. The developed wireless distributed infrared sensor system can run as a standalone prisoner/patient monitoring system under any illumination conditions, as well as a complement for conventional video and audio human tracking and identification systems.","Humans,
Wireless sensor networks,
Pyroelectricity,
Sensor systems,
Biosensors,
Infrared sensors,
Sensor arrays,
Biometrics,
Thermal sensors,
Prototypes"
What are good apertures for defocus deblurring?,"In recent years, with camera pixels shrinking in size, images are more likely to include defocused regions. In order to recover scene details from defocused regions, deblurring techniques must be applied. It is well known that the quality of a deblurred image is closely related to the defocus kernel, which is determined by the pattern of the aperture. The design of aperture patterns has been studied for decades in several fields, including optics, astronomy, computer vision, and computer graphics. However, previous attempts at designing apertures have been based on intuitive criteria related to the shape of the power spectrum of the aperture pattern. In this paper, we present a comprehensive framework for evaluating an aperture pattern based on the quality of deblurring. Our criterion explicitly accounts for the effects of image noise and the statistics of natural images. Based on our criterion, we have developed a genetic algorithm that converges very quickly to near-optimal aperture patterns. We have conducted extensive simulations and experiments to compare our apertures with previously proposed ones.","Apertures,
Noise,
Optimization,
Equations,
Lenses,
Mathematical model,
Deconvolution"
Receding horizon temporal logic planning for dynamical systems,"This paper bridges the advances in computer science and control to allow automatic synthesis of control strategies for complex dynamical systems which are guaranteed, by construction, to satisfy the desired properties even in the presence of adversary. The desired properties are expressed in the language of temporal logic. With its expressive power, a wider class of properties than safety and stability can be specified. The resulting system consists of a discrete planner that plans, in the abstracted discrete domain, a set of transitions of the system to ensure the correct behaviors and a continuous controller that continuously implements the plan. To address the computational difficulties in the synthesis of a discrete planner, we present a receding horizon based scheme for executing finite state automata that essentially reduces the synthesis problem to a set of smaller problems.",
Graph Model Based Indoor Tracking,"The tracking of the locations of moving objects in large indoor spaces is important, as it enables a range of applications related to, e.g., security and indoor navigation and guidance. This paper presents a graph model based approach to indoor tracking that offers a uniform data management infrastructure for different symbolic positioning technologies, e.g., Bluetooth and RFID. More specifically, the paper proposes a model of indoor space that comprises a base graph and mappings that represent the topology of indoor space at different levels. The resulting model can be used for one or several indoor positioning technologies. Focusing on RFID-based positioning, an RFID specific reader deployment graph model is built from the base graph model. This model is then used in several algorithms for constructing and refining trajectories from raw RFID readings. Empirical studies with implementations of the models and algorithms suggest that the paper's proposals are effective and efficient.","Space technology,
Radiofrequency identification,
Partitioning algorithms,
Data structures,
Computer science,
Navigation,
Technology management,
Bluetooth,
Topology,
Mobile computing"
Quantitative ultrasound backscatter for pulsed cavitational ultrasound therapy-histotripsy,"Histotripsy is a well-controlled ultrasonic tissue ablation technology that mechanically and progressively fractionates tissue structures using cavitation. The fractionated tissue volume can be monitored with ultrasound imaging because a significant ultrasound backscatter reduction occurs. This paper correlates the ultrasound backscatter reduction with the degree of tissue fractionation characterized by the percentage of remaining normal-appearing cell nuclei on histology. Different degrees of tissue fractionation were generated in vitro in freshly excised porcine kidneys by varying the number of therapeutic ultrasound pulses from 100 to 2000 pulses per treatment location. All ultrasound pulses were 15 cycles at 1 MHz delivered at 100 Hz pulse repetition frequency and 19 MPa peak negative pressure. The results showed that the normalized backscatter intensity decreased exponentially with increasing number of pulses. Correspondingly, the percentage of normal appearing nuclei in the treated area decreased exponentially as well. A linear correlation existed between the normalized backscatter intensity and the percentage of normal appearing cell nuclei in the treated region. This suggests that the normalized backscatter intensity may be a potential quantitative real-time feedback parameter for histotripsy-induced tissue fractionation. This quantitative feedback may allow the prediction of local clinical outcomes, i.e., when a tissue volume has been sufficiently treated.",
Inferring Resource Specifications from Natural Language API Documentation,"Typically, software libraries provide API documentation, through which developers can learn how to use libraries correctly. However, developers may still write code inconsistent with API documentation and thus introduce bugs, as existing research shows that many developers are reluctant to carefully read API documentation. To find those bugs, researchers have proposed various detection approaches based on known specifications. To mine specifications, many approaches have been proposed, and most of them rely on existing client code. Consequently, these mining approaches would fail to mine specifications when client code is not available. In this paper, we propose an approach, called Doc2Spec, that infers resource specifications from API documentation. For our approach, we implemented a tool and conducted an evaluation on Javadocs of five libraries. The results show that our approach infers various specifications with relatively high precisions, recalls, and F-scores. We further evaluated the usefulness of inferred specifications through detecting bugs in open source projects. The results show that specifications inferred by Doc2Spec are useful to detect real bugs in existing projects.",
Promises and Challenges of Ambient Assisted Living Systems,"The population of elderly people keeps increasing rapidly, which becomes a predominant aspect of our societies. As such, solutions both efficacious and cost-effective need to be sought. Ambient Assisted Living (AAL) is a new approach which promises to address the needs from elderly people. Ambient Intelligence technologies are widely developed in this domain aiming to construct safe environments around assisted peoples and help them maintain independent living. However, there are still many fundamental issues in AAL that remain open. Most of the current efforts still do not fully express the power of human being, and the importance of social connections and social activities is less noticed. Our conjecture is that such features are fundamental prerequisites towards truly effective AAL services. This paper reviews the current status of researches on AAL, discusses the promises and possible advantages of AAL, and also indicates the challenges we must meet in order to develop practical and efficient AAL systems for elderly people. In this paper, we also propose an approach to construct effective home-care system for the elderly people.",
Multi-channel opportunistic access: A case of restless bandits with multiple plays,"This paper considers the following stochastic control problem that arises in opportunistic spectrum access: a system consists of n channels where the state (“good” or “bad”) of each channel evolves as independent and identically distributed Markov processes. A user can select exactly k channels to sense and access (based on the sensing result) in each time slot. A reward is obtained whenever the user senses and accesses a “good” channel. The objective is to design a channel selection policy that maximizes the expected discounted total reward accrued over a finite or infinite horizon. In our previous work we established the optimality of a greedy policy for the special case of k=1 (i.e., single channel access) under the condition that the channel state transitions are positively correlated over time. In this paper we show under the same condition the greedy policy is optimal for the general case of k≥1; the methodology introduced here is thus more general. This problem may be viewed as a special case of the restless bandit problem, with multiple plays. We discuss connections between the current problem and existing literature on this class of problems.","Markov processes,
Infinite horizon,
Stochastic systems,
Control systems,
Stochastic processes,
Probability distribution,
Computer science,
Application software,
Fading,
Interference"
Region-based connectivity - a new paradigm for design of fault-tolerant networks,"The studies in fault-tolerance in networks mostly focus on the connectivity of the graph as the metric of faulttolerance. If the underlying graph is k-connected, it can tolerate up to k — 1 failures. In measuring the fault tolerance in terms of connectivity, no assumption regarding the locations of the faulty nodes are made - the failed nodes may be close to each other or far from each other. In other words, the connectivity metric has no way of capturing the notion of locality of faults. However in many networks, faults may be highly localized. This is particularly true in military networks, where an enemy bomb may inflict massive but localized damage to the network. To capture the notion of locality of faults in a network, a new metric region-based connectivity (RBC) was introduced in [1]. It was shown that RBC can achieve the same level of fault-tolerance as the metric connectivity, with much lower networking resources. The study in [1] was restricted to single region fault model (SRFM), where faults are confined to one region only. In this paper, we extend the notion of RBC to multiple region fault model (MRFM), where faults are no longer confined to a single region. As faults in MRFM are still confined to regions, albeit multiple of them, it is different from unconstrained fault model where no constraint on locality of faults is imposed. The MRFM leads to several new concepts, such as region-disjoint paths and region cuts. We show that the classical result, the maximum number of node-disjoint paths between a pair of nodes is equal to the minimum number of nodes whose removal disconnects the pair, is no longer valid when region-disjoint paths and region cuts are considered. We prove that the problems of finding (i) the maximum number of region-disjoint paths between a pair of nodes, and (ii) minimum number of regions whose removal disconnect a pair of nodes, are both NP-complete. We provide heuristic solution to these two problems and evaluate their efficacy by comparing the results with optimal solutions.","Fault tolerance,
Weapons,
Computer science,
Design engineering,
Wireless sensor networks,
US Government,
Vehicles"
Folding@home: Lessons from eight years of volunteer distributed computing,"Accurate simulation of biophysical processes requires vast computing resources. Folding@home is a distributed computing system first released in 2000 to provide such resources needed to simulate protein folding and other biomolecular phenomena. Now operating in the range of 5 PetaFLOPS sustained, it provides more computing power than can typically be gathered and operated locally due to cost, physical space, and electrical/cooling load. This paper describes the architecture and operation of Folding@home, along with some lessons learned over the lifetime of the project.","Distributed computing,
Proteins,
Computational modeling,
Internet,
Biological system modeling,
Computer simulation,
Computer architecture,
Recruitment,
Cryptography,
Biology computing"
Configurable Composition and Adaptive Provisioning of Web Services,"Web services composition has been an active research area over the last few years. However, the technology is still not mature yet and several research issues need to be addressed. In this paper, we describe the design of CCAP, a system that provides tools for adaptive service composition and provisioning. We introduce a composition model where service context and exceptions are configurable to accommodate needs of different users. This allows for reusability of a service in different contexts and achieves a level of adaptiveness and contextualization without recoding and recompiling of the overall composed services. The execution semantics of the adaptive composite service is provided by an event-driven model. This execution model is based on Linda Tuple Spaces and supports real-time and asynchronous communication between services. Three core services, coordination service, context service, and event service, are implemented to automatically schedule and execute the component services, and adapt to user configured exceptions and contexts at run time. The proposed system provides an efficient and flexible support for specifying, deploying, and accessing adaptive composite services. We demonstrate the benefits of our system by conducting usability and performance studies.",
Amplify-and-forward based cooperation for secure wireless communications,"A physical layer approach to security for wireless networks is considered. In single-antenna wireless systems, such approaches are hampered by channel conditions in the presence of one or more eavesdroppers. Cooperation has the potential to overcome this problem and improve the security of of wireless communications. In this paper, an amplify-and-forward based cooperative protocol is proposed. Assuming availability of global channel state information, system design that maximizes the secrecy capacity is considered. Since the optimal solution to this problem is intractable, suboptimal closed-form solutions are proposed that optimize bounds on secrecy capacity for the case of a single eavesdropper, or that introduce additional constraints, such as nulling of signals at all eavesdroppers, for the case of multiple eavesdroppers.","Wireless communication,
Communication system security,
Relays,
Protocols,
Decoding,
Physical layer,
Information security,
Costs,
Power system security,
Broadcasting"
The Graph Neural Network Model,"Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function tau(G,n) isin IRm that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities.","Neural networks,
Biological system modeling,
Data engineering,
Computer vision,
Chemistry,
Biology,
Pattern recognition,
Data mining,
Supervised learning,
Parameter estimation"
OneClick: A Framework for Measuring Network Quality of Experience,"As the service requirements of network applications shift from high throughput to high media quality, interactivity, and responsiveness, the definition of QoE (Quality of Experience) has become multidimensional. Although it may not be difficult to measure individual dimensions of the QoE, how to capture users' overall perceptions when they are using network applications remains an open question. In this paper, we propose a framework called OneClick to capture users' perceptions when they are using network applications. The framework only requires a subject to click a dedicated key whenever he/she feels dissatisfied with the quality of the application in use. OneClick is particularly effective because it is intuitive, lightweight, efficient, time-aware, and application-independent. We use two objective quality assessment methods, PESQ and VQM, to validate OneClick's ability to evaluate the quality of audio and video clips. To demonstrate the proposed framework's efficiency and effectiveness in assessing user experiences, we implement it on two applications, one for instant messaging applications, and the other for first- person shooter games. A Flash implementation of the proposed framework is also presented.",
Hyperbolic Embedding and Routing for Dynamic Graphs,"We propose an embedding and routing scheme for arbitrary network connectivity graphs, based on greedy routing and utilizing virtual node coordinates. In dynamic multihop packet-switching communication networks, routing elements can join or leave during network operation or exhibit intermittent failures. We present an algorithm for online greedy graph embedding in the hyperbolic plane that enables incremental embedding of network nodes as they join the network, without disturbing the global embedding. Even a single link or node removal may invalidate the greedy routing success guarantees in network embeddings based on an embedded spanning tree subgraph. As an alternative to frequent reembedding of temporally dynamic network graphs in order to retain the greedy embedding property, we propose a simple but robust generalization of greedy distance routing called Gravity-Pressure (GP) routing. Our routing method always succeeds in finding a route to the destination provided that a path exists, even if a significant fraction of links or nodes is removed subsequent to the embedding. GP routing does not require precomputation or maintenance of special spanning subgraphs and, as demonstrated by our numerical evaluation, is particularly suitable for operation in tandem with our proposed algorithm for online graph embedding.","Routing,
Tree graphs,
Peer to peer computing,
Computer science,
Communication networks,
Extraterrestrial measurements,
Communications Society,
Spread spectrum communication,
Robustness,
Internet"
Transactions papers a routing-driven Elliptic Curve Cryptography based key management scheme for Heterogeneous Sensor Networks,"Previous research on sensor network security mainly considers homogeneous sensor networks, where all sensor nodes have the same capabilities. Research has shown that homogeneous ad hoc networks have poor performance and scalability. The many-to-one traffic pattern dominates in sensor networks, and hence a sensor may only communicate with a small portion of its neighbors. Key management is a fundamental security operation. Most existing key management schemes try to establish shared keys for all pairs of neighbor sensors, no matter whether these nodes communicate with each other or not, and this causes large overhead. In this paper, we adopt a Heterogeneous Sensor Network (HSN) model for better performance and security. We propose a novel routing-driven key management scheme, which only establishes shared keys for neighbor sensors that communicate with each other. We utilize Elliptic Curve Cryptography in the design of an efficient key management scheme for sensor nodes. The performance evaluation and security analysis show that our key management scheme can provide better security with significant reductions on communication overhead, storage space and energy consumption than other key management schemes.","Elliptic curve cryptography,
Energy management,
Ad hoc networks,
Scalability,
Telecommunication traffic,
Performance analysis,
Security,
Energy storage,
Secure storage,
Energy consumption"
Defining Insight for Visual Analytics,"Many have argued that providing insight is the main goal of information visualization. Stuart Card, Jock Mackinlay, and Ben Shneiderman declare that ""the purpose of visualization is insight, while Jim Thomas and Kris Cook propose in Illuminating the Path that the purpose of visual analytics is to enable and discover insight. The idea that visualization should lead to insight seems logical, but researchers in the community have been slow to build on the concept because insight is difficult to define. As Ji Soo Yi and his colleagues point out, although a few definitions of insight exist, no commonly accepted definition has emerged in the community.",
Selection and Orientation of Directional Sensors for Coverage Maximization,"Sensor nodes may be equipped with a ""directional"" sensing device (such as a camera) which senses a physical phenomenon in a certain direction depending on the chosen orientation. In this article, we address the problem of selection and orientation of such directional sensors with the objective of maximizing coverage area. Prior works on sensor coverage have largely focused on coverage with sensors that are associated with a unique sensing region. In contrast, directional sensors have multiple sensing regions associated with them, and the orientation of the sensor determines the actual sensing region. Thus, the coverage problems in the context of directional sensors entails selection as well as orientation of sensors needed to activate in order to maximize/ensure coverage. In this article, we address the problem of selecting a minimum number of sensors and assigning orientations such that the given area (or set of target points) is k-covered (i.e., each point is covered k times). The above problem is NP-complete, and even NP-hard to approximate. Thus, we design a simple greedy algorithm that delivers a solution that k-covers at least half of the target points using at most M log(k|C|) sensors, where |C| is the maximum number of target points covered by a sensor and M is the minimum number of sensor required to k-cover all the given points. The above result holds for almost arbitrary sensing regions. We design a distributed implementation of the above algorithm, and study its performance through simulations. In addition to the above problem, we also look at other related coverage problems in the context of directional sensors, and design similar approximation algorithms for them.",
Janus: An FPGA-Based System for High-Performance Scientific Computing,"Janus is a modular, massively parallel, and reconfigurable FPGA-based computing system. Each Janus module has one computational core and one host. Janus is tailored to, but not limited to, the needs of a class of hard scientific applications characterized by regular code structure, unconventional data-manipulation requirements, and a few Megabits database. The authors discuss this configurable system's architecture and focus on its use for Monte Carlo simulations of statistical mechanics, as Janus performs impressively on this class of application.",
Variational Bayesian Sparse Kernel-Based Blind Image Deconvolution With Student's-t Priors,"In this paper, we present a new Bayesian model for the blind image deconvolution (BID) problem. The main novelty of this model is the use of a sparse kernel-based model for the point spread function (PSF) that allows estimation of both PSF shape and support. In the herein proposed approach, a robust model of the BID errors and an image prior that preserves edges of the reconstructed image are also used. Sparseness, robustness, and preservation of edges are achieved by using priors that are based on the Student's-t probability density function (PDF). This pdf, in addition to having heavy tails, is closely related to the Gaussian and, thus, yields tractable inference algorithms. The approximate variational inference methodology is used to solve the corresponding Bayesian model. Numerical experiments are presented that compare this BID methodology to previous ones using both simulated and real data.","Bayesian methods,
Deconvolution,
TV,
Robustness,
Inference algorithms,
Inverse problems,
Computer science education,
Educational programs,
Shape,
Image reconstruction"
TP-CRAHN: a Transport Protocol for Cognitive Radio Ad-Hoc Networks,"Existing research in transport protocols for wireless ad-hoc networks has focused on reliable end-to-end packet delivery under uncertain channel conditions, route failures due to node mobility and link congestion. In a cognitive radio (CR) environment, there are several key challenges that must be addressed apart from the above concerns. The intermittent spectrum sensing undertaken by the CR users, the activity of the licensed users of the spectrum, large-scale bandwidth variation based on spectrum availability, and the channel switching process need to be considered in the transport protocol design. In this paper, a window-based transport protocol for CR ad-hoc networks, TP-CRAHN, is proposed that distinguishes each of these events by a combination of explicit feedback from the intermediate nodes and the destination. This is achieved by adapting the classical TCP rate control algorithm running at the source to closely interact with the physical layer channel information, the link layer functions of spectrum sensing and buffer management, and a predictive mobility framework that is developed at the network layer. To the best of our knowledge, this is the first work on the transport layer to specifically address the concerns of the CR ad-hoc networks and our approach is thoroughly validated by simulation experiments.","Transport protocols,
Cognitive radio,
Ad hoc networks,
Chromium,
Wireless sensor networks,
USA Councils,
Peer to peer computing,
Communications Society,
Computer science,
Computer network reliability"
The HFB Face Database for Heterogeneous Face Biometrics research,"A face database, composed of visual (VIS), near infrared (NIR) and three-dimensional (3D) face images, is collected. Called the HFB face database, it is released now to promote research and development of heterogeneous face biometrics (HFB). This release of version 1 contains a total of 992 images from 100 subjects; there are 4 VIS, 4 NIR, and 1 or 2 3D face images per subject. In this paper, we describe the apparatuses, environments and procedure of the data collection and present baseline performances of the standard PCA and LDA methods on the database.","Biometrics,
Image databases,
Visual databases,
Linear discriminant analysis,
Infrared imaging,
Principal component analysis,
Face recognition,
Image matching,
Research and development,
Infrared spectra"
(Meta) Kernelization,"Polynomial time preprocessing to reduce instance size is one of the most commonly deployed heuristics to tackle computationally hard problems. In a parameterized problem, every instance I comes with a positive integer k. The problem is said to admit a polynomial kernel if, in polynomial time, we can reduce the size of the instance I to a polynomial in k, while preserving the answer. In this paper, we show that all problems expressible in Counting Monadic Second Order Logic and satisfying a compactness property admit a polynomial kernel on graphs of bounded genus. Our second result is that all problems that have finite integer index and satisfy a weaker compactness condition admit a linear kernel on graphs of bounded genus. The study of kernels on planar graphs was initiated by a seminal paper of Alber, Fellows, and Niedermeier [J. ACM, 2004 ] who showed that Planar Dominating Set admits a linear kernel. Following this result, a multitude of problems have been shown to admit linear kernels on planar graphs by combining the ideas of Alber et al. with problem specific reduction rules. Our theorems unify and extend all previously known kernelization results for planar graph problems. Combining our theorems with the Erdos-Posa property we obtain various new results on linear kernels for a number of packing and covering problems.","Kernel,
Polynomials,
Computer science,
Informatics,
Mathematics,
Logic,
History,
Mathematical analysis,
NP-hard problem,
Councils"
Tracking a hand manipulating an object,"We present a method for tracking a hand while it is interacting with an object. This setting is arguably the one where hand-tracking has most practical relevance, but poses significant additional challenges: strong occlusions by the object as well as self-occlusions are the norm, and classical anatomical constraints need to be softened due to the external forces between hand and object. To achieve robustness to partial occlusions, we use an individual local tracker for each segment of the articulated structure. The segments are connected in a pairwise Markov random field, which enforces the anatomical hand structure through soft constraints on the joints between adjacent segments. The most likely hand configuration is found with belief propagation. Both range and color data are used as input. Experiments are presented for synthetic data with ground truth and for real data of people manipulating objects.","Fingers,
Computer vision,
Robustness,
Markov random fields,
Joints,
Belief propagation,
Application software,
Laboratories,
Computer science,
Tracking"
Tracking with Unreliable Node Sequences,"Tracking mobile targets using sensor networks is a challenging task because of the impacts of in-the-flled factors such as environment noise, sensing irregularity and etc. This paper proposes a robust tracking framework using node sequences, an ordered list extracted from unreliable sensor readings. Instead of estimating each position point separately in a movement trace, we convert the original tracking problem to the problem of finding the shortest path in a graph, which is equivalent to optimal matching of a series of node sequences. In addition to the basic design, multidimensional smoothing is developed to enhance tracking accuracy. Practical system deployment related issues are discussed in the paper, and the design is evaluated with both simulation and a system implementation using Pioneer III Robot and MICAz sensor nodes. In fact, tracking with node sequences provides a useful layer of abstraction, making the design framework generic and compatible with different physical sensing modalities.","Target tracking,
Peer to peer computing,
Wireless sensor networks,
Working environment noise,
Surveillance,
Monitoring,
Communications Society,
Helium,
Computer science,
USA Councils"
Small-file access in parallel file systems,"Today's computational science demands have resulted in ever larger parallel computers, and storage systems have grown to match these demands. Parallel file systems used in this environment are increasingly specialized to extract the highest possible performance for large I/O operations, at the expense of other potential workloads. While some applications have adapted to I/O best practices and can obtain good performance on these systems, the natural I/O patterns of many applications result in generation of many small files. These applications are not well served by current parallel file systems at very large scale. This paper describes five techniques for optimizing small-file access in parallel file systems for very large scale systems. These five techniques are all implemented in a single parallel file system (PVFS) and then systematically assessed on two test platforms. A microbenchmark and the mdtest benchmark are used to evaluate the optimizations at an unprecedented scale. We observe as much as a 905% improvement in small-file create rates, 1,106% improvement in small-file stat rates, and 727% improvement in small-file removal rates, compared to a baseline PVFS configuration on a leadership computing platform using 16,384 cores.","File systems,
Concurrent computing,
Application software,
Computer science,
Laboratories,
Large-scale systems,
System testing,
Mathematics,
Best practices,
Benchmark testing"
SIMPS: Using Sociology for Personal Mobility,"Assessing mobility in a thorough fashion is a crucial step toward more efficient mobile network design. Recent research on mobility has focused on two main points: analyzing models and studying their impact on data transport. These works investigate the consequences of mobility. In this paper, instead, we focus on the causes of mobility. Starting from established research in sociology, we propose SIMPS, a mobility model of human crowds with pedestrian motion. This model defines a process called sociostation, rendered by two complimentary behaviors, namely socialize and isolate, that regulate an individual with regard to her/his own sociability level. SIMPS leads to results that agree with scaling laws observed both in small-scale and large-scale human motion. Although our model defines only two simple individual behaviors, we observe many emerging collective behaviors (group formation/splitting, path formation, and evolution).","Sociology,
Humans,
Biological system modeling,
Proposals,
Evolution (biology),
Communication systems,
Contracts,
Laboratories,
Associate members,
Large-scale systems"
Stacks of convolutional Restricted Boltzmann Machines for shift-invariant feature learning,"In this paper we present a method for learning class-specific features for recognition. Recently a greedy layer-wise procedure was proposed to initialize weights of deep belief networks, by viewing each layer as a separate restricted Boltzmann machine (RBM). We develop the convolutional RBM (C-RBM), a variant of the RBM model in which weights are shared to respect the spatial structure of images. This framework learns a set of features that can generate the images of a specific object class. Our feature extraction model is a four layer hierarchy of alternating filtering and maximum subsampling. We learn feature parameters of the first and third layers viewing them as separate C-RBMs. The outputs of our feature extraction hierarchy are then fed as input to a discriminative classifier. It is experimentally demonstrated that the extracted features are effective for object detection, using them to obtain performance comparable to the state of the art on handwritten digit recognition and pedestrian detection.","Machine learning,
Feature extraction,
Computer vision,
Cellular neural networks,
Object detection,
Object recognition,
Neural networks,
Nonhomogeneous media,
Filters,
Large-scale systems"
Evolving content in the Galactic Arms Race video game,"Video game content includes the levels, models, items, weapons, and other objects encountered and wielded by players during the game. In most modern video games, the set of content shipped with the game is static and unchanging, or at best, randomized within a narrow set of parameters. However, ideally, if game content could be constantly renewed, players would remain engaged longer in the evolving stream of novel content. To realize this ambition, this paper introduces the content-generating NeuroEvolution of Augmenting Topologies (cgNEAT) algorithm, which automatically evolves game content based on player preferences, as the game is played. To demonstrate this approach, the Galactic Arms Race (GAR) video game is also introduced. In GAR, players pilot space ships and fight enemies to acquire unique particle system weapons that are evolved by the game. As shown in this paper, players can discover a wide variety of content that is not only novel, but also based on and extended from previous content that they preferred in the past. The implication is that it is now possible to create games that generate their own content to satisfy players, potentially significantly reducing the cost of content creation and increasing the replay value of games.","Arm,
Games,
Weapons,
Machine learning,
Topology,
Costs,
Streaming media,
Marine vehicles,
Evolutionary computation,
Artificial neural networks"
A statistical inter-cell interference model for downlink cellular OFDMA networks under log-normal shadowing and multipath Rayleigh fading,"The probability density function (PDF) of the intercell interference for downlink cellular orthogonal frequency division multiple access networks under log-normal shadowing and multipath Rayleigh fading is derived. The derived PDF is observed to highly deviate from the Gaussian distribution, especially at moderate to small network loads. Based on the derived inter-cell interference statistics, the decoder performance can be significantly improved compared to the case when the traditional decoder optimized for the AWGN channel is employed. A simple suboptimum decoding algorithm that closely approximates the derived optimum decoder performance is also proposed.","Interference,
Downlink,
Cellular networks,
Shadow mapping,
Rayleigh channels,
Decoding,
Probability density function,
Frequency conversion,
Gaussian distribution,
Statistical distributions"
User identification across multiple social networks,"Today, more and more people have their virtual identities on the web. It is common that people are users of more than one social network and also their friends may be registered on multiple websites. A facility to aggregate our online friends into a single integrated environment would enable the user to keep up-to-date with their virtual contacts more easily, as well as to provide improved facility to search for people across different websites. In this paper, we propose a method to identify users based on profile matching. We use data from two popular social networks to study the similarity of profile definition. We evaluate the importance of fields in the web profile and develop a profile comparison tool. We demonstrate the effectiveness and efficiency of our tool in identifying and consolidating duplicated users on different websites.","Training,
Testing,
Facebook,
Electronic mail,
Manganese,
Data mining"
Year,,
An adaptive energy efficient MAC protocol for the medical body area network,"Medical body area networks will employ both implantable and bodyworn devices to support a diverse range of applications with throughputs ranging from several bits per hour up to 10 Mbps. The challenge is to accommodate this range of applications within a single wireless network based on a suitably flexible and power efficient medium access control protocol. To this end, we present a Medical Medium Access Control (MedMAC) protocol for energy efficient and adaptable channel access in body area networks. The MedMAC incorporates a novel synchronisation mechanism and initial power efficiency simulations show that the MedMAC protocol outperforms the IEEE 802.15.4 protocol for two classes of medical applications.","Energy efficiency,
Media Access Protocol,
Body area networks,
Access protocols,
Throughput,
Wireless networks,
Wireless application protocol,
Medical simulation,
Medical services,
Biomedical equipment"
Image-Driven Population Analysis Through Mixture Modeling,"We present iCluster, a fast and efficient algorithm that clusters a set of images while co-registering them using a parameterized, nonlinear transformation model. The output of the algorithm is a small number of template images that represent different modes in a population. This is in contrast with traditional, hypothesis-driven computational anatomy approaches that assume a single template to construct an atlas. We derive the algorithm based on a generative model of an image population as a mixture of deformable template images. We validate and explore our method in four experiments. In the first experiment, we use synthetic data to explore the behavior of the algorithm and inform a design choice on parameter settings. In the second experiment, we demonstrate the utility of having multiple atlases for the application of localizing temporal lobe brain structures in a pool of subjects that contains healthy controls and schizophrenia patients. Next, we employ iCluster to partition a data set of 415 whole brain MR volumes of subjects aged 18 through 96 years into three anatomical subgroups. Our analysis suggests that these subgroups mainly correspond to age groups. The templates reveal significant structural differences across these age groups that confirm previous findings in aging research. In the final experiment, we run iCluster on a group of 15 patients with dementia and 15 age-matched healthy controls. The algorithm produces two modes, one of which contains dementia patients only. These results suggest that the algorithm can be used to discover subpopulations that correspond to interesting structural or functional ldquomodesrdquo.","Image analysis,
Clustering algorithms,
Aging,
Dementia,
Anatomy,
Deformable models,
Partitioning algorithms,
Algorithm design and analysis,
Temporal lobe,
Brain"
Refactoring sequential Java code for concurrency via concurrent libraries,"Parallelizing existing sequential programs to run efficiently on multicores is hard. The Java 5 package java.util.concurrent (j.u.c.) supports writing concurrent programs: much of the complexity of writing thread-safe and scalable programs is hidden in the library. To use this package, programmers still need to reengineer existing code. This is tedious because it requires changing many lines of code, is error-prone because programmers can use the wrong APIs, and is omission-prone because programmers can miss opportunities to use the enhanced APIs. This paper presents our tool, Concurrencer, that enables programmers to refactor sequential code into parallel code that uses three j.u.c. concurrent utilities. Concurrencer does not require any program annotations. Its transformations span multiple, non-adjacent, program statements. A find-and-replace tool can not perform such transformations, which require program analysis. Empirical evaluation shows that Concurrencer refactors code effectively: Concurrencer correctly identifies and applies transformations that some open-source developers overlooked, and the converted code exhibits good speedup.","Java,
Concurrent computing,
Libraries,
Programming profession,
Ash,
Packaging,
Writing,
Open source software,
Parallel processing,
Yarn"
Learning actions from the Web,"This paper proposes a generic method for action recognition in uncontrolled videos. The idea is to use images collected from the Web to learn representations of actions and use this knowledge to automatically annotate actions in videos. Our approach is unsupervised in the sense that it requires no human intervention other than the text querying. Its benefits are two-fold: 1) we can improve retrieval of action images, and 2) we can collect a large generic database of action poses, which can then be used in tagging videos. We present experimental evidence that using action images collected from the Web, annotating actions is possible.",
Managing Risks in Distributed Software Projects: An Integrative Framework,"Software projects are increasingly geographically distributed with limited face-to-face interaction between participants. These projects face particular challenges that need careful managerial attention. While risk management has been adopted with success to address other challenges within software development, there are currently no frameworks available for managing risks related to geographical distribution. On this background, we systematically review the literature on geographically distributed software projects. Based on the review, we synthesize what we know about risks and risk resolution techniques into an integrative framework for managing risks in distributed contexts. Subsequent implementation of a Web-based tool helped us refine the framework based on empirical evaluation of its practical usefulness. We conclude by discussing implications for both research and practice.","Software,
Collaboration,
Book reviews,
Organizations,
Programming,
Risk management,
Data mining"
Multi-object tracking through occlusions by local tracklets filtering and global tracklets association with detection responses,"This paper presents an online detection-based two-stage multi-object tracking method in dense visual surveillances scenarios with a single camera. In the local stage, a particle filter with observer selection that could deal with partial object occlusion is used to generate a set of reliable tracklets. In the global stage, the detection responses are collected from a temporal sliding window to deal with ambiguity caused by full object occlusion to generate a set of potential tracklets. The reliable tracklets generated in the local stage and the potential tracklets generated within the temporal sliding window are associated by Hungarian algorithm on a modified pairwise tracklets association cost matrix to get the global optimal association. This method is applied to the pedestrian class and evaluated on two challenging datasets. The experimental results prove the effectiveness of our method.","Filtering,
Particle tracking,
Object detection,
Particle filters,
Cost function,
Robustness,
Detectors,
Humans,
Cameras,
Surveillance"
An Edge-Weighted Centroidal Voronoi Tessellation Model for Image Segmentation,"Centroidal Voronoi tessellations (CVTs) are special Voronoi tessellations whose generators are also the centers of mass (centroids) of the Voronoi regions with respect to a given density function and CVT-based methodologies have been proven to be very useful in many diverse applications in science and engineering. In the context of image processing and its simplest form, CVT-based algorithms reduce to the well-known k -means clustering and are easy to implement. In this paper, we develop an edge-weighted centroidal Voronoi tessellation (EWCVT) model for image segmentation and propose some efficient algorithms for its construction. Our EWCVT model can overcome some deficiencies possessed by the basic CVT model; in particular, the new model appropriately combines the image intensity information together with the length of cluster boundaries, and can handle very sophisticated situations. We demonstrate through extensive examples the efficiency, effectiveness, robustness, and flexibility of the proposed method.","Image segmentation,
Image processing,
Clustering algorithms,
Partitioning algorithms,
Computer vision,
Biological system modeling,
Partial differential equations,
Density functional theory,
Robustness,
Image edge detection"
Band selection for hyperspectral imagery using affinity propagation,"Hyperspectral imagery generally contains enormous amounts of data because of hundreds of spectral bands. Band selection is often adopted to reduce computational cost and accelerate knowledge discovery and other tasks such as subsequent classification. An exemplar-based clustering algorithm termed affinity propagation for band selection is proposed. Affinity propagation is derived from factor graph, and operates by initially considering all data points as potential cluster centres (exemplars) and then exchanging messages between data points until a good set of exemplars and clusters emerges. Affinity propagation has been applied to computer vision and bioinformatics, and shown to be much faster than other clustering methods for large data. By combining the information about the discriminative capability of each individual band and the correlation/similarity between bands, the exemplars generated by affine propagation have higher importance and less correlation/similarity. The performance of band selection is evaluated through a pixel image classification task. Experimental results demonstrate that, compared with some popular band selection methods, the bands selected by affinity propagation best characterise the hyperspectral imagery from the pixel classification standpoint.","pattern clustering,
computer vision,
data mining,
graph theory,
image classification,
image resolution"
Tight Enforcement of Information-Release Policies for Dynamic Languages,This paper studies the problem of securing information release in dynamic languages. We propose (i) an intuitive framework for information-release policies expressing both what can be released by an application and where in the code this release may take place and (ii) tight and modular enforcement by hybrid mechanisms that combine monitoring with on-the-fly static analysis for a language with dynamic code evaluation and communication primitives. The policy framework and enforcement mechanisms support both termination-sensitive and insensitive security policies.,"Information security,
Computer science,
Java,
Monitoring,
Computer security,
Remuneration,
Acoustical engineering,
Credit cards,
Information analysis,
Communication system security"
Piecewise planar city 3D modeling from street view panoramic sequences,"City environments often lack textured areas, contain repetitive structures, strong lighting changes and therefore are very difficult for standard 3D modeling pipelines. We present a novel unified framework for creating 3D city models which overcomes these difficulties by exploiting image segmentation cues as well as presence of dominant scene orientations and piecewise planar structures. Given panoramic street view sequences, we first demonstrate how to robustly estimate camera poses without a need for bundle adjustment and propose a multi-view stereo method which operates directly on panoramas, while enforcing the piecewise planarity constraints in the sweeping stage. At last, we propose a new depth fusion method which exploits the constraints of urban environments and combines advantages of volumetric and viewpoint based fusion methods. Our technique avoids expensive voxelization of space, operates directly on 3D reconstructed points through effective kd-tree representation, and obtains a final surface by tessellation of backprojections of those points into the reference image.",
Evolutionary many-objective optimization by NSGA-II and MOEA/D with large populations,"Evolutionary multiobjective optimization (EMO) is an active research area in the field of evolutionary computation. EMO algorithms are designed to find a non-dominated solution set that approximates the entire Pareto front of a multiobjective optimization problem. Whereas EMO algorithms usually work well on two-objective and three-objective problems, their search ability is degraded by the increase in the number of objectives. One difficulty in the handling of many-objective problems is the exponential increase in the number of non-dominated solutions necessary for approximating the entire Pareto front. A simple countermeasure to this difficulty is to use large populations in EMO algorithms. In this paper, we examine the behavior of EMO algorithms with large populations (e.g., with 10,000 individuals) through computational experiments on multiobjective and many-objective knapsack problems with two, four, six, eight and ten objectives. We examine two totally different algorithms: NSGA-II and MOEA/D. NSGA-II is a Pareto dominance-based algorithm while MOEA/D uses scalarizing functions. Their search ability is examined for various specifications of the population size under the fixed computation load. That is, we use the total number of examined solutions as the stopping condition of each algorithm. Thus the use of a very large population leads to the termination at an early generation (e.g., 20th generation). It is demonstrated through computational experiments that the use of too large populations makes NSGA-II very slow and inefficient. On the other hand, MOEA/D works well even when it is executed with a very large population. We also discuss why MOEA/D works well even when the population size is unusually large.","Degradation,
Algorithm design and analysis,
Design optimization,
Pareto optimization,
Cybernetics,
USA Councils,
Computer science,
Intelligent systems,
Evolutionary computation,
Genetic algorithms"
Queuing Network Models for Multi-Channel P2P Live Streaming Systems,"In recent years there have been several large-scale deployments of P2P live video systems. Existing and future P2P live video systems will offer a large number of channels, with users switching frequently among the channels. In this paper, we develop infinite-server queueing network models to analytically study the performance of multi-channel P2P streaming systems. Our models capture essential aspects of multi-channel video systems, including peer channel switching, peer churn, peer bandwidth heterogeneity, and Zipf-like channel popularity. We apply the queueing network models to two P2P streaming designs: the isolated channel design (ISO) and the View-Upload Decoupling (VUD) design. For both of these designs, we develop efficient algorithms to calculate critical performance measures, develop an asymptotic theory to provide closed-form results when the number of peers approaches infinity, and derive near- optimal provisioning rules for assigning peers to groups in VUD. We use the analytical results to compare VUD with ISO. We show that VUD design generally performs significantly better, particularly for systems with heterogeneous channel popularities and streaming rates.",
Comparison of Tree-Child Phylogenetic Networks,"Phylogenetic networks are a generalization of phylogenetic trees that allow for the representation of non-tree-like evolutionary events, like recombination, hybridization, or lateral gene transfer. While much progress has been made to find practical algorithms for reconstructing a phylogenetic network from a set of sequences, all attempts to endorse a class of phylogenetic networks (strictly extending the class of phylogenetic trees) with a well-founded distance measure have, to the best of our knowledge and with the only exception of the bipartition distance on regular networks, failed so far. In this paper, we present and study a new meaningful class of phylogenetic networks, called tree-child phylogenetic networks, and we provide an injective representation of these networks as multisets of vectors of natural numbers, their path multiplicity vectors. We then use this representation to define a distance on this class that extends the well-known Robinson-Foulds distance for phylogenetic trees and to give an alignment method for pairs of networks in this class. Simple polynomial algorithms for reconstructing a tree-child phylogenetic network from its path multiplicity vectors, for computing the distance between two tree-child phylogenetic networks and for aligning a pair of tree-child phylogenetic networks, are provided. They have been implemented as a Perl package and a Java applet, which can be found at http://bioinfo.uib.es/~recerca/phylonetworks/mudistance/.","Phylogeny,
History,
Genetic mutations,
Reconstruction algorithms,
Polynomials,
Computer networks,
Packaging,
Java,
Organisms,
Databases"
Multi-agent Q-learning of channel selection in multi-user cognitive radio systems: A two by two case,"Resource allocation is an important issue in cognitive radio systems. It can be done by carrying out negotiation among secondary users. However, significant overhead may be incurred by the negotiation since the negotiation needs to be done frequently due to the rapid change of primary users' activity. In this paper, a channel selection scheme without negotiation is considered for multi-user and multi-channel cognitive radio systems. To avoid collision incurred by non-coordination, each secondary user learns how to select channels according to its experience. Multi-agent reinforcement leaning (MARL) is applied in the framework of Q-learning by considering opponent secondary users as a part of the environment. The dynamics of the Q-learning are illustrated using Metrick-Polak plot. A rigorous proof of the convergence of Q-learning is provided via the similarity between the Q-learning and Robinson-Monro algorithm, as well as the analysis of convergence of the corresponding ordinary differential equation (via Lyapunov function). Examples are illustrated and the performance of learning is evaluated by numerical simulations.","Cognitive radio,
Resource management,
Frequency,
Convergence,
Wireless communication,
Microeconomics,
Data communication,
Switches,
Cybernetics,
USA Councils"
Video-aware opportunistic network coding over wireless networks,"In this paper, we study video streaming over wireless networks with network coding capabilities. We build upon recent work, which demonstrated that network coding can increase throughput over a broadcast medium, by mixing packets from different flows into a single packet, thus increasing the information content per transmission. Our key insight is that, when the transmitted flows are video streams, network codes should be selected so as to maximize not only the network throughput but also the video quality. We propose video-aware opportunistic network coding schemes that take into account both the decodability of network codes by several receivers and the importance and deadlines of video packets. Simulation results show that our schemes significantly improve both video quality and throughput. This work is a first step towards content-aware network coding.","Network coding,
Wireless networks,
Streaming media,
Throughput,
Decoding,
Wireless mesh networks,
Video compression,
Broadcasting,
IEEE members,
Multimedia communication"
Information Fusion for Diabetic Retinopathy CAD in Digital Color Fundus Photographs,"The purpose of computer-aided detection or diagnosis (CAD) technology has so far been to serve as a second reader. If, however, all relevant lesions in an image can be detected by CAD algorithms, use of CAD for automatic reading or prescreening may become feasible. This work addresses the question how to fuse information from multiple CAD algorithms, operating on multiple images that comprise an exam, to determine a likelihood that the exam is normal and would not require further inspection by human operators. We focus on retinal image screening for diabetic retinopathy, a common complication of diabetes. Current CAD systems are not designed to automatically evaluate complete exams consisting of multiple images for which several detection algorithm output sets are available. Information fusion will potentially play a crucial role in enabling the application of CAD technology to the automatic screening problem. Several different fusion methods are proposed and their effect on the performance of a complete comprehensive automatic diabetic retinopathy screening system is evaluated. Experiments show that the choice of fusion method can have a large impact on system performance. The complete system was evaluated on a set of 15 000 exams (60 000 images). The best performing fusion method obtained an area under the receiver operator characteristic curve of 0.881. This indicates that automated prescreening could be applied in diabetic retinopathy screening programs.",
How do scientists develop and use scientific software?,"New knowledge in science and engineering relies increasingly on results produced by scientific software. Therefore, knowing how scientists develop and use software in their research is critical to assessing the necessity for improving current development practices and to making decisions about the future allocation of resources. To that end, this paper presents the results of a survey conducted online in October-December 2008 which received almost 2000 responses. Our main conclusions are that (1) the knowledge required to develop and use scientific software is primarily acquired from peers and through self-study, rather than from formal education and training; (2) the number of scientists using supercomputers is small compared to the number using desktop or intermediate computers; (3) most scientists rely primarily on software with a large user base; (4) while many scientists believe that software testing is important, a smaller number believe they have sufficient understanding about testing concepts; and (5) that there is a tendency for scientists to rank standard software engineering concepts higher if they work in large software development projects and teams, but that there is no uniform trend of association between rank of importance of software engineering concepts and project/team size.","Software testing,
Software engineering,
Knowledge engineering,
Resource management,
Computer science education,
Supercomputers,
Peer to peer computing,
Automatic testing,
Software standards,
Standards development"
A case for dynamic frequency tuning in on-chip networks,"Performance and power are the first order design metrics for network-on-chips (NoCs) that have become the de-facto standard in providing scalable communication backbones for multicores/CMPs. However, NoCs can be plagued by higher power consumption and degraded throughput if the network and router are not designed properly. Towards this end, this paper proposes a novel router architecture, where we tune the frequency of a router in response to network load to manage both performance and power. We propose three dynamic frequency tuning techniques, FreqBoost, FreqThrtl and FreqTune, targeted at congestion and power management in NoCs. As enablers for these techniques, we exploit Dynamic Voltage and Frequency Scaling (DVFS) and the imbalance in a generic router pipeline through time stealing. Experiments using synthetic workloads on a 8x8 wormhole-switched mesh interconnect show that FreqBoost is a better choice for reducing average latency (maximum 40%) while, FreqThrtl provides the maximum benefits in terms of power saving and energy delay product (EDP). The FreqTune scheme is a better candidate for optimizing both performance and power, achieving on an average 36% reduction in latency, 13% savings in power (up to 24% at high load), and 40% savings (up to 70% at high load) in EDP. With application benchmarks, we observe IPC improvement up to 23% using our design. The performance and power benefits also scale for larger NoCs.","Frequency,
Tuning,
Network-on-a-chip,
Delay,
Energy management,
Communication standards,
Spine,
Multicore processing,
Energy consumption,
Degradation"
Prediction and management in energy harvested wireless sensor nodes,"Solar panels are frequently used in wireless sensor nodes because they can theoretically provide quite a bit of harvested energy. However, they are not a reliable, consistent source of energy because of the Sun's cycles and the everchanging weather conditions. Thus, in this paper we present a fast, efficient and reliable solar prediction algorithm, namely, weather-conditioned moving average (WCMA) that is capable of exploiting the solar energy more efficiently than state-of-the-art energy prediction algorithms (e.g. exponential weighted moving average EWMA). In particular, WCMA is able to effectively take into account both the current and past-days weather conditions, obtaining a relative mean error of only 10%. When coupled with energy management algorithm, it can achieve gains of more than 90% in energy utilization with respect to EWMA under the real working conditions of the Shimmer node, an active sensing platform for structural health monitoring.","Energy management,
Wireless sensor networks,
Prediction algorithms,
Solar energy,
Batteries,
Sensor systems,
Weather forecasting,
Energy storage,
Capacitors,
Monitoring"
Shift-map image editing,"Geometric rearrangement of images includes operations such as image retargeting, inpainting, or object rearrangement. Each such operation can be characterized by a shiftmap: the relative shift of every pixel in the output image from its source in an input image.",
"MABEL, a new robotic bipedal walker and runner","This paper introduces MABEL, a new platform for the study of bipedal locomotion in robots. One of the purposes of building the mechanism is to explore a novel powertrain design that incorporates compliance, with the objective of improving the power efficiency of the robot, both in steady state operation and in responding to disturbances. A second purpose is to inspire the development of new feedback control algorithms for running on level surfaces and walking on rough terrain. A third motivation for building the robot is science and technology outreach; indeed, it is already included in tours when K-through-12 students visit the College of Engineering at the University of Michigan. MABEL is currently walking at 1.1 m/s on a level surface, and a related mono-pod at Carnegie Mellon is hopping well, establishing that the testbed has the potential to realize its many objectives.","Legged locomotion,
Robots,
Rough surfaces,
Surface roughness,
Buildings,
Mechanical power transmission,
Steady-state,
Feedback control,
Educational institutions,
Testing"
Automatic Segmentation of Pulmonary Segments From Volumetric Chest CT Scans,"Automated extraction of pulmonary anatomy provides a foundation for computerized analysis of computed tomography (CT) scans of the chest. A completely automatic method is presented to segment the lungs, lobes and pulmonary segments from volumetric CT chest scans. The method starts with lung segmentation based on region growing and standard image processing techniques. Next, the pulmonary fissures are extracted by a supervised filter. Subsequently the lung lobes are obtained by voxel classification where the position of voxels in the lung and relative to the fissures are used as features. Finally, each lobe is subdivided in its pulmonary segments by applying another voxel classification that employs features based on the detected fissures and the relative position of voxels in the lobe. The method was evaluated on 100 low-dose CT scans obtained from a lung cancer screening trial and compared to estimates of both interobserver and intraobserver agreement. The method was able to segment the pulmonary segments with high accuracy (77%), comparable to both interobserver and intraobserver accuracy (74% and 80%, respectively).","Computed tomography,
Lungs,
Image segmentation,
Surgery,
High-resolution imaging,
Image analysis,
Lesions,
Anatomy,
Image processing,
Filters"
A quantitative analysis of high performance computing with Amazon's EC2 infrastructure: The death of the local cluster?,"The introduction of affordable infrastructure on demand, specifically Amazon's Elastic Compute Cloud (EC2), has had a significant impact in the business IT community and provides reasonable and attractive alternatives to locally-owned infrastructure. For scientific computation however, the viability of EC2 has come into question due to its use of virtualization and network shaping and the performance impacts of both. Several works have shown that EC2 cannot compete with a dedicated HPC cluster utilizing high-performance interconnects, but how does EC2 compare with smaller departmental and lab-sized commodity clusters that are often the primary computational resource for scientists? To answer that question we have run MPI and memory bandwidth benchmarks on EC2 clusters with each of the 64-bit instance types to compare the performance of a 16 node cluster of each to a dedicated locally-owned commodity cluster based on Gigabit Ethernet. Our results show that while EC2 does experience reduced performance, it is still viable for smaller-scale applications.","Performance analysis,
High performance computing,
Cloud computing,
Computer networks,
Delay,
Debugging,
Resource virtualization,
Scientific computing,
Computer science,
Bandwidth"
Fuzzy Spectral and Spatial Feature Integration for Classification of Nonferrous Materials in Hyperspectral Data,"Hyperspectral data allows the construction of more elaborate models to sample the properties of the nonferrous materials than the standard RGB color representation. In this paper, the nonferrous waste materials are studied as they cannot be sorted by classical procedures due to their color, weight and shape similarities. The experimental results presented in this paper reveal that factors such as the various levels of oxidization of the waste materials and the slight differences in their chemical composition preclude the use of the spectral features in a simplistic manner for robust material classification. To address these problems, the proposed FUSSER (fuzzy spectral and spatial classifier) algorithm detailed in this paper merges the spectral and spatial features to obtain a combined feature vector that is able to better sample the properties of the nonferrous materials than the single pixel spectral features when applied to the construction of multivariate Gaussian distributions. This approach allows the implementation of statistical region merging techniques in order to increase the performance of the classification process. To achieve an efficient implementation, the dimensionality of the hyperspectral data is reduced by constructing bio-inspired spectral fuzzy sets that minimize the amount of redundant information contained in adjacent hyperspectral bands. The experimental results indicate that the proposed algorithm increased the overall classification rate from 44% using RGB data up to 98% when the spectral-spatial features are used for nonferrous material classification.",
Dynamic optimization using Self-Adaptive Differential Evolution,In this paper we investigate a Self-Adaptive Differential Evolution algorithm (jDE) where F and CR control parameters are self-adapted and a multi-population method with aging mechanism is used. The performance of the jDE algorithm is evaluated on the set of benchmark functions provided for the CEC 2009 special session on evolutionary computation in dynamic and uncertain environments.,"Evolutionary computation,
Benchmark testing,
Chromium,
Optimization methods,
Aging,
Space stations,
Performance analysis,
Computer science,
Hybrid intelligent systems,
Control systems"
Towards Automated RESTful Web Service Composition,"Emerging as the popular choice for leading Internet companies to expose internal data and resources, Restful Web services are attracting increasing attention in the industry.While automating WSDL/SOAP based Web service composition has been extensively studied in the research community, automated RESTful Web service composition in the context of service-oriented architecture (SOA), to the best of our knowledge, is less explored. As an early paper addressing this problem, this paper discusses the challenges of composing RESTful Web services and proposes a formal model for describing individual Web services and automating the composition. It demonstrates our approach by applying it to a real-world RESTful Web service composition problem. This paper represents our initial efforts towards the problem of automated RESTful Web service composition.We are hoping that it will draw interests from the research community on Web services, and engage more researchers in this challenge.","Web services,
Simple object access protocol,
Service oriented architecture,
Lead,
Web and internet services,
Encapsulation,
Scalability,
Computer science,
Computer industry,
Context-aware services"
Joint routing and link rate allocation under bandwidth and energy constraints in sensor networks,"In sensor networks, both energy and bandwidth are scarce resources. In the past, many energy efficient routing algorithms have been devised in order to maximize network lifetime, in which wireless link bandwidth has been optimistically assumed to be sufficient. This article shows that ignoring the bandwidth constraint can lead to infeasible routing solutions. As energy constraint affects how data should be routed, link bandwidth also affects not only the routing topology but also the allowed data rate on each link. In this paper, we discuss the sufficient condition on link bandwidth that makes a routing solution feasible, then provide mathematical optimization models to tackle both energy and bandwidth constraints.We first present a basic mathematical model to address using uniform transmission power for routing without data aggregation, then extend it to handle nonuniform transmission power, and then routing with data aggregation. We propose two efficient heuristics to compute the routing topology and link data rate. Simulation results show that these heuristics provide more feasible routing solutions than previous work, and provide significant improvement on throughput and lifetime.","Routing,
Bandwidth,
Mathematical model,
Energy efficiency,
Wireless sensor networks,
Network topology,
Sufficient conditions,
Constraint optimization,
Computational modeling,
Throughput"
Choreographing Web Services,"This paper introduces the multiagent protocols (MAP) Web service choreography language and demonstrates how service choreographies can be specified, verified, and enacted with a comparatively simple process language. MAP is a directly executable specification, services do not have to be preconfigured at design-time. Instead, a choreography, specified in MAP, can be sent dynamically to a group of distributed peers to execute at runtime. Furthermore, MAP is based on a formal foundation, this allows model checking of the choreography definition prior to live distribution and enactment. A motivating scenario, taken from the AstroGrid science use-cases, serves as the focal point for the paper and highlights the benefits of choreography, through data flow optimization and lack of centralized server. The MAP formal syntax and model checking environment are discussed in the context of the motivating scenario, along with MagentA, an implementation of MAP which provides a concrete, and open-source framework for the enactment of distributed choreographies. MAP is evaluated by demonstrating the languages conformance to the service interaction patterns, a collection of 13 recurring workflow patterns.",
DT-REFinD: Diffusion Tensor Registration With Exact Finite-Strain Differential,"In this paper, we propose the DT-REFinD algorithm for the diffeomorphic nonlinear registration of diffusion tensor images. Unlike scalar images, deforming tensor images requires choosing both a reorientation strategy and an interpolation scheme. Current diffusion tensor registration algorithms that use full tensor information face difficulties in computing the differential of the tensor reorientation strategy and consequently, these methods often approximate the gradient of the objective function. In the case of the finite-strain (FS) reorientation strategy, we borrow results from the pose estimation literature in computer vision to derive an analytical gradient of the registration objective function. By utilizing the closed-form gradient and the velocity field representation of one parameter subgroups of diffeomorphisms, the resulting registration algorithm is diffeomorphic and fast. We contrast the algorithm with a traditional FS alternative that ignores the reorientation in the gradient computation. We show that the exact gradient leads to significantly better registration at the cost of computation time. Independently of the choice of Euclidean or Log-Euclidean interpolation and sum of squared differences dissimilarity measure, the exact gradient achieves better alignment over an entire spectrum of deformation penalties. Alignment quality is assessed with a battery of metrics including tensor overlap, fractional anisotropy, inverse consistency and closeness to synthetic warps. The improvements persist even when a different reorientation scheme, preservation of principal directions, is used to apply the final deformations.","Tensile stress,
Diffusion tensor imaging,
Interpolation,
Anisotropic magnetoresistance,
Face detection,
Computer vision,
Computational efficiency,
Battery charge measurement,
In vivo,
Biological tissues"
Combining automated analysis and visualization techniques for effective exploration of high-dimensional data,"Visual exploration of multivariate data typically requires projection onto lower-dimensional representations. The number of possible representations grows rapidly with the number of dimensions, and manual exploration quickly becomes ineffective or even unfeasible. This paper proposes automatic analysis methods to extract potentially relevant visual structures from a set of candidate visualizations. Based on features, the visualizations are ranked in accordance with a specified user task. The user is provided with a manageable number of potentially useful candidate visualizations, which can be used as a starting point for interactive data analysis. This can effectively ease the task of finding truly useful visualizations and potentially speed up the data exploration task. In this paper, we present ranking measures for class-based as well as non class-based Scatterplots and Parallel Coordinates visualizations. The proposed analysis methods are evaluated on different datasets.","Data visualization,
Scattering,
Visual analytics,
Data mining,
Coordinate measuring machines,
Image storage,
Information retrieval,
Image retrieval,
Displays,
Business"
On-Demand Resource Provisioning for BPEL Workflows Using Amazon's Elastic Compute Cloud,"BPEL is the de facto standard for business process modeling in today's enterprises and is a promising candidate for the integration of business and Grid applications. Current BPEL implementations do not provide mechanisms to schedule service calls with respect to the load of the target hosts. In this paper, a solution that automatically schedules workflow steps to underutilized hosts and provides new hosts using Cloud computing infrastructures in peak-load situations is presented. The proposed approach does not require any changes to the BPEL standard.  An implementation based on the ActiveBPEL engine and Amazon's Elastic Compute Cloud is presented.","Cloud computing,
Grid computing,
Web services,
Engines,
Processor scheduling,
Service oriented architecture,
Runtime,
Paramagnetic resonance,
Mathematics,
Computer science"
Registration of Cervical MRI Using Multifeature Mutual Information,"Radiation therapy for cervical cancer can benefit from image registration in several ways, for example by studying the motion of organs, or by (partially) automating the delineation of the target volume and other structures of interest. In this paper, the registration of cervical data is addressed using mutual information (MI) of not only image intensity, but also features that describe local image structure. Three aspects of the registration are addressed to make this approach feasible. First, instead of relying on a histogram-based estimation of mutual information, which poses problems for a larger number of features, a graph-based implementation of alpha-mutual information (alpha-MI) is employed. Second, the analytical derivative of alpha-MI is derived. This makes it possible to use a stochastic gradient descent method to solve the registration problem, which is substantially faster than non-derivative-based methods. Third, the feature space is reduced by means of a principal component analysis, which also decreases the registration time. The proposed technique is compared to a standard approach, based on the mutual information of image intensity only. Experiments are performed on 93 T2-weighted MR clinical data sets acquired from 19 patients with cervical cancer. Several characteristics of the proposed algorithm are studied on a subset of 19 image pairs (one pair per patient). On the remaining data (36 image pairs, one or two pairs per patient) the median overlap is shown to improve significantly compared to standard MI from 0.85 to 0.86 for the clinical target volume (CTV, p = 2 ldr10-2), from 0.75 to 0.81 for the bladder (p = 8 ldr 10-6), and from 0.76 to 0.77 for the rectum (p = 2 ldr 10-4). The registration error is improved at important tissue interfaces, such as that of the bladder with the CTV, and the interface of the rectum with the uterus and cervix.",
A framework for automated measurement of the intensity of non-posed Facial Action Units,"This paper presents a framework to automatically measure the intensity of naturally occurring facial actions. Naturalistic expressions are non-posed spontaneous actions. The facial action coding system (FACS) is the gold standard technique for describing facial expressions, which are parsed as comprehensive, nonoverlapping action units (Aus). AUs have intensities ranging from absent to maximal on a six-point metric (i.e., 0 to 5). Despite the efforts in recognizing the presence of non-posed action units, measuring their intensity has not been studied comprehensively. In this paper, we develop a framework to measure the intensity of AU12 (lip corner puller) and AU6 (cheek raising) in videos captured from infant-mother live face-to-face communications. The AU12 and AU6 are the most challenging case of infant's expressions (e.g., low facial texture in infant's face). One of the problems in facial image analysis is the large dimensionality of the visual data. Our approach for solving this problem is to utilize the spectral regression technique to project high dimensionality facial images into a low dimensionality space. Represented facial images in the low dimensional space are utilized to train support vector machine classifiers to predict the intensity of action units. Analysis of 18 minutes of captured video of non-posed facial expressions of several infants and mothers shows significant agreement between a human FACS coder and our approach, which makes it an efficient approach for automated measurement of the intensity of non-posed facial action units.",
Static Security Optimization for Real-Time Systems,"An increasing number of real-time applications like railway signaling control systems and medical electronics systems require high quality of security to assure confidentiality and integrity of information. Therefore, it is desirable and essential to fulfill security requirements in security-critical real-time systems. This paper addresses the issue of optimizing quality of security in real-time systems. To meet the needs of a wide variety of security requirements imposed by real-time systems, a group-based security service model is used in which the security services are partitioned into several groups depending on security types. While services within the same security group provide the identical type of security service, the services in the group can achieve different quality of security. Security services from a number of groups can be combined to deliver better quality of security. In this study, we seamlessly integrate the group-based security model with a traditional real-time scheduling algorithm, namely earliest deadline first (EDF). Moreover, we design and develop a security-aware EDF schedulability test. Given a set of real-time tasks with chosen security services, our scheduling scheme aims at optimizing the combined security value of the selected services while guaranteeing the schedulability of the real-time tasks. We study two approaches to solve the security-aware optimization problem. Experimental results show that the combined security values are substantially higher than those achieved by alternatives for real-time tasks without violating real-time constraints.","Real time systems,
Radar tracking,
Information security,
Control systems,
Computer science,
Rail transportation,
Application software,
Medical control systems,
Scheduling algorithm,
Testing"
A cooperative coevolutionary algorithm with Correlation based Adaptive Variable Partitioning,"A cooperative coevolutionary algorithm (CCEA) is an extension to an evolutionary algorithm (EA); it employs a divide and conquer strategy to solve an optimization problem. In its basic form, a CCEA splits the variables of an optimization problem into multiple smaller subsets and evolves them independently in different subpopulations. The dynamics of a CCEA is far more complex than an EA and its performance can vary from good to bad depending on the separability of the optimization problem. This paper provides some insights into why CCEA in its basic form is not suitable for nonseparable problems and introduces a Cooperative Coevolutionary Algorithm with Correlation based Adaptive Variable Partitioning (CCEA-AVP) to deal with such problems. The performance of CCEA-AVP is compared with CCEA and EA to highlight its benefits. CCEA-AVP offers the possibility to deal with problems where separability among variables might vary in different regions of the search space.",
Evaluation of 3D registration reliability and speed - A comparison of ICP and NDT,"To advance robotic science it is important to perform experiments that can be replicated by other researchers to compare different methods. However, these comparisons tend to be biased, since re-implementations of reference methods often lack thoroughness and do not include the hands-on experience obtained during the original development process. This paper presents a thorough comparison of 3D scan registration algorithms based on a 3D mapping field experiment, carried out by two research groups that are leading in the field of 3D robotic mapping. The iterative closest points algorithm (ICP) is compared to the normal distributions transform (NDT). We also present an improved version of NDT with a substantially larger valley of convergence than previously published versions.","Iterative closest point algorithm,
Robotics and automation,
Iterative algorithms,
Robot sensing systems,
Performance evaluation,
Gaussian distribution,
Convergence,
Guidelines,
Algorithm design and analysis,
Vehicles"
Distinguishability of Quantum States by Separable Operations,"In this paper, we study the distinguishability of multipartite quantum states by separable operations. We first present a necessary and sufficient condition for a finite set of orthogonal quantum states to be distinguishable by separable operations. An analytical version of this condition is derived for the case of (D-1) pure states, where D is the total dimension of the state space under consideration. A number of interesting consequences of this result are then carefully investigated. Remarkably, we show there exists a large class of 2 otimes 2 separable operations not being realizable by local operations and classical communication. Before our work, only a class of 3 otimes 3 nonlocal separable operations was known [Bennett , Phys. Rev. A 59, 1070 (1999)]. We also show that any basis of the orthogonal complement of a multipartite pure state is indistinguishable by separable operations if and only if this state cannot be a superposition of one or two orthogonal product states, i.e., has an orthogonal Schmidt number not less than three, thus generalize the recent work about indistinguishable bipartite subspaces [Watrous, Phys. Rev. Lett. 95, 080505 (2005)]. Notably, we obtain an explicit construction of indistinguishable subspaces of dimension 7 (or 6) by considering a composite quantum system consisting of two qutrits (resp., three qubits), which is slightly better than the previously known indistinguishable bipartite subspace with dimension 8.",
Machine learning based encrypted traffic classification: Identifying SSH and Skype,"The objective of this work is to assess the robustness of machine learning based traffic classification for classifying encrypted traffic where SSH and Skype are taken as good representatives of encrypted traffic. Here what we mean by robustness is that the classifiers are trained on data from one network but tested on data from an entirely different network. To this end, five learning algorithms — AdaBoost, Support Vector Machine, Naïe Bayesian, RIPPER and C4.5 — are evaluated using flow based features, where IP addresses, source/destination ports and payload information are not employed. Results indicate the C4.5 based approach performs much better than other algorithms on the identification of both SSH and Skype traffic on totally different networks.","Machine learning,
Cryptography,
Telecommunication traffic,
Traffic control,
Payloads,
Bayesian methods,
Robustness,
Support vector machines,
Support vector machine classification,
Financial management"
Performance Analysis of Multichannel Medium Access Control Algorithms for Opportunistic Spectrum Access,"In this paper, different control channel (CC) implementations for multichannel medium access control (MAC) algorithms are compared and analyzed in the context of opportunistic spectrum access (OSA) as a function of spectrum-sensing performance and licensed user activity. The analysis is based on a discrete Markov chain model of a subset of representative multichannel OSA MAC classes that incorporates physical layer effects, such as spectrum sensing and fading. The analysis is complemented with extensive simulations. The major observations are given as follows: 1) When the CC is implemented through a dedicated channel, sharing such dedicated channel with the licensed user does not significantly decrease the throughput achieved by the OSA network when the data packet sizes are sufficiently large or the number of considered data channels is small. 2) Hopping OSA MACs, where the CC is spread over all channels, are less susceptible to licensed user activity than those with a dedicated CC (in terms of both average utilization and on/off times). 3) Scanning efficiency has a large impact on the achievable performance of licensed and OSA users for all analyzed protocols. 4) The multiple rendezvous MAC class, which has yet to be proposed in OSA literature, outperforms all the multichannel MAC designs analyzed in this paper.",
Convergence Acceleration Operator for Multiobjective Optimization,"A convergence acceleration operator (CAO) is described which enhances the search capability and the speed of convergence of the host multiobjective optimization algorithm. The operator acts directly in the objective space to suggest improvements to solutions obtained by a multiobjective evolutionary algorithm (MOEA). The suggested improved objective vectors are then mapped into the decision variable space and tested. This method improves upon prior work in a number of important respects, such as mapping technique and solution improvement. Further, the paper discusses implications for many-objective problems and studies the impact of the use of the CAO as the number of objectives increases. The CAO is incorporated with two leading MOEAs, the non-dominated sorting genetic algorithm and the strength Pareto evolutionary algorithm and tested. Results show that the hybridized algorithms consistently improve the speed of convergence of the original algorithm while maintaining the desired distribution of solutions. It is shown that the operator is a transferable component that can be hybridized with any MOEA.",
A comparison of SLAM algorithms based on a graph of relations,"In this paper, we address the problem of creating an objective benchmark for comparing SLAM approaches. We propose a framework for analyzing the results of SLAM approaches based on a metric for measuring the error of the corrected trajectory. The metric uses only relative relations between poses and does not rely on a global reference frame. The idea is related to graph-based SLAM approaches in the sense that it considers the energy needed to deform the trajectory estimated by a SLAM approach to the ground truth trajectory. Our method enables us to compare SLAM approaches that use different estimation techniques or different sensor modalities since all computations are made based on the corrected trajectory of the robot. We provide sets of relative relations needed to compute our metric for an extensive set of datasets frequently used in the SLAM community. The relations have been obtained by manually matching laser-range observations. We believe that our benchmarking framework allows the user an easy analysis and objective comparisons between different SLAM approaches.","Simultaneous localization and mapping,
Mobile robots,
Contracts,
Particle filters,
Intelligent robots,
USA Councils,
Error correction,
Robot sensing systems,
Laser modes,
Transportation"
Taint-based directed whitebox fuzzing,"We present a new automated white box fuzzing technique and a tool, BuzzFuzz, that implements this technique. Unlike standard fuzzing techniques, which randomly change parts of the input file with little or no information about the underlying syntactic structure of the file, BuzzFuzz uses dynamic taint tracing to automatically locate regions of original seed input files that influence values used at key program attack points (points where the program may contain an error). BuzzFuzz then automatically generates new fuzzed test input files by fuzzing these identified regions of the original seed input files. Because these new test files typically preserve the underlying syntactic structure of the original seed input files, they tend to make it past the initial input parsing components to exercise code deep within the semantic core of the computation. We have used BuzzFuzz to automatically find errors in two open-source applications: Swfdec (an Adobe Flash player) and MuPDF (a PDF viewer). Our results indicate that our new directed fuzzing technique can effectively expose errors located deep within large programs. Because the directed fuzzing technique uses taint to automatically discover and exploit information about the input file format, it is especially appropriate for testing programs that have complex, highly structured input file formats.","Libraries,
Automatic testing,
Character generation,
Law,
Legal factors,
Instruments,
Computer science,
Artificial intelligence,
Laboratories,
Open source software"
Thwarting Blackhole Attacks in Disruption-Tolerant Networks using Encounter Tickets,"Nodes in disruption-tolerant networks (DTNs) usually exhibit repetitive motions. Several recently proposed DTN routing algorithms have utilized the DTNs' cyclic properties for predicting future forwarding. The prediction is based on metrics abstracted from nodes' contact history. However, the robustness of the encounter prediction becomes vital for DTN routing since malicious nodes can provide forged metrics or follow sophisticated mobility patterns to attract packets and gain a significant advantage in encounter prediction. In this paper, we examine the impact of the blackhole attack and its variations in DTN routing. We introduce the concept of encounter tickets to secure the evidence of each contact. In our scheme, nodes adopt a unique way of interpreting the contact history by making observations based on the collected encounter tickets. Then, following the Dempster-Shafer theory, nodes form trust and confidence opinions towards the competency of each encountered forwarding node. Extensive real-trace-driven simulation results are presented to support the effectiveness of our system.","Disruption tolerant networking,
Peer to peer computing,
History,
Computer science,
Robustness,
Communications Society,
Statistics,
Uncertainty,
Routing protocols,
Security"
Networks Evolving Step by Step: Statistical Analysis of Dyadic Event Data,"With few exceptions, statistical analysis of social networks is currently focused on cross-sectional or panel data. On the other hand, automated collection of network-data often produces event data, i.e., data encoding the exact time of interaction between social actors. In this paper we propose models and methods to analyze such networks of dyadic events and to determine the factors that influence the frequency and quality of interaction. We apply our methods to empirical datasets about political conflicts and test several hypotheses concerning reciprocity and structural balance theory.","Statistical analysis,
Social network services,
Encoding,
Testing,
Information analysis,
Computer networks,
Information science,
Statistics,
Frequency,
Design methodology"
On capacity of random wireless networks with physical-layer network coding,"Throughput capacity of a random wireless network has been studied extensively in the literature. Most existing studies were based on the assumption that each transmission involves only one transmitter in order to avoid interference. However, recent studies on physical-layer network coding (PLNC) have shown that such an assumption can be relaxed to improve throughput performance of a wireless network. In PLNC, signals from different senders can be transmitted to the same receiver in the same channel simultaneously. In this paper, we investigate the impact of PLNC on throughput capacity of a random wireless network. Our study reveals that, although PLNC scheme does not change the scaling law, it can improve throughput capacity by a fixed factor. Specifically, for a one-dimensional network, we observe that PLNC can eliminate the effect of interference in some scenarios. A tighter capacity bound is derived for a two-dimensional network. In addition, we also show achievable lower bounds for random wireless networks with network coding and PLNC.","Wireless networks,
Network coding,
Throughput,
Communication networks,
Switches,
Decoding,
Physical layer,
Transmitters,
Interference elimination,
Computer networks"
Quantitative Optoacoustic Signal Extraction Using Sparse Signal Representation,"We report on a new quantification methodology of optoacoustic tomographic reconstructions under heterogeneous illumination conditions representative of realistic whole-body imaging scenarios. Our method relies on the differences in the spatial characteristics of the absorption coefficient and the optical energy density within the medium. By using sparse-representation based decomposition, we exploit these different characteristics to extract both the absorption coefficient and the photon density within the imaged object from the optoacoustic image. In contrast to previous methods, this algorithm is not based on the solution of theoretical light transport equations and it does not require explicit knowledge of the illumination geometry or the optical properties of the object and other unknown or loosely defined experimental parameters, leading to highly robust performance. The method was successfully examined with numerically and experimentally generated data and was found to be ideally suited for practical implementations in tomographic schemes of varying complexity, including multiprojection illumination systems and multispectral optoacoustic tomography (MSOT) studies of tissue biomarkers.","Signal representations,
Tomography,
Lighting,
Absorption,
Biomedical optical imaging,
Image reconstruction,
Optical imaging,
Equations,
Geometrical optics,
Robustness"
Reachability-guided sampling for planning under differential constraints,"Rapidly-exploring Random Trees (RRTs) are widely used to solve large planning problems where the scope prohibits the feasibility of deterministic solvers, but the efficiency of these algorithms can be severely compromised in the presence of certain kinodynamics constraints. Obstacle fields with tunnels, or tubes are notoriously difficult, as are systems with differential constraints, because the tree grows inefficiently at the boundaries. Here we present a new sampling strategy for the RRT algorithm, based on an estimated feasibility set, which affords a dramatic improvement in performance in these severely constrained systems. We demonstrate the algorithm with a detailed look at the expansion of an RRT in a swingup task, and on path planning for a nonholonomic car.","Sampling methods,
State-space methods,
Power system planning,
Motion planning,
Costs,
Space technology,
Robotics and automation,
Path planning,
Robot motion,
Mobile robots"
Mismatch Patterns and Adaptation Aspects: A Foundation for Rapid Development of Web Service Adapters,"Standardization in Web services simplifies integration. However, it does not remove the need for adapters due to possible heterogeneity among service interfaces and protocols. In this paper, we characterize the problem of Web services adaptation focusing on business interfaces and protocols adapters. Our study shows that many of the differences between business interfaces and protocols are recurring. We introduce mismatch patterns to capture these recurring differences and to provide solutions to resolve them. We leverage mismatch patterns for service adaptation with two approaches: by developing stand-alone adapters and via service modification. We then dig into the notion of adaptation aspects that, following aspect-oriented programming paradigm and service modification approach, allow for rapid development of adapters. We present a study showing that it is a preferable approach in many cases. The proposed approach is implemented in a proof-of-concept prototype tool, and evaluated using both qualitative and quantitative methods.","Protocols,
Business,
Web services,
Context,
Strontium,
Runtime,
Database languages"
Delay with network coding and feedback,"We consider the problem of minimizing delay when broadcasting over erasure channels with feedback. A sender wishes to communicate the same set of µ messages to several receivers over separate erasure channels. The sender can broadcast a single message or a combination (encoding) of messages at each timestep. Receivers provide feedback as to whether the transmission was received. If at some time step a receiver cannot identify a new message, delay is incurred. Our notion of delay is motivated by real-time applications that request progressively refined input, such as the successive refinement of an image encoded using multiple description coding. Our setup is novel because it combines coding techniques with feedback information to the end of minimizing delay. It allows Θ(µ) benefits as compared to previous approaches for offline algorithms, while feedback allows online algorithms to achieve smaller delay than online algorithms without feedback. Our main complexity results are that the offline minimization problem is NP-hard when the sender only schedules single messages and that the general problem remains NP-hard even when coding is allowed. However we show that coding does offer delay and complexity gains over scheduling. We also discuss online heuristics and evaluate their performance through simulations.","Network coding,
Feedback,
Image coding,
Image reconstruction,
Delay effects,
Satellite broadcasting,
Road vehicles,
Data compression,
Computer networks,
Electronic mail"
Random Projection Trees for Vector Quantization,"A simple and computationally efficient scheme for tree-structured vector quantization is presented. Unlike previous methods, its quantization error depends only on the intrinsic dimension of the data distribution, rather than the apparent dimension of the space in which the data happen to lie.","Vector quantization,
Source coding,
Euclidean distance,
Statistical analysis,
Computer science,
Partitioning algorithms,
Statistics,
Machine learning,
Manifolds,
Algorithm design and analysis"
Sequential detection of cyclostationary signal for cognitive radio systems,"The cyclostationary feature detector is a viable candidate for a primary user (PU) detection method of the cognitive radio (CR) system. However, it requires very long detection time, which leads to inefficient spectrum utilization. To reduce the detection time, we propose to apply the sequential detection framework to the cyclostationary feature detector. Unfortunately, a straightforward application cannot achieve a sufficient gain, which is expected with the sequential detection. To solve this problem, we design a novel detector, taking account of the cyclic phase of the cyclostationary signal. The simulation results show that the proposed detector reduces the average detection time almost in half. The proposed detector can well be applied to the CR systems that operate in the frequency bands, where the PUs have long interarrival and sojourn time. For example, the CR systems equipped with the proposed detector can efficiently exploit the white space in the VHF/UHF TV bands.","Signal detection,
Cognitive radio,
Detectors,
Chromium,
Computer vision,
Signal design,
Phase detection,
Frequency,
White spaces,
TV"
An Efficient Privacy Preserving Keyword Search Scheme in Cloud Computing,"A user stores his personal files in a cloud, and retrieves them wherever and whenever he wants. For the sake of protecting the user data privacy and the user queries privacy, a user should store his personal files in an encrypted form in a cloud, and then sends queries in the form of encrypted keywords. However, a simple encryption scheme may not work well when a user wants to retrieve only files containing certain keywords using a thin client. First, the user needs to encrypt and decrypt files frequently, which depletes too much CPU capability and memory power of the client. Second, the service provider couldn't determine which files contain keywords specified by a user if the encryption is not searchable. Therefore, it can only return back all the encrypted files. A thin client generally has limited bandwidth, CPU and memory, and this may not be a feasible solution under the circumstances. In this paper, we investigate the characteristics of cloud computing and propose an efficient privacy preserving keyword search scheme in cloud computing. It allows a service provider to participate in partial decipherment to reduce a client's computational overhead, and enables the service provider to search the keywords on encrypted files to protect the user data privacy and the user queries privacy efficiently. By proof, our scheme is semantically secure.","Keyword search,
Cloud computing,
Cryptography,
Data privacy,
Protection,
Bandwidth,
Personal digital assistants,
Information science,
Computer science,
Information retrieval"
Different models for model matching: An analysis of approaches to support model differencing,"Calculating differences between models is an important and challenging task in Model Driven Engineering. Model differencing involves a number of steps starting with identifying matching model elements, calculating and representing their differences, and finally visualizing them in an appropriate way. In this paper, we provide an overview of the fundamental steps involved in the model differencing process and summarize the advantages and shortcomings of existing approaches for identifying matching model elements. To assist potential users in selecting one of the existing methods for the problem at stake, we investigate the trade-offs these methods impose in terms of accuracy and effort required to implement each one of them.",
Initializing Partition-Optimization Algorithms,"Clustering datasets is a challenging problem needed in a wide array of applications. Partition-optimization approaches, such as k-means or expectation-maximization (EM) algorithms, are sub-optimal and find solutions in the vicinity of their initialization. This paper proposes a staged approach to specifying initial values by finding a large number of local modes and then obtaining representatives from the most separated ones. Results on test experiments are excellent. We also provide a detailed comparative assessment of the suggested algorithm with many commonly-used initialization approaches in the literature. Finally, the methodology is applied to two datasets on diurnal microarray gene expressions and industrial releases of mercury.","Partitioning algorithms,
Degradation,
Gene expression,
Iterative algorithms,
Public healthcare,
Testing,
Proteins,
Singular value decomposition,
Clustering algorithms,
Minimization methods"
Implicit human-centered tagging [Social Sciences],"Tagging is the annotation of multimedia data with userspecified keywords known as tags, with the aim of facilitating fast and accurate data retrieval based on these tags. In contrast to this process, also referred to as explicit tagging, implicit human-centered tagging (IHCT) refers to exploiting the information on user's nonverbal reactions (e.g., facial expressions like smiles or head gestures like shakes) to multimedia data, with which he or she interacts, to assign new or improve the existing tags associated with the target data. Thus, implicit tagging allows that a data item gets tagged each time a user interacts with it based on the reactions of the user to the data (e.g., laughter when seeing a funny video), in contrast to explicit tagging paradigm in which a data item gets tagged only if a user is requested (or chooses) to associate tags with it. As nonverbal reactions to observed multimedia are displayed naturally and spontaneously, no purposeful explicit action (effort) is required from the user; hence, the resulting tagging process is said to be ""implicit"" and ""human centered"" (in contrast to being dictated by computer and being ""computer-centered"").","Tagging,
Information retrieval,
Humans,
Indexing,
Computer displays,
Robustness,
Internet,
Computer networks,
Time sharing computer systems,
Collaboration"
The Virtual Mirror: A New Interaction Paradigm for Augmented Reality Environments,"Medical augmented reality (AR) has been widely discussed within the medical imaging as well as computer aided surgery communities. Different systems for exemplary medical applications have been proposed. Some of them produced promising results. One major issue still hindering AR technology to be regularly used in medical applications is the interaction between physician and the superimposed 3-D virtual data. Classical interaction paradigms, for instance with keyboard and mouse, to interact with visualized medical 3-D imaging data are not adequate for an AR environment. This paper introduces the concept of a tangible/controllable virtual mirror for medical AR applications. This concept intuitively augments the direct view of the surgeon with all desired views on volumetric medical imaging data registered with the operation site without moving around the operating table or displacing the patient. We selected two medical procedures to demonstrate and evaluate the potentials of the Virtual Mirror for the surgical workflow. Results confirm the intuitiveness of this new paradigm and its perceptive advantages for AR-based computer aided interventions.","Mirrors,
Augmented reality,
Biomedical imaging,
Surgery,
Medical services,
Biomedical equipment,
Keyboards,
Mice,
Data visualization,
Surges"
An Efficient Directed Localization Recursion Protocol for Wireless Sensor Networks,"The establishment of a localization system is an important task in wireless sensor networks. Due to the geographical correlation between sensed data, location information is commonly used to name the gathered data and address nodes and regions in data dissemination protocols. In general, to estimate its location, a node needs the position information of at least three reference points (neighbors that know their positions). In this work, we propose a different scheme in which only two reference points are required in order to estimate a position. To choose between the two possible solutions of an estimate, we use the known direction of the recursion. This approach leads to a recursive localization system that works with low-density networks (increasing by 40 percent the number of nodes with estimates in some cases), reduces the position error by almost 30 percent, requires 37 percent less processor resources to estimate a position, uses fewer beacon nodes, and also indicates the node position error based on its distance to the recursion origin. No GPS-enabled node is required, since the recursion origin can be used as a relative coordinate system. The algorithm's evaluation is performed by comparing it with a similar localization system; also, experiments are made to evaluate the impact of both systems in geographic algorithms.","Wireless sensor networks,
Distance measurement,
Biomedical monitoring,
Algorithm design and analysis,
Monitoring,
Protocols,
Floods"
A Generalization of LSB Matching,"Recently, a significant improvement of the well-known least significant bit (LSB) matching steganography has been proposed, reducing the changes to the cover image for the same amount of embedded secret data. When the embedding rate is 1, this method decreases the expected number of modification per pixel (ENMPP) from 0.5 to 0.375. In this letter, we propose the so-called generalized LSB matching (G-LSB-M) scheme, which generalizes this method and LSB matching. The lower bound of ENMPP for G-LSB-M is investigated, and a construction of G-LSB-M is presented by using the sum and difference covering set of finite cyclic group. Compared with the previous works, we show that the suitable G-LSB-M can further reduce the ENMPP and lead to more secure steganographic schemes. Experimental results illustrate clearly the better resistance to steganalysis of G-LSB-M.","Steganography,
Payloads,
Signal processing algorithms,
Error correction codes,
Upper bound,
Digital images,
Pixel,
Computer science,
Mathematics"
Many-to-Many Disjoint Path Covers in the Presence of Faulty Elements,"A many-to-many k-disjoint path cover (k-DPC) of a graph G is a set of k disjoint paths joining k sources and k sinks in which each vertex of G is covered by a path. It is called a paired many-to-many disjoint path cover when each source should be joined to a specific sink, and it is called an unpaired many-to-many disjoint path cover when each source can be joined to an arbitrary sink. In this paper, we discuss about paired and unpaired many-to-many disjoint path covers including their relationships, application to strong Hamiltonicity, and necessary conditions. And then, we give a construction scheme for paired many-to-many disjoint path covers in the graph H0 oplus H1 obtained from connecting two graphs H0 and H1 with |V(H0)| = |V(H1)| by |V(H1)| pairwise nonadjacent edges joining vertices in H0 and vertices in H1, where H0 = G0 oplus G1 and H1 = G2 oplus G3 for some graphs Gj. Using the construction, we show that every m-dimensional restricted HL-graph and recursive circulant G(2m, 4) with f or less faulty elements have a paired k-DPC for any f and k ges 2 with f + 2k les m.","Construction industry,
Multiprocessor interconnection,
Electronic mail,
Bipartite graph,
Educational institutions,
Computer science,
Joining processes"
Carbon Nanotube circuits in the presence of carbon nanotube density variations,"Carbon nanotubes (CNTs) are grown using chemical synthesis. As a result, it is extremely difficult to ensure exact positioning and uniform density of CNTs. Density variations in CNT growth can compromise reliability of carbon nanotube field effect transistor (CNFET) circuits, and result in increased delay variations. A parameterized model for CNT density variations is presented based on experimental data extracted from aligned CNT growth. This model is used to quantify the impact of such variations on design metrics such as noise margin and delay variations of CNFET circuits. Finally, we analyze correlation that exists in aligned CNT growth, and demonstrate how the reliability of CNFET circuits can be significantly improved by taking advantage of such correlation.","Carbon nanotubes,
Circuit synthesis,
Scanning electron microscopy,
Circuit noise,
Atomic force microscopy,
Permission,
CNTFETs,
Delay effects,
Circuit analysis,
Noise reduction"
Consistency Management Strategies for Data Replication in Mobile Ad Hoc Networks,"In a mobile ad hoc network, data replication drastically improves data availability. However, since mobile hosts' mobility causes frequent network partitioning, consistency management of data operations on replicas becomes a crucial issue. In such an environment, the global consistency of data operations on replicas is not desirable by many applications. Thus, new consistency maintenance based on local conditions such as location and time need to be investigated. This paper attempts to classify different consistency levels according to requirements from applications and provides protocols to realize them. We report simulation results to investigate the characteristics of these consistency protocols in a mobile ad hoc network.","Mobile ad hoc networks,
Protocols,
Mobile computing,
Computational modeling,
Ad hoc networks,
Computer network management,
Computer networks,
Disaster management,
System performance,
Proposals"
Constructing implicit 3D shape models for pose estimation,"We present a system that constructs “implicit shape models” for classes of rigid 3D objects and utilizes these models to estimating the pose of class instances in single 2D images. We use the framework of implicit shape models to construct a voting procedure that allows for 3D transformations and projection and accounts for self occlusion. The model is comprised of a collection of learned features, their 3D locations, their appearances in different views, and the set of views in which they are visible. We further learn the parameters of a model from training images by applying a method that relies on factorization. We demonstrate the utility of the constructed models by applying them in pose estimation experiments to recover the viewpoint of class instances.","Shape,
Solid modeling,
Voting,
Deformable models,
Computer vision,
Computer science,
Image reconstruction,
Machine vision,
Buildings,
Laboratories"
Optimizing the Throughput of Data-Driven Peer-to-Peer Streaming,"During recent years, the Internet has witnessed a rapid growth in deployment of data-driven (or swarming based) peer-to-peer (P2P) media streaming. In these applications, each node independently selects some other nodes as its neighbors (i.e. gossip-style overlay construction), and exchanges streaming data with the neighbors (i.e. data scheduling). To improve the performance of such protocol, many existing works focus on the gossip-style overlay construction issue. However, few of them concentrate on optimizing the streaming data scheduling to maximize the throughput of a constructed overlay. In this paper, we analytically study the scheduling problem in data-driven streaming system and model it as a classical min-cost network flow problem. We then propose both the global optimal scheduling scheme and distributed heuristic algorithm to optimize the system throughput. Furthermore, we introduce layered video coding into data-driven protocol and extend our algorithm to deal with the end-host heterogeneity. The results of simulation with the real world traces indicate that our distributed algorithm significantly outperforms conventional ad hoc scheduling strategies especially in stringent buffer and bandwidth constraints.","Throughput,
Peer to peer computing,
Optimal scheduling,
Streaming media,
Protocols,
Internet,
Heuristic algorithms,
Scheduling algorithm,
Video coding,
Distributed algorithms"
Robust sensor-based grasp primitive for a three-finger robot hand,This paper addresses the problem of robot grasping in conditions of uncertainty. We propose a grasp controller that deals robustly with this uncertainty using feedback from different contact-based sensors. This controller assumes a description of grasp consisting of a primitive that only determines the initial configuration of the hand and the control law to be used. We exhaustively validate the controller by carrying out a large number of tests with different degrees of inaccuracy in the pose of the target objects and by comparing it with results of a naive grasp controller.,"Robustness,
Robot sensing systems,
Intelligent robots,
Uncertainty,
Intelligent sensors,
Feedback,
Mobile robots,
Shape,
Force sensors,
Robust control"
A systematic review of software maintainability prediction and metrics,This paper presents the results of a systematic review conducted to collect evidence on software maintainability prediction and metrics. The study was targeted at the software quality attribute of maintainability as opposed to the process of software maintenance. The evidence was gathered from the selected studies against a set of meaningful and focused questions. 710 studies were initially retrieved; however of these only 15 studies were selected; their quality was assessed; data extraction was performed; and data was synthesized against the research questions. Our results suggest that there is little evidence on the effectiveness of software maintainability prediction techniques and models.,"Software maintenance,
Software systems,
Costs,
Software quality,
Strontium,
Software measurement,
State estimation,
Software engineering,
Predictive models,
Software performance"
CCF-LRU: a new buffer replacement algorithm for flash memory,"NAND flash memory has been widely used for storage in embedded systems and recently in enterprise computing environment, owing to its shock-resistance, nonvolatile, low energy consumption, and high I/O speed. In addition, flash memory has the characteristics of not-in-place update and asymmetric I/O costs among read, write, and erase operations, in which the cost of write/erase operations is much higher than that of read operation. Hence, the buffer replacement algorithms in flash-based systems should take the asymmetric I/O costs into account. Previous solutions to this issue, such as CFLRU and LRU-WSR, used a clean-first scheme to first evict clean pages from the buffer. Although they outperform the traditional LRU policy in performance, they do not consider the access frequency of clean page, which will consequently result in poor I/O performance. In order to solve this problem, we present a new buffering algorithm in this paper, called CCF-LRU, which enhances the previous CFLRU and LRU-WSR methods by differentiating clean pages into cold and hot ones, and evicting cold clean pages first and delaying the eviction of hot clean pages. We conduct a trace-driven experiment on a flash memory simulation environment, and use six types of synthesized traces as well as a real OLTP trace. The results show that the CCF-LRU algorithm outperforms LRU, CFLRU and LRUWSR in write count and runtime in all experiments. In particular, it provides more than 20% improvement over its competitors both in write count and runtime, when running the real OLTP trace.","Flash memory,
Costs,
Runtime,
Energy storage,
Embedded system,
Embedded computing,
Nonvolatile memory,
Energy consumption,
Frequency,
Delay"
Generalized velocity obstacles,"We address the problem of real-time navigation in dynamic environments for car-like robots. We present an approach to identify controls that will lead to a collision with a moving obstacle at some point in the future. Our approach generalizes the concept of velocity obstacles, which have been used for navigation among dynamic obstacles, and takes into account the constraints of a car-like robot. We use this formulation to find controls that will allow collision free navigation in dynamic environments. Finally, we demonstrate the performance of our algorithm on a simulated car-like robot among moving obstacles.","Fingers,
Grasping,
Shape control,
Control systems,
Tactile sensors,
Information science,
Intelligent robots,
USA Councils,
Stability analysis,
Numerical simulation"
A compressive sensing approach for expression-invariant face recognition,"We propose a novel technique based on compressive sensing for expression invariant face recognition. We view the different images of the same subject as an ensemble of intercorrelated signals and assume that changes due to variation in expressions are sparse with respect to the whole image. We exploit this sparsity using distributed compressive sensing theory, which enables us to grossly represent the training images of a given subject by only two feature images: one that captures the holistic (common) features of the face, and the other that captures the different expressions in all training samples. We show that a new test image of a subject can be fairly well approximated using only the two feature images from the same subject. Hence we can drastically reduce the storage space and operational dimensionality by keeping only these two feature images or their random measurements. Based on this, we design an efficient expression invariant classifier. Furthermore, we show that substantially low dimensional versions of the training features, such as ones extracted from critically downsampled training images, or low dimensional random projection of original feature images, still have sufficient information for good classification. Extensive experiments with publically available databases show that, on average, our approach performs better than the state of the art despite using only such super compact feature representation.","Face recognition,
Feature extraction,
Independent component analysis,
Spatial databases,
Algorithm design and analysis,
Image coding,
Image storage,
Linear discriminant analysis,
Computer science,
Lighting"
Security against hardware Trojan through a novel application of design obfuscation,"Malicious hardware Trojan circuitry inserted in safety-critical applications is a major threat to national security. In this work, we propose a novel application of a key-based obfuscation technique to achieve security against hardware Trojans. The obfuscation scheme is based on modifying the state transition function of a given circuit by expanding its reachable state space and enabling it to operate in two distinct modes - the normal mode and the obfuscated mode. Such a modification obfuscates the rareness of the internal circuit nodes, thus making it difficult for an adversary to insert hard-to-detect Trojans. It also makes some inserted Trojans benign by making them activate only in the obfuscated mode. The combined effect leads to higher Trojan detectability and higher level of protection against such attack. Simulation results for a set of benchmark circuits show that the scheme is capable of achieving high levels of security at modest design overhead.","Hardware,
Information security,
Protection,
National security,
State-space methods,
Circuit simulation,
Circuit testing,
Permission,
Computational modeling,
Logic design"
Localization and Segmentation of Left Ventricle in Cardiac Cine-MR Images,"Accurate delineation of the left ventricular myocardial boundaries on cardiac cine magnetic resonance (MR) images is essential for volumetric and functional cardiac analysis. Automated myocardial contour delineation often suffers from misalignment of slices, nonuniform coil sensitivity, blood-flow-related inter- and intraslice intensity inhomogeneities, blurring due to motion, partial voluming, and a need to circumscribe the papillary muscles and the trabeculae. In this paper, we propose a novel method for data-driven localization and segmentation of the left ventricle in the cine-MR images toward automated computation of ejection fraction (EF). Our hybrid segmentation method combines intensity- and texture-based fuzzy affinity maps obtained from a novel multiclass, multifeature fuzzy connectedness method with dynamic-programming-based boundary detection to delineate the myocardial contours. Bland-Altman analysis indicates that the mean biases of the end-diastolic volume, end-systolic volume, and EF estimates of our method are comparable to the interobserver variability when compared with the annotations from two experts.","Image segmentation,
Myocardium,
Heart,
Magnetic resonance,
Biomedical computing,
Coils,
Muscles,
Magnetic resonance imaging,
Computer science,
Biomedical imaging"
Link Prediction on Evolving Data Using Matrix and Tensor Factorizations,"The data in many disciplines such as social networks, web analysis, etc. is link-based, and the link structure can be exploited for many different data mining tasks. In this paper, we consider the problem of temporal link prediction: Given link data for time periods 1 through T, can we predict the links in time period T +1? Specifically, we look at bipartite graphs changing over time and consider matrix- and tensor-based methods for predicting links. We present a weight-based method for collapsing multi-year data into a single matrix. We show how the well-known Katz method for link prediction can be extended to bipartite graphs and, moreover, approximated in a scalable way using a truncated singular value decomposition. Using a CANDECOMP/PARAFAC tensor decomposition of the data, we illustrate the usefulness of exploiting the natural three-dimensional structure of temporal link data. Through several numerical experiments, we demonstrate that both matrix and tensor-based techniques are effective for temporal link prediction despite the inherent difficulty of the problem.","Tensile stress,
Matrix decomposition,
Informatics,
Laboratories,
Social network services,
Bipartite graph,
Data mining,
Computer science,
Singular value decomposition,
Collaboration"
Feature Selection with High-Dimensional Imbalanced Data,"Feature selection is an important topic in data mining, especially for high dimensional datasets. Filtering techniques in particular have received much attention, but detailed comparisons of their performance is lacking. This work considers three filters using classifier performance metrics and six commonly-used filters. All nine filtering techniques are compared and contrasted using five different microarray expression datasets. In addition, given that these datasets exhibit an imbalance between the number of positive and negative examples, the utilization of sampling techniques in the context of feature selection is examined.","Data mining,
Measurement,
Sampling methods,
Conferences,
Computer science,
USA Councils,
Diversity reception,
Information filtering,
Information filters,
Data analysis"
Automated Quantitative Assessment of HER-2/neu Immunohistochemical Expression in Breast Cancer,"The expression of the HER-2/neu (HER2) gene, a member of the epidermal growth factor receptor family, has been shown to be a valuable prognostic indicator for breast cancer. However, interobserver variability has been reported in the evaluation of HER2 with immunohistochemistry. It has been suggested that automated computer-based evaluation can provide a consistent and objective evaluation of HER2 expression. In this manuscript, we present an automated method for the quantitative assessment of HER2 using digital microscopy. The method processes microscopy images from tissue slides with a multistage algorithm, including steps of color pixel classification, nuclei segmentation, and cell membrane modeling, and extracts quantitative, continuous measures of cell membrane staining intensity and completeness. A minimum cluster distance classifier merges the features to classify the slides into HER2 categories. An evaluation based on agreement analysis with pathologist-derived HER2 scores, showed good agreement with the provided truth. Agreement varied within the different classes with highest agreement (up to 90%) for positive (3+) slides, and lowest agreement (72%-78%) for equivocal (2+) slides which contained ambiguous scoring. The developed automated method has the potential to be used as a computer aid for the immunohistochemical evaluation of HER2 expression with the objective of increasing observer reproducibility.","Breast cancer,
Microscopy,
Cells (biology),
Biomembranes,
Epidermis,
Clustering algorithms,
Color,
Pixel,
Image segmentation,
Nuclear measurements"
Application possibilities of artificial neural networks for recognizing partial discharges measured by the acoustic emission method,"The genesis of the research work presented in this paper constitutes the issue of the effective and efficient recognition of single-source one-time partial discharge forms that can occur in insulation systems of power transformers. The paper presents research results referring to the use of single-direction artificial neural networks for recognizing basic partial discharge forms that can occur in paper-oil insulation impaired by aging processes. The research work results presented show the recognition effectiveness of basic partial discharge forms depending on the descriptor of the analysis of the acoustic emission signal analysis. The detailed cognitive aim was selection of input parameters and an artificial neural network which would be the best, considering recognition effectiveness and processing time, and which could be used as a classifier in an expert diagnostic system making identification of partial discharges measured by using the acoustic method possible.",
Efficient MAC in cognitive radio systems: A game-theoretic approach,"In this paper, we study the problem of efficient medium access control (MAC) among cognitive radio devices that are equipped with multiple radios and thus are capable of transmitting simultaneously at different frequencies (channels). We assume that radios contend on each channel using the carrier sense multiple access with collision avoidance (CSMA/CA) protocol. We study two MAC problems: (i) the allocation of the available channels among radios, and (ii) the optimal usage of each allocated channel by the radios occupying it. Both problems are studied in a game-theoretic setting, where devices aim to selfishly maximize their share of the available bandwidth. As for the first problem, we show that the ldquoprice of anarchyrdquo is close to 1, that is, Nash equilibria imply nearly system optimal allocations of the available channels. For the second problem, we design a game such that it admits a unique Nash equilibrium that is is both fair and Pareto-optimal. Furthermore, we propose simple mechanisms that enable selfish cognitive radio devices not only to coordinate efficiently on the available channels but also to optimally use every single allocated channel.","Cognitive radio,
Frequency,
Access protocols,
Multiaccess communication,
Bandwidth,
Nash equilibrium,
Wireless communication,
Communications technology,
Communication system control,
Media Access Protocol"
Fast algorithms for recovering a corrupted low-rank matrix,"This paper studies algorithms for solving the problem of recovering a low-rank matrix with a fraction of its entries arbitrarily corrupted. This problem can be viewed as a robust version of classical PCA, and arises in a number of application domains, including image processing, web data ranking, and bioinformatic data analysis. It was recently shown that under surprisingly broad conditions, it can be exactly solved via a convex programming surrogate that combines nuclear norm minimization and ℓ1-norm minimization. This paper develops and compares two complementary approaches for solving this convex program. The first is an accelerated proximal gradient algorithm directly applied to the primal; while the second is a gradient algorithm applied to the dual problem. Both are several orders of magnitude faster than the previous state-of-the-art algorithm for this problem, which was based on iterative thresholding. Simulations demonstrate the performance improvement that can be obtained via these two algorithms, and clarify their relative merits.","Principal component analysis,
Iterative algorithms,
Sparse matrices,
Convergence,
Robustness,
Data analysis,
Conferences,
Asia,
Computers,
Mathematics"
Engineering the Software for Understanding Climate Change,"Climate scientists build large, complex simulations with little or no software engineering training—and don't readily adopt the latest software engineering tools and techniques. This ethnographic study of climate scientists shows that their culture and practices share many features of agile and open source projects, but with highly customized software validation and verification techniques.","Programming,
Open source software,
Software engineering,
Computational modeling,
Software quality,
High performance computing,
Physics computing,
Software performance,
Software maintenance,
Earth"
Building text features for object image classification,"We introduce a text-based image feature and demonstrate that it consistently improves performance on hard object classification problems. The feature is built using an auxiliary dataset of images annotated with tags, downloaded from the Internet. We do not inspect or correct the tags and expect that they are noisy. We obtain the text feature of an unannotated image from the tags of its k-nearest neighbors in this auxiliary collection. A visual classifier presented with an object viewed under novel circumstances (say, a new viewing direction) must rely on its visual examples. Our text feature may not change, because the auxiliary dataset likely contains a similar picture. While the tags associated with images are noisy, they are more stable when appearance changes. We test the performance of this feature using PASCAL VOC 2006 and 2007 datasets. Our feature performs well, consistently improves the performance of visual object classifiers, and is particularly effective when the training dataset is small.","Image classification,
Internet,
Dogs,
Testing,
Layout,
Text categorization,
Computer science,
Positron emission tomography,
Animals,
Histograms"
Bayesian Poisson regression for crowd counting,"Poisson regression models the noisy output of a counting function as a Poisson random variable, with a log-mean parameter that is a linear function of the input vector. In this work, we analyze Poisson regression in a Bayesian setting, by introducing a prior distribution on the weights of the linear function. Since exact inference is analytically unobtainable, we derive a closed-form approximation to the predictive distribution of the model. We show that the predictive distribution can be kernelized, enabling the representation of non-linear log-mean functions. We also derive an approximate marginal likelihood that can be optimized to learn the hyperparameters of the kernel. We then relate the proposed approximate Bayesian Poisson regression to Gaussian processes. Finally, we present experimental results using Bayesian Poisson regression for crowd counting from low-level features.","Bayesian methods,
Layout,
Lighting,
Least squares methods,
Light sources,
Least squares approximation,
Automation,
Educational institutions,
Information science,
Geometry"
VRB modeling for storage in stand-alone wind energy systems,"This paper proposes into determining an appropriate electrical vanadium redox flow battery (VRB) model and its integration with a typical stand-alone wind energy system during wind speed variation as well as transient performance under variable load. The investigated system consists of a 3 kW variable speed wind turbine with permanent magnet synchronous generator (PMSG), diode rectifier bridge, buck-boost converter, bidirectional charge controller, transformer, inverter, AC loads and VRB. Vanadium redox batteries are well suited for this type of application because of their high efficiency, high scalability, fast response, long life and low maintenance requirements.","Energy storage,
Wind energy,
Batteries,
Wind speed,
Wind turbines,
Permanent magnets,
Synchronous generators,
Diodes,
Rectifiers,
Bridges"
Analysis of LLC Resonant Converter considering effects of parasitic components,"Nowadays the trend of power supply market is more inclined to high switching frequency, high efficiency and high power density. To meet this trend, resonant power supply holds more attraction, because it can be operated in high switching frequency with high efficiency. There are many resonant power supplies such as Series-Resonant Converter (SRC), Parallel-Resonant Converter (PRC) and Series-Parallel Resonant Converter (SPRC). Among them, LLC Resonant Converter has a lot of advantages over the conventional SRC and PRC considering relatively narrow switching frequency variation over wide input and load variation and Zero-Voltage-Switching for entire load range. Therefore, the LLC Resonant Converter has been widely used and discussed. However, the conventional analysis of LLC Resonant Converter with Fundamental Harmonic Approximation (FHA) can not explain the practical operation of LLC Resonant Converter. To overcome this limitation, in this paper, analysis and design of the LLC Resonant Converter including parasitic components which are affecting converter operation are proposed using a traditional analysis based on FHA. The effect of each parasitic component is analyzed with simulation results and the design guideline standing on this analysis will be described. Moreover, the experimental results of prototype designing on the basis of the analysis are shown to demonstrate the proposed analysis and design guideline.","Resonance,
Power supplies,
Switching frequency,
Partial response channels,
Guidelines,
Switching converters,
Load management,
Harmonic analysis,
Analytical models,
Prototypes"
Fast Large-Tip-Angle Multidimensional and Parallel RF Pulse Design in MRI,"Large-tip-angle multidimensional radio-frequency (RF) pulse design is a difficult problem, due to the nonlinear response of magnetization to applied RF at large tip-angles. In parallel excitation, multidimensional RF pulse design is further complicated by the possibility for transmit field patterns to change between subjects, requiring pulses to be designed rapidly while a subject lies in the scanner. To accelerate pulse design, we introduce a fast version of the optimal control method for large-tip-angle parallel excitation. The new method is based on a novel approach to analytically linearizing the Bloch equation about a large-tip-angle RF pulse, which results in an approximate linear model for the perturbations created by adding a small-tip-angle pulse to a large-tip-angle pulse. The linear model can be evaluated rapidly using nonuniform fast Fourier transforms, and we apply it iteratively to produce a sequence of pulse updates that improve excitation accuracy. We achieve drastic reductions in design time and memory requirements compared to conventional optimal control, while producing pulses of similar accuracy. The new method can also compensate for nonidealities such as main field inhomogeneties.","Multidimensional systems,
Radio frequency,
Magnetic resonance imaging,
Equations,
Nonuniform electric fields,
Optimal control,
Linear approximation,
Fast Fourier transforms,
Information systems,
Algorithm design and analysis"
A post-disaster mobility model for Delay Tolerant Networking,"Delay Tolerant Networking (DTN) considers how to provide communication in contexts where it is unreasonable to assume end-to-end connectivity. Network devices exchange buffered messages when they come into communication range; messages may be buffered and carried physically several times before ultimately being received. Service characteristics of a DTN depend intimately on the underlying movement of devices through physical space; correspondingly, an assessment of DTN technology (e.g. routing protocols, message exchange policies, etc.) depends on that same movement. Existing mobility models provided in simulators lack characteristics one expects in post-disaster communication. We propose a mobility model that includes the impact of the disaster on the transportation network, and that models population and relief vehicle movement. We augment the “Opportunistic Network Environment” (ONE) simulator of DTNs with required extensions and show that characteristics of the DTN are very different using the new model than it is under models that ONE currently provides.","Disruption tolerant networking,
Computer science,
Space technology,
Routing protocols,
Vehicles,
Humans,
Computational modeling,
Context,
Transportation,
IP networks"
Remote Laboratories Extending Access to Science and Engineering Curricular,"This paper draws on research, development, and deployment of remote laboratories undertaken by the authors since 2000. They jointly worked on the PEARL project (http://iet.open.ac.uk/pearl/) from 2000 to 2003 and have worked on further projects within their own institutions (the Open University, United Kingdom, and the University of Porto, Portugal, respectively) since then. The paper begins with a statement of the rationale for remote experiments, then offers a review of past work of the authors and highlights the key lessons for remote labs drawn from this. These lessons include (1) the importance of removing accessibility barriers, (2) the importance of a pedagogic strategy, (3) evaluation of pedagogic effectiveness, (4) the ease of automation or remote control, and (5) learning objectives and design decisions. The paper then discusses key topics including assessment issues, instructional design, pedagogical strategies, relations to industry, and cost benefits. A conclusion summarizes key points from the paper within a review of the current status of remote labs in education.","Training,
Remote laboratories,
Object recognition,
Educational institutions,
Data mining"
Data-driven grasping with partial sensor data,"To grasp a novel object, we can index it into a database of known 3D models and use precomputed grasp data for those models to suggest a new grasp. We refer to this idea as data-driven grasping, and we have previously introduced the Columbia Grasp Database for this purpose. In this paper we demonstrate a data-driven grasp planner that requires only partial 3D data of an object in order to grasp it. To achieve this, we introduce a new shape descriptor for partial 3D range data, along with an alignment method that can rigidly register partial 3D models to models that are globally similar but not identical. Our method uses SIFT features of depth images, and encapsulates “nearby” views of an object in a compact shape descriptor.","Spatial databases,
Solid modeling,
Shape measurement,
Image databases,
Geometry,
Robot kinematics,
Sensor systems and applications,
Intelligent robots,
USA Councils,
Intelligent sensors"
Magneto-Optical Tracking of Flexible Laparoscopic Ultrasound: Model-Based Online Detection and Correction of Magnetic Tracking Errors,"Electromagnetic tracking is currently one of the most promising means of localizing flexible endoscopic instruments such as flexible laparoscopic ultrasound transducers. However, electromagnetic tracking is also susceptible to interference from ferromagnetic material, which distorts the magnetic field and leads to tracking errors. This paper presents new methods for real-time online detection and reduction of dynamic electromagnetic tracking errors when localizing a flexible laparoscopic ultrasound transducer. We use a hybrid tracking setup to combine optical tracking of the transducer shaft and electromagnetic tracking of the flexible transducer tip. A novel approach of modeling the poses of the transducer tip in relation to the transducer shaft allows us to reliably detect and significantly reduce electromagnetic tracking errors. For detecting errors of more than 5 mm, we achieved a sensitivity and specificity of 91% and 93%, respectively. Initial 3-D rms error of 6.91 mm were reduced to 3.15 mm.","Laparoscopes,
Ultrasonic imaging,
Error correction,
Ultrasonic transducers,
Optical distortion,
Shafts,
Instruments,
Electromagnetic fields,
Magnetic susceptibility,
Electromagnetic interference"
Stereoscopic image quality prediction,"Three-dimensional (3D) imaging has attracted considerable attention recently due to its increasingly wide range of applications. Consequently, perceived quality is a great important issue to assess the performance of all 3D imaging applications. Perceived distortion and depth of any stereoscopic images are strongly dependent on the local features, such as edge, flat and texture. In this paper, we propose an noreference (NR) perceptual quality assessment for JPEG coded stereoscopic images based on segmented local features of artifacts and disparity. The local features information of stereoscopic pair images such as edge, flat and texture areas and also the blockiness and zero crossing rate within the block of the images are evaluated for artifacts and disparity in this method. The result on our subjective stereoscopic images database indicates that the model performs quite well over a wide rang of image content and distortion levels.","Image quality,
Image coding,
Quality assessment,
Cameras,
Rendering (computer graphics),
Bandwidth,
Image storage,
Material storage,
Low pass filters,
Filtering"
Blueshift: Designing processors for timing speculation from the ground up.,"Several recent processor designs have proposed to enhance performance by increasing the clock frequency to the point where timing faults occur, and by adding error-correcting support to guarantee correctness. However, such Timing Speculation (TS) proposals are limited in that they assume traditional design methodologies that are suboptimal under TS. In this paper, we present a new approach where the processor itself is designed from the ground up for TS. The idea is to identify and optimize the most frequently-exercised critical paths in the design, at the expense of the majority of the static critical paths, which are allowed to suffer timing errors. Our approach and design optimization algorithm are called BlueShift. We also introduce two techniques that, when applied under BlueShift, improve processor performance: On-demand Selective Biasing (OSB) and Path Constraint Tuning (PCT). Our evaluation with modules from the OpenSPARC T1 processor shows that, compared to conventional TS, BlueShift with OSB speeds up applications by an average of 8% while increasing the processor power by an average of 12%. Moreover, compared to a high-performance TS design, BlueShift with PCT speeds up applications by an average of 6% with an average processor power overhead of 23% . providing a way to speed up logic modules that is orthogonal to voltage scaling.","Process design,
Timing,
Design optimization,
Clocks,
Frequency,
Error correction,
Proposals,
Design methodology,
Logic,
Voltage"
Performance Analysis of Amplify-and-Forward Cooperative Networks with Relay Selection over Rayleigh Fading Channels,"A performance analysis for cooperative diversity system with best relay selection over Rayleigh fading channels is presented. We obtain analytical expressions for the probability density function (PDF), cumulative density function (CDF), and the moment generating function (MGF) of end-to-end SNR of the system under study. Using these expressions we derive closed-form expressions for the average symbol error rate (SER), the outage probability and the average end-to-end SNR gain obtained form relay selection. Using numerical simulations and calculation of the mathematical expressions, the performances of different cases are evaluated and compared to show the significant advantages of the relay selection in a cooperative communication.","Performance analysis,
Relays,
Fading,
Wireless sensor networks,
Binary phase shift keying,
Closed-form solution,
Error analysis,
Decoding,
Receiving antennas,
Computer science"
Elastic scaling of data parallel operators in stream processing,"We describe an approach to elastically scale the performance of a data analytics operator that is part of a streaming application. Our techniques focus on dynamically adjusting the amount of computation an operator can carry out in response to changes in incoming workload and the availability of processing cycles. We show that our elastic approach is beneficial in light of the dynamic aspects of streaming workloads and stream processing environments. Addressing another recent trend, we show the importance of our approach as a means to providing computational elasticity in multicore processor-based environments such that operators can automatically find their best operating point. Finally, we present experiments driven by synthetic workloads, showing the space where the optimizing efforts are most beneficial and a radioastronomy imaging application, where we observe substantial improvements in its performance-critical section.","Data analysis,
Availability,
Streaming media,
Intelligent sensors,
Runtime,
Computer science,
Performance analysis,
Application software,
Elasticity,
Multicore processing"
Tackling high dimensional nonseparable optimization problems by cooperatively coevolving particle swarms,"This paper attempts to address the question of scaling up Particle Swarm Optimization (PSO) algorithms to high dimensional optimization problems. We present a cooperative coevolving PSO (CCPSO) algorithm incorporating random grouping and adaptive weighting, two techniques that have been shown to be effective for handling high dimensional nonseparable problems. The proposed CCPSO algorithms out-performed a previously developed coevolving PSO algorithm on nonseparable functions of 30 dimensions. Furthermore, the scalability of the proposed algorithm to high dimensional nonseparable problems (of up to 1000 dimensions) is examined and compared with two existing coevolving Differential Evolution (DE) algorithms, and new insights are obtained. Our experimental results show the proposed CCPSO algorithms can perform reasonably well with only a small number of evaluations. The results also suggest that both the random grouping and adaptive weighting schemes are viable approaches that can be generalized to other evolutionary optimization methods.","Particle swarm optimization,
Computer science,
Testing,
Scalability,
Performance evaluation,
Optimization methods,
Stochastic processes,
Evolutionary computation,
Australia"
How tagging helps bridge the gap between social and technical aspects in software development,"Empirical research on collaborative software development practices indicates that technical and social aspects of software development are often intertwined. The processes followed are tacit and constantly evolving, thus not all of them are amenable to formal tool support. In this paper, we explore how “tagging”, a lightweight social computing mechanism, is used to bridge the gap between technical and social aspects of managing work items. We present the results from an empirical study on how tagging has been adopted and adapted over the past two years of a large project with 175 developers. Our research shows that the tagging mechanism was eagerly adopted by the team, and that it has become a significant part of many informal processes. Our findings indicate that lightweight informal tool support, prevalent in the social computing domain, may play an important role in improving team-based software development practices.","Tagging,
Bridges,
Programming,
Software tools,
Collaborative software,
Collaborative work,
Social network services,
Software development management,
Computer science,
Humans"
Kinodynamic motion planning for mobile robots using splines,"This paper presents an approach to time-optimal kinodynamic motion planning for a mobile robot. A global path planner is used to generate collision-free straight-line paths from the robot's position to a given goal location. With waypoints of this path, an initial trajectory is generated which defines the planned position of the robot over time. A velocity profile is computed that accounts for constraints on the velocity and acceleration of the robot. The trajectory is refined to minimize the time needed for traversal by an any-time optimization algorithm. An error-feedback controller generates motor commands to execute the planned trajectory. Quintic Bézier splines are used to allow for curvature-continuous joins of trajectory segments, which enables the system to replan trajectories in order to react to unmapped obstacles. Experiments on real robots are presented that show our system's capabilities of smooth, precise, and predictive motion.","Motion planning,
Mobile robots,
Error correction,
Trajectory,
Orbital robotics,
Robot sensing systems,
Path planning,
Acceleration,
Shape control,
Constraint optimization"
Data Security in Unattended Wireless Sensor Networks,"In recent years, wireless sensor networks (WSNs) have been a very popular research topic, offering a treasure trove of systems, networking, hardware, security, and application-related problems. Much of prior research assumes that the WSN is supervised by a constantly present sink and sensors can quickly offload collected data. In this paper, we focus on unattended WSNs (UWSNs) characterized by intermittent sink presence and operation in hostile settings. Potentially lengthy intervals of sink absence offer greatly increased opportunities for attacks resulting in erasure, modification, or disclosure of sensor-collected data. This paper presents an in-depth investigation of security problems unique to UWSNs (including a new adversarial model) and proposes some simple and effective countermeasures for a certain class of attacks.","Cryptography,
Wireless sensor networks,
Mobile communication,
Data mining,
Monitoring"
A Computational Geometry Approach to Automated Pulmonary Fissure Segmentation in CT Examinations,"Identification of pulmonary fissures, which form the boundaries between the lobes in the lungs, may be useful during clinical interpretation of computed tomography (CT) examinations to assess the early presence and characterization of manifestation of several lung diseases. Motivated by the unique nature of the surface shape of pulmonary fissures in 3-D space, we developed a new automated scheme using computational geometry methods to detect and segment fissures depicted on CT images. After a geometric modeling of the lung volume using the marching cubes algorithm, Laplacian smoothing is applied iteratively to enhance pulmonary fissures by depressing nonfissure structures while smoothing the surfaces of lung fissures. Next, an extended Gaussian image based procedure is used to locate the fissures in a statistical manner that approximates the fissures using a set of plane ldquopatchesrdquo. This approach has several advantages such as independence of anatomic knowledge of the lung structure except the surface shape of fissures, limited sensitivity to other lung structures, and ease of implementation. The scheme performance was evaluated by two experienced thoracic radiologists using a set of 100 images (slices) randomly selected from 10 screening CT examinations. In this preliminary evaluation 98.7% and 94.9% of scheme segmented fissure voxels are within 2 mm of the fissures marked independently by two radiologists in the testing image dataset. Using the scheme detected fissures as reference, 89.4% and 90.1% of manually marked fissure points have distance les2 mm to the reference suggesting a possible under-segmentation of the scheme. The case-based root mean square (rms) distances (ldquoerrorsrdquo) between our scheme and the radiologist ranged from 1.48plusmn0.92 to 2.04plusmn3.88 mm. The discrepancy of fissure detection results between the automated scheme and either radiologist is smaller in this dataset than the interreader variability.","Computational geometry,
Lungs,
Computed tomography,
Shape,
Image segmentation,
Smoothing methods,
Diseases,
Solid modeling,
Iterative algorithms,
Laplace equations"
Sparse and low-rank matrix decompositions,"We consider the following fundamental problem: given a matrix that is the sum of an unknown sparse matrix and an unknown low-rank matrix, is it possible to exactly recover the two components? Such a capability enables a considerable number of applications, but the goal is both ill-posed and NP-hard in general. In this paper we develop (a) a new uncertainty principle for matrices, and (b) a simple method for exact decomposition based on convex optimization. Our uncertainty principle is a quantification of the notion that a matrix cannot be sparse while having diffuse row/column spaces. It characterizes when the decomposition problem is ill-posed, and forms the basis for our decomposition method and its analysis. We provide deterministic conditions — on the sparse and low-rank components — under which our method guarantees exact recovery.","Matrix decomposition,
Sparse matrices,
Uncertainty,
Optimization methods,
Machine learning,
System identification,
Computational complexity,
Laboratories"
Dense 3D reconstruction method using a single pattern for fast moving object,"Dense 3D reconstruction of extremely fast moving objects could contribute to various applications such as body structure analysis and accident avoidance and so on. The actual cases for scanning we assume are, for example, acquiring sequential shape at the moment when an object explodes, or observing fast rotating turbine's blades. In this paper, we propose such a technique based on a one-shot scanning method that reconstructs 3D shape from a single image where dense and simple pattern are projected onto an object. To realize dense 3D reconstruction from a single image, there are several issues to be solved; e.g. instability derived from using multiple colors, and difficulty on detecting dense pattern because of influence of object color and texture compression. This paper describes the solutions of the issues by combining two methods, that is (1) an efficient line detection technique based on de Bruijn sequence and belief propagation, and (2) an extension of shape from intersections of lines method. As a result, a scanning system that can capture an object in fast motion has been actually developed by using a high-speed camera. In the experiments, the proposed method successfully captured the sequence of dense shapes of an exploding balloon, and a breaking ceramic dish at 300–1000 fps.",
Unit commitment with vehicle-to-Grid using particle swarm optimization,"Vehicle-to-Grid (V2G) technology has drawn great interest in the recent years. Success of the V2G research depends on efficient scheduling of gridable vehicles in limited parking lots. V2G can reduce dependencies on small expensive units in the existing power systems as energy storage that can decrease running costs. It can efficiently manage load fluctuation, peak load; however, it increases spinning reserves and reliability. As number of gridable vehicles in V2G is much higher than small units of existing systems, unit commitment (UC) with V2G is more complex than basic UC for thermal units. Particle swarm optimization (PSO) is used to solve the UC with V2G, as PSO can reliably and accurately solve complex constrained optimization problems easily and quickly without any dimension limitation and physical computer memory limit. In the proposed model, binary PSO is used to optimize the on/off states of power generating units and in the same model, discrete version of PSO is used to optimize the scheduling of the gridable vehicles in the parking lots to reduce the dimension of the problem. Finally, simulation results show a considerable amount of profit for using V2G after proper UC with V2G scheduling of gridable vehicles in constrained parking lots.","Particle swarm optimization,
Vehicles,
Power system reliability,
Energy storage,
Costs,
Load management,
Power system management,
Fluctuations,
Spinning,
Constraint optimization"
The Pilot Way to Grid Resources Using glideinWMS,"Grid computing has become very popular in big and widespread scientific communities with high computing demands, like high energy physics. Computing resources are being distributed over many independent sites with only a thin layer of Grid middleware shared between them. This deployment model has proven to be very convenient for computing resource providers, but has introduced several problems for the users of the system, the three major being the complexity of job scheduling, the non-uniformity of compute resources, and the lack of good job monitoring.Pilot jobs address all the above problems by creating a virtual private computing pool on top of Grid resources. This paper presents both the general pilot concept, as well as a concrete implementation, called glideinWMS, deployed in the Open Science Grid.","Grid computing,
Middleware,
Monitoring,
Distributed computing,
Physics computing,
Concrete,
Scalability,
Computer science,
Processor scheduling,
Authentication"
Learning image similarity from Flickr groups using Stochastic Intersection Kernel MAchines,"Measuring image similarity is a central topic in computer vision. In this paper, we learn similarity from Flickr groups and use it to organize photos. Two images are similar if they are likely to belong to the same Flickr groups. Our approach is enabled by a fast Stochastic Intersection Kernel MAchine (SIKMA) training algorithm, which we propose. This proposed training method will be useful for many vision problems, as it can produce a classifier that is more accurate than a linear classifier, trained on tens of thousands of examples in two minutes. The experimental results show our approach performs better on image matching, retrieval, and classification than using conventional visual features.","Machine learning,
Stochastic processes,
Kernel,
Support vector machines,
Histograms,
Computer vision,
Support vector machine classification,
Large-scale systems,
Feedback,
Computer science"
Manifold Discriminant Analysis,"This paper presents a novel discriminative learning method, called manifold discriminant analysis (MDA), to solve the problem of image set classification. By modeling each image set as a manifold, we formulate the problem as classification-oriented multi-manifolds learning. Aiming at maximizing “manifold margin”, MDA seeks to learn an embedding space, where manifolds with different class labels are better separated, and local data compactness within each manifold is enhanced. As a result, new testing manifold can be more reliably classified in the learned embedding space. The proposed method is evaluated on the tasks of object recognition with image sets, including face recognition and object categorization. Comprehensive comparisons and extensive experiments demonstrate the effectiveness of our method.","Linear discriminant analysis,
Testing,
Object recognition,
Image analysis,
Information analysis,
Content addressable storage,
Laplace equations,
Image recognition,
Information processing,
Computers"
Optimal scanning for faster object detection,"Recent years have seen the development of fast and accurate algorithms for detecting objects in images. However, as the size of the scene grows, so do the running-times of these algorithms. If a 128×102 pixel image requires 20 ms to process, searching for objects in a 1280×1024 image will take 2 s. This is unsuitable under real-time operating constraints: by the time a frame has been processed, the object may have moved. An analogous problem occurs when controlling robot camera that need to scan scenes in search of target objects. In this paper, we consider a method for improving the run-time of general-purpose object-detection algorithms. Our method is based on a model of visual search in humans, which schedules eye fixations to maximize the long-term information accrued about the location of the target of interest. The approach can be used to drive robot cameras that physically scan scenes or to improve the scanning speed for very large high resolution images. We consider the latter application in this work by simulating a “digital fovea” and sequentially placing it in various regions of an image in a way that maximizes the expected information gain. We evaluate the approach using the OpenCV version of the Viola-Jones face detector. After accounting for all computational overhead introduced by the fixation controller, the approach doubles the speed of the standard Viola-Jones detector at little cost in accuracy.","Object detection,
Layout,
Robot vision systems,
Cameras,
Face detection,
Detectors,
Pixel,
Time factors,
Robot control,
Runtime"
Improving the Speed of Parallel Decimal Multiplication,"Hardware support for decimal computer arithmetic is regaining popularity. One reason is the recent growth of decimal computations in commercial, scientific, financial, and Internet-based computer applications. Newly commercialized decimal arithmetic hardware units use radix-10 sequential multipliers that are rather slow for multiplication-intensive applications. Therefore, the future relevant processors are likely to host fast parallel decimal multiplication circuits. The corresponding hardware algorithms are normally composed of three steps: partial product generation (PPG), partial product reduction (PPR), and final carry-propagating addition. The state of the art is represented by two recent full solutions with alternative designs for all the three aforementioned steps. In addition, PPR by itself has been the focus of other recent studies. In this paper, we examine both of the full solutions and the impact of a PPR-only design on the appropriate one. In order to improve the speed of parallel decimal multiplication, we present a new PPG method, fine-tune the PPR method of one of the full solutions and the final addition scheme of the other; thus, assembling a new full solution. Logical Effort analysis and 0.13 mum synthesis show at least 13 percent speed advantage, but at a cost of at most 36 percent additional area consumption.","Hardware,
Data mining,
Adders,
Table lookup,
Digital arithmetic,
Floating-point arithmetic,
Delay"
Advance Reservations and Scheduling for Bulk Transfers in Research Networks,"Data-intensive e-science collaborations often require the transfer of large files with predictable performance. To meet this need, we design novel admission control (AC) and scheduling algorithms for bulk data transfer in research networks for e-science. Due to their small sizes, the research networks can afford a centralized resource management platform. In our design, each bulk transfer job request, which can be made in advance to the central network controller, specifies a start time and an end time. If admitted, the network guarantees to complete the transfer before the end time. However, there is flexibility in how the actual transfer is carried out, that is, in the bandwidth assignment on each allowed path of the job on each time interval, and it is up to the scheduling algorithm to decide this. To improve the network resource utilization or lower the job rejection ratio, the network controller solves optimization problems in making AC and scheduling decisions. Our design combines the following elements into a cohesive optimization-based framework: advance reservations, multipath routing, and bandwidth reassignment via periodic reoptimization. We evaluate our algorithm in terms of both network efficiency and the performance level of individual transfer. We also evaluate the feasibility of our scheme by studying the algorithm execution time.","Scheduling algorithm,
Resource management,
Bandwidth,
Collaboration,
Algorithm design and analysis,
Admission control,
Job design,
Centralized control,
Design optimization,
Routing"
Joint Throughput Optimization for Wireless Mesh Networks,"In this paper, we address the problem of joint channel assignment, link scheduling, and routing for throughput optimization in wireless networks with multi-radios and multi-channels. We mathematically formulate this problem by taking into account the interference, the number of available radios the set of usable channels, and other resource constraints at nodes. We also consider the possible combining of several consecutive channels into one so that a network interface card (NIC) can use the channel with larger range of frequencies and thus improve the channel capacity. Furthermore, we consider several interference models and assume a general yet practical network model in which two nodes may still not communicate directly even if one is within the transmission range of the other. We designed efficient algorithm for throughput (or fairness) optimization by finding flow routing, scheduling of transmissions, and dynamic channel assignment and combining. We show that the performance, fairness and throughput, achieved by our method is within a constant factor of the optimum. Our model also can deal with the situation when each node will charge a certain amount for relaying data to a neighboring node and each flow has a budget constraint. Our extensive evaluation shows that our algorithm can effectively exploit the number of channels and radios. In addition, it shows that combining multiple channels and assigning them to a single user at some time slots indeed increases the maximum throughput of the system compared to assigning a single channel.","Throughput,
Wireless mesh networks,
Routing,
Interference constraints,
Network interfaces,
Frequency,
Channel capacity,
Algorithm design and analysis,
Scheduling algorithm,
Design optimization"
The Weight Distribution of Some Irreducible Cyclic Codes,"Irreducible cyclic codes have been an interesting subject of study for a long time. Their weight distribution is known in only a few cases. In this paper, the weight distribution of the irreducible cyclic codes in a number of other cases is determined. The number of nonzero weights in the codes dealt with in this paper varies between one and four.","Distributed computing,
Linear code,
Computer science,
Gaussian processes,
Fourier transforms"
Adapted One-versus-All Decision Trees for Data Stream Classification,"One versus all (OVA) decision trees learn k individual binary classifiers, each one to distinguish the instances of a single class from the instances of all other classes. Thus OVA is different from existing data stream classification schemes whose majority use multiclass classifiers, each one to discriminate among all the classes. This paper advocates some outstanding advantages of OVA for data stream classification. First, there is low error correlation and hence high diversity among OVA's component classifiers, which leads to high classification accuracy. Second, OVA is adept at accommodating new class labels that often appear in data streams. However, there also remain many challenges to deploy traditional OVA for classifying data streams. First, as every instance is fed to all component classifiers, OVA is known as an inefficient model. Second, OVA's classification accuracy is adversely affected by the imbalanced class distribution in data streams. This paper addresses those key challenges and consequently proposes a new OVA scheme that is adapted for data stream classification. Theoretical analysis and empirical evidence reveal that the adapted OVA can offer faster training, faster updating and higher classification accuracy than many existing popular data stream classification algorithms.","Decision trees,
Classification tree analysis,
Algorithm design and analysis,
Classification algorithms,
Data mining,
Computer networks,
Telecommunication traffic,
IP networks,
Intrusion detection,
Change detection algorithms"
"Learning a dense multi-view representation for detection, viewpoint classification and synthesis of object categories","Recognizing object classes and their 3D viewpoints is an important problem in computer vision. Based on a part-based probabilistic representation [31], we propose a new 3D object class model that is capable of recognizing unseen views by pose estimation and synthesis. We achieve this by using a dense, multiview representation of the viewing sphere parameterized by a triangular mesh of viewpoints. Each triangle of viewpoints can be morphed to synthesize new viewpoints. By incorporating 3D geometrical constraints, our model establishes explicit correspondences among object parts across viewpoints. We propose an incremental learning algorithm to train the generative model. A cellphone video clip of an object is first used to initialize model learning. Then the model is updated by a set of unsorted training images without viewpoint labels. We demonstrate the robustness of our model on object detection, viewpoint classification and synthesis tasks. Our model performs superiorly to and on par with state-of-the-art algorithms on the Savarese et al. 2007 and PASCAL datasets in object detection. It outperforms all previous work in viewpoint classification and offers promising results in viewpoint synthesis.","Object detection,
Computer science,
Computer vision,
Cellular phones,
Robustness,
Mesh generation,
Solid modeling,
Image segmentation,
Image recognition,
Layout"
A Novel Cell Segmentation Method and Cell Phase Identification Using Markov Model,"Optical microscopy is becoming an important technique in drug discovery and life science research. The approaches used to analyze optical microscopy images are generally classified into two categories: automatic and manual approaches. However, the existing automatic systems are rather limited in dealing with large volume of time-lapse microscopy images because of the complexity of cell behaviors and morphological variance. On the other hand, manual approaches are very time-consuming. In this paper, we propose an effective automated, quantitative analysis system that can be used to segment, track, and quantize cell cycle behaviors of a large population of cells nuclei effectively and efficiently. We use adaptive thresholding and watershed algorithm for cell nuclei segmentation followed by a fragment merging method that combines two scoring models based on trend and no trend features. Using the context information of time-lapse data, the phases of cell nuclei are identified accurately via a Markov model. Experimental results show that the proposed system is effective for nuclei segmentation and phase identification.","Image segmentation,
Bioinformatics,
Biomedical imaging,
Image analysis,
Merging,
Hospitals,
Drugs,
Optical microscopy,
Fluorescence,
Radiology"
Interactive segmentation for manipulation in unstructured environments,"To perform successful manipulation, robots depend on information about objects in their environment. In unstructured environments, such information cannot be given to the robot a priori. It is thus critical for the robot to be able to continuously acquire task-specific information about objects. Towards this goal, we present a robust perceptual skill for identifying, tracking, and segmenting objects in a cluttered environment. We increase the robot's perceptual capabilities by closely coupling them with the robot's manipulation skills. The robot's interaction with objects in the environment creates a perceptual signal, i.e. motion, that renders segmentation and tracking robust and reliable. In addition, the resulting perceptual signal reveals the type of segmentation most relevant to manipulation, namely a segmentation of rigidly connected physical bodies. We demonstrate our approach with experiments on a real world mobile manipulation platform with multiple objects in a cluttered scene.","Image segmentation,
Robots,
Layout,
Robotics and automation,
Robustness,
Object recognition,
Orbital robotics,
Feedback,
Video sequences,
Tracking"
Design of Subarrayed Linear and Planar Array Antennas With SLL Control Based on an Excitation Matching Approach,"In this paper, the synthesis of subarrayed antennas with subarrays having different weights and dimensions is considered. The problem is formulated as the definition of the element clustering into spatially contiguous subarrays and the corresponding weights to obtain a pattern with the lowest (i.e., minimum achievable) sidelobe level (SLL). The problem is addressed with an unconstrained approach based on an excitation matching method where the synthesis of subarray weights and sizes is simultaneously carried out. Successively, a hybrid version of such an approach is proposed to directly enforce into the optimization procedure the SLL constraints. A set of representative results concerned with both sum and difference patterns are shown. Furthermore, some comparisons with state-of-the-art hybrid evolutionary-based techniques are reported, as well.","Planar arrays,
Constraint optimization,
Costs,
Linear antenna arrays,
Antenna arrays,
Manufacturing,
Large-scale systems,
Production,
Computer science"
Evaluating algorithm performance metrics tailored for prognostics,"Prognostics has taken center stage in Condition Based Maintenance (CBM) where it is desired to estimate Remaining Useful Life (RUL) of a system so that remedial measures may be taken in advance to avoid catastrophic events or unwanted downtimes. Validation of such predictions is an important but difficult proposition and a lack of appropriate evaluation methods renders prognostics meaningless. Evaluation methods currently used in the research community are not standardized and in many cases do not sufficiently assess key performance aspects expected out of a prognostics algorithm. In this paper we introduce several new evaluation metrics tailored for prognostics and show that they can effectively evaluate various algorithms as compared to other conventional metrics. Four prognostic algorithms, Relevance Vector Machine (RVM), Gaussian Process Regression (GPR), Artificial Neural Network (ANN), and Polynomial Regression (PR), are compared. These algorithms vary in complexity and their ability to manage uncertainty around predicted estimates. Results show that the new metrics rank these algorithms in a different manner; depending on the requirements and constraints suitable metrics may be chosen. Beyond these results, this paper offers ideas about how metrics suitable to prognostics may be designed so that the evaluation procedure can be standardized.","Measurement,
Computer science,
Artificial neural networks,
Mission critical systems,
Space technology,
NASA,
Life estimation,
Gaussian processes,
Ground penetrating radar,
Polynomials"
Exploring the multiple-GPU design space,"Graphics Processing Units (GPUs) have been growing in popularity due to their impressive processing capabilities, and with general purpose programming languages such as NVIDIA's CUDA interface, are becoming the platform of choice in the scientific computing community. Previous studies that used GPUs focused on obtaining significant performance gains from execution on a single GPU. These studies employed low-level, architecture-specific tuning in order to achieve sizeable benefits over multicore CPU execution. In this paper, we consider the benefits of running on multiple (parallel) GPUs to provide further orders of performance speedup. Our methodology allows developers to accurately predict execution time for GPU applications while varying the number and configuration of the GPUs, and the size of the input data set. This is a natural next step in GPU computing because it allows researchers to determine the most appropriate GPU configuration for an application without having to purchase hardware, or write the code for a multiple-GPU implementation. When used to predict performance on six scientific applications, our framework produces accurate performance estimates (11% difference on average and 40% maximum difference in a single case) for a range of short and long running scientific programs.","Hardware,
Scientific computing,
Performance gain,
Libraries,
Application software,
Computer graphics,
Computer languages,
Multicore processing,
Central Processing Unit,
Programming profession"
A hybrid network coding technique for single-hop wireless networks,"In this paper, we investigate a hybrid network coding technique to be used at a wireless base station (BS) or access point (AP) to increase the throughput efficiency of single-hop wireless networks. Traditionally, to provide reliability, lost packets from different flows (applications) are retransmitted separately, leading to inefficient use of wireless bandwidth. Using the proposed hybrid network coding approach, the BS encodes these lost packets, possibly from different flows together before broadcasting them to all wireless users. In this way, multiple wireless receivers can recover their lost packets simultaneously with a single transmission from the BS. Furthermore, simulations and theoretical analysis showed that when used in conjunction with an appropriate channel coding technique under typical channel conditions, this approach can increase the throughput efficiency up to 3.5 times over the automatic repeat request (ARQ), and up to 1.5 times over the HARQ techniques.","Network coding,
Wireless networks,
Automatic repeat request,
Throughput,
Bandwidth,
Routing,
Forward error correction,
Femtocell networks,
Broadcasting,
Channel coding"
Semi-partitioned Fixed-Priority Scheduling on Multiprocessors,"This paper presents a new algorithm for fixed-priority scheduling of sporadic task systems on multiprocessors.The algorithm is categorized to such a scheduling class that qualifies a few tasks to migrate across processors, while most tasks are fixed to particular processors. We design the algorithm so that a task is qualified to migrate, only if it cannot be assigned to any individual processors, in such a way that it is never returned to the same processor within the same period, once it is migrated from one processor to another processor. The scheduling policy is then conformed to Deadline Monotonic. According to the simulation results, the new algorithm significantly outperforms the traditional fixed-priority algorithms in terms of schedulability.","Processor scheduling,
Scheduling algorithm,
Partitioning algorithms,
Real time systems,
Application software,
Computer science,
Process design,
Algorithm design and analysis,
Multicore processing,
Operating systems"
GraphSig: A Scalable Approach to Mining Significant Subgraphs in Large Graph Databases,"Graphs are being increasingly used to model a wide range of scientific data. Such widespread usage of graphs has generated considerable interest in mining patterns from graph databases. While an array of techniques exists to mine frequent patterns, we still lack a scalable approach to mine statistically significant patterns, specifically patterns with low p-values, that occur at low frequencies. We propose a highly scalable technique, called GraphSig, to mine significant subgraphs from large graph databases. We convert each graph into a set of feature vectors where each vector represents a region within the graph. Domain knowledge is used to select a meaningful feature set. Prior probabilities of features are computed empirically to evaluate statistical significance of patterns in the feature space. Following analysis in the feature space, only a small portion of the exponential search space is accessed for further analysis. This enables the use of existing frequent subgraph mining techniques to mine significant patterns in a scalable manner even when they are infrequent. Extensive experiments are carried out on the proposed techniques, and empirical results demonstrate that GraphSig is effective and efficient for mining significant patterns. To further demonstrate the power of significant patterns, we develop a classifier using patterns mined by GraphSig. Experimental results show that the proposed classifier achieves superior performance, both in terms of quality and computation cost, over state-of-the-art classifiers.","Frequency,
Chemical compounds,
Data engineering,
Computer science,
USA Councils,
Spatial databases,
Probability,
Computational efficiency,
Chemical technology,
Social network services"
Activity Invariant Sets and Exponentially Stable Attractors of Linear Threshold Discrete-Time Recurrent Neural Networks,"This technical note proposes to study the activity invariant sets and exponentially stable attractors of linear threshold discrete-time recurrent neural networks. The concept of activity invariant sets deeply describes the property of an invariant set by that the activity of some neurons keeps invariant all the time. Conditions are obtained for locating activity invariant sets. Under some conditions, it shows that an activity invariant set can have one equilibrium point which attracts exponentially all trajectories starting in the set. Since the attractors are located in activity invariant sets, each attractor has binary pattern and also carries analog information. Such results can provide new perspective to apply attractor networks for applications such as group winner-take-all, associative memory, etc.","Recurrent neural networks,
Neurons,
Computer science,
Associative memory,
Transfer functions,
Computer networks,
Neural networks,
Analog-digital conversion,
Analog computers,
Neural network hardware"
Domain Transfer SVM for video concept detection,"Cross-domain learning methods have shown promising results by leveraging labeled patterns from auxiliary domains to learn a robust classifier for target domain, which has a limited number of labeled samples. To cope with the tremendous change of feature distribution between different domains in video concept detection, we propose a new cross-domain kernel learning method. Our method, referred to as Domain Transfer SVM (DTSVM), simultaneously learns a kernel function and a robust SVM classifier by minimizing both the structural risk functional of SVM and the distribution mismatch of labeled and unlabeled samples between the auxiliary and target domains. Comprehensive experiments on the challenging TRECVID corpus demonstrate that DTSVM outperforms existing cross-domain learning and multiple kernel learning methods.","Support vector machines,
Kernel,
Support vector machine classification,
Learning systems,
Robustness,
Testing,
Training data,
Multimedia communication,
Broadcasting,
Humans"
Cooperative multi-robot localization under communication constraints,"This paper addresses the problem of cooperative localization (CL) under severe communication constraints. Specifically, we present minimum mean square error (MMSE) and maximum a posteriori (MAP) estimators that can process measurements quantized with as little as one bit per measurement. During CL, each robot quantizes and broadcasts its measurements and receives the quantized observations of its teammates. The quantization process is based on the appropriate selection of thresholds, computed using the current state estimates, that minimize the estimation error metric considered. Extensive simulations demonstrate that the proposed Iteratively-Quantized Extended Kalman filter (IQEKF) and the Iteratively Quantized MAP (IQMAP) estimator achieve performance indistinguishable of that of their real-valued counterparts (EKF and MAP, respectively) when using as few as 4 bits for quantizing each robot measurement.","Orbital robotics,
Robot sensing systems,
Broadcasting,
Quantization,
Power measurement,
Rotation measurement,
Velocity measurement,
Noise measurement,
Time measurement,
Mean square error methods"
Treatment of Rabbit Elastase-Induced Aneurysm Models by Flow Diverters: Development of Quantifiable Indexes of Device Performance Using Digital Subtraction Angiography,"It has been known for more than a decade that intracranial aneurysms can be successfully treated by deploying a porous meshed tube in the parent vessel of the aneurysm. Such devices are currently called flow diverters because they promote intraneurysmal flow stasis and thrombosis by diverting blood flow away from the aneurysm sac. The objective of this study was to use angiographic data to quantify and compare the performance of flow diverters of original design in successfully occluding an experimental aneurysm model. Three different configurations of a novel flow diverter with varying porosities and pore densities were implanted in 30 rabbit elastase-induced aneurysms. Temporal variations in angiographic contrast intensity within the aneurysms were fit to a mathematical model. Optimized model parameters were supplemented by the angiographic percentage aneurysm occlusion and an angiographic measure of device flexibility to derive composite scores of performance. Angiographic quantification further suggested a parameter, which could be employed to estimate long-term aneurysm occlusion probabilities immediately after treatment. Performance scores showed that the device with a porosity of 70% and pore density of 18 pores/mm2 performed better than devices with 65% porosity, 14 pores/mm2, and 70% porosity, 12 pores/mm2 with relative efficacies of 100%, 84%, and 76%, respectively. The pore density of flow diverters, rather than porosity, may thus be a critical factor modulating device efficacy. A value of the prognostic parameter of less than 30 predicted greater than 97% angiographic aneurysm occlusion over six months with a sensitivity of 73% and specificity of 82%.","Rabbits,
Aneurysm,
Angiography,
Biomedical engineering,
Surface treatment,
Blood flow,
Mathematical model,
Radiology,
Arteries,
Biomedical measurements"
Mining exception-handling rules as sequence association rules,"Programming languages such as Java and C++ provide exception-handling constructs to handle exception conditions. Applications are expected to handle these exception conditions and take necessary recovery actions such as releasing opened database connections. However, exception-handling rules that describe these necessary recovery actions are often not available in practice. To address this issue, we develop a novel approach that mines exception-handling rules as sequence association rules of the form “(FC1c1…FCcn) ∧ FCa ⇒ (FCe1…FCem)”. This rule describes that function call FCa should be followed by a sequence of function calls (FCe1…FCem) when FCa is preceded by a sequence of function calls (FCe1…FCcn). Such form of rules is required to characterize common exception-handling rules. We show the usefulness of these mined rules by applying them on five real-world applications (including 285 KLOC) to detect violations in our evaluation. Our empirical results show that our approach mines 294 real exception-handling rules in these five applications and also detects 160 defects, where 87 defects are new defects that are not found by a previous related approach.","Data mining,
Association rules,
Databases,
Computer languages,
Application software,
Computer science,
Java,
Degradation,
Lead"
Optimal beamforming for non-regenerative MIMO relays with direct link,"In this letter, we generalize the existing works on the design of the optimal relay amplifying matrix for nonregenerative multiple-input multiple-output (MIMO) relay communication systems by including the direct source-destination link. We show that for most commonly used objective functions, the optimal relay amplifying matrix has a general beamforming structure, that is, the relay first sets beams to the direction of the source-relay channel, then conducts a linear precoding, and finally beamforming towards the direction of the relay-destination channel.","Array signal processing,
MIMO,
Power system relaying,
Jacobian matrices,
Digital relays,
Mutual information,
Signal design,
Signal to noise ratio,
Closed-form solution"
Optimizing Service Systems Based on Application-Level QoS,"Making software systems service-oriented is becoming the practice, and an increasingly large number of service systems play important roles in today's business and industry. Currently, not enough attention has been paid to the issue of optimization of service systems. In this paper, we argue that the key elements to be considered in optimizing service systems are robustness, system orientation, and being dynamic and transparent. We present our solution to optimizing service systems based on application-level QoS management. Our solution incorporates three capabilities, i.e., 1) the ability to cater to the varying rigidities on Web service QoS in distinct application domains and of various users in a robust and heuristic manner, 2) the ability to formulate the overall system utility of a service system perceived by a particular system end user and to suggest its maximization using a utility model incorporated into a three-dimensional weighting scheme, and 3) the ability to dynamically achieve a higher perceived system utility of a service system via transparent negotiations. The calculation of the system utility encompasses a negotiation algorithm and a robust search algorithm for selecting heuristically best Web services. The effectiveness of the proposed algorithms and our solution is demonstrated by simulation experiments and our demo deployment, SSO.","Quality of service,
Decision support systems,
Web services,
Probability density function,
Data mining,
Robustness"
Locally constrained diffusion process on locally densified distance spaces with applications to shape retrieval,"The matching and retrieval of 2D shapes is an important challenge in computer vision. A large number of shape similarity approaches have been developed, with the main focus being the comparison or matching of pairs of shapes. In these approaches, other shapes do not influence the similarity measure of a given pair of shapes. In the proposed approach, other shapes do influence the similarity measure of each pair of shapes, and we show that this influence is beneficial even in the unsupervised setting (without any prior knowledge of shape classes). The influence of other shapes is propagated as a diffusion process on a graph formed by a given set of shapes. However, the classical diffusion process does not perform well in shape space for two reasons: it is unstable in the presence of noise and the underlying local geometry is sparse. We introduce a locally constrained diffusion process which is more stable even if noise is present, and we densify the shape space by adding synthetic points we call 'ghost points'. We present experimental results that demonstrate very significant improvements over state-of-the-art shape matching algorithms. On the MPEG-7 data set, we obtained a bull's-eye retrieval score of 93.32%, which is the highest score ever reported in the literature.",
Automatically detecting the small group structure of a crowd,"Recent work on computer vision analysis of crowds tends to focus on robustly tracking individuals through the crowd or on analyzing the overall pattern of flow. Our work seeks a deeper analysis of social behavior by identifying the small group structure of crowds, forming the basis for mid-level activity analysis at the granularity of human social groups. Building upon state-of-the-art algorithms for pedestrian detection and multi-object tracking, and inspired by social science models of human collective behavior, we automatically detect small groups of individuals who are traveling together. These groups are discovered using a bottom-up hierarchical clustering approach that compares sets of individuals based on a generalized, symmetric Hausdorff distance defined with respect to pairwise proximity and velocity. We validate our results quantitatively and qualitatively on videos of real-world pedestrian scenes. Where human-coded ground truth is available, we find substantial statistical agreement between our results and the human-perceived small group structure of the crowd.","Pattern analysis,
Humans,
Clustering algorithms,
Layout,
Computer vision,
Videos,
Robustness,
Sociology,
Buildings,
Data mining"
Analysis of worst-case delay bounds for best-effort communication in wormhole networks on chip,"In packet-switched network-on-chip, computing worst-case delay bounds is crucial for designing predictable and cost-effective communication systems but yet an intractable problem due to complicated resource sharing scenarios. For wormhole networks with credit-based flow control, the existence of cyclic dependency between flit delivery and credit generation further complicates the problem. Based on network calculus, we propose a technique for analyzing communication delay bounds for individual flows in wormhole networks. We first propose router service analysis models for flow control, link and buffer sharing. Based on these analysis models, we obtain a buffering-sharing analysis network, which is open-ended and captures both flow control and link sharing. Furthermore, we compute equivalent service curves for individual flows using the network contention tree model in the buffer-sharing analysis network, and then derive their delay bounds. Our experimental results verify that the theoretical bounds are correct and tight.","Network-on-a-chip,
Communication system control,
Delay,
Traffic control,
Computer networks,
Communication system traffic control,
Communication systems,
Resource management,
Calculus,
Telecommunication traffic"
Variable granularity for improving precision of impact analysis,"Impact analysis is a specialized process of program comprehension that investigates the nature and extent of a planned software change. Traditionally, impact analysis involves inspecting dependencies among the software components of a fixed granularity; these components constitute a dependency graph. In this paper, we argue that a single granularity is insufficient and leads to imprecise analysis. We explain how the precision can be improved by variable granularity, where the programmers choose among the granularity of classes, the granularity of class members, and the granularity of code fragments. We assess the resulting precision in a case study on open-source software.","Programming profession,
Object oriented modeling,
Open source software,
Software maintenance,
Iterative methods,
Collaborative software,
Software tools,
Computer science,
Software design,
Collaborative tools"
Digital ecosystems in the clouds: Towards community cloud computing,"Cloud Computing is rising fast, with its data centres growing at an unprecedented rate. However, this has come with concerns of privacy, efficiency at the expense of resilience, and environmental sustainability, because of the dependence on Cloud vendors such as Google, Amazon, and Microsoft. Community Cloud Computing makes use of the principles of Digital Ecosystems to provide a paradigm for Clouds in the community, offering an alternative architecture for the use cases of Cloud Computing. It is more technically challenging to deal with issues of distributed computing, such as latency, differential resource management, and additional security requirements. However, these are not insurmountable challenges, and with the need to retain control over our digital lives and the potential environmental consequences, it is a challenge we must pursue.","Ecosystems,
Cloud computing,
Privacy,
Resilience,
Green products,
Computer architecture,
Distributed computing,
Delay,
Resource management,
Security"
To Game or Not to Game?,"One challenge in software engineering education is to give students sufficient hands-on experience in actually building software. This is necessary so that students can understand which practices and techniques are useful in various situations. Some researchers have advocated alternative teaching methods to help in this regard. If successful, such methods could give students some experience with different approaches' effects in a shorter, more constrained time period. We examine one such approach, game-based learning, here.","Project management,
Computational modeling,
Computer simulation,
Engineering education,
Software engineering,
Computer science education,
Software libraries,
Databases,
Search methods,
World Wide Web"
Efficient simulation of large-scale Spiking Neural Networks using CUDA graphics processors,"Neural network simulators that take into account the spiking behavior of neurons are useful for studying brain mechanisms and for engineering applications. Spiking Neural Network (SNN) simulators have been traditionally simulated on large-scale clusters, super-computers, or on dedicated hardware architectures. Alternatively, Graphics Processing Units (GPUs) can provide a low-cost, programmable, and high-performance computing platform for simulation of SNNs. In this paper we demonstrate an efficient, Izhikevich neuron based large-scale SNN simulator that runs on a single GPU. The GPU-SNN model (running on an NVIDIA GTX-280 with 1GB of memory), is up to 26 times faster than a CPU version for the simulation of 100K neurons with 50 Million synaptic connections, firing at an average rate of 7Hz. For simulation of 100K neurons with 10 Million synaptic connections, the GPU-SNN model is only 1.5 times slower than real-time. Further, we present a collection of new techniques related to parallelism extraction, mapping of irregular communication, and compact network representation for effective simulation of SNNs on GPUs. The fidelity of the simulation results were validated against CPU simulations using firing rate, synaptic weight distribution, and inter-spike interval analysis. We intend to make our simulator available to the modeling community so that researchers will have easy access to large-scale SNN simulations.","Large-scale systems,
Neural networks,
Graphics,
Computational modeling,
Brain modeling,
Neurons,
Central Processing Unit,
Analytical models,
Neural network hardware,
Computer architecture"
Facial marks: Soft biometric for face recognition,"We propose to utilize micro features, namely facial marks (e.g., freckles, moles, and scars) to improve face recognition and retrieval performance. Facial marks can be used in three ways: i) to supplement the features in an existing face matcher, ii) to enable fast retrieval from a large database using facial mark based queries, and iii) to enable matching or retrieval from a partial or profile face image with marks. We use Active Appearance Model (AAM) to locate and segment primary facial features (e.g., eyes, nose, and mouth). Then, Laplacian-of-Gaussian (LoG) and morphological operators are used to detect facial marks. Experimental results based on FERET (426 images, 213 subjects) and Mugshot (1,225 images, 671 subjects) databases show that the use of facial marks improves the rank-1 identification accuracy of a state-of-the-art face recognition system from 92.96% to 93.90% and from 91.88% to 93.14%, respectively.","Biometrics,
Face recognition,
Information retrieval,
Image retrieval,
Image databases,
Spatial databases,
Face detection,
Active appearance model,
Image segmentation,
Facial features"
Age estimation using Active Appearance Models and Support Vector Machine regression,"In this paper, we introduce a novel age estimation technique that combines Active Appearance Models (AAMs) and Support Vector Machines (SVMs), to dramatically improve the accuracy of age estimation over the current state-of-the-art techniques. In this method, characteristics of the input images, face image, are interpreted as feature vectors by AAMs, which are used to discriminate between childhood and adulthood, prior to age estimation. Faces classified as adults are passed to the adult age-determination function and the others are passed to the child age-determination function. Compared to published results, this method yields the highest accuracy recognition rates, both in overall mean-absolute error (MAE) and mean-absolute error for the two periods of human development: childhood and adulthood.","Active appearance model,
Support vector machines,
Aging,
State estimation,
Skin,
Support vector machine classification,
Humans,
Pediatrics,
Image databases,
Eyes"
Effective image splicing detection based on image chroma,"A color image splicing detection method based on gray level co-occurrence matrix (GLCM) of thresholded edge image of image chroma is proposed in this paper. Edge images are generated by subtracting horizontal, vertical, main and minor diagonal pixel values from current pixel values respectively and then thresholded with a predefined threshold T. The GLCMs of edge images along the four directions serve as features for image splicing detection. Boosting feature selection is applied to select optimal features and Support Vector Machine (SVM) is utilized as classifier in our approach. The effectiveness of the proposed method has been demonstrated by our experimental results.","Splicing,
Image edge detection,
Pixel,
Support vector machines,
Support vector machine classification,
Color,
DC generators,
Image generation,
Boosting,
Computer vision"
Markov-HTN Planning Approach to Enhance Flexibility of Automatic Web Service Composition,"Automatic Web services composition can be achieved by using AI planning techniques. HTN planning has been adopted to handle the OWL-S Web service composition problem. However, existing composition methods based on HTN planning have not considered the choice of decompositions available to a problem which can lead to a variety of valid solutions.In this paper, we propose a model of combining a Markov decision process model and HTN planning to address Web services composition. In the model, HTN planning is enhanced to decompose a task in multiple ways and hence be able to find more than one plan,taking both functional and non-functional properties into account. Furthermore, an evaluation method to choose the optimal plan and some experimental results illustrate that the proposed approach works effectively.","Web services,
Process planning,
Artificial intelligence,
Cities and towns,
Petroleum,
Computer science,
Software systems,
Computer applications,
Application software,
Costs"
Utility of beamforming strategies for secrecy in multiuser MIMO wiretap channels,"This paper examines linear beamforming methods for secure communications in a multiuser wiretap channel with a single transmitter, multiple legitimate receivers, and a single eavesdropper, where all nodes are equipped with multiple antennas. No information regarding the eavesdropper is presumed at the transmitter, and we examine both the broadcast MIMO downlink with independent information, and the multicast MIMO downlink with common information for all legitimate receivers. In both cases the information signal is transmitted with just enough power to guarantee a certain SINR at the desired receivers, while the remainder of the power is used to broadcast artificial noise. The artificial interference selectively degrades the passive eavesdropper's signal while remaining orthogonal to the desired receivers. We analyze the confidentiality provided by zero-forcing and optimal minimum-power beamforming designs for the broadcast channel, and optimal minimum-MSE beamformers for the multicast channel. Numerical simulations for the relative SINR and BER performance of the eavesdropper demonstrate the effectiveness of the proposed physical-layer security schemes.","Array signal processing,
MIMO,
Broadcasting,
Transmitters,
Downlink,
Signal to noise ratio,
Receiving antennas,
Interference,
Degradation,
Numerical simulation"
Cooperative target-capturing strategy for multi-vehicle systems with dynamic network topology,"This paper deals with cooperative target-capturing problem for multi-vehicle systems with dynamic network topology. Firstly, we introduce a dynamic network topology that depends on relative distance between the vehicles. Secondly, we propose the target-capturing strategy based on consensus seeking with dynamic network topology. In proposed strategy, at least one vehicle can acquire the information of the target-object and network topology among vehicles is time-varying but always connected. To analyze the convergence of target-capturing behavior with dynamic network topology, algebraic graph theory and matrix theory are utilized. Finally, numerical simulation results and experimental results are provided that demonstrate the effectiveness of the proposed method.",
A Data Mining Approach for Detecting Higher-Level Clones in Software,"Code clones are similar program structures recurring in variant forms in software system(s). Several techniques have been proposed to detect similar code fragments in software, so-called simple clones. Identification and subsequent unification of simple clones is beneficial in software maintenance. Even further gains can be obtained by elevating the level of code clone analysis. We observed that recurring patterns of simple clones often indicate the presence of interesting higher-level similarities that we call structural clones. Structural clones show a bigger picture of similarity situation than simple clones alone. Being logical groups of simple clones, structural clones alleviate the problem of huge number of clones typically reported by simple clone detection tools, a problem that is often dealt with postdetection visualization techniques. Detection of structural clones can help in understanding the design of the system for better maintenance and in reengineering for reuse, among other uses. In this paper, we propose a technique to detect some useful types of structural clones. The novelty of our approach includes the formulation of the structural clone concept and the application of data mining techniques to detect these higher-level similarities. We describe a tool called clone miner that implements our proposed technique. We assess the usefulness and scalability of the proposed techniques via several case studies. We discuss various usage scenarios to demonstrate in what ways the knowledge of structural clones adds value to the analysis based on simple clones alone.","Data mining,
Cloning,
Software systems,
Software maintenance,
Collaboration,
Portals,
Computer Society,
Visualization,
Scalability,
Reverse engineering"
Wide-baseline image matching using Line Signatures,"We present a wide-baseline image matching approach based on line segments. Line segments are clustered into local groups according to spatial proximity. Each group is treated as a feature called a Line Signature. Similar to local features, line signatures are robust to occlusion, image clutter, and viewpoint changes. The descriptor and similarity measure of line signatures are presented. Under our framework, the feature matching is not only robust against affine distortion but also a considerable range of 3D viewpoint changes for non-planar surfaces. When compared to matching approaches based on existing local features, our method shows improved results with low-texture scenes. Moreover, extensive experiments validate that our method has advantages in matching structured non-planar scenes under large viewpoint changes and illumination variations.","Image matching,
Image segmentation,
Robustness,
Layout,
Lighting,
Object detection,
Geometry,
Object recognition,
Computer science,
Distortion measurement"
Bridging dream and reality: Programmable baseband processors for software-defined radio,"A programmable radio baseband signal processor is one of the essential enablers of software- defined radio. As wireless standards evolve, the processing power needed for baseband processing increases dramatically and the underlying hardware needs to cope with various standards or even simultaneously maintaining several radio links. Meanwhile, the maximum power consumption allowed by mobile terminals is still strictly limited. These challenges require both system and architecture level innovations. This article introduces a design methodology for radio baseband processors discussing the challenges and solutions of radio baseband signal processing. The LeoCore architecture is presented here as an example of a baseband processor design aimed at reducing power and silicon cost while maintaining sufficient flexibility.","Baseband,
Signal processing,
Hardware,
Radio link,
Energy consumption,
Technological innovation,
Design methodology,
Process design,
Silicon,
Costs"
Event-driven gate-level simulation with GP-GPUs,"Logic simulation is a critical component of the design tool flow in modern hardware development efforts. It is used widely from high level descriptions down to gate level ones to validate several aspects of the design, particularly functional correctness. Despite development houses investing vast resources in the simulation task, particularly at the gate level, it is still far from achieving the performance demands required to validate complex modern designs. In this work, we propose the first event driven logic simulator accelerated by a parallel, general purpose graphics processor (GPGPU). Our simulator leverages a gate level event driven design to exploit the benefits of the low switching activity that is typical of large hardware designs. We developed novel algorithms for circuit netlist partitioning and optimized for a highly parallel GPGPU host. Moreover, our flow is structured to extract the best simulation performance from the target hardware platform. We found that our experimental prototype could handle large, industrial scale designs comprised of millions of gates and deliver a 13x speedup on average over current commercial event driven simulators.","Discrete event simulation,
Circuit simulation,
Computational modeling,
Hardware,
Logic design,
Graphics,
Computer architecture,
Computer simulation,
Silicon,
Circuit testing"
SJMR: Parallelizing spatial join with MapReduce on clusters,"MapReduce is a widely used parallel programming model and computing platform. With MapReduce, it is very easy to develop scalable parallel programs to process data-intensive applications on clusters of commodity machines. However, it does not directly support heterogeneous related data sets processing, which is common in operations like spatial joins. This paper presents SJMR (Spatial Join with MapReduce), a novel parallel algorithm to relieve the problem. The strategies include strip-based plane sweeping algorithm, tile-based spatial partitioning function and duplication avoidance technology. We evalauted the performance of SJMR algorithm in various situations with the real world data sets. It demonstrates the applicability of computing-intensive spatial applications with MapReduce on small scale clusters.","Clustering algorithms,
Partitioning algorithms,
Parallel programming,
Concurrent computing,
Computer applications,
Parallel processing,
Logic,
Rivers,
Mathematics,
Computer science"
A strategic deployment and cluster-header selection for wireless sensor networks,"A new cluster-based approach to increase the overall network lifetime of the wireless sensor networks is proposed in this paper. We first consider a strategic deployment especially when there are heterogeneous sensor nodes to be deployed. With the strategic deployment, the cluster header selection scheme is proposed. Unlike conventional approaches, we develop our scheme by designing the network with multiple-sized fixed grids while taking into account the arbitrary-shaped area sensed by the sensor nodes. Our simulation results show that the proposed scheme alleviates high energy consumption and a short lifetime of the wireless sensor networks supported by existing schemes.","Wireless sensor networks,
Clustering algorithms,
Routing,
Energy consumption,
Mechanical sensors,
Energy efficiency,
Computer science,
Wireless communication,
Mechanical systems,
Micromechanical devices"
A Super-Resolution Framework for 3-D High-Resolution and High-Contrast Imaging Using 2-D Multislice MRI,"A novel super-resolution reconstruction (SRR) framework in magnetic resonance imaging (MRI) is proposed. Its purpose is to produce images of both high resolution and high contrast desirable for image-guided minimally invasive brain surgery. The input data are multiple 2-D multislice inversion recovery MRI scans acquired at orientations with regular angular spacing rotated around a common frequency encoding axis. The output is a 3-D volume of isotropic high resolution. The inversion process resembles a localized projection reconstruction problem. Iterative algorithms for reconstruction are based on the projection onto convex sets (POCS) formalism. Results demonstrate resolution enhancement in simulated phantom studies, and ex vivo and in vivo human brain scans, carried out on clinical scanners. A comparison with previously published SRR methods shows favorable characteristics in the proposed approach.","Image resolution,
High-resolution imaging,
Magnetic resonance imaging,
Image reconstruction,
Minimally invasive surgery,
Frequency,
Encoding,
Iterative algorithms,
Brain modeling,
Imaging phantoms"
Development with Off-the-Shelf Components: 10 Facts,Empirical studies have revealed a discrepancy between academic theory and industrial practices regarding the selection and integration of commercial off-the-shelf and open source software components in software system development.,"Open source software,
Software quality,
Risk management,
Computer industry,
Programming,
Costs,
Software systems,
Electronic switching systems,
Software libraries,
Quality management"
HFTL: hybrid flash translation layer based on hot data identification for flash memory,"For the last years, a number of flash translation layers (FTL) have been proposed for hiding erase-before-write architecture of NAND flash memory. However, although many conventional FTLs efficiently provide the logical to physical address remapping algorithms, they could not escape from the performance degradation when handling the hot data which tends to generate so many overwrites on the same logical address. In this paper, we propose a novel FTL algorithm called Hybrid Flash Translation Layer (HFTL) that adaptively exploits the sector mapping and log block based mapping schemes. To do so, HFTL first separates the hot data from the cold data by using the hot data identifier. And then it dynamically manages the former by using the sector mapping scheme showing an optimal performance for intensive overwrites at the same location, and the latter by using the log block based mapping scheme. By using this adaptive hybrid method, HFTL is always guaranteed to yield good performance for the pattern with the hot data as well as the pattern without it. Through a series of experiments, we show that HFTL yields better performance than conventional FTLs.","Flash memory,
Degradation,
Computer architecture,
Computer science,
Consumer electronics,
Digital audio players,
Personal digital assistants,
Mobile handsets,
Portable computers,
Nonvolatile memory"
Secure management of personal health records by applying attribute-based encryption,"The confidentiality of personal health records is a major problem when patients use commercial Web-based systems to store their health data. Traditional access control mechanisms have several limitations with respect to enforcing access control policies and ensuring data confidentiality. In particular, the data has to be stored on a central server locked by the access control mechanism, and the data owner loses control on the data from the moment when the data is sent to the server. Therefore, these mechanisms do not fulfill the requirements of data outsourcing scenarios where the third party storing the data should not have access to the plain data, and it is not trusted to enforce access policies. In this paper, we present a new variant of ciphertext-policy attribute-based encryption (CP-ABE) scheme which is used to enforce patient/organizational access control policies. In CP-ABE, the data is encrypted according to an access policy over a set of attributes. The access policy specifies which attributes a user needs to have in order to decrypt the encrypted data. Once the data is encrypted, it can be safely stored in an untrusted server such that everyone can download the encrypted data but only authorized users who satisfy the access policy can decrypt. The novelty of our construction is that attributes can be from two security domains: social domain (e.g. family, friends, or fellow patients) and professional domain (e.g. doctors or nurses).",Cryptography
Deformable 2D-3D Registration of Vascular Structures in a One View Scenario,"Alignment of angiographic 3D scans to 2D projections is an important issue for 3D depth perception and navigation during interventions. Currently, in a setting where only one 2D projection is available, methods employing a rigid transformation model present the state of the art for this problem. In this work, we introduce a method capable of deformably registering 3D vessel structures to a respective single projection of the scene. Our approach addresses the inherent ill-posedness of the problem by incorporating a priori knowledge about the vessel structures into the formulation. We minimize the distance between the 2D points and corresponding projected 3D points together with regularization terms encoding the properties of length preservation of vessel structures and smoothness of deformation. We demonstrate the performance and accuracy of the proposed method by quantitative tests on synthetic examples as well as real angiographic scenes.","Angiography,
Navigation,
Layout,
Abdomen,
Catheters,
Biomedical imaging,
Medical diagnostic imaging,
Augmented reality,
Visualization,
Encoding"
A Survey and Evaluation of Simulators Suitable for Teaching Courses in Computer Architecture and Organization,"Courses in Computer Architecture and Organization are regularly included in Computer Engineering curricula. These courses are usually organized in such a way that students obtain not only a purely theoretical experience, but also a practical understanding of the topics lectured. This practical work is usually done in a laboratory using simulators of computer systems. Since the open literature contains a variety of simulators being used for such purposes, this paper attempts to give a survey of simulators suitable for teaching courses in computer architecture and organization, to establish the evaluation criteria and to evaluate selected simulators according to these criteria.","Computer science education,
Computer architecture,
Computer aided instruction,
Simulation,
Educational technology"
Multiresolution Spatial and Temporal Coding in a Wireless Sensor Network for Long-Term Monitoring Applications,"In many WSN (wireless sensor network) applications, such as [1], [2], [3], the targets are to provide long-term monitoring of environments. In such applications, energy is a primary concern because sensor nodes have to regularly report data to the sink and need to continuously work for a very long time so that users may periodically request a rough overview of the monitored environment. On the other hand, users may occasionally query more in-depth data of certain areas to analyze abnormal events. These requirements motivate us to propose a multiresolution compression and query (MRCQ) framework to support in-network data compression and data storage in WSNs from both space and time domains. Our MRCQ framework can organize sensor nodes hierarchically and establish multiresolution summaries of sensing data inside the network, through spatial and temporal compressions. In the space domain, only lower resolution summaries are sent to the sink; the other higher resolution summaries are stored in the network and can be obtained via queries. In the time domain, historical data stored in sensor nodes exhibit a finer resolution for more recent data, and a coarser resolution for older data. Our methods consider the hardware limitations of sensor nodes. So, the result is expected to save sensors' energy significantly, and thus, can support long-term monitoring WSN applications. A prototyping system is developed to verify its feasibility. Simulation results also show the efficiency of MRCQ compared to existing work.","Sensors,
Pixel,
Image coding,
Wireless sensor networks,
Spatial resolution,
Monitoring,
Compression algorithms"
Evolutionary multi-objective optimization in robot soccer system for education,"As the robot soccer system becomes stabilized, it has been used as an educational platform with which various topics on mobile robotics can be taught. As one of key topics in the education of mobile robotics is computational intelligence-based navigation, this paper proposes a multiobjective population-based incremental learning (MOPBIL) algorithm to obtain the fuzzy path planner for optimal path to the ball, minimizing three objectives such as elapsed time, heading direction and posture angle errors in a robot soccer system. MOPBIL employs the probabilistic mechanism, which generates new population using probability vectors. As the probability vectors are updated by referring to nondominated solutions, population converges to Pareto-optimal solution set. Simulation and experiment results show the effectiveness of the proposed MOPBIL from the viewpoint of the proximity to the Pareto-optimal set, size of the dominated space, coverage of two sets and diversity metric. By implementing each of the solutions into the educational platform, it can be educated how multi-objective optimization is realized in the real-world problem.","Educational robots,
Navigation,
Fuzzy systems,
Robot vision systems,
Computer science education,
Educational programs,
Control systems,
Evolutionary computation,
Sorting,
Cameras"
Real-time perception-guided motion planning for a personal robot,"This paper presents significant steps towards the online integration of 3D perception and manipulation for personal robotics applications. We propose a modular and distributed architecture, which seamlessly integrates the creation of 3D maps for collision detection and semantic annotations, with a real-time motion replanning framework. To validate our system, we present results obtained during a comprehensive mobile manipulation scenario, which includes the fusion of the above components with a higher level executive.","Motion planning,
Robot sensing systems,
Intelligent robots,
USA Councils,
Layout,
Robustness,
Computer science,
Software safety,
Real time systems,
Intelligent systems"
Generating Fixes from Object Behavior Anomalies,"Advances in recent years have made it possible in some cases to locate a bug (the source of a failure) automatically. But debugging is also about correcting bugs. Can tools do this automatically? The results reported in this paper, from the new PACHIKA tool, suggest that such a goal may be reachable. PACHIKA leverages differences in program behavior to generate program fixes directly. It automatically summarizes executions to object behavior models, determines differences between passing and failing runs, generates possible fixes, and assesses them via the regression test suite. Evaluated on the ASPECTJ bug history, PACHIKA generates a valid fix for 3 out of 18 crashing bugs; each fix pinpoints the bug location and passes the ASPECTJ test suite.","Debugging,
Computer bugs,
Vehicle crash testing,
Software engineering,
Computer crashes,
Java,
Computer science,
Automatic testing,
History,
Programming profession"
Surface charge accumulation and partial discharge activity for small gaps of electrode/epoxy interface in sf6 gas,"The electrical insulation reliability of solid spacers in gas insulated switchgears (GISs) is an important issue to achieve a safe operation of such equipment. Among different phenomena, charge accumulation represents the most important matter that can degrade the overall performance of these insulation systems. For this respect, this paper discusses the contribution of partial discharge (PD) activity by ac voltage application to charge accumulation in the small gap at the electrode/epoxy interface as one of the weakest points in GIS solid spacers. The partial discharge inception voltages for non-accumulated charge case (PDIV0) and after exposing to PD activity (PDIVn) are measured among different gap lengths, simulating delamination at the electrode/epoxy interface. The PD activity is generated using applied voltage with 1.2×PDIV0 for all gap lengths examined in this study (50~500 ¿m). In these measurements, PDIV increased with increasing the number of PD pulses as a result of accumulated charges. The accumulated surface charge density is estimated using the boundary equations and is compared for the different gap lengths. The accumulated charge density was larger for the smaller gap lengths. Comparing PD parameters with accumulated charge density enabled us to identify that the number of negative and positive PD pulses is the main parameter that corresponds to charge accumulation process.","Surface discharges,
Partial discharges,
Electrodes,
Voltage,
Gas insulation,
Solids,
Current measurement,
Pulse measurements,
Dielectrics and electrical insulation,
Switchgear"
Feature vector classification based speech emotion recognition for service robots,"This paper proposes an efficient feature vector classification for Speech Emotion Recognition (SER) in service robots. Since service robots interact with diverse users who are in various emotional states, two important issues should be addressed: acoustically similar characteristics between emotions and variable speaker characteristics due to different user speaking styles. Each of these issues may cause a substantial amount of overlap between emotion models in feature vector space, thus decreasing SER accuracy. In order to reduce the effects caused by such overlaps, this paper proposes an efficient feature vector classification for SER. The conventional feature vector classification applied to speaker identification categorizes feature vectors as overlapped and non-overlapped. Because this method discards all of the overlapped vectors in model reconstruction, it has limitations in constructing robust models when the number of overlapped vectors is significantly increased such as in emotion recognition. The method proposed herein classifies overlapped vectors in a more sophisticated manner, selecting discriminative vectors among overlapped vectors, and adds those vectors in model reconstruction. On SER experiments using an emotional speech corpus, the proposed classification approach exhibited superior performance to conventional methods, and displayed an almost human-level performance. In particular, we achieved commercially applicable performance for two-class (negative vs. non-negative) emotion recognition.","Emotion recognition,
Service robots,
Humans,
Computer science,
Electronic mail,
Speech recognition,
Loudspeakers,
Robustness,
Manufacturing industries,
Hospitals"
Experimental Comparison of Lesion Detectability for Four Fully-3D PET Reconstruction Schemes,"The objective of this work was to evaluate the lesion detection performance of four fully-3D positron emission tomography (PET) reconstruction schemes using experimentally acquired data. A multi-compartment anthropomorphic phantom was set up to mimic whole-body 18F-fluorodeoxyglucose (FDG) cancer imaging and scanned 12 times in 3D mode, obtaining count levels typical of noisy clinical scans. Eight of the scans had 26 68Ge ldquoshell-lessrdquo lesions (6, 8-, 10-, 12-, 16-mm diameter) placed throughout the phantom with various target:background ratios. This provided lesion-present and lesion-absent datasets with known truth appropriate for evaluating lesion detectability by localization receiver operating characteristic (LROC) methods. Four reconstruction schemes were studied: 1) Fourier rebinning (FORE) followed by 2D attenuation-weighted ordered-subsets expectation-maximization, 2) fully-3D AW-OSEM, 3) fully-3D ordinary-Poisson line-of-response (LOR-)OSEM; and 4) fully-3D LOR-OSEM with an accurate point-spread function (PSF) model. Two forms of LROC analysis were performed. First, a channelized nonprewhitened (CNPW) observer was used to optimize processing parameters (number of iterations, post-reconstruction filter) for the human observer study. Human observers then rated each image and selected the most-likely lesion location. The area under the LROC curve ( A LROC) and the probability of correct localization were used as figures-of-merit. The results of the human observer study found no statistically significant difference between FORE and AW-OSEM3D ( A LROC=0.41 and 0.36, respectively), an increase in lesion detection performance for LOR-OSEM3D ( A LROC=0.45, p=0.076), and additional improvement with the use of the PSF model ( A LROC=0.55, p=0.024). The numerical CNPW observer provided the same rankings among algorithms, but obtained different values of A LROC. These results show improved lesion detection performance for the reconstruction algorithms with more sophisticated statistical and imaging models as compared to the previous-generation algorithms.","Lesions,
Positron emission tomography,
Image reconstruction,
Humans,
Whole-body PET,
Imaging phantoms,
Anthropomorphism,
Cancer,
Noise level,
Performance analysis"
Improving robot navigation in structured outdoor environments by identifying vegetation from laser data,"This paper addresses the problem of vegetation detection from laser measurements. The ability to detect vegetation is important for robots operating outdoors, since it enables a robot to navigate more efficiently and safely in such environments. In this paper, we propose a novel approach for detecting low, grass-like vegetation using laser remission values. In our algorithm, the laser remission is modeled as a function of distance, incidence angle, and material. We classify surface terrain based on 3D scans of the surroundings of the robot. The model is learned in a self-supervised way using vibration-based terrain classification. In all real world experiments we carried out, our approach yields a classification accuracy of over 99%. We furthermore illustrate how the learned classifier can improve the autonomous navigation capabilities of mobile robots.","Navigation,
Vegetation mapping,
Mobile robots,
Laser modes,
Surface emitting lasers,
Intelligent robots,
Contracts,
USA Councils,
Optical materials,
Wheelchairs"
Frequency diverse array: Simulation and design,"In this paper, the radiation characteristics of frequency diversity array are concerned. With a set of CW signals of different frequencies transmitted simultaneously from the array, the transient field is examined by electromagnetic field simulation software. A periodically scanning beam is observed and the scanning speed is shown to be related to the frequency increment between two neighboring elements. Based on electromagnetic field simulation results, a low cost frequency diverse array is designed. 4 PLL frequency synthesizers sharing the same reference signal generate the desired signals. The output frequencies can be easily configured and flexibly changed by 16 bit parallel programming.",
Improving API documentation usability with knowledge pushing,"The documentation of API functions typically conveys detailed specifications for the benefit of interested readers. In some cases, however, it also contains usage directives, such as rules or caveats, of which authors of invoking code must be made aware to prevent errors and inefficiencies. There is a risk that these directives may be “lost” within the verbose text, or that the text would not be read because there are so many invoked functions. To address these concerns for Java, an Eclipse plug-in named eMoose decorates method invocations whose targets have associated directives. Our goal is to lead readers to investigate further, which we aid by highlighting the tagged directives in the JavaDoc hover. We present a lab study that demonstrates the directive awareness problem in traditional documentation use and the potential benefits of our approach.","Documentation,
Usability,
Java,
Computer science,
Software systems,
Software libraries,
Application software,
Guidelines,
Runtime,
Inspection"
Real-time volume imaging using a crossed electrode array,"This paper describes a unique crossed electrode array for real-time volume ultrasound imaging. By placing orthogonal linear array electrode patterns on the opposite sides of a hemispherically shaped composite transducer substrate, a 2D array can be fabricated using a small fraction of the elements required for a traditional 2D array. The performance of the array is investigated using a computer simulation of the radiation pattern. We show that by using a 288-element crossed electrode pattern it is possible to collect large field of view volume images (60deg times 60degsector) at real-time frame rates (>20 volume images/s), with image contrast and resolution comparable to what can be obtained using a conventional 128-element linear phased array.","Electrodes,
Ultrasonic imaging,
Phased arrays,
Ultrasonic transducer arrays,
Ultrasonic transducers,
Computer simulation,
Image resolution"
Data replication protocols for mobile ad-hoc networks: a survey and taxonomy,"In mobile ad-hoc networks, frequent network partitioning and the failure of mobile nodes due to exhaustion of their battery power can considerably decrease data availability. In addition, the increase in network size and node mobility cause the performance of data access to degrade. To deal with these issues, a number of data replication protocols have been proposed in the recent years. This paper surveys the existing data replication protocols in mobile ad-hoc networks and proposes a classification scheme that categorizes the protocols into various classes, with respect to the issues they address. Network partitioning, energy consumption, and scalability are the three issues that are identified in this paper, and which have not been previously considered in the fixed networks. The paper also provides a comparison of the protocols and investigates opportunities for future research.","Ad hoc networks,
Taxonomy,
Availability,
Access protocols,
Batteries,
Scalability,
Wireless sensor networks,
Receivers,
Degradation,
Energy consumption"
Accurate Estimation of the Degree Distribution of Private Networks,"We describe an efficient algorithm for releasing a provably private estimate of the degree distribution of a network. The algorithm satisfies a rigorous property of differential privacy, and is also extremely efficient, running on networks of 100 million nodes in a few seconds. Theoretical analysis shows that the error scales linearly with the number of unique degrees, whereas the error of conventional techniques scales linearly with the number of nodes. We complement the theoretical analysis with a thorough empirical analysis on real and synthetic graphs, showing that the algorithm's variance and bias is low, that the error diminishes as the size of the input graph increases, and that common analyses like fitting a power-law can be carried out very accurately.","Data mining,
Analysis of variance,
Algorithm design and analysis,
Social network services,
Chaotic communication,
Distortion measurement,
Computer science,
Data privacy,
Diseases,
Communication networks"
Amplify-and-Forward Relay Networks Under Received Power Constraint,"Relay networks have received considerable attention recently, especially when limited size and power resources impose constraints on the number of antennas at each node. While fixed and mobile relays can cooperate to improve reception at the desired destination, they also contribute to unintended interference for neighboring cells reusing the same frequency. In this paper, we propose and analyze a relay scheme to simultaneously maximize SNR and minimize MSE, for an amplify-and-forward (AF) relay network operating under a receive power constraint guaranteeing that the received signal power is bounded to control interference to neighboring cells. If the intended destination lies at the periphery of the cell, then the proposed scheme guarantees that the total power leaking into neighboring cells is bounded. The optimal relay factors are provided for both correlated and uncorrelated noise at the relays. Simulation results are presented to verify the analysis.","Interference constraints,
Signal to noise ratio,
Digital relays,
Receiving antennas,
Frequency,
Signal analysis,
Analytical models,
Communications technology,
Cost function,
Degradation"
Landmark classification in large-scale image collections,"With the rise of photo-sharing websites such as Facebook and Flickr has come dramatic growth in the number of photographs online. Recent research in object recognition has used such sites as a source of image data, but the test images have been selected and labeled by hand, yielding relatively small validation sets. In this paper we study image classification on a much larger dataset of 30 million images, including nearly 2 million of which have been labeled into one of 500 categories. The dataset and categories are formed automatically from geotagged photos from Flickr, by looking for peaks in the spatial geotag distribution corresponding to frequently-photographed landmarks. We learn models for these landmarks with a multiclass support vector machine, using vector-quantized interest point descriptors as features. We also explore the non-visual information available on modern photo-sharing sites, showing that using textual tags and temporal constraints leads to significant improvements in classification rate. We find that in some cases image features alone yield comparable classification accuracy to using text tags as well as to the performance of human observers.","Large-scale systems,
Object recognition,
Image classification,
Internet,
Testing,
Support vector machines,
Support vector machine classification,
Computer vision,
Computer science,
Facebook"
What's it going to cost you?: Predicting effort vs. informativeness for multi-label image annotations,"Active learning strategies can be useful when manual labeling effort is scarce, as they select the most informative examples to be annotated first. However, for visual category learning, the active selection problem is particularly complex: a single image will typically contain multiple object labels, and an annotator could provide multiple types of annotation (e.g., class labels, bounding boxes, segmentations), any of which would incur a variable amount of manual effort. We present an active learning framework that predicts the tradeoff between the effort and information gain associated with a candidate image annotation, thereby ranking unlabeled and partially labeled images according to their expected “net worth” to an object recognition system. We develop a multi-label multiple-instance approach that accommodates multi-object images and a mixture of strong and weak labels. Since the annotation cost can vary depending on an image's complexity, we show how to improve the active selection by directly predicting the time required to segment an unlabeled image. Given a small initial pool of labeled data, the proposed method actively improves the category models with minimal manual intervention.","Costs,
Image segmentation,
Image recognition,
Labeling,
Object recognition,
Training data,
Learning systems"
Comparison of Analytic and Algebraic Methods for Motion-Compensated Cone-Beam CT Reconstruction of the Thorax,"Respiratory motion is a major concern in cone-beam (CB) computed tomography (CT) of the thorax. It causes artifacts such as blur, streaks, and bands, in particular when using slow-rotating scanners mounted on the gantry of linear accelerators. In this paper, we compare two approaches for motion-compensated CBCT reconstruction of the thorax. The first one is analytic; it is heuristically adapted from the method of Feldkamp, Davis, and Kress (FDK). The second one is algebraic: the system of linear equations is generated using a new algorithm for the projection of deformable volumes and solved using the simultaneous algebraic reconstruction technique (SART). For both methods, we propose to estimate the motion on patient data using a previously acquired 4-D CT image. The methods were tested on two digital and one mechanical motion-controlled phantoms and on a patient dataset. Our results indicate that the two methods correct most motion artifacts. However, the analytic method does not fully correct streaks and bands even if the motion is perfectly estimated due to the underlying approximation. In contrast, the algebraic method allows us full correction of respiratory-induced artifacts.","Motion analysis,
Thorax,
Computed tomography,
Image reconstruction,
Motion estimation,
Linear accelerators,
Equations,
Testing,
Imaging phantoms,
Image motion analysis"
"Modeling and analysis of coupling between TSVs, metal, and RDL interconnects in TSV-based 3D IC with silicon interposer","In this paper, we present a lumped element model for coupled interconnect structures of TSV, metal interconnects, and Redistribution Layer (RDL) in Through-Silicon-Via (TSV)-based 3D IC with silicon interposer. We also analyzed the electrical characteristic of coupling between 3D silicon interposer interconnects. The equivalent lumped model is derived and verified with the S-parameter measurement results. The lumped model for TSV, metal, and RDL combined interconnects is verified with the EM solver simulation results. The S-parameter from the proposed model shows good agreement with the result from the measurement and simulation up to 20GHz. We also proposed shielding structures to suppress coupling between silicon interposer interconnects.","Integrated circuit modeling,
Three-dimensional integrated circuits,
Silicon,
Through-silicon vias,
Integrated circuit interconnections,
Scattering parameters,
Coupling circuits,
Coupled mode analysis,
Integrated circuit packaging,
Electric variables"
FeatureIDE: A tool framework for feature-oriented software development,"Tools support is crucial for the acceptance of a new programming language. However, providing such tool support is a huge investment that can usually not be provided for a research language. With FeatureIDE, we have built an IDE for AHEAD that integrates all phases of feature-oriented software development. To reuse this investment for other tools and languages, we refactored FeatureIDE into an open source framework that encapsulates the common ideas of feature-oriented software development and that can be reused and extended beyond AHEAD. Among others, we implemented extensions for FeatureC++ and FeatureHouse, but in general, FeatureIDE is open for everybody to showcase new research results and make them usable to a wide audience of students, researchers, and practitioners.","Programming,
Computer languages,
Computer science,
Investments,
Open source software,
Java,
Informatics,
Mathematics,
Education,
Availability"
Dependability and security models,"There is a need to quantify system properties methodically. Dependability and security models have evolved nearly independently. Therefore, it is crucial to develop a classification of dependability and security models which can meet the requirement of professionals in both fault-tolerant computing and security community. In this paper, we present a new classification of dependability and security models. First we present the classification of threats and mitigations in systems and networks. And then we present several individual model types such as availability, confidentiality, integrity, performance, reliability, survivability, safety and maintainability. Finally we show that each model type can be combined and represented by one of the model representation techniques: combinatorial (such as reliability block diagrams (RBD), reliability graphs, fault trees, attack trees), state-space (continuous time Markov chains, stochastic Petri nets, fluid stochastic Petri nets, etc) and hierarchical (e.g., fault trees in the upper level and Markov chains in the lower level). We show case studies for each individual model types as well as composite model types.",Security
Practical Deadlock-Free Fault-Tolerant Routing in Meshes Based on the Planar Network Fault Model,"The number of virtual channels required for deadlock-free routing is important for cost-effective and high-performance system design. The planar adaptive routing scheme is an effective deadlock avoidance technique using only three virtual channels for each physical channel in 3D or higher dimensional mesh networks with a very simple deadlock avoidance scheme. However, there exist one idle virtual channel for all physical channels along the first dimension and two idle virtual channels for channels along the last dimension in a mesh network based on the planar adaptive routing algorithm. A new deadlock avoidance technique is proposed for 3D meshes using only two virtual channels by making full use of the idle channels. The deadlock-free adaptive routing scheme is then modified to a deadlock-free adaptive fault-tolerant routing scheme based on a planar network (PN) fault model. The proposed deadlock-free adaptive routing scheme is also extended to n-dimensional meshes still using two virtual channels. Sufficient simulation results are presented to demonstrate the effectiveness of the proposed algorithm.",
Computational Intelligence for Evolving Trading Rules,"This paper describes an adaptive computational intelligence system for learning trading rules. The trading rules are represented using a fuzzy logic rule base, and using an artificial evolutionary process the system learns to form rules that can perform well in dynamic market conditions. A comprehensive analysis of the results of applying the system for portfolio construction using portfolio evaluation tools widely accepted by both the financial industry and academia is provided.","Computational intelligence,
Portfolios,
Australia,
Computer science,
Fuzzy logic,
Fuzzy systems,
Asset management,
Evolutionary computation,
Adaptive systems,
Construction industry"
Classification and separation of partial discharge signals by means of their auto-correlation function evaluation,"This paper describes a K-Means Clustering classification algorithm for the separation of Partial Discharge (PD) signals and pulsating noise due to multiple sources occurring in practical objects. It is based on the comparison of the Auto-Correlation Function (ACF) of the recorded signals assuming that the same source can generate signals having similar ACF while ACF differ when signals with different shapes are compared. The ACF has been selected for its capability of well summarize both time- and frequency-dependent features of the signals. A correlation index that presents the best compromise between strong and weak discrimination among pulses, has been selected out of different distance measurements. The final result of the algorithm is a set of classes containing signals having similar shape which can be processed successively for signal source identification. Meaningful applications of the proposed algorithm are also reported. Improvements in separation effectiveness can enhance the clearness of the PD patterns and, consequently, the quality of the defect identification.","Partial discharges,
Autocorrelation,
Shape,
Signal processing,
Classification algorithms,
Clustering algorithms,
Multi-stage noise shaping,
Signal generators,
Frequency,
Pulse measurements"
Energy-Efficient Computing for Extreme-Scale Science,"A many-core processor design for high-performance systems draws from embedded computing's low-power architectures and design processes, providing a radical alternative to cluster solutions.","Energy efficiency,
Process design,
Embedded computing,
Computer architecture"
Annotation-based empirical performance tuning using Orio,"For many scientific applications, significant time is spent in tuning codes for a particular high-performance architecture. Tuning approaches range from the relatively nonintrusive (e.g., by using compiler options) to extensive code modifications that attempt to exploit specific architecture features. Intrusive techniques often result in code changes that are not easily reversible, and can negatively impact readability, maintainability, and performance on different architectures. We introduce an extensible annotation-based empirical tuning system called Orio that is aimed at improving both performance and productivity. It allows software developers to insert annotations in the form of structured comments into their source code to trigger a number of low-level performance optimizations on a specified code fragment. To maximize the performance tuning opportunities, the annotation processing infrastructure is designed to support both architecture-independent and architecture-specific code optimizations. Given the annotated code as input, Orio generates many tuned versions of the same operation and empirically evaluates the alternatives to select the best performing version for production use. We have also enabled the use of the Pluto automatic parallelization tool in conjunction with Orio to generate efficient OpenMP-based parallel code. We describe our experimental results involving a number of computational kernels, including dense array and sparse matrix operations.",
Integrated scheduling and synthesis of control applications on distributed embedded systems,"Many embedded control systems comprise several control loops that are closed over a network of computation nodes. In such systems, complex timing behavior and communication lead to delay and jitter, which both degrade the performance of each control loop and must be considered during the controller synthesis. Also, the control performance should be taken into account during system scheduling. The contribution of this paper is a control-scheduling co-design method that integrates controller design with both static and priority-based scheduling of the tasks and messages, and in which the overall control performance is optimized.","Control system synthesis,
Distributed control,
Embedded system,
Communication system control,
Control systems,
Processor scheduling,
Network synthesis,
Computer networks,
Embedded computing,
Timing jitter"
Video stabilization using principal component analysis and scale invariant feature transform in particle filter framework,"This paper presents a novel approach to digital video stabilization that uses adaptive particle filter for global motion estimation. In this approach, dimensionality of the feature space is first reduced by the principal component analysis (PCA) method using the features obtained from a scale invariant feature transform (SIFT), and hence the resultant features may be termed as the PCA-SIFT features. The trajectory of these features extracted from video frames is used to estimate undesirable motion between frames. A new cost function called SIFT-BMSE (SIFT Block Mean Square Error) is proposed in adaptive particle filter framework to disregard the foreground object pixels and reduce the computational cost. Frame compensation based on these estimates yields stabilized full-frame video sequences. Experimental results show that the proposed algorithm is both accurate and efficient.","Principal component analysis,
Particle filters,
Motion estimation,
Cameras,
Feature extraction,
Video sequences,
Hardware,
Image processing,
Computer science,
Digital images"
Energy loss in MEMS resonators and the impact on inertial and RF devices,"In this paper, we review the current understanding of energy loss mechanisms in micromachined (MEMS and NEMS) devices. We describe the importance of high quality factor (Q) to the performance of MEMS gyros and MEMS resonators used in radio-frequency applications.","Energy loss,
Micromechanical devices,
Radio frequency,
Phonons,
Solids,
Q factor,
Capacitive sensors,
Microelectromechanical devices,
Damping,
Vibrations"
Protocols for reliable data transport in space internet,"A variety of protocols have been proposed for reliable data transport in space Internet and similar network environments. It is necessary to conduct a survey on these protocols to investigate and compare among them. In this article, we present a survey on the protocols proposed for reliable data transport in space Internet, with a focus on the latest developments. The survey includes the following contents: (1) classification of these protocols into different approaches; (2) discussions and comments on the design and operation methods of the protocols; and (3) comparisons and comments on the main techniques and performance of the protocols.",
A Hypercube-Based Encoding for Evolving Large-Scale Neural Networks,"Research in neuroevolution—that is, evolving artificial neural networks (ANNs) through evolutionary algorithms—is inspired by the evolution of biological brains, which can contain trillions of connections. Yet while neuroevolution has produced successful results, the scale of natural brains remains far beyond reach. This article presents a method called hypercube-based NeuroEvolution of Augmenting Topologies (HyperNEAT) that aims to narrow this gap. HyperNEAT employs an indirect encoding called connective compositional pattern-producing networks (CPPNs) that can produce connectivity patterns with symmetries and repeating motifs by interpreting spatial patterns generated within a hypercube as connectivity patterns in a lower-dimensional space. This approach can exploit the geometry of the task by mapping its regularities onto the topology of the network, thereby shifting problem difficulty away from dimensionality to the underlying problem structure. Furthermore, connective CPPNs can represent the same connectivity pattern at any resolution, allowing ANNs to scale to new numbers of inputs and outputs without further evolution. HyperNEAT is demonstrated through visual discrimination and food-gathering tasks, including successful visual discrimination networks containing over eight million connections. The main conclusion is that the ability to explore the space of regular connectivity patterns opens up a new class of complex high-dimensional tasks to neuroevolution.",
Identifying Architectural Bad Smells,"Certain design fragments in software architectures can have a negative impact on system maintainability. In this paper, we introduce the concept of architectural ""bad smells,"" which are frequently recurring software designs that can have non-obvious and significant detrimental effects on system lifecycle properties. We define architectural smells and differentiate them from related concepts, such as architectural antipatterns and code smells. We also describe four representative architectural smells we encountered in the context of reverse-engineering eighteen grid technologies and refactoring one large industrial system.",
Automatic Generation of Object Usage Specifications from Large Method Traces,"Formal specifications are used to identify programming errors, verify the correctness of programs, and as documentation. Unfortunately, producing them is error-prone and time-consuming, so they are rarely used in practice. Inferring specifications from a running application is a promising solution. However, to be practical, such an approach requires special techniques to treat large amounts of runtime data. We present a scalable dynamic analysis that infers specifications of correct method call sequences on multiple related objects. It preprocesses method traces to identify small sets of related objects and method calls which can be analyzed separately. We implemented our approach and applied the analysis to eleven real-world applications and more than 240 million runtime events. The experiments show the scalability of our approach. Moreover, the generated specifications describe correct and typical behavior, and match existing API usage documentation.",
Large Online Social Footprints--An Emerging Threat,"We study large online social footprints by collecting data on 13,990 active users. After parsing data from 10 of the 15 most popular social networking sites, we find that a user with one social network reveals an average of 4.3 personal information fields. For users with over 8 social networks, this average increases to 8.25 fields. We also investigate the ease by which an attacker can reconstruct a person’s social network profile. Over 40% of an individual’s social footprint can be reconstructed by using a single pseudonym (assuming the attacker guesses the most popular pseudonym), and an attacker can reconstruct 10% to 35% of an individual’s social footprint by using the person’s name. We also perform an initial investigation of matching profiles using public information in a person’s profile.","Social network services,
Size measurement,
Data engineering,
Educational institutions,
Computer science,
MySpace,
Facebook,
Privacy,
Preforms"
"FEATUREHOUSE: Language-independent, automated software composition","Superimposition is a composition technique that has been applied successfully in many areas of software development. Although superimposition is a general-purpose concept, it has been (re)invented and implemented individually for various kinds of software artifacts. We unify languages and tools that rely on superimposition by using the language-independent model of feature structure trees (FSTs). On the basis of the FST model, we propose a general approach to the composition of software artifacts written in different languages, Furthermore, we offer a supporting framework and tool chain, called FEATUREHOUSE. We use attribute grammars to automate the integration of additional languages, in particular, we have integrated Java, C#, C, Haskell, JavaCC, and XML. Several case studies demonstrate the practicality and scalability of our approach and reveal insights into the properties a language must have in order to be ready for superimposition.",
In defense of orthonormality constraints for nonrigid structure from motion,"In factorization approaches to nonrigid structure from motion, the 3D shape of a deforming object is usually modeled as a linear combination of a small number of basis shapes. The original approach to simultaneously estimate the shape basis and nonrigid structure exploited ortho-normality constraints for metric rectification. Recently, it has been asserted that structure recovery through ortho-normality constraints alone is inherently ambiguous and cannot result in a unique solution. This assertion has been accepted as conventional wisdom and is the justification of many remedial heuristics in literature. Our key contribution is to prove that ortho-normality constraints are in fact sufficient to recover the 3D structure from image observations alone. We characterize the true nature of the ambiguity in using ortho-normality constraints for the shape basis and show that it has no impact on structure reconstruction. We conclude from our experimentation that the primary challenge in using shape basis for nonrigid structure from motion is the difficulty in the optimization problem rather than the ambiguity in ortho-normality constraints.","Shape,
Image reconstruction,
Constraint optimization,
Cameras,
Vectors,
Robots,
Deformable models,
Video sequences,
Motion estimation,
Linear approximation"
Simulating Haptic Feedback Using Vision: A Survey of Research and Applications of Pseudo-Haptic Feedback,"This paper presents a survey of the main results obtained in the field of “pseudo-haptic feedback”: a technique meant to simulate haptic sensations in virtual environments using visual feedback and properties of human visuo-haptic perception. Pseudo-haptic feedback uses vision to distort haptic perception and verges on haptic illusions. Pseudo-haptic feedback has been used to simulate various haptic properties such as the stiffness of a virtual spring, the texture of an image, or the mass of a virtual object. This paper describes the several experiments in which these haptic properties were simulated. It assesses the definition and the properties of pseudo-haptic feedback. It also describes several virtual reality applications in which pseudo-haptic feedback has been successfully implemented, such as a virtual environment for vocational training of milling machine operations, or a medical simulator for training in regional anesthesia procedures.",
Bio-inspired node localization in wireless sensor networks,"Many applications of wireless sensor networks (WSNs) require location information of the randomly deployed nodes. A common solution to the localization problem is to deploy a few special beacon nodes having location awareness, which help the ordinary nodes to localize. In this approach, non-beacon nodes estimate their locations using noisy distance measurements from three or more non-collinear beacons they can receive signals from. In this paper, the ranging-based localization task is formulated as a multidimensional optimization problem, and addressed using bio-inspired algorithms, exploiting their quick convergence to quality solutions. An investigation on distributed iterative localization is presented in this paper. Here, the nodes that get localized in an iteration act as references for remaining nodes to localize. The problem has been addressed using particle swarm optimization (PSO) and bacterial foraging algorithm (BFA). A comparison of the performances of PSO and BFA in terms of the number of nodes localized, localization accuracy and computation time is presented.",
Modeling player experience in Super Mario Bros,"This paper investigates the relationship between level design parameters of platform games, individual playing characteristics and player experience. The investigated design parameters relate to the placement and sizes of gaps in the level and the existence of direction changes; components of player experience include fun, frustration and challenge. A neural network model that maps between level design parameters, playing behavior characteristics and player reported emotions is trained using evolutionary preference learning and data from 480 platform game sessions. Results show that challenge and frustration can be predicted with a high accuracy (77.77% and 88.66% respectively) via a simple single-neuron model whereas model accuracy for fun (69.18%) suggests the use of more complex non-linear approximators for this emotion. The paper concludes with a discussion on how the obtained models can be utilized to automatically generate game levels which will enhance player experience.","Game theory,
Predictive models,
Mathematical model,
Algorithm design and analysis,
Augmented reality,
Concurrent computing,
Neural networks,
Process design,
Animation,
Cost function"
Development of a Software Engineering Ontology for Multisite Software Development,"This paper aims to present an ontology model of software engineering to represent its knowledge. The fundamental knowledge relating to software engineering is well described in the textbook entitled Software Engineering by Sommerville that is now in its eighth edition (2004) and the white paper, Software Engineering Body of Knowledge (SWEBOK), by the IEEE (203) upon which software engineering ontology is based. This paper gives an analysis of what software engineering ontology is, what it consists of, and what it is used for in the form of usage example scenarios. The usage scenarios presented in this paper highlight the characteristics of the software engineering ontology. The software engineering ontology assists in defining information for the exchange of semantic project information and is used as a communication framework. Its users are software engineers sharing domain knowledge as well as instance knowledge of software engineering.",
Performance Evaluation of Cloud Computing Offerings,"Advanced computing on cloud computing infrastructures can only become viable alternative for the enterprise if these infrastructures can provide proper levels of nonfunctional properties (NPFs). A company that focuses on service-oriented architectures (SOA) needs to know what configuration would provide the proper levels for individual services if they are deployed in the cloud. In this paper we present an approach for performance evaluation of cloud computing configurations. While cloud computing providers assure certain service levels, this it typically done for the platform and not for a particular service instance. Our approach focuses on NFPs of individual services and thereby provides a more relevant and granular information. An experimental evaluation in Amazon Elastic Compute Cloud (EC2) verified our approach.",
Real-time visual tracking via Incremental Covariance Tensor Learning,"Visual tracking is a challenging problem, as an object may change its appearance due to pose variations, illumination changes, and occlusions. Many algorithms have been proposed to update the target model using the large volume of available information during tracking, but at the cost of high computational complexity. To address this problem, we present a tracking approach that incrementally learns a low-dimensional covariance tensor representation, efficiently adapting online to appearance changes for each mode of the target with only ̃(1) computational complexity. Moreover, a weighting scheme is adopted to ensure less modeling power is expended fitting older observations. Both of these features contribute measurably to improving overall tracking performance. Tracking is then led by the Bayesian inference framework in which a particle filter is used to propagate sample distributions over time. With the help of integral images, our tracker achieves real-time performance. Extensive experiments demonstrate the effectiveness of the proposed tracking algorithm for the targets undergoing appearance variations.",
Using Latent Dirichlet Allocation for automatic categorization of software,"In this paper, we propose a technique called LACT for automatically categorizing software systems in open-source repositories. LACT is based on Latent Dirichlet Allocation, an information retrieval method which is used to index and analyze source code documents as mixtures of probabilistic topics. For an initial evaluation, we performed two studies. In the first study, LACT was compared against an existing tool, MUDABlue, for classifying 41 software systems written in C into problem domain categories. The results indicate that LACT can automatically produce meaningful category names and yield classification results comparable to MUDABlue. In the second study, we applied LACT to 43 software systems written in different programming languages such as C/C++, Java, C#, PHP, and Perl. The results indicate that LACT can be used effectively for the automatic categorization of software systems regardless of the underlying programming language or paradigm. Moreover, both studies indicate that LACT can identify several new categories that are based on libraries, architectures, or programming languages, which is a promising improvement as compared to manual categorization and existing techniques.",
High-accuracy 3D sensing for mobile manipulation: Improving object detection and door opening,"High-resolution 3D scanning can improve the performance of object detection and door opening, two tasks critical to the operation of mobile manipulators in cluttered homes and workplaces. We discuss how high-resolution depth information can be combined with visual imagery to improve the performance of object detection beyond what is (currently) achievable with 2D images alone, and we present door-opening and inventory-taking experiments.","Object detection,
Layout,
Manipulators,
Cameras,
Mobile computing,
Noise figure,
Robotics and automation,
Computer science,
Employment,
Laser transitions"
Locality-Awareness in BitTorrent-Like P2P Applications,"This paper presents the measurement study of locality-aware P2P solutions over real-world Internet autonomous systems (AS) topology. By using the accesses of nodes of PlanetLab testbed, we create a detailed AS-level map including the end-to-end path of all nodes, as well as the relationship of all involved ASes. Based on this map, we evaluate the performance of a set of locality-aware P2P solutions, including an optimal solution guaranteeing the minimum AS hop count, as well as modified BitTorrent system with locality-awareness built into its neighbor selection, peer choking/unchoking, and piece selection processes. Our findings suggest that locality-awareness can help existing P2P solution to significantly decrease load on Internet, and achieve shorter downloading time. By comparing the performance of different kinds of locality-aware and traditional BitTorrent systems, we also point out the necessity to tradeoff between the goals of optimizing AS-related performance and achieving fairness among peers such as intra-AS traffic and peer burden fairness.","Peer to peer computing,
Traffic control,
Topology,
Bandwidth,
Web and internet services,
Computer science,
Extraterrestrial measurements,
Testing,
Internet telephony,
TV broadcasting"
Opportunistic Use of Client Repeaters to Improve Performance of WLANs,"Currently deployed IEEE 802.11 WLANs (Wi-Fi networks) share access point (AP) bandwidth on a per-packet basis. However, various stations communicating with the AP often have different signal qualities, resulting in different transmission rates. This induces a phenomenon known as the rate anomaly problem, in which stations with lower signal quality transmit at lower rates and consume a significant majority of airtime, thereby dramatically reducing the throughput of stations transmitting at higher rates. We propose SoftRepeater, a practical, deployable system in which stations cooperatively address the rate anomaly problem. Specifically, higher rate Wi-Fi stations opportunistically transform themselves into repeaters for lower rate stations when transmitting data to/from the AP. The key challenge is to determine when it is beneficial to enable the repeater functionality. In view of this, we propose an initiation protocol that ensures that repeater functionality is enabled only when appropriate. Also, our system can run directly on top of today's 802.11 infrastructure networks. In addition, we describe a novel, zero-overhead network coding scheme that further alleviates undesirable symptoms of the rate anomaly problem. Using simulation and testbed implementation, we find that SoftRepeater can improve cumulative throughput by up to 200%.","Repeaters,
Throughput,
Testing,
Computer science,
Bandwidth,
Network coding,
Access protocols,
Portable computers,
Hardware,
Spread spectrum communication"
Decentralized Laplacian eigenvalues estimation for networked multi-agent systems,"In this paper we present a novel decentralized algorithm to estimate the eigenvalues of the Laplacian of the network topology of a multi-agent system. The basic idea is to provide a local interaction rule among agents so that their state oscillates only at frequencies corresponding to eigenvalues of the network topology. In this way, the problem of decentralized eigenvalue estimation is mapped into a problem of signal processing, solvable by applying the fast Fourier transform (FFT).","Laplace equations,
Eigenvalues and eigenfunctions,
Multiagent systems,
Signal processing algorithms,
Network topology,
Mobile robots,
Frequency,
Fast Fourier transforms,
Graph theory,
Time varying systems"
Manual and automated media and intima thickness measurements of the common carotid artery,"The intima-media thickness (IMT) of the common carotid artery (CCA) is widely used as an early indicator of the development of cardiovascular disease (CVD). It was proposed but not thoroughly investigated that the media layer (ML) thickness (MLT), its composition, and its texture may be indicative of cardiovascular risk and for differentiating between patients with high and low risk. In this study, we investigate an automated method for segmenting the ML and the intima layer (IL) and measurement of the MLT and the intima layer thickness (ILT) in ultrasound images of the CCA. The snakes segmentation method was used and was evaluated on 100 longitudinal ultrasound images acquired from asymptomatic subjects, against manual segmentation performed by a neurovascular expert. The mean plusmn standard deviation (sd) for the first and second sets of manual and the automated IMT, MLT, and ILT measurements were 0.71 plusmn 0.17 mm, 0.72 plusmn 0.17 mm, 0.67 plusmn 0.12 mm; 0.25 plusmn 0.12 mm, 0.27 plusmn 0.14 mm, 0.25 plusmn 0.11 mm; and 0.43 plusmn 0.10 mm, 0.44 plusmn 0.13 mm, and 0.42 plusmn 0.10 mm, respectively. There was overall no significant difference between the manual and the automated IMC, ML, and IL segmentation measurements. Therefore, the automated segmentation method proposed in this study may be used successfully in the measurement of the MLT and ILT complementing the manual measurements. MLT was also shown to increase with age (for both the manual and the automated measurements). Future research will incorporate the extraction of texture features from the segmented ML and IL bands, which may indicate the risk of future cardiovascular events. However, more work is needed for validating the proposed technique in a larger sample of subjects.",
Fast car detection using image strip features,"This paper presents a fast method for detecting multi-view cars in real-world scenes. Cars are artificial objects with various appearance changes, but they have relatively consistent characteristics in structure that consist of some basic local elements. Inspired by this, we propose a novel set of image strip features to describe the appearances of those elements. The new features represent various types of lines and arcs with edge-like and ridge-like strip patterns, which significantly enrich the simple features such as haar-like features and edgelet features. They can also be calculated efficiently using the integral image. Moreover, we develop a new complexity-aware criterion for RealBoost algorithm to balance the discriminative capability and efficiency of the selected features. The experimental results on widely used single view and multi-view car datasets show that our approach is fast and has good performance.","Strips,
Face detection,
Object detection,
Biological system modeling,
Deformable models,
Computer vision,
Content addressable storage,
Support vector machines,
Humans,
Wheels"
Hands-On Remote Labs: Collaborative Web Laboratories as a Case Study for IT Engineering Classes,"The development of a reusable collaborative software framework for the remote control of a large range of laboratory equipments is an interesting topic to teach information technologies in an engineering school. The design and implementation of this kind of framework, in fact, requires the ability to integrate skills about software engineering, computer networks, human-computer interaction, distributed architectures, and remote control of hardware devices (i.e., laboratory equipments). In the paper, we describe our experience in the development of a reusable framework for remote laboratories, which has been adopted as a case study in two different scenarios at our university.","Laboratories,
Servers,
Collaboration,
Streaming media,
Microscopy,
Software,
Computers"
ESC: Energy Synchronized Communication in sustainable sensor networks,"With advances in energy harvesting techniques, it is now feasible to build sustainable sensor networks (SSN) to support long-term applications. Unlike battery-powered sensor networks, the objective of sustainable sensor networks is to effectively utilize a continuous stream of ambient energy. Instead of pushing the limits of energy conservation, we are aiming at energy-synchronized designs1 to keep energy supplies and demands in balance. Specifically, this work presents the Energy Synchronized Communication (ESC) as a transparent middle-ware between the network layer and data link layer that controls the amount and timing of RF activity at receiving nodes. In this work, we first derive a delay model for cross-traffic at individual nodes, which reveals an interesting stair effect in low-duty-cycle networks. This effect allows us to design a localized energy synchronization control with O(1) time complexity that shuffles or adjusts the working schedule of a node to optimize cross-traffic delays in the presence of changing duty-cycle budgets. Under different rates of energy fluctuations, shuffle-based and adjustment-based methods have different influences on logical connectivity and cross-traffic delay, due to the inconsistent views of working schedules among neighboring nodes before schedule updates. We study the trade-off between them and propose methods to update working schedules efficiently. To evaluate our work, ESC is implemented on MicaZ nodes with two state-of-the-art routing protocols. Both test-bed experiment and large scale simulation results show significant performance improvements over randomized synchronization controls.","Delay effects,
Energy conservation,
Supply and demand,
Communication system control,
Timing,
Radio frequency,
Design optimization,
Fluctuations,
Routing protocols,
Testing"
Scalable and Interactive Segmentation and Visualization of Neural Processes in EM Datasets,"Recent advances in scanning technology provide high resolution EM (electron microscopy) datasets that allow neuro-scientists to reconstruct complex neural connections in a nervous system. However, due to the enormous size and complexity of the resulting data, segmentation and visualization of neural processes in EM data is usually a difficult and very time-consuming task. In this paper, we present NeuroTrace, a novel EM volume segmentation and visualization system that consists of two parts: a semi-automatic multiphase level set segmentation with 3D tracking for reconstruction of neural processes, and a specialized volume rendering approach for visualization of EM volumes. It employs view-dependent on-demand filtering and evaluation of a local histogram edge metric, as well as on-the-fly interpolation and ray-casting of implicit surfaces for segmented neural structures. Both methods are implemented on the GPU for interactive performance. NeuroTrace is designed to be scalable to large datasets and data-parallel hardware architectures. A comparison of NeuroTrace with a commonly used manual EM segmentation tool shows that our interactive workflow is faster and easier to use for the reconstruction of complex neural processes.","Image reconstruction,
Data visualization,
Surface reconstruction,
Hardware,
Rendering (computer graphics),
Scanning electron microscopy,
Nervous system,
Level set,
Filtering,
Histograms"
Experimental Validation of the Learning Effect for a Pedagogical Game on Computer Fundamentals,"The question/answer-based computer game Age of Computers was introduced to replace traditional weekly paper exercises in a course in computer fundamentals in 2003. Questionnaire evaluations and observation of student behavior have indicated that the students found the game more motivating than paper exercises and that a majority of the students also perceived the game to have a higher learning effect than paper exercises or textbook reading. This paper reports on a controlled experiment to compare the learning effectiveness of game play with traditional paper exercises, as well as with textbook reading. The results indicated that with equal time being spent on the various learning activities, the effect of game play was only equal to that of the other activities, not better. Yet this result is promising enough, as the increased motivation means that students work harder in the course. Also, the results indicate that the game has potential for improvement, in particular with respect to its feedback on the more complicated questions.",
Defense against Sybil attack in vehicular ad hoc network based on roadside unit support,"In this paper, we propose a timestamp series approach to defend against Sybil attack in a vehicular ad hoc network (VANET) based on roadside unit support. The proposed approach targets the initial deployment stage of VANET when basic roadside unit (RSU) support infrastructure is available and a small fraction of vehicles have network communication capability. Unlike previously proposed schemes that require a dedicated vehicular public key infrastructure to certify individual vehicles, in our approach RSUs are the only components issuing the certificates. Due to the differences of moving dynamics among vehicles, it is rare to have two vehicles passing by multiple RSUs at exactly the same time. By exploiting this spatial and temporal correlation between vehicles and RSUs, two messages will be treated as Sybil attack issued by one vehicle if they have the similar timestamp series issued by RSUs. The timestamp series approach needs neither vehicular-based public-key infrastructure nor Internet accessible RSUs, which makes it an economical solution suitable for the initial stage of VANET.","Ad hoc networks,
Satellites,
Protection,
Frequency selective surfaces,
Costs,
Interference,
Proposals,
Military standards,
Error correction,
Vehicles"
The use of socially assistive robots in the design of intelligent cognitive therapies for people with dementia,"Currently the 2 percent growth rate for the world's older population exceeds the 1.2 rate for the world's population as a whole. By 2050, the number of individuals over the age 85 is projected to be three times more than there is today. Most of these individuals will need physical, emotional, and cognitive assistance. In this paper, we present a new adaptive robotic system based on the socially assistive robotics (SAR) technology that tries to provide a customized help protocol through motivation, encouragements, and companionship to users suffering from cognitive changes related to aging and/or Alzheimer's disease. Our results show that this approach can engage the patients and keep them interested in interacting with the robot, which, in turn, increases their positive behavior.",
Top-down color attention for object recognition,"Generally the bag-of-words based image representation follows a bottom-up paradigm. The subsequent stages of the process: feature detection, feature description, vocabulary construction and image representation are performed independent of the intentioned object classes to be detected. In such a framework, combining multiple cues such as shape and color often provides below-expected results.","Object recognition,
Shape,
Image representation,
Vocabulary,
Computer vision,
Histograms,
Object detection,
Computer science,
Information analysis,
Image color analysis"
Estimating human shape and pose from a single image,"We describe a solution to the challenging problem of estimating human body shape from a single photograph or painting. Our approach computes shape and pose parameters of a 3D human body model directly from monocular image cues and advances the state of the art in several directions. First, given a user-supplied estimate of the subject's height and a few clicked points on the body we estimate an initial 3D articulated body pose and shape. Second, using this initial guess we generate a tri-map of regions inside, outside and on the boundary of the human, which is used to segment the image using graph cuts. Third, we learn a low-dimensional linear model of human shape in which variations due to height are concentrated along a single dimension, enabling height-constrained estimation of body shape. Fourth, we formulate the problem of parametric human shape from shading. We estimate the body pose, shape and reflectance as well as the scene lighting that produces a synthesized body that robustly matches the image evidence. Quantitative experiments demonstrate how smooth shading provides powerful constraints on human shape. We further demonstrate a novel application in which we extract 3D human models from archival photographs and paintings.",
Message passing on data-parallel architectures,"This paper explores the challenges in implementing a message passing interface usable on systems with data-parallel processors. As a case study, we design and implement the “DCGN” API on NVIDIA GPUs that is similar to MPI and allows full access to the underlying architecture. We introduce the notion of data-parallel thread-groups as a way to map resources to MPI ranks. We use a method that also allows the data-parallel processors to run autonomously from user-written CPU code. In order to facilitate communication, we use a sleep-based polling system to store and retrieve messages. Unlike previous systems, our method provides both performance and flexibility. By running a test suite of applications with different communication requirements, we find that a tolerable amount of overhead is incurred, somewhere between one and five percent depending on the application, and indicate the locations where this overhead accumulates. We conclude that with innovations in chipsets and drivers, this overhead will be mitigated and provide similar performance to typical CPU-based MPI implementations while providing fully-dynamic communication.",
A shape-based object class model for knowledge transfer,"Object class models trained on hundreds or thousands of images have shown to enable robust detection. Transferring knowledge from such models to new object classes trained from a few or even as little as one training instance however is still in its infancy. This paper designs a shape-based model that allows to easily and explicitly transfer knowledge on three different levels: transfer of individual parts' shape and appearance information, transfer of local symmetry between parts, and transfer of part topology. Due to the factorized form of the model, knowledge can either be transferred for the complete model or just partial knowledge corresponding to certain aspects of the model. The experiments clearly demonstrate that the proposed model is competitive with the state-of-the-art and enables both full and partial knowledge transfer.","Knowledge transfer,
Shape,
Object detection,
Training data,
Machine learning,
Robustness,
Computer vision,
Computer science,
Topology,
Animals"
Voice Transformation: A survey,"Voice transformation refers to the various modifications one may apply to the sound produced by a person, speaking or singing. Voice Transformation is usually seen as an add-on or an external system in speech synthesis systems since it may create virtual voices in a simple and flexible way. In this paper we review the state-of-the-art Voice Transformation methodology showing its limitations in producing good speech quality and its current challenges. Addressing quality issues of current voice transformation algorithms in conjunction with properties of the speech production and speech perception systems we try to pave the way for more natural Voice Transformation algorithms in the future. Facing the challenges, will allow Voice Transformation systems to be applied in important and versatile areas of speech technology; applications that are far beyond speech synthesis.",
A single-letter characterization of optimal noisy compressed sensing,"Compressed sensing deals with the reconstruction of a high-dimensional signal from far fewer linear measurements, where the signal is known to admit a sparse representation in a certain linear space. The asymptotic scaling of the number of measurements needed for reconstruction as the dimension of the signal increases has been studied extensively. This work takes a fundamental perspective on the problem of inferring about individual elements of the sparse signal given the measurements, where the dimensions of the system become increasingly large. Using the replica method, the outcome of inferring about any fixed collection of signal elements is shown to be asymptotically decoupled, i.e., those elements become independent conditioned on the measurements. Furthermore, the problem of inferring about each signal element admits a single-letter characterization in the sense that the posterior distribution of the element, which is a sufficient statistic, becomes asymptotically identical to the posterior of inferring about the same element in scalar Gaussian noise. The result leads to simple characterization of all other elemental metrics of the compressed sensing problem, such as the mean squared error and the error probability for reconstructing the support set of the sparse signal. Finally, the single-letter characterization is rigorously justified in the special case of sparse measurement matrices where belief propagation becomes asymptotically optimal.",
A simple tree search method for playing Ms. Pac-Man,"Ms. Pac-Man is a challenging game for software agents that has been the focus of a significant amount of research. This paper describes the current state of a tree-search software agent that will be entered into the IEEE CIG 2009 screen-capture based Ms. Pac-Man software agent competition. While game-tree search is a staple technique for many games, this paper is, perhaps surprisingly, the first attempt we know of to apply it to Ms. Pac-Man. The approach we take is to expand a route-tree based on possible moves that the Ms. Pac-Man agent can take to depth 40, and evaluate which path is best using hand-coded heuristics. On a simulator of the game our agent has achieved a high score of 40,000, but only around 15,000 on the original game using a screen-capture interface. Our next steps are focussed on using an improved screen-capture system, and on using evolutionary algorithms to tune the parameters of the agent.","Search methods,
Computational intelligence,
Learning,
Games,
Software agents,
Artificial intelligence,
Competitive intelligence,
Computational modeling,
Evolutionary computation,
Genetic programming"
Encoding Probabilistic Brain Atlases Using Bayesian Inference,"This paper addresses the problem of creating probabilistic brain atlases from manually labeled training data. Probabilistic atlases are typically constructed by counting the relative frequency of occurrence of labels in corresponding locations across the training images. However, such an ldquoaveragingrdquo approach generalizes poorly to unseen cases when the number of training images is limited, and provides no principled way of aligning the training datasets using deformable registration. In this paper, we generalize the generative image model implicitly underlying standard ldquoaveragerdquo atlases, using mesh-based representations endowed with an explicit deformation model. Bayesian inference is used to infer the optimal model parameters from the training data, leading to a simultaneous group-wise registration and atlas estimation scheme that encompasses standard averaging as a special case. We also use Bayesian inference to compare alternative atlas models in light of the training data, and show how this leads to a data compression problem that is intuitive to interpret and computationally feasible. Using this technique, we automatically determine the optimal amount of spatial blurring, the best deformation field flexibility, and the most compact mesh representation. We demonstrate, using 2-D training datasets, that the resulting models are better at capturing the structure in the training data than conventional probabilistic atlases. We also present experiments of the proposed atlas construction technique in 3-D, and show the resulting atlases' potential in fully-automated, pulse sequence-adaptive segmentation of 36 neuroanatomical structures in brain MRI scans.",
Optimal coverage for multiple hovering robots with downward facing cameras,This paper presents a distributed control strategy for deploying hovering robots with multiple downward facing cameras to collectively monitor an environment. Information per pixel is proposed as an optimization criterion for multi-camera placement problems. This metric is used to derive a specific cost function for multiple downward facing cameras mounted on hovering robot platforms. The cost function leads to a gradient-based distributed controller for positioning the robots. A convergence proof using LaSalle's invariance principle is given to show that the robots converge to locally optimal positions. The controller is demonstrated in experiments with three flying quad-rotor robots.,
Work-first and help-first scheduling policies for async-finish task parallelism,"Multiple programming models are emerging to address an increased need for dynamic task parallelism in applications for multicore processors and shared-address-space parallel computing. Examples include OpenMP 3.0, Java Concurrency Utilities, Microsoft Task Parallel Library, Intel Thread Building Blocks, Cilk, X10, Chapel, and Fortress. Scheduling algorithms based on work stealing, as embodied in Cilk's implementation of dynamic spawn-sync parallelism, are gaining in popularity but also have inherent limitations. In this paper, we address the problem of efficient and scalable implementation of X10's async-finish task parallelism, which is more general than Cilk's spawn-sync parallelism. We introduce a new work-stealing scheduler with compiler support for async-finish task parallelism that can accommodate both work-first and help-first scheduling policies. Performance results on two different multicore SMP platforms show significant improvements due to our new work-stealing algorithm compared to the existing work-sharing scheduler for X10, and also provide insights on scenarios in which the help-first policy yields better results than the work-first policy and vice versa.",
Curvature regularity for region-based image segmentation and inpainting: A linear programming relaxation,"We consider a class of region-based energies for image segmentation and inpainting which combine region integrals with curvature regularity of the region boundary. To minimize such energies, we formulate an integer linear program which jointly estimates regions and their boundaries. Curvature regularity is imposed by respective costs on pairs of adjacent boundary segments.",
Active circuit to through silicon via (TSV) noise coupling,"In this paper, we propose a coupling model between through silicon via (TSV) and substrate based on a 3-Dimensional transmission line matrix (3D-TLM), which utilizes equivalent lumped circuit model of silicon substrate and TSV. The proposed model is verified by S-parameter simulations using a 3D field solver and analyzed with various structural parameters: TSV diameter, distance between TSV and noise source, and silicon substrate height. Based on the model, timing jitter degradation on phase locked loop (PLL) caused by substrate noise coupling is investigated. A shielding technique using a guard ring structure is applied to suppress the coupling noise.","Active circuits,
Silicon,
Through-silicon vias,
Active noise reduction,
Circuit noise,
Coupling circuits,
Transmission line matrix methods,
Phase locked loops,
Distributed parameter circuits,
Scattering parameters"
The Effect of Noise Correlation in Amplify-and-Forward Relay Networks,"In wireless relay networks, noise at the relays can be correlated possibly due to common interference or noise propagation from preceding hops. A parallel relay network with noise correlation is considered in this network. For the relay strategy of amplify and forward (AF), the optimal rate maximizing relay gains when correlation knowledge is available at the relays are determined. Interestingly, it is shown that, on average, noise correlation is beneficial regardless of whether the relays know the noise covariance matrix. However, the knowledge of correlation can greatly improve the performance. Typically, the performance improvement from correlation knowledge increases with the relay power and the number of relays. With perfect correlation knowledge the system is capable of canceling interference if the number of interferers is less than the number of relays. For a two-hop multiple-access parallel network, closed-form expressions for the maximum sum rate and the optimal relay strategy are determined. Relay optimization for networks with three hops is also considered. Based on the result of two-hop network with noise correlation, an iterative algorithm is proposed for solving the relay optimization problem for three-hop networks.","Power system relaying,
Semiconductor device noise,
Digital relays,
Noise cancellation,
Decoding,
Computer science,
Covariance matrix,
Interference cancellation,
Closed-form solution,
Iterative algorithms"
Distributed Compressed Video Sensing,"This paper proposes a novel framework called Distributed Compressed Video Sensing (DISCOS) - a solution for Distributed Video Coding (DVC) based on the Compressed Sensing (CS) theory. The DISCOS framework compressively samples each video frame independently at the encoder and recovers video frames jointly at the decoder by exploiting an interframe sparsity model and by performing sparse recovery with side information. Simulation results show that DISCOS significantly outperforms the baseline CS-based scheme of intraframe-coding and intraframe-decoding. Moreover, our DISCOS framework can perform most encoding operations in the analog domain with very low-complexity. This makes DISCOS a promising candidate for real-time, practical applications where the analog to digital conversion is expensive, e.g., in Terahertz imaging.",
Fixed-rate power allocation strategies for enhanced secrecy in MIMO wiretap channels,"This paper studies the use of artificial interference in reducing the likelihood that a confidential message transmitted between two multi-antenna nodes is intercepted by a passive eavesdropper. A portion of the transmit power is used to broadcast the information signal with just enough power to guarantee a certain data rate for the intended receiver, and the remainder of the power is used to broadcast artificial noise in order to mask the desired signal from a potential eavesdropper. The interference is designed to be orthogonal to the multiple-stream information signal when it reaches the desired receiver. A modified water-filling algorithm is proposed that balances the required transmit power with the number of spatial dimensions available for jamming the eavesdropper. Numerical results verify the increase in secrecy capacity under the proposed transmission scheme.","MIMO,
Interference,
Broadcasting,
Transmitters,
Wireless communication,
Cryptography,
Degradation,
Transmitting antennas,
Computer science,
Signal design"
Millimeter-wave soldier-to-soldier communications for covert battlefield operations,"Mobile ad hoc networking of dismounted combat personnel is expected to play an important role in the future of network-centric operations. High-speed, short-range, soldier-to-soldier wireless communications will be required to relay information on situational awareness, tactical instructions, and covert surveillance related data during special operations reconnaissance and other missions. This article presents some of the work commissioned by the U.K. Ministry of Defence to assess the feasibility of using 60 GHz millimeter-wave smart antenna technology to provide covert communications capable of meeting these stringent networking needs. Recent advances in RF front-end technology, alongside physical layer transmission schemes that could be employed in millimeter-wave soldier- mounted radio, are discussed. The introduction of covert communications between soldiers will require the development of a bespoke directive medium access layer. A number of adjustments to the IEEE 802.11 distribution coordination function that will enable directional communications are suggested. The successful implementation of future smart antenna technologies and direction of arrival-based protocols will be highly dependent on thorough knowledge of transmission channel characteristics prior to deployment. A novel approach to simulating dynamic soldier-to-soldier signal propagation using state-of-the-art animation-based technology developed for computer game design is described, and important channel metrics such as root mean square angle and delay spread for a team of four networked infantry soldiers over a range of indoor and outdoor environments is reported.","Millimeter wave technology,
Directive antennas,
Mobile communication,
Personnel,
Wireless communication,
Relays,
Surveillance,
Reconnaissance,
Radio frequency,
Physical layer"
Compute Unified Device Architecture Application Suitability,"Graphics processing units (GPUs) can provide excellent speedups on some, but not all, general-purpose workloads. Using a set of computational GPU kernels as examples, the authors show how to adapt kernels to utilize the architectural features of a GeForce 8800 GPU and what finally limits the achievable performance.","Computer architecture,
Kernel,
Graphics,
Central Processing Unit,
Hardware,
Parallel processing,
Phased arrays,
Costs,
Multicore processing"
On the Effects of Adding Objectives to Plateau Functions,"In this paper, we examine how adding objectives to a given optimization problem affects the computational effort required to generate the set of Pareto-optimal solutions. Experimental studies show that additional objectives may change the running time behavior of an algorithm drastically. Often it is assumed that more objectives make a problem harder as the number of different tradeoffs may increase with the problem dimension. We show that additional objectives, however, may be both beneficial and obstructive depending on the chosen objective. Our results are obtained by rigorous running time analyses that show the different effects of adding objectives to a well-known plateau function. Additional experiments show that the theoretically shown behavior can be observed for problems with more than one objective.",
Extending Grids with cloud resource management for scientific computing,"From its start using supercomputers, scientific computing constantly evolved to the next levels such as cluster computing, meta-computing, or computational Grids. Today, Cloud Computing is emerging as the paradigm for the next generation of large-scale scientific computing, eliminating the need of hosting expensive computing hardware. Scientists still have their Grid environments in place and can benefit from extending them by leased Cloud resources whenever needed. This paradigm shift opens new problems that need to be analyzed, such as integration of this new resource class into existing environments, applications on the resources and security. The virtualization overheads for deployment and starting of a virtual machine image are new factors which will need to be considered when choosing scheduling mechanisms. In this paper we investigate the usability of compute Clouds to extend a Grid workflow middleware and show on a real implementation that this can speed up executions of scientific workflows.","Resource management,
Scientific computing,
Grid computing,
Cloud computing,
Supercomputers,
Large-scale systems,
Hardware,
Security,
Virtual machining,
Processor scheduling"
Temporal Outlier Detection in Vehicle Traffic Data,"Outlier detection in vehicle traffic data is a practical problem that has gained traction lately due to an increasing capability to track moving vehicles in city roads. In contrast to other applications, this particular domain includes a very dynamic dimension: time. Many existing algorithms have studied the problem of outlier detection at a single instant in time. This study proposes a method for detecting temporal outliers with an emphasis on historical similarity trends between data points. Outliers are calculated from drastic changes in the trends. Experiments with real world traffic data show that this approach is effective and efficient.",
"Multi-robot, multi-target Particle Swarm Optimization search in noisy wireless environments",Multiple small robots (swarms) can work together using Particle Swarm Optimization (PSO) to perform tasks that are difficult or impossible for a single robot to accomplish. The problem considered in this paper is exploration of an unknown environment with the goal of finding a target(s) at an unknown location(s) using multiple small mobile robots. This work demonstrates the use of a distributed PSO algorithm with a novel adaptive RSS weighting factor to guide robots for locating target(s) in high risk environments. The approach was developed and analyzed on multiple robot single and multiple target search. The approach was further enhanced by the multi-robot-multi-target search in noisy environments. The experimental results demonstrated how the availability of radio frequency signal can significantly affect robot search time to reach a target.,
"Using the Model-Based Residual Bootstrap to Quantify Uncertainty in Fiber Orientations From
Q
-Ball Analysis","Bootstrapping of repeated diffusion-weighted image datasets enables nonparametric quantification of the uncertainty in the inferred fiber orientation. The wild bootstrap and the residual bootstrap are model-based residual resampling methods which use a single dataset. Previously, the wild bootstrap method has been presented as an alternative to conventional bootstrapping for diffusion tensor imaging. Here we present a study of an implementation of model-based residual bootstrapping using q -ball analysis and compare the outputs with conventional bootstrapping. We show that model-based residual bootstrap q-ball generates results that closely match the output of the conventional bootstrap. Both the residual and conventional bootstrap of multifiber methods can be used to estimate the probability of different numbers of fiber populations existing in different brain tissues. Also, we have shown that these methods can be used to provide input for probabilistic tractography, avoiding existing limitations associated with data calibration and model selection.","Uncertainty,
Biomedical imaging,
Optical fiber testing,
Magnetic resonance imaging,
Humans,
Calibration,
Image resolution,
Signal resolution,
High-resolution imaging,
Biomedical engineering"
Reconfigurable rateless codes,"We propose novel reconfigurable rateless codes, that are capable of not only varying the block length but also adaptively modify their encoding strategy by incrementally adjusting their degree distribution according to the prevalent channel conditions without the availability of the channel state information at the transmitter. In particular, we characterize a reconfigurable rateless code designed for the transmission of 9,500 information bits that achieves a performance, which is approximately 1 dB away from the discrete-input continuous-output memoryless channel's (DCMC) capacity over a diverse range of channel signal-to-noise (SNR) ratios.","Transmitters,
Channel state information,
Channel coding,
Iterative decoding,
Availability,
Signal design,
Channel capacity,
Parity check codes,
Iterative algorithms,
Signal to noise ratio"
Distributed peer-to-peer beamforming for multiuser relay networks,"A computationally efficient distributed beamforming technique for multi-user relay networks is developed. The channel state information is assumed to be known at the relays or destinations, and the total relay transmitted power is minimized subject to the destination quality-of-service constraints. It is shown that this problem can be approximately converted to a convex second-order cone programming form. As a result, the proposed network beamforming technique offers a substantially reduced computational complexity than earlier state-of-the-art techniques that are based on semidefinite relaxation.",
Sampling and Visualizing Creases with Scale-Space Particles,"Particle systems have gained importance as a methodology for sampling implicit surfaces and segmented objects to improve mesh generation and shape analysis. We propose that particle systems have a significantly more general role in sampling structure from unsegmented data. We describe a particle system that computes samplings of crease features (i.e. ridges and valleys, as lines or surfaces) that effectively represent many anatomical structures in scanned medical data. Because structure naturally exists at a range of sizes relative to the image resolution, computer vision has developed the theory of scale-space, which considers an n-D image as an (n + 1)-D stack of images at different blurring levels. Our scale-space particles move through continuous four-dimensional scale-space according to spatial constraints imposed by the crease features, a particle-image energy that draws particles towards scales of maximal feature strength, and an inter-particle energy that controls sampling density in space and scale. To make scale-space practical for large three-dimensional data, we present a spline-based interpolation across scale from a small number of pre-computed blurrings at optimally selected scales. The configuration of the particle system is visualized with tensor glyphs that display information about the local Hessian of the image, and the scale of the particle. We use scale-space particles to sample the complex three-dimensional branching structure of airways in lung CT, and the major white matter structures in brain DTI.",
Real-Time Reconstruction of Sensitivity Encoded Radial Magnetic Resonance Imaging Using a Graphics Processing Unit,"A barrier to the adoption of non-Cartesian parallel magnetic resonance imaging for real-time applications has been the times required for the image reconstructions. These times have exceeded the underlying acquisition time thus preventing real-time display of the acquired images. We present a reconstruction algorithm for commodity graphics hardware (GPUs) to enable real time reconstruction of sensitivity encoded radial imaging (radial SENSE). We demonstrate that a radial profile order based on the golden ratio facilitates reconstruction from an arbitrary number of profiles. This allows the temporal resolution to be adjusted on the fly. A user adaptable regularization term is also included and, particularly for highly undersampled data, used to interactively improve the reconstruction quality. Each reconstruction is fully self-contained from the profile stream, i.e., the required coil sensitivity profiles, sampling density compensation weights, regularization terms, and noise estimates are computed in real-time from the acquisition data itself. The reconstruction implementation is verified using a steady state free precession (SSFP) pulse sequence and quantitatively evaluated. Three applications are demonstrated; real-time imaging with real-time SENSE 1) or k-t SENSE 2) reconstructions, and 3) offline reconstruction with interactive adjustment of reconstruction settings.","Image reconstruction,
Magnetic resonance imaging,
Graphics,
Displays,
Reconstruction algorithms,
Hardware,
High-resolution imaging,
Streaming media,
Coils,
Sampling methods"
MIMO-assisted hard versus soft decoding-and-forwarding for network coding aided relaying systems,"This paper proposes two types of new decoding algorithms for a network coding aided relaying (NCR) system, which adopts multiple antennas at both the transmitter and receiver. In the NCR system, the relay station (RS) decodes the data received from both the base station (BS) as well as from the mobile station (MS) and combines the decoded signals into a single data stream before forwarding it to both. In this paper, we consider the realistic scenario of encountering decoding errors at the RS, which results in erroneous forwarded data. Under this assumption, we derive decoding algorithms for both the BS and the MS in order to reduce the deleterious effects of imperfect decoding at the RS. We first propose a decoding algorithm for a hard decision based forwarding (HDF) system. Then, for the sake of achieving further performance improvements, we also employ soft decision forwarding (SDF) and propose a novel error model, which divides the error pattern into two components: hard and soft errors. Given this error model, we then modify the HDF decoder for employment in SDF systems. We also derive estimation algorithms for their parameters that are required for the efficient operation of the proposed decoders. Our simulation results show that the proposed algorithms provide substantial performance improvements in terms of the attainable packet error rate as a benefit of our more accurate error model.",
Learning sign language by watching TV (using weakly aligned subtitles),"The goal of this work is to automatically learn a large number of British sign language (BSL) signs from TV broadcasts. We achieve this by using the supervisory information available from subtitles broadcast simultaneously with the signing. This supervision is both weak and noisy: it is weak due to the correspondence problem since temporal distance between sign and subtitle is unknown and signing does not follow the text order; it is noisy because subtitles can be signed in different ways, and because the occurrence of a subtitle word does not imply the presence of the corresponding sign. The contributions are: (i) we propose a distance function to match signing sequences which includes the trajectory of both hands, the hand shape and orientation, and properly models the case of hands touching; (ii) we show that by optimizing a scoring function based on multiple instance learning, we are able to extract the sign of interest from hours of signing footage, despite the very weak and noisy supervision. The method is automatic given the English target word of the sign to be learnt. Results are presented for 210 words including nouns, verbs and adjectives.","Handicapped aids,
Training data,
TV broadcasting,
Video sequences,
Shape,
Noise shaping,
Multi-stage noise shaping,
History,
Cranes,
Deafness"
Service Mining on the Web,"The Web is transforming from a Web of data to a Web of both Semantic data and services. This trend is providing us with increasing opportunities to compose potentially interesting and useful services from existing services. While we may not sometimes have the specific queries needed in top-down service composition approaches to identify them, the early and proactive exposure of these opportunities will be key to harvest the great potential of the large body of Web services. In this paper, we propose a Web service mining framework that allows unexpected and interesting service compositions to automatically emerge in a bottom-up fashion. We present several mining techniques aiming at the discovery of such service compositions. We also present evaluation measures of their interestingness and usefulness. As a novel application of this framework, we demonstrate its effectiveness and potential by applying it to service-oriented models of biological processes for the discovery of interesting and useful pathways.","Web services,
Ontologies,
Data mining,
Libraries,
Lead,
Biological system modeling,
Biology"
Large-Scale Deduplication with Constraints Using Dedupalog,"We present a declarative framework for collective deduplication of entity references in the presence of constraints. Constraints occur naturally in many data cleaning domains and can improve the quality of deduplication. An example of a constraint is ""each paper has a unique publication venue''; if two paper references are duplicates, then their associated conference references must be duplicates as well. Our framework supports collective deduplication, meaning that we can dedupe both paper references and conference references collectively in the example above. Our framework is based on a simple declarative Datalog-style language with precise semantics. Most previous work on deduplication either ignoreconstraints or use them in an ad-hoc domain-specific manner. We also present efficient algorithms to support the framework. Our algorithms have precise theoretical guarantees for a large subclass of our framework. We show, using a prototype implementation, that our algorithms scale to very large datasets. We provide thoroughexperimental results over real-world data demonstrating the utility of our framework for high-quality and scalable deduplication.","Large-scale systems,
Data engineering,
Art,
USA Councils,
Cleaning,
Databases,
Computer science,
Prototypes,
Concrete,
Clustering algorithms"
Region Segmentation in the Frequency Domain Applied to Upper Airway Real-Time Magnetic Resonance Images,"We describe a method for unsupervised region segmentation of an image using its spatial frequency domain representation. The algorithm was designed to process large sequences of real-time magnetic resonance (MR) images containing the 2-D midsagittal view of a human vocal tract airway. The segmentation algorithm uses an anatomically informed object model, whose fit to the observed image data is hierarchically optimized using a gradient descent procedure. The goal of the algorithm is to automatically extract the time-varying vocal tract outline and the position of the articulators to facilitate the study of the shaping of the vocal tract during speech production.",
Remote attestation to dynamic system properties: Towards providing complete system integrity evidence,"Remote attestation of system integrity is an essential part of trusted computing. However, current remote attestation techniques only provide integrity proofs of static properties of the system. To address this problem we present a novel remote dynamic attestation system named ReDAS (Remote Dynamic Attestation System) that provides integrity evidence for dynamic system properties. Such dynamic system properties represent the runtime behavior of the attested system, and enable an attester to prove its runtime integrity to a remote party. ReDAS currently provides two types of dynamic system properties for running applications: structural integrity and global data integrity. In this work, we present the challenges of remote dynamic attestation, provide an in-depth security analysis and introduce a first step towards providing a complete runtime dynamic attestation framework. Our prototype implementation and evaluation with real-world applications show that we can improve on current static attestation techniques with an average performance overhead of 8%.",
On the anonymity of some authentication schemes for wireless communications,"In 2004, Zhu and Ma proposed a new and efficient authentication scheme claiming to provide anonymity for wireless environments. Two years later, Lee et al. revealed several previously unpublished flaws in Zhu-Ma's authentication scheme and proposed a fix. More recently in 2008, Wu et al. pointed out that Lee et al.'s proposed fix fails to preserve anonymity as claimed and then proposed yet another fix to address the problem. In this paper, we use Wu et al.'s scheme as a case study and demonstrate that due to an inherent design flaw in Zhu-Ma's scheme, the latter and its successors are unlikely to provide anonymity. We hope that by identifying this design flaw, similar structural mistakes can be avoided in future designs.","Authentication,
Wireless communication,
Australia,
Communications technology,
Identity-based encryption,
Military communication,
History,
Privacy,
Cryptography,
Computer science"
Factorial Switching Linear Dynamical Systems Applied to Physiological Condition Monitoring,"Condition monitoring often involves the analysis of systems with hidden factors that switch between different modes of operation in some way. Given a sequence of observations, the task is to infer the filtering distribution of the switch setting at each time step. In this paper, we present factorial switching linear dynamical systems as a general framework for handling such problems. We show how domain knowledge and learning can be successfully combined in this framework, and introduce a new factor (the ldquoX-factorrdquo) for dealing with unmodeled variation. We demonstrate the flexibility of this type of model by applying it to the problem of monitoring the condition of a premature baby receiving intensive care. The state of health of a baby cannot be observed directly, but different underlying factors are associated with particular patterns of physiological measurements and artifacts. We have explicit knowledge of common factors and use the X-factor to model novel patterns which are clinically significant but have unknown cause. Experimental results are given which show the developed methods to be effective on typical intensive care unit monitoring data.",
Pulmonary Lobe Segmentation in CT Examinations Using Implicit Surface Fitting,"Lobe identification in computed tomography (CT) examinations is often an important consideration during the diagnostic process as well as during treatment planning because of their relative independence of each other in terms of anatomy and function. In this paper, we present a new automated scheme for segmenting lung lobes depicted on 3-D CT examinations. The unique characteristic of this scheme is the representation of fissures in the form of implicit functions using radial basis functions (RBFs), capable of seamlessly interpolating ldquoholesrdquo in the detected fissures and smoothly extrapolating the fissure surfaces to the lung boundaries resulting in a ldquonaturalrdquo segmentation of lung lobes. A previously developed statistically based approach is used to detect pulmonary fissures and the constraint points for implicit surface fitting are selected from detected fissure surfaces in a greedy manner to improve fitting efficiency. In a preliminary assessment study, lobe segmentation results of 65 chest CT examinations, five of which were reconstructed with three section thicknesses of 0.625 mm, 1.25 mm, and 2.5 mm, were subjectively and independently evaluated by two experienced chest radiologists using a five category rating scale (i.e., excellent, good, fair, poor, and unacceptable). Thirty-three of 65 examinations (50.8%) with a section thickness of 0.625 mm were rated as either ldquoexcellentrdquo or ldquogoodrdquo by both radiologists and only one case (1.5%) was rated by both radiologists as ldquopoorrdquo or ldquounacceptable.rdquo Comparable performance was obtained with a slice thickness of 1.25 mm, but substantial performance deterioration occurred in examinations with a section thickness of 2.5 mm. The advantages of this scheme are its full automation, relative insensitivity to fissure completeness, and ease of implementation.",
Self-contained powered knee and ankle prosthesis: Initial evaluation on a transfemoral amputee,"This paper presents an overview of the design and control of a fully self-contained prosthesis, which is intended to improve the mobility of transfemoral amputees. A finite-state based impedance control approach, previously developed by the authors, is used for the control of the prosthesis during walking and standing. The prosthesis was tested on an unilateral amputee subject for over-ground walking. Prosthesis sensor data (joint angles and torques) acquired during level ground walking experiments at a self-selected cadence demonstrates the ability of the device to provide a functional gait similar to normal gait biomechanics. Battery measurements during level ground walking experiments show that the self-contained device provides over 4,500 strides (9.0 km of walking at a speed of 5.1 km/h) between battery charges.",
Micropower non-contact EEG electrode with active common-mode noise suppression and input capacitance cancellation,"A non-contact EEG electrode with input capacitance neutralization and common-mode noise suppression circuits is presented. The coin sized sensor capacitively couples to the scalp without direct contact to the skin. To minimize the effect of signal attenuation and channel gain mismatch, the input capacitance of each sensor is actively neutralized using positive feedback and bootstrapping. Common-mode suppression is achieved through a single conductive sheet to establish a common mode reference. Each sensor electrode provides a differential gain of 60dB. Signals are transmitted in a digital serial daisy-chain directly from a local 16-bit ADC, minimizing the number of wires required to establish a high density EEG sensor network. The micropower electrode consumes only 600μW from a single 3.3V supply.",
Age regression from faces using random forests,Predicting the age of a person through face image analysis holds the potential to drive an extensive array of real world applications from human computer interaction and security to advertising and multimedia. In this paper the first application of the random forest for age regression is proposed. This method offers the advantage of few parameters that are relatively easy to initialize. Our method learns salient anthropometric quantities without a prior model. Significant implications include a dramatic reduction in training time while maintaining high regression accuracy throughout human development.,
An Improved Meta-heuristic Search for Constrained Interaction Testing,"Combinatorial interaction testing (CIT) is a cost-effective sampling  technique for discovering interaction faults in highly configurable systems.  Recent work with greedy CIT algorithms efficiently supports constraints on the features that can coexist in a configuration. But when testing a single system configuration is expensive, greedy techniques perform worse than meta-heuristic algorithms because they produce larger samples. Unfortunately, current meta-heuristic algorithms are inefficient when constraints are present.We investigate the sources of inefficiency, focusing on simulated annealing, a well-studied meta-heuristic algorithm. From our findings we propose changes to improve performance, including a reorganized search space based on the CIT problem structure. Our empirical evaluation demonstrates that the optimizations reduce run-time by three orders of magnitude and yield smaller samples. Moreover, on real problems the new version compares favorably with greedy algorithms.",
An exploratory study on assessing feature location techniques,"This paper presents an exploratory study of ten feature location techniques that use various combinations of textual, dynamic, and static analyses. Unlike previous studies, the approaches are evaluated in terms of finding multiple relevant methods, not just a single starting point of a feature's implementation. Additionally, a new way of applying textual analysis is introduced by which queries are automatically composed of the identifiers of a method known to be relevant to a feature. Our results show that this new type of query is just as effective as a query formulated by a human. We also provide insights into situations when certain feature location approaches work well and fall short. Our results and observations can be used to guide future research on feature location.",
Time-bounded lattice for efficient planning in dynamic environments,"For vehicles navigating initially unknown cluttered environments, current state-of-the-art planning algorithms are able to plan and re-plan dynamically-feasible paths efficiently and robustly. It is still a challenge, however, to deal well with the surroundings that are both cluttered and highly dynamic. Planning under these conditions is more difficult for two reasons. First, tracking and predicting the trajectories of moving objects (i.e., cars, humans) is very noisy. Second, the planning process is computationally more expensive because of the increased dimensionality of the state-space, with time as an additional variable. Moreover, re-planning needs to be invoked more often since the trajectories of moving obstacles need to be constantly re-estimated. In this paper, we develop a path planning algorithm that addresses these challenges. First, we choose a representation of dynamic obstacles that efficiently models their predicted trajectories and the uncertainty associated with the predictions. Second, to provide real-time guarantees on the performance of planning with dynamic obstacles, we propose to utilize a novel data structure for planning - a time-bounded lattice - that merges together short-term planning in time with longterm planning without time. We demonstrate the effectiveness of the approach in both simulations with up to 30 dynamic obstacles and on real robots.","Lattices,
Vehicle dynamics,
Path planning,
Trajectory,
Vehicles,
Navigation,
Robustness,
Humans,
Process planning,
Predictive models"
Outage probability of multi-hop MIMO relaying with transmit antenna selection and ideal relay gain over rayleigh fading channels,"We present a study on the outage probability of multihop wireless communication systems with multiple-input multiple-output (MIMO) link based on the transmit antenna selection and the maximal-ratio combining (MRC) at the receiver. A nonregenerative system (NS) is investigated with an ideal amplifying gain. MIMO channels are assumed in uncorrelated Rayleigh fading.We derive a moment generating function (MGF) of the reciprocal of the end-to-end signal-to-noise ratio (SNR) and obtain a closed-form approximation on the outage probability through the numerical inversion of a Laplace transform. Numerical results show that the presented outage is exactly matched with the outage probability when assuming the ideal relay gain. For more practical gains, the result is shown to be a lowerbound that gets tight at high average SNR as well as for a small number of hops and/or of antennas. We also compare the outage probabilities of nonregenerative MIMO relaying with a regenerative counterpart for multiple hops.",
Region Incrementing Visual Cryptography,"This letter presents a novel visual cryptography scheme, called region incrementing visual cryptography (RIVC), for sharing visual secrets with multiple secrecy levels in a single image. In the proposed n-level RIVC scheme, the content of an image S is designated to multiple regions associated with n secret levels, and encoded to n+1 shares with the following features: (a) each share cannot obtain any of the secrets in S, (b) any t (2lestlesn+1) shares can be used to reveal t-1 levels of secrets, (c) the number and locations of not-yet-revealed secrets are unknown to users, (d) all secrets in S can be disclosed when all of the n+1 shares are available, and (e) the secrets are recognized by visually inspecting correctly stacked shares without computation. The basis matrices for constructing the proposed n-level RIVC with small values of n=2, 3, 4 are introduced, and the results from two experiments are presented.",
Embedding Overlap Priors in Variational Left Ventricle Tracking,"We propose to embed overlap priors in variational tracking of the left ventricle (LV) in cardiac magnetic resonance (MR) sequences. The method consists of evolving two curves toward the LV endo- and epicardium boundaries. We derive the curve evolution equations by minimizing two functionals each containing an original overlap prior constraint. The latter measures the conformity of the overlap between the nonparametric (kernel-based) intensity distributions within the three target regions-LV cavity, myocardium and background-to a prior learned from a given segmentation of the first frame. The Bhattacharyya coefficient is used as an overlap measure. Different from existing intensity-driven constraints, the proposed priors do not assume implicitly that the overlap between the intensity distributions within different regions has to be minimal. This prevents both the papillary muscles from being included erroneously in the myocardium and the curves from spilling into the background. Although neither geometric training nor preprocessing were used, quantitative evaluation of the similarities between automatic and independent manual segmentations showed that the proposed method yields a competitive score in comparison with existing methods. This allows more flexibility in clinical use because our solution is based only on the current intensity data, and consequently, the results are not bounded to the characteristics, variability, and mathematical description of a finite training set. We also demonstrate experimentally that the overlap measures are approximately constant over a cardiac sequence, which allows to learn the overlap priors from a single frame.","Myocardium,
Target tracking,
Magnetic resonance,
Muscles,
Image segmentation,
Medical services,
Photometry,
Solid modeling,
Equations,
Magnetic resonance imaging"
Analysis of per-node traffic load in multi-hop wireless sensor networks,"The energy expended by sensor nodes in data communication makes up a significant quantum of their total energy consumption. Consequently, a mathematical model that can accurately predict the communication traffic load of a sensor node is critical for designing efficient sensor network protocols. In this paper, we present an analytical model for estimating the per-node traffic load in a multi-hop wireless sensor network. We consider a typical scenario wherein, the sensor nodes periodically sense the environment and forward the collected samples to a sink using greedy geographic routing. The analysis incorporates the idealistic circular coverage radio model as well as a realistic model, log-normal shadowing. Our results confirm that irrespective of the radio model, the traffic load generally increases as a function of the node's proximity to the sink. However, in the immediate vicinity of the sink, the two radio models yield quite contrasting results. The ideal radio model reveals the existence of a volcano region near the sink, where the traffic load drops significantly. On the contrary, with the log-normal shadowing model, the opposite effect is observed, wherein the traffic load actually increases at a much higher rate as one approaches the sink, resulting in the formation of a mountain peak. The results from our analysis are validated by extensive simulations.","Telecommunication traffic,
Spread spectrum communication,
Wireless sensor networks,
Analytical models,
Shadow mapping,
Data communication,
Energy consumption,
Mathematical model,
Protocols,
Routing"
Qilin: Exploiting parallelism on heterogeneous multiprocessors with adaptive mapping,"Heterogeneous multiprocessors are increasingly important in the multi-core era due to their potential for high performance and energy efficiency. In order for software to fully realize this potential, the step that maps computations to processing elements must be as automated as possible. However, the state-of-the-art approach is to rely on the programmer to specify this mapping manually and statically. This approach is not only labor intensive but also not adaptable to changes in runtime environments like problem sizes and hardware/software configurations. In this study, we propose adaptive mapping, a fully automatic technique to map computations to processing elements on a CPU+GPU machine. We have implemented it in our experimental heterogeneous programming system called Qilin. Our results show that, by judiciously distributing works over the CPU and GPU, automatic adaptive mapping achieves a 25% reduction in execution time and a 20% reduction in energy consumption than static mappings on average for a set of important computation benchmarks. We also demonstrate that our technique is able to adapt to changes in the input problem size and system configuration.",
Arch-Explore: A natural user interface for immersive architectural walkthroughs,"In this paper we propose the Arch-Explore user interface, which supports natural exploration of architectural 3D models at different scales in a real walking virtual reality (VR) environment such as head-mounted display (HMD) or CAVE setups. We discuss in detail how user movements can be transferred to the virtual world to enable walking through virtual indoor environments. To overcome the limited interaction space in small VR laboratory setups, we have implemented redirected walking techniques to support natural exploration of comparably large-scale virtual models. Furthermore, the concept of virtual portals provides a means to cover long distances intuitively within architectural models. We describe the software and hardware setup and discuss benefits of Arch-Explore.",
Power management and energy harvesting techniques for wireless sensor nodes,"Wireless sensor networks, WSNs, are large networks composed of small sensor nodes, SNs, with limited computer resources capable for gathering, data processing and communicating. Energy consumption represents a barrier challenge in many sensor network applications that require long lifetimes, usually an order of several years. Sensor nodes, as constituents of wireless sensor networks, are battery driven devices and operate on an extremely frugal energy budget. Conventional low-power design techniques and hardware architectures only provide partial solutions which are insufficient for sensor networks with energy-hungry sensors. This paper surveys several techniques used in today's wireless sensor networks with order to surpass the problem of energy consumption, power management and energy harvesting. It provides an insight into how various power reduction techniques can be used and orchestrated such that satisfactory performance can be achieved within a given energy budget.",
Action detection in complex scenes with spatial and temporal ambiguities,"In this paper, we investigate the detection of semantic human actions in complex scenes. Unlike conventional action recognition in well-controlled environments, action detection in complex scenes suffers from cluttered backgrounds, heavy crowds, occluded bodies, and spatial-temporal boundary ambiguities caused by imperfect human detection and tracking. Conventional algorithms are likely to fail with such spatial-temporal ambiguities. In this work, the candidate regions of an action are treated as a bag of instances. Then a novel multiple-instance learning framework, named SMILE-SVM (Simulated annealing Multiple Instance LEarning Support Vector Machines), is presented for learning human action detector based on imprecise action locations. SMILE-SVM is extensively evaluated with satisfactory performances on two tasks: 1) human action detection on a public video action database with cluttered backgrounds, and 2) a real world problem of detecting whether the customers in a shopping mall show an intention to purchase the merchandise on shelf (even if they didn't buy it eventually). In addition, the complementary nature of motion and appearance features in action detection are also validated, demonstrating a boosted performance in our experiments.","Layout,
Humans,
Machine learning,
Simulated annealing,
Support vector machines,
Detectors,
Performance evaluation,
Spatial databases,
Merchandise,
Computer vision"
Cooperative jamming for wireless physical layer security,"Cooperative jamming is an approach that has been recently proposed for improving physical layer based security for wireless networks in the presence of an eavesdropper. While the source transmits its message to its destination, a relay node transmits a jamming signal to create interference at the eavesdropper. In this paper, a scenario in which the relay is equipped with multiple antennas is considered. A novel system design is proposed for determining the antenna weights and transmit power of source and relay, so that the system secrecy rate is maximized subject to a total transmit power constraint, or, the transmit power is minimized subject to a secrecy rate constraint. Since the optimal solutions to these problems are difficult to obtain, suboptimal closed-form solutions are proposed that introduce an additional constraint, i.e., the complete nulling of jamming signal at the destination.","Jamming,
Communication system security,
Physical layer,
Relays,
Power system relaying,
Wireless networks,
Computer security,
Physics computing,
Signal design,
Antenna feeds"
Therapeutic and educational objectives in robot assisted play for children with autism,"This article is a methodological paper that describes the therapeutic and educational objectives that were identified during the design process of a robot aimed at robot assisted play. The work described in this paper is part of the IROMEC project (Interactive Robotic Social Mediators as Companions) that recognizes the important role of play in child development and targets children who are prevented from or inhibited in playing. The project investigates the role of an interactive, autonomous robotic toy in therapy and education for children with special needs. This paper specifically addresses the therapeutic and educational objectives related to children with autism. In recent years, robots have already been used to teach basic social interaction skills to children with autism. The added value of the IROMEC robot is that play scenarios have been developed taking children's specific strengths and needs into consideration and covering a wide range of objectives in children's development areas (sensory, communicational and interaction, motor, cognitive and social and emotional). The paper describes children's developmental areas and illustrates how different experiences and interactions with the IROMEC robot are designed to target objectives in these areas.",
"Mobile Learning in a Large Blended Computer Science Classroom: System Function, Pedagogies, and Their Impact on Learning","The computer science classes in China's institutions of higher education often have large numbers of students. In addition, many institutions offer ldquoblendedrdquo classes that include both on-campus and online students. These large blended classrooms have long suffered from a lack of interactivity. Many online classes simply provide recorded instructor lectures to which distance students listen after downloading. This format only reinforces the negative effects of passive nonparticipatory learning. At a major university in Shanghai, researchers and developers actively seek technological interventions that can greatly increase interactivity in blended classes. They have developed a cutting-edge mobile learning system that can deliver live broadcasts of real-time classroom teaching to online students with mobile devices. Their system allows students to customize their means of content-reception, based on when and where the students are tuning into the broadcast. The system also supports short text-messaging and instant polls. Through these features, students can ask questions and make suggestions in real time, and the instructor can respond immediately. This paper describes this system in detail and also reports results from a formal implementation of the system with a blended classroom of 562 students (of whom 90% were online).","Interactive systems,
Computer science education,
Large scale integration,
Distance learning,
Learning systems,
Mobile communication"
ICCD: Interactive Continuous Collision Detection between Deformable Models Using Connectivity-Based Culling,"We present an interactive algorithm for continuous collision detection between deformable models. We introduce multiple techniques to improve the culling efficiency and the overall performance of continuous collision detection. First, we present a novel formulation for continuous normal cones and use these normal cones to efficiently cull large regions of the mesh as part of self-collision tests. Second, we introduce the concept of ldquoprocedural representative trianglesrdquo to remove all redundant elementary tests between nonadjacent triangles. Finally, we exploit the mesh connectivity and introduce the concept of ldquoorphan setsrdquo to eliminate redundant elementary tests between adjacent triangle primitives. In practice, we can reduce the number of elementary tests by two orders of magnitude. These culling techniques have been combined with bounding volume hierarchies and can result in one order of magnitude performance improvement as compared to prior collision detection algorithms for deformable models. We highlight the performance of our algorithm on several benchmarks, including cloth simulations, N-body simulations, and breaking objects.","Deformable models,
Charge coupled devices,
Computational modeling,
Automatic testing,
Computer simulation,
Computer science,
Detection algorithms,
Benchmark testing,
Computer aided manufacturing,
CADCAM"
"Preemptive Virtual Clock: A flexible, efficient, and cost-effective QOS scheme for networks-on-chip","Future many-core chip multiprocessors (CMPs) and systems-on-a-chip (SoCs) will have numerous processing elements executing multiple applications concurrently. These applications and their respective threads will interfere at the on-chip network level and compete for shared resources such as cache banks, memory controllers, and specialized accelerators. Often, the communication and sharing patterns of these applications will be impossible to predict off-line, making fairness guarantees and performance isolation difficult through static thread and link scheduling. Prior techniques for providing network quality-of-service (QOS) have too much algorithmic complexity, cost (area and/or energy) or performance overhead to be attractive for on-chip implementation. To better understand the preferred solution space, we define desirable features and evaluation metrics for QOS in a network-on-a-chip (NOC). Our insights lead us to propose a novel QOS system called preemptive virtual clock (PVC). PVC provides strong guarantees, reduces packet delay variation, and enables efficient reclamation of idle network bandwidth without per-flow buffering at the routers and with minimal buffering at the source nodes. PVC averts priority inversion through preemption of lower-priority packets. By controlling preemption aggressiveness, PVC enables a trade-off between the strength of the guarantees and overall throughput. Finally, PVC simplifies network management through a flexible allocation mechanism that enables per-application bandwidth provisioning independent of thread count and supports transparent bandwidth recycling among an application's threads.","Clocks,
Yarn,
Network-on-a-chip,
Bandwidth,
System-on-a-chip,
Communication system control,
Quality of service,
Costs,
Throughput,
Recycling"
On the structure and evolution of vehicular networks,"Vehicular ad hoc networks have emerged recently as a platform to support intelligent inter-vehicle communication and improve traffic safety and performance. The road-constrained and high mobility of the vehicles, their unbounded power source, and the emergence of roadside wireless infrastructures make VANETs a challenging research topic. A key to the development of protocols for intervehicle communication and services lies in the knowledge of the topological characteristics of the VANET communication graph. This article provides answers to the general question: how does a VANET communication graph look like over time and space? This study is the first one that examines a very large-scale VANET graph and conducts a thorough investigation of its topological characteristics using several metrics, not examined in previous studies. Our work characterizes a VANET graph at the connectivity (link) level, quantifies the notion of “qualitative” nodes as required by routing and dissemination protocols, and examines the existence and evolution of communities (dense clusters of vehicles) in the VANET. Several latent facts about the VANET graph are revealed and incentives for their exploitation in protocol design are examined.","Road vehicles,
Peer to peer computing,
Mobile communication,
Ad hoc networks,
Road accidents,
Access protocols,
Telecommunication traffic,
Safety,
Computer science,
Computer networks"
Spatio-Temporal Event Model for Cyber-Physical Systems,"The emerging Cyber-Physical Systems (CPSs) are envisioned to integrate computation, communication and control with the physical world. Therefore, CPS requires close-interactions between the cyber and physical worlds both in time and space. These interactions are usually governed by events, which occur in the physical world and should autonomously be reflected in the cyber-world, and actions, which are taken by the CPS as a result of detection of events and certain decision mechanisms. Both event detection and action decision operations should be performed accurately and timely to guarantee temporal and spatial correctness. This calls for a flexible architecture and task representation framework to analyze CP operations. In this paper, we explore the temporal and spatial properties of events, define a novel CPS architecture, and develop a layered spatio-temporal event model for CPS. The event is represented as a function of attribute-based, temporal, and spatial event conditions. Moreover, logical operators are used to combine different types of event conditions to capture composite events. To the best of our knowledge, this is the first event model that captures the heterogeneous characteristics of CPS for formal temporal and spatial analysis.","Distributed computing,
Event detection,
Physics computing,
Communication system control,
Sensor phenomena and characterization,
Conferences,
Computer science,
Spatiotemporal phenomena,
Control systems,
Embedded computing"
Continuous Monitoring of Spatial Queries in Wireless Broadcast Environments,"Wireless data broadcast is a promising technique for information dissemination that leverages the computational capabilities of the mobile devices in order to enhance the scalability of the system. Under this environment, the data are continuously broadcast by the server, interleaved with some indexing information for query processing. Clients may then tune in the broadcast channel and process their queries locally without contacting the server. Previous work on spatial query processing for wireless broadcast systems has only considered snapshot queries over static data. In this paper, we propose an air indexing framework that 1) outperforms the existing (i.e., snapshot) techniques in terms of energy consumption while achieving low access latency and 2) constitutes the first method supporting efficient processing of continuous spatial queries over moving objects.",
Linear solution to scale and rotation invariant object matching,"Images of an object undergoing ego- or camera-motion often appear to be scaled, rotated, and deformed versions of each other. To detect and match such distorted patterns to a single sample view of the object requires solving a hard computational problem that has eluded most object matching methods. We propose a linear formulation that simultaneously finds feature point correspondences and global geometrical transformations in a constrained solution space. Further reducing the search space based on the lower convex hull property of the formulation, our method scales well with the number of candidate features. Our results on a variety of images and videos demonstrate that our method is accurate, efficient, and robust over local deformation, occlusion, clutter, and large geometrical transformations.",
Action recognition via local descriptors and holistic features,"In this paper we propose a unified action recognition framework fusing local descriptors and holistic features. The motivation is that the local descriptors and holistic features emphasize different aspects of actions and are suitable for the different types of action databases. The proposed unified framework is based on frame differencing, bag-of-words and feature fusion. We extract two kinds of local descriptors, i.e. 2D and 3D SIFT feature descriptors, both based on 2D SIFT interest points. We apply Zernike moments to extract two kinds of holistic features, one is based on single frames and the other is based on motion energy image. We perform action recognition experiments on the KTH and Weizmann databases, using Support Vector Machines. We apply the leave-one-out and pseudo leave-N-out setups, and compare our proposed approach with state-of-the-art results. Experiments show that our proposed approach is effective. Compared with other approaches our approach is more robust, more versatile, easier to compute and simpler to understand.","Spatial databases,
Hidden Markov models,
Support vector machines,
Surveillance,
Feature extraction,
Sun,
Image databases,
Robustness,
Human computer interaction,
Motion analysis"
Increasing spectrum capacity for ad hoc networks using cognitive radios: an analytical model,"In this letter, an analytical model to evaluate the performance of ad hoc devices equipped with cognitive radio capabilities, is investigated. By applying cognitive radio technology, the ad hoc devices will utilize the unused spectrum of the existing legacy systems in an opportunistic manner in addition to using the unlicensed spectrum. Therefore, the network throughput will be increased. The results show that the performance is improved in terms of blocking and dropping probabilities.",
Habit: Leveraging human mobility and social network for efficient content dissemination in Delay Tolerant Networks,"This paper proposes Habit, an efficient multi-layered approach to content dissemination in Delay Tolerant Networks (DTN) that leverages information about nodes' colocation (physical layer) and their social network (application layer). More precisely, the regularity of users' colocation is learned based on historical colocation observations; also, the users' social network (or ‘network of interest’) is dynamically propagated during periods of colocation; finally, these distinct pieces of information are locally combined and used to compute the paths that content should follow, in a way that maximises both precision (i.e., nodes receive only content they are interested in) and recall (i.e., all relevant content is received by interested nodes).",
Test-wrapper optimization for embedded cores in TSV-based three-dimensional SOCs,"System-on-chip (SOC) designs comprised of a number of embedded cores are widespread in today's integrated circuits. Embedded core-based design is likely to be equally popular for three-dimensional integrated circuits (3D ICs), the manufacture of which has become feasible in recent years. 3D integration offers a number of advantages over traditional two-dimensional (2D) technologies, such as the reduction in the average interconnect length, higher performance, lower interconnect power consumption, and smaller IC footprint. Despite recent advances in 3D fabrication and design methods, no attempt has been made thus far to design a 1500-style test wrapper for an embedded core that spans multiple layers in a 3D SOC. This paper addresses wrapper optimization in 3D ICs based on through-silicon vias (TSVs) for vertical interconnects. Our objective is to minimize the scan-test time for a core under constraints on the total number of TSVs available for testing. We present two polynomial-time heuristic solutions. Simulation results are presented for embedded cores from the ITC 2002 SOC test benchmarks.","Integrated circuit interconnections,
Circuit testing,
System-on-a-chip,
Three-dimensional integrated circuits,
Integrated circuit manufacture,
Integrated circuit technology,
Energy consumption,
Fabrication,
Design methodology,
Through-silicon vias"
Otsu Method and K-means,"Otsu method is one of the most successful methods for image thresholding. This paper proves that the objective function of Otsu method is equivalent to that of K-means method in multilevel thresholding . They are both based on a same criterion that minimizes the within-class variance. However, Otsu method is an exhaustive algorithm of searching the global optimal threshold, while K-means is a local optimal method. Moreover, K-means does not require computing a gray-level histogram before running, but Otsu method needs to compute a gray-level histogram firstly. Therefore, K-means can be more efficiently extended to multilevel thresholding method, two-dimensional thresholding method and three-dimensional method than Otsu method. This paper proved that the clustering results of K-means keep the order of the initial centroids with respect to one-dimensional data set. The experiments show that the k-means thresholding method performs well with less computing time than Otsu method does on three dimensional image thresholding.","Histograms,
Hybrid intelligent systems,
Computer science,
Image segmentation,
Statistics,
Noise reduction,
Pixel,
Probability distribution"
"Full-Coverage and k-Connectivity (k=14,6) Three Dimensional Networks","In this paper, we study the problem of constructing full-coverage three dimensional networks with multiple connectivity. We design a set of patterns for full coverage and two representative connectivity requirements, i.e. 14- and 6-connectivity. We prove their optimality under any ratio of the communication range over the sensing range among regular lattice deployment patterns. We also conduct a study on the proposed patterns under practical settings. To our knowledge, our work is the first one that provides deployment patterns with proven optimality that achieve both coverage and connectivity in three dimensional networks.",
Physiological Modulations in Arterial Spin Labeling Perfusion Magnetic Resonance Imaging,"The purpose of this study is to evaluate cardiac and respiratory modulations in the signals of arterial spin labeling (ASL) perfusion magnetic resonance imaging (MRI) using RETROICOR, an image domain based retrospective correction method. Systematic comparisons were conducted for tagging schemes, pulsed (PASL) versus frequency-modulated continuous (CASL) methods, and the use of background suppression (BGS). Results showed that cardiac pulsation accounted for more signal fluctuation in PASL than in CASL (two-tailed paired student's t-test, p<10-3), whereas no significant difference was found in the effect of respiratory motion (p=0.55) on the two tagging schemes studied. For PASL, significantly more improvement was achieved by the inclusion of cardiac pulsation than respiratory motion in (p<0.01). On the contrary, the inclusion of respiratory motion offers more improvement for CASL (p<0.02). BGS effectively improved the temporal signal-to-noise ratio (tSNR) as previous studies reported, but no significant difference was measured in the spectral power of physiological modulations relative to the entire spectrum of PASL signals before and after the superimposition of BGS (p=0.63 for cardiac component, p=0.67 for respiratory component). Thus, we conclude that BGS reduces noise without spectral selectivity, and the improvements of tSNR from RETROICOR and BGS are additive. CASL with a labeling duration at a multiple of an R-R interval can be used to minimize signal fluctuation originating from cardiac pulsation.",
iTopicModel: Information Network-Integrated Topic Modeling,"Document networks, i.e., networks associated with text information, are becoming increasingly popular due to the ubiquity of Web documents, blogs, and various kinds of online data. In this paper, we propose a novel topic modeling framework for document networks, which builds a unified generative topic model that is able to consider both text and structure information for documents. A graphical model is proposed to describe the generative model. On the top layer of this graphical model, we define a novel multivariate Markov Random Field for topic distribution random variables for each document, to model the dependency relationships among documents over the network structure. On the bottom layer, we follow the traditional topic model to model the generation of text for each document. A joint distribution function for both the text and structure of the documents is thus provided. A solution to estimate this topic model is given, by maximizing the log-likelihood of the joint probability. Some important practical issues in real applications are also discussed, including how to decide the topic number and how to choose a good network structure. We apply the model on two real datasets, DBLP and Cora, and the experiments show that this model is more effective in comparison with the state-of-the-art topic modeling algorithms.",
Noise Reduction in Computed Tomography Scans Using 3-D Anisotropic Hybrid Diffusion With Continuous Switch,"Noise filtering techniques that maintain image contrast while decreasing image noise have the potential to optimize the quality of computed tomography (CT) images acquired at reduced radiation dose. In this paper, a hybrid diffusion filter with continuous switch (HDCS) is introduced, which exploits the benefits of three-dimensional edge-enhancing diffusion (EED) and coherence-enhancing diffusion (CED). Noise is filtered, while edges, tubular structures, and small spherical structures are preserved. From ten high dose thorax CT scans, acquired at clinical doses, ultra low dose ( 15 mAs ) scans were simulated and used to evaluate and compare HDCS to other diffusion filters, such as regularized Perona-Malik diffusion and EED. Quantitative results show that the HDCS filter outperforms the other filters in restoring the high dose CT scan from the corresponding simulated low dose scan. A qualitative evaluation was performed on filtered real low dose CT thorax scans. An expert observer scored artifacts as well as fine structures and was asked to choose one of three scans (two filtered (blinded), one unfiltered) for three different settings (trachea, lung, and mediastinal). Overall, the HDCS filtered scan was chosen most often.",
Contextual Resource Negotiation-Based Task Allocation and Load Balancing in Complex Software Systems,"In the complex software systems, software agents always need to negotiate with other agents within their physical and social contexts when they execute tasks. Obviously, the capacity of a software agent to execute tasks is determined by not only itself but also its contextual agents; thus, the number of tasks allocated on an agent should be directly proportional to its self-owned resources as well as its contextual agents' resources. This paper presents a novel task allocation model based on the contextual resource negotiation. In the presented task allocation model, while a task comes to the software system, it is first assigned to a principal agent that has high contextual enrichment factor for the required resources; then, the principal agent will negotiate with its contextual agents to execute the assigned task. However, while multiple tasks come to the software system, it is necessary to make load balancing to avoid overconvergence of tasks at certain agents that are rich of contextual resources. Thus, this paper also presents a novel load balancing method: if there are overlarge number of tasks queued for a certain agent, the capacities of both the agent itself and its contextual agents to accept new tasks will be reduced. Therefore, in this paper, the task allocation and load balancing are implemented according to the contextual resource distribution of agents, which can be well suited for the characteristics of complex software systems; and the presented model can reduce more communication costs between allocated agents than the previous methods based on self-owned resource distribution of agents.","Resource management,
Load management,
Software systems,
Software agents,
Context modeling,
Costs,
Software development management"
Performance study of ETX based wireless routing metrics,"Being most popular and IETF standard metric, minimum hop count is appropriately used by Ad hoc Networks, as new paths must rapidly be found in the situations where quality paths could not be found in due time due to high node mobility. There always has been a tradeoff between throughput and energy consumption, but stationary topology of WMNs and high node density of WSN's benefit the algorithms to consider quality-aware routing to choose the best routes. In this paper, we analytically review ongoing research on wireless routing metrics which are based on ETX (Expected Transmission Count) as it performs better than minimum hop count under link availability. Performances over ETX, target platforms and design requirements of these ETX based metrics are high-lighted. Consequences of the criteria being adopted (in addition to expected link layer transmissions & retransmissions) in the form of incremental: (1) performance overheads and computational complexity causing inefficient use of network resources and instability of the routing algorithm, (2) throughput gains achieved with better utilization of wireless medium resources have been elaborated.",
Robust and scalable trust management for collaborative intrusion detection,"The accuracy of detecting intrusions within an Intrusion Detection Network (IDN) depends on the efficiency of collaboration between the peer Intrusion Detection Systems (IDSes) as well as the security itself of the IDN against insider threats. In this paper, we study host-based IDNs and introduce a Dirichlet-based model to measure the level of trustworthiness among peer IDSes according to their mutual experience. The model has strong scalability properties and is robust against common insider threats, such as a compromised or malfunctioning peer. We evaluate our system based on a simulated collaborative host-based IDS network. The experimental results demonstrate the improved robustness, efficiency, and scalability of our system in detecting intrusions in comparison with existing models.","Robustness,
Collaboration,
Intrusion detection,
Scalability,
Collaborative work,
Testing,
Peer to peer computing,
Computer network management,
Computer science,
Computer security"
A Binary Variable Model for Affinity Propagation,"Affinity propagation (AP) was recently introduced as an unsupervised learning algorithm for exemplar-based clustering. We present a derivation of AP that is much simpler than the original one and is based on a quite different graphical model. The new model allows easy derivations of message updates for extensions and modifications of the standard AP algorithm. We demonstrate this by adjusting the new AP model to represent the capacitated clustering problem. For those wishing to investigate or extend the graphical model of the AP algorithm, we suggest using this new formulation since it allows a simpler and more intuitive model manipulation.",
Distribution of Target Registration Error for Anisotropic and Inhomogeneous Fiducial Localization Error,"In point-based rigid-body registration, target registration error (TRE) is an important measure of the accuracy of the performed registration. The registration's accuracy depends on the fiducial localization error (FLE) which, in turn, is due to the measurement errors in the points (fiducials) used to perform the registration. FLE may have different characteristics and distributions at each point of the registering data sets, and along each orthogonal axis. Previously, the distribution of TRE was estimated based on the assumption that FLE has an independent, identical, and isotropic or anisotropic distribution for each point in the registering data sets. In this article, we present a general solution based on the maximum likelihood (ML) algorithm that estimates the distribution of TRE for the cases where FLE has an independent, identical or inhomogeneous, isotropic or anisotropic, distribution at each point in the registering data sets, and when an algorithm is available that is capable of calculating the optimum registration to first order. Mathematically, we show that the proposed algorithm simplifies to the one proposed by Fitzpatrick and West when FLE has an independent, identical, and isotropic distribution in the registering data sets. Furthermore, we use numerical simulations to show that the proposed algorithm accurately estimates the distribution of TRE when FLE has an independent, inhomogeneous, and anisotropic distribution in the registering data sets.",
A QoS-aware fault tolerant middleware for dependable service composition,"Based on the framework of service-oriented architecture (SOA), complex distributed systems can be dynamically and automatically composed by integrating distributed Web services provided by different organizations, making dependability of the distributed SOA systems a big challenge. In this paper, we propose a QoS-aware fault tolerant middleware to attack this critical problem. Our middleware includes a user-collaborated QoS model, various fault tolerance strategies, and a context-aware algorithm in determining optimal fault tolerance strategy for both stateless and stateful Web services. The benefits of the proposed middleware are demonstrated by experiments, and the performance of the optimal fault tolerance strategy selection algorithm is investigated extensively. As illustrated by the experimental results, fault tolerance for the distributed SOA systems can be efficient, effective and optimized by the proposed middleware.",
Ranking with Uncertain Scores,"Large databases with uncertain information are becoming more common in many applications including data integration, location tracking, and Web search. In these applications, ranking records with uncertain attributes needs to handle new problems that are fundamentally different from conventional ranking. Specifically, uncertainty in records' scores induces a partial order over records, as opposed to the total order that is assumed in the conventional ranking settings. In this paper, we present a new probabilistic model, based on partial orders, to encapsulate the space of possible rankings originating from score uncertainty. Under this model, we formulate several ranking query types with different semantics. We describe and analyze a set of efficient query evaluation algorithms. We show that our techniques can be used to solve the problem of rank aggregation in partial orders. In addition, we design novelsampling techniques to compute approximate query answers. Our experimental evaluation uses both real and synthetic data. The experimental study demonstrates the efficiency and effectiveness of our techniques in different settings.","Databases,
Uncertainty,
Application software,
Data engineering,
Computer science,
Web search,
Algorithm design and analysis,
Query processing,
Sampling methods,
Temperature sensors"
Analysis of the 802.11e enhanced distributed channel access function,"We propose an analytical model for the performance analysis of the enhanced distributed channel access (EDCA) function of the IEEE 802.11e standard. The proposed discrete-time Markov chain (DTMC) model incorporates the main quality-of-service (QoS) features of 802.11e, namely, contention window (CW), arbitration interframe space (AIFS), and transmit opportunity (TXOP) differentiation. Due to its specific design, the proposed DTMC model jointly considers the state of the medium access control (MAC) layer buffer and the MAC differentiation mechanisms which facilitates a novel performance analysis framework at an arbitrary traffic load. Analytical and simulation results are compared to demonstrate the accuracy of the proposed approach for varying traffic load, EDCA parameters, and MAC layer buffer space.","Analytical models,
Performance analysis,
Pervasive computing,
Quality of service,
Media Access Protocol,
Telecommunication traffic,
Wireless LAN,
Centralized control,
Communication system control"
Evolutionary Undersampling for Classification with Imbalanced Datasets: Proposals and Taxonomy,"Learning with imbalanced data is one of the recent challenges in machine learning. Various solutions have been proposed in order to find a treatment for this problem, such as modifying methods or the application of a preprocessing stage. Within the preprocessing focused on balancing data, two tendencies exist: reduce the set of examples (undersampling) or replicate minority class examples (oversampling). Undersampling with imbalanced datasets could be considered as a prototype selection procedure with the purpose of balancing datasets to achieve a high classification rate, avoiding the bias toward majority class examples. Evolutionary algorithms have been used for classical prototype selection showing good results, where the fitness function is associated to the classification and reduction rates. In this paper, we propose a set of methods called evolutionary undersampling that take into consideration the nature of the problem and use different fitness functions for getting a good trade-off between balance of distribution of classes and performance. The study includes a taxonomy of the approaches and an overall comparison among our models and state of the art undersampling methods. The results have been contrasted by using nonparametric statistical procedures and show that evolutionary undersampling outperforms the nonevolutionary models when the degree of imbalance is increased.","evolutionary algorithms,
Classification,
class imbalance problem,
undersampling,
prototype selection"
SEARCH: A routing protocol for mobile cognitive radio ad-Hoc networks,"Recent research in the emerging field of cognitive radio (CR) has mainly focused on spectrum sensing and sharing, that allow an opportunistic use of the vacant portions of the licensed frequency bands by the CR users. Efficiently leveraging this node channel information in order to provide timely end-to-end delivery over the network is a key concern for CR based routing protocols. In addition, the primary users (PUs) of the licensed band affect the channels to varying extents, depending on the proportion of the transmission power that gets leaked into the adjacent channels. This also effects the geographical region, in which, the channel is rendered unusable for the CR users. In this paper, a geographic forwarding based SpEctrum Aware Routing protocol for Cognitive ad-Hoc networks (SEARCH), is proposed that jointly undertakes path and channel selection to avoid regions of PU activity during route formation. Specifically, the optimal paths found by geographic forwarding on each channel are combined at the destination with an aim to minimize the hop count. By binding the route to regions found free of PU activity, rather than particular CR users, the effect of the PU activity is mitigated. Our proposed approach is thoroughly evaluated through simulation study.",
Pose estimation with radial distortion and unknown focal length,"This paper presents a solution to the problem of pose estimation in the presence of heavy radial distortion and a potentially large number of outliers. The main contribution is an algorithm that solves for radial distortion, focal length and camera pose using a minimal set of four point correspondences between 3D world points and image points. We use a RANSAC loop to find a set of inliers and an initial estimate for bundle adjustment. Unlike previous approaches where one starts out by assuming a linear projection model, our minimal solver allows us to handle large radial distortions already at the RANSAC stage. We demonstrate that with the inclusion of radial distortion in an early stage of the process, a broader variety of cameras can be handled than was previously possible. In the experiments, no calibration whatsoever is applied to the camera. Instead we assume square pixels, zero skew and centered principal point. Although these assumptions are not strictly true, we show that good results are still obtained and by that conclude that the proposed method is applicable to uncalibrated photographs.","Cameras,
Equations,
Computer vision,
Calibration,
Lenses,
Polynomials,
Engines,
Kernel,
Voting,
Layout"
Fighting Phishing with Discriminative Keypoint Features,"Phishing is a form of online identity theft associated with both social engineering and technical subterfuge and is a major threat to information security and personal privacy. Here, the authors present an effective image-based antiphishing scheme based on discriminative keypoint features in Web pages. Their invariant content descriptor, the Contrast Context Histogram (CCH), computes the similarity degree between suspicious and authentic pages. The results show that the proposed scheme achieves high accuracy and low error rates.",
A Study on the Relationships of Classifier Performance Metrics,"There is no general consensus on which classifier performance metrics are better to use as compared to others. While some studies investigate a handful of such metrics in a comparative fashion, an evaluation of specific relationships among a large set of commonly-used performance metrics is much needed in the data mining and machine learning community. This study provides a unique insight into the underlying relationships among classifier performance metrics. We do so with a large case study involving 35 datasets from various domains and the C4.5 decision tree algorithm. A common property of the 35 datasets is that they suffer from the class imbalance problem. Our approach is based on applying factor analysis to the classifier performance space which is characterized by 22 performance metrics. It is shown that such a large number of performance metrics can be grouped into two-to-four relationship-based groups extracted by factor analysis. This work is a step in the direction of providing the analyst with an improved understanding about the different relationships and groupings among the performance metrics, thus facilitating the selection of performance metrics that capture relatively independent aspects of a classifier’s performance.","Performance analysis,
Machine learning,
Decision trees,
Performance evaluation,
Computer science,
Artificial intelligence,
Data mining,
Machine learning algorithms,
Extraterrestrial measurements,
Information science"
A comparison of SOAP and REST implementations of a service based interaction independence middleware framework,"This paper describes the conceptual design of an interaction independence middleware framework and describes the role that web services plays within it. We investigate two pervasive service-oriented architecture paradigms, SOAP and REST, in order to gauge their potential effectiveness in meeting underlying back-end data transmission requirements; provide implementations for the service-oriented architecture and data model; and, finally, critically evaluate both implementations with an emphasis on their performance with regard to both efficiency and scalability.",
VOVO: VCR-Oriented Video-on-Demand in Large-Scale Peer-to-Peer Networks,"Most P2P Video-On-Demand (VOD) schemes mainly focus more on mending service architectures and optimizing overlays but do not carefully consider the user behavior and the benefit of prefetching strategies. As a result, they cannot better support VCR-oriented services in terms of substantive asynchronous clients, and free VCR controls for P2P VODs. In this paper, we propose VOVO, VCR-oriented VOD for large-scale P2P networks. By mining associations inside a video, the segments requested in VCR interactivities are accurately predicted based on the information collected through gossips. Together with a hybrid caching strategy, a collaborative prefetching scheme is proposed to optimize resource distribution among neighboring peers. We evaluate VOVO through extensive experiments. Results show that VOVO is scalable and effective, providing short startup latencies and good performance in VCR interactivities.","Large-scale systems,
Peer to peer computing,
Prefetching,
Streaming media,
Video recording,
Delay,
Stress,
Video sharing,
Collaboration"
Innovative Decision Support System for Railway Traffic Control,"Traffic controllers monitor railway traffic in a wide control area and may actively set new targets to trains for smooth operations. A decision support system, ROMA (Railway traffic Optimization by Means of Alternative graphs), is developed to cope with real-time timetable disturbances (e.g., multiple train delays and blocked tracks) more effectively. This dynamic traffic control system co-ordinates the speed of successive trains on open track (re-timing), solves expected route conflicts (re-ordering) and provides dynamic use of platform tracks in a station or alternative paths in a corridor between stations (local re-routing). We adopt blocking time theory for modeling track occupation and signaling constraints and alternative graphs for solving dynamic traffic control problems with the aim of increasing punctuality through intelligent use of the rail infrastructure. An extensive computational study is carried out on two complicated and densely used areas of the Dutch railways.",
Non-Exposure Location Anonymity,"Location cloaking has been proposed and well studied to protect user privacy. It blurs the accurate user location (i.e., a point withcoordinates) and replaces it with a well-shaped cloaked region (usually a circle or a rectangle). However, to obtain such a cloaked region, all existing cloaking algorithms require to know the accurate locations of all users. Since such information is exactly what the user wants to hide, these algorithms can work only if all parties involved in the cloaking process are trusted. However, in practice this assumption rarely holds as any of these parties could be malicious. Therefore, location cloaking without exposing the accurate user location to any party is urgently needed. In this paper, we present such a non-exposure cloaking algorithm. It is designed for k-anonymity and cloaking is performed based on the proximity information among mobile users, instead of directly on their coordinates. We decompose the problem into two subproblems --- proximity minimum k-clustering and secure bounding, and develop distributed algorithms for both of them. Experimental results consistently show that these algorithms are efficient and robust under various proximity topologies and system settings.",
Path planning in 1000+ dimensions using a task-space Voronoi bias,"The reduction of the kinematics and/or dynamics of a high-DOF robotic manipulator to a low-dimension “task space” has proven to be an invaluable tool for designing feedback controllers. When obstacles or other kinodynamic constraints complicate the feedback design process, motion planning techniques can often still find feasible paths, but these techniques are typically implemented in the high-dimensional configuration (or state) space. Here we argue that providing a Voronoi bias in the task space can dramatically improve the performance of randomized motion planners, while still avoiding non-trivial constraints in the configuration (or state) space. We demonstrate the potential of task-space search by planning collision-free trajectories for a 1500 link arm through obstacles to reach a desired end-effector position.","Path planning,
Manipulator dynamics,
Kinematics,
Orbital robotics,
Adaptive control,
State feedback,
Process design,
Motion planning,
Process planning,
Trajectory"
Evolving coordinated quadruped gaits with the HyperNEAT generative encoding,"Legged robots show promise for complex mobility tasks, such as navigating rough terrain, but the design of their control software is both challenging and laborious. Traditional evolutionary algorithms can produce these controllers, but require manual decomposition or other problem simplification because conventionally-used direct encodings have trouble taking advantage of a problem's regularities and symmetries. Such active intervention is time consuming, limits the range of potential solutions, and requires the user to possess a deep understanding of the problem's structure. This paper demonstrates that HyperNEAT, a new and promising generative encoding for evolving neural networks, can evolve quadruped gaits without an engineer manually decomposing the problem. Analyses suggest that HyperNEAT is successful because it employs a generative encoding that can more easily reuse phenotypic modules. It is also one of the first neuroevolutionary algorithms that exploits a problem's geometric symmetries, which may aid its performance. We compare HyperNEAT to FT-NEAT, a direct encoding control, and find that HyperNEAT is able to evolve impressive quadruped gaits and vastly outperforms FT-NEAT. Comparative analyses reveal that HyperNEAT individuals are more holistically affected by genetic operators, resulting in better leg coordination. Overall, the results suggest that HyperNEAT is a powerful algorithm for evolving control systems for complex, yet regular, devices, such as robots.","Encoding,
Robot kinematics,
Legged locomotion,
Mobile robots,
Navigation,
Evolutionary computation,
Neural networks,
Genetics,
Leg,
Control systems"
Dynamic 2D Ultrasound and 3D CT Image Registration of the Beating Heart,"Two-dimensional ultrasound (US) is widely used in minimally invasive cardiac procedures due to its convenience of use and noninvasive nature. However, the low quality of US images often limits their utility as a means for guiding procedures, since it is often difficult to relate the images to their anatomical context. To improve the interpretability of the US images while maintaining US as a flexible anatomical and functional real-time imaging modality, we describe a multimodality image navigation system that integrates 2D US images with their 3D context by registering them to high quality preoperative models based on magnetic resonance imaging (MRI) or computed tomography (CT) images. The mapping from such a model to the patient is completed using spatial and temporal registrations. Spatial registration is performed by a two-step rapid registration method that first approximately aligns the two images as a starting point to an automatic registration procedure. Temporal alignment is performed with the aid of electrocardiograph (ECG) signals and a latency compensation method. Registration accuracy is measured by calculating the TRE. Results show that the error between the US and preoperative images of a beating heart phantom is 1.7plusmn0.4 mm, with a similar performance being observed in in vivo animal experiments.","Ultrasonic imaging,
Computed tomography,
Image registration,
Heart,
Magnetic resonance imaging,
Minimally invasive surgery,
Real time systems,
Navigation,
Context modeling,
Electrocardiography"
Constructing Level-2 Phylogenetic Networks from Triplets,"Jansson and Sung showed that, given a dense set of input triplets T (representing hypotheses about the local evolutionary relationships of triplets of taxa), it is possible to determine in polynomial time whether there exists a level-1 network consistent with T, and if so, to construct such a network. Here, we extend this work by showing that this problem is even polynomial time solvable for the construction of level-2 networks. This shows that, assuming density, it is tractable to construct plausible evolutionary histories from input triplets even when such histories are heavily nontree-like. This further strengthens the case for the use of triplet-based methods in the construction of phylogenetic networks. We also implemented the algorithm and applied it to yeast data.","Phylogeny,
Polynomials,
History,
Biology,
Evolution (biology),
Fungi,
Computer science,
Computer errors,
Bayesian methods,
Tree data structures"
MR Image Segmentation Using a Power Transformation Approach,"This study proposes a segmentation method for brain MR images using a distribution transformation approach. The method extends traditional Gaussian mixtures expectation-maximization segmentation to a power transformed version of mixed intensity distributions, which includes Gaussian mixtures as a special case. As MR intensities tend to exhibit non-Gaussianity due to partial volume effects, the proposed method is designed to fit non-Gaussian tissue intensity distributions. One advantage of the method is that it is intuitively appealing and computationally simple. To avoid performance degradation caused by intensity inhomogeneity, different methods for correcting bias fields were applied prior to image segmentation, and their correction effects on the segmentation results were examined in the empirical study. The partitions of brain tissues (i.e., gray and white matter) resulting from the method were validated and evaluated against manual segmentation results based on 38 real T1-weighted image volumes from the Internet brain segmentation repository, and 18 simulated image volumes from BrainWeb. The Jaccard and Dice similarity indexes were computed to evaluate the performance of the proposed approach relative to the expert segmentations. Empirical results suggested that the proposed segmentation method yielded higher similarity measures for both gray matter and white matter as compared with those based on the traditional segmentation using the Gaussian mixtures approach.","Image segmentation,
Partitioning algorithms,
Biomedical imaging,
Design methodology,
Degradation,
Internet,
Brain modeling,
Computational modeling,
Councils,
Computer science"
A latent model of discriminative aspect,"Recognition using appearance features is confounded by phenomena that cause images of the same object to look different, or images of different objects to look the same. This may occur because the same object looks different from different viewing directions, or because two generally different objects have views from which they look similar. In this paper, we introduce the idea of discriminative aspect, a set of latent variables that encode these phenomena. Changes in view direction are one cause of changes in discriminative aspect, but others include changes in texture or lighting. However, images are not labelled with relevant discriminative aspect parameters. We describe a method to improve discrimination by inferring and then using latent discriminative aspect parameters. We apply our method to two parallel problems: object category recognition and human activity recognition. In each case, appearance features are powerful given appropriate training data, but traditionally fail badly under large changes in view. Our method can recognize an object quite reliably in a view for which it possesses no training example. Our method also reweights features to discount accidental similarities in appearance. We demonstrate that our method produces a significant improvement on the state of the art for both object and activity recognition.",
Comparative analysis of evolving software systems using the Gini coefficient,"Software metrics offer us the promise of distilling useful information from vast amounts of software in order to track development progress, to gain insights into the nature of the software, and to identify potential problems. Unfortunately, however, many software metrics exhibit highly skewed, non-Gaussian distributions. As a consequence, usual ways of interpreting these metrics — for example, in terms of “average” values — can be highly misleading. Many metrics, it turns out, are distributed like wealth — with high concentrations of values in selected locations. We propose to analyze software metrics using the Gini coefficient, a higherorder statistic widely used in economics to study the distribution of wealth. Our approach allows us not only to observe changes in software systems efficiently, but also to assess project risks and monitor the development process itself. We apply the Gini coefficient to numerous metrics over a range of software projects, and we show that many metrics not only display remarkably high Gini values, but that these values are remarkably consistent as a project evolves over time.",
The Ariadne Infrastructure for Managing and Storing Metadata,"Reusing digital resources for learning has been a goal for several decades, driven by potential time savings and quality enhancements. Although the rapid development of Web-based learning has increased opportunities for reuse significantly, managing learning objects and making them accessible still entails many challenges. This article presents and analyzes the standards-based Ariadne infrastructure for managing learning objects in an open and scalable architecture. The architecture supports the integration of learning objects in multiple, distributed repository networks. The authors capture lessons learned in four architectural patterns.",
Web-Enabled Remote Scientific Environments,"A new approach to developing Web-enabled environments for remote diagnostics, maintenance, and experimentation in engineering is based on a middleware layer that uses a Java-lntemet-Labview server to provide communication between Java programs and Labview virtual instruments. The authors illustrate their technique by applying it to the development of a complete Web-enabled application for remote control of a thermal process.","Java,
Communication system control,
Internet,
TCPIP,
Control systems,
Instruments,
Industrial training,
Hardware,
Middleware,
Software libraries"
Modelling pedestrian trajectory patterns with Gaussian processes,"We propose a non-parametric model for pedestrian motion based on Gaussian Process regression, in which trajectory data are modelled by regressing relative motion against current position. We show how the underlying model can be learned in an unsupervised fashion, demonstrating this on two databases collected from static surveillance cameras. We furthermore exemplify the use of model for prediction, comparing the recently proposed GP-Bayesfilters with a Monte Carlo method. We illustrate the benefit of this approach for long term motion prediction where parametric models such as Kalman Filters would perform poorly.",
A Simple and Approximate Model for Nonsaturated IEEE 802.11 DCF,"We propose an approximate model for a nonsaturated IEEE 802.11 DCF network that is simpler than others that have appeared in the literature. Our key simplification is that the attempt rate in the nonsaturated setting can be approximated by scaling the attempt rate of the saturated setting with an appropriate factor. Use of different scaling factors leads to variants of the model for a small buffer and an infinite buffer. We develop a general fixed-point analysis that we demonstrate can have nonunique solutions for the infinite buffer model variant under moderate traffic. Nevertheless, in an asymptotic regime that applies to light traffic, we are able to prove uniqueness of the fixed point and predict the offered load at which the maximum throughput is achieved. We verify our model using ns-2 simulation and show that our MAC access delay results are the most accurate among related work, while our collision probability and throughput results achieve comparable accuracy to (D. Malone et al., 2007), (K. Duffy et al., 2007).","Traffic control,
Delay,
Throughput,
Road accidents,
Wireless LAN,
Measurement,
Probability,
Statistics,
Multidimensional systems,
Equations"
An EM Approach to MAP Solution of Segmenting Tissue Mixtures: A Numerical Analysis,"This work presents an iterative expectation-maximization (EM) approach to the maximum a posteriori (MAP) solution of segmenting tissue mixtures inside each image voxel. Each tissue type is assumed to follow a normal distribution across the field-of-view (FOV). Furthermore, all tissue types are assumed to be independent from each other. Under these assumptions, the summation of all tissue mixtures inside each voxel leads to the image density mean value at that voxel. The summation of all the tissue mixtures' unobservable random processes leads to the observed image density at that voxel, and the observed image density value also follows a normal distribution (image data are observed to follow a normal distribution in many applications). By modeling the underlying tissue distributions as a Markov random field across the FOV, the conditional expectation of the posteriori distribution of the tissue mixtures inside each voxel is determined, given the observed image data and the current-iteration estimation of the tissue mixtures. Estimation of the tissue mixtures at next iteration is computed by maximizing the conditional expectation. The iterative EM approach to a MAP solution is achieved by a finite number of iterations and reasonable initial estimate. This MAP-EM framework provides a theoretical solution to the partial volume effect, which has been a major cause of quantitative imprecision in medical image processing. Numerical analysis demonstrated its potential to estimate tissue mixtures accurately and efficiently.","Numerical analysis,
Image segmentation,
Gaussian distribution,
Iterative methods,
Clustering algorithms,
Radiology,
Random processes,
Markov random fields,
Biomedical image processing,
Parameter estimation"
Localization Systems Using Passive UHF RFID,"Since gathering spatial information of objects often provides a large number of extended functions in RFID based applications, many different localization systems have been developed in recent years. Determining and estimating the physical location of tagged objects in an interrogating area is known as localization. In this paper, we study properties of passive UHF RFID systems such as the relationship between distance and RSSI (Received Signal Strength Indictor), performance variations among the same type of passive tags, and readabilities of tags. We propose a received signal strength based Localization Algorithm using passive UHF RFID system. The localization uses the k-nearest neighbor algorithm to estimate the physical position of the target tag. To improve the accuracy of the passive tag attached object location, the properties and characteristics are applied to the localization algorithm. According to the analysis of experimentation, our proposed approach shows over 34% improvement compared with the k-Nearest Neighbor algorithm with the use of single reader and a single antenna, and 13 reference tags.",
The compound capacity of polar codes,"We consider the compound capacity of polar codes under successive cancellation decoding for a collection of binary-input memoryless output-symmetric channels. By deriving a sequence of upper and lower bounds, we show that in general the compound capacity under successive decoding is strictly smaller than the unrestricted compound capacity.",
"Granules: A lightweight, streaming runtime for cloud computing with support, for Map-Reduce","Cloud computing has gained significant traction in recent years. The Map-Reduce framework is currently the most dominant programming model in cloud computing settings. In this paper, we describe Granules, a lightweight, streaming-based runtime for cloud computing which incorporates support for the Map-Reduce framework. Granules provides rich lifecycle support for developing scientific applications with support for iterative, periodic and data driven semantics for individual computations and pipelines. We describe our support for variants of the Map-Reduce framework. The paper presents a survey of related work in this area. Finally, this paper describes our performance evaluation of various aspects of the system, including (where possible) comparisons with other comparable systems.","Runtime,
Cloud computing,
Hardware,
Pipelines,
Computer science,
Application software,
Concurrent computing,
Parallel programming,
Parallel processing,
Machine learning algorithms"
Shape Google: a computer vision approach to isometry invariant shape retrieval,"Feature-based methods have recently gained popularity in computer vision and pattern recognition communities, in applications such as object recognition and image retrieval. In this paper, we explore analogous approaches in the 3D world applied to the problem of non-rigid shape search and retrieval in large databases.",
Articulated object tracking by rendering consistent appearance parts,"We describe a general methodology for tracking 3-dimensional objects in monocular and stereo video that makes use of GPU-accelerated filtering and rendering in combination with machine learning techniques. The method operates on targets consisting of kinematic chains with known geometry. The tracked target is divided into one or more areas of consistent appearance. The appearance of each area is represented by a classifier trained to assign a class-conditional probability to image feature vectors. A search is then performed on the configuration space of the target to find the maximum likelihood configuration. In the search, candidate hypotheses are evaluated by rendering a 3D model of the target object and measuring its consistency with the class probability map. The method is demonstrated for tool tracking on videos from two surgical domains, as well as in a human hand-tracking task.","Target tracking,
Image edge detection,
Humans,
Solid modeling,
Kinematics,
Rendering (computer graphics),
Information geometry,
Surgery,
Robots,
Histograms"
Cardiac C-Arm CT: A Unified Framework for Motion Estimation and Dynamic CT,"Generating 3-D images of the heart during interventional procedures is a significant challenge. In addition to real time fluoroscopy, angiographic C-arm systems can also now be used to generate 3-D/4-D CT images on the same system. One protocol for cardiac CT uses ECG triggered multisweep scans. A 3-D volume of the heart at a particular cardiac phase is then reconstructed by applying Feldkamp (FDK) reconstruction to the projection images with retrospective ECG gating. In this work we introduce a unified framework for heart motion estimation and dynamic cone-beam reconstruction using motion corrections. The benefits of motion correction are 1) increased temporal and spatial resolution by removing cardiac motion which may still exist in the ECG gated data sets and 2) increased signal-to-noise ratio (SNR) by using more projection data than is used in standard ECG gated methods. Three signal-enhanced reconstruction methods are introduced that make use of all of the acquired projection data to generate a 3-D reconstruction of the desired cardiac phase. The first averages all motion corrected back-projections; the second and third perform a weighted averaging according to 1) intensity variations and 2) temporal distance relative to a time resolved and motion corrected reference FDK reconstruction. In a comparison study seven methods are compared: nongated FDK, ECG-gated FDK, ECG-gated, and motion corrected FDK, the three signal-enhanced approaches, and temporally aligned and averaged ECG-gated FDK reconstructions. The quality measures used for comparison are spatial resolution and SNR. Evaluation is performed using phantom data and animal models. We show that data driven and subject-specific motion estimation combined with motion correction can decrease motion-related blurring substantially. Furthermore, SNR can be increased by up to 70% while maintaining spatial resolution at the same level as is provided by the ECG-gated FDK. The presented framework provides excellent image quality for cardiac C-arm CT.",
Learning locomotion over rough terrain using terrain templates,"We address the problem of foothold selection in robotic legged locomotion over very rough terrain. The difficulty of the problem we address here is comparable to that of human rock-climbing, where foot/hand-hold selection is one of the most critical aspects. Previous work in this domain typically involves defining a reward function over footholds as a weighted linear combination of terrain features. However, a significant amount of effort needs to be spent in designing these features in order to model more complex decision functions, and hand-tuning their weights is not a trivial task. We propose the use of terrain templates, which are discretized height maps of the terrain under a foothold on different length scales, as an alternative to manually designed features. We describe an algorithm that can simultaneously learn a small set of templates and a foothold ranking function using these templates, from expert-demonstrated footholds. Using the LittleDog quadruped robot, we experimentally show that the use of terrain templates can produce complex ranking functions with higher performance than standard terrain features, and improved generalization to unseen terrain.",
Fair Round-Robin: A Low Complexity Packet Schduler with Proportional and Worst-Case Fairness,"Round robin based packet schedulers generally have a low complexity and provide long-term fairness. The main limitation of such schemes is that they do not support short-term fairness. In this paper, we propose a new low complexity round robin scheduler, called Fair Round Robin (FRR), that overcomes this limitation. FRR has similar complexity and long-term fairness properties as the stratified round robin scheduler, a recently proposed scheme that arguably provides the best quality-of-service properties among all existing round robin based low complexity packet schedulers. FRR offers better short-term fairness than stratified round robin and other existing round robin schedulers.",
Combining textual and structural analysis of software artifacts for traceability link recovery,"Existing methods for recovering traceability links among software documentation artifacts analyze textual similarities among these artifacts. It may be the case, however, that related documentation elements share little terminology or phrasing. This paper presents a technique for indirectly recovering these traceability links in requirements documentation by combining textual with structural information as we conjecture that related requirements share related source code elements. A preliminary case study indicates that our combined approach improves the precision and recall of recovering relevant links among documents as compared to stand-alone methods based solely on analyzing textual similarities.","Documentation,
Large scale integration,
Matrix decomposition,
Terminology,
Information analysis,
Computer science,
Educational institutions,
Information retrieval,
Software tools,
Indexing"
Motion planning for active cannulas,"An active cannula is a medical device composed of thin, pre-curved, telescoping tubes that may enable many new surgical procedures. Planning optimal motions for these devices is challenging due to their kinematics, which involve both beam mechanics and space curves. In this paper, we propose an optimization-based motion planning algorithm that computes actions to guide the device to a target point while avoiding obstacles in the environment. The planner uses a simplified active cannula kinematic model that neglects beam mechanics, and focuses on planning for the (piecewise circular) space curves. The method is intended for use in image-guided procedures where the target and obstacles can be segmented from preprocedure images. Given the target location, the start position and orientation, and a geometric representation of obstacles, the algorithm computes the insertion length and orientation angle for each tube of the active cannula such that the device follows a collision-free path to the target. We formulate the planning problem as a constrained nonlinear optimization problem and use a penalty method to convert this formulation into a sequence of more easily solvable unconstrained optimization problems. Simulations demonstrate optimal paths for a 3-tube active cannula with spherical obstacles. The algorithm typically computes plans in less than 1 minute on a standard PC.","Biomedical imaging,
Minimally invasive surgery,
Motion planning,
Magnetic resonance imaging,
Image segmentation,
Constraint optimization,
Robot kinematics,
Shafts,
Brain,
Biopsy"
A low-overhead energy detection based cooperative sensing protocol for cognitive radio systems,"Cognitive radio and dynamic spectrum access represent a new paradigm shift in more effective use of limited radio spectrum. One core component behind dynamic spectrum access is the sensing of primary user activity in the shared spectrum. Conventional distributed sensing and centralized decision framework involving multiple sensor nodes is proposed to enhance the sensing performance. However, it is difficult to apply the conventional schemes in reality since the overhead in sensing measurement and sensing reporting as well as in sensing report combining limit the number of sensor nodes that can participate in distributive sensing. In this paper, we shall propose a novel, low overhead and low complexity energy detection based cooperative sensing framework for the cognitive radio systems which addresses the above two issues. The energy detection based cooperative sensing scheme greatly reduces the quiet period overhead (for sensing measurement) as well as sensing reporting overhead of the secondary systems and the power scheduling algorithm dynamically allocate the transmission power of the cooperative sensor nodes based on the channel statistics of the links to the BS as well as the quality of the sensing measurement. In order to obtain design insights, we also derive the asymptotic sensing performance of the proposed cooperative sensing framework based on the mobility model. We show that the false alarm and mis-detection performance of the proposed cooperative sensing framework improve as we increase the number of cooperative sensor nodes.",
Process SEER: A Tool for Semantic Effect Annotation of Business Process Models,"A key challenge in devising solutions to a range of problems associated with business process management: process life cycle management, compliance management, enterprise process architectures etc. is the problem of identifying process semantics. The current industry standard business process modeling notation, BPMN, provides little by way of semantic description of the effects of a process (beyond what can be conveyed via the nomenclature of tasks and the decision conditions associated with gateways). In this paper, we describe the conceptual underpinnings, design, implementation and evaluation of the ProcessSEER tool that supports several strategies for obtaining semantic effect descriptions of BPMN process models, without imposing an overly onerous burden of using formal specification on the analyst. The tool requires analysts to describe the immediate effects of each task. These are then accumulated in an automated fashion to obtain cumulative effect annotations for each task in a process. The tool leverages domain ontologies wherever they are available. The tool permits the analyst to specify immediate effect annotations in a practitioner-accessible controlled natural language, which enables formal specification using a limited repertoire of natural language sentence formats. The tool also leverages semantic web services in a similar fashion.",
"First test results Of MIMOSA-26, a fast CMOS sensor with integrated zero suppression and digitized output","The MIMOSA pixel sensors developed in Strasbourg have demonstrated attractive features for the detection of charged particles in high energy physics. So far, full-size sensors have been prototyped only with analog readout, which limits the output rate to about 1000 frames/second. The new MIMOSA 26 sensor provides a 2.2 cm2 sensitive surface with an improved readout speed of 10,000 frames/second and data throughput compression. It incorporates pixel output discrimination for binary readout and zero suppression micro-circuits at the sensor periphery to stream only fired pixel out. The sensor is back from foundry since february 2009 and has being characterized in laboratory and in test beam. The temporal noise is measured around 13-14 e- and an operation point corresponding to an efficiency of 99.5±0.1 % for a fake rate of 10-4 per pixel can be reached at room temperature. MIMOSA 26 equips the final version of the EUDET beam telescope and prefigures the architecture of monolithic active pixel sensors (MAPS) for coming vertex detectors (STAR, CBM and ILC experiments) which have higher requirements. Developments in the architecture and technology of the sensors are ongoing and should allow to match the desired readout speed and radiation tolerance. Finally, the integration of MAPS into a micro-vertex detector is addressed. A prototype ladder equipped, on both sides, with a row of 6 MIMOSA 26-like sensors is under study, aiming for a total material budget about 0.3% X0.","Testing,
Sensor phenomena and characterization,
Prototypes,
Computer vision,
Throughput,
Foundries,
Laboratories,
Noise measurement,
Temperature sensors,
Telescopes"
Integration of active and passive compliance control for safe human-robot coexistence,"In this paper we discuss the integration of active and passive approaches to robotic safety in an overall scheme for real-time manipulator control. The active control approach is based on the use of a supervisory visual system, which detects the presence and position of humans in the vicinity of the robot arm, and generates motion references. The passive control approach uses variable joint impedance which combines with velocity control to guarantee safety in worst-case conditions, i.e. unforeseen impacts. The implementation of these techniques in a 3-dof, variable impedance arm is described, and the effectiveness of their functional integration is demonstrated through experiments.","Robots,
Safety,
Velocity control,
Impedance,
Manipulators,
Motion control,
Control systems,
Visual system,
Motion detection,
Humans"
Multiple view semantic segmentation for street view images,"We propose a simple but powerful multi-view semantic segmentation framework for images captured by a camera mounted on a car driving along streets. In our approach, a pair-wise Markov Random Field (MRF) is laid out across multiple views. Both 2D and 3D features are extracted at a super-pixel level to train classifiers for the unary data terms of MRF. For smoothness terms, our approach makes use of color differences in the same image to identify accurate segmentation boundaries, and dense pixel-to-pixel correspondences to enforce consistency across different views. To speed up training and to improve the recognition quality, our approach adaptively selects the most similar training data for each scene from the label pool. Furthermore, we also propose a powerful approach within the same framework to enable large-scale labeling in both the 3D space and 2D images. We demonstrate our approach on more than 10,000 images from Google Maps Street View.","Image segmentation,
Cameras,
Markov random fields,
Feature extraction,
Data mining,
Pixel,
Training data,
Layout,
Large-scale systems,
Labeling"
Design and implementation of a high-fidelity AC metering network,"We present the architecture, design, and preliminary evaluation of ACme, a wireless sensor and actuator network for monitoring AC energy usage and controlling AC devices in a large and diverse building environment. The ACme system consists of three tiers: the ACme node which provides a metering and control interface to a single outlet, a network fabric which allows this interface to be exported to arbitrary IP endpoints, and application software that uses this networked interface to provide various power-centric applications. The ACme node integrates an Epic core module with a dedicated energy metering IC to provide real, reactive, and apparent power measurements, with optional control of an attached load. The network comprises a complete IPv6/6LoWPAN stack on every node and an edge router that connects to other IP networks. The application tier receives and stores readings in a database and uses a web server for visualization. Nodes automatically join the IPv6 subnet after being plugged in, and begin interactions with the application layer. We evaluate our system in a preliminary green building deployment with 49 nodes spread over several floors of a Computer Science Building and present energy consumption data from this preliminary deployment.",
Estimating contact dynamics,"Motion and interaction with the environment are fundamentally intertwined. Few people-tracking algorithms exploit such interactions, and those that do assume that surface geometry and dynamics are given. This paper concerns the converse problem, i.e., the inference of contact and environment properties from motion. For 3D human motion, with a 12-segment articulated body model, we show how one can estimate the forces acting on the body in terms of internal forces (joint torques), gravity, and the parameters of a contact model (e.g., the geometry and dynamics of a spring-based model). This is tested on motion capture data and video-based tracking data, with walking, jogging, cartwheels, and jumping.","Biological system modeling,
Solid modeling,
Geometry,
Inference algorithms,
Humans,
Motion estimation,
Joints,
Gravity,
Testing,
Tracking"
High-Precision Boundary Length Estimation by Utilizing Gray-Level Information,"We present a novel method that provides an accurate and precise estimate of the length of the boundary (perimeter) of an object by taking into account gray levels on the boundary of the digitization of the same object. Assuming a model where pixel intensity is proportional to the coverage of a pixel, we show that the presented method provides error-free measurements of the length of straight boundary segments in the case of nonquantized pixel values. For a more realistic situation, where pixel values are quantized, we derive optimal estimates that minimize the maximal estimation error. We show that the estimate converges toward a correct value as the number of gray levels tends toward infinity. The method is easy to implement; we provide the complete pseudocode. Since the method utilizes only a small neighborhood, it is very easy to parallelize. We evaluate the estimator on a set of concave and convex shapes with known perimeters, digitized at increasing resolution. In addition, we provide an example of applicability of the method on real images, by suggesting appropriate preprocessing steps and presenting results of a comparison of the suggested method with other local approaches.",
A brief survey of software architecture concepts and service oriented architecture,"A critical issue in the design and construction of any complex software system is its architecture. Software architecture as an important column of software development process has various methods and roadmaps that all of them have some common principles and inception. Architecture-based approaches have been promoted as a means of controlling the complexity of systems construction and evolution. In this paper we try to describe basics and main structure of software architecture with a conceptual view to this issue. First, architecture definitions are presented. Finally service-oriented architecture (SOA) as one of useful choices for software architecture to develop web software and systems is glossed in a survey.",
Understanding the Performance Gap Between Pull-Based Mesh Streaming Protocols and Fundamental Limits,"Pull-based mesh streaming protocols have recently received much research attention, with successful commercial systems showing their viability in the Internet. Despite the remarkable popularity in real-world systems, the fundamental properties and limitations of pull-based protocols are not yet well understood from a theoretical perspective, as there exists no prior work that studies the performance gap between the fundamental limits and the actual performance. In this paper, we develop a unified framework based on trellis graph techniques to mathematically analyze and understand the performance of pull-based mesh streaming protocols, with a particular focus on such a performance gap. We show that there exists a significant performance gap that separates the actual and optimal performance of pull-based mesh protocols. Moreover, periodic buffer map exchanges account for most of this performance gap. Our analytical characterization of the performance gap brings us not only a better understanding of several fundamental tradeoffs in pull-based mesh protocols, but also important insights on the design of practical streaming systems that can achieve high streaming rates and short initial buffering delays.","Protocols,
Performance analysis,
Delay,
Internet,
Communications Society,
Computer science,
Appropriate technology,
Costs,
Network coding,
Solids"
Sparse event detection in wireless sensor networks using compressive sensing,"Compressive sensing is a revolutionary idea proposed recently to achieve much lower sampling rate for sparse signals. For large wireless sensor networks, the events are relatively sparse compared with the number of sources. Because of deployment cost, the number of sensors is limited, and due to energy constraint, not all the sensors are turned on all the time. In this paper, the first contribution is to formulate the problem for sparse event detection in wireless sensor networks as a compressive sensing problem. The number of (wake-up) sensors can be greatly reduced to the similar level of the number of sparse events, which is much smaller than the total number of sources. Second, we suppose the event has the binary nature, and employ the Bayesian detection using this prior information. Finally, we analyze the performance of the compressive sensing algorithms under the Gaussian noise. From the simulation results, we show that the sampling rate can reduce to 25% without sacrificing performance. With further decreasing the sampling rate, the performance is gradually reduced until 10% of sampling rate. Our proposed detection algorithm has much better performance than the l1-magic algorithm proposed in the literature.","Event detection,
Wireless sensor networks,
Image coding,
Sampling methods,
Signal processing algorithms,
Gaussian noise,
Signal sampling,
Image processing,
Bayesian methods,
Signal to noise ratio"
Analysis of a Location-Based Social Network,"Location-based Social Networks (LSNs) allow users to see where their friends are, to search location-tagged contentwithin their social graph, and to meet others nearby. The recent availability of open mobile platforms, such as Apple iPhones and Google Android phones, makes LSNs much more accessible to mobile users.To study how users share their location in real world, wecollected traces from a commercial LSN service operated by astartup company. In this paper, we present results of data analysis over user profiles, update activities, mobility characteristics, social graphs, and attribute correlations. To the best of our knowledge, this study is the first large-scale quantitative analysis of a real-world commercial LSN service.",
MITHRA: Multiple data independent tasks on a heterogeneous resource architecture,"With the advent of high-performance COTS clusters, there is a need for a simple, scalable and fault-tolerant parallel programming and execution paradigm. In this paper, we show that the popular MapReduce programming model can be utilized to solve many interesting scientific simulation problems with much higher performance than regular cluster computers by leveraging GPGPU accelerators in cluster nodes. We use the Massive Unordered Distributed (MUD) formalism and establish a one-to-one correspondence between it and general Monte Carlo simulation methods. Our architecture, MITHRA, leverages NVIDIA CUDA technology along with Apache Hadoop to produce scalable performance gains using the MapReduce programming model. The evaluation of our proposed architecture using the Black Scholes option pricing model shows that a MITHRA cluster of 4 GPUs can outperform a regular cluster of 62 nodes, achieving a speedup of about 254 times in our testbed, while providing scalable near linear performance with additional nodes.","Fault tolerance,
Parallel programming,
Computational modeling,
Computer simulation,
High performance computing,
Multiuser detection,
Computer architecture,
Performance gain,
Pricing,
Testing"
A multilevel analytical placement for 3D ICs,"In this paper we propose a multilevel non-linear programming based 3D placement approach that minimizes a weighted sum of total wirelength and TS via number subject to area density constraints. This approach relaxes the discrete layer assignments so that they are continuous in the z-direction and the problem can be solved by an analytical global placer. A key idea is to do the overlap removal and device layer assignment simultaneously by adding a density penalty function for both area & TS via density constraints. Experimental results show that this analytical placer in a multilevel framework is effective to achieve trade-offs between wirelength and TS via number. Compared to the recently published transformation-based 3D placement method [1], we are able to achieve on average 12% shorter wirelength and 29% fewer TS via compared to their cases with best wirelength; we are also able to achieve on average 20% shorter wirelength and 50% fewer TS via number compared to their cases with best TS via numbers.","Three-dimensional integrated circuits,
Temperature,
Radio frequency,
Thermal force,
Engines,
Computer science,
Electronic mail,
Integrated circuit interconnections,
Power system interconnection,
Delay systems"
A meta-analysis of face recognition covariates,"This paper presents a meta-analysis for covariates that affect performance of face recognition algorithms. Our review of the literature found six covariates for which multiple studies reported effects on face recognition performance. These are: age of the person, elapsed time between images, gender of the person, the person's expression, the resolution of the face images, and the race of the person. The results presented are drawn from 25 studies conducted over the past 12 years. There is near complete agreement between all of the studies that older people are easier to recognize than younger people, and recognition performance begins to degrade when images are taken more than a year apart. While individual studies find men or women easier to recognize, there is no consistent gender effect. There is universal agreement that changing expression hurts recognition performance. If forced to compare different expressions, there is still insufficient evidence to conclude that any particular expression is better than another. Higher resolution images improve performance for many modern algorithms. Finally, given the studies summarized here, no clear conclusions can be drawn about whether one racial group is harder or easier to recognize than another.","Face recognition,
Image recognition,
Image resolution,
Image quality,
Lighting,
Degradation,
Glass,
Pixel,
Eyes,
Image coding"
Differential Evolution with Self-adaptation and Local Search for Constrained Multiobjective Optimization,"This paper presents Differential Evolution with Self-adaptation and Local Search for Constrained Multiobjective Optimization algorithm (DECMOSA-SQP), which uses the self-adaptation mechanism from DEMOwSA algorithm presented at CEC 2007 and a SQP local search. The constrained handling mechanism is also incorporated in the new algorithm. Assessment of the algorithm using CEC 2009 special session and competition on constrained multiobjective optimization test functions is presented. The functions are composed of unconstrained and constrained problems. Their results are assessed using the IGD metric. Based on this metric, algorithm strengths and weaknesses are discussed.","Constraint optimization,
Testing,
Evolutionary computation,
Computer science,
Quadratic programming,
Simulated annealing,
Computational modeling,
Guidelines,
Computer architecture,
Search methods"
Wyner-Ziv coding based on TCQ and LDPC codes,"This paper considers trellis coded quantization (TCQ) and low-density parity-check (LDPC) codes for the quadratic Gaussian Wyner-Ziv coding problem. After TCQ of the source X, LDPC codes are used to implement Slepian-Wolf coding of the quantized source Q(X) with side information Y at the decoder. Assuming 256-state TCQ and ideal Slepian-Wolf coding in the sense of achieving the theoretical limit H(Q(X)|Y ), we experimentally show that Slepian-Wolf coded TCQ performs 0.2 dB away from the Wyner-Ziv distortion-rate function DWZ(R) at high rate. This result mirrors that of entropy-constrained TCQ in classic source coding of Gaussian sources. Furthermore, using 8,192-state TCQ and assuming ideal Slepian-Wolf coding, our simulations show that Slepian-Wolf coded TCQ performs only 0.1 dB away from DWZ(R) at high rate. These results establish the practical performance limit of Slepian-Wolf coded TCQ for quadratic Gaussian Wyner-Ziv coding. Practical designs give performance very close to the theoretical limit. For example, with 8,192-state TCQ, irregular LDPC codes for Slepian-Wolf coding and optimal non-linear estimation at the decoder, our performance gap to DWZ(R) is 0.20 dB, 0.22 dB, 0.30 dB, and 0.93 dB at 3.83 bit per sample (b/s), 1.83 b/s, 1.53 b/s, and 1.05 b/s, respectively. When 256-state 4-D trellis-coded vector quantization instead of TCQ is employed, the performance gap to DWZ(R) is 0.51 dB, 0.51 dB, 0.54 dB, and 0.80 dB at 2.04 b/s, 1.38 b/s, 1.0 b/s, and 0.5 b/s, respectively.","Parity check codes,
Decoding,
Source coding,
Nonlinear distortion,
Rate-distortion,
Mirrors,
Vector quantization,
Lattices,
Channel coding,
Communications Society"
An auditory-based feature for robust speech recognition,"A conventional automatic speech recognizer does not perform well in the presence of noise, while human listeners are able to segregate and recognize speech in noisy conditions. We study a novel feature based on an auditory periphery model for robust speech recognition. Specifically, gammatone frequency cepstral coefficients are derived by applying a cepstral analysis on gammatone filterbank responses. Our evaluations show that the proposed feature performs considerably better than conventional acoustic features. We further demonstrate that integrating the proposed feature with a computational auditory scene analysis system yields promising recognition performance.",
Estimating natural illumination from a single outdoor image,"Given a single outdoor image, we present a method for estimating the likely illumination conditions of the scene. In particular, we compute the probability distribution over the sun position and visibility. The method relies on a combination of weak cues that can be extracted from different portions of the image: the sky, the vertical surfaces, and the ground. While no single cue can reliably estimate illumination by itself, each one can reinforce the others to yield a more robust estimate. This is combined with a data-driven prior computed over a dataset of 6 million Internet photos. We present quantitative results on a webcam dataset with annotated sun positions, as well as qualitative results on consumer-grade photographs downloaded from Internet. Based on the estimated illumination, we show how to realistically insert synthetic 3-D objects into the scene.","Lighting,
Sun,
Layout,
Yield estimation,
Internet,
Atmosphere,
Humans,
Computer science,
Distributed computing,
Probability distribution"
Behavioral control for multi-robot perimeter patrol: A Finite State Automata approach,"This paper proposes a multiple robot control algorithm to approach the problem of patrolling an open or closed line. The algorithm is fully decentralized, i.e., no communication occurs between robots or with a central station. Robots behave according only to their sensing and computing capabilities to ensure high scalability and robustness towards robots' fault. The patrolling algorithm is designed in the framework of behavioral control and it is based on the concept of Action: an higher level of abstraction with respect to the behaviors. Each Action is obtained by combining more elementary behaviors in the Null-Space-Behavioral framework. A Finite-State-Automata is designed as supervisor in charge of selecting the appropriate action. The approach has been validated in simulation as well as experimentally with a patrol of 3 Pioneer robots available at the Distributed Intelligence Laboratory of the University of Tennessee.","Automatic control,
Automata,
Robot sensing systems,
Communication system control,
Robotics and automation,
Intelligent robots,
Robot control,
Scalability,
Robustness,
Algorithm design and analysis"
On the Design of Fault-Tolerant Scheduling Strategies Using Primary-Backup Approach for Computational Grids with Low Replication Costs,"Fault-tolerant scheduling is an imperative step for large-scale computational grid systems, as often geographically distributed nodes co-operate to execute a task. By and large, primary-backup approach is a common methodology used for fault tolerance wherein each task has a primary copy and a backup copy on two different processors. In this paper, we identify two cases that may happen when scheduling dependent tasks with primary-backup approach. We derive two important constraints that must be satisfied. Further, we show that these two constraints play a crucial role in limiting the schedulability and overloading efficiency of backups of dependent tasks. We then propose two strategies to improve schedulability and overloading efficiency, respectively. We propose two algorithms (MRC-ECT and MCT-LRC), to schedule backups of independent jobs and dependent jobs, respectively. MRC-ECT is shown to guarantee an optimal backup schedule in terms of replication cost for an independent task, while MCT-LRC can schedule a backup of a dependent task with minimum completion time and less replication cost. We conduct extensive simulation experiments to quantify the performance of the proposed algorithms.","Program processors,
Processor scheduling,
Fault tolerance,
Fault tolerant systems,
Schedules,
Algorithm design and analysis,
Scheduling"
Efficient Geometric Routing in Three Dimensional Ad Hoc Networks,"Efficient geometric routing algorithms have been studied extensively in two-dimensional ad hoc networks, or simply 2D networks. These algorithms are efficient and they have been proven to be the worst-case optimal, localized routing algorithms. However, few prior works have focused on efficient geometric routing in 3D networks due to the lack of an efficient method to limit the search once the greedy routing algorithm encounters a local-minimum, like face routing in 2D networks. In this paper, we tackle the problem of efficient geometric routing in 3D networks. We propose routing on hulls, a 3D analogue to face routing, and present the first 3D partial unit Delaunay triangulation (PUDT) algorithm to divide the entire network space into a number of closed subspaces. The proposed greedy- hull-greedy (GHG) routing is efficient because it bounds the local- minimum recovery process from the whole network to the surface structure (hull) of only one of the subspaces.","Ad hoc networks,
Routing protocols,
Surface structures,
Communications Society,
Computer science,
Geometry,
Network topology,
Costs,
Performance evaluation"
Image tamper detection based on demosaicing artifacts,"In this paper, we introduce tamper detection techniques based on artifacts created by Color Filter Array (CFA) processing in most digital cameras. The techniques are based on computing a single feature and a simple threshold based classifier. The efficacy of the approach was tested over thousands of authentic, tampered, and computer generated images. Experimental results demonstrate reasonably low error rates.","Digital cameras,
Sensor arrays,
Colored noise,
Image generation,
Splicing,
Filtering,
Character generation,
Color,
Digital filters,
Testing"
System-level comparison of power delivery design for 2D and 3D ICs,"Three-dimensional integrated circuits (IC) promise high bandwidth, low latency, low device power, and a small form factor. Increased device density and asymmetrical packaging, however, render 3D power delivery design a challenge. In this paper, we provide a system-level comparison of power delivery for 2D and 3D ICs. We investigate various techniques that can impact the quality of power delivery in 3D ICs. These include through-silicon via (TSV) size and spacing, controlled collapse chip connection (C4) spacing, and a combination of dedicated and shared power delivery. Our evaluation system is composed of quad-core chip multiprocessor, memory, and accelerator engine. Each of these modules is running representative SPEC benchmark traces. Our findings are practical and provide clear guidelines for 3D power delivery optimization. More importantly, we show that it is possible to achieve 2D-like or even better power quality by increasing C4 granularity and selecting suitable TSV size and spacing.","Bonding,
Through-silicon vias,
Integrated circuit technology,
High speed integrated circuits,
Copper,
Dielectric substrates,
Packaging,
Engines,
Frequency,
Integrated circuit interconnections"
Autonomous driving in a multi-level parking structure,"Recently, the problem of autonomous navigation of automobiles has gained substantial interest in the robotics community. Especially during the two recent DARPA grand challenges, autonomous cars have been shown to robustly navigate over extended periods of time through complex desert courses or through dynamic urban traffic environments. In these tasks, the robots typically relied on GPS traces to follow pre-defined trajectories so that only local planners were required. In this paper, we present an approach for autonomous navigation of cars in indoor structures such as parking garages. Our approach utilizes multi-level surface maps of the corresponding environments to calculate the path of the vehicle and to localize it based on laser data in the absence of sufficiently accurate GPS information. It furthermore utilizes a local path planner for controlling the vehicle. In a practical experiment carried out with an autonomous car in a real parking garage we demonstrate that our approach allows the car to autonomously park itself in a large-scale multi-level structure.",
Distributed image-based 3-D localization of camera sensor networks,"We consider the problem of distributed estimation of the poses of N cameras in a camera sensor network using image measurements only. The relative rotation and translation (up to a scale factor) between pairs of neighboring cameras can be estimated using standard computer vision techniques. However, due to noise in the image measurements, these estimates may not be globally consistent. We address this problem by minimizing a cost function on SE(3)N in a distributed fashion using a generalization of the classical consensus algorithm for averaging Euclidean data. We also derive a condition for convergence, which relates the step-size of the consensus algorithm and the degree of the camera network graph. While our methods are designed with the camera sensor network application in mind, our results are applicable to other localization problems in a more general setting. We also provide synthetic simulations to test the validity of our approach.",
A Bayesian framework for video affective representation,"Emotions that are elicited in response to a video scene contain valuable information for multimedia tagging and indexing. The novelty of this paper is to introduce a Bayesian classification framework for affective video tagging that allows taking contextual information into account. A set of 21 full length movies was first segmented and informative content-based features were extracted from each shot and scene. Shots were then emotionally annotated, providing ground truth affect. The arousal of shots was computed using a linear regression on the content-based features. Bayesian classification based on the shots arousal and content-based features allowed tagging these scenes into three affective classes, namely calm, positive excited and negative excited. To improve classification accuracy, two contextual priors have been proposed: the movie genre prior, and the temporal dimension prior consisting of the probability of transition between emotions in consecutive scenes. The f1 classification measure of 54.9% that was obtained on three emotional classes with a naïve Bayes classifier was improved to 63.4% after utilizing all the priors.","Bayesian methods,
Layout,
Motion pictures,
Feature extraction,
Tagging,
Indexing,
Video on demand,
Multimedia databases,
Data mining,
Internet"
Exemplar-based Visualization of Large Document Corpus (InfoVis2009-1115),"With the rapid growth of the World Wide Web and electronic information services, text corpus is becoming available online at an incredible rate. By displaying text data in a logical layout (e.g., color graphs), text visualization presents a direct way to observe the documents as well as understand the relationship between them. In this paper, we propose a novel technique, Exemplar-based visualization (EV), to visualize an extremely large text corpus. Capitalizing on recent advances in matrix approximation and decomposition, EV presents a probabilistic multidimensional projection model in the low-rank text subspace with a sound objective function. The probability of each document proportion to the topics is obtained through iterative optimization and embedded to a low dimensional space using parameter embedding. By selecting the representative exemplars, we obtain a compact approximation of the data. This makes the visualization highly efficient and flexible. In addition, the selected exemplars neatly summarize the entire data set and greatly reduce the cognitive overload in the visualization, leading to an easier interpretation of large text corpus. Empirically, we demonstrate the superior performance of EV through extensive experiments performed on the publicly available text data sets.",
A liveness detection method for face recognition based on optical flow field,"It is a common spoof to use a photograph to fool face recognition algorithm. In light of differences in optical flow fields generated by movements of two-dimensional planes and three-dimensional objects, we proposed a new liveness detection method for face recognition. Under the assumption that the test region is a two-dimensional plane, we can obtain a reference field from the actual optical flow field data. Then the degree of differences between the two fields can be used to distinguish between a three-dimensional face and a two-dimensional photograph. Empirical study shows that the proposed approach is both feasible and effective.","Face detection,
Face recognition,
Image motion analysis,
Optical filters,
Biomedical optical imaging,
Testing,
Biometrics,
Motion detection,
Character generation,
Pixel"
Music Recommendation Based on Acoustic Features and User Access Patterns,"Music recommendation is receiving increasing attention as the music industry develops venues to deliver music over the Internet. The goal of music recommendation is to present users lists of songs that they are likely to enjoy. Collaborative-filtering and content-based recommendations are two widely used approaches that have been proposed for music recommendation. However, both approaches have their own disadvantages: collaborative-filtering methods need a large collection of user history data and content-based methods lack the ability of understanding the interests and preferences of users. To overcome these limitations, this paper presents a novel dynamic music similarity measurement strategy that utilizes both content features and user access patterns. The seamless integration of them significantly improves the music similarity measurement accuracy and performance. Based on this strategy, recommended songs are obtained by a means of label propagation over a graph representing music similarity. Experimental results on a real data set collected from http://www.newwisdom.net demonstrate the effectiveness of the proposed approach.","Recommender systems,
Collaboration,
Music,
History,
Computer science,
Feature extraction,
Internet,
Instruction sets,
Timbre,
Rhythm"
A Survey of Human Computation Systems,"Human computation is a technique that makes use of human abilities for computation to solve problems. The human computation problems are the problems those computers are not good at solving but are trivial for humans. In this paper, we give a survey of various human computation systems which are categorized into initiatory human computation, distributed human computation and social game-based human computation with volunteers, paid engineers and online players. For the existing large number of social games, some previous works defined various types of social games, but the recent developed social games cannot be categorized based on the previous works. In this paper, we define the categories and the characteristics of social games which are suitable for all existing ones. Besides, we present a survey on the performance aspects of human computation system. This paper gives a better understanding on human computation system.","Humans,
Distributed computing,
Artificial intelligence,
Internet,
Optical character recognition software,
Computer science,
Learning systems,
Testing,
Information science,
Scalability"
The libflame Library for Dense Matrix Computations,"Researchers from the Formal Linear Algebra Method Environment (Flame) project have developed new methodologies for analyzing, designing, and implementing linear algebra libraries. These solutions, which have culminated in the libflame library, seem to solve many of the programmability problems that have arisen with the advent of multicore and many-core architectures.","Libraries,
Linear algebra,
Fires,
Software algorithms,
Computer architecture,
Programming profession,
Licenses,
Books,
World Wide Web,
Target recognition"
Navigating a smart wheelchair with a brain-computer interface interpreting steady-state visual evoked potentials,"In order to allow severely disabled people who cannot move their arms and legs to steer an automated wheelchair, this work proposes the combination of a non-invasive EEG-based human-robot interface and an autonomous navigation system that safely executes the issued commands. The robust classification of steady-state visual evoked potentials in brain activity allows for the seamless projection of qualitative directional navigation commands onto a frequently updated route graph representation of the environment. The deduced metrical target locations are navigated to by the application of an extended version of the well-established nearness diagram navigation method. The applicability of the system proposed is demonstrated by a real-world pilot study in which eight out of nine untrained subjects successfully navigated an automated wheelchair, requiring only some ten minutes of preparation.","Navigation,
Wheelchairs,
Brain computer interfaces,
Steady-state,
Communication system control,
Hardware,
Contracts,
Signal processing,
Electroencephalography,
Intelligent robots"
Iris Recognition Using Signal-Level Fusion of Frames From Video,"We take advantage of the temporal continuity in an iris video to improve matching performance using signal-level fusion. From multiple frames of a frontal iris video, we create a single average image. For comparison, we reimplement three score-level fusion methods (Ma, Krichen, and Schmid). We find that our signal-level fusion of N images performs better than Ma's or Krichen's score-level fusion methods of N Hamming distance scores. Our signal-level fusion performs comparably to Schmid's log-likelihood method of score-level fusion, and our method achieves this performance using less computation time. We compare our signal fusion method with another new method: a multigallery, multiprobe method involving score-level fusion of N 2 Hamming distances. The multigallery, multiprobe score fusion has slightly better recognition performance, while the signal fusion has significant advantages in memory and computation requirements. No published prior work has shown any advantage of the use of video over still images in iris biometrics.",
Optimal tracking control of affine nonlinear discrete-time systems with unknown internal dynamics,"In this paper, direct dynamic programming techniques are utilized to solve the Hamilton Jacobi-Bellman equation forward-in-time for the optimal tracking control of general affine nonlinear discrete-time systems using online approximators (OLA's). The proposed approach, referred as adaptive dynamic programming (ADP), is utilized to solve the infinite horizon optimal tracking control of affine nonlinear discrete-time systems in the presence of unknown internal dynamics and a known control coefficient matrix. The design is implemented using OLA's to realize the optimal feedback control signal and the associated cost function. The feedforward portion of the control input is derived and approximated using an additional OLA for steady state conditions. Novel tuning laws for the OLA's are derived, and all parameters are tuned online. Lyapunov techniques are used to show that all signals are uniformly ultimately bounded (UUB) and that the approximated control signal approaches the optimal control input with small bounded error. In the ideal case when there are no approximation errors, the approximated control converges to the optimal value asymptotically. Simulation results are included to show the effectiveness of the approach.","Optimal control,
Nonlinear control systems,
Control systems,
Dynamic programming,
Jacobian matrices,
Nonlinear equations,
Programmable control,
Adaptive control,
Infinite horizon,
Signal design"
Web Science 2.0: Identifying Trends through Semantic Social Network Analysis,"We introduce a novel set of social network analysis based algorithms for mining the Web, blogs, and online forums to identify trends and find the people launching these new trends. These algorithms have been implemented in Condor, a software system for predictive search and analysis of the Web and especially social networks. Algorithms include the temporal computation of network centrality measures, the visualization of social networks as Cybermaps, a semantic process of mining and analyzing large amounts of text based on social network analysis, and sentiment analysis and information filtering methods. The temporal calculation of betweenness of concepts permits to extract and predict long-term trends on the popularity of relevant concepts such as brands, movies, and politicians. We illustrate our approach by qualitatively comparing Web buzz and our Web betweenness for the 2008 US presidential elections, as well as correlating the Web buzz index with share prices.","Social network services,
Algorithm design and analysis,
Information analysis,
Data mining,
Blogs,
Software algorithms,
Software systems,
Filtering algorithms,
Computer networks,
Visualization"
Circuits/Cutsets Duality and a Unified Algorithmic Framework for Survivable Logical Topology Design in IP-over-WDM Optical Networks,"Given a logical topology GL and a physical topology G, the survivable logical topology design problem in an IP-over- WDM optical network is to map the logical links into lightpaths in G such that GL remains connected after the failure of any edge in G. In view of its fundamental nature and its practical importance, this problem has received considerable attention in the literature. The SMART algorithmic framework based on the circuits in GL is a novel and very significant contribution to this problem. Taking advantage of the dual relationship between circuits and cutsets in a graph, we first present in this paper the primal algorithm CIRCUIT-SMART (similar to SMART) and algorithm CUTSET-SMART that is dual of CIRCUIT-SMART and proofs of correctness of these algorithms. To guarantee survivability we add additional logical links called protection edges, if necessary. This investigation has provided much insight into the structural properties of solutions to this problem and the structure of survivable logical graphs. Specifically, we present a highly simplified version of CUTSET-SMART that always provides a survivable mapping as long as G is 3-edge connected, and a survivable logical topology structure. We also present algorithm INCIDENCE-SMART that uses incidence sets that are special cases of a cut. Two efficient heuristics, one based on maximum matching theory and the other based on both the primal and dual algorithms are also presented. Simulation results comparing the different algorithms in terms of computational time, protection capacity and survivability success rate are also presented.","Network topology,
Circuit topology,
Optical design,
Algorithm design and analysis,
Optical fiber networks,
Optical fiber cables,
Optical fiber devices,
Protection,
Communication cables,
Computer science"
Analysis of photonic networks for a chip multiprocessor using scientific applications,"As multiprocessors scale to unprecedented numbers of cores in order to sustain performance growth, it is vital that these gains are not nullified by high energy consumption from inter-core communication. With recent advances in 3D Integration CMOS technology, the possibility for realizing hybrid photonic-electronic networks-on-chip warrants investigating real application traces on functionally comparable photonic and electronic network designs. We present a comparative analysis using both synthetic benchmarks as well as real applications, run through detailed cycle accurate models implemented under the OMNeT++ discrete event simulation environment. Results show that when utilizing standard process-to-processor mapping methods, this hybrid network can achieve 75× improvement in energy efficiency for synthetic benchmarks and up to 37× improvement for real scientific applications, defined as network performance per energy spent, over an electronic mesh for large messages across a variety of communication patterns.","CMOS technology,
Network-on-a-chip,
Performance analysis,
Photonics,
Power dissipation,
Laboratories,
Computer science,
Energy consumption,
Discrete event simulation,
Energy efficiency"
LARVA --- Safer Monitoring of Real-Time Java Programs (Tool Paper),"The use of runtime verification, as a lightweight approach to guarantee properties of systems, has been increasingly employed on real-life software. In this paper, we present the tool LARVA, for the runtime verification of properties of Java programs, including real-time properties. Properties can be expressed in a number of notations, including timed-automata enriched with stopwatches, Lustre, and a subset of the duration calculus. The tool has been successfully used on a number of case-studies, including an industrial system handling financial transactions. LARVA also performs analysis of real-time properties, to calculate, if possible, an upper-bound on the memory and temporal overheads induced by monitoring. Moreover, through property analysis, LARVA assesses the impact of slowing down the system through monitoring, on the satisfaction of the properties.","Java,
Runtime,
Logic,
Computer science,
Calculus,
Performance analysis,
Feedback,
Software engineering,
Computerized monitoring,
Real time systems"
Context by region ancestry,"In this paper, we introduce a new approach for modeling visual context. For this purpose, we consider the leaves of a hierarchical segmentation tree as elementary units. Each leaf is described by features of its ancestral set, the regions on the path linking the leaf to the root. We construct region trees by using a high-performance segmentation method. We then learn the importance of different descriptors (e.g. color, texture, shape) of the ancestors for classification. We report competitive results on the MSRC segmentation dataset and the MIT scene dataset, showing that region ancestry efficiently encodes information about discriminative parts, objects and scenes.","Statistics,
Statistical distributions,
Pixel,
Image denoising,
Markov random fields,
Application software,
Computer science,
Educational institutions,
Probability distribution,
Random number generation"
Performance of DS-CDMA downlink using transmitter preprocessing and relay diversity over Nakagami-m fading channels,"In this letter we propose and investigate a cooperative downlink transmission scheme for direct-sequence code-division multiple-access (DS-CDMA) systems, in order to achieve relay diversity. Transmitter zero-forcing (TZF)-based preprocessing is employed at the base-station (BS) of our proposed scheme for suppressing the downlink multiuser interference (MUI). At the mobile terminals (MTs) the signals are combined based on the maximal ratio combining (MRC) principles. As a benefit of TZF, the BS decontaminates the relayed signals from MUI without requiring any information from the relays. Furthermore, only low-complexity signal processing is necessary at the relays for forwarding information to the destination MTs (DMTs), while achieving the relay diversity by only exploiting the knowledge of the DMTs' spreading sequences.",
Compressive sensing for MIMO radar,"Multiple-input multiple-output (MIMO) radar systems have been shown to achieve superior resolution as compared to traditional radar systems with the same number of transmit and receive antennas. This paper considers a distributed MIMO radar scenario, in which each transmit element is a node in a wireless network, and investigates the use of compressive sampling for direction-of-arrival (DOA) estimation. According to the theory of compressive sampling, a signal that is sparse in some domain can be recovered based on far fewer samples than required by the Nyquist sampling theorem. The DOA of targets form a sparse vector in the angle space, and therefore, compressive sampling can be applied for DOA estimation. The proposed approach achieves the superior resolution of MIMO radar with far fewer samples than other approaches. This is particularly useful in a distributed scenario, in which the results at each receive node need to be transmitted to a fusion center for further processing.","MIMO,
Radar antennas,
Radar cross section,
Direction of arrival estimation,
Image coding,
Sparse matrices,
Receiving antennas,
Sampling methods,
Radar imaging,
Ground penetrating radar"
"Digging Digg: Comment Mining, Popularity Prediction, and Social Network Analysis","Using comment information available from Digg we define a co-participation network between users. We focus on the analysis of this implicit network, and study the behavioral characteristics of users. Using an entropy measure, we infer that users at Digg are not highly focused and participate across a wide range of topics. We also use the comment data and social network derived features to predict the popularity of online content linked at Digg using a classification and regression framework. We show promising results for predicting the popularity scores even after limiting our feature extraction to the first few hours of comment activity that follows a Digg submission.","Social network services,
Information analysis,
Computer science,
Collaborative work,
Discussion forums,
Yarn,
Pattern analysis,
Information systems,
Entropy,
Particle measurements"
Security flaw of authentication scheme with anonymity for wireless communications,"Recently, Wu et al. discussed some security flaws of enhanced authentication scheme with anonymity for wireless environments proposed by Lee et al. and showed how to overcome the problems regarding anonymity and backward secrecy. However, in the paper, we will show that Wu et al.'s improved scheme still did not provide user anonymity as they claimed.",
3D morphable face models revisited,"In this paper we revisit the process of constructing a high resolution 3D morphable model of face shape variation. We demonstrate how the statistical tools of thin-plate splines and Procrustes analysis can be used to construct a morphable model that is both more efficient and generalises to novel face surfaces more accurately than previous models. We also reformulate the probabilistic prior that the model provides on the distribution of parameter vector lengths. This distribution is determined solely by the number of model dimensions and can be used as a regularisation constraint in fitting the model to data without the need to empirically choose a parameter controlling the trade off between plausibility and quality of fit. As an example application of this improved model, we show how it may be fitted to a sparse set of 2D feature points (approximately 100). This provides a rapid means to estimate high resolution 3D face shape for a face in any pose given only a single face image. We present experimental results using ground truth data and hence provide absolute reconstruction errors. On average, the per vertex error of the reconstructed faces is less than 3.6 mm.","Lighting,
Shape,
Face recognition,
Surface fitting,
Image reconstruction,
Reflectivity,
Robustness,
Image recognition,
Computer science,
Image resolution"
"Minimum Latency Broadcasting in Multiradio, Multichannel, Multirate Wireless Meshes","This paper addresses the problem of ""efficientrdquo broadcast in a multiradio, multichannel, multirate wireless mesh network (MR2-MC WMN). In such an MR2-MC WMN, nodes are equipped with multiple radio interfaces, tuned to orthogonal channels, that can dynamically adjust their transmission rate by choosing a modulation scheme appropriate for the channel conditions. We choose ""broadcast latency,rdquo defined as the maximum delay between a packet's network-wide broadcast at the source and its eventual reception at all network nodes, as the ldquoefficiencyrdquo metric of broadcast performance. We study in this paper how the availability of multirate transmission capability and multiple radio interfaces tuned to orthogonal channels in MR2-MC WMN nodes can be exploited, in addition to the medium's ldquowireless broadcast advantagerdquo (WBA), to improve the ldquobroadcast latencyrdquo performance. In this paper, we present four heuristic solutions to our considered problem. We present detailed simulation results for these algorithms for an idealized scheduler, as well as for a practical 802.11-based scheduler. We also study the effect of channel assignment on broadcast performance and show that channel assignment can affect the broadcast performance substantially. More importantly, we show that a channel assignment that performs well for unicast does not necessarily perform well for broadcast/multicast.","Delay,
Radio broadcasting,
Wireless mesh networks,
Unicast,
Multimedia communication,
Availability,
Scheduling algorithm,
Mesh networks,
Spread spectrum communication,
Interference"
Single view reconstruction using shape grammars for urban environments,"In this paper we introduce a novel approach to single view reconstruction using shape grammars. Our approach consists in modeling architectural styles using a set of basic shapes and a set of parametric rules, corresponding to increasing levels of detail. This approach is able to model elaborate and varying architectural styles, using a tree representation of variable depth and complexity. Towards reconstruction, the parameters of the rules are optimized using image-based and architectural costs. This is done through an efficient MRF formulation based on the shape grammar itself. The resulting framework can produce precise 3D models from single views, can deal with lack of texture and the presence of occlusions and specular reflections, while maintaining the ability to cope with very complex architectural styles. Promising results demonstrate the potential of our approach.","Shape,
Image reconstruction,
Buildings,
Computer science,
Computer vision,
Layout,
Scalability,
Cities and towns,
Surface fitting,
Navigation"
Improving depth perception with motion parallax and its application in teleconferencing,"Depth perception, or 3D perception, can add a lot to the feeling of immersiveness in many applications such as 3D TV, 3D teleconferencing, etc. Stereopsis and motion parallax are two of the most important cues for depth perception. Most of the 3D displays today rely on stereopsis to create 3D perception. In this paper, we propose to improve user's depth perception by tracking their motions and creating motion parallax for the rendered image, which can be done even with legacy displays. Two enabling technologies, face tracking and foreground/background segmentation, are discussed in detail. In particular, we propose an efficient and robust feature based face tracking algorithm that is capable of estimating the face's location and scale accurately. We also propose a novel foreground/background segmentation and matting algorithm with time-of-flight camera, which is robust to moving background, lighting variations, moving camera, etc. We demonstrate the application of the above technologies in teleconferencing on legacy displays to create pseudo-3D effects.",
On the Feasibility of the Link Abstraction in Wireless Mesh Networks,"Outdoor community mesh networks based on IEEE 802.11 have seen tremendous growth in the recent past. The current understanding is that wireless link performance in these settings is inherently unpredictable, due to multipath delay spread. Consequently, researchers have focused on developing intelligent routing techniques to achieve the best possible performance. In this paper, we are specifically interested in mesh networks in rural locations. We first present detailed measurements to show that the PHY layer in these settings is indeed stable and predictable. There is a strong correlation between the error rate and the received signal strength. We show that interference, and not multipath fading, is the primary cause of unpredictable performance. This is in sharp contrast with current widespread knowledge from prior studies. Furthermore, we corroborate our view with a fresh analysis of data presented in these prior studies. While our initial measurements focus on 802.11b, we then use two different PHY technologies as well, operating in the 2.4-GHz ISM band: 802.11g and 802.15.4. These show similar results too. Based on our results, we argue that outdoor rural mesh networks can indeed be built with the link abstraction being valid. This has several design implications, including at the MAC and routing layers, and opens up a fresh perspective on a wide range of technical issues in this domain.","Wireless mesh networks,
Mesh networks,
Error analysis,
Routing,
Physical layer,
Delay,
Interference,
Computer science,
Signal to noise ratio,
Fading"
A multi-view probabilistic model for 3D object classes,"We propose a novel probabilistic framework for learning visual models of 3D object categories by combining appearance information and geometric constraints. Objects are represented as a coherent ensemble of parts that are consistent under 3D viewpoint transformations. Each part is a collection of salient image features. A generative framework is used for learning a model that captures the relative position of parts within each of the discretized viewpoints. Contrary to most of the existing mixture of viewpoints models, our model establishes explicit correspondences of parts across different viewpoints of the object class. Given a new image, detection and classification are achieved by determining the position and viewpoint of the model that maximize recognition scores of the candidate objects. Our approach is among the first to propose a generative probabilistic framework for 3D object categorization. We test our algorithm on the detection task and the viewpoint classification task by using “car” category from both the Savarese et al. 2007 and PASCAL VOC 2006 datasets. We show promising results in both the detection and viewpoint classification tasks on these two challenging datasets.",
]Video object segmentation by hypergraph cut,"In this paper, we present a new framework of video object segmentation, in which we formulate the task of extracting prominent objects from a scene as the problem of hypergraph cut. We initially over-segment each frame in the sequence, and take the over-segmented image patches as the vertices in the graph. Different from the traditional pairwise graph structure, we build a novel graph structure, hypergraph, to represent the complex spatio-temporal neighborhood relationship among the patches. We assign each patch with several attributes that are computed from the optical flow and the appearance-based motion profile, and the vertices with the same attribute value is connected by a hyperedge. Through all the hyperedges, not only the complex non-pairwise relationships between the patches are described, but also their merits are integrated together organically. The task of video object segmentation is equivalent to the hypergraph partition, which can be solved by the hypergraph cut algorithm. The effectiveness of the proposed method is demonstrated by extensive experiments on nature scenes.","Object segmentation,
Layout,
Motion detection,
Optical computing,
Motion estimation,
Object detection,
Computer vision,
Computer science,
Image motion analysis,
Partitioning algorithms"
The VIKING project: An initiative on resilient control of power networks,"This paper presents the work on resilient and secure power transmission and distribution developed within the VIKING (Vital Infrastructure, networKs, INformation and control system ManaGement) project. VIKING receives funding from the European Community's Seventh Framework Program. We will present the consortium, the motivation behind this research, the main objective of the project together with the current status.",
The humanoid museum tour guide Robotinho,"Wheeled tour guide robots have already been deployed in various museums or fairs worldwide. A key requirement for successful tour guide robots is to interact with people and to entertain them. Most of the previous tour guide robots, however, focused more on the involved navigation task than on natural interaction with humans. Humanoid robots, on the other hand, offer a great potential for investigating intuitive, multimodal interaction between humans and machines. In this paper, we present our mobile full-body humanoid tour guide robot Robotinho. We provide mechanical and electrical details and cover perception, the integration of multiple modalities for interaction, navigation control, and system integration aspects. The multimodal interaction capabilities of Robotinho have been designed and enhanced according to the questionnaires filled out by the people who interacted with the robot at previous public demonstrations. We present experiences we have made during experiments in which untrained users interacted with the robot.",
Color Seamlessness in Multi-Projector Displays Using Constrained Gamut Morphing,"Multi-projector displays show significant spatial variation in 3D color gamut due to variation in the chromaticity gamuts across the projectors, vignetting effect of each projector and also overlap across adjacent projectors. In this paper we present a new constrained gamut morphing algorithm that removes all these variations and results in true color seamlessness across tiled multi-projector displays. Our color morphing algorithm adjusts the intensities of light from each pixel of each projector precisely to achieve a smooth morphing from one projector's gamut to the other's through the overlap region. This morphing is achieved by imposing precise constraints on the perceptual difference between the gamuts of two adjacent pixels. In addition, our gamut morphing assures a C1 continuity yielding visually pleasing appearance across the entire display. We demonstrate our method successfully on a planar and a curved display using both low and high-end projectors. Our approach is completely scalable, efficient and automatic. We also demonstrate the real-time performance of our image correction algorithm on GPUs for interactive applications. To the best of our knowledge, this is the first work that presents a scalable method with a strong foundation in perception and realizes, for the first time, a truly seamless display where the number of projectors cannot be deciphered.",
Hardware architecture for high-accuracy real-time pedestrian detection with CoHOG features,"Co-occurrence histograms of oriented gradients (CoHOG) is a powerful feature descriptor for pedestrian detection. However, its calculation cost is large because the feature vector for the CoHOG descriptor is very high-dimensional. In this paper, in order to achieve real-time detection on embedded systems, we propose a novel hardware architecture for the CoHOG feature extraction. Our architecture exploits high degree of fine-grained parallelism and adopts an efficient histogram generator combined with a linear SVM classifier. The proposed architecture is implemented on a Xilinx Virtex-5 FPGA and it achieves real-time pedestrian detection on 38 fps 320×240 video. That is more than 100 times faster than the execution on a state-of-the-art Intel CPU.","Hardware,
Computer vision,
Histograms,
Costs,
Real time systems,
Embedded system,
Feature extraction,
Support vector machines,
Support vector machine classification,
Field programmable gate arrays"
A modular parametric architecture for the TORCS racing engine,"This paper presents our approach to TORCS Car Racing Competition 2009, it is based on a complete modular architecture capable of driving automatically a car along a track with or without oppents. The architecture is composed of five simple modules being each one responsible for a basic aspect of car driving. The modules control gear shiftings, steer movements and pedals positions by using of simple functions meanwhile the allowed speed in a certain track segment is managed by a simple TSK fuzzy system.",
Cross-layer multirate interaction with Distributed Source Coding in Wireless Sensor Networks,"Distributed Source Coding (DSC) is an effective means in reducing data redundancy in wireless sensor networks (WSNs). However, the issues on designing multirate DSC with multirate transmission in WSNs have not been well addressed in the literature. In this paper, we propose a cross-layer approach to achieve optimal DSC data quality while assuring energy efficiency and latency requirement through adjusting multirate DSC data dependencies and network multirate transmission parameters jointly. Specifically, the DSC coding dependency among multirate source coding sensors are fine-tuned to balance compression efficiency and transmission robustness. Transmission rate and retransmission limit on each wireless link are optimized accordingly to achieve unequal error protection (UEP) among inter-dependent DSC streams. Simulation studies demonstrate that the proposed scheme ensures satisfactory DSC data quality and energy efficiency.",
Population distributions in biogeography-based optimization algorithms with elitism,"Biogeography-based optimization (BBO) is an evolutionary algorithm that is based on the science of biogeography. Biogeography is the study of the geographical distribution of organisms. In BBO, problem solutions are represented as islands, and the sharing of features between solutions is represented as migration between islands. This paper develops a Markov analysis of BBO, including the option of elitism. Our analysis gives the probability of BBO convergence to each possible population distribution for a given problem. We compare our BBO Markov analysis with a similar genetic algorithm (GA) Markov analysis. Analytical comparisons on three simple problems show that with high mutation rates the performance of GAs and BBO is similar, but with low mutation rates BBO outperforms GAs. Our analysis also shows that elitism is not necessary for all problems, but for some problems it can significantly improve performance.",
Measuring acceptance of an assistive social robot: a suggested toolkit,"The human robot interaction community is multidisciplinary by nature and has members from social science to engineering backgrounds. In this paper we aim to provide human robot developers with a straightforward toolkit to evaluate users' acceptance of assistive social robots they are designing or developing for elderly care environments. We will explain how we developed the measures for this analysis, provide do's and don'ts in designing the experiments, demonstrate the application of the measures we have developed for this purpose and the analysis and interpretation of the data. As such we hope to engage human robot interaction developers in evaluating the acceptability of their own robot to inform the development process and improve the final robot's design.","Senior citizens,
Human robot interaction,
Service robots,
Testing,
Instruments,
Monitoring,
Mediation,
Usability,
Informatics"
Instability of MaxWeight Scheduling Algorithms,"MaxWeight scheduling algorithms provide an effective mechanism for achieving queue stability and guaranteeing maximum throughput in a wide variety of scenarios. The maximum-stability guarantees however rely on the fundamental premise that the system consists of a fixed set of sessions with stationary ergodic traffic processes. In the present paper we examine a scenario where the population of active sessions varies over time, as sessions eventually end while new sessions occasionally start. We identify a simple necessary and sufficient condition for stability, and show that MaxWeight policies may fail to provide maximum stability. The intuitive explanation is that these policies tend to give preferential treatment to flows with large backlogs, so that the rate variations of flows with smaller backlogs are not fully exploited. In the usual framework with a fixed collection of flows, the latter phenomenon cannot persist since the flows with smaller backlogs will build larger queues and gradually start receiving more service. With a dynamic population of flows, however, MaxWeight policies may constantly get diverted to arriving flows, while neglecting the rate variations of a persistently growing number of flows in progress with relatively small remaining backlogs. We also perform extensive simulation experiments to corroborate the analytical findings.","Scheduling algorithm,
Stability,
Throughput,
Traffic control,
Wireless networks,
Interference,
Communications Society,
Mathematics,
Computer science,
USA Councils"
Real-Time FPGA Processing for High-Speed Optical Frequency Domain Imaging,"We present a novel algorithm for reconstructing interferograms acquired in optical frequency domain imaging (OFDI). The algorithm was developed specifically for processing in field programmable gate arrays (FPGAs) and featured the use of a finite-impulse-response (FIR) filter implementation of B-spline interpolation for efficiently re-sampling k-space. When implemented in FPGAs, the algorithm allowed for real-time processing of interferograms acquired with a high-speed OFDI system at 54 kHz and a sampling rate of 100 MS/s.","Field programmable gate arrays,
High speed optical techniques,
Frequency domain analysis,
Optical imaging,
Optical filters,
Finite impulse response filter,
Image reconstruction,
Interpolation,
Real time systems,
Sampling methods"
Performance analysis of wireless opportunistic schedulers using stochastic Petri nets,"In this paper, performance of wireless opportunistic schedulers in multiuser systems is studied under a dynamic data arrival setting. Different from the previous studies which mostly focus on the network stability and the worst case scenarios, we emphasize on the average performance of wireless opportunistic schedulers. We first develop a framework based on Markov queueing model and then analyze it by applying decomposition and iteration techniques in the stochastic Petri nets (SPN). Since the size of the state space in our analytical model is small, the proposed framework shows an improved efficiency in computational complexity. Based on the established analytical model, performance of both opportunistic and non-opportunistic schedulers are studied and compared in terms of average queue length, mean throughput, average delay and dropping probability. Analytical results demonstrate that the multiuser diversity effect as observed in the infinite backlog scenario is only valid in the heavy traffic regime. The performance of the opportunistic schedulers in the light traffic regime is worse than that of the non-opportunistic round-robin scheduler, and becomes worse especially with the increase of the number of users. Simulations are also performed to verify the accuracy of the analytical results.","Performance analysis,
Stochastic processes,
Petri nets,
Analytical models,
Processor scheduling,
Queueing analysis,
Traffic control,
Dynamic scheduling,
Stability,
State-space methods"
A global perspective on MAP inference for low-level vision,"In recent years the Markov Random Field (MRF) has become the de facto probabilistic model for low-level vision applications. However, in a maximum a posteriori (MAP) framework, MRFs inherently encourage delta function marginal statistics. By contrast, many low-level vision problems have heavy tailed marginal statistics, making the MRF model unsuitable. In this paper we introduce a more general Marginal Probability Field (MPF), of which the MRF is a special, linear case, and show that convex energy MPFs can be used to encourage arbitrary marginal statistics. We introduce a flexible, extensible framework for effectively optimizing the resulting NP-hard MAP problem, based around dual-decomposition and a modified min-cost flow algorithm, and which achieves global optimality in some instances. We use a range of applications, including image denoising and texture synthesis, to demonstrate the benefits of this class of MPF over MRFs.","Statistics,
Statistical distributions,
Pixel,
Image denoising,
Markov random fields,
Application software,
Computer science,
Educational institutions,
Probability distribution,
Random number generation"
"A collaborative framework for enforcing server commitments, and for regulating server interactive behavior in SOA-based systems","We are presenting a collaborative framework to enforce server commitments, and to regulate server interactive behavior in SOA systems. Trusted components collaborate to establish a network of enforcement components, one per server, to maintain proofs of what servers have committed with their clients and peer servers. The trusted components and the enforcement components also collaborate to make servers accountable for their commitments, and to enforce policies on the interactions between the servers and their clients/peer servers. In our framework, such collaborative efforts are explicitly stated in the form of policies to ease the management of collaborative processes. Our implementation is based on the LGI mechanism, complies with SOA standards, and does not require any change of the conventional SOAP-based protocol, the UDDI server, or the programs employed by servers and their clients.",
Scaling laws on multicast capacity of large scale wireless networks,"In this paper, we focus on the networking-theoretic multicast capacity for both random extended networks (REN) and random dense networks (RDN) under Gaussian Channel model, when all nodes are individually power-constrained. During the transmission, the power decays along path with the attenuation exponent alpha > 2. In REN and RDN, n nodes are randomly distributed in the square region with side-length radic(n) and 1, respectively. We randomly choose ns nodes as the sources of multicast sessions, and for each source v, we pick uniformly at random nd nodes as the destination nodes. Based on percolation theory, we propose multicast schemes and analyze the achievable throughput by considering all possible values of ns and nd. As a special case of our results, we show that for ns = Theta(n), the per-session multicast capacity of RDN is Theta((1)/(radic(ndn))) when nd = O((n)/((log n)3)) and is Theta((1)/(n)) when nd = Omega((1)/(log n)); the per-session multicast capacity of REN is Theta((1)/radic(ndn)) when nd = O((n)/((log n)alpha+1)) and is Theta((1)/(nd) ldr (log n)-(alpha)/(2)) when nd = Omega((n)/(log n)).",
A New Constructive Algorithm for Architectural and Functional Adaptation of Artificial Neural Networks,"The generalization ability of artificial neural networks (ANNs) is greatly dependent on their architectures. Constructive algorithms provide an attractive automatic way of determining a near-optimal ANN architecture for a given problem. Several such algorithms have been proposed in the literature and shown their effectiveness. This paper presents a new constructive algorithm (NCA) in automatically determining ANN architectures. Unlike most previous studies on determining ANN architectures, NCA puts emphasis on architectural adaptation and functional adaptation in its architecture determination process. It uses a constructive approach to determine the number of hidden layers in an ANN and of neurons in each hidden layer. To achieve functional adaptation, NCA trains hidden neurons in the ANN by using different training sets that were created by employing a similar concept used in the boosting algorithm. The purpose of using different training sets is to encourage hidden neurons to learn different parts or aspects of the training data so that the ANN can learn the whole training data in a better way. In this paper, the convergence and computational issues of NCA are analytically studied. The computational complexity of NCA is found to be O(W times Pt times tau), where W is the number of weights in the ANN, Pt is the number of training examples, and tau is the number of training epochs. This complexity has the same order as what the backpropagation learning algorithm requires for training a fixed ANN architecture. A set of eight classification and two approximation benchmark problems was used to evaluate the performance of NCA. The experimental results show that NCA can produce ANN architectures with fewer hidden neurons and better generalization ability compared to existing constructive and nonconstructive algorithms.",
Discovering spatiotemporal mobility profiles of cellphone users,"Mobility path information of cellphone users play a crucial role in a wide range of cellphone applications, including context-based search and advertising, early warning systems, city-wide sensing applications such as air pollution exposure estimation and traffic planning. However, there is a disconnect between the low level location data logs available from the cellphones and the high level mobility path information required to support these cellphone applications. In this paper, we present formal definitions to capture the cellphone users' mobility patterns and profiles, and provide a complete framework, Mobility Profiler, for discovering mobile user profiles starting from cell based location log data. We use real-world cellphone log data (of over 350K hours of coverage) to demonstrate our framework and perform experiments for discovering frequent mobility patterns and profiles. Our analysis of mobility profiles of cellphone users expose a significant long tail in a user's location-time distribution: A total of 15% of a user's time is spent on average in locations that each appear with less than 1% of time.","Spatiotemporal phenomena,
Cellular phones,
Advertising,
Alarm systems,
Air pollution,
Path planning,
Probability distribution"
Intrinsic mean shift for clustering on Stiefel and Grassmann manifolds,"The mean shift algorithm, which is a nonparametric density estimator for detecting the modes of a distribution on a Euclidean space, was recently extended to operate on analytic manifolds. The extension is extrinsic in the sense that the inherent optimization is performed on the tangent spaces of these manifolds. This approach specifically requires the use of the exponential map at each iteration. This paper presents an alternative mean shift formulation, which performs the iterative optimization “on” the manifold of interest and intrinsically locates the modes via consecutive evaluations of a mapping. In particular, these evaluations constitute a modified gradient ascent scheme that avoids the computation of the exponential maps for Stiefel and Grassmann manifolds. The performance of our algorithm is evaluated by conducting extensive comparative studies on synthetic data as well as experiments on object categorization and segmentation of multiple motions.","Clustering algorithms,
Algorithm design and analysis,
Computer vision,
Iterative algorithms,
Kernel,
Performance analysis,
Motion segmentation,
Image segmentation,
Vectors,
Gene expression"
Tensor-Based AAM with Continuous Variation Estimation: Application to Variation-Robust Face Recognition,"The active appearance model (AAM) is a well-known model that can represent a non-rigid object effectively. However, the fitting result is often unsatisfactory when an input image deviates from the training images due to its fixed shape and appearance model. To obtain more robust AAM fitting, we propose a tensor-based AAM that can handle a variety of subjects, poses, expressions, and illuminations in the tensor algebra framework, which consists of an image tensor and a model tensor. The image tensor estimates image variations such as pose, expression, and illumination of the input image using two different variation estimation techniques: discrete and continuous variation estimation. The model tensor generates variation-specific AAM basis vectors from the estimated image variations, which leads to more accurate fitting results. To validate the usefulness of the tensor-based AAM, we performed variation-robust face recognition using the tensor-based AAM fitting results. To do, we propose indirect AAM feature transformation. Experimental results show that tensor-based AAM with continuous variation estimation outperforms that with discrete variation estimation and conventional AAM in terms of the average fitting error and the face recognition rate.","Active appearance model,
Face recognition,
Lighting,
Active shape model,
Principal component analysis,
Robustness,
Algebra,
Convergence,
Stability"
Rapid Dynamic Image Registration of the Beating Heart for Diagnosis and Surgical Navigation,"Dynamic cardiac magnetic resonance imaging (MR) and computed tomography (CT) provide cardiologists and cardiac surgeons with high-quality 4-D images for diagnosis and therapy, yet the effective use of these high-quality anatomical models remains a challenge. Ultrasound (US) is a flexible imaging tool, but the US images produced are often difficult to interpret unless they are placed within their proper 3-D anatomical context. The ability to correlate real-time 3-D US volumes (RT3D US) with dynamic MR/CT images would offer a significant contribution to improve the quality of cardiac procedures. In this paper, we present a rapid two-step method for registering RT3D US to high-quality dynamic 3-D MR/CT images of the beating heart. This technique overcomes some major limitations of image registration (such as the correct registration result not necessarily occurring at the maximum of the mutual information (MI) metric) using the MI metric. We demonstrate the effectiveness of our method in a dynamic heart phantom (DHP) study and a human subject study. The achieved mean target registration error of CT+US images in the phantom study is 2.59 mm. Validation using human MR/US volumes shows a target registration error of 1.76 mm. We anticipate that this technique will substantially improve the quality of cardiac diagnosis and therapies.","Image registration,
Heart,
Surgery,
Navigation,
Computed tomography,
Magnetic resonance imaging,
Medical treatment,
Ultrasonic imaging,
Imaging phantoms,
Humans"
Coupled Spectral Regression for matching heterogeneous faces,"Face recognition algorithms need to deal with variable lighting conditions. Near infrared (NIR) image based face recognition technology has been proposed to effectively overcome this difficulty. However, it requires that the enrolled face images be captured using NIR images whereas many applications require visual (VIS) images for enrollment templates. To take advantage of NIR face images for illumination-invariant face recognition and allow the use of VIS face images for enrollment, we encounter a new face image pattern recognition problem, that is, heterogeneous face matching between NIR versus VIS faces. In this paper, we present a subspace learning framework named Coupled Spectral Regression (CSR) to solve this challenge problem of coupling the two types of face images and matching between them. CSR first models the properties of different types of data separately and then learns two associated projections to project heterogeneous data (e.g. VIS and NIR) respectively into a discriminative common subspace in which classification is finally performed. Compared to other existing methods, CSR is computational efficient, benefiting from the efficiency of spectral regression and has better generalization performance. Experimental results on VIS-NIR face database show that the proposed CSR method significantly outperforms the existing methods.",
Robotic wheelchair based on observations of people using integrated sensors,"Recently, several robotic/intelligent wheelchairs have been proposed that employ user-friendly interfaces or autonomous functions. Although it is often desirable for user to operate wheelchairs on their own, they are often accompanied by a caregiver or companion. In designing wheelchairs, it is important to reduce the caregiver load. In this paper we propose a robotic wheelchair that can move with a caregiver side by side. In contrast to a front-behind position, in a side-by-side position it is more difficult for wheelchairs to adjust when the caregiver makes a turn. To cope with this problem we present a visual-laser tracking technique. In this technique, a laser range sensor and an omni-directional camera are integrated to observe the caregiver. A Rao-Blackwellized particle filter framework is employed to track the caregiver's position and orientation of both body and head based on the distance data and panorama images captured from the laser range sensor and the omni-directional camera. After presenting this technique, we introduce an application of the wheelchair for museum visit use.",
Stochastic games for security in networks with interdependent nodes,"This paper studies a stochastic game theoretic approach to security and intrusion detection in communication and computer networks. Specifically, an Attacker and a Defender take part in a two-player game over a network of nodes whose security assets and vulnerabilities are correlated. Such a network can be modeled using weighted directed graphs with the edges representing the influence among the nodes. The game can be formulated as a non-cooperative zero-sum or nonzero-sum stochastic game. However, due to correlation among the nodes, if some nodes are compromised, the effective security assets and vulnerabilities of the remaining ones will not stay the same in general, which leads to complex system dynamics. We examine existence, uniqueness, and structure of the solution and also provide numerical examples to illustrate our model.","Stochastic processes,
Intrusion detection,
Game theory,
Computer security,
Computer networks,
Costs,
Monitoring,
Laboratories,
Distributed computing,
Bayesian methods"
Statistical Sinogram Restoration in Dual-Energy CT for PET Attenuation Correction,"Dual-energy (DE) X-ray computed tomography (CT) has been found useful in various applications. In medical imaging, one promising application is using low-dose DECT for attenuation correction in positron emission tomography (PET). Existing approaches to sinogram material decomposition ignore noise characteristics and are based on logarithmic transforms, producing noisy component sinogram estimates for low-dose DECT. In this paper, we propose two novel sinogram restoration methods based on statistical models: penalized weighted least square (PWLS) and penalized likelihood (PL), yielding less noisy component sinogram estimates for low-dose DECT than classical methods. The proposed methods consequently provide more precise attenuation correction of the PET emission images than do previous methods for sinogram material decomposition with DECT. We report simulations that compare the proposed techniques and existing approaches.",
Supporting the Development of Mobile Adaptive Learning Environments: A Case Study,"In this paper, we describe a system to support the generation of adaptive mobile learning environments. In these environments, students and teachers can accomplish different types of individual and collaborative activities in different contexts. Activities are dynamically recommended to users depending on different criteria (user features, context, etc.), and workspaces to support the corresponding activity accomplishment are dynamically generated. In this article, we present the main characteristics of the mechanism that suggests the most suitable activities at each situation, the system in which this mechanism has been implemented, the authoring tool to facilitate the specification of context-based adaptive m-learning environments, and two environments generated following this approach will be presented. The outcomes of two case studies carried out with students of the first and second courses of ldquoComputer Engineeringrdquo at the ldquoUniversidad Autonoma de Madridrdquo are also presented.","Collaboration,
Mobile communication,
Adaptation model,
Adaptive systems,
Mobile handsets,
Collaborative tools,
Indexes"
Dynamic evaluation of hardware trust,"Current research into Trojan detection suggests that exhaustive Trojan detection in a chip during limited manufacturing test time is an extremely difficult problem. Indeed, an especially nefarious form of Trojan known as the time bomb has a payload activated in a delayed manner making it extremely hard to detect. As a result, chip trust detection at manufacturing test time may not be adequate especially for critical applications. This suggests that some form of dynamic trust detection of the chip both preliminary (possibly during a preproduction phase) and during in-field use at run time is required. We explore an approach to this problem that combines multicore hardware with dynamic distributed software scheduling to determine hardware trust during in-field use at run time. Our approach involves the scheduling and execution of functionally equivalent variants (obtained by different compilations, or different algorithm variations) simultaneously on different PEs and comparing the results. The process dynamically achieves trust determination by identifying the existence of Trojans with a high level of confidence.",
Improving the MFIE's accuracy by using a mixed discretization,"The scattering of time-harmonic electromagnetic waves by perfect electrical conductors (PECs) can be modelled by several boundary integral equations, the magnetic and electric field integral equations (MFIE and EFIE) being the most prominent ones[1]. These equations can be discretized by expanding current distributions in terms of Rao-Wilton-Glisson (RWG) functions defined on a triangular mesh approximating the scatterer's surface and by testing the equations using the same RWG functions [2].","Electromagnetic scattering,
Integral equations,
Testing,
Boundary conditions,
Electromagnetic fields,
Current density,
Information technology,
Computer science,
Conductors,
Electromagnetic modeling"
Modeling class cohesion as mixtures of latent topics,"The paper proposes a new measure for the cohesion of classes in Object-Oriented software systems. It is based on the analysis of latent topics embedded in comments and identifiers in source code. The measure, named as Maximal Weighted Entropy, utilizes the Latent Dirichlet Allocation technique and information entropy measures to quantitatively evaluate the cohesion of classes in software. This paper presents the principles and the technology that stand behind the proposed measure. Two case studies on a large open source software system are presented. They compare the new measure with an extensive set of existing metrics and use them to construct models that predict software faults. The case studies indicate that the novel measure captures different aspects of class cohesion compared to the existing cohesion measures and improves fault prediction for most metrics, which are combined with Maximal Weighted Entropy.","Software measurement,
Open source software,
Software maintenance,
Software quality,
Object oriented modeling,
Information entropy,
Software reusability,
Linear discriminant analysis,
Computer science,
Educational institutions"
Haptic Augmented Reality: Taxonomy and an Example of Stiffness Modulation,"Haptic augmented reality (AR) enables the user to feel a real environment augmented with synthetic haptic stimuli. This article addresses two important topics in haptic AR. First, a new taxonomy for haptic AR is established based on a composite visuo-haptic reality-virtuality continuum extended from the conventional continuum for visual AR. Previous studies related to haptic AR are reviewed and classified using the composite continuum, and associated research issues are discussed. Second, the feasibility of haptically modulating the feel of a real object with the aid of virtual force feedback is investigated, with the stiffness as a goal haptic property. All required algorithms for contact detection, stiffness modulation, and force control are developed, and their individual performances are thoroughly evaluated. The resulting haptic AR system is also assessed in a psychophysical experiment, demonstrating its competent perceptual performance for stiffness modulation. To our knowledge, this work is among the first efforts in haptic AR for systematic augmentation of real object attributes with virtual forces, and it serves as an initial building block toward a general haptic AR system. Finally, several research issues identified during the feasibility study are introduced, with the aim of eliciting more research interest in this exciting yet unexplored area.",
Outage probability of multiple-input single-output (MISO) systems with delayed feedback,"We investigate the effect of feedback delay on the outage probability of multiple-input single-output (MISO) fading channels. Channel state information at the transmitter (CSIT) is a delayed version of the channel state information available at the receiver (CSIR). We consider two cases of CSIR: (a) perfect CSIR and (b) CSI estimated at the receiver using training symbols. With perfect CSIR, under a short-term power constraint, we determine: (a) the outage probability for beamforming with imperfect CSIT (BF-IC) analytically, and (b) the optimal spatial power allocation (OSPA) scheme that minimizes outage numerically. Results show that, for delayed CSIT, BF-IC is close to optimal for low SNR and uniform spatial power allocation (USPA) is close to optimal at high SNR. Similarly, under a longterm power constraint, we show that BF-IC is better for low SNR and USPA is better at high SNR. With imperfect CSIR, we obtain an upper bound on the outage probability with USPA and BF-IC. Results show that the loss in performance due to imperfection in CSIR is not significant, if the training power is chosen appropriately.",
Architectural design decision: Existing models and tools,"In the field of software architecture, there has been a paradigm shift from describing the outcome of architecting process mostly described by component and connector (know-what) to documenting architectural design decisions and their rationale (know-how) which leads to the production of an architecture. This paradigm shift results in emergence of various models and related tools for capturing, managing and sharing architectural design decisions and their rationale explicitly. This paper analyzes existing architectural design decisions models and provides a criteria-based comparison on tools that support these models. The major contribution of this paper is twofold: to show that all of these models have a consensus on capturing the essence of an architectural design decision; and to clarify the major difference among the tools and show what desired features are missing in these tools.","Design engineering,
Software architecture,
Computer architecture,
Connectors,
Context modeling,
Ontologies,
Mathematical model,
Mathematics,
Production,
Disaster management"
Analysis of the (1+1)-EA for Finding Approximate Solutions to Vertex Cover Problems,"Vertex cover is one of the best known NP-hard combinatorial optimization problems. Experimental work has claimed that evolutionary algorithms (EAs) perform fairly well for the problem and can compete with problem-specific ones. A theoretical analysis that explains these empirical results is presented concerning the random local search algorithm and the (1+1)-EA. Since it is not expected that an algorithm can solve the vertex cover problem in polynomial time, a worst case approximation analysis is carried out for the two considered algorithms and comparisons with the best known problem-specific ones are presented. By studying instance classes of the problem, general results are derived. Although arbitrarily bad approximation ratios of the (1+1)-EA can be proved for a bipartite instance class, the same algorithm can quickly find the minimum cover of the graph when a restart strategy is used. Instance classes where multiple runs cannot considerably improve the performance of the (1+1)-EA are considered and the characteristics of the graphs that make the optimization task hard for the algorithm are investigated and highlighted. An instance class is designed to prove that the (1+1)-EA cannot guarantee better solutions than the state-of-the-art algorithm for vertex cover if worst cases are considered. In particular, a lower bound for the worst case approximation ratio, slightly less than two, is proved. Nevertheless, there are subclasses of the vertex cover problem for which the (1+1)-EA is efficient. It is proved that if the vertex degree is at most two, then the algorithm can solve the problem in polynomial time.","Image analysis,
Algorithm design and analysis,
Evolutionary computation,
Approximation algorithms,
Polynomials,
Computational complexity,
Computational intelligence,
Application software,
Computer science"
Constraint Satisfaction Problems of Bounded Width,We provide a full characterization of applicability of The Local Consistency Checking algorithm to solving the non-uniform Constraint Satisfaction Problems. This settles the conjecture of Larose and Zadori.,"Computer science,
Algebra,
Constraint theory,
Computational complexity,
Artificial intelligence,
Equations,
Polynomials,
Electrooculography"
A Multi-Threading Architecture to Support Interactive Visual Exploration,"During continuous user interaction, it is hard to provide rich visual feedback at interactive rates for datasets containing millions of entries. The contribution of this paper is a generic architecture that ensures responsiveness of the application even when dealing with large data and that is applicable to most types of information visualizations. Our architecture builds on the separation of the main application thread and the visualization thread, which can be cancelled early due to user interaction. In combination with a layer mechanism, our architecture facilitates generating previews incrementally to provide rich visual feedback quickly. To help avoiding common pitfalls of multi-threading, we discuss synchronization and communication in detail. We explicitly denote design choices to control trade-offs. A quantitative evaluation based on the system VI S P L ORE shows fast visual feedback during continuous interaction even for millions of entries. We describe instantiations of our architecture in additional tools.",
Bounds on the throughput gain of network coding in unicast and multicast wireless networks,"Gupta and Kumar established that the per node throughput of ad hoc networks with multi-pair unicast traffic scales with an increasing number of nodes n as lambda(n) = ominus(1/radic(n log n)), thus indicating that performance does not scale well. However, Gupta and Kumar did not consider network coding and wireless broadcasting, which recent works suggest have the potential to significantly improve throughput. Here, we establish bounds on the improvement provided by such techniques. For random networks of any dimension under either the protocol or physical model that were introduced by Gupta and Kumar, we show that network coding and broadcasting lead to at most a constant factor improvement in per node throughput. For the protocol model, we provide bounds on this factor. We also establish bounds on the throughput benefit of network coding and broadcasting for multiple source multicast in random networks. Finally, for an arbitrary network deployment, we show that the coding benefit ratio is at most O(log n) for both the protocol and physical communication models. These results give guidance on the application space of network coding, and, more generally, indicate the difficulty in improving the scaling behavior of wireless networks without modification of the physical layer.","Throughput,
Network coding,
Unicast,
Wireless networks,
Broadcasting,
Ad hoc networks,
Telecommunication traffic,
Physical layer,
Space technology,
Protocols"
Planning ahead for PTZ camera assignment and handoff,"We present a visual sensor network, comprising wide field-of-view (FOV) passive cameras and pan/tilt/zoom (PTZ) active cameras, which automatically captures high quality surveillance video of selected pedestrians during their prolonged presence in an area of interest. A wide-FOV static camera can track multiple pedestrians, while any PTZ active camera can follow a single pedestrian at a time. The proactive control of multiple PTZ cameras is required to record seamless, high quality video of a roaming individual despite the observational constraints of the different cameras. We formulate PTZ camera assignment and handoff as a planning problem whose solution achieves optimal camera assignment with respect to predefined observational goals.","Cameras,
Surveillance,
Automatic control,
Layout,
Technology planning,
Computer science,
Humans,
Object detection,
Image resolution,
Biometrics"
New results for the multivariate Nakagami-m fading model with arbitrary correlation matrix and applications,"New results for the multichannel Nakagami-m fading model with an arbitrary correlation matrix are presented in this paper. By using an efficient tridiagonalization method based on Householder matrices, the inverse of the Gaussian correlation matrix is transformed to tridiagonal, managing to derive a closed-form union upper bound for the joint Nakagami-m probability density function and an exact analytical expression for the moment generating function of the sum of identically distributed gamma random variables. Our analysis considers an arbitrary correlation structure, which includes as special cases the exponential, constant, circular, and linear correlation ones. Based on the proposed mathematical analysis, we obtain a tight union upper bound for the outage probability of multibranch selection diversity receivers as well as exact analytical expressions for the outage and the average error probability of multibranch maximal-ratio diversity receivers. Our analysis is verified by comparing numerically evaluated with extensive computer simulation performance evaluation results, showing the usefulness of the proposed approach.",
A Divide-and-Conquer Strategy to Deadlock Prevention in Flexible Manufacturing Systems,"Petri nets are a popular mathematical tool to investigate the deadlock problems in resource allocation systems. As an important problem solution paradigm in computer science, the divide-and-conquer strategy is used in this paper to investigate the deadlock prevention for flexible manufacturing systems (FMSs) that are modeled with Petri nets. Based on the concept of resource circuits, a plant net model is divided into an idle subnet, an autonomous subnet, and a number of small but independent subnets, called toparchies, from the viewpoint of deadlock control. A liveness-enforcing supervisor, called toparch, is designed for each toparchy. If a particular separation condition holds in a plant net model, the computational complexity of toparches is significantly reduced. This research shows that the resultant net, called monarch, by composing the toparches derived for the toparchies can serve as a liveness-enforcing Petri net supervisor for the whole plant model. FMS examples are given to illustrate the proposed method.","System recovery,
Flexible manufacturing systems,
Petri nets,
Educational programs,
Resource management,
Computer science,
Circuits,
Computational modeling,
Computational complexity,
Mathematical model"
Quantifying Path Exploration in the Internet,"Previous measurement studies have shown the existence of path exploration and slow convergence in the global Internet routing system, and a number of protocol enhancements have been proposed to remedy the problem. However, existing measurements were conducted only over a small number of testing prefixes. There has been no systematic study to quantify the pervasiveness of Border Gateway Protocol (BGP) slow convergence in the operational Internet, nor any known effort to deploy any of the proposed solutions. In this paper, we present our measurement results that identify BGP slow convergence events across the entire global routing table. Our data shows that the severity of path exploration and slow convergence varies depending on where prefixes are originated and where the observations are made in the Internet routing hierarchy. In general, routers in tier-1 Internet service providers (ISPs) observe less path exploration, hence they experience shorter convergence delays than routers in edge ASs; prefixes originated from tier-1 ISPs also experience less path exploration than those originated from edge ASs. Furthermore, our data show that the convergence time of route fail-over events is similar to that of new route announcements and is significantly shorter than that of route failures. This observation is contrary to the widely held view from previous experiments but confirms our earlier analytical results. Our effort also led to the development of a path-preference inference method based on the path usage time, which can be used by future studies of BGP dynamics.",
Fast scanning using piezoelectric tube nanopositioners: A negative imaginary approach,"Most commercially available Atomic Force Microscopes (AFMs) use piezoelectric tube nano-positioners for scanning. Current scanning frequencies are less than 0.01 ƒr, where ƒr is the frequency of the first resonant mode of the piezoelectric tube used. An improvement in the scanning rates without losing the nano-scale precision is desired. Here, a prototype of the scanning unit of an AFM is considered. The dynamics of the piezo tube, used in the prototype, is approximated by a model that satisfies the negative imaginary property. The resonant mode that hampers the fast scanning is identified from the model and damped using a feedback control technique known as the Integral Resonant Control (IRC). The piezoelectric tube is then actuated to have fast and accurate scans.","Nanopositioning,
Resonance,
Frequency,
Hysteresis,
Atomic force microscopy,
Surface topography,
Vibrations,
Control systems,
Prototypes,
Feedback control"
Driver's style classification using jerk analysis,This paper presents an innovative approach to classifying the driver's driving style by analyzing the jerk profile of the driver. Driving style is a dynamic behavior of a driver on the road. At times a driver can be calm but aggressive at others. The information about driver's dynamic driving style can be used to better control fuel economy. We propose to classify driver's style based on the measure of how fast a driver is accelerating and decelerating. We developed an algorithm that classifies driver's style utilizing the statistical information from the jerk profile and the road way type and traffic congestion level prediction. Our experiment results show that our approach generates more reasonable results than those generated by using other published methods.,
Minimum-energy all-to-all multicasting in wireless ad hoc networks,"A wireless ad hoc network consists of mobile nodes that are powered by batteries. The limited battery lifetime imposes a severe constraint on the network performance, energy conservation in such a network thus is of paramount importance, and energy efficient operations are critical to prolong the lifetime of the network. All-to-all multicasting is one fundamental operation in wireless ad hoc networks, in this paper we focus on the design of energy efficient routing algorithms for this operation. Specifically, we consider the following minimum-energy all-to-all multicasting problem. Given an all-to-all multicast session consisting of a set of terminal nodes in a wireless ad hoc network, where the transmission power of each node is either fixed or adjustable, assume that each terminal node has a message to share with each other, the problem is to build a shared multicast tree spanning all terminal nodes such that the total energy consumption of realizing the all-to-all multicast session by the tree is minimized. We first show that this problem is NP-complete. We then devise approximation algorithms with guaranteed approximation ratios. We also provide a distributed implementation of the proposed algorithm. We finally conduct experiments by simulations to evaluate the performance of the proposed algorithm. The experimental results demonstrate that the proposed algorithm significantly outperforms all the other known algorithms.",
Nonrigid Registration of Joint Histograms for Intensity Standardization in Magnetic Resonance Imaging,"A major disadvantage of magnetic resonance imaging (MRI) compared to other imaging modalities like computed tomography is the fact that its intensities are not standardized. Our contribution is a novel method for MRI signal intensity standardization of arbitrary MRI scans, so as to create a pulse sequence dependent standard intensity scale. The proposed method is the first approach that uses the properties of all acquired images jointly (e.g., T1- and T2-weighted images). The image properties are stored in multidimensional joint histograms. In order to normalize the probability density function (pdf) of a newly acquired data set, a nonrigid image registration is performed between a reference and the joint histogram of the acquired images. From this matching a nonparametric transformation is obtained, which describes a mapping between the corresponding intensity spaces and subsequently adapts the image properties of the newly acquired series to a given standard. As the proposed intensity standardization is based on the probability density functions of the data sets only, it is independent of spatial coherence or prior segmentations of the reference and current images. Furthermore, it is not designed for a particular application, body region or acquisition protocol. The evaluation was done using two different settings. First, MRI head images were used, hence the approach can be compared to state-of-the-art methods. Second, whole body MRI scans were used. For this modality no other normalization algorithm is known in literature. The Jeffrey divergence of the pdfs of the whole body scans was reduced by 45%. All used data sets were acquired during clinical routine and thus included pathologies.",
Efficient planar graph cuts with applications in Computer Vision,"We present a fast graph cut algorithm for planar graphs. It is based on the graph theoretical work and leads to an efficient method that we apply on shape matching and image segmentation. In contrast to currently used methods in computer vision, the presented approach provides an upper bound for its runtime behavior that is almost linear. In particular, we are able to match two different planar shapes of N points in O(N2 log N) and segment a given image of N pixels in O(N log N). We present two experimental benchmark studies which demonstrate that the presented method is also in practice faster than previously proposed graph cut methods: On planar shape matching and image segmentation we observe a speed-up of an order of magnitude, depending on resolution.",
Piggyback Cooperative Repetition for Reliable Broadcasting of Safety Messages in VANETs,"Vehicle safety communication applications require safety messages to be received by all the targeted vehicles within their lifetime. We propose a piggybacked cooperative repetition approach for reliably broadcasting safety messages in VANETs. Repetitions by neighbors that receive the original transmission can effectively cover the areas that are missed in the original transmissions. Moreover, the repetitions are piggybacked by the newly generated messages so that no extra messages are injected into the network. Our experiments show a significant performance improvement, achieving over 90% reception rate of safety messages at distances up to 200 meters to the sender.",
Roadcast: A Popularity Aware Content Sharing Scheme in VANETs,"Content sharing through vehicle-to-vehicle communication can help people find their interested content on the road. In VANETs, due to limited contact duration and unreliable wireless connection, a vehicle can get the useful data only when it meets another vehicle and the encountered vehicle has the exactly matched data. However, the probability of such case is very low. To improve the performance of content sharing in intermittently connected VANETs, we propose a novel P2P content sharing scheme called Roadcast. Roadcast ensures popular data is more likely to be shared with other vehicles so that the overall query delay and the query hit ratio can be improved. Roadcast consists of two components called popularity aware content retrieval and popularity aware data replacement. The popularity aware content retrieval scheme makes use of Information Retrieval (IR) techniques to find the most relevant and popular data towards user's query. The popularity aware data replacement algorithm ensures that the density of different data is proportional to their popularity in the system steady state, which firmly obeys the optimal ""square-root"" replication rule. Results based on real city map and real traffic model show that Roadcast outperforms other content sharing schemes in VANETs.",
Probabilistic situation recognition for vehicular traffic scenarios,"To act intelligently in dynamic environments, a system must understand the current situation it is involved in at any given time. This requires dealing with temporal context, handling multiple and ambiguous interpretations, and accounting for various sources of uncertainty. In this paper we propose a probabilistic approach to modeling and recognizing situations. We define a situation as a distribution over sequences of states that have some meaningful interpretation. Each situation is characterized by an individual hidden Markov model that describes the corresponding distribution. In particular, we consider typical traffic scenarios and describe how our framework can be used to model and track different situations while they are evolving. The approach was evaluated experimentally in vehicular traffic scenarios using real and simulated data. The results show that our system is able to recognize and track multiple situation instances in parallel and make sensible decisions between competing hypotheses. Additionally, we show that our models can be used for predicting the position of the tracked vehicles.",
TO-GO: TOpology-assist geo-opportunistic routing in urban vehicular grids,"Road topology information has recently been used to assist geo-routing, thereby improving the overall performance. However, the unreliable wireless channel nature in urban vehicular grids (due to motion, obstructions, etc) still creates problems with the basic greedy forwarding. In this paper, we propose TO-GO (TOpology-assisted Geo-Opportunistic Routing), a geo-routing protocol that exploits topology knowledge acquired via 2-hop beaconing to select the best target forwarder and incorporates opportunistic forwarding with the best chance to reach it. The forwarder selection takes into account of wireless channel quality, thus significantly improving performance in error and interference situations. Extensive simulations confirm TO-GO superior robustness to errors/losses as compared to conventional topology-assisted geographic routing.",
Dynamic Partial Reconfiguration in FPGAs,Dynamic parital reconfigurable FPGAs offer new design space with a variety of benefits: reduce the configuration time and save memory as the partial reconfiguration files (bitstreams) are smaller than full ones. This paper introduces a simple reconfigurable system and focuses on the advantages of the newest dynamic partial reconfiguration design flow.,
Scalable video multicast in multi-carrier wireless data systems,"Future 4G cellular networks are featured with high data rate and improved coverage, which will enable realtime video multicast and broadcast services. Scalable video coding is very appropriate for wireless multicast service because it allows to choose different modulation and coding schemes (MCSs) for different video layers and can provide good video quality to users in good channel conditions while still maintaining reasonable video quality for other users. In order to apply scalable video coding to wireless multicast streaming, it is important to choose appropriate MCS for each video layer and to determine the right resource allocation among multiple video sessions. In this paper we propose a two-step dynamic programming algorithm that finds the optimal total system utility of all users where the system utility can be a generic non-negative, non-decreasing function of received rate. Our algorithm supports variable layer size and thus allows flexibility during the video encoding process. Simulation results show that our algorithm offers significant improvement on the video quality over a naive algorithm and an adapted greedy algorithm, especially in the scenarios with multiple real video sequences and limited radio resources.",
Gait recognition using Gait Entropy Image,"Gait as a behavioural biometric is concerned with how people walk. However, most existing gait representations capture both motion and appearance information. They are thus sensitive to changes in various covariate conditions such as carrying and clothing. In this paper, a novel gait representation termed as Gait Entropy Image (GEnI) is proposed. Based on computing entropy, a GEnI encodes in a single image the randomness of pixel values in the silhouette images over a complete gait cycle. It thus captures mostly motion information and is robust to covariate condition changes that affect appearance. Extensive experiments on the USF HumanID dataset, CASIA dataset and the SOTON dataset have been carried out to demonstrate that the proposed gait representation outperforms existing methods, especially when there are significant appearance changes. Our experiments also show clear advantage of GEnI over the alternatives without the assumption on cooperative subjects, i.e. both the gallery and the probe sets consist of a mixture of gait sequences under different and unknown covariate conditions.","image representation,
biomimetics,
entropy,
gait analysis"
Network Coding: A Computational Perspective,"In this work, we study the computational perspective of network coding, focusing on two issues. First, we address the computational complexity of finding a network code for acyclic multicast networks. Second, we address the issue of reducing the amount of computation performed by network nodes. In particular, we consider the problem of finding a network code with the minimum possible number of encoding nodes, i.e. nodes that generate new packets by performing algebraic operations on packets received over incoming links.We present a deterministic algorithm that finds a feasible network code for a multicast network over an underlying graph G(V,E) in time 0(\E\kh + \V\k2h2 + h4k3(k + h)), where k is the number of destinations and h is the number of packets. Our algorithm improves the best known running time for network code construction. In addition, our algorithm guarantees that the number of encoding nodes in the obtained network code is upper- bounded by 0(h3k2). Next, we address the problem of finding integral and fractional network codes with the minimum number of encoding nodes. We prove that in the majority of settings this problem is NP-hard. However, we show that if h = O(1),k = O(1), and the underlying communication graph is acyclic, then there exists an algorithm that solves this problem in polynomial time.","Network coding,
Computer networks,
Encoding,
Multicast algorithms,
Computational complexity,
Polynomials,
Routing,
Computer science"
Universal rewriting in constrained memories,"A constrained memory is a storage device whose elements change their states under some constraints. A typical example is flash memories, in which cell levels are easy to increase but hard to decrease. In a general rewriting model, the stored data changes with some pattern determined by the application. In a constrained memory, an appropriate representation is needed for the stored data to enable efficient rewriting.",
Characterizing Locally Indistinguishable Orthogonal Product States,"Bennett [Physical Review A, vol. 59, no. 2, p. 1070, 1999] identified a set of orthogonal product states in the Hilbert space \BBC 3 otimes\BBC 3 such that reliably distinguishing those states requires nonlocal quantum operations. While more examples have been found for this counterintuitive ldquononlocality without entanglementrdquo phenomenon, a complete and computationally verifiable characterization for all such sets of states remains unknown. In this paper, we give such a characterization for both \BBC 3 otimes\BBC 3 and \BBC 2 otimes\BBC 2 otimes\BBC 2. As a consequence, we show that in both spaces, there is no additional set of a fundamentally different structure than those of the known instances.",
The Comparative Research on Image Segmentation Algorithms,"As the premise of feature extraction and pattern recognition, image segmentation is one of the fundamental approaches of digital image processing. This paper enumerates and reviews main image segmentation algorithms, then presents basic evaluation methods for them, finally discusses the prospect of image segmentation. Some valuable characteristics of image segmentation come out after a large number of comparative experiments.","Image segmentation,
Image edge detection,
Colored noise,
Educational technology,
Digital images,
Filters,
Merging,
Pixel,
Laplace equations,
Neural networks"
Efficient Mining of Closed Repetitive Gapped Subsequences from a Sequence Database,"There is a huge wealth of sequence data available, for example, customer purchase histories, program execution traces, DNA, and protein sequences. Analyzing this wealth of data to mine important knowledge is certainly a worthwhile goal.In this paper, as a step forward to analyzing patterns in sequences, we introduce the problem of mining closed repetitive gapped subsequences and propose efficient solutions. Given a database of sequences where each sequence is an ordered list of events, the pattern we would like to mine is called repetitive gapped subsequence, which is a subsequence (possibly with gaps between two successive events within it) of some sequences in the database. We introduce the concept of repetitive support to measure how frequently a pattern repeats in the database. Different from the sequential pattern mining problem, repetitive support captures not only repetitions of a pattern in different sequences but also the repetitions within a sequence. Given a userspecified support threshold min_sup, we study finding the set of all patterns with repetitive support no less than min_sup. To obtain a compact yet complete result set and improve the efficiency, we also study finding closed patterns. Efficient mining algorithms to find the complete set of desired patterns are proposed based on the idea of instance growth. Our performance study on various datasets shows the efficiency of our approach. A case study is also performed to show the utility of our approach.","Sequences,
Data mining,
Computer science,
History,
DNA,
Proteins,
Transaction databases,
Data engineering,
Management information systems,
Conference management"
Secure and Robust Localization in a Wireless Ad Hoc Environment,"We study the problem of accurate localization of static or mobile nodes in a wireless ad hoc network using the distance estimates of a group of untrusted anchors within the communication range of the nodes. Some of the anchors may be malicious and may independently lie about the distance estimate. The malicious anchors may also collude to lie about the distance estimates. In both cases, accurate node localization may be seriously undermined. We propose a scheme that performs accurate localization of the nodes in the network despite the presence of such malicious anchors. We also show how to identify most of these malicious anchors. In the case where measurements are error free, we derive a critical threshold B for the number of malicious anchors that can be tolerated in the localization process without undermining accuracy. We also show how to correctly localize a node and identify all the malicious anchors in this setting. In the presence of measurement errors, we propose a convex optimization-based localization scheme that can accurately localize a node, as long as the number of malicious anchors in its communication range is no more than B. Simulation results show that our schemes are very effective. When the measurements are error prone and the number of malicious anchors is no more than B , our scheme localizes a node with an error less than 8% and is also able to identify a significant number of malicious anchors. Our schemes guarantee that a true anchor is not identified as malicious.","Robustness,
Communication system security,
Wireless networks,
Mobile ad hoc networks,
Mobile communication,
Measurement errors,
US Government,
Computer science,
Costs,
Global Positioning System"
Three-Dimensional Blood Vessel Quantification via Centerline Deformation,"It is clinically important to quantify the geometric parameters of an abnormal vessel, as this information can aid radiologists in choosing appropriate treatments or apparatuses. Centerline and cross-sectional diameters are commonly used to characterize the morphology of vessel in various clinical applications. Due to the existence of stenosis or aneurysm, the associated vessel centerline is unable to truly portray the original, healthy vessel shape and may result in inaccurate quantitative measurement. To remedy such a problem, a novel method using an active tube model is proposed. In the method, a smoothened centerline is determined as the axis of a deformable tube model that is registered onto the vessel lumen. Three types of regions, normal, stenotic, and aneurysmal regions, are defined to classify the vessel segment under-analyzed by use of the algorithm of a cross-sectional-based distance field. The registration process used on the tube model is governed by different region-adaptive energy functionals associated with the classified vessel regions. The proposed algorithm is validated on the 3-D computer-generated phantoms and 3-D rotational digital subtraction angiography (DSA) datasets. Experimental results show that the deformed centerline provides better vessel quantification results compared with the original centerline. It is also shown that the registered model is useful for measuring the volume of aneurysmal regions.","Blood vessels,
Aneurysm,
Deformable models,
Shape measurement,
Biomedical imaging,
Image segmentation,
Length measurement,
Morphology,
Geophysics computing,
Imaging phantoms"
Prosodic and other Long-Term Features for Speaker Diarization,"Speaker diarization is defined as the task of determining ldquowho spoke whenrdquo given an audio track and no other prior knowledge of any kind. The following article shows how a state-of-the-art speaker diarization system can be improved by combining traditional short-term features (MFCCs) with prosodic and other long-term features. First, we present a framework to study the speaker discriminability of 70 different long-term features. Then, we show how the top-ranked long-term features can be combined with short-term features to increase the accuracy of speaker diarization. The results were measured on standardized datasets (NIST RT) and show a consistent improvement of about 30% relative in diarization error rate compared to the best system presented at the NIST evaluation in 2007.","NIST,
Speaker recognition,
Cepstral analysis,
Computer science,
Error analysis,
Mel frequency cepstral coefficient,
Speech analysis,
Density estimation robust algorithm,
System testing,
Speech processing"
Wrapped Gaussian Mixture Models for Modeling and High-Rate Quantization of Phase Data of Speech,"The harmonic representation of speech signals has found many applications in speech processing. This paper presents a novel statistical approach to model the behavior of harmonic phases. Phase information is decomposed into three parts: a minimum phase part, a translation term, and a residual term referred to as dispersion phase. Dispersion phases are modeled by wrapped Gaussian mixture models (WGMMs) using an expectation-maximization algorithm suitable for circular vector data. A multivariate WGMM-based phase quantizer is then proposed and constructed using novel scalar quantizers for circular random variables. The proposed phase modeling and quantization scheme is evaluated in the context of a narrowband harmonic representation of speech. Results indicate that it is possible to construct a variable-rate harmonic codec that is equivalent to iLBC at approximately 13 kbps.",
Churn Prediction in MMORPGs: A Social Influence Based Approach,"Massively Multiplayer Online Role Playing Games(MMORPGs) are computer based games in which players interactwith one another in the virtual world. Worldwide revenuesfor MMORPGs have seen amazing growth in last few years and itis more than a 2 billion dollars industry as per current estimates.Huge amount of revenue potential has attracted several gamingcompanies to launch online role playing games. One of the majorproblems these companies suffer apart from fierce competitionis erosion of their customer base. Churn is a big problem for thegaming companies as churners impact negatively in the wordof-mouth reports for potential and existing customers leading tofurther erosion of user base.We study the problem of player churn in the popularMMORPG EverQuest II. The problem of churn predictionhas been studied extensively in the past in various domainsand social network analysis has recently been applied to theproblem to understand the effects of the strength of social tiesand the structure and dynamics of a social network in churn.In this paper, we propose a churn prediction model based onexamining social influence among players and their personalengagement in the game. We hypothesize that social influence is avector quantity, with components negative influence and positiveinfluence. We propose a modified diffusion model to propagatethe influence vector in the players network which represents thesocial influence on the player from his network. We measure aplayers personal engagement based on his activity patterns anduse it in the modified diffusion model and churn prediction. Ourmethod for churn prediction which combines social influenceand player engagement factors has shown to improve predictionaccuracy significantly for our dataset as compared to predictionusing the conventional diffusion model or the player engagementfactor, thus validating our hypothesis that combination of boththese factors could lead to a more accurate churn prediction.",
A successful interdisciplinary course on coputational intelligence,"This article presents experiences from the introduction of a new three hour interdisciplinary course on computational intelligence (CI) taught at the Missouri University of Science and Technology, USA at the undergraduate and graduate levels. This course is unique in the sense that it covers five main paradigms of CI and their integration to develop hybrid intelligent systems. The paradigms covered are artificial immune systems (AISs), evolutionary computing (EC), fuzzy systems (FSs), neural networks (NNs) and swarm intelligence (SI). While individual CI paradigms have been applied successfully to solve real-world problems, the current trend is to develop hybrids of these paradigms since no one paradigm is superior to any other for solving all types of problems. In doing so, respective strengths of individual components in a hybrid CI system are capitalized while their weaknesses are eliminated. This CI course is at the introductory level and the objective is to lead students to in-depth courses and specialization in a particular paradigm (AISs, EC, FSs, NNs, SI). The idea of an integrated and interdisciplinary course like this, especially at the undergraduate level, is to expose students to different CI paradigms at an early stage in their degree program and career. The curriculum, assessment, implementation, and impacts of an interdisciplinary CI course are described.",
Use of Agile Methods in Software Engineering Education,"The use of contemporary software development approaches such as agile methods is growing in widespread use throughout the world. Although some universities are starting to teach them, courses on agile methods at the undergraduate and graduate levels are still a new phenomenon. The University of Maryland University College (UMUC) adapted agile methods for its capstone course towards a master’s degree in software engineering in the Fall of 2008. Three distributed teams of five students were asked to use agile methods to build competing electronic commerce websites. With little training in agile methods, virtual teams, collaboration tools, or web design, each of the three teams completed fully functional e-commerce websites using agile methods in little more than 13 weeks. Teams who struck an optimum balance of customer collaboration, use of agile methods, and technical programming ability had better productivity and website quality.","Software engineering,
Educational institutions,
Programming profession,
Computer industry,
Service oriented architecture,
Software standards,
Military computing,
Dynamic programming,
Testing,
Electronic commerce"
Multi-label sparse coding for automatic image annotation,"In this paper, we present a multi-label sparse coding framework for feature extraction and classification within the context of automatic image annotation. First, each image is encoded into a so-called supervector, derived from the universal Gaussian Mixture Models on orderless image patches. Then, a label sparse coding based subspace learning algorithm is derived to effectively harness multi-label information for dimensionality reduction. Finally, the sparse coding method for multi-label data is proposed to propagate the multi-labels of the training images to the query image with the sparse ℓ1 reconstruction coefficients. Extensive image annotation experiments on the Corel5k and Corel30k databases both show the superior performance of the proposed multi-label sparse coding framework over the state-of-the-art algorithms.",
Electric power distribution system design and planning in a deregulated environment,"This study presents a hierarchical dynamic optimisation model to address the design, planning and energy scheduling problem in power distribution systems. In addition to planner controlled investments, uncoordinated investor distributed generation investments are also respected, for which recommendations given policies can be made. Beyond planning, this model's usefulness also extends into predicting the impact of different energy policies on system operation and economics.","power distribution planning,
power distribution economics"
Fast Track: A Software System for Speculative Program Optimization,"Fast track is a software speculation system that enables unsafe optimization of sequential code. It speculatively runs optimized code to improve performance and then checks the correctness of the speculative code by running the original program on multiple processors. We present the interface design and system implementation for Fast Track. It lets a programmer or a pro¿ling tool mark fast-track code regions and uses a run-time system to manage the parallel execution of the speculative process and its checking processes and ensures the correct display of program outputs. The core of the run-time system is a novel concurrent algorithm that balances exploitable parallelism and available processors when the fast track is too slow or too fast. The programming interface closely affects the run-time support. Our system permits both explicit and implicit end markers for speculatively optimized code regions as well as extensions that allow the use of multiple tracks and user de¿ned correctness checking. We discuss the possible uses of speculative optimization and demonstrate the effectiveness of our prototype system by examples of unsafe semantic optimization and a general system for fast memory-safety checking, which is able to reduce the checking time by factors between 2 and 7 for large sequential code on a 8-CPU system.",
Integrating Behavioral Trust in Web Service Compositions,"Algorithms for composing Web services (WS) traditionally utilize the functional and quality-of-service parameters of candidate services to decide which services to include in the composition. Users often have differing experiences with a WS. While trust in a WS is multi-faceted and consists of security and behavioral aspects, our focus in this paper is on the latter. We adopt a formal model for trust in a WS, which meets many of our intuitions about trustworthy WSs. We hypothesize predictors of a positive experience with a WS and conduct a small pilot study to explore correlations between subjects' experiences with WSs in a composition and the predictor values for those WSs. Furthermore, we show how we may derive trust for compositions from trust models of individual services. We conclude by presenting and evaluating a novel framework, called Wisp, that utilizes the trust models and, in combination with any WS composition tool, chooses compositions to deploy that are deemed most trustworthy.",
Queries and tags in affect-based multimedia retrieval,"An approach for implementing affective information as tags for multimedia content indexing and retrieval is presented. The approach can be used for implicit as well as explicit tags and is presented here using data recorded during the viewing of movie fragments containing annotations and physiological signal recordings. For retrieval based on affective queries, a representation of the query-words is defined in the arousalvalence space in the form of a Gaussian probability distribution and a retrieval method based on this representation is presented. Validation of retrieval accuracy is performed using Precision and Recall parameters. Results show that the use of arousal and valence as affective tags can improve retrieval results.",
Empirical capacity of mmWave WLANS,"The 60 GHz spectrum gives us the opportunity to deliver gigabit rates to users in a WLAN (wireless local area network) setting. The constrained propagation of signals at this frequency band ensures limited coverage which in turn enables the construction of very efficient STDMA (spatial time division multiple access) schedules. In this paper we study the achievable aggregate capacity in a room when using two types of smart antenna arrays - linear and circular. Using detailed Matlab simulations, we show that with just 400 MHz of the spectrum, aggregate data rates of 9 Gbps (4.5 Gbps) can be achieved with linear (circular) arrays. We also study the energy efficiency of the communication and show that the energy/bit is as low as 0.2 times 10-10 (0.2 times 10-9)Joules/Bit by using variable transmit powers at different parts of the room. Finally, we study the problem of coverage due to blocking of the LoS (line of sight) path. To mitigate this problem we study the use of static reflectors and show that coverage in the entire room can indeed be maintained.",
Nonlinear attitude observers on SO(3) for complementary and compatible measurements: A theoretical study,"This paper considers the question of designing an attitude observer exploiting the structure of the Special Orthogonal Group SO(3) for both inertial and body-fixedframe measurements. We consider measurements from a minimal sensor suite, typically a rate gyroscope along with several measurements of inertial and/or body-fixed vector directions. We propose fully nonlinear pose observers based directly on the vector measurements, allowing a mix of both inertial and body-fixed measurements.We provide a comprehensive stability analysis of the observer error dynamics. We show that even with a single vector measurement it is often possible to recover asymptotically stable observer error dynamics.",
On the SIR of a cellular infrared optical wireless system for an aircraft,"In this paper, first, path loss models are developed for infrared optical wireless transmission inside an aircraft cabin. Second, a cellular network in the aircraft is considered and signal-to-interference ratio (SIR) maps are determined via simulation. For this purpose, a Monte Carlo ray-tracing (MCRT) simulation is performed in a geometric computer-aided design (CAD) cabin model with defined position, azimuth (AZ), elevation (EL) and field of view (FOV) properties of transmitters and receivers. Mathematical models are developed for line-of-sight (LOS) and non-line-of-sight (NLOS) path losses along particular paths, including estimation of the path loss exponent and the shadowing component. The shadowing is modeled according to a log-normal distribution with zero mean and standard deviation sigma. The validity of this model is confirmed in the paper. It is shown that irradiance distribution under LOS conditions experiences an attenuation with a path loss exponent of 1.92 and a shadowing standard deviation of 0.81 dB. In NLOS conditions, however, the path loss exponent varies, depending on the nature of the NLOS cases considered. The presented NLOS scenarios yield path loss exponent values of 2.26 and 1.28, and shadowing standard deviation values of 1.27 dB and 0.7 dB, respectively. Finally, the cabin is divided into cells and SIR maps are presented for different frequency reuse factors. It is shown that at the edges of the circular cells with diameter of 2.8 m, a SIR of -5.5 dB is achieved in a horizontal cross section of the cabin for frequency reuse of 1, and -2 dB and 3 dB for frequency reuse factors of 2 and 3, respectively. This means that in an aircraft cabin, for reuse factors less than three, viable communication at the cell edges is not feasible without additional interference avoidance or interference mitigation techniques.","Aircraft,
Shadow mapping,
Optical transmitters,
Optical attenuators,
Optical receivers,
Computational modeling,
Solid modeling,
Design automation,
Frequency,
Interference"
Internet Mapping: From Art to Science,"We are designing, implementing, deploying, and operating a secure measurement platform capable of performing various types of Internet infrastructure measurements and assessments. We integrate state-of-the-art measurement and analysis capabilities to try to build a coherent view of Internet topology. In September 2007 we began to use this novel architecture to support ongoing global Internet topology measurement and mapping, and are now gathering the largest set of IP topology data for use by academic researchers. We are using the best available techniques for IP topology mapping, and are developing some new techniques, as well as supporting software for data analysis, topology generation, and interactive visualization of resulting large annotated graphs. This paper presents our current results, next steps, and future goals.",
Exploring FPGAs for accelerating the phylogenetic likelihood function,"Driven by novel biological wet lab techniques such as pyrosequencing there has been an unprecedented molecular data explosion. The growth of biological sequence data has significantly out-paced Moore's law. This development also poses new computational and architectural challenges for the field of phylogenetic inference, i.e., the reconstruction of evolutionary histories (trees) for a set of organisms which are represented by respective molecular sequences. Phylogenetic trees are currently increasingly reconstructed from multiple genes or even whole genomes. The introduced term ""phylogenomics"" reflects this development. Hence, there is an urgent need to deploy and develop new techniques and computational solutions to calculate the computationally intensive scoring functions for phylogenetic trees. In this paper, we propose a dedicated computer architecture to compute the phylogenetic maximum likelihood (ML) function. The ML criterion represents one of the most accurate statistical models for phylogenetic inference and accounts for 85% to 95% of total execution time in all state-of-the-art ML-based phylogenetic inference programs. We present the implementation of our architecture on an FPGA (field programmable gate array) and compare the performance to an efficient C implementation of the ML function on a high-end multi-core architecture with 16 cores. Our results are two-fold: (i) the initial exploratory implementation of the ML function for trees comprising 4 up to 512 sequences on an FPGA yields speedups of a factor 8.3 on average compared to execution on a single-core and is faster than the OpenMP-based parallel implementation on up to 16 cores in all but one case; and (ii) we are able to show that current FPGAs are capable to efficiently execute floating point intensive computational kernels.","Field programmable gate arrays,
Acceleration,
Phylogeny,
Computer architecture,
Explosions,
Moore's Law,
Biology computing,
History,
Organisms,
Genomics"
A Rule-Based Classification Algorithm for Uncertain Data,"Data uncertainty is common in real-world applications due to various causes, including imprecise measurement, network latency, outdated sources and sampling errors. These kinds of uncertainty have to be handled cautiously, or else the mining results could be unreliable or even wrong. In this paper, we propose a new rule-based classification and prediction algorithm called uRule for classifying uncertain data. This algorithm introduces new measures for generating, pruning and optimizing rules. These new measures are computed considering uncertain data interval and probability distribution function. Based on the new measures, the optimal splitting attribute and splitting value can be identified and used for classification and prediction. The proposed uRule algorithm can process uncertainty in both numerical and categorical data. Our experimental results show that uRule has excellent performance even when data is highly uncertain.",
Personalized X-Ray 3-D Reconstruction of the Scoliotic Spine From Hybrid Statistical and Image-Based Models,"This paper presents a novel 3-D reconstruction method of the scoliotic spine using prior vertebra models with image-based information taken from biplanar X-ray images. We first propose a global modeling approach by exploiting the 3-D scoliotic curve reconstructed from a coronal and sagittal X-ray image in order to generate an approximate statistical model from a 3-D database of scoliotic patients based on a transformation algorithm which incorporates intuitive geometrical properties. The personalized 3-D reconstruction of the spine is then achieved with a novel segmentation method which takes into account the variable appearance of scoliotic vertebrae (rotation, wedging) from standard quality images in order to segment and isolate individual vertebrae on the radiographic planes. More specifically, it uses prior 3-D models regulated from 2-D image level set functionals to identify and match corresponding bone structures on the biplanar X-rays. An iterative optimization procedure integrating similarity measures such as deformable vertebral contours regulated from high-level anatomical primitives, morphological knowledge and epipolar constraints is then applied to globally refine the 3-D anatomical landmarks on each vertebra level of the spine. This method was validated on twenty scoliotic patients by comparing results to a standard manual approach. The qualitative evaluation of the retro-projection of the vertebral contours confirms that the proposed method can achieve better consistency to the X-ray image's natural content. A comparison to synthetic models and real patient data also yields good accuracy on the localization of low-level primitives such as anatomical landmarks identified by an expert on each vertebra. The experiments reported in this paper demonstrate that the proposed method offers a better matching accuracy on a set of landmarks from biplanar views when compared to a manual technique for each evaluated cases, and its precision is comparable to 3-D models generated from magnetic resonance images, thus suitable for routine 3-D clinical assessment of spinal deformities.","X-ray imaging,
Three dimensional displays,
Solid modeling,
Image segmentation,
Spine,
Image reconstruction,
Image databases,
Spatial databases,
Radiography,
Level set"
Reconstruction of Vectorial Acoustic Sources in Time-Domain Tomography,"A new theory is proposed for the reconstruction of curl-free vector field, whose divergence serves as acoustic source. The theory is applied to reconstruct vector acoustic sources from the scalar acoustic signals measured on a surface enclosing the source area. It is shown that, under certain conditions, the scalar acoustic measurements can be vectorized according to the known measurement geometry and subsequently be used to reconstruct the original vector field. Theoretically, this method extends the application domain of the existing acoustic reciprocity principle from a scalar field to a vector field, indicating that the stimulating vectorial source and the transmitted acoustic pressure vector (acoustic pressure vectorized according to certain measurement geometry) are interchangeable. Computer simulation studies were conducted to evaluate the proposed theory, and the numerical results suggest that reconstruction of a vector field using the proposed theory is not sensitive to variation in the detecting distance. The present theory may be applied to magnetoacoustic tomography with magnetic induction (MAT-MI) for reconstructing current distribution from acoustic measurements. A simulation on MAT-MI shows that, compared to existing methods, the present method can give an accurate estimation on the source current distribution and a better conductivity reconstruction.",
Integrating contour and skeleton for shape classification,"Shape analysis has been a long standing problem in the literature. In this paper, we address the shape classification problem and make the following contributions: (1) We combine both contour and skeleton (also local and global) information for shape analysis, and we derive an effective classifier. (2) We collect a challenging shape database in which there are 20 categories of animals, with each having 100 shapes. All these shapes are obtained from real images with a large variation in pose, viewing angle, articulation, and self-occlusion. (3) We emphasize the importance of having good representation for shape classification to address the unique characteristics of shape. A thorough experimental study is conducted showing significant improvement by the proposed algorithm over many of the state-of-the-art shape matching and classification algorithms, on both our dataset and the well-known MPEG-7 dataset [19]. In addition, we applied our algorithm for recognizing and classifying objects from natural images and obtained very encouraging results.",
Connectivity analysis of wireless sensor networks with regular topologies in the presence of channel fading,"This paper investigates the probabilistic connectivity of wireless sensor networks in the presence of channel fading. Due to the stochastic nature of wireless channels in sensor networks, the geometric disk shaped model widely used for connectivity analysis of sensor networks can be misleading. In this paper, we develop a mathematical model to evaluate the probabilistic connectivity of sensor networks which incorporates the characteristics of wireless channels, multi-access interference, the network topology and the propagation environment. We present an analytical framework for the computation of node isolation probability and network connectivity under different channel fading models. We also analyze the connectivity of sensor networks in the presence of unreliable sensors. We present numerical and simulation results that compare different regular topologies in terms of several metrics such as node isolation probability, end-to-end connectivity, and network connectivity.","Wireless sensor networks,
Network topology,
Fading,
Sensor phenomena and characterization,
Stochastic processes,
Solid modeling,
Mathematical model,
Interference,
Computer networks,
Computational modeling"
A vision of smart transmission grids,"Modern power grid is required to become smarter in order to provide an affordable, reliable, and sustainable supply of electricity. Under such circumstances, considerable activities have been carried out in the U.S. and Europe to formulate and promote a vision for the development of the future smart power grids. However, the majority of these activities only placed emphasis on the distribution grid and demand side; while the big picture of the transmission grid in the context of smart grids is still unclear. This paper presents a unique vision for the future smart transmission grids in which the major features that these grids must have are clearly identified. In this vision, each smart transmission grid is regarded as an integrated system that functionally consists of three interactive, smart components, i.e., smart control centers, smart transmission networks, and smart substations. The features and functions of each of the three functional components as well as the enabling technologies to achieve these features and functions are discussed in detail in the paper.",
An Adaptive and Predictive Respiratory Motion Model for Image-Guided Interventions: Theory and First Clinical Application,"This paper describes a predictive and adaptive single parameter motion model for updating roadmaps to correct for respiratory motion in image-guided interventions. The model can adapt its motion estimates to respond to changes in breathing pattern, such as deep or fast breathing, which normally would result in a decrease in the accuracy of the motion estimates. The adaptation is made possible by interpolating between the motion estimates of multiple submodels, each of which describes the motion of the target organ during cycles of different amplitudes. We describe a predictive technique which can predict the amplitude of a breathing cycle before it has finished. The predicted amplitude is used to interpolate between the motion estimates of the submodels to tune the adaptive model to the current breathing pattern. The proposed technique is validated on affine motion models formed from cardiac magnetic resonance imaging (MRI) datasets acquired from seven volunteers and one patient. The amplitude prediction technique showed errors of 1.9-6.5 mm. The combined predictive and adaptive technique showed 3-D motion prediction errors of 1.0-2.8 mm, which represents an improvement in modelling performance of up to 40% over a standard nonadaptive single parameter motion model. We also applied the combined technique in a clinical setting to test the feasibility of using it for respiratory motion correction of roadmaps in image-guided cardiac catheterisations. In this clinical case we show that 2-D registration errors due to respiratory motion are reduced from 7.7 to 2.8 mm using the proposed technique.",
Learning optimized MAP estimates in continuously-valued MRF models,"We present a new approach for the discriminative training of continuous-valued Markov Random Field (MRF) model parameters. In our approach we train the MRF model by optimizing the parameters so that the minimum energy solution of the model is as similar as possible to the ground-truth. This leads to parameters which are directly optimized to increase the quality of the MAP estimates during inference. Our proposed technique allows us to develop a framework that is flexible and intuitively easy to understand and implement, which makes it an attractive alternative to learn the parameters of a continuous-valued MRF model. We demonstrate the effectiveness of our technique by applying it to the problems of image denoising and in-painting using the Field of Experts model. In our experiments, the performance of our system compares favourably to the Field of Experts model trained using contrastive divergence when applied to the denoising and in-painting tasks.","Markov random fields,
Noise reduction,
Computer science,
Image denoising,
Maximum likelihood estimation,
Parameter estimation,
Energy measurement,
Loss measurement,
Machine vision,
Stochastic processes"
Simple channel coding bounds,"New channel coding converse and achievability bounds are derived for a single use of an arbitrary channel. Both bounds are expressed using a quantity called the “smooth 0-divergence”, which is a generalization of Rényi's divergence of order 0. The bounds are also studied in the limit of large block-lengths. In particular, they combine to give a general capacity formula which is equivalent to the one derived by Verdú and Han.","Channel coding,
Error probability,
Decoding,
Physics,
Signal processing,
Information processing,
Laboratories,
Computer science,
Stochastic processes,
Kernel"
Analyzing Information Flow in JavaScript-Based Browser Extensions,"JavaScript-based browser extensions (JSEs) enhance the core functionality of web browsers by improving their look and feel, and are widely available for commodity browsers. To enable a rich set of functionalities, browsers typically execute JSEs with elevated privileges. For example, unlike JavaScript code in a web application, code in a JSE is not constrained by the same-origin policy. Malicious JSEs can misuse these privileges to compromise confidentiality and integrity, e.g., by stealing sensitive information, such as cookies and saved passwords, or executing arbitrary code on the host system. Even if a JSE is not overtly malicious, vulnerabilities in the JSE and the browser may allow a remote attacker to compromise browser security. We present Sabre (Security Architecture for Browser Extensions), a system that uses in-browser information-flow tracking to analyze JSEs. Sabre associates a label with each in-memory JavaScript object in the browser, which determines whether the object contains sensitive information. Sabre propagates labels as objects are modified by the JSE and passed between browser subsystems. Sabre raises an alert if an object containing sensitive information is accessed in an unsafe way, e.g., if a JSE attempts to send the object over the network or write it to a file. We implemented Sabre by modifying the Firefox browser and evaluated it using both malicious JSEs as well as benign ones that contained exploitable vulnerabilities. Our experiments show that Sabre can precisely identify potential information flow violations by JSEs.",
A new method to forecast enrollments using fuzzy time series and clustering techniques,"This paper presents a new method to forecast enrollments using fuzzy time series and clustering techniques. First, we present an automatic clustering algorithm to partition the universe of discourse into different lengths of intervals. Then, we present a new method for forecasting enrollments using fuzzy time series and the proposed clustering algorithm. The historical data of the University of Alabama are used to illustrate the forecasting process of the proposed method. The experimental results show that the proposed method gets a higher average forecasting accuracy rate than the existing methods.",
Optimally controlling Hybrid Electric Vehicles using path forecasting,The paper examines path-dependent control of Hybrid Electric Vehicles (HEVs). In this approach we seek to improve HEV fuel economy by optimizing charging and discharging of the vehicle battery depending on the forecasted vehicle route. The route is decomposed into a series connection of route segments with (partially) known properties. The dynamic programming is used as a tool to quantify the benefits offered by route information availability.,
Accurately Measuring Denial of Service in Simulation and Testbed Experiments,"Researchers in the denial-of-service (DoS) field lack accurate, quantitative, and versatile metrics to measure service denial in simulation and testbed experiments. Without such metrics, it is impossible to measure severity of various attacks, quantify success of proposed defenses, and compare their performance. Existing DoS metrics equate service denial with slow communication, low throughput, high resource utilization, and high loss rate. These metrics are not versatile because they fail to monitor all traffic parameters that signal service degradation. They are not quantitative because they fail to specify exact ranges of parameter values that correspond to good or poor service quality. Finally, they are not accurate since they were not proven to correspond to human perception of service denial. We propose several DoS impact metrics that measure the quality of service experienced by users during an attack. Our metrics are quantitative: they map QoS requirements for several applications into measurable traffic parameters with acceptable, scientifically determined thresholds. They are versatile: they apply to a wide range of attack scenarios, which we demonstrate via testbed experiments and simulations. We also prove metrics' accuracy through testing with human users.",
Differentiating the roles of IR measurement and simulation for power and temperature-aware design,"In temperature-aware design, the presence or absence of a heatsink fundamentally changes the thermal behavior with important design implications. In recent years, chip-level infrared (IR) thermal imaging has been gaining popularity in studying thermal phenomena and thermal management, as well as reverse-engineering chip power consumption. Unfortunately, IR thermal imaging needs a peculiar cooling solution, which removes the heatsink and applies an IR-transparent liquid flow over the exposed bare die to carry away the dissipated heat. Because this cooling solution is drastically different from a normal thermal package, its thermal characteristics need to be closely examined. In this paper, we characterize the differences between two cooling configurations—forced air flow over a copper heatsink (AIR-SINK) and laminar oil flow over bare silicon (OIL-SILICON). For the comparison, we modify the HotSpot thermal model by adding the IR-transparent oil flow and the secondary heat transfer path through the package pins, hence modeling what the IR camera actually sees at runtime. We show that OIL-SILICON and AIR-SINK are significantly different in both transient and steady-state thermal responses. OIL-SILICON has a much slower short-term transient response, which makes dynamic thermal management less efficient. In addition, for OIL-SILICON, the direction of oil flow plays an important role by changing hot spot location, thus impacting hot spot identification and thermal sensor placement. These results imply that the power- and temperature-aware design process cannot just rely on IR measurements. Simulation and IR measurement are both needed and are complementary techniques.",
"Just What Is an Ontology, Anyway?","This tutorial article describes some definitions of ""ontology"" as it relates to computer applications and gives an overview of some common ontology-based applications.","Ontologies,
OWL,
Unified modeling language,
Dairy products,
Microstrip,
Resource description framework,
Computer science,
Markup languages,
Semantic Web,
Application software"
Towards social-aware routing in dynamic communication networks,"Many communication networks such as Mobile Ad Hoc Networks (MANETs) involve in human interactions and exhibit properties of social networks. Hence, it is interesting to see how knowledge from social networks can be used to enhance the communication processes. We focus on the use of identifying modular structure in social networks to improve the efficiency of routing strategies. Since nodes mobility in a network often alters its modular structure and requires recomputing of modules from scratch, updating the modules is the main bottleneck in current social-aware routing strategies where nodes often have limited processing speed. Towards real-time routing strategies, we develop an adaptive method to efficiently update modules in a dynamic network in which a novel compact representation of the network is used to significantly reduces the network size while preserving essential network structure.","Communication networks,
Social network services,
Network topology,
Mobile ad hoc networks,
Humans,
Mobile communication,
Routing protocols,
Bandwidth,
Computer networks,
Information science"
Home appliance energy monitoring and controlling based on Power Line Communication,"In this paper an embedded remote monitoring and controlling power socket (RMCPS) has been developed with high suitability for automatic and power management of home electric appliances. It requires no new layout and has the advantage of low cost, low electricity consumption, small volume and convenient installation to replace the PC with a Web server construction. The RMCPS consists of three modules: the Essential Module, the Power Line Communication (PLC) Module and the Detection Module. This allows the user a remote Internet connection to an embedded power socket to both monitor and control home electric appliances.","Home appliances,
Communication system control,
Power line communications,
Remote monitoring,
Automatic control,
Sockets,
Energy management,
Costs,
Energy consumption,
Web server"
Through-Silicon Via (TSV)-induced noise characterization and noise mitigation using coaxial TSVs,"Through-Silicon Via (TSV) is a critical interconnect element in 3D integration technology. TSVs introduce many new design challenges. In addition to competing with devices for real estate, TSVs can act as a major noise source throughout the substrate. We present in this paper a comprehensive study of TSV-induced noise as a function of several critical design and process parameters including substrate type, signal slew rate, TSV height, ILD thickness, and TSV-to-device and TSV-to-TSV spacing. We create a SPICE model for simulating TSV-to-device and TSV-to-TSV noise couplings in two different types of substrates: a lightly doped bulk substrate, and a lightly doped thin epitaxial layer on top of a heavily doped bulk. Our SPICE model provides small error when compared with a detailed Finite Element Analysis Method. Our findings show the importance of using a grounded backplane in reducing noise and how coaxial TSVs further mitigate TSV-induced noise.","Coaxial components,
Substrates,
Through-silicon vias,
SPICE,
Semiconductor process modeling,
Noise reduction,
Process design,
Signal design,
Signal processing,
Optical coupling"
A clustering particle swarm optimizer for dynamic optimization,"In the real world, many applications are nonstationary optimization problems. This requires that optimization algorithms need to not only find the global optimal solution but also track the trajectory of the changing global best solution in a dynamic environment. To achieve this, this paper proposes a clustering particle swarm optimizer (CPSO) for dynamic optimization problems. The algorithm employs hierarchical clustering method to track multiple peaks based on a nearest neighbor search strategy. A fast local search method is also proposed to find the near optimal solutions in a local promising region in the search space. Six test problems generated from a generalized dynamic benchmark generator (GDBG) are used to test the performance of the proposed algorithm. The numerical experimental results show the efficiency of the proposed algorithm for locating and tracking multiple optima in dynamic environments.",
A comparative simulation study of link quality estimators in wireless sensor networks,"Link quality estimation (LQE) in wireless sensor networks (WSNs) is a fundamental building block for an efficient and cross-layer design of higher layer network protocols. Several link quality estimators have been reported in the literature; however, none has been thoroughly evaluated. There is thus a need for a comparative study of these estimators as well as the assessment of their impact on higher layer protocols. In this paper, we perform an extensive comparative simulation study of some well-known link quality estimators using TOSSIM. We first analyze the statistical properties of the link quality estimators independently of higher-layer protocols, then we investigate their impact on the Collection Tree Routing Protocol (CTP). This work is a fundamental step to understand the statistical behavior of LQE techniques, helping system designers choose the most appropriate for their network protocol architectures.","Wireless sensor networks,
Routing protocols,
Educational institutions,
Radio link,
Computer science,
Information systems,
Cross layer design,
Wireless application protocol,
Network topology,
Radio control"
Mining of Frequent Itemsets from Streams of Uncertain Data,"Frequent itemset mining plays an essential role in the mining of various patterns and is in demand in many real-life applications. Hence, mining of frequent itemsets has been the subject of numerous studies since its introduction. Generally, most of these studies find frequent itemsets from traditional transaction databases, in which the content of each transaction--namely, items--is definitely known and precise. However, there are many real-life situations in which ones are uncertain about the content of transactions. This calls for the mining of uncertain data. Moreover, due to advances in technology, a flood of precise or uncertain data can be produced in many situations. This calls for the mining of data streams. To deal with these situations, we propose two tree-based mining algorithms to efficiently find frequent itemsets from streams of uncertain data, where each item in the transactions in the streams is associated with an existential probability. Experimental results show the effectiveness of our algorithms in mining frequent itemsets from streams of uncertain data.","Data mining,
Itemsets,
Transaction databases,
Floods,
Test pattern generators,
Testing,
Data engineering,
Computer science,
Application software,
Fires"
eOCSA: An algorithm for burst mapping with strict QoS requirements in IEEE 802.16e Mobile WiMAX networks,"Mobile WiMAX systems based on the IEEE 802.16e standard require all downlink allocations to be mapped to a rectangular region in the two dimensional subcarrier-time map. Many published resource allocation schemes ignore this requirement. It is possible that the allocations when mapped to rectangular regions may exceed the capacity of the downlink frame, and the QoS of some flows may be violated. The rectangle mapping problem is a variation of the bin or strip packing problem, which is known to be NP-complete. In a previous paper, an algorithm called OCSA (One Column Striping with non-increasing Area first mapping) for rectangular mapping was introduced. In this paper, we propose an enhanced version of the algorithm. Similar to OCSA, the enhanced algorithm is also simple and fast to implement; however, eOCSA considers the allocation of an additional resource to ensure the QoS. eOCSA also avoids an enumeration process and so lowers the complexity to O(n2).","WiMAX,
Downlink,
Resource management,
Mobile computing,
Quality of service,
Frequency conversion,
Computer science,
USA Councils,
Strips,
Bandwidth"
Optimal Threshold Selection for Tomogram Segmentation by Projection Distance Minimization,"Grey value thresholding is a segmentation technique commonly applied to tomographic image reconstructions. Many procedures have been proposed to optimally select the grey value thresholds based on the tomogram data only (e.g., using the image histogram). In this paper, a projection distance minimization (PDM) method is presented that uses the tomographic projection data to determine optimal thresholds. These thresholds are computed by minimizing the distance between the forward projection of the segmented image and the measured projection data. An important contribution of the current paper is the efficient implementation of the forward projection method, which makes the use of the original projection data as a segmentation criterion feasible. Simulation experiments applied to various phantom images show that our proposed method obtains superior results compared to established histogram-based projection data methods.",
A Novel Interface for Interactive Exploration of DTI Fibers,"Visual exploration is essential to the visualization and analysis of densely sampled 3D DTI fibers in biological speciments, due to the high geometric, spatial, and anatomical complexity of fiber tracts. Previous methods for DTI fiber visualization use zooming, color-mapping, selection, and abstraction to deliver the characteristics of the fibers. However, these schemes mainly focus on the optimization of visualization in the 3D space where cluttering and occlusion make grasping even a few thousand fibers difficult. This paper introduces a novel interaction method that augments the 3D visualization with a 2D representation containing a low-dimensional embedding of the DTI fibers. This embedding preserves the relationship between the fibers and removes the visual clutter that is inherent in 3D renderings of the fibers. This new interface allows the user to manipulate the DTI fibers as both 3D curves and 2D embedded points and easily compare or validate his or her results in both domains. The implementation of the framework is GPU based to achieve real-time interaction. The framework was applied to several tasks, and the results show that our method reduces the user's workload in recognizing 3D DTI fibers and permits quick and accurate DTI fiber selection.","Diffusion tensor imaging,
Visualization,
Tensile stress,
Magnetic resonance imaging,
Rendering (computer graphics),
Computer science,
Grasping,
Magnetic field measurement,
Velocity measurement,
Biological tissues"
Dual-electrode CMUT with non-uniform membranes for high electromechanical coupling coefficient and high bandwidth operation,"In this paper, we report measurement results on dual-electrode CMUT demonstrating electromechanical coupling coefficient (k2) of 0.82 at 90% of collapse voltage as well as 136% 3 dB one-way fractional bandwidth at the transducer surface around the design frequency of 8 MHz. These results are within 5% of the predictions of the finite element simulations. The large bandwidth is achieved mainly by utilizing a non-uniform membrane, introducing center mass to the design, whereas the dual-electrode structure provides high coupling coefficient in a large dc bias range without collapsing the membrane. In addition, the non-uniform membrane structure improves the transmit sensitivity of the dual-electrode CMUT by about 2dB as compared with a dual electrode CMUT with uniform membrane.","Biomembranes,
Bandwidth,
Frequency measurement,
Voltage,
Transducers,
Finite element methods,
Predictive models,
Electrodes"
A MapReduce-Enabled Scientific Workflow Composition Framework,"MapReduce has recently gained a lot of attention as a parallel programming model for scalable data-intensive business and scientific analysis. In order to benefit from this powerful programming model in a scientific workflow environment, we propose a MapReduce-enabled scientific workflow composition framework consisting of: i) a dataflow based scientific workflow model that separates the declaration of the workflow interface from the definition of its functional body; ii) a set of dataflow constructs, including Map, Reduce, Loop, and Conditional, and their composition semantics to enable MapReduce-style scientific workflows; iii) an XML-based scientific workflow specification language, called WSL, in which both Map and Reduce are fully composable with other dataflow constructs in both flat and hierarchical manners. Besides leveraging the power of MapReduce to the workflow level, our workflow composition framework is unique in that workflows are the only operands for composition; in this way, our approach elegantly solves the two-world problem of existing composition frameworks, in which composition needs to deal with both the world of tasks and the world of workflows. The proposed framework is implemented and a case study is conducted to validate our techniques.","Parallel programming,
Functional programming,
Acceleration,
Humans,
Web services,
Computer science,
Specification languages,
Process control,
Large-scale systems,
Distributed computing"
"Engineering future cyber-physical energy systems: Challenges, research needs, and roadmap","Cyber-physical energy systems require the integration of a heterogeneous physical layers and decision control networks, mediated by decentralized and distributed local sensing/actuation structures backed by an information layer. With the North American Electric Reliability Corporation (NERC) Critical Infrastructure Protection (CIP) [1] requirements and president's visions of more secure, reliable and controllable cyber-physical system, a new paradigm for modeling and research investigation is needed. In this paper, we present common challenges and our vision of solutions to design advanced Cyber-physical energy systems with embedded security and distributed control. Finally, we present a survey of our research results in this domain.","Power engineering and energy,
Control systems,
Communication system control,
Distributed control,
Process control,
Actuators,
Physical layer,
Optimal control,
Humans,
Intelligent sensors"
Monocular real-time 3D articulated hand pose estimation,"Markerless, vision based estimation of human hand pose over time is a prerequisite for a number of robotics applications, such as learning by demonstration (LbD), health monitoring, teleoperation, human-robot interaction. It has special interest in humanoid platforms, where the number of degrees of freedom makes conventional programming challenging. Our primary application is LbD in natural environments where the humanoid robot learns how to grasp and manipulate objects by observing a human performing a task. This paper presents a method for continuous vision based estimation of human hand pose. The method is non-parametric, performing a nearest neighbor search in a large database (100000 entries) of hand pose examples. The main contribution is a real time system, robust to partial occlusions and segmentation errors, that provides full hand pose recognition from markerless data. An additional contribution is the modeling of constraints based on temporal consistency in hand pose, without explicitly tracking the hand in the high dimensional pose space. The pose representation is rich enough to enable a descriptive human-to-robot mapping. Experiments show the pose estimation to be more robust and accurate than a non-parametric method without temporal constraints.","Humanoid robots,
Real time systems,
Robustness,
Human robot interaction,
Robot vision systems,
Fingers,
Computer vision,
Computer science,
Application software,
Computerized monitoring"
An eLearning Standard Approach for Supporting PBL in Computer Engineering,"Problem-based learning (PBL) has proved to be a highly successful pedagogical model in many fields, although it is not that common in computer engineering. PBL goes beyond the typical teaching methodology by promoting student interaction. This paper presents a PBL trial applied to a course in a computer engineering degree at the University of Seville, Spain. To promote the reusability and interoperability of the PBL trial, a design-driven approach was used, based on the Internet protocol Multimedia Subsystems (IMS) Learning Design (LD) standard. This paper presents the outcomes of using this method to innovate teaching practices in a blended learning environment. Design and implementation results, as well as users' opinions, are presented and analyzed.","Least squares approximation,
Education,
Electronic learning,
Unified modeling language,
Computers,
Probability density function,
Laboratories"
Open Wireless Positioning System: A Wi-Fi-Based Indoor Positioning System,"Wireless network positioning is the main pillar of the continuity of rich and mobile multimedia applications. Good position accuracy is particularly difficult to obtain in urban or leafy areas and indoors or in mixed (both indoor and outdoor) environments. A system proposing such positioning must localize any mobile terminal accurately within hostile environments and ideally be low-cost and easy to deploy. We propose an indoor positioning system, based on the IEEE 802.11 wireless network. This system, named OWLPS (Open WireLess Positioning System), implements several of the major mobile position computation algorithms and techniques: fingerprinting location, topology-based and viterbi-like algorithm, propagation models. These algorithms result from community work and our personal researches.","Mobile computing,
Multimedia systems,
Wireless networks,
Fingerprint recognition,
Urban areas,
Irrigation,
Computer science,
Laboratories,
Electronic mail,
Application software"
Wireless telemetry for electronic pill technology,"This work will address the challenges to facilitate the development of a high capacity radio system for a small, miniaturized electronic pill device that can be swallowable or implantable in human body in order to detect biological signals or capture images that could eventually be used for diagnostic and therapeutic purposes. In addition to reviewing and discussing the recent attempts in electronic pill technology, a wideband (UWB) telemetry system aimed for the development of an electronic pill will be presented in this paper. We have successfully realized more than half a meter UWB link under conditions emulating an implant","Telemetry,
Endoscopes,
Wideband,
Implants,
Biomedical monitoring,
Frequency,
Biomedical imaging,
Medical diagnostic imaging,
Computer displays,
Transmitters"
Indoor location tracking in non-line-of-sight environments using a IEEE 802.15.4a wireless network,Indoor location tracking of mobile robots or transport vehicles using wireless technology is attractive for many applications. IEEE 802.15.4a wireless networks offer an inexpensive facility for localizing mobile devices by time-based range measurements. The main problems of time-based range measurements in indoor environments are errors by multipath and non-line-of-sight (NLOS) signal propagation. This paper describes indoor tracking using range measurements and an Extended Kalman Filter with NLOS mitigation. The commercially available nanoLOC wireless network is utilized for range measurements. The paper presents experimental results of tracking a forklift truck in an industrial environment.,
A High-Performance Hybrid Computing Approach to Massive Contingency Analysis in the Power Grid,"Operating the electrical power grid to prevent power black-outs is a complex task. An important aspect of this is contingency analysis, which involves understanding and mitigating potential failures in power grid elements such as transmission lines. When taking into account the potential for multiple simultaneous failures (known as the N-x contingency problem), contingency analysis becomes a massively computational task. In this paper we describe a novel hybrid computational approach to contingency analysis. This approach exploits the unique graph processing performance of the Cray XMT in conjunction with a conventional massively parallel compute cluster to identify likely simultaneous failures that could cause widespread cascading power failures that have massive economic and social impact on society. The approach has the potential to provide the first practical and scalable solution to the N-x contingency problem. When deployed in power grid operations, it will increase the grid operator’s ability to deal effectively with outages and failures with power grid components while preserving stable and safe operation of the grid. The paper describes the architecture of our solution and presents preliminary performance results that validate the efficacy of our approach.","Grid computing,
Power grids,
Power systems,
Power generation,
Computer architecture,
Power system faults,
Power system protection,
Failure analysis,
Power generation economics,
Power transmission lines"
Fast ultrasound imaging simulation in K-space,"Most available ultrasound imaging simulation methods are based on the spatial impulse response approach. The execution speed of such a simulation is of the order of days for one heart-sized frame using desktop computers. For some applications, the accuracy of such rigorous simulation approaches is not necessary. This work outlines a much faster 3-D ultrasound imaging simulation approach that can be applied to tasks like simulating 3-D ultrasound images for speckle-tracking. The increased speed of the proposed simulation method is based primarily on the approximation that the point spread function is set to be spatially invariant, which is a reasonably good approximation when using polar coordinates for simulating images from phased arrays with constant aperture. Ultrasound images are found as the convolution of the PSF and an object of sparsely distributed scatterers. The scatterers are passed through an anti-aliasing filter before insertion into a regular beam-space grid to reduce the bandwidth and significantly reduce the amount of data. A comparison with the well-established simulation software package field II has been made. A simulation of a cyst image using the same input object was found to be in the order of 7000 times slower than the presented method. Following these considerations, the proposed simulation method can be a rapid and valuable tool for working with 3-D ultrasound imaging and in particular 3-D speckle-tracking.","Ultrasonic imaging,
Computational modeling,
Phased arrays,
Scattering,
Computer simulation,
Application software,
Apertures,
Convolution,
Band pass filters,
Bandwidth"
Accelerating SIFT on parallel architectures,SIFT is a widely-used algorithm that extracts features from images; using it to extract information from hundreds of terabytes of aerial and satellite photographs requires paral-lelization in order to be feasible. We explore accelerating an existing serial SIFT implementation with OpenMP parallelization and GPU execution.,
A WSN-based solution for precision farm purposes,"Driven by economic and environmental pressures, precision agriculture has brought many technological enhancements to traditional farm machinery and management tools. For a small tillage, manual control of inputs (such as water, fertilizer, pesticide) is still possible, but such an approach becomes unfeasible for larger cultivations. Furthermore, the manual control is based on the operator's opinion and often have no quantitative basis. In order to optimize the yield and the use of the available resources, wireless sensor networks can play a relevant role because of their ability of providing real-time data collected by spatially distributed sensors.","Wireless sensor networks,
Agriculture,
Sensor systems,
Condition monitoring,
Temperature sensors,
Human factors,
Production,
Global Positioning System,
Costs,
Sensor phenomena and characterization"
Multi-scale object detection by clustering lines,"Object detection in cluttered, natural scenes has a high complexity since many local observations compete for object hypotheses. Voting methods provide an efficient solution to this problem. When Hough voting is extended to location and scale, votes naturally become lines through scale space due to the local scale-location-ambiguity. In contrast to this, current voting methods stick to the location-only setting and cast point votes, which require local estimates of scale. Rather than searching for object hypotheses in the Hough accumulator, we propose a weighted, pairwise clustering of voting lines to obtain globally consistent hypotheses directly. In essence, we propose a hierarchical approach that is based on a sparse representation of object boundary shape. Clustering of voting lines (CVL) condenses the information from these edge points in few, globally consistent candidate hypotheses. A final verification stage concludes by refining the candidates. Experiments on the ETHZ shape dataset show that clustering voting lines significantly improves state-of-the-art Hough voting techniques.",Object detection
Spotting agreement and disagreement: A survey of nonverbal audiovisual cues and tools,"While detecting and interpreting temporal patterns of non-verbal behavioral cues in a given context is a natural and often unconscious process for humans, it remains a rather difficult task for computer systems. Nevertheless, it is an important one to achieve if the goal is to realise a naturalistic communication between humans and machines. Machines that are able to sense social attitudes like agreement and disagreement and respond to them in a meaningful way are likely to be welcomed by users due to the more natural, efficient and human-centered interaction they are bound to experience. This paper surveys the nonverbal cues that could be present during agreement and disagreement behavioural displays and lists a number of tools that could be useful in detecting them, as well as a few publicly available databases that could be used to train these tools for analysis of spontaneous, audiovisual instances of agreement and disagreement.","Humans,
Educational institutions,
Auditory displays,
Cameras,
Context,
Audio databases,
Face,
Arm,
Application software,
Biosensors"
Convergence to Equilibrium in Local Interaction Games,"We study a simple game theoretic model for the spread of an innovation in a network. The diffusion of the innovation is modeled as the dynamics of a coordination game in which the adoption of a common strategy between players has a higher payoff. Classical results in game theory provide a simple condition for an innovation to become widespread in the network. The present paper characterizes the rate of convergence as a function of graph structure. In particular, we derive a dichotomy between well-connected (e.g. random) graphs that show slow convergence and poorly connected, low dimensional graphs that show fast convergence.",
Microgenetic multiobjective reconfiguration algorithm considering power losses and reliability indices for medium voltage distribution network,"This study proposes and applies an evolutionary-based approach for multiobjective reconfiguration in electrical power distribution networks. In this model, two types of indicators of power quality are minimised: (i) power system's losses and (ii) reliability indices. Four types of reliability indices are considered. A microgenetic algorithm ('GA) is used to handle the reconfiguration problem as a multiobjective optimisation problem with competing and non-commensurable objectives. In this context, experiments have been conducted on two standard test systems and a real network. Such problems characterise typical distribution systems taking into consideration several factors associated with the practical operation of medium voltage electrical power networks. The results show the ability of the proposed approach to generate well-distributed Pareto optimal solutions to the multiobjective reconfiguration problem. In the systems adopted for assessment purposes, our proposed approach was able to find the entire Pareto front. Furthermore, better performance indexes were found in comparison to the Pareto envelope-based selection algorithm 2 (PESA 2) technique, which is another well-known multiobjective evolutionary algorithm available in the specialised literature. From a practical point of view, the results established, in general, that a compact trade-off region exists between the power losses and the reliability indices. This means that the proposed approach can recommend to the decision maker a small set of possible solutions in order to select from them the most suitable radial topology.","power supply quality,
genetic algorithms,
power distribution reliability"
The Future of Mobile Wireless Communication Networks,"The future of mobile wireless communication networks will be experienced several generations as which have been experienced. This kind of development will drive the researches of information technology in industrial area. In this paper, we predict the future generations of mobile wireless communication networks including 4th, 5th, 6th and 7th generations. The main objective of this paper is to propose a technical frame for industry in the future. Thus, this paper is focused on the specification of future generations of wireless mobile communication networks.",
Efficient retrieval of deformable shape classes using local self-similarities,"We present an efficient object retrieval system based on the identification of abstract deformable ‘shape’ classes using the self-similarity descriptor of Shechtman and Irani [13]. Given a user-specified query object, we retrieve other images which share a common ‘shape’ even if their appearance differs greatly in terms of colour, texture, edges and other common photometric properties. In order to use the self-similarity descriptor for efficient retrieval we make three contributions: (i) we sparsify the descriptor points by locating discriminative regions within each image, thus reducing the computational expense of shape matching; (ii) we extend [13] to enable matching despite changes in scale; and (iii) we show that vector quantizing the descriptor does not inhibit performance, thus providing the basis of a large-scale shape-based retrieval system using a bag-of-visual-words approach. Performance is demonstrated on the challenging ETHZ deformable shape dataset and a full episode from the television series Lost, and is shown to be superior to appearancebased approaches for matching non-rigid shape classes.","Shape,
Image retrieval,
Deformable models,
Photometry,
Large-scale systems,
TV,
Heart,
Pattern matching,
Information retrieval,
Cows"
Towards Autonomic Service Discovery A Survey and Comparison,"Service-oriented architecture has become the standard paradigm for software component integration. However, with the permanently increasing amount of available services and dynamic changes, the complexity of such service infrastructures, their maintenance, and consequently the expenditures spent for their operation increase equally. To deal with these effects, an improvement of service composition and discovery becomes necessary, especially a higher degree of automation. Following the idea of Autonomic Computing, which similarly aims at automating processes and workflows to a high degree, service composition and discovery have to proceed autonomously, which will on the one hand side reduce human involvement to a minimum, but on the other side require certain capabilities on the part of these mechanisms. For these purposes, in this paper we define prime criteria that have to be fulfilled for an autonomic service discovery. Based on that we present a comprehensive survey on existing service discovery approaches and evaluate to which extent they already fulfill these criteria. As a result, the paper reveals that there already exist some approaches that support or even fulfill a couple of the proposed criteria, which principally enables autonomic properties, but what is missing is an holistic approach focusing explicitly on providing autonomic properties.","Service oriented architecture,
Software standards,
Automation,
Humans,
Costs,
Computer science,
Computer architecture,
Application software,
Software architecture,
Web and internet services"
Ultra-low-power wearable biopotential sensor nodes,This paper discusses ultra-low-power wireless sensor nodes intended for wearable biopotential monitoring. Specific attention is given to mixed-signal design approaches and their impact on the overall system power dissipation. Examples of trade-offs in power dissipation between analog front-ends and digital signal processing are also given. It is shown how signal filtering can further reduce the internal power consumption of a node. Such power saving approaches are indispensable as real-life tests of custom wireless ECG patches reveal the need for artifact detection and correction. The power consumption of such additional features has to come from power savings elsewhere in the system as the overall power budget cannot increase.,"Biosensors,
Biomedical monitoring,
Power dissipation,
Energy consumption,
Wireless sensor networks,
Wearable sensors,
Digital signal processing,
Filtering,
Testing,
Electrocardiography"
Product of shifted exponential variates and outage capacity of multicarrier systems,"The probability density function and the cumulative distribution function of the product of shifted exponential variates are obtained in terms of the generalized upper incomplete Fox's H function. Using these new results, the exact outage capacity of multi carrier transmission through a slow Rayleigh fading channel is presented. Moreover, it is shown that analytical and simulation results are in perfect agreement.","Probability density function,
Fading,
Statistics,
Computer science education,
Educational products,
Cities and towns,
Distribution functions,
Analytical models,
Wireless communication,
Robustness"
Uncoverning Groups via Heterogeneous Interaction Analysis,"With the pervasive availability of Web 2.0 and social networking sites, people can interact with each other easily through various social media. For instance, popular sites like Del.icio.us, Flickr, and YouTube allow users to comment shared content (bookmark, photos, videos), and users can tag their own favorite content. Users can also connect to each other, and subscribe to or become a fan or a follower of others. These diverse individual activities result in a multi-dimensional network among actors, forming cross-dimension group structures with group members sharing certain similarities. It is challenging to effectively integrate the network information of multiple dimensions in order to discover cross-dimension group structures. In this work, we propose a two-phase strategy to identify the hidden structures shared across dimensions in multi-dimensional networks. We extract structural features from each dimension of the network via modularity analysis, and then integrate them all to find out a robust community structure among actors. Experiments on synthetic and real-world data validate the superiority of our strategy, enabling the analysis of collective behavior underneath diverse individual activities in a large scale.","Social network services,
YouTube,
Videos,
Humans,
Computer science,
Data engineering,
Data mining,
Large-scale systems,
Twitter,
Predictive models"
A diagnostic approach for electro-mechanical actuators in aerospace systems,"Electro-mechanical actuators (EMA) are finding increasing use in aerospace applications, especially with the trend towards all all-electric aircraft and spacecraft designs. However, electro-mechanical actuators still lack the knowledge base accumulated for other fielded actuator types, particularly with regard to fault detection and characterization. This paper presents a thorough analysis of some of the critical failure modes documented for EMAs and describes experiments conducted on detecting and isolating a subset of them. The list of failures has been prepared through an extensive Failure Modes and Criticality Analysis (FMECA) reference, literature review, and accessible industry experience. Methods for data acquisition and validation of algorithms on EMA test stands are described. A variety of condition indicators were developed that enabled detection, identification, and isolation among the various fault modes. A diagnostic algorithm based on an artificial neural network is shown to operate successfully using these condition indicators and furthermore, robustness of these diagnostic routines to sensor faults is demonstrated by showing their ability to distinguish between them and component failures. The paper concludes with a roadmap leading from this effort towards developing successful prognostic algorithms for electromechanical actuators.","Actuators,
Fault detection,
Failure analysis,
Aircraft,
Space vehicles,
Data acquisition,
Testing,
Fault diagnosis,
Artificial neural networks,
Robustness"
"Identity theft, computers and behavioral biometrics","The increase of online services, such as eBanks, WebMails, in which users are verified by a username and password, is increasingly exploited by Identity Theft procedures. Identity Theft is a fraud, in which someone pretends to be someone else is order to steal money or get other benefits. To overcome the problem of Identity Theft an additional security layer is required. Within the last decades the option of verifying users based on their keystroke dynamics was proposed during login verification. Thus, the imposter has to be able to type in a similar way to the real user in addition to having the username and password. However, verifying users upon login is not enough, since a logged station/mobile is vulnerable for imposters when the user leaves her machine. Thus, verifying users continuously based on their activities is required. Within the last decade there is a growing interest and use of biometrics tools, however, these are often costly and require additional hardware. Behavioral biometrics, in which users are verified, based on their keyboard and mouse activities, present potentially a good solution. In this paper we discuss the problem of Identity Theft and propose behavioral biometrics as a solution. We survey existing studies and list the challenges and propose solutions.","Computer crime,
Biometrics,
Mice,
Internet,
Terrorism,
Laboratories,
Hardware,
Keyboards,
Business,
Microcomputers"
Group-sensitive multiple kernel learning for object categorization,"In this paper, we propose a group-sensitive multiple kernel learning (GS-MKL) method to accommodate the intra-class diversity and the inter-class correlation for object categorization. By introducing an intermediate representation “group” between images and object categories, GS-MKL attempts to find appropriate kernel combination for each group to get a finer depiction of object categories. For each category, images within a group share a set of kernel weights while images from different groups may employ distinct sets of kernel weights. In GS-MKL, such group-sensitive kernel combinations together with the multi-kernels based classifier are optimized in a joint manner to seek a trade-off between capturing the diversity and keeping the invariance for each category. Extensive experiments show that our proposed GS-MKL method has achieved encouraging performance over three challenging datasets.","Kernel,
Bridges,
Robustness,
Support vector machines,
Support vector machine classification,
Learning systems,
Information processing,
Computers,
Laboratories,
Shape"
Switching cells and their implications for power electronic circuits,"This paper will introduce two basic switching cells, P-cell and N-cell, along with their implications and applications in power electronic circuits. The concept of switching cells in power electronic circuits started in the late 1970's. The basic cells presented in this paper have one switching element (transistor) and one diode. The P-cell is the mirror circuit of the N-cell and vice-versa, and this paper suggests that (1) most power electronic circuits can be analyzed and re-constructed using these basic switching cells, (2) single, dual, and 6-pack switching modules should be configured and laid-out according to the basic switching cells and not necessarily the conventional way used by industry, and (3) many benefits such as minimal parasitic inductance and dead-time elimination or minimization may come about. The present paper will describe the construction and operation of these basic switching cells, and it will also show a sequential method to reconstruct several classical dc-dc converters, a voltage source inverter (VSI), and a current source inverter (CSI) using these basic switching cells. In addition, the use of basic switching cells introduces some new topologies of dc-dc converters that originate from the buck, boost, and Ćuk converter for negative input voltages. This paper will also illustrate the experimental results of the new and existing topologies constructed from basic switching cells.","Switching circuits,
Power electronics,
DC-DC power converters,
Switching converters,
Voltage,
Inverters,
Topology,
Diodes,
Mirrors,
Circuit analysis"
Designing Intrusion Detection to Detect Black Hole and Selective Forwarding Attack in WSN Based on Local Information,"The wireless sensor networks (WSNs) are being deployed frequently in variety of environments such as military surveillance, forest fire monitoring, chemical leakage monitoring etc. Sensor nodes have limited communication capability and low computation resources so each node is weak entity that can be easily compromised by adversary by launching malicious software inside the network .Performance evaluation of wireless sensor network requires realistic modeling of Intrusion detection system since most of WSNs are application specific. In WSN, nodes have specific properties such as stable neighbors’ information that helps in detection of anomalies in network. Nodes monitor their neighborhood and collaborate with cluster head to detect malicious behavior. Even though nodes don’t have global view but they can still detect an intrusion with certain probability and report to cluster head. This paper introduces a specification based Intrusion Detection System for wireless sensor networks.","Intrusion detection,
Wireless sensor networks,
Monitoring,
Military computing,
Chemical sensors,
Surveillance,
Fires,
Computer networks,
Application software,
Collaboration"
Experimental Comparison of Bandwidth Estimation Tools for Wireless Mesh Networks,"Measurement of available bandwidth in a network has always been a topic of great interest. This knowledge can be applied to a wide variety of applications and can be instrumental in providing quality of service to end users. Several probe-based tools have been proposed to measure available bandwidth in wired networks. However, the performance of these tools in the realm of wireless networks has not been evaluated extensively. In recent years, there has also been some work on estimating bandwidth in wireless networks via passively monitoring the channel and determining the 'busy' and 'idle' periods. However, such techniques have primarily been evaluated via simulations only. In this work, we perform an extensive experimental comparison study of both passive and active bandwidth estimation tools for 802.11-based wireless mesh networks. We investigate the impact of interference, packet loss, and 802.11 rate-adaptation, on the performance of these tools. Our results indicate that for wireless networks, a passive technique provides much greater accuracy than the probe-based tools.","Bandwidth,
Wireless mesh networks,
Testing,
Interference,
Bit rate,
Wireless networks,
Quality of service,
Media Access Protocol,
Communications Society,
Computer science"
Accelerating leukocyte tracking using CUDA: A case study in leveraging manycore coprocessors,"The availability of easily programmable manycore CPUs and GPUs has motivated investigations into how to best exploit their tremendous computational power for scientific computing. Here we demonstrate how a systems biology application—detection and tracking of white blood cells in video microscopy—can be accelerated by 200× using a CUDA-capable GPU. Because the algorithms and implementation challenges are common to a wide range of applications, we discuss general techniques that allow programmers to make efficient use of a manycore GPU.","Acceleration,
White blood cells,
Coprocessors,
Application software,
Concurrent computing,
Programming profession,
Computer architecture,
Iterative algorithms,
Biology computing,
Systems biology"
PADD: Power Aware Domain Distribution,"Modern data centers usually have computing resources sized to handle expected peak demand, but average demand is generally much lower than peak. This means that the systems in the data center usually operate at very low utilization rates. Past techniques have exploited this fact to achieve significant power savings, but they generally focus on centrally managed, throughput-oriented systems that process a single fine-grained request stream. We propose a more general solution — a technique to save power by dynamically migrating virtual machines and packing them onto fewer physical machines when possible. We call our scheme Power-Aware Domain Distribution (PADD). In this paper, we report on simulation results for PADD and demonstrate that the power and performance changes from using PADD are primarily dependent on how much buffering or reserve capacity it maintains. Our adaptive buffering scheme achieves energy savings within 7% of the idealized system that has no performance penalty. Our results also show that we can achieve an energy savings up to 70% with fewer than 1% of the requests violating their service level agreements.","Virtual machining,
Distributed computing,
Computer science,
Laboratories,
Energy management,
Power system management,
Physics computing,
Transaction databases,
Logic,
Throughput"
Towards macro- and micro-expression spotting in video using strain patterns,"This paper presents a novel method for automatic spotting (temporal segmentation) of facial expressions in long videos comprising of continuous and changing expressions. The method utilizes the strain impacted on the facial skin due to the non-rigid motion caused during expressions. The strain magnitude is calculated using the central difference method over the robust and dense optical flow field of each subjects face. Testing has been done on 2 datasets (which includes 100 macro-expressions) and promising results have been obtained. The method is robust to several common drawbacks found in automatic facial expression segmentation including moderate in-plane and out-of-plane motion. Additionally, the method has also been modified to work with videos containing micro-expressions. Micro-expressions are detected utilizing their smaller spatial and temporal extent. A subject's face is divided in to sub-regions (mouth, cheeks, forehead, and eyes) and facial strain is calculated for each of these regions. Strain patterns in individual regions are used to identify subtle changes which facilitate the detection of micro-expressions.","Capacitive sensors,
Biomedical optical imaging,
Face recognition,
Nonlinear optics,
Robustness,
Image motion analysis,
Face detection,
Strain measurement,
Facial animation,
Video sequences"
On simultaneous shift- and capture-power reduction in linear decompressor-based test compression environment,"Growing test data volume and excessive test power consumption in scan-based testing are both serious concerns for the semiconductor industry. Various test data compression (TDC) schemes and low-power X-filling techniques were proposed to address the above problems. These methods, however, exploit the very same “don't-care” bits in the test cubes to achieve different objectives and hence may contradict to each other. In this work, we propose a generic framework for test power reduction in linear decompressor-based test compression environment, which is able to effectively reduce shift-and capture-power simultaneously. Experimental results on benchmark circuits demonstrate that our proposed techniques significantly outperform existing solutions.","Circuit testing,
Automatic testing,
Semiconductor device testing,
Test data compression,
Filling,
Integrated circuit testing,
Input variables,
Power dissipation,
Automatic test pattern generation,
Semiconductor device reliability"
An Improved CANNY Edge Detection Algorithm,"CANNY arithmetic operator has been proved to have good detective effect in the common usage of edge detection. However, CANNY operator also has certain deficiencies. Based on the analysis of the traditional CANNY algorithm, an improved canny algorithm is proposed in this paper. In the algorithm, self-adaptive filter is used to replace the Gaussian filter, morphological thinning is adopted to thin the edge and morphological operator is used to achieved the refining treatment of edge points detection and the single pixel level edge. The results of experiment show the improved CANNY algorithm is reasonable.","Image edge detection,
Gaussian noise,
Computer science,
Digital arithmetic,
Algorithm design and analysis,
Adaptive filters,
Shape,
Image segmentation,
Image registration,
Pattern recognition"
Alchemist: A Transparent Dependence Distance Profiling Infrastructure,"Effectively migrating sequential applications to take advantage of parallelism available on multicore platforms is a well-recognized challenge. This paper addresses important aspects of this issue by proposing a novel profiling technique to automatically detect available concurrency in C programs. The profiler, called Alchemist, operates completely transparently to applications, and identifies constructs at various levels of granularity (e.g., loops, procedures, and conditional statements) as candidates for asynchronous execution. Various dependences including read-after-write (RAW), write-after-read (WAR), and write-after-write (WAW), are detected between a construct and its continuation, the execution following the completion of the construct.  The time-ordered {\em distance} between program points forming a dependence gives a measure of the effectiveness of parallelizing that construct, as well as identifying the transformations necessary to facilitate such parallelization. Using the notion of post-dominance, our profiling algorithm builds an execution index tree at run-time. This tree is used to differentiate among multiple instances of the same static construct, and leads to improved accuracy in the computed profile, useful to better identify constructs that are amenable to parallelization. Performance results indicate that the profiles generated by Alchemist pinpoint strong candidates for parallelization, and can help significantly ease the burden of application migration to multicore environments.","Parallel processing,
Concurrent computing,
Runtime,
Multicore processing,
Programming profession,
Data mining,
Yarn,
Frequency estimation,
Concurrency control,
Computer science"
K-Means on Commodity GPUs with CUDA,"K-means algorithm is one of the most famous unsupervised clustering algorithms. Many theoretical improvements for the performance of original algorithms have been put forward, while almost all of them are based on Single Instruction Single Data(SISD) architecture processors (CPUs), which partly ignored the inherent paralleled characteristic of the algorithms. In this paper, a novel Single Instruction Multiple Data (SIMD) architecture processors (GPUs)based k-means algorithm is proposed. In this algorithm, in order to accelerate compute-intensive portions of traditional k-means, both data objects assignment and k centroids recalculation are offloaded to the GPU in parallel. We have implemented this GPU-based k-means on the newest generation GPU with Compute Unified Device Architecture(CUDA). The numerical experiments demonstrated that the speed of GPU-based k-means could reach as high as 40 times of the CPU-based k-means.","Hardware,
Yarn,
Clustering algorithms,
Computer architecture,
Kernel,
Concurrent computing,
Graphics,
Computer science,
Helium,
Acceleration"
Realization of open cloud computing federation based on mobile agent,"Although cloud computing is generally recognized as a technology which will has a significant impact on IT in the future. However, Cloud computing is still in its infancy, currently, there is not a standard available for it, portability and interoperability is also impossible between different Cloud Computing Service Providers, therefore, handicaps the widely deploy and quick development of cloud computing, there is still a long distance to the fine scenery which theoretically depicted by cloud computing. We analyze the problems in the current state of the art, put forward that Open Cloud Computing Federation is an inevitable approach for the widely use of cloud computing and to realize the greatest value of it. Accordingly, we proposal the MABOCCF (Mobile Agent Based Open Cloud Computing Federation) mechanism in this paper, it combines the advantages of Mobile Agent and cloud computing to provide a realization for the Open Cloud Computing Federation, MABOCCF can span over multiple heterogeneous Cloud Computing platforms and realizes portability and interoperability, it can be a beginning of open cloud computing federation and a future part of cloud computing. We also present in this paper the rationalities and the motivations for the combination of Mobile Agent and Cloud Computing, finally, a prototype is given with a performance analysis.","Cloud computing,
Mobile agents,
Computer networks,
Distributed computing,
Scalability,
Computer industry,
Availability,
Electric breakdown,
Information science,
Standards development"
Learning based digital matting,"We cast some new insights into solving the digital matting problem by treating it as a semi-supervised learning task in machine learning. A local learning based approach and a global learning based approach are then produced, to fit better the scribble based matting and the trimap based matting, respectively. Our approaches are easy to implement because only some simple matrix operations are needed. They are also extremely accurate because they can efficiently handle the nonlinear local color distributions by incorporating the kernel trick, that are beyond the ability of many previous works. Our approaches can outperform many recent matting methods, as shown by the theoretical analysis and comprehensive experiments. The new insights may also inspire several more works.","Machine learning,
Pixel,
Semisupervised learning,
Kernel,
Motion pictures,
Computer science,
Digital images,
Production,
Labeling,
Data mining"
LayerP2P: A New Data Scheduling Approach for Layered Streaming in Heterogeneous Networks,"Although layered streaming in heterogeneous peer-to-peer networks has drawn great interest in recent years, there's still a lack of systematical studies on its data scheduling issue. In this paper, we propose a new scheduling approach for layered video streaming, called LayerP2P. The key idea and main contributions of LayerP2P come in two-fold: 1) According to the characteristics caused by layered coding, we propose four objectives that should be achieved by data scheduling: high throughput, high layer delivery ratio, low useless packets ratio, and low subscription jitter; 2) We design a 3-stage scheduling mechanism to request absent blocks, where the min-cost flow model, probability decision mechanism and multi-window remedy mechanism are employed in Free Stage, Decision Stage and Remedy Stage, respectively. Each stage has different scheduling objective while collaborates with each other, to achieve the above four objectives. Experimental results indicate that our approach outperforms other schemes in simulation environment. Besides, LayerP2P is implemented in the PDEPS Project in China, which is expected to be the first practical layered streaming system for education in peer-to-peer networks.",
Analysis and optimization of fault-tolerant embedded systems with hardened processors,"In this paper we propose an approach to the design optimization of fault-tolerant hard real-time embedded systems, which combines hardware and software fault tolerance techniques. We trade-off between selective hardening in hardware and process re-execution in software to provide the required levels of fault tolerance against transient faults with the lowest-possible system costs. We propose a system failure probability (SFP) analysis that connects the hardening level with the maximum number of re-executions in software. We present design optimization heuristics, to select the fault-tolerant architecture and decide process mapping such that the system cost is minimized, deadlines are satisfied, and the reliability requirements are fulfilled.","Fault tolerant systems,
Embedded system,
Circuit faults,
Costs,
Radiation hardening,
Hardware,
Fault tolerance,
Real time systems,
Error analysis,
Design optimization"
Exploiting uncertainty in random sample consensus,"In this work, we present a technique for robust estimation, which by explicitly incorporating the inherent uncertainty of the estimation procedure, results in a more efficient robust estimation algorithm. In addition, we build on recent work in randomized model verification, and use this to characterize the ‘non-randomness’ of a solution. The combination of these two strategies results in a robust estimation procedure that provides a significant speed-up over existing RANSAC techniques, while requiring no prior information to guide the sampling process. In particular, our algorithm requires, on average, 3–10 times fewer samples than standard RANSAC, which is in close agreement with theoretical predictions. The efficiency of the algorithm is demonstrated on a selection of geometric estimation problems.","Uncertainty,
Robustness,
Sampling methods,
Computer science,
Computer vision,
Solid modeling,
Runtime,
Noise generators,
Resumes,
Application software"
The Crosspoint-Queued Switch,"This paper calls for rethinking packet-switch architectures by cutting all dependencies between the switch fabric and the linecards. Most single-stage packet-switch architectures rely on an instantaneous communication between the switch fabric and the linecards. Today, however, this assumption is breaking down, because effective propagation times are too high and keep increasing with the line rates. In this paper, we argue for a self-sufficient switch fabric by moving all the buffering from the linecards to the switch fabric. We introduce the crosspoint-queued (CQ) switch, a new buffered-crossbar switch architecture with large crosspoint buffers and no input queues, and show how it can be readily implemented in a single SRAM-based chip using current technology. For a crosspoint buffer size of one, we provide a closed-form throughput formula for all work-conserving schedules under uniform Bernoulli i.i.d. arrivals. Furthermore, we study the performance of the switch for larger buffer sizes and show that it nearly behaves as an ideal output-queued switch. Finally, we confirm our results using synthetic as well as trace-based simulations.","Switches,
Communication switching,
Packet switching,
Fabrics,
Computer architecture,
Optical propagation,
Communications Society,
Computer science,
Throughput,
Internet"
Dynamic policy-based IDS configuration,"Intrusion Detection System (IDS) is an important security enforcement tool in modern networked information systems. Obtaining an optimal IDS configuration for effective detection of attacks is far from trivial. There exists a tradeoff between security enforcement levels and the performance of information systems. It is critical to configure an IDS in a dynamic and iterative fashion to balance the security overhead and system performance. In this paper, we use noncooperative game approaches to address this problem. We first build a fundamental game framework to model the zero-sum interactions between the detector and the attacker. Building on this platform, we then formulate a stochastic game model in which the transitions between system states are determined by the actions chosen by both players. An optimal policy-based configuration can be found by minimizing a discounted cost criterion, using an iterative method. In addition, we propose a Q-learning algorithm to find the optimal game values when the transitions between system states are unknown. We show the convergence of the algorithm to the optimal Q-function and illustrate the concepts by simulation.","Intrusion detection,
Information security,
Information systems,
Iterative algorithms,
System performance,
Detectors,
Stochastic systems,
Cost function,
Iterative methods,
Convergence"
Accelerated Nonrigid Intensity-Based Image Registration Using Importance Sampling,"Nonrigid image registration methods using intensity-based similarity metrics are becoming increasingly common tools to estimate many types of deformations. Nonrigid warps can be very flexible with a large number of parameters and gradient optimization schemes are widely used to estimate them. However, for large datasets, the computation of the gradient of the similarity metric with respect to these many parameters becomes very time consuming. Using a small random subset of image voxels to approximate the gradient can reduce computation time. This work focuses on the use of importance sampling to reduce the variance of this gradient approximation. The proposed importance sampling framework is based on an edge-dependent adaptive sampling distribution designed for use with intensity-based registration algorithms. We compare the performance of registration based on stochastic approximations with and without importance sampling to that using deterministic gradient descent. Empirical results, on simulated magnetic resonance brain data and real computed tomography inhale-exhale lung data from eight subjects, show that a combination of stochastic approximation methods and importance sampling accelerates the registration process while preserving accuracy.","Acceleration,
Image registration,
Monte Carlo methods,
Image sampling,
Algorithm design and analysis,
Stochastic processes,
Brain modeling,
Computational modeling,
Magnetic resonance,
Computed tomography"
Applications of Wireless Sensor Networks and RFID in a Smart Home Environment,"With the aging population and increased need to care for the elderly there are fewer of the younger generation to administer the necessary care and supervision. This condition is one of the reasons many researchers devote their time in evolving smart homes. These homes offer the occupant(s) a level of convenience not seen in traditional homes by using technology to create an environment that is aware of the activities taking place within it. The focus of this paper is on the integration of radio frequency identification (RFID) and wireless sensor network (WSN) in smart homes and applications of this system such as identifying a caregiver who enters the home. In the following work we present an architecture consisting of RFID, a WSN to identify motion within an environment and who is moving as well as several useful applications which take advantage of this information.","Wireless sensor networks,
Radiofrequency identification,
Smart homes,
Senior citizens,
Communication networks,
RFID tags,
Application software,
Computer science,
Aging,
Signal processing"
Strong Barrier Coverage with Directional Sensors,"The barrier coverage model was proposed for applications in which sensors are deployed for intrusion detection. In this paper, we study a strong barrier coverage problem in wireless sensor networks with directional sensors. First, we introduce the directional coverage graph to model barrier coverage with directional sensors. Based on this graph model, we present an integer linear programming formulation for the barrier coverage problem, which can be used to provide optimal solutions. Moreover, we present efficient centralized algorithms and a distributed algorithm to solve the problem. It has been shown by simulation results that the proposed algorithms provide close-to-optimal solutions and consistently outperform a simple greedy algorithm.","Wireless sensor networks,
Infrared image sensors,
Intrusion detection,
Distributed algorithms,
Belts,
Computer science,
Integer linear programming,
Greedy algorithms,
Infrared sensors,
Algorithm design and analysis"
Monitoring connectivity in wireless sensor networks,"It is important to have continuous connectivity in a wireless sensor network after it is deployed in a hostile environment. However, such networks are constrained by the low user-to-node ratio, limited energy and bandwidth resources, entities that are usually mobile, networks without fixed infrastructure and frequent failure due to problems of energy, vulnerability to attack, etc. To address these difficulties, there is a need for wireless sensor networks to be self-organizing and self-configuring so as to improve performance, increase energy efficiency, save resources and reduce data transmission. In this paper, we present a method for monitoring, maintaining and repairing the communication network of a dynamic mobile wireless sensor network, so that network connectivity is continuously available and provides fault tolerance. Specifically, we propose an algorithm for the detection and surveillance of articulation points in graph connectivity, including an algorithm for network auto-organization in the event that this occurs.","Monitoring,
Wireless sensor networks"
Exploring the optimal chunk selection policy for data-driven P2P streaming systems,"Data-driven P2P streaming systems can potentially provide good playback rate to a large number of viewers. One important design problem in such P2P systems is to determine the optimal chunk selection policy that provides high continuity playback under the server's upload capacity constraint. We present a general and unified mathematical framework to analyze a large class of chunk selection policies. The analytical framework is asymptotically exact when the number of viewers is large. More importantly, we provide some interesting observations on the optimal chunk selection policy: it is of shaped and becomesmore greedy as the upload capacity of the server increases. This insight helps content providers to deploy large scale streaming systems with a QoS-guarantee under a given cost constraint.","Streaming media,
Costs,
Web server,
Scalability,
Bridges,
Computer science,
Large-scale systems,
Web and internet services,
Quality of service,
File servers"
Efficient resonant drive of flapping-wing robots,"Flapping-wing air vehicles can improve efficiency by running at resonance to reduce inertial costs of accelerating and decelerating the wings. For battery-powered, DC motor-driven systems with gears and cranks, the drive torque and velocity is a complicated function of battery voltage. Hence, resonant behavior is not as well defined as for flapping-wing systems with elastic actuators. In this paper, we analyze a resonant drive to reduce average battery power consumption for DC motor-driven flapping-wing robots. We derive a nondimensionalized analysis of the generic class of a motor-driven slider crank, considering motor and battery resistance. This analysis is used to demonstrate the benefits of efficient resonant drive on a 5.8g flapping-wing robot and experiments showed a 30% average power reduction by integrating a tuned compliant element.","Resonance,
Robots,
Batteries,
Vehicles,
Costs,
Acceleration,
Gears,
Torque,
Voltage,
Actuators"
High level activity recognition using low resolution wearable vision,"This paper presents a system aimed to serve as the enabling platform for a wearable assistant. The method observes manipulations from a wearable camera and classifies activities from roughly stabilized low resolution images (160 × 120 pixels) with the help of a 3-level Dynamic Bayesian Network and adapted temporal templates. Our motivation is to explore robust but computationally inexpensive visual methods to perform as much activity inference as possible without resorting to more complex object or hand detectors. The description of the method and results obtained are presented, as well as the motivation for further work in the area of wearable visual sensing.","Cameras,
Wearable sensors,
Bayesian methods,
Manipulator dynamics,
Robustness,
Sensor arrays,
Image resolution,
Pixel,
Object detection,
Wearable computers"
ABySS-Explorer: Visualizing Genome Sequence Assemblies,"One bottleneck in large-scale genome sequencing projects is reconstructing the full genome sequence from the short subsequences produced by current technologies. The final stages of the genome assembly process inevitably require manual inspection of data inconsistencies and could be greatly aided by visualization. This paper presents our design decisions in translating key data features identified through discussions with analysts into a concise visual encoding. Current visualization tools in this domain focus on local sequence errors making high-level inspection of the assembly difficult if not impossible. We present a novel interactive graph display, ABySS-Explorer, that emphasizes the global assembly structure while also integrating salient data features such as sequence length. Our tool replaces manual and in some cases pen-and-paper based analysis tasks, and we discuss how user feedback was incorporated into iterative design refinements. Finally, we touch on applications of this representation not initially considered in our design phase, suggesting the generality of this encoding for DNA sequence data.","Genomics,
Bioinformatics,
Assembly,
Sequences,
Data visualization,
Inspection,
Encoding,
Large-scale systems,
Displays,
Feedback"
Automatic Role Recognition in Multiparty Recordings: Using Social Affiliation Networks for Feature Extraction,"Automatic analysis of social interactions attracts increasing attention in the multimedia community. This letter considers one of the most important aspects of the problem, namely the roles played by individuals interacting in different settings. In particular, this work proposes an automatic approach for the recognition of roles in both production environment contexts (e.g., news and talk-shows) and spontaneous situations (e.g., meetings). The experiments are performed over roughly 90 h of material (one of the largest databases used for role recognition in the literature) and show that the recognition effectiveness depends on how much the roles influence the behavior of people. Furthermore, this work proposes the first approach for modeling mutual dependences between roles and assesses its effect on role recognition performance.","Feature extraction,
Production,
Databases,
Social network services,
Pervasive computing,
TV,
Motion pictures,
Information management,
Gaussian distribution"
Finding comparable temporal categorical records: A similarity measure with an interactive visualization,"An increasing number of temporal categorical databases are being collected: Electronic Health Records in healthcare organizations, traffic incident logs in transportation systems, or student records in universities. Finding similar records within these large databases requires effective similarity measures that capture the searcher's intent. Many similarity measures exist for numerical time series, but temporal categorical records are different. We propose a temporal categorical similarity measure, the M&M (Match & Mismatch) measure, which is based on the concept of aligning records by sentinel events, then matching events between the target and the compared records. The M&M measure combines the time differences between pairs of events and the number of mismatches. To accom-modate customization of parameters in the M&M measure and results interpretation, we implemented Similan, an interactive search and visualization tool for temporal categorical records. A usability study with 8 participants demonstrated that Similan was easy to learn and enabled them to find similar records, but users had difficulty understanding the M&M measure. The usability study feedback, led to an improved version with a continuous timeline, which was tested in a pilot study with 5 participants.","Visualization,
Visual databases,
Time measurement,
Usability,
Medical services,
Transportation,
Educational institutions,
Particle measurements,
Feedback,
Testing"
Modeling and dynamic management of 3D multicore systems with liquid cooling,"Three-dimensional (3D) circuits reduce communication delay in multicore SoCs, and enable efficient integration of cores, memories, sensors, and RF devices. However, vertical integration of layers exacerbates the reliability and thermal problems, and cooling efficiency becomes a limiting factor. Liquid cooling is a solution to overcome the accelerated thermal problems imposed by multi-layer architectures. In this paper, we first provide a 3D thermal simulation model including liquid cooling, supporting both fixed and variable fluid injection rates. Our model has been integrated in HotSpot to study the impact on multicore SoCs. We design and evaluate several dynamic management policies that complement liquid cooling. Our results for 3D multicore SoCs, which are based on 3D versions of UltraSPARC T1, show that thermal management approaches that combine liquid cooling with proactive task allocation are extremely effective in preventing temperature problems. Our proactive management technique provides an additional 75% average reduction in hot spots in comparison to applying only liquid cooling. Furthermore, for systems capable of varying the coolant flow rate at runtime, our feedback controller increases the improvement to 95% on average.","Three dimensional displays,
Liquid cooling,
Thermal management,
Microchannel,
Through-silicon vias,
Thermal resistance"
Performance assessment of DMOEA-DD with CEC 2009 MOEA competition test instances,"In this paper, the DMOEA-DD, which is an improvement of DMOEA[1, 2] by using domain decomposition technique, is applied to tackle the CEC 2009 MOEA competition test instances that are multiobjective optimization problems (MOPs) with complicated Pareto set (PS) geometry shapes. The performance assessment is given by using IGD [3, 4] as performance metric.","Testing,
Evolutionary computation,
Thermodynamics,
Shape,
Entropy,
Pareto optimization,
Computational geometry,
Measurement,
Genetics,
Temperature distribution"
Volterrafaces: Discriminant analysis using Volterra kernels,"In this paper we present a novel face classification system where we represent face images as a spatial arrangement of image patches, and seek a smooth nonlinear functional mapping for the corresponding patches such that in the range space, patches of the same face are close to one another, while patches from different faces are far apart, in L2 sense. We accomplish this using Volterra kernels, which can generate successively better approximations to any smooth nonlinear functional. During learning, for each set of corresponding patches we recover a Volterra kernel by minimizing a goodness functional defined over the range space of the sought functional. We show that for our definition of the goodness functional, which minimizes the ratio between intraclass distances and interclass distances, the problem of generating Volterra approximations, to any order, can be posed as a generalized eigenvalue problem. During testing, each patch from the test image that is classified independently, casts a vote towards image classification and the class with the maximum votes is chosen as the winner. We demonstrate the effectiveness of the proposed technique in recognizing faces by extensive experiments on Yale, CMU PIE and Extended Yale B benchmark face datasets and show that our technique consistently outperforms the state-of-the-art in learning based face discrimination.","Kernel,
Tensile stress,
Face recognition,
Testing,
Voting,
Image recognition,
Lighting,
Linear discriminant analysis,
Image analysis,
Information analysis"
Competition of wireless providers for atomic users: Equilibrium and social optimality,"We study a problem where wireless service providers compete for heterogenous and atomic (non-infinitesimal) wireless users. The users differ in their utility functions as well as in the perceived quality of service of individual providers. We model the interaction of an arbitrary number of providers and users as a two-stage multi-leader-follower game, and prove existence and uniqueness of the subgame perfect Nash equilibrium for a generic channel model and a wide class of users' utility functions. We show that, interestingly, the competition of resource providers leads to a globally optimal outcome under fairly general technical conditions. Our results show that some users need to purchase their resource from several providers at the equilibrium. While the number of such users is typically small (smaller than the number of providers), our simulations indicate that the percentage of cases where at least one undecided user exists can be significant.","Nash equilibrium,
Switches,
Wireless networks,
Quality of service,
Communication industry,
Telecommunication switching,
Frequency,
Power generation economics,
Base stations,
Resource management"
Load Balancing in IEEE 802.11 Networks,"Because wireless stations independently select which access points to camp on, the total wireless station traffic on all available IEEE 802.11 network APs might be unevenly distributed. This load-balancing problem can lead to overloading and network congestion. This survey examines the problem, along with state-of-the-art network- and wireless-station-based solutions. It also presents experimental results using off-the-shelf IEEE 802.11 devices. As the results show, effectively balancing AP traffic loads can increase overall system throughputs.","Load management,
Telecommunication traffic,
Throughput,
Probes,
Space technology,
Bandwidth,
Broadcasting,
Protocols,
Local area networks,
IP networks"
Linear Suffix Array Construction by Almost Pure Induced-Sorting,"We present a linear time and space suffix array (SA) construction algorithm called the SA-IS algorithm.The SA-IS algorithm is novel because of the LMS-substrings used for the problem reduction and the pure induced-sorting (specially coined for this algorithm)used to propagate the order of suffixes as well as that of LMS-substrings, which makes the algorithm almost purely relying on induced sorting at both its crucial steps.The pure induced-sorting renders the algorithm an elegant design and in turn a surprisingly compact implementation which consists of less than 100 lines of C code.The experimental results demonstrate that this newly proposed algorithm yields noticeably better time and space efficiencies than all the currently published linear time algorithms for SA construction.","Sorting,
Educational institutions,
Algorithm design and analysis,
Data compression,
Computer science,
Sun,
Mathematics,
Data structures,
Indexing,
Information retrieval"
Live Analysis: Progress and Challenges,"As computer technologies become increasingly ubiquitous, so must supporting digital forensics tools and techniques for efficiently and effectively analyzing associated systems' behavior. Live analysis is a logical and challenging step forward in this area and a method that has recently received increased R&D focus. This article describes some live analysis approaches as well as tools and techniques for live analysis on real and virtual machines. The discussion includes research challenges and open problems.","Cryptography,
Image analysis,
Digital forensics,
Security,
Force control,
Control systems,
File systems,
Legal factors,
Law enforcement,
Containers"
Efficient discriminative learning of parts-based models,"Supervised learning of a parts-based model can be formulated as an optimization problem with a large (exponential in the number of parts) set of constraints. We show how this seemingly difficult problem can be solved by (i) reducing it to an equivalent convex problem with a small, polynomial number of constraints (taking advantage of the fact that the model is tree-structured and the potentials have a special form); and (ii) obtaining the globally optimal model using an efficient dual decomposition strategy. Each component of the dual decomposition is solved by a modified version of the highly optimized SVM-Light algorithm. To demonstrate the effectiveness of our approach, we learn human upper body models using two challenging, publicly available datasets. Our model accounts for the articulation of humans as well as the occlusion of parts. We compare our method with a baseline iterative strategy as well as a state of the art algorithm and show significant efficiency improvements.","Layout,
Lighting,
Least squares methods,
Light sources,
Least squares approximation,
Automation,
Educational institutions,
Information science,
Geometry,
Jacobian matrices"
Use of Normal Tissue Context in Computer-Aided Detection of Masses in Mammograms,"When reading mammograms, radiologists do not only look at local properties of suspicious regions but also take into account more general contextual information. This suggests that context may be used to improve the performance of computer-aided detection (CAD) of malignant masses in mammograms. In this study, we developed a set of context features that represent suspiciousness of normal tissue in the same case. For each candidate mass region, three normal reference areas were defined in the image at hand. Corresponding areas were also defined in the contralateral image and in different projections. Evaluation of the context features was done using 10-fold cross validation and case based bootstrapping. Free response receiver operating characteristic (FROC) curves were computed for feature sets including context features and a feature set without context. Results show that the mean sensitivity in the interval of 0.05-0.5 false positives/image increased more than 6% when context features were added. This increase was significant (p < 0.0001). Context computed using multiple views yielded a better performance than using a single view (mean sensitivity increase of 2.9%, p < 0.0001). Besides the importance of using multiple views, results show that best CAD performance was obtained when multiple context features were combined that are based on different reference areas in the mammogram.","Cancer detection,
Image segmentation,
Breast cancer,
Radiology,
Biomedical imaging,
Mammography,
Feature extraction,
Probability,
Data mining"
Design of a mobile video streaming system using adaptive spatial resolution control,"An efficient mobile video streaming system needs to cope with unstable network bandwidth and limited battery life. We propose a novel streaming system which jointly considers picture quality, bit-rate and energy consumption. Reducing the spatial resolution of a video stream adaptively proves to be more efficient in terms of picture quality and energy consumption than conventional rate control using only adjustment of quantization parameter. We apply the same scheme to scalable coding for large-scale mobile video streaming. This extends the adaptation of an SVC stream to lower bit-rates, while maintaining temporal stability. Our approach has been shown to improve picture quality by approximately 0.5 dB in low bit-rate conditions, and also reduces energy consumption by more than 50% compared to conventional video streaming.",
Energy efficient multiprocessor task scheduling under input-dependent variation,"In this paper, we propose a novel, energy aware scheduling algorithm for applications running on DVS-enabled multiprocessor systems, which exploits variation in execution times of individual tasks. In particular, our algorithm takes into account latency and resource constraints, precedence constraints among tasks and input-dependent variation in execution times of tasks to produce a scheduling solution and voltage assignment such that the average energy consumption is minimized. Our algorithm is based on a mathematical programming formulation of the scheduling and voltage assignment problem and runs in polynomial time. Experiments with randomly generated task graphs show that up to 30% savings in energy can be obtained by using our algorithm over existing techniques. We perform experiments on two real-world applications - MPEG-4 decoder and MJPEG encoder. Simulations show that the scheduling solution generated by our algorithm can provide up to 25% reduction in energy consumption over greedy dynamic slack reclamation algorithms.","Energy efficiency,
Scheduling algorithm,
Voltage,
Energy consumption,
Multiprocessing systems,
Delay,
Mathematical programming,
Polynomials,
MPEG 4 Standard,
Decoding"
A New Buck-boost Type Battery Equalizer,"A new technique based on buck-boost topology is proposed to equalize a series-connected battery stack in this paper. The proposed scheme transfers the energy from fully charged battery cell to the weakest charged battery using buck-boost operation. This operation maintains batteries at the same charge and voltage level. Unlike previous battery equalizing schemes, the new battery equalizer uses only one magnetic component, resulting low cost and small size. Experiment results are provided to verify the operation of the proposed battery equalizer.","Batteries,
Equalizers,
Switches,
Shunt (electrical),
Voltage,
Circuits,
DC-DC power converters,
Inductors,
Costs,
Hybrid electric vehicles"
Performance evaluation of efficient and reliable routing protocols for fixed-power sensor networks,"Fixed-power wireless sensor networks are prevalent and cost-effective. However, they face mote failures, RF interference from environmental noise and energy constraints. Routing protocols for such networks must overcome these problems to achieve reliability, energy efficiency and scalability in message delivery. Achievement of these requirements, however, poses conflicting demands. In this paper, we propose an efficient and reliable routing protocol (EAR) that achieves reliable and scalable performance with minimal compromise of energy efficiency. The routing design of EAR is based on four parameters - expected path length and a weighted combination of distance traversed, energy levels and link transmission success history, to dynamically determine and maintain the best routes. Simulation experiments of EAR with four existing protocols demonstrate that a design based on a combination of routing parameters exhibits collectively better performance than protocols based on just hop-count and energy or those using flooding.","Routing protocols,
Ear,
Energy efficiency,
Wireless sensor networks,
Electromagnetic interference,
Working environment noise,
Interference constraints,
Scalability,
Energy states,
History"
Evaluating the use of GPUs in liver image segmentation and HMMER database searches,"In this paper we present the results of parallelizing two life sciences applications, Markov random fields-based (MRF) liver segmentation and HMMER's Viterbi algorithm, using GPUs. We relate our experiences in porting both applications to the GPU as well as the techniques and optimizations that are most beneficial. The unique characteristics of both algorithms are demonstrated by implementations on an NVIDIA 8800 GTX Ultra using the CUDA programming environment. We test multiple enhancements in our GPU kernels in order to demonstrate the effectiveness of each strategy. Our optimized MRF kernel achieves over 130× speedup, and our hmmsearch implementation achieves up to 38× speedup. We show that the differences in speedup between MRF and hmmsearch is due primarily to the frequency at which the hmmsearch must read from the GPU's DRAM.","Liver,
Image segmentation,
Hidden Markov models,
Image databases,
Kernel,
Viterbi algorithm,
Programming environments,
Testing,
Frequency,
Random access memory"
How Well Do Test Case Prioritization Techniques Support Statistical Fault Localization,"In continuous integration, a tight integration of test case prioritization techniques and fault-localization techniques may both expose failures faster and locate faults more effectively. Statistical fault-localization techniques use the execution information collected during testing to locate faults. Executing a small fraction of a prioritized test suite reduces the cost of testing, and yet the subsequent fault localization may suffer. This paper presents the first empirical study to examine the impact of test case prioritization on the effectiveness of fault localization. Among many interesting empirical results, we find that coverage-based and random techniques can be more effective than distribution-based techniques in supporting statistical fault localization.","Software testing,
Application software,
Costs,
Software debugging,
Computer applications,
Resumes,
Australia Council,
Computer science,
Feedback"
PDA-based mobile robot system with remote monitoring for home environment,"This paper proposes a home automation system using a PDA(Personal Digital Assistants)-based intelligent robot system architecture which consists of three layers; a user layer, a manager layer, and an action layer. In the user layer, users manage and control the robot, and get visual information via the remote monitoring system. In the case of showing maps with the status of the robot, synchronization is very important. In the manager layer, there are three parts; a server part, a home appliances part, and a storage part. The server part manages all the status information of the house, the home appliances part controls all appliances, and the storage system stores all the status information of the house. In the action layer, the main robot system uses a PDA instead of a computer. As a PDA has limited performance, simple algorithms are required. It has an intelligent functional engine for SLAM(Simultaneous Localization And Mapping) and vision processing. We have developed the PDA-based mobile robot system for home automation based on this architecture in order to verify its efficiency.","Mobile robots,
Remote monitoring,
Home appliances,
Personal digital assistants,
Home automation,
Intelligent robots,
Control systems,
Robotics and automation,
Automatic control,
Robot control"
Powers of 10: Modeling Complex Information-Seeking Systems at Multiple Scales,"New models of information-seeking support systems offer two advantages: They move us from prescientific conceptual frameworks about information seeking to more rigorous scientific theories and predictive models while, at the same time, expanding the kinds of things we study and develop.","Power system modeling,
Information retrieval,
Predictive models,
Problem-solving,
Information analysis,
Shape,
Search engines,
Libraries,
Navigation,
System testing"
Transmission capacity of ad-hoc networks with multiple antennas using transmit stream adaptation and interference cancelation,"The transmission capacity of an ad-hoc network is the maximum density of active transmitters in an unit area, given an outage constraint at each receiver for a fixed rate of transmission. Assuming channel state information is available at the receiver, this paper presents bounds on the transmission capacity as a function of the number of antennas used for transmission, and the spatial receive degrees of freedom used for interference cancelation at the receiver. Canceling the strongest interferers, using a single antenna for transmission together with using all but one spatial receive degrees of freedom for interference cancelation is shown to maximize the transmission capacity. Canceling the closest interferers, using a single antenna for transmission together with using a fraction of the total spatial receive degrees of freedom for interference cancelation depending on the path loss exponent, is shown to maximize the transmission capacity.","Ad hoc networks,
Adaptive arrays,
Transmitting antennas,
Interference cancellation,
Receiving antennas,
Transmitters,
Interference constraints,
Channel state information,
Propagation losses,
Wireless networks"
Improving Social Recommender Systems,"Recommender systems play a significant role in reducing information overload for people visiting online sites, but their accuracy could be improved by using data from online social networks and electronic communication tools.","Recommender systems,
Collaborative work,
Filtering,
Laboratories,
Social network services,
Application specific integrated circuits,
MySpace,
Business,
Psychology,
Motion pictures"
Memory Efficient Protocols for Detecting Node replication attacks in wireless sensor networks,"Sensor networks deployed in hostile areas are subject to node replication attacks, in which an adversary compromises a few sensors, extracts the security keys, and clones them in a large number of replicas, which are introduced into the network to perform insider attacks. Memory overhead, energy efficiency and detection probability are the main technical concerns for any replication detection protocol. The previous distributed solutions either require network-wide spontaneous change of pseudo-random numbers or incur significant memory and energy overhead to the sensors, especially in the central area of the deployment. In this paper, we propose four replication detection protocols that have high detection probability, low memory requirement, and balanced energy consumption. The new protocols use Bloom filters to compress the information stored at the sensors, and use two new techniques, called cell forwarding and cross forwarding, to improve detection probability, further reduce memory consumption, and in the mean time distribute the memory and energy overhead evenly across the whole network. Simulations show that the protocols can achieve nearly 100% detection probability with average memory reduction up to 91%.","Wireless application protocol,
Wireless sensor networks,
Authentication,
Filters,
Energy efficiency,
Read-write memory,
Random access memory,
Satellite broadcasting,
Energy consumption,
Information filtering"
Identifying High Cardinality Internet Hosts,"The Internet host cardinality, defined as the number of distinct peers that an Internet host communicates with, is an important metric for profiling Internet hosts. Some example applications include behavior based network intrusion detection, p2p hosts identification, and server identification. However, due to the tremendous number of hosts in the Internet and high speed links, tracking the exact cardinality of each host is not feasible due to the limited memory and computation resource. Existing approaches on host cardinality counting have primarily focused on hosts of extremely high cardinalities. These methods do not work well with hosts of moderately large cardinalities that are needed for certain host behavior profiling such as detection of p2p hosts or port scanners. In this paper, we propose an online sampling approach for identifying hosts whose cardinality exceeds some moderate prescribed threshold, e.g. 50, or within specific ranges. The main advantage of our approach is that it can filter out the majority of low cardinality hosts while preserving the hosts of interest, and hence minimize the memory resources wasted by tracking irrelevant hosts. Our approach consists of three components: 1) two-phase filtering for eliminating low cardinality hosts, 2) thresholded bitmap for counting cardinalities, and 3) bias correction. Through both theoretical analysis and experiments using real Internet traces, we demonstrate that our approach requires much less memory than existing approaches do whereas yields more accurate estimates.","Internet,
Sampling methods,
Filtering,
Telecommunication traffic,
Intrusion detection,
Statistics,
Communications Society,
Computer science,
Application software,
Network servers"
Improved Time Series Reconstruction for Dynamic Magnetic Resonance Imaging,"Time series of in vivo magnetic resonance images exhibit high levels of temporal correlation. Higher temporal resolution reconstructions are obtained by acquiring data at a fraction of the Nyquist rate and resolving the resulting aliasing using the correlation information. The dynamic imaging experiment is modeled as a linear dynamical system. A Kalman filter based unaliasing reconstruction is described for accelerated dynamic magnetic resonance imaging (MRI). The algorithm handles arbitrary readout trajectories naturally. The reconstruction is causal and very fast, making it applicable to real-time imaging. In vivo results are presented for cardiac MRI of healthy volunteers.","Image reconstruction,
Magnetic resonance imaging,
Image sampling,
In vivo,
Data acquisition,
Spatial resolution,
Heart,
Magnetic separation,
Acceleration,
Image resolution"
Image encryption algorithm based on Henon chaotic system,"In this paper, a new image encryption algorithm is presented based on Henon chaotic system in order to meet the requirements of secure image transfer. Shuffling the positions and changing the grey values of image pixels are combined to shuffle the relationship between the cipher-image and the original-image. First, the Arnold cat map is used to shuffle the positions of the image pixels. Second, the shuffled-image is encrypted based on Henon's chaotic system pixel by pixel. There are several parameters in this kind of chaos system and Arnold cat map. The results of several experimental, statistical analysis and key sensitivity tests show that the proposed image encryption scheme provides an efficient and secure way for image encryption. The distribution of grey values of the encrypted image exhibits a random-like behavior.","Cryptography,
Chaos,
Pixel,
Security,
Educational institutions,
Biomedical engineering,
Computer science,
Biomedical imaging,
Statistical analysis,
Testing"
Designing color filter arrays for the joint capture of visible and near-infrared images,"Digital camera sensors are inherently sensitive to the near-infrared (NIR) part of the light spectrum. In this paper, we propose a general design for color filter arrays that allow the joint capture of visible/NIR images using a single sensor. We pose the CFA design as a novel spatial domain optimization problem, and provide an efficient iterative procedure that finds (locally) optimal solutions. Numerical experiments confirm the effectiveness of the proposed CFA design, which can simultaneously capture high quality visible and NIR image pairs.","Sensor arrays,
Cameras,
Image sensors,
Mirrors,
Layout,
Digital filters,
Design optimization,
Digital photography,
Optical imaging,
Optical arrays"
Rotational Speed Control of Na^{+}-Driven Flagellar Motor by Dual Pipettes,"Single cell analysis has attracted much attention to reveal the detailed and localized biological information. Local environmental control technique is desired to analyze the detailed and localized properties of single cells. In this paper, we propose the local environmental control system with nano/micro dual pipettes to control the local reagent concentration dynamically and arbitrarily. Local environmental control by dual pipettes is applied to the rotational speed control of bacterial flagellar motor, which is a rotary molecular machine. We demonstrate quick response and iterative rotational speed control of Na+-driven flagellar motor in both accelerating and relaxing directions by switching the local spout between Na+-containing and Na+-free solutions with dual pipettes. It is shown that the rotational speed might be controllable by changing the spouting velocity of Na+-containing solution with multiplying the applied dc voltage.","Velocity control,
Nanobioscience,
Microorganisms,
Cells (biology),
Information analysis,
Stators,
Biomembranes,
Control systems,
Systems engineering and theory,
Torque"
A Stochastic Approach to Estimate the UncertaintyInvolved in B-Spline Image Registration,"Uncertainties in image registration may be a significant source of errors in anatomy mapping as well as dose accumulation in radiotherapy. It is, therefore, essential to validate the accuracy of image registration. Here, we propose a method to detect areas where mono modal B-spline registration performs well and to distinguish those from areas of the same image, where the registration is likely to be less accurate. It is a stochastic approach to automatically estimate the uncertainty of the resulting displacement vector field. The coefficients resulting from the B-spline registration are subject to moderate and randomly performed variations. A quantity is proposed to characterize the local sensitivity of the similarity measure to these variations. We demonstrate the statistical dependence between the local image registration error and this quantity by calculating their mutual information. We show the significance of the statistical dependence with an approach based on random redistributions. The proposed method has the potential to divide an image into subregions which differ in the magnitude of their average registration error.","Stochastic processes,
Spline,
Image registration,
Biomedical imaging,
Oncology,
Uncertainty,
Physics,
Cancer,
Anatomy,
MONOS devices"
Young's Modulus Reconstruction for Radio-Frequency Ablation Electrode-Induced Displacement Fields: A Feasibility Study,"Radio-frequency (RF) ablation is a minimally invasive treatment for tumors in various abdominal organs. It is effective if good tumor localization and intraprocedural monitoring can be done. In this paper, we investigate the feasibility of using an ultrasound-based Young's modulus reconstruction algorithm to image an ablated region whose stiffness is elevated due to tissue coagulation. To obtain controllable tissue deformations for abdominal organs during and/or intermediately after the RF ablation, the proposed modulus imaging method is specifically designed for using tissue deformation fields induced by the RF electrode. We have developed a new scheme under which the reconstruction problem is simplified to a 2-D problem. Based on this scheme, an iterative Young's modulus reconstruction technique with edge-preserving regularization was developed to estimate the Young's modulus distribution. The method was tested in experiments using a tissue-mimicking phantom and on ex vivo bovine liver tissues. Our preliminary results suggest that high contrast modulus images can be successfully reconstructed. In both experiments, the geometries of the reconstructed modulus images of thermal ablation zones match well with the phantom design and the gross pathology image, respectively.","Radio frequency,
Image reconstruction,
Neoplasms,
Abdomen,
Ultrasonic imaging,
Imaging phantoms,
Minimally invasive surgery,
Monitoring,
Reconstruction algorithms,
Coagulation"
XOR Rescue: Exploiting Network Coding in Lossy Wireless Networks,"It is well-known that wireless links are error-prone and require retransmissions for recovering frames from errors and losses. Network coding (NC) has been proposed for more efficient MAC-layer retransmissions in WLANs. However, existing schemes employed the reception report mechanism, which is both inefficient and expensive. Furthermore, they considered neither fairness nor the effects of time-varying heterogeneous wireless networks. These issues are critical for achieving full benefit of network coding. Without addressing them, these schemes may even impair system performance. In this paper, a novel MAC-layer retransmission scheme, namely XOR rescue (XORR) is proposed. It estimates the reception status without extra overheads and devises a new coding metric, which accommodates the effects of the frames size and the channel condition. Finally, XORR employs NC-aware fair opportunistic scheduling, which is theoretically proven to be fair, i.e. not only the service time is evenly allocated, but also it always improves the expected goodput for every wireless station. It is further verified by theoretic analyses, extensive simulations and testbed experiments. Our results show that XORR outperforms the non-coding fair opportunistic scheduling and 802.11 by 25% and 40%, respectively.","Network coding,
Wireless networks,
Automatic repeat request,
Erbium,
Computer science,
Computer errors,
Communications Society,
Asia,
USA Councils,
System performance"
Patterns of Delegate MAS,"Delegate MAS has been proposed and investigated as an integrated coordination technique for so-called self-organizing coordination-and-control applications. Delegate MAS consist of three types of light weight, ant-like agents that assist domain agents in their coordination tasks - the types are exploration, intention and feasibility ants. The technique is especially suitable for distributed applications in large-scale, dynamic systems. Literature shows that, for various application domains, solution approaches based on self-organization have been proposed that have several similarities to delegate MAS, yet are not identical. In this paper, we specify three reusable solution patterns for coordination in distributed, large-scale, dynamic systems. To motivate the patterns, we first visit several solution approaches from various domains that bear resemblance with respect to coordination. We then identify common application characteristics as well as common technical challenges that underlie the approaches. We describe recurring solution techniques, and consolidate these in three solution patterns. The patterns are ""smart messages"", ""delegate MAS"", and ""delegate ant MAS"" which rely on stigmergic ant agents. Identifying the patterns fosters reuse of particularly useful coordination techniques, and can serve as a catalyst for new or altered approaches.","Large-scale systems,
Application software,
Middleware,
Manufacturing,
Computer science,
Computer industry,
Conference management,
Systems engineering and theory,
Logistics,
Peer to peer computing"
Performance assessment of Generalized Differential Evolution 3 with a given set of constrained multi-objective test problems,"This paper presents results for the CEC 2009 Special Session on “Performance Assessment of Constrained / Bound Constrained Multi-Objective Optimization Algorithms” when Generalized Differential Evolution 3 has been used to solve a given set of test problems. The set consist of 23 problems having two, three, or five objectives. Problems have different properties in the sense of separability, modality, and geometry of the Pareto-front. The most of the problems are unconstrained, but 10 problems have one or two constraints. According to the numerical results with an inverted generational distance, Generalized Differential Evolution 3 performed well with all the problems except with one five objective problem. It was noticed that a low crossover control parameter value provides the best average results according to the metric.","Testing,
Constraint optimization,
Geometry,
Genetic mutations,
Size control,
Evolutionary computation,
Optimization methods,
Encoding,
Chromium"
Texture based classification of hyperspectral colon biopsy samples using CLBP,"Computer aided diagnosis (CAD) is aimed at supporting the pathologists in their diagnosis. In this paper, we present an algorithm for texture-based classification of colon tissue patterns. In this method, a single band is selected from its hyperspectral cube and spatial analysis is performed using circular local binary pattern (CLBP) features. A novel method for feature selection is presented resulting in the best feature set without actually running the classifier. Classification results using Gaussian kernel SVM, with an accuracy of 90%, demonstrate that texture analysis based on CLBP features is able to distinguish the benign and malignant patterns.","Colon,
Biopsy,
Hyperspectral imaging,
Cancer,
Hyperspectral sensors,
Performance analysis,
Classification algorithms,
Support vector machines,
Support vector machine classification,
Pattern analysis"
Iterative carrier-frequency offset estimation for generalized OFDMA uplink transmission,"Maximum likelihood (ML) carrier-frequency offset (CFO) estimation for orthogonal frequency-division multiple-access (OFDMA) uplink with generalized carrier-assignment scheme (GCAS) is a complex multi-parameter estimation problem. The computational complexity of ML solution based on a multi-dimensional exhaustive search is prohibitive. The existing sub-optimal solutions reduce the complexity by replacing the multi-dimensional search with a sequence of single-dimensional searches. However, these solutions suffer from either poor estimation accuracy or being still of fairly high complexity. In this paper, we propose a new approach called divide-and-update frequency estimator (DUFE), for CFO estimation. Compared with the existing approaches, the proposed DUFE has lower computational complexity while maintaining high estimation accuracy similar to that of the exact ML solution. Performance and complexity comparisons are provided, along with numerical results to illustrate the effectiveness of the proposed method.",
Wheelchair control using an EOG- and EMG-based gesture interface,"In our previous study, we presented a nonverbal interface that used biopotential signals, such as electrooculargraphic (EOG) and electromyographic (EMG), captured by a simple brain-computer interface. In this paper, we apply the nonverbal interface to hands-free control of an electric wheelchair. Based on the biopotential signals, the interface recognizes the operator's gestures, such as closing the jaw, wrinkling the forehead, and looking towards left and right. By combining these gestures, the operator controls linear and turning motions, velocity, and the steering angle of the wheelchair. Experimental results for navigating the wheelchair in a hallway environment confirmed the feasibility of the proposed method.","Wheelchairs,
Electrooculography,
Electromyography,
Electroencephalography,
Humans,
Computer interfaces,
Forehead,
Motion control,
Speech recognition,
Biomedical engineering"
On quantized consensus by means of gossip algorithm - Part I: Convergence proof,"This paper is concerned with the distributed averaging problem subject to a quantization constraint. Given a group of agents associated with scalar numbers, it is assumed that each pair of agents can communicate with a prescribed probability, and that the data being exchanged between them is quantized. In this part of the paper, it is proved that the stochastic gossip algorithm proposed in a recent paper leads to reaching the quantized consensus. Some important steady-state properties of the system (after reaching the consensus) are also derived. The results developed here hold true for any arbitrary quantization, provided that the tuning parameter of the gossip algorithm is chosen properly. The expected value of the convergence time is lower and upper bounded in the second part of the paper.","Convergence,
Quantization,
Stochastic processes,
Computer networks,
Distributed computing,
Java,
Tuning,
History,
Computer science,
Frequency synchronization"
Automatic Writer Identification of Ancient Greek Inscriptions,"This paper introduces a novel methodology for the classification of ancient Greek inscriptions according to the writer who carved them. Inscription writer identification is crucial for dating the written content, which in turn is of fundamental importance in the sciences of history and archaeology. To achieve this, we first compute an ideal or ""platonicrdquo prototype for the letters of each inscription separately. Next, statistical criteria are introduced to reject the hypothesis that two inscriptions are carved by the same writer. In this way, we can determine the number of distinct writers who carved a given ensemble of inscriptions. Next, maximum likelihood considerations are employed to attribute all inscriptions in the collection to the respective writers. The method has been applied to 24 Ancient Athenian inscriptions and attributed these inscriptions to six different identified hands in full accordance with expert epigraphists' opinions.","Leg,
Pattern recognition,
Writing,
History,
Prototypes,
Pattern analysis,
Building materials,
Engineering profession,
Image processing,
Information systems"
Minimizing WCET for Real-Time Embedded Systems via Static Instruction Cache Locking,"Cache is effective in bridging the gap between processor and memory speed. It is also a source of unpredictability because of its dynamic and adaptive behavior. Worst-case execution time (WCET) of an application is one of the most important criteria for real-time embedded system design. The unpredictability of instruction miss/hit behavior in the instruction cache (I-Cache) leads to an unnecessary over-estimation of the real-time application's WCET. A lot of modern processors provide cache locking capability. Static I-Cache locking locks function/instruction blocks of a program into the I-Cache before program execution. In this way, a more precise estimation of WCET can be achieved. The selection of functions/instructions to be locked in the I-Cache has dramatic influence on the performance of the real-time application. This paper focuses on the static I-Cache locking problem to minimize WCET for real-time embedded systems. We formulate the problem using an Execution Flow Tree (EFT) and a linear programming model. For a subset of the problems with certain properties, corresponding polynomial time optimal algorithms are proposed. We prove that the general problem is an NP-Hard problem. We also show that for a subset of the general problem with certain patterns, optimal solutions can be achieved in polynomial time. Experimental results show that our algorithms can reduce the WCET of applications further compared to current best known techniques.","Real time systems,
Embedded system,
Application software,
Linear programming,
Polynomials,
Computer science,
NP-hard problem,
Councils,
Hardware,
Costs"
Low-Rank Matrix Fitting Based on Subspace Perturbation Analysis with Applications to Structure from Motion,"The task of finding a low-rank (r) matrix that best fits an original data matrix of higher rank is a recurring problem in science and engineering. The problem becomes especially difficult when the original data matrix has some missing entries and contains an unknown additive noise term in the remaining elements. The former problem can be solved by concatenating a set of r-column matrices that share a common single r-dimensional solution space. Unfortunately, the number of possible submatrices is generally very large and, hence, the results obtained with one set of r-column matrices will generally be different from that captured by a different set. Ideally, we would like to find that solution that is least affected by noise. This requires that we determine which of the r-column matrices (i.e., which of the original feature points) are less influenced by the unknown noise term. This paper presents a criterion to successfully carry out such a selection. Our key result is to formally prove that the more distinct the r vectors of the r-column matrices are, the less they are swayed by noise. This key result is then combined with the use of a noise model to derive an upper bound for the effect that noise and occlusions have on each of the r-column matrices. It is shown how this criterion can be effectively used to recover the noise-free matrix of rank r. Finally, we derive the affine and projective structure-from-motion (SFM) algorithms using the proposed criterion. Extensive validation on synthetic and real data sets shows the superiority of the proposed approach over the state of the art.","Motion analysis,
Computer vision,
Optical noise,
Pattern recognition,
Bioinformatics,
Data engineering,
Additive noise,
Upper bound,
Pattern analysis,
Computer graphics"
Data mining for blood glucose prediction and knowledge discovery in diabetic patients: The METABO diabetes modeling and management system,"METABO is a diabetes monitoring and management system which aims at recording and interpreting patient's context, as well as, at providing decision support to both the patient and the doctor. The METABO system consists of (a) a Patient's Mobile Device (PMD), (b) different types of unobtrusive biosensors, (c) a Central Subsystem (CS) located remotely at the hospital and (d) the Control Panel (CP) from which physicians can follow-up their patients and gain also access to the CS. METABO provides a multi-parametric monitoring system which facilitates the efficient and systematic recording of dietary, physical activity, medication and medical information (continuous and discontinuous glucose measurements). Based on all recorded contextual information, data mining schemes that run in the PMD are responsible to model patients' metabolism, predict hypo/hyper-glycaemic events, and provide the patient with short and long-term alerts. In addition, all past and recently-recorded data are analyzed to extract patterns of behavior, discover new knowledge and provide explanations to the physician through the CP. Advanced tools in the CP allow the physician to prescribe personalized treatment plans and frequently quantify patient’s adherence to treatment.","Diabetes,
Data mining,
Blood,
Sugar,
Predictive models,
Knowledge management,
Patient monitoring,
Remote monitoring,
Medical treatment,
Health information management"
Voltage limiter calculation method for fast torque response of IPMSM in overmodulation range,"In this paper, a voltage limiter calculation method for the fast torque response of an interior permanent magnet synchronous motor (IPMSM) when an inverter operates in the overmodulation range is considered. The relation between torque response of an IPMSM and voltage phase angle in the overmodulation range is analyzed, and based on this relation, the optimal voltage limiter calculation method that can be realized by real time calculation is proposed. By using the proposed method, torque response of an IPMSM can be improved as fast as twice the conventional methods. The effectiveness of the proposed method is confirmed by simulation and experimental results.","Torque,
Voltage control,
Inverters"
Pitfalls of modeling wind power using Markov chains,"An increased penetration of wind turbines have given rise to a need for wind speed/power models that generate realistic synthetic data. Such data, for example, might be used in simulations to size energy storage or spinning reserve. In much literature, Markov chains have been proposed as an acceptable method to generate synthetic wind data, but we have observed that the autocorrelation plots of wind speeds generated by Markov chains are often inaccurate. This paper describes when using Markov chains is appropriate and demonstrates the gross underestimation of storage requirements that occurs at short time steps. We found that Markov chains should not be used for time steps shorter than 15 to 40 minutes, depending on the order of the Markov chain and the number of wind power states. This result implies that Markov chains are of limited use as synthetic data generators for small microgrid models and other applications requiring short simulation time steps. New algorithms for generating synthetic wind data at shorter time steps must be developed.","Wind energy,
Wind energy generation,
Wind speed,
Power generation,
Autocorrelation,
Wind turbines,
Energy storage,
Spinning,
Wind power generation,
Yield estimation"
Optimal control of affine nonlinear discrete-time systems,"In this paper, direct neural dynamic programming techniques are utilized to solve the Hamilton Jacobi-Bellman equation in real time for the optimal control of general affine nonlinear discrete-time systems. In the presence of partially unknown dynamics, the optimal regulation control problem is addressed while the optimal tracking control problem is addressed in the presence of known dynamics. Each design entails two portions: an action neural network (NN) that is designed to produce a nearly optimal control signal, and a critic NN which evaluates the performance of the system. Novel weight update laws for the critic and action NN's are derived, and all parameters are tuned online. Lyapunov techniques are used to show that all signals are uniformly ultimately bounded (UUB) and that the output of the action NN approaches the optimal control input with small bounded error. Simulation results are also presented to demonstrate the effectiveness of the approach.","Optimal control,
Neural networks,
Control systems,
Nonlinear equations,
Cost function,
Dynamic programming,
Nonlinear control systems,
Nonlinear dynamical systems,
Riccati equations,
Jacobian matrices"
Digital VLSI logic technology using Carbon Nanotube FETs: Frequently Asked Questions,"Carbon nanotube field-effect transistors (CNFETs) show promise as extensions to silicon-CMOS. Ideal CNFET circuits can potentially provide 20X energy-delay-product benefits over silicon-CMOS at the 16 nm technology node. However, several challenges must be overcome before such performance benefits can be experimentally realized. In this paper, we present a brief overview of CNFET technology, and address commonly raised concerns through a series of frequently asked questions (FAQs). We also provide a CNFET technology outlook which includes a survey of challenges as well as existing and potential solutions to these challenges.","Very large scale integration,
Logic,
Circuits,
CMOS technology,
Silicon,
Inverters,
Carbon nanotubes,
Lithography,
Contacts,
CNTFETs"
"A Low-energy, Multi-copy Inter-contact Routing Protocol for Disaster Response Networks","This paper presents a novel multi-copy routing protocol for disruption-tolerant networks whose objective is to minimize energy expended on communication. The protocol is designed for disaster-response applications, where power and infrastructure resources are disrupted. Unlike other delay-tolerant networks, energy is a vital resource in post-disaster scenarios to ensure availability of (disruption-tolerant) communication until infrastructure is restored. Our approach exploits naturally recurrent mobility and contact patterns in the network, formed by rescue workers, volunteers, survivors, and their (possibly stranded) vehicles to reduce the number of message copies needed to attain an adequate delivery ratio in the face of disconnection and intermittent connectivity. A new notion of inter-contact routing is proposed that allows estimating route delays and delivery probabilities, identifying more reliable routes and controlling message replication and forwarding accordingly. We simulate the scheme using a mobility model that reflects recurrence inspired by disaster scenarios, and compare our results to previous DTN routing techniques. The evaluation shows that the new approach reduces the resource overhead per message over previous approaches while maintaining a comparable delivery ratio at the expense of a small (bounded) increase in latency.","Routing protocols,
Disruption tolerant networking,
Delay estimation,
Vehicles,
Fires,
Energy efficiency,
Educational institutions,
Communications Society,
Computer science,
Peer to peer computing"
Requirements Engineering for E-science: Experiences in Epidemiology,"The Advises (Adaptive Visualization of E-science) project is developing tools to support geographic visualization in epidemiology and public-health decision making. In this project, a user-centered requirements process focuses on the research questions epidemiologists ask, the language they use, and the tacit knowledge employed in reasoning about epidemiological data. Combining a range of requirements-gathering techniques provides considerable advantages.","Visualization,
Diseases,
Iterative methods,
Geographic Information Systems,
Biomedical informatics,
Decision making,
Genetics,
Technological innovation,
Usability,
Data engineering"
Real-Time Depth-of-Field Rendering Using Anisotropically Filtered Mipmap Interpolation,"This article presents a real-time GPU-based post-filtering method for rendering acceptable depth-of-field effects suited for virtual reality. Blurring is achieved by nonlinearly interpolating mipmap images generated from a pinhole image. Major artifacts common in the post-filtering techniques such as bilinear magnification artifact, intensity leakage, and blurring discontinuity are practically eliminated via magnification with a circular filter, anisotropic mipmapping, and smoothing of blurring degrees. The whole framework is accelerated using GPU programs for constant and scalable real-time performance required for virtual reality. We also compare our method to recent GPU-based methods in terms of image quality and rendering performance.","Anisotropic magnetoresistance,
Interpolation,
Rendering (computer graphics),
Virtual reality,
Focusing,
Image quality,
Pixel,
Filtering,
Cameras,
Image generation"
An Application of Evidential Networks to Threat Assessment,"Decision makers operating in modern defence theatres need to comprehend and reason with huge quantities of potentially uncertain and imprecise data in a timely fashion. An automatic information fusion system is developed which aims at supporting a commander's decision making by providing a threat assessment, that is an estimate of the extent to which an enemy platform poses a threat based on evidence about its intent and capability. Threat is modelled by a network of entities and relationships between them, while the uncertainties in the relationships are represented by belief functions as defined in the theory of evidence. To support the implementation of the threat assessment functionality, an efficient valuation-based reasoning scheme, referred to as an evidential network, is developed. To reduce computational overheads, the scheme performs local computations in the network by applying an inward propagation algorithm to the underlying binary join tree. This allows the dynamic nature of the external evidence, which drives the evidential network, to be taken into account by recomputing only the affected paths in the binary join tree.","Australia,
Humans,
Bayesian methods,
Computer networks,
Command and control systems,
Decision making,
Uncertainty,
Computer science,
Military computing,
Organizational aspects"
A Unified Framework for Automated 3-D Segmentation of Surface-Stained Living Cells and a Comprehensive Segmentation Evaluation,"This work presents a unified framework for whole cell segmentation of surface stained living cells from 3-D data sets of fluorescent images. Every step of the process is described, image acquisition, prefiltering, ridge enhancement, cell segmentation, and a segmentation evaluation. The segmentation results from two different automated approaches for segmentation are compared to manual segmentation of the same data using a rigorous evaluation scheme. This revealed that combination of the respective cell types with the most suitable microscopy method resulted in high success rates up to 97%. The described approach permits to automatically perform a statistical analysis of various parameters from living cells.","Image segmentation,
Surface morphology,
Fluorescence,
Statistical analysis,
Humans,
Shape,
Cancer,
Labeling,
Clustering algorithms,
Cells (biology)"
A real-time kernel for wireless sensor networks employed in rescue scenarios,"In rescue scenarios, real-time requirements are one key issue when using wireless sensor networks (WSNs) for tracking and monitoring of rescue forces. If a node detects an alarm condition, the alarm message must be delivered to the base station in time. To guarantee the timely delivery the whole system architecture has to fulfill real-time requirements. There are only a few real-time architectures which can be used for sensor networks. These architectures are too generic to comply with the other requirements of a WSN architecture, like RAM usage and energy awareness. In this paper, we present FireKernel, a real-time micro kernel designed for WSN operating systems with a special focus on hard real-time requirements and strict energy management. It offers a preemptive real-time scheduler, mutexes and synchronous message passing for interprocess communication (IPC). The scheduler is prioritybased and uses no periodical timers. As part of the kernel we introduce a tickless timer system. We have implemented and tested the kernel on MSP430 and ARM7TDMI based sensor nodes. We discuss the kernel's architecture, implementation and resource usage. Further, we present benchmark results for interrupt latency and energy consumption of the timer architecture.","Kernel,
Wireless sensor networks,
Real time systems,
Base stations,
Operating systems,
Energy management,
Message passing,
Benchmark testing,
Delay,
Energy consumption"
OS support for detecting Trojan circuit attacks,"Rapid advances in integrated circuit (IC) development predicted by Moore's Law lead to increasingly complex, hard to verify IC designs. Design insiders or adversaries employed at untrusted locations can insert malicious Trojan circuits capable of launching attacks in hardware or supporting software-based attacks. In this paper, we provide a method for detecting Trojan circuit denial-of-service attacks using a simple, verifiable hardware guard external to the complex CPU. The operating system produces liveness checks, embedded in the software clock, to which the guard can respond. We also present a novel method for the OS to detect a hardware-software (HW/SW) Trojan privilege escalation attack by using OS-generated checks to test if the CPU hardware is enforcing memory protection (MP). Our implementation of fine-grained periodic checking of MP enforcement incurs only 2.2% overhead using SPECint 2006.",
Development of a 55 kW 3X dc-dc converter for HEV systems,"The design of a 55 kW 3X dc-dc converter is presented for hybrid electric vehicle (HEV) traction drives. It can interface the battery with the inverter dc bus with three output/input voltage ratios and have smooth transition in voltage ratio changes. By making use of the parasitic inductance, the size and weight of the converter are significantly reduced. Its magnetic-less feature and high efficiency provide the great potential for the very high temperature operation. The circuit parameter design and the circuit modeling are provided. Experimental results are given to verify the analysis and design concepts.","DC-DC power converters,
Hybrid electric vehicles,
Voltage,
Pulse width modulation inverters,
Temperature,
Batteries,
Pulse width modulation converters,
Saturation magnetization,
Capacitors,
Switching converters"
Personalized digital TV content recommendation with integration of user behavior profiling and multimodal content rating,This paper presents the novel development of an embedded system that aims at digital TV content recommendation based on descriptive metadata collected from versatile sources. The described system comprises a user profiling subsystem identifying user preferences and a user agent subsystem performing content rating. TV content items are ranked using a combined multimodal approach integrating classification-based and keyword-based similarity predictions so that a user is presented with a limited subset of relevant content. Observable user behaviors are discussed as instrumental in user profiling and a formula is provided for implicitly estimating the degree of user appreciation of content. A new relation-based similarity measure is suggested to improve categorized content rating precision. Experimental results show that our system can recommend desired content to users with significant amount of accuracy.,"Digital TV,
Home appliances,
Embedded system,
Instruments,
Information systems,
Web and internet services,
Hardware,
Collaborative work,
Business,
Computer science"
Building a distributed robot garden,"This paper describes the architecture and implementation of a distributed autonomous gardening system. The garden is a mesh network of robots and plants. The gardening robots are mobile manipulators with an eye-in-hand camera. They are capable of locating plants in the garden, watering them, and locating and grasping fruit. The plants are potted cherry tomatoes enhanced with sensors and computation to monitor their well-being (e.g. soil humidity, state of fruits) and with networking to communicate servicing requests to the robots. Task allocation, sensing and manipulation are distributed in the system and de-centrally coordinated. We describe the architecture of this system and present experimental results for navigation, object recognition and manipulation.",
Road vehicle classification using Support Vector Machines,"The Support Vector Machine (SVM) provides a robust, accurate and effective technique for pattern recognition and classification. Although the SVM is essentially a binary classifier, it can be adopted to handle multi-class classification tasks. The conventional way to extent the SVM to multi-class scenarios is to decompose an m-class problem into a series of two-class problems, for which either the one-vs-one (OVO) or one-vs-all (OVA) approaches are used. In this paper, a practical and systematic approach using a kernelised SVM is proposed and developed such that it can be implemented in embedded hardware within a road-side camera. The foreground segmentation of the vehicle is obtained using a Gaussian mixture model background subtraction algorithm. The feature vector describing the foreground (vehicle) silhouette encodes size, aspect ratio, width, solidity in order to classify vehicle type (car, van, HGV), In addition 3D colour histograms are used to generate a feature vector encoding vehicle color. The good recognition rates achieved in the our experiments indicate that our approach is well suited for pragmatic embedded vehicle classification applications.","Road vehicles,
Support vector machines,
Support vector machine classification,
Cameras,
Surveillance,
Pattern recognition,
Hardware,
Encoding,
Vehicle detection,
Shape"
Parallelizing sparse Matrix Solve for SPICE circuit simulation using FPGAs,"Fine-grained dataflow processing of sparse matrix-solve computation (Ax¿ = b¿) in the SPICE circuit simulator can provide an order of magnitude performance improvement on modern FPGAs. Matrix solve is the dominant component of the simulator especially for large circuits and is invoked repeatedly during the simulation, once for every iteration. We process sparse-matrix computation generated from the SPICE-oriented KLU solver in dataflow fashion across multiple spatial floating-point operators coupled to high-bandwidth on-chip memories and interconnected by a low-latency network. Using this approach, we are able to show speedups of 1.2-64× (geometric mean of 8.8×) for a range of circuits and benchmark matrices when comparing double-precision implementations on a 250 MHz Xilinx Virtex-5 FPGA (65 nm) and an Intel Core i7 965 processor (45 nm).","Sparse matrices,
SPICE,
Circuit simulation,
Field programmable gate arrays,
Computational modeling,
Nonlinear equations,
Concurrent computing,
Integrated circuit interconnections,
Runtime,
Computer architecture"
Building the Trident Scientific Workflow Workbench for Data Management in the Cloud,"Scientific workflows have gained popularity for modeling and executing in silico experiments by scientists for problem-solving. These workflows primarily engage in computation and data transformation tasks to perform scientific analysis in the Science Cloud. Increasingly workflows are gaining use in managing the scientific data when they arrive from external sensors and are prepared for becoming science ready and available for use in the Cloud. While not directly part of the scientific analysis, these workflows operating behind the Cloud on behalf of the -data valets¿ play an important role in end-to-end management of scientific data products. They share several features with traditional scientific workflows: both are data intensive and use Cloud resources. However, they also differ in significant respects, for example, in the reliability required, scheduling constraints and the use of provenance collected. In this article, we investigate these two classes of workflows – Science Application workflows and Data Preparation workflows – and use these to drive common and distinct requirements from workflow systems for eScience in the Cloud. We use workflow examples from two collaborations, the NEPTUNE oceanography project and the Pan-STARRS astronomy project, to draw out our comparison. Our analysis of these workflows classes can guide the evolution of workflow systems to support emerging applications in the Cloud and the Trident Scientific Workbench is one such workflow system that has directly benefitted from this to meet the needs of these two eScience projects","Cloud computing,
Instruments,
Data analysis,
Grid computing,
Resource management,
Scheduling,
Evolution (biology),
Data engineering,
Computer applications,
Conference management"
Balancing wind power with virtual power plants of micro-CHPs,"Higher participation levels of wind power in power systems will increase the need for flexible back-up generation to balance the differences between predicted and realized wind power production. This is often an expensive solution. With distributed energy resources and more ICT at the demand side, novel, and possibly cheaper, ways for imbalance minimization arise. Micro combined heat-and-power (micro-CHP) is a novel domestic-level generation technology, producing heat and power simultaneously. Clusters of micro-CHPs can function as flexible virtual power plants (VPPs). This paper presents the design of an online coordination scheme that can substantially reduce the imbalance volumes and the associated costs for wind power traders by actively controlling a VPP comprising micro-CHP systems. It is shown that the imbalance volume and associated cost can be reduced by 73 % and 38 %, respectively.","Wind energy,
Power generation,
Costs,
Wind energy generation,
Cogeneration,
Power systems,
Energy resources,
Distributed power generation,
Technology management,
Electronic mail"
Outsourcing Search Services on Private Spatial Data,"Social networking and content sharing service providers, e.g., Facebook and Google Maps, enable their users to upload and share a variety of user-generated content, including location data such as points of interest. Users wish to share location data through an (untrusted) service provider such that trusted friends can perform spatial queries on the data. We solve the problem by transforming the location data before uploading them. We contribute spatial transformations that re-distribute locations in space and a transformation that employs cryptographic techniques. The data owner selects transformation keys and shares them with the trusted friends. Without the keys, it is infeasible for an attacker to reconstruct the exact original data points from the transformed points. These transformations achieve different tradeoffs between query efficiency and data security. In addition, we describe an attack model for studying the security properties of the transformations. Empirical studies suggest that the proposed methods are secure and efficient.","Outsourcing,
Cryptography,
User-generated content,
Data security,
Computer science,
Facebook,
Data engineering,
Lungs,
Social network services,
Cameras"
Detecting Spammers and Content Promoters in Online Video Social Networks,"Online video social networks provides features that allow users to post a video as a response to a discussion topic. These features open opportunities for users to introduce polluted content into the system. For instance, spammers may post an unrelated video as response to a popular one aiming at increasing the likelihood of the response being viewed by a larger number of users. Moreover, opportunistic users - promoters- may try to gain visibility to a specific video by posting a large number of responses to boost the rank of the responded video, making it appear in the top lists maintained by the system. In this paper, we address the issue of detecting video spammers and promoters. Towards that end, we build a test collection of real YouTube users. Using our test collection we investigate the feasibility of using a supervised classification algorithm to detect spammers and promoters. We found that our approach is able to correctly identify the majority of the promoters, misclassifying only a small percentage of legitimate users. In contrast, although we are able to detect a significant fraction of spammers, they showed to be much harder to distinguish from legitimate users.","Social network services,
Video sharing,
Pollution,
YouTube,
Testing,
Crawlers,
Computer science,
Classification algorithms,
Bandwidth,
Supervised learning"
A Security Modeling Approach for Web-Service-Based Business Processes,"The rising need for security in SOA applications requires better support for management of non-functional properties in web-based business processes. Here, the model-driven approach may provide valuable benefits in terms of maintainability and deployment. Apart from modeling the pure functionality of a process, the consideration of security properties at the level of a process model is a promising approach. In this work-in-progress paper we present an extension to the ARIS SOA Architect that is capable of modeling security requirements as a separate security model view. Further we provide a transformation that automatically derives WS-Security Policy-conformant security policies from the process model, which in conjunction with the generated WS-BPEL processes and WSDL documents provides the ability to deploy and run the complete security-enhanced process based on Web Service technology.","Data security,
Web services,
Service oriented architecture,
Computer security,
Application software,
Programming,
Information security,
Access control,
Conferences,
Computer science"
Cluster-based inter-domain routing (cidr) protocol for MANETs,"Nowadays, inter-domain routing for MANETs draws increasing attention because of military and vehicular applications. The challenges in wireless, mobile inter-domain routing include dynamic network topology, intermittent connectivity, and routing protocol heterogeneity. The existing Border Gateway Protocol (BGP) is the de facto inter-domain routing protocol for the Internet. But BGP is not applicable to MANETs because the BGP design is based on a static Internet which does not support dynamic discovery of members, and cannot scale to mobile, dynamic topology environments. The proposed cluster-based inter-domain routing (CIDR) protocol obtains efficient communications among MANETs and achieves scalability in large networks by using the clustering technique. The proposed approach generates clusters using group affinity. In each domain, the distributed clustering algorithm discovers the set of “traveling companions” - these are the nodes that stick together as a group for some time or for some common tasks. It elects within each set a Cluster Head (CH) for each affinity group. Affinity is defined in terms of some common characteristics, such as group motion or task. The clusters (i.e., subnets) can be defined a priori or may evolve dynamically by the affinity of geography, motion, or task. The cluster head in the subnet acts as local DNS for own cluster and also (redundantly) for neighbor clusters. The cluster head advertises to neighbors and the rest of the network its connectivity, members, and domain information. The advertising protocol plays the role of BG Protocol. In the proposed protocol, packets to remote nodes are routed via cluster-head advertised routes, and packets to local destinations are routed using the local routing algorithm. The experiments have shown that the proposed inter-domain routing has achieved scalability and robustness to mobility.",
Monaural Musical Sound Separation Based on Pitch and Common Amplitude Modulation,"Monaural musical sound separation has been extensively studied recently. An important problem in separation of pitched musical sounds is the estimation of time-frequency regions where harmonics overlap. In this paper, we propose a sinusoidal modeling-based separation system that can effectively resolve overlapping harmonics. Our strategy is based on the observations that harmonics of the same source have correlated amplitude envelopes and that the change in phase of a harmonic is related to the instrument's pitch. We use these two observations in a least squares estimation framework for separation of overlapping harmonics. The system directly distributes mixture energy for harmonics that are unobstructed by other sources. Quantitative evaluation of the proposed system is shown when ground truth pitch information is available, when rough pitch estimates are provided in the form of a MIDI score, and finally, when a multi pitch tracking algorithm is used. We also introduce a technique to improve the accuracy of rough pitch estimates. Results show that the proposed system significantly outperforms related monaural musical sound separation systems.","Amplitude modulation,
Sparse matrices,
Psychoacoustic models,
Image analysis,
Instruments,
Music information retrieval,
Computer science,
Independent component analysis,
Least squares approximation,
Signal processing algorithms"
Influence of surface charges on impulse flashover characteristics of alumina dielectrics in vacuum,"For higher electrical insulator performance of vacuum circuit breakers (VCB), the surface flashover in vacuum should be improved. In particular, it is important to clarify how strongly surface charges influence the surface flashover mechanism. In this paper, we investigated the surface flashover characteristics based on the existence of surface charges on an alumina insulator in vacuum. We changed the location and magnitude of surface charges and measured surface flashover voltage. From our experimental results, an attempt was made to clarify the influence of charges on alumina dielectrics on impulse surface flashover characteristics in vacuum. From this, we concluded that (1) when surface charges were located near the cathode electrode, a positive charge made the surface flashover voltage lower and a negative charge made the surface flashover voltage higher; and, (2) when surface charges were located in the way of the discharge path, both positive and negative charges made the surface flashover voltage lower. We were able to explain these results by considering the influence of surface charges on the electric field at flashover and on the secondary electron emission avalanche.","Flashover,
Surface discharges,
Dielectrics and electrical insulation,
Circuit breakers,
Current measurement,
Dielectric measurements,
Voltage measurement,
Cathodes,
Electrodes,
Fault location"
Parallel artificial bee colony (PABC) algorithm,"The artificial bee colony (ABC) algorithm is a metaheuristic algorithm for numerical optimization. It is based on the intelligent foraging behavior of honey bees. This paper presents a parallel version of the algorithm for shared memory architectures. The entire colony of bees is divided equally among the available processors. A set of solutions is placed in the local memory of each processor. A copy of each solution is also maintained in a global shared memory. During each cycle, the set of bees at a processor improves the solutions in the local memory. At the end of the cycle, the solutions are copied into the corresponding slots in the shared memory and made available to all other bees. It is shown that the proposed parallelization strategy does not degrade the quality of solutions obtained, but achieves substantial speedup.","Particle swarm optimization,
Educational institutions,
Artificial intelligence,
Memory architecture,
Search methods,
Traveling salesman problems,
Computer science,
Degradation,
Space exploration,
Constraint optimization"
Automatic Static Unpacking of Malware Binaries,"Current malware is often transmitted in packed or encrypted form to prevent examination by anti-virus software.To analyze new malware, researchers typically resort to dynamic code analysis techniques to unpack the code for examination.Unfortunately, these dynamic techniques are susceptible to a variety of anti-monitoring defenses, as well as ""time bombs"" or ""logic bombs,"" and can be slow and tedious to identify and disable. This paper discusses an alternative approach that relies on static analysis techniques to automate this process. Alias analysis can be used to identify the existence of unpacking,static slicing can identify the unpacking code, and control flow analysis can be used to identify and neutralize dynamic defenses. The identified unpacking code can be instrumented and transformed, then executed to perform the unpacking.We present a working prototype that can handle a variety of malware binaries, packed with both custom and commercial packers, and containing several examples of dynamic defenses.","Cryptography,
Reverse engineering,
Weapons,
Logic,
Computer worms,
Computer science,
Automatic control,
Instruments,
Prototypes,
Explosives"
A model-based approach to clock synchronization,"In a network of clocks, we consider a given reference node to determine the time evolution t. We introduce and analyze a stochastic model for clocks, in which the relative speedup of a clock, called the skew, is characterized by some given stochastic process. We study the problem of synchronizing clocks in a network, which amounts to estimating the instantaneous relative skews and relative offsets by exchange of time-stamped packets across the links of the network. We present a scheme for obtaining measurements in a communication link. We develop an algorithm for optimal filtering of measurements across a link (i; j) in order to estimate the logarithm of the relative speedup of node j with respect to node i, and we further study some implementation issues. We also present a scheme for pairwise offset estimation based on skew estimates. We study the properties of our algorithms and provide theoretical guarantees on their performance. We also develop an online centralized model-based asynchronous algorithm for optimal filtering of the time-stamps in the entire network, and an efficient distributed suboptimal scheme.","Clocks,
Synchronization,
Displays,
Stochastic processes,
Filtering algorithms,
Delay estimation,
Nonlinear filters,
Maximum likelihood detection,
Velocity measurement,
Distributed control"
The Organization and Management of Grid Infrastructures,"Grid computing technology has become fundamental to e-Science. As the virtual organizations established by scientific communities progress from testing their applications to more routine usage, maintaining reliable and adaptive grid infrastructures becomes increasingly important.","Large Hadron Collider,
Grid computing,
Data analysis,
Particle beams,
Colliding beam accelerators,
Physics computing,
Europe,
Prototypes,
Testing,
Acceleration"
Vision-based road detection using road models,"Vision-based road detection is very challenging since the road is in an outdoor scenario imaged from a mobile platform. In this paper, a new top-down road detection algorithm is proposed. The method is based on scene (road) classification which provides the probability that an image contains certain type of road geometry (straight, left/right curve, etc.). During the training of the classifier a road probability map is also learned for each road geometry. Then, the proper pixel-based method is selected and fused to provide an improved road detection approach. From experiments it is concluded that the proposed method outperforms state-of-the-art algorithms in a frame by frame context.","Layout,
Detection algorithms,
Information geometry,
Pixel,
Road vehicles,
Vehicle detection,
Shape,
Computer vision,
Computer science,
Proposals"
SURF Tracking,"Most motion-based tracking algorithms assume that objects undergo rigid motion, which is most likely disobeyed in real world. In this paper, we present a novel motion-based tracking framework which makes no such assumptions. Object is represented by a set of local invariant features, whose motions are observed by a feature correspondence process. A generative model is proposed to depict the relationship between local feature motions and object global motion, whose parameters are learned efficiently by an on-line EM algorithm. And the object global motion is estimated in term of maximum likelihood of observations. Then an updating mechanism is employed to adapt object representation. Experiments show that our framework is flexible and robust in dealing with appearance changes, background clutter, illumination changes and occlusion.","Tracking,
Motion estimation,
Maximum likelihood estimation,
Robustness,
Lighting,
Computer science,
Pattern recognition,
Shape,
Search methods,
Monitoring"
Sensitivity analysis of the economic benefits from electricity storage at the end consumer level,"The article presents the results of simulations based on a linear optimization model of a storage system that calculates the economic benefits of distributed storage devices at the end consumer level by determining the cost optimal charge-discharge-schedule. The primary objective of the storage application is arbitrage accommodation. Particularly, parameters for a li-ion-based and a lead-acid-based storage system are simulated. All parameters of the model are varied and analyzed regarding their impact on the economic benefits. The simulation results quantify these impacts and show that the costs per storage capacity unit (EUR/kWh) and the efficiency degrees of the storage system have the highest impact. Additionally, the price spreads as well as the distribution of the market price curve in relation with the distribution of the consumer's load curve (demand) influence the achievable benefits significantly. Overall, the model reveals a saving potential of 17% on total cost for the reference case.","Sensitivity analysis,
Energy consumption,
Power generation economics,
Energy storage,
Costs,
Power generation,
Distributed power generation,
Investments,
Technological innovation,
Power grids"
Price of Anarchy for Cognitive MAC Games,"In this paper, we model and analyze the interactions between secondary users in a spectrum overlay cognitive system as a cognitive MAC game. In this game, each secondary user can sense (and transmit) one of several channels, the availability of each channel is determined by the activity of the corresponding primary user. We show that this game belongs to the class of congestion game and thus there exists at least one Nash Equilibrium. We focus on analyzing the worst case efficiency loss (i.e., price of anarchy) at any Nash Equilibrium of such a game. Closed-form expressions of price of anarchy are derived for both symmetric and asymmetric games, with arbitrary channel and user heterogeneity. Several insights are also derived in terms of how to design better cognitive radio systems with less severe efficiency loss.","Cognitive radio,
Nash equilibrium,
Media Access Protocol,
Resource management,
Computer science,
Information analysis,
Closed-form solution,
Monitoring,
Interference,
Frequency"
B-Mode Ultrasound Image Simulation in Deformable 3-D Medium,"This paper presents an algorithm for fast image synthesis inside deformed volumes. Given the node displacements of a mesh and a reference 3-D image dataset of a predeformed volume, the method first maps the image pixels that need to be synthesized from the deformed configuration to the nominal predeformed configuration, where the pixel intensities are obtained easily through interpolation in the regular-grid structure of the reference voxel volume. This mapping requires the identification of the mesh element enclosing each pixel for every image frame. To accelerate this point location operation, a fast method of projecting the deformed mesh on image pixels is introduced in this paper. The method presented was implemented for ultrasound B-mode image simulation of a synthetic tissue phantom. The phantom deformation as a result of ultrasound probe motion was modeled using the finite element method. Experimental images of the phantom under deformation were then compared with the corresponding synthesized images using sum of squared differences and mutual information metrics. Both this quantitative comparison and a qualitative assessment show that realistic images can be synthesized using the proposed technique. An ultrasound examination system was also implemented to demonstrate that real-time image synthesis with the proposed technique can be successfully integrated into a haptic simulation.","Pixel,
Ultrasonic imaging,
Imaging phantoms,
Image generation,
Interpolation,
Acceleration,
Probes,
Deformable models,
Finite element methods,
Mutual information"
Gesture-recognizing hand-held interface with vibrotactile feedback for 3D interaction,"This article presents a hand-held interface system for 3D interaction with digital media contents. The system is featured with 1) tracking of the full 6 degrees-of-freedom position and orientation of a hand-held controller, 2) robust gesture recognition using continuous hidden Markov models based on the acceleration and position measurements, and 3) dual-mode vibrotactile feedback using both vibration motor and voice-coil actuator. We also demonstrate the advantages of the system through a usability experiment.","Feedback,
Robust control,
Speech recognition,
Hidden Markov models,
Accelerometers,
Acceleration,
Position measurement,
Vibration control,
Vibration measurement,
Actuators"
Development of a miniature self-stabilization jumping robot,"We present the design and implementation of a new jumping robot for mobile sensor network. Unlike other jumping robots, the robot is based on a simple two-mass-spring model. After we throw it on ground, it can stabilize itself and then jump once. The detailed mechanism design including the load holding and self-stabilization are presented. Jumping heights and distances with different robot weights are measured and compared with calculated values from the two-mass-spring model.",
A Collaborative sensor-fault detection scheme for robust distributed estimation in sensor networks,"This work addresses the problem of robust distributed estimation in the presence of sensor faults when the fusion center sequentially receives quantized messages from local sensors. The mean square error (MSE) of distributed estimation schemes increases dramatically if the information received from the faulty sensors within the network is not excluded from the estimation process. Accordingly, an efficient collaborative sensor-fault detection (CSFD) scheme is proposed in which the results of a homogeneity test are used to identify the faulty nodes within the network such that their quantized messages can be filtered out when estimating the parameter of interest. Utilizing an asymptotic analytical technique, a lower bound is derived for the MSE of the proposed distributed estimation scheme. A good agreement is observed between the simulated MSE results and the lower bound values, and thus it is inferred that the lower bound provides a convenient and reliable means of predicting the performance of the proposed estimation scheme in real-world sensor networks. In addition, a low-complexity CSFD (LC-CSFD) scheme is proposed to identify faulty sensors in WSNs with a very large number of nodes. The simulation results confirm that the accuracy of the estimates obtained from the CSFD and LCCSFD schemes is significantly better than that obtained from a conventional estimation scheme when applied in sensor networks characterized by an unknown number of sensor faults of various types.","Collaboration,
Robustness,
Sensor phenomena and characterization,
Sensor fusion,
Fault diagnosis,
Collaborative work,
Mean square error methods,
Fault detection,
Testing,
Parameter estimation"
On complex event processing for sensor networks,"Sensor networks have to cope with a high volume of events continuously. Conventional software architectures do not explicitly target the efficient processing of continuous event streams. Recently, event-driven architectures (EDA) have been proposed as a new paradigm for event-based applications. In this paper we propose a reference architecture for sensor-based networks, which enables the analysis and processing of complex event streams in real-time. Our approach is based on semantically rich event models using ontologies that allow representation of structural properties of event types and constraints between them. Then, we argue in favor of a declarative approach of complex event processing that draws upon rule languages such as Esper and JESS. Also, we illustrate our approach in the domain of road traffic management for the high-capacity road network in Bilbao, Spain.",
Dual-Lattice Ordering and Partial Lattice Reduction for SIC-Based MIMO Detection,"In this paper, we propose low-complexity lattice detection algorithms for successive interference cancelation (SIC) in multi-input multi-output (MIMO) communications. First, we present a dual-lattice view of the vertical Bell Labs Layered Space-Time (V-BLAST) detection. We show that V-BLAST ordering is equivalent to applying sorted QR decomposition to the dual basis, or equivalently, applying sorted Cholesky decomposition to the associated Gram matrix. This new view results in lower detection complexity and allows simultaneous ordering and detection. Second, we propose a partial reduction algorithm that only performs lattice reduction for the last several, weak substreams, whose implementation is also facilitated by the dual-lattice view. By tuning the block size of the partial reduction (hence the complexity), it can achieve a variable diversity order, hence offering a graceful tradeoff between performance and complexity for SIC-based MIMO detection. Numerical results are presented to compare the computational costs and to verify the achieved diversity order.","Lattices,
MIMO,
Decoding,
Interference cancellation,
Gallium nitride,
Detection algorithms,
Silicon carbide,
Matrix decomposition,
Computational efficiency,
Quadrature amplitude modulation"
Defeating Dynamic Data Kernel Rootkit Attacks via VMM-Based Guest-Transparent Monitoring,"Targeting the operating system kernel, the core of trust in a system, kernel rootkits are able to compromise the entire system, placing it under malicious control, while eluding detection efforts. Within the realm of kernel rootkits, dynamic data rootkits are particularly elusive due to the fact that they attack only data targets. Dynamic data rootkits avoid code injection and instead use existing kernel code to manipulate kernel data. Because they do not execute any new code, they are able to complete their attacks without violating kernel code integrity. We propose a prevention solution that blocks dynamic data kernel rootkit attacks by monitoring kernel memory access using virtual machine monitor (VMM) policies. Although the VMM is an external monitor, our system preemptively detects changes to monitored kernel data states and enables fine-grained inspection of memory accesses on dynamically changing kernel data. In addition, readable and writable kernel data can be protected by exposing the illegal use of existing code by dynamic data kernel rootkits.We have implemented a prototype of our system using the QEMU VMM. Our experiments show that it successfully defeats synthesized dynamic data kernel rootkits in real-time, demonstrating its effectiveness and practicality.","Kernel,
Monitoring,
Protection,
Operating systems,
Control systems,
Computer science,
Manipulator dynamics,
Virtual machine monitors,
Data structures,
Availability"
MSMOTE: Improving Classification Performance When Training Data is Imbalanced,"Learning from data sets that contain very few instances of the minority class usually produces biased classifiers that have a higher predictive accuracy over the majority class, but poorer predictive accuracy over the minority class. SMOTE (Synthetic Minority Over-sampling Technique) is specifically designed for learning from imbalanced data sets. This paper presents a modified approach (MSMOTE) for learning from imbalanced data sets, based on the SMOTE algorithm. MSMOTE not only considers the distribution of minority class samples, but also eliminates noise samples by adaptive mediation. The combination of MSMOTE and AdaBoost are applied to several highly and moderately imbalanced data sets. The experimental results show that the prediction performance of MSMOTE is better than SMOTEBoost in the minority class and F-values are also improved.",
RDF Data-Centric Storage,"The vision of the Semantic Web has brought about new challenges at the intersection of web research and data management. One fundamental research issue at this intersection is the storage of the Resource Description Framework (RDF) data: the model at the core of the Semantic Web. We present a data-centric approach for storage of RDF in relational databases. The intuition behind our approach is that each RDF dataset requires a tailored table schema that achieves efficient query processing by (1) reducing the need for joins in the query plan and (2) keeping null storage below a given threshold. Using a basic structure derived from the RDF data, we propose a two-phase algorithm involving clustering and partitioning. The clustering phase aims to reduce the need for joins in a query. The partitioning phase aims to optimize storage of extra (i.e., null) data in the underlying relational database. Our approach does not assume a particular query workload, relevant for RDF knowledge bases with a large number of ad-hoc queries. Extensive experimental evidence using three publicly available real-world RDF data sets (i.e., DBLP, DBPedia, and Uniprot) shows that our schema creation technique provides superior query processing performance compared to state-of-the art storage approaches. Further, our approach is easily implemented, and complements existing RDF-specific databases.",
Automatically Classifying Documents by Ideological and Organizational Affiliation,We show how an Arabic language religious-political document can be automatically classified according to the ideological stream and organizational affiliation that it represents. Tests show that our methods achieve near-perfect accuracy.,"Text categorization,
Computer science,
Testing,
Security,
Law enforcement,
Monitoring,
Frequency,
Algorithm design and analysis"
The Minimum Distance of Turbo-Like Codes,"Worst-case upper bounds are derived on the minimum distance of parallel concatenated turbo codes, serially concatenated convolutional codes, repeat-accumulate codes, repeat-convolute codes, and generalizations of these codes obtained by allowing nonlinear and large-memory constituent codes. It is shown that parallel-concatenated turbo codes and repeat-convolute codes with sub-linear memory are asymptotically bad. It is also shown that depth-two serially concatenated codes with constant-memory outer codes and sublinear-memory inner codes are asymptotically bad. Most of these upper bounds hold even when the convolutional encoders are replaced by general finite-state automata encoders. In contrast, it is proven that depth-three serially concatenated codes obtained by concatenating a repetition code with two accumulator codes through random permutations can be asymptotically good.","Concatenated codes,
Turbo codes,
Convolutional codes,
Upper bound,
Automata,
Computer science,
Mathematics,
Random number generation"
Dynamic Load Balancing and Job Replication in a Global-Scale Grid Environment: A Comparison,"Global-scale grids provide a massive source of processing power, providing the means to support processor intensive parallel applications. The strong burstiness and unpredictability of the available processing and network resources raise the strong need to make applications robust against the dynamics of grid environments. The two main techniques that are most suitable to cope with the dynamic nature of the grid are dynamic load balancing (DLB) and job replication (JR). In this paper, we analyze and compare the effectiveness of these two approaches by means of trace-driven simulations. We observe that there exists an easy-to-measure statistic Y and a corresponding threshold value Y*, such that DLB consistently outperforms JR when Y > Y*, whereas the reverse is true for Y < Y*. Based on this observation, we propose a simple and easy-to-implement approach, throughout referred to as the DLB/JR method, that can make dynamic decisions about whether to use DLB or JR. Extensive simulations based on a large set of real data monitored in a global-scale grid show that our DLB/JR method consistently performs at least as good as both DLB and JR in all circumstances, which makes our DLB/JR method highly robust against the unpredictable nature of global-scale grids.","Load management,
Analytical models,
Robustness,
Concurrent computing,
Testing,
Performance evaluation,
Statistics,
Monitoring,
Bandwidth,
Runtime"
Oblivious Routing in On-Chip Bandwidth-Adaptive Networks,"Oblivious routing can be implemented on simple router hardware, but network performance suffers when routes become congested. Adaptive routing attempts to avoid hot spots by re-routing flows, but requires more complex hardware to determine and configure new routing paths. We propose onchip bandwidth-adaptive networks to mitigate the performance problems of oblivious routing and the complexity issues of adaptive routing. In a bandwidth-adaptive network, the bisection bandwidth of network can adapt to changing network conditions. We describe one implementation of a bandwidth-adaptive network in the form of a two-dimensional mesh with adaptive bidirectional links, where the bandwidth of the link in one direction can be increased at the expense of the other direction. Efficient local intelligence is used to reconfigure each link, and this reconfiguration can be done very rapidly in response to changing traffic demands. We compare the hardware designs of a unidirectional and bidirectional link and evaluate the performance gains provided by a bandwidth-adaptive network in comparison to a conventional network under uniform and bursty traffic when oblivious routing is used.","Routing,
Network-on-a-chip,
Hardware,
Bandwidth,
Telecommunication traffic,
Performance gain,
Traffic control,
Parallel architectures,
Computer science,
Artificial intelligence"
Distributed wake-up scheduling for data collection in tree-based wireless sensor networks,"In a multi-hop wireless network, a conventional way of defining interference neighbors is to prohibit a node from using the same slot/code as those of its 1-hop and 2-hop neighbors. However, for data collection in a wireless sensor network, since the set of communication nodes is limited and the transmission directions are toward the sink, we show that a less strict set of interference neighbors can be defined. Based on this observation, we develop an efficient distributed wake-up scheduling scheme for data collection in a sensor network that achieves both energy conservation and low reporting latency.","Wireless sensor networks,
Interference,
Delay,
Switches,
Spread spectrum communication,
Energy conservation,
Wireless application protocol,
Processor scheduling,
Computer science,
Relays"
Leakage Aware Feasibility Analysis for Temperature-Constrained Hard Real-Time Periodic Tasks,"As semiconductor technology continues to evolve, the chip temperature increases rapidly due to the exponentially growing power consumption. In the meantime, the high chip temperature increases the leakage power, which is becoming the dominate part in the overall power consumption for sub-micron IC circuits. A power/thermal-aware computing technique becomes ineffective if this temperature/leakage relation is not properly addressed in the sub-micron domain.In this paper, we study the feasibility problem for scheduling a hard real-time periodic task set under the peak temperature constraint, with the interaction between temperature and leakage being taken into consideration. Three analysis techniques are developed to guarantee the schedulability of periodic real-time task sets under the maximal temperature constraint. Our experiments, based on technical parameters from a processor using the 65nm technology, show that the feasibility analysis without considering the interactions between temperature and leakage can be significantly overoptimistic.","Energy consumption,
Optimal scheduling,
Processor scheduling,
Real time systems,
Power system modeling,
Circuits,
Electronic packaging thermal management,
Temperature dependence,
Scheduling algorithm,
Computer science"
Robust codiagnosability of discrete event systems,"We consider robust decentralized diagnosis of discrete event systems, where the goal is to detect the occurrence of unobservable fault events using a set of local diagnosers that are themselves subject to failures. We introduce a formal notion of robust decentralized diagnosability, called robust codiagnosability, and study its properties. Two different tests of robust codiagnosability are presented; one uses diagnoser automata and the other uses verifier automata. We also revisit the problem of centralized diagnosability and study the problem of diagnosability under partial observation, where the set of observable events is reduced; in this regard, we introduce the notions of partial diagnosers and indeterminate hidden cycles, which are subsequently used in the study of robust codiagnosability.","Robustness,
Discrete event systems,
Automata,
Fault diagnosis,
Engines,
Robust control,
Automatic testing,
Computer architecture,
Brazil Council,
Control systems"
"Mixed Reality Humans: Evaluating Behavior, Usability, and Acceptability","This paper presents mixed reality humans (MRHs), a new type of embodied agent enabling touch-driven communication. Affording touch between human and agent allows MRHs to simulate interpersonal scenarios in which touch is crucial. Two studies provide initial evaluation of user behavior with a MRH patient and the usability and acceptability of a MRH patient for practice and evaluation of medical students' clinical skills. In Study I (n = 8) it was observed that students treated MRHs as social actors more than students in prior interactions with virtual human patients (n = 27), and used interpersonal touch to comfort and reassure the MRH patient similarly to prior interactions with human patients (n = 76). In the within-subjects Study II (n = 11), medical students performed a clinical breast exam on each of a MRH and human patient. Participants performed equivalent exams with the MRH and human patients, demonstrating the usability of MRHs to evaluate students' exam skills. The acceptability of the MRH patient for practicing exam skills was high as students rated the experience as believable and educationally beneficial. Acceptability was improved from Study I to Study II due to an increase in the MRH's visual realism, demonstrating that visual realism is critical for simulation of specific interpersonal scenarios.","Virtual reality,
Humans,
Usability,
Medical simulation,
Biomedical imaging,
Image processing,
Computational modeling,
Technological innovation,
Medical treatment,
Breast"
Automatic Quantification of Joint Space Narrowing and Erosions in Rheumatoid Arthritis,"Rheumatoid arthritis (RA) is a chronic disease that affects and potentially destroys the joints of the appendicular skeleton. The precise and reproducible quantification of the progression of joint space narrowing and the erosive bone destructions caused by RA is crucial during treatment and in imaging biomarkers in clinical trials. Current manual scoring methods exhibit high interreader variability, even after intensive training, and thus, impede the efficient monitoring of the disease. We propose a fully automatic quantitative assessment of the radiographic changes that result from RA, to increase the accuracy, reproducibility, and speed of image interpretation. Initial joint location estimates are obtained by local linear mappings based on texture features. Bone contours are delineated by active shape models comprised of statistical models of bone shape and local texture. These models are refined by snakes which increase the accuracy and allow for a fitting of pathological deviations from the training population. The method then measures joint space widths and detects erosions on the bone contour. Joint space widths are measured with a coefficient of variation of 2%-7% for repeated measurements and erosion detection exhibits an area under the receiver operating characteristic (ROC) curve of 0.89. Model landmarks serve as a reference system along the contour. These landmarks enable the definition of joint regions and more specific follow-up monitoring. The automatic quantification allows for a remote analysis, relevant for multicenter clinical trials, and reduces the workload of clinical experts since parts of the process can be managed by nonexpert personnel.","Arthritis,
Joints,
Active shape model,
Bone diseases,
Clinical trials,
Extraterrestrial measurements,
Area measurement,
Skeleton,
Biomarkers,
Impedance"
Smoke Detection in Video,"In this paper, we propose a method for smoke detection in outdoor video sequences. We assume that the camera is mounted on a pan/tilt device.The proposed method is composed of three steps. The first step is to decide whether the camera is moving or not. While the camera is moving, we skip the ensuing steps. Otherwise, the second step is to detect the areas of change in the current input frame against the background image and to locate regions of interest (ROIs) by connected component analysis. The block-based approach is applied in both the first and second steps. In the final step, we decide whether the detected ROI is smoke by using the k- temporal information of its color and shape extracted from the ROI. We show the experimental results using in the forest surveillance videos.","Smoke detectors,
Fires,
Cameras,
Infrared detectors,
Motion detection,
Shape,
Computer science,
Image segmentation,
Monitoring,
Digital images"
Implementing logical synchrony in integrated modular avionics,"Many avionics systems must be implemented as redundant, distributed systems in order to provide the necessary level of fault tolerance. To correctly perform their function, the individual nodes of these systems must agree on some part of the global system state. Developing protocols to achieve this agreement is greatly simplified if the nodes execute synchronously relative to each other, but many Integrated Modular Avionics architectures assume nodes will execute asynchronously. This paper presents a simple design pattern, Physically Asynchronous/Logically Synchronous (PALS), that allows developers to design and verify a distributed, redundant system as though all nodes execute synchronously. This synchronous design can then be distributed over a physically asynchronous architecture in such a way that the logical correctness of the design is preserved. Use of this complexity reducing design pattern greatly simplifies the development and verification of fault tolerant distributed applications, ensures optimal system performance, and provides a standard argument for system certification.","Aerospace electronics,
Synchronization,
Clocks,
Logic,
Fault tolerant systems,
Protocols,
System performance,
Standards development,
Certification,
Fault tolerance"
Adaptive Management of Virtualized Resources in Cloud Computing Using Feedback Control,"Cloud computing as newly emergent computing environment offers dynamic flexible infrastructures and QoS guaranteed services in pay-as-you-go manner to the public. System virtualization technology which renders flexible and scalable system services is the base of the cloud computing. How to provide a self-managing and autonomic infrastructure for cloud computing through virtualization becomes an important challenge. In this paper, using feedback control theory, we present VM-based architecture for adaptive management of virtualized resources in cloud computing and model an adaptive controller that dynamically adjusts multiple virtualized resources utilization to achieve application Service Level Objective (SLO) in cloud computing. Compared with Xen, KVM is chosen as a virtual machine monitor (VMM) to implement the architecture. Evaluation of the proposed controller model showed that the model could allocate resources reasonably in response to the dynamically changing resource requirements of different applications which execute on different VMs in the virtual resource pool to achieve applications SLOs.","Programmable control,
Adaptive control,
Resource management,
Resource virtualization,
Cloud computing,
Feedback control,
Computer architecture,
Application virtualization,
Virtual machine monitors,
Voice mail"
SourcererDB: An aggregated repository of statically analyzed and cross-linked open source Java projects,"Abstract The open source movement has made vast quantities of source code available online for free, providing an extremely large dataset for empirical study and potential resuse. A major difficulty in exploiting this potential fully is that the data are currently scattered between competing source code repositories, none of which are structured for empirical analysis and cross-project comparison. As a result, software researchers and developers are left to compile their own datasets, resulting in duplicated effort and limited results. To address this challenge, we built SourcererDB, an aggregated repository of statically analyzed and cross-linked open source Java projects. SourcererDB contains local snapshots of 2,852 Java projects taken from Sourceforge, Apache and Java.net. These projects are statically analyzed to extract rich structural information, which is then stored in a relational database. References to entities in the 16,058 external jars are resolved and grouped, allowing for cross-project usage information to be accessed easily. This paper describes: (a) the mechanism for resolving and grouping these cross-project references, (b) the structure of and the metamodel for the SourcererDB repository, and (d) end-user dataset access mechanisms. Our goal in building SourcererDB is to provide a rich dataset of source code to facilitate the sharing of extracted data and to encourage reuse and repeatability of experiments.","Java,
Open source software,
Data mining,
Information analysis,
Search engines,
Collaboration,
Solids,
Standardization,
Scattering,
Relational databases"
Applying moving windows to software effort estimation,"Models for estimating software development effort are commonly built and evaluated using a set of historical projects. An important question is which projects to use as training data to build the model: should it be all of them, or a subset that seems particularly relevant? One factor to consider is project age: is it best to use the entire history of past projects, or is it more appropriate in a rapidly changing world to use a window of recent projects? We investigate the effect on estimation accuracy of using a moving window, using projects from the ISBSG data set. We find that using a moving window can improve accuracy, and we make some observations about factors that influence the range of possible window sizes and the best window size.","Training data,
Software engineering,
Software measurement,
Programming,
Testing,
Predictive models,
Computer science,
History,
Size measurement,
Application software"
Concurrent Error Detection and Correction in Gaussian Normal Basis Multiplier over GF(2^m),"Fault-based cryptanalysis has been developed to effectively break both private-key and public-key cryptosystems, making robust finite field multiplication a very important research topic in recent years. However, no robust normal basis multiplier has been proposed in the literature. Therefore, this investigation presents a semisystolic Gaussian normal basis multiplier. Based on the proposed Gaussian normal basis multiplier, both concurrent error detection and correction capabilities can be easily achieved using time redundancy technology with no hardware modification.","Gaussian processes,
Cryptography,
Elliptic curve cryptography,
Circuit faults,
Finite element methods,
Galois fields,
Complexity theory"
Quantifying ruggedness of continuous landscapes using entropy,"A major unsolved problem in the field of optimisation and computational intelligence is how to determine which algorithms are best suited to solving which problems. This research aims to analytically characterise individual problems as a first step towards attempting to link problem types with the algorithms best suited to solving them. In particular, an information theoretic technique for analysing the ruggedness of a fitness landscape with respect to neutrality was adapted to work in continuous landscapes and to output a single measure of ruggedness. Experiments run on test functions with increasing ruggedness show that the proposed measure of ruggedness produced relative values consistent with a visual inspection of the problem landscapes. Combined with other measures of complexity, the proposed ruggedness measure could be used to more broadly characterise the complexity of fitness landscapes in continuous domains.","Entropy,
Particle measurements,
Polynomials,
Computational intelligence,
Algorithm design and analysis,
Information analysis,
Testing,
Inspection,
Prediction algorithms,
Labeling"
People modify their tutoring behavior in robot-directed interaction for action learning,"In developmental research, tutoring behavior has been identified as scaffolding infants' learning processes. It has been defined in terms of child-directed speech (Motherese), child-directed motion (Motionese), and contingency. In the field of developmental robotics, research often assumes that in human-robot interaction (HRI), robots are treated similar to infants, because their immature cognitive capabilities benefit from this behavior. However, according to our knowledge, it has barely been studied whether this is true and how exactly humans alter their behavior towards a robotic interaction partner. In this paper, we present results concerning the acceptance of a robotic agent in a social learning scenario obtained via comparison to adults and 8-11 months old infants in equal conditions. These results constitute an important empirical basis for making use of tutoring behavior in social robotics. In our study, we performed a detailed multimodal analysis of HRI in a tutoring situation using the example of a robot simulation equipped with a bottom-up saliency-based attention model [1]. Our results reveal significant differences in hand movement velocity, motion pauses, range of motion, and eye gaze suggesting that for example adults decrease their hand movement velocity in an Adult-Child Interaction (ACI), opposed to an Adult-Adult Interaction (AAI) and this decrease is even higher in the Adult-Robot Interaction (ARI). We also found important differences between ACI and ARI in how the behavior is modified over time as the interaction unfolds. These findings indicate the necessity of integrating top-down feedback structures into a bottom-up system for robots to be fully accepted as interaction partners.","Human robot interaction,
Cognitive robotics,
Pediatrics,
Speech,
Robot kinematics,
Informatics,
Business communication,
Information science,
Europe,
Performance analysis"
Reverse Furthest Neighbors in Spatial Databases,"Given a set of points
P
and a query point
q
, the {\em reverse furthest neighbor} (\trfn) query fetches the set of points
p∈P
such that
q
is their furthest neighbor among all points in
P∪{q}
. This is the monochromatic \trfn (\mrfn) query. Another interesting version of \trfn query is the {\em bichromatic reverse furthest neighbor} (\brfn) query. Given a set of points
P
, a query set
Q
and a query point
q∈Q
, a \brfn query fetches the set of points
p∈P
such that
q
is the furthest neighbor of
p
among all points in
Q
. The \trfn query has many interesting applications in spatial databases and beyond. For instance, given a large residential database (as
P
) and a set of potential sites (as
Q
) for building a chemical plant complex, the construction site should be selected as the one that has the maximum number of reverse furthest neighbors. This is an instance of the \brfn query. This paper presents the challenges associated with such queries and proposes efficient, R-tree based algorithms for both monochromatic and bichromatic versions of the \trfn queries. We analyze properties of the \trfn query that differentiate it from the widely studied reverse nearest neighbor queries and enable the design of novel algorithms. Our approach takes advantage of the furthest Voronoi diagrams as well asthe convex hulls of either the data set
P
(in the \mrfn case) or the query set
Q
(in the \brfn case). For the \brfn queries, we also extend the analysis to the situation when
Q
is large in size and becomes disk-resident. Experiments on both synthetic and real data sets confirm the efficiency and scalability of proposed algorithmsover the brute-force search based approach.","Spatial databases,
Algorithm design and analysis,
Data engineering,
Computer science,
USA Councils,
Buildings,
Chemicals,
Nearest neighbor searches,
Scalability,
Query processing"
Novel low power full adder cells in 180nm CMOS technology,"This paper proposes four low power adder cells using different XOR and XNOR gate architectures. Two sets of circuit designs are presented. One implements full adders with 3 transistors (3-T) XOR and XNOR gates. The other applies Gate-Diffusion-Input (GDI) technique to full adders. Simulations are performed by using Hspice based on 180nm CMOS technology. In comparison with Static Energy Recovery Full (SERF) adder cell module, the proposed four full adder cells demonstrate their advantages, including lower power consumption, smaller area, and higher speed.","CMOS technology,
Adders,
Equations,
Logic,
Microelectronics,
Energy consumption,
Paper technology,
Circuit synthesis,
Circuit simulation,
Computer science"
3-Approximation algorithm for joint routing and link scheduling in wireless relay networks,"In emerging wireless relay networks (WRNs) such as IEEE 802.16j, efficient resource allocation is becoming a substantial issue for throughput optimization. In this paper, we propose an algorithm for joint routing and link scheduling in WRNs. The developed theoretical analysis indicates that the performance of the proposed algorithm is within a factor of three of that of any optimal algorithm in the worst case. Through simulation experiments, the numerical results show that our algorithm outperforms the previously proposed routing and link-scheduling algorithms. Furthermore, the proposed algorithm can effectively achieve near-optimal performance, and provide much better throughput than the theoretical worst-case bound in the average case.","Scheduling algorithm,
Routing,
Relays,
Throughput,
Spread spectrum communication,
Quality of service,
Computer science,
Processor scheduling,
Resource management,
Performance analysis"
Learning a Channelized Observer for Image Quality Assessment,"It is now widely accepted that image quality should be evaluated using task-based criteria, such as human-observer performance in a lesion-detection task. The channelized Hotelling observer (CHO) has been widely used as a surrogate for human observers in evaluating lesion detectability. In this paper, we propose that the problem of developing a numerical observer can be viewed as a system-identification or supervised-learning problem, in which the goal is to identify the unknown system of the human observer. Following this approach, we explore the possibility of replacing the Hotelling detector within the CHO with an algorithm that learns the relationship between measured channel features and human observer scores. Specifically, we develop a channelized support vector machine (CSVM) which we compare to the CHO in terms of its ability to predict human-observer performance. In the examples studied, we find that the CSVM is better able to generalize to unseen images than the CHO, and therefore may represent a useful improvement on the CHO methodology, while retaining its essential features.","Image quality,
Humans,
Lesions,
Support vector machines,
Biomedical imaging,
Medical diagnostic imaging,
Detectors,
Machine learning,
Machine learning algorithms,
Image processing"
3D reconstruction based on underwater video from ROV Kiel 6000 considering underwater imaging conditions,"This work presents a system for 3D reconstruction from underwater images or video. Aside from a camera in an underwater housing, no special equipment is required. However, if navigational data is available, it is utilized in the algorithm. The algorithm is in essence a classical structure from motion approach, which is adapted to account for the special imaging conditions. Hence, there is no need for the camera to follow a specialized trajectory. Adaptions to the underwater imaging environment include a special filtering of background and floating particles, which allows a robust estimation of the camera poses and a sparse set of 3D points. Based on the estimated camera track, dense image correspondence computation enables building a detailed 3D surface model. Once the 3D surface model is completed, the colors of the texture are corrected by a physical model for underwater light propagation, allowing to view the model without the effects of scattering and attenuation or to simulate the effects of water on light in a 3D viewer.","Image reconstruction,
Remotely operated vehicles,
Cameras,
Navigation,
Filtering,
Robustness,
Underwater tracking,
Surface texture,
Color,
Optical propagation"
Developing DCE-CT to Quantify Intra-Tumor Heterogeneity in Breast Tumors With Differing Angiogenic Phenotype,"The objective of this study is to evaluate the ability of dynamic contrast enhanced computed tomography (DCE-CT) to assess intratumor physiological heterogeneity in tumors with different angiogenic phenotypes. DCE-CT imaging was performed on athymic nude mice bearing xenograft wild type (MCF-7neo) and VEGF-transfected (MCF-7VEGF) tumors by using a clinical multislice CT, and compared to skeletal muscle. Parametrical maps of tumor physiology-perfusion (F), permeability-surface area (PS), fractional intravascular plasma (fp), and interstitial space ( fis)-were obtained by fitting the time-dependent contrast-enhanced curves to a two-compartmental kinetic model for each voxel (0.3 x 0.3 x 0.75 mm3). Mean physiological measurements were compared with positron emission tomography (PET) imaging, and the spatial distribution of tumor vasculature was compared with histology. No statistically significant difference was found in mean physiological values of F, PS, and fp in MCF-7neo and muscle, while fis of MCF-7neo was a factor of two higher (p<0.04). MCF-7neo tumors also showed a radial heterogeneity with significant higher physiological values in periphery than those in middle and core regions (p<0.01 for all physiological parameters). MCF-7VEGF tumors demonstrated significant increases in all physiological parameters compared with MCF-7neo tumors, and a distinct saccular heterogeneous pattern compared with MCF-7neo and muscle. Both PET imaging and histological results showed good correlation with the above results for this same mouse model. No statistically significant difference was found in the mean perfusion and intravascular volume measured by PET imaging and DCE-CT. Increases in cross-sectional area of blood vessels (p<0.002) were observed in MCF-7VEGF tumors than MCF-7neo, and their spatial distribution correlated well with the spatial distribution of fp obtained by DCE-CT. The results of this study demonstrated the feasibility of DCE-CT in quantification of spatial heterogeneity in tumor physiology in small animal models. Monitoring variations in the tumor environment using DCE-CT offers an in vivo tool for the evaluation and optimization of new therapeutic strategies.","Breast tumors,
Biomedical monitoring,
Positron emission tomography,
Muscles,
Computed tomography,
Mice,
Breast neoplasms,
Plasma measurements,
Curve fitting,
Kinetic theory"
Quantitative Analysis of Dynamic Contrast-Enhanced MR Images Based on Bayesian P-Splines,"Dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) is an important tool for detecting subtle kinetic changes in cancerous tissue. Quantitative analysis of DCE-MRI typically involves the convolution of an arterial input function (AIF) with a nonlinear pharmacokinetic model of the contrast agent concentration. Parameters of the kinetic model are biologically meaningful, but the optimization of the nonlinear model has significant computational issues. In practice, convergence of the optimization algorithm is not guaranteed and the accuracy of the model fitting may be compromised. To overcome these problems, this paper proposes a semi-parametric penalized spline smoothing approach, where the AIF is convolved with a set of B-splines to produce a design matrix using locally adaptive smoothing parameters based on Bayesian penalized spline models (P-splines). It has been shown that kinetic parameter estimation can be obtained from the resulting deconvolved response function, which also includes the onset of contrast enhancement. Detailed validation of the method, both with simulated and in vivo data, is provided.","Image analysis,
Bayesian methods,
Biological system modeling,
Kinetic theory,
Magnetic analysis,
Spline,
Smoothing methods,
Magnetic resonance imaging,
Cancer detection,
Convolution"
Multicore power management: Ensuring robustness via early-stage formal verification,"Dynamic power management (DPM) is important for multicore architectures. One important challenge for multicore DPM schemes is verifying that they are both safe (cannot lead to power or thermal catastrophes) and efficient (achieve as much performance as possible without exceeding power constraints). The verification difficulty varies among designs, depending, for example, on the particular power management mechanisms utilized and the algorithms used to adjust them. However, verification effort is often not considered in the early stages of DPM scheme design, leading to proposals that can be extremely difficult to verify. To address this problem, we propose using formal verification (with probabilistic model checking) of a high-level, early-stage model of the DPM scheme. Using the model checker, we estimate the required verification effort, providing insight on how certain design parameters impact this effort. Furthermore, we supplement the verifiability results with high-level estimates of power consumption and performance, which allow us to perform a trade-off analysis between power, performance, and verification. We show that this trade-off analysis uncovers design points that are better than those that consider only power and performance.","Multicore processing,
Energy management,
Robustness,
Formal verification,
Power system management,
Computer bugs,
Computational modeling,
Performance analysis,
Power system dynamics,
Battery management systems"
A Latent Topic Model for Complete Entity Resolution,"In bibliographies like DBLP and Citeseer, there are three kinds of entity-name problems that need to be solved. First, multiple entities share one name, which is called the name sharing problem. Second, one entity has different names, which is called the name variant problem. Third, multiple entities share multiple names, which is called the name mixing problem. We aim to solve these problems based on one model in this paper. We call this task complete entity resolution. Different from previous work, our work use global information based on data with two types of information, words and author names. We propose a generative latent topic model that involves both author names and words — the LDA-dual model, by extending the LDA (Latent Dirichlet Allocation) model. We also propose a method to obtain model parameters that is global information. Based on obtained model parameters, we propose two algorithms to solve the three problems mentioned above. Experimental results demonstrate the effectiveness and great potential of the proposed model and algorithms.","Bibliographies,
Web pages,
Data engineering,
Computer science,
Linear discriminant analysis,
Cleaning,
Couplings,
Collaboration"
Towards a better understanding of software evolution: An empirical study on open source software,"Software evolution is a fact of life. Over the past thirty years, researchers have proposed hypotheses on how software changes, and provided evidence that both supports and refutes these hypotheses. To paint a clearer image of the software evolution process, we performed an empirical study on long spans in the lifetime of seven open source projects. Our analysis covers 653 official releases, and a combined 69 years of evolution. We first tried to verify Lehman's laws of software evolution. Our findings indicate that several of these laws are confirmed, while the rest can be either confirmed or infirmed depending on the laws' operational definitions. Second, we analyze the growth rate for projects' development and maintenance branches, and the distribution of software changes. We find similarities in the evolution patterns of the programs we studied, which brings us closer to constructing rigorous models for software evolution.","Open source software,
Software quality,
Costs,
Paints,
Software maintenance,
Application software,
Computer science,
Software performance,
Software systems,
Software development management"
3D pose estimation of vehicles using a stereo camera,"This study introduces an approach to three-dimensional vehicle pose estimation using a stereo camera system. After computation of stereo and optical flow on the investigated scene, a four-dimensional clustering approach separates the static from the moving objects in the scene. The iterative closest point algorithm (ICP) estimates the vehicle pose using a cuboid as a weak vehicle model. In contrast to classical ICP optimisation a polar distance metric is used which especially takes into account the error distribution of the stereo measurement process. The tracking approach is based on tracking-by-detection such that no temporal filtering is used. The method is evaluated on seven different real-world sequences, where different stereo algorithms, baseline distances, distance metrics, and optimisation algorithms are examined. The results show that the proposed polar distance metric yields a higher accuracy for yaw angle estimation of vehicles than the common Euclidean distance metric, especially when using pixel-accurate stereo points.","Vehicles,
Cameras,
Iterative closest point algorithm,
Layout,
Optical computing,
Optical filters,
Image motion analysis,
Filtering,
Optimization methods,
Yield estimation"
Joint network-wide opportunistic scheduling and power control in multi-cell networks,"We present a unified analytical framework that maximizes generalized utilities of a wireless network by network-wide opportunistic scheduling and power control. That is, base stations in the network jointly decide mobile stations to be served at the same time as the transmission powers of base stations are coordinated to mitigate the mutually interfering effect. Although the maximization at the first glance appears to be a mixed, twofold and nonlinear optimization requiring excessive computational complexity, we show that the maximization can be transformed into a pure binary optimization with much lower complexity. To be exact, it is proven that binary power control of base stations is necessary and sufficient for maximizing the network-wide utilities under a physical layer regime where the channel capacity is linear in the signal-to-interference-noise ratio. To further reduce the complexity of the problem, a distributed heuristic algorithm is proposed that performs much better than existing opportunistic algorithms. Through extensive simulations, it becomes clear that network-wide opportunistic scheduling and power control is most suitable for fairness-oriented networks and under loaded networks. We believe that our work will serve as a cornerstone for network-wide scheduling approaches from theoretical and practical standpoints.","Power control,
Job shop scheduling,
Wireless networks,
Throughput,
Base stations,
Quality of service,
Communication systems,
Councils,
Computational complexity,
Physical layer"
Layered Interval Codes for TCAM-Based Classification,"Ternary content-addressable memories (TCAMs) are increasingly used for high-speed packet classification. TCAMs compare packet headers against all rules in a classification database in parallel and thus provide high throughput. TCAMs are not well-suited, however, for representing rules that contain range fields and prior art algorithms typically represent each such rule by multiple TCAM entries. The resulting range expansion can dramatically reduce TCAM utilization because it introduces a large number of redundant TCAM entries. This redundancy can be mitigated by making use of extra bits, available in each TCAM entry. We present a scheme for constructing efficient representations of range rules, based on the simple observation that sets of disjoint ranges may be encoded much more efficiently than sets of overlapping ranges. Since the ranges in real-world classification databases are, in general, non-disjoint, the algorithms we present split ranges between multiple layers each of which consists of mutually disjoint ranges. Each layer is then coded independently and assigned its own set of extra bits. Our layering algorithms are based on approximations for specific variants of interval-graph coloring. We evaluate these algorithms by performing extensive comparative analysis on real-life classification databases. Our analysis establishes that our algorithms reduce the number of redundant TCAM entries caused by range rules by more than 60% as compared with best range-encoding prior art.","Databases,
Computer science,
Throughput,
Art,
Algorithm design and analysis,
Communications Society,
Performance evaluation,
Performance analysis,
Data analysis,
Internet"
SOA Middleware Support for Service Process Reconfiguration with End-to-End QoS Constraints,"In SOA, services may become volatile and fail to deliver the quality of service as requested by users. In this paper, we present an approach for repairing failed services by replacing them with new services and ensuring the new service process still meets the user specified end-to-end QoS constraints. An iterative structural inspection algorithm is designed to produce reconfiguration regions that include one or more failed service. By reconfiguring only services in the selected regions, the business process will not be affected significantly. The algorithm may also utilize those available QoS constraints to relax the original constraints of a reconfiguration region and to provide more effective reconfiguration solutions. We also present the middleware components to support the service reconfiguration in the LLAMA framework.","Service oriented architecture,
Middleware,
Quality of service,
Iterative algorithms,
Web services,
USA Councils,
Computer science,
Inspection,
Algorithm design and analysis,
Delay"
Recursive partitioning multicast: A bandwidth-efficient routing for Networks-on-Chip,"Chip Multi-processor (CMP) architectures have become mainstream for designing processors. With a large number of cores, Networks-on-Chip (NOCs) provide a scalable communication method for CMP architectures. NOCs must be carefully designed to meet constraints of power consumption and area, and provide ultra low latencies. Existing NOCs mostly use Dimension Order Routing (DOR) to determine the route taken by a packet in unicast traffic. However, with the development of diverse applications in CMPs, one-to-many (multicast) and one-to-all (broadcast) traffic are becoming more common. Current unicast routing cannot support multicast and broadcast traffic efficiently. In this paper, we propose Recursive Partitioning Multicast (RPM) routing and a detailed multicast wormhole router design for NOCs. RPM allows routers to select intermediate replication nodes based on the global distribution of destination nodes. This provides more path diversities, thus achieves more bandwidth-efficiency and finally improves the performance of the whole network. Our simulation results using a detailed cycle-accurate simulator show that compared with the most recent multicast scheme, RPM saves 25% of crossbar and link power, and 33% of link utilization with 50% network performance improvement. Also RPM is more scalable to large networks than the recently proposed VCTM.","Routing,
Network-on-a-chip,
Broadcasting,
Bandwidth,
Energy consumption,
Computer architecture,
Delay,
Unicast,
Telecommunication traffic,
Mesh networks"
Economic scheduling of distributed generators in a microgrid considering various constraints,"This paper deals with the economic dispatch problem in a microgrid to provide the optimal power reference of distributed generators. The objective function and the constraints related to the operation of the microgrid are formulated and the solution method for the problem is introduced. The constraints for this problem include i) additional reserve requirement due to the uncertainty in the output of renewable resources, ii) flow limit between the control areas, and iii) additional reserve for the stable operation during the islanded mode. In order to examine the effect of the constraints on the generation cost, some numerical experiments are conducted.","Control systems,
Centralized control,
Power generation economics,
Voltage control,
Temperature control,
Distributed power generation,
Power generation,
Moon,
Search methods,
Power system economics"
AutoSig-Automatically Generating Signatures for Applications,"Classifying network traffic according to the applications is important to a broad range of network areas. Compared with the traditional method which classifies traffic using predefined well-known port numbers, the method using application signatures is more accurate. Unfortunately, analyzing signatures and maintaining up-to-date signatures for various applications is very difficult. To solve the problem, the paper proposes AutoSig which is an automatic application signature generation system. AutoSig extracts multiple common substring sequences from sample flows as application signature. First all possible common substrings in an application protocol are extracted and then a substring tree is constructed to generate the final signature of the application. The method is evaluated on campus traffic traces, and the experiment results show that AutoSig can generate effective application signatures with very high accuracy automatically.","Telecommunication traffic,
Protocols,
Application software,
Information technology,
Inspection,
Computer networks,
Computer science,
Technology management,
Telecommunication network management,
Communication industry"
Mining Temporal Specifications from Object Usage,"A caller must satisfy the callee's precondition--that is, reach a state in which the callee may be called. Preconditions describe the state that needs to be reached, but not how to reach it. We combine static analysis with model checking to mine Computation Tree Logic (CTL) formulas that describe the operations a parameter goes through: ""In parseProperties(String xml), the parameter xml normally stems from getProperties()."" Such operational preconditions can be learned from program code, and the code can be checked for their violations. Applied to AspectJ, our Tikanga prototype found 189 violations of operational preconditions, uncovering 9 unique defects and 36 unique code smells---with 44% true positives in the 50 top-ranked violations.","Computer science,
Logic,
Software engineering,
Prototypes,
Programming profession,
Data flow computing,
Documentation"
Analysis on the Use of Multiple Returns LiDAR Data for the Estimation of Tree Stems Volume,"Small footprint Light Detection and Ranging (LiDAR) data have been shown to be a very accurate technology to predict stem volume. In particular, most recent sensors are able to acquire multiple return (more than 2) data at very high hit density, allowing one to have detailed characterization of the canopy. In this paper, we utilize very high density ( >8 hits per m2) LiDAR data acquired over a forest stand in Italy. Our approach was as follows: Individual trees were first extracted from the LiDAR data and a series of attributes from both the first, and non-first (multiple), hits associated with each crown were then extracted. These variables were then correlated with ground truth individual estimates of stem volume. Our results indicate that: (i) non-first returns are informative for the estimation of stem volume (in particular the second return); (ii) some attributes (e.g., maximum at the power of n) better emphasize the information content of returns different from the first respect to other metrics (e.g., minimum, mean); and (iii) the combined use of variables belonging to different returns slightly increases the overall model accuracy. Moreover, we found that the best model for stem volume estimation (adj - R2 = 0.77, P < 0.0001, SE = 0.06) comprised four variables belonging to three returns (first, second, and third). The results of this analysis are important as they underline the effectiveness of the use of multiple return LiDAR data, underling the connection between LiDAR hits different from the first and tree structure and characteristics.","Forestry,
Laser scanning,
Input variables"
Optical packet and burst switched networks: a review,"Over the past several years, a significant amount of research has been conducted in the areas of optical packet switching (OPS) and optical burst switching (OBS). This research has been motivated by the need for techniques that are capable of supporting the demanding requirements of emerging dynamic high-bandwidth network applications in a flexible and efficient manner. Although optical packet and burst switching have yet to be widely deployed in commercial settings, recent research progress indicates that such deployments are not infeasible in the near future. The authors review the literature on OPS and OBS. Basic concepts are discussed and an overview of current and emerging research issues and challenges for optical packet and burst switched networks is presented. The authors attempt to cover issues that are essential for the practical deployment of such networks and highlight promising research directions that are likely to facilitate such deployments.","packet switching,
optical burst switching,
optical fibre networks"
Large-scale object recognition with CUDA-accelerated hierarchical neural networks,"Robust recognition of arbitrary object classes in natural visual scenes is an aspiring goal with numerous practical applications, for instance, in the area of autonomous robotics and autonomous vehicles. One obstacle on the way towards human-like recognition performance is the limitation of computational power, restricting the size of the training and testing dataset as well as the complexity of the object recognition system. In this work, we present a hierarchical, locally-connected neural network model that is well-suited for large-scale, high-performance object recognition. By using the NVIDIA CUDA framework, we create a massively parallel implementation of the model which is executed on a state-of-the-art graphics card. This implementation is up to 82 times faster than a single-core CPU version of the system. This significant gain in computational performance allows us to evaluate the model on a very large, realistic, and challenging set of natural images which we extracted from the LabelMe dataset. To compare our model to other approaches, we also evaluate the recognition performance using the well-known MNIST and NORB datasets, achieving a testing error rate of 0.76% and 2.87 %, respectively.","Large-scale systems,
Object recognition,
Neural networks,
Robustness,
Layout,
Remotely operated vehicles,
Mobile robots,
System testing,
Power system modeling,
Graphics"
On the application of predictive control techniques for adaptive performance management of computing systems,"This paper addresses adaptive performance management of real-time computing systems. We consider a generic model-based predictive control approach that can be applied to a variety of computing applications in which the system performance must be tuned using a finite set of control inputs. The paper focuses on several key aspects affecting the application of this control technique to practical systems. In particular, we present techniques to enhance the speed of the control algorithm for real-time systems. Next we study the feasibility of the predictive control policy for a given system model and performance specification under uncertain operating conditions. The paper then introduces several measures to characterize the performance of the controller, and presents a generic tool for system modeling and automatic control synthesis. Finally, we present a case study involving a real-time computing system to demonstrate the applicability of the predictive control framework.","Predictive control,
Programmable control,
Adaptive control,
Real time systems,
Automatic control,
Predictive models,
Control systems,
Computer applications,
System performance,
Modeling"
Image registration by minimization of residual complexity,"Accurate definition of similarity measure is a key component in image registration. Most commonly used intensity-based similarity measures rely on the assumptions of independence and stationarity of the intensities from pixel to pixel. Such measures cannot capture the complex interactions among the pixel intensities, and often result in less satisfactory registration performances, especially in the presence of nonstationary intensity distortions. We propose a novel similarity measure that accounts for intensity non-stationarities and complex spatially-varying intensity distortions. We derive the similarity measure by analytically solving for the intensity correction field and its adaptive regularization. The final measure can be interpreted as one that favors a registration with minimum compression complexity of the residual image between the two registered images. This measure produces accurate registration results on both artificial and real-world problems that we have tested, whereas many other state-of-the-art similarity measures have failed to do so.","Image registration,
Distortion measurement,
Pixel,
Biomedical imaging,
Performance evaluation,
Image coding,
Testing,
Reflectivity,
Crops,
Gaussian processes"
"A stereo vision based mapping algorithm for detecting inclines, drop-offs, and obstacles for safe local navigation","Mobile robots have to detect and handle a variety of potential hazards to navigate autonomously. We present a real-time stereo vision based mapping algorithm for identifying and modeling various hazards in urban environments - we focus on inclines, drop-offs, and obstacles. In our algorithm, stereo range data is used to construct a 3D model consisting of a point cloud with a 3D grid overlaid on top. A novel plane fitting algorithm is then used to segment the 3D model into distinct potentially traversable ground regions and fit planes to the regions. The planes and segments are analyzed to identify safe and unsafe regions and the information is captured in an annotated 2D grid map called a local safety map. The safety map can be used by wheeled mobile robots for planning safe paths in their local surroundings. We evaluate our algorithm comprehensively by testing it in varied environments and comparing the results to ground truth data.",
Enhancing Learning Effectiveness in Digital Design Courses Through the Use of Programmable Logic Boards,"Incorporating programmable logic devices (PLD) in digital design courses has become increasingly popular. The advantages of using PLDs, such as complex programmable logic devices (CPLDs) and field programmable gate arrays (FPGA), have been discussed before. However, previous studies have focused on the experiences from the point of view of the instructor, with some general impressions from the students. This paper instead focuses on the students' point of view, and most importantly, investigates whether the use of programmable logic has helped facilitate the learning process. This investigation was based on the results of a comprehensive survey that was given to the class shortly before the end of the term, and a comparison of final exam scores between a previous class that did not use PLDs and the current class that did use PLDs. The surveys and their test scores provide solid evidence as to gauge whether using PLDs really does benefit students trying to learn digital design and basic computer architecture.","Education,
Materials,
Logic design,
Tutorials,
Hardware,
Computer science,
Computers"
A Trace-Driven Approach to Evaluate the Scalability of P2P-Based Video-on-Demand Service,"Peer-to-Peer (P2P) networks have emerged as one of the most promising approaches to improve the scalability of Video-on-Demand (VoD) service over Internet. However, despite a number of architectures and streaming protocols have been proposed in past years, there is few work to study the practical performance of P2P-based VoD service especially in consideration of real user behavior which actually has significant impact on system scalability. Therefore, in this paper, we first characterize the user behavior by analyzing a large amount of real traces from a popular VoD system supported by the biggest television station in China, cctv.com. Then we ex-amine the practical scalability of P2P-based VoD service through extensive trace-driven simula-tion under a general system framework. The results show that P2P networks scale well in provid-ing VoD service under real user behavior by obtaining a considerable good cache hit ratio. Moreover, it is observed that adopting hard cache at client side help achieves better system scal-ability than that with soft cache. We also identify the impact of various aspects of user behavior upon system scalability through detailed simulation. We believe that our study will shine insight-ful light on the understanding of practical scalability of P2P-based VoD service and be helpful to future system design and optimization.","Scalability,
Streaming media,
Network servers,
Peer to peer computing,
IP networks,
Bandwidth,
Web and internet services,
Protocols,
TV,
Design optimization"
Mobile speed estimation for broadband wireless communications over Rician fading channels,"In this paper, a new algorithm is proposed to estimate mobile speed for broadband wireless communications, which often encounter large number of fading channel taps causing severe intersymbol interference. Different from existing algorithms, which commonly assume that the fading channel coefficients are available for the speed estimators, the proposed algorithm is based on the received signals which contain unknown transmitted data, unknown frequency selective fading channel coefficients possibly including line-of-sight (LOS) components, and random receiver noise. Theoretical analysis is first carried out from the received signals, and a practical algorithm is proposed based on the analytical results. The algorithm employs a modified normalized auto-covariance of received signal power to estimate the speed of mobiles. The algorithm works well for frequency selective Rayleigh and Rician channels. The algorithm is very resistant to noise, it provides accurate speed estimation even if the signal-to-noise ratio (SNR) is as low as 0 dB. Simulation results indicate that the new algorithm is very reliable and effective to estimate mobile speed corresponding to a maximum Doppler up to 500 Hz. The algorithm has high computational efficiency and low estimation latency, with results being available within one second after communication is established.",
Variable Strength Interaction Testing with an Ant Colony System Approach,"Interaction testing (also called combinatorial testing) is an cost-effective test generation technique in software testing. Most research work focuses on finding effective approaches to build optimal t-way interaction test suites. However, the strength of different factor sets may not be consistent due to the practical test requirements. To solve this problem, a variable strength combinatorial object and several approaches based on it have been proposed. These approaches include simulated annealing (SA) and greedy algorithms. SA starts with a large randomly generated test suite and then uses a binary search process to find the optimal solution. Although this approach often generates the minimal test suites, it is time consuming. Greedy algorithms avoid this shortcoming but the size of generated test suites is usually not as small as SA. In this paper, we propose a novel approach to generate variable strength interaction test suites (VSITs). In our approach, we adopt a one-test-at-a-time strategy to build final test suites. To generate a single test, we adopt ant colony system (ACS) strategy, an effective variant of ant colony optimization (ACO). In order to successfully adopt ACS, we formulize the solution space, the cost function and several heuristic settings in this framework. We also apply our approach to some typical inputs. Experimental results show the effectiveness of our approach especially compared to greedy algorithms and several existing tools.",
Engineering the cloud from software modules,"Cloud computing faces many of the challenges and difficulties of distributed and parallel software. While the service interface hides the actual application from the remote user, the application developer still needs to come to terms with distributed software that needs to run on dynamic clusters and operate under a wide range of configurations. In this paper, we outline our vision of a model and runtime platform for the development, deployment, and management of software applications on the cloud. Our basic idea is to turn the notion of software module into a first class entity used for management and distribution that can be autonomously managed by the underlying software fabric of the cloud. In the paper we present our model, outline an initial implementation, and describe a first application developed using the ideas presented in the paper.","Application software,
Cloud computing,
Runtime,
Fabrics,
Software testing,
Software development management,
Computer science,
Outsourcing,
Information processing,
Platform virtualization"
Coevolution of Role-Based Cooperation in Multiagent Systems,"In tasks such as pursuit and evasion, multiple agents need to coordinate their behavior to achieve a common goal. An interesting question is, how can such behavior be best evolved? A powerful approach is to control the agents with neural networks, coevolve them in separate subpopulations, and test them together in the common task. In this paper, such a method, called multiagent enforced subpopulations (multiagent ESP), is proposed and demonstrated in a prey-capture task. First, the approach is shown to be more efficient than evolving a single central controller for all agents. Second, cooperation is found to be most efficient through stigmergy, i.e., through role-based responses to the environment, rather than communication between the agents. Together these results suggest that role-based cooperation is an effective strategy in certain multiagent tasks.","Multiagent systems,
Electrostatic precipitators,
Neural networks,
Centralized control,
Testing,
Communication system control,
Problem-solving,
Robustness,
Machine learning,
Genetic algorithms"
Making resonance a common case: A high-performance implementation of collective I/O on parallel file systems,"Collective I/O is a widely used technique to improve I/O performance in parallel computing. It can be implemented as a client-based or as a server-based scheme. The client-based implementation is more widely adopted in the MPIIO software such as ROMIO because of its independence from the storage system configuration and its greater portability. However, existing implementations of client-side collective I/O do not consider the actual pattern of file striping over multiple I/O nodes in the storage system. This can cause a large number of requests for non-sequential data at I/O nodes, substantially degrading I/O performance. Investigating a surprisingly high I/O throughput achieved when there is an accidental match between a particular request pattern and the data striping pattern on the I/O nodes, we reveal the resonance phenomenon as the cause. Exploiting readily available information on data striping from the metadata server in popular file systems such as PVFS2 and Lustre, we design a new collective I/O implementation technique, named as resonant I/O, that makes resonance a common case. Resonant I/O rearranges requests from multiple MPI processes according to the presumed data layout on the disks of I/O nodes so that non-sequential access of disk data can be turned into sequential access, significantly improving I/O performance without compromising the independence of a client-based implementation. We have implemented our design in ROMIO. Our experimental results on a small- and medium-scale cluster show that the scheme can increase I/O throughput for some commonly used parallel I/O benchmarks such as mpi-io-test and ior-mpi-io over the existing implementation of ROMIO by up to 157%, with no scenario demonstrating significantly decreased performance.","Resonance,
File systems,
Throughput,
Concurrent computing,
High performance computing,
Laboratories,
Parallel processing,
Degradation,
Pattern matching,
File servers"
High-Performance Hardware Architectures for Galois Counter Mode,"Various high-performance hardware architectures for Galois counter mode (GCM) in conjunction with various advanced encryption standard (AES) circuits and multiplier-adders are proposed. A total of 17 GCM-AES circuits were synthesized by using a 130-nm CMOS standard cell library, and the trade-offs between speed and hardware resources were evaluated. Our flexible architectures achieved a wide variety of performances from compact (2.56 Gbps with 34.5 Kgates) to high speed (62.6 Gbps with 979.3 Kgates). All of our architectures support key sizes of 128, 192, and 256 bits, while only one previous approach does. Even with variable-length key support, our architecture also achieved the highest hardware efficiency (defined as throughput per gate) among the designs using the same generation of process technology.","Hardware,
Computer architecture,
Cryptography,
Clocks,
Registers,
Radiation detectors,
Equations"
Unification of Evidence-Theoretic Fusion Algorithms: A Case Study in Level-2 and Level-3 Fingerprint Features,"This paper formulates an evidence-theoretic multimodal unification approach using belief functions that take into account the variability in biometric image characteristics. While processing nonideal images, the variation in the quality of features at different levels of abstraction may cause individual classifiers to generate conflicting genuine-impostor decisions. Existing fusion approaches are nonadaptive and do not always guarantee optimum performance improvements. We propose a contextual unification framework to dynamically select the most appropriate evidence-theoretic fusion algorithm for a given scenario. In the first approach, the unification framework uses deterministic rules to select the most appropriate fusion algorithm; while in the second approach, the framework intelligently learns from the input evidences using a 2nu-granular support vector machine. The effectiveness of the unification approach is experimentally validated by fusing match scores from level-2 and level-3 fingerprint features. Compared to existing fusion algorithms, the proposed unification approach is computationally efficient, and the verification accuracy is not compromised even when conflicting decisions are encountered.","Fingerprint recognition,
Biometrics,
Support vector machines,
Fusion power generation,
Support vector machine classification,
Computational intelligence,
Learning systems,
Image quality,
Computer science"
Distributed CSMA/CA algorithms for achieving maximum throughput in wireless networks,"Recently, it has been shown that CSMA-type algorithms can achieve the maximum possible throughput in wireless ad hoc networks [3], [5]. Central to these results is a distributed randomized algorithm which selects schedules according to a product-form distribution. The product-form distribution is achieved by considering a continuous-time Markov model of an idealized CSMA protocol [1] (continuous backoff times, zero propagation/sensing delay, no hidden terminals) under which collisions cannot occur.","Multiaccess communication,
Throughput,
Wireless networks,
Scheduling algorithm,
Protocols,
Propagation delay,
Broadcasting,
Mobile ad hoc networks,
Processor scheduling,
Degradation"
Rate-optimal and reduced-complexity sequential sensing algorithms for cognitive OFDM radios,"Sequential sensing algorithms are developed for OFDM-based hierarchical cognitive radio (CR) systems. Secondary users sense multiple sub-bands simultaneously for possible spectrum availabilities under hard miss-detection constraints to prevent interference to the primary users. Accounting for the fact that the sensing time overhead can often be significant, a performance metric is developed based on the effective achievable data rate. An optimization problem is formulated in the framework of optimal stopping problems to maximize the average effective data rate by determining the best time to stop taking samples for sensing, as well as the best set of channels to use for data transmission. A basis expansion-based sub-optimal algorithm is derived to reduce the prohibitive complexity of the optimal solution.","OFDM,
Detectors,
Chromium,
Transceivers,
Wideband,
Sequential analysis,
Cognitive radio,
Availability,
Interference,
Data communication"
Cooperative Autonomous Marine Vehicle motion control in the scope of the EU GREX Project: Theory and Practice,"This paper describes the core of the research work done by the ISR/IST team on cooperative Autonomous Marine Vehicle (AMV) motion control in the scope of the EU Project GREX - coordination and control of cooperating heterogeneous unmanned systems in uncertain environments. The first part of the paper affords the reader a concise introduction to the general problem of cooperative motion control of fleets of AMVs by highlighting illustrative mission scenarios developed collectively by the GREX partners and summarizing the main challenges that they pose to system engineers. This is followed by the description of a general architecture for cooperative autonomous marine vehicle control in the presence of time-varying communication topologies and communication losses that is rooted in a solid control-theoretic framework. The results of simulations with the NetMarSyS - Networked Marine Systems Simulator - of ISR/IST are presented and show the efficacy of the algorithms developed for cooperative motion control. The last part of the paper focuses on practical issues and describes the results of tests at sea in the Azores, in the Summer of 2008. The paper concludes with a critical review of the work done and a discussion of theoretical and practical implementation issues that warrant further research and development effort.",
On the Power of Randomization in Algorithmic Mechanism Design,"In many settings the power of truthful mechanisms is severely bounded. In this paper we use randomization to overcome this problem. In particular, we construct an FPTAS for multi-unit auctions that is truthful in expectation, whereas there is evidence that no polynomial-time truthful deterministic mechanism provides an approximation ratio better than 2. We also show for the first time that truthful in expectation polynomial-time mechanisms are provably stronger than polynomial-time universally truthful mechanisms. Specifically, we show that there is a setting in which: (1) there is a non-polynomial time truthful mechanism that always outputs the optimal solution, and that (2) no universally truthful randomized mechanism can provide an approximation ratio better than 2 in polynomial time, but (3) an FPTAS that is truthful in expectation exists.","Algorithm design and analysis,
Polynomials,
Computer science,
Approximation algorithms,
Design engineering,
Power engineering and energy,
Complexity theory,
Probability distribution"
Multi-channel scheduling algorithms for fast aggregated convergecast in sensor networks,"Fast and periodic collection of aggregated data is of considerable interest for mission-critical and continuous monitoring applications in sensor networks. In the many-to-one communication paradigm known as convergecast, we consider scenarios where data packets are aggregated at each hop en route to a sink node along a tree-based routing topology and focus on maximizing the data collection rate at the sink by employing TDMA scheduling and multiple frequency channels. Our key result in the paper lies in proving that minimizing the schedule length for an arbitrary network in the presence of multiple frequencies is NP-hard, and in designing approximation algorithms with worst-case provable performance guarantees for geometric networks. In particular, we design a constant factor approximation for networks modeled as unit disk graphs (UDG) where every node has a uniform transmission range, and a O(Δ(T)log n) approximation for general disk graphs where nodes have different transmission ranges; n is the number of nodes in the network and Δ(T) is the maximum node degree on a given routing tree T. We also prove that a constant factor approximation is achievable on UDG even for unknown routing topologies so long as the maximum node degree in the tree is bounded by a constant. We also show that finding the minimum number of frequencies required to remove all the interfering links in an arbitrary network in NP-hard. We give an upper bound on the maximum number of such frequencies required and propose a polynomial time algorithm that minimizes the schedule length under this scenario. Finally, we evaluate our algorithms through simulations and show various trends in performance for different network parameters.","Scheduling algorithm,
Frequency,
Routing,
Network topology,
Mission critical systems,
Monitoring,
Time division multiple access,
Algorithm design and analysis,
Approximation algorithms,
Tree graphs"
Improving Reliability and Accuracy of Vibration Parameters of Vocal Folds Based on High-Speed Video and Electroglottography,"Quantified vibration parameters of vocal folds, including parameters directly extracted from high-speed video (HSV) and electroglottography (EGG), and inverse parameters based on models, can accurately describe the mechanism of phonation and also classify the abnormal in clinics. In order to improve the reliability and accuracy of these parameters, this paper provides a method based on an integrated recording system. This system includes two parts: HSV and EGG, which can record vibration information of vocal folds simultaneously. An image processing approach that bases on Zernike moments operator and an improved level set algorithm is proposed to detect glottal edges at subpixel-level aiming at image series recorded by HSV. An approach is also introduced for EGG data to extract three kinds of characteristic points for special vibration instants. Finally, inverse parameters of vocal folds can be optimized by a genetic algorithm based on the experimental vibration behaviors synthesized with these parameters and the simulations of a two-mass model. The results of a normal phonation experiment indicate that the parameters extracted by this method are more accurate and reliable than those extracted by general methods, which were only on the basis of HSV data and with pixel-level processing approaches in former studies.",
Moving cast shadow detection using physics-based features,"Cast shadows induced by moving objects often cause serious problems to many vision applications. We present in this paper an online statistical learning approach to model the background appearance variations under cast shadows. Based on the bi-illuminant (i.e. direct light sources and ambient illumination) dichromatic reflection model, we derive physics-based color features under the assumptions of constant ambient illumination and light sources with common spectral power distributions. We first use one Gaussian mixture model (GMM) to learn the color features, which are constant regardless of the background surfaces or illuminant colors in a scene. Then, we build up one pixel based GMM for each pixel to learn the local shadow features. To overcome the slow convergence rate in the conventional GMM learning, we update the pixel-based GMMs through confidence-rated learning. The proposed method can rapidly learn model parameters in an unsupervised way and adapt to illumination conditions or environment changes. Furthermore, we demonstrate that our method is robust to scenes with few foreground activities and videos captured at low or unsteady frame rates.","Computer vision,
Lighting,
Light sources,
Layout,
Statistical learning,
Optical reflection,
Power distribution,
Convergence,
Robustness,
Videos"
Packet Loss Pattern and Parametric Video Quality Model for IPTV,"This paper proposes a packet-layer parametric quality of experience (QoE) oriented model for IPTV service over next generation communication system. It is a model that based on the ITU-T G.1070 video quality model, whose video quality parameters can be adjusted for IPTV applications. In the paper, three metrics for evaluation of the packet loss burst are introduced for the enhancement of ITU G.1070 model. The experiment results show that the proposed model, T-model, correlates quite well with subjective opinion of IPTV video quality, and the results also prove that T-model, by considering packet loss pattern of the network, is more accurate for estimate of the video quality than standard ITU-T G.1070 video model.","IPTV,
Quality of service,
Next generation networking,
Codecs,
Propagation losses,
Monitoring,
HDTV,
Streaming media,
Costs,
Parametric statistics"
Robust fitting of multiple structures: The statistical learning approach,"We propose an unconventional but highly effective approach to robust fitting of multiple structures by using statistical learning concepts. We design a novel Mercer kernel for the robust estimation problem which elicits the potential of two points to have emerged from the same underlying structure. The Mercer kernel permits the application of well-grounded statistical learning methods, among which nonlinear dimensionality reduction, principal component analysis and spectral clustering are applied for robust fitting. Our method can remove gross outliers and in parallel discover the multiple structures present. It functions well under severe outliers (more than 90% of the data) and considerable inlier noise without requiring elaborate manual tuning or unrealistic prior information. Experiments on synthetic and real problems illustrate the superiority of the proposed idea over previous methods.","Statistical learning,
Kernel,
Statistics,
Noise robustness,
Computer vision,
Computer science,
Australia,
Principal component analysis,
Feature extraction,
Pipelines"
Dynamic updating for sparse time varying signals,"Many signal processing applications revolve around finding a sparse solution to a (often underdetermined) system of linear equations. Recent results in compressive sensing (CS) have shown that when the signal we are trying to acquire is sparse and the measurements are incoherent, the signal can be reconstructed reliably from an incomplete set of measurements. However, the signal recovery is an involved process, usually requiring the solution of an ℓ1 minimization program. In this paper we discuss the problem of estimating a time-varying sparse signal from a series of linear measurements. We propose an efficient way to dynamically update the solution to two types of ℓ1 problems when the underlying signal changes. The proposed dynamic update scheme is based on homotopy continuation, which systematically breaks down the solution update into a small number of linear steps. The computational cost for each step is just a few matrix-vector multiplications.",
Mutation Testing for Java Database Applications,"Database application programs are ubiquitous, so good techniques for testing them are needed. Recently, several research groups have proposed new approaches to generating tests for database applications and for assessing test data adequacy. This paper describes a mutation testing tool, JDAMA (Java Database Application Mutation Analyzer), for Java programs that interact with a database via the JDBC interface. Our approach extends the mutation testing approach for SQL by Tuya et al, by integrating it with analysis and instrumentation of the application bytecode. JDAMA's use is illustrated through a small study which uses mutation scores to compare two test generation techniques for database applications.","Genetic mutations,
Java,
Application software,
Relational databases,
Software testing,
Logic testing,
Computer science,
Data engineering,
Data analysis,
Instruments"
An Adaptive Web Services Selection Method Based on the QoS Prediction Mechanism,"In recent years, many QoS-based web service selection methods have been proposed. However, as QoS changes dynamically, the atomic services of a composite web service could be replaced with other ones that have better quality. The performance of a composite web service will be decreased if this replacement happens frequently in runtime. Predicting the change of QoS accurately in select phase can effectively reduce this web services “thrash”. In this paper, we propose a web service selection algorithm GFS (Goodness-Fit Selection algorithm) based on QoS prediction mechanism in dynamic environments. We use structural equation to model the QoS measurement of web services. By taking the advantage of the prediction mechanism of structural equation model, we can quantitatively predict the change of quality of service dynamically. Optimal web service is selected based on the predicted results. Simulation results show that in dynamic environments, GFS provides higher selection accuracy than previous selection methods.","Web services,
Quality of service,
Intelligent agent,
Equations,
Numerical analysis,
Predictive models,
Costs,
Conferences,
Computer science,
Runtime"
Model-based and data-driven prognosis of automotive and electronic systems,"Recent advances in sensor technology, remote communication and computational capabilities, and standardized hardware/software interfaces are creating a dramatic shift in the way the health of vehicles is monitored and managed. Concomitantly, there is an increased trend towards the forecasting of system degradation through a prognostic process to fulfill the needs of customers demanding high vehicle availability. Prognosis is viewed as an add-on capability to diagnosis that assesses the current health of a system and predicts its remaining life based on sensed features that capture the gradual degradation in the operation of the vehicle. This paper discusses a hybrid model-based, data-driven and knowledge-based integrated diagnosis and prognosis framework, and applies it to automotive (suspension and battery systems) and on-board electronic systems.","Automotive engineering,
Vehicles,
Degradation,
Communications technology,
Computer interfaces,
Hardware,
Remote monitoring,
Technology management,
Demand forecasting,
Availability"
FRAME: An Innovative Incentive Scheme in Vehicular Networks,"Vehicular ad hoc networks (VANETs) are envisioned to provide promising applications and services. One critical deployment issue in VANETs is to motivate vehicles and their drivers to cooperate and contribute to packet forwarding in vehicle-to-vehicle or vehicle-to-roadside communication. In this paper, we examine this problem, analyze the drawbacks of two straightforward schemes, and present a secure incentive scheme to stimulate cooperation in VANETs. We define the measurement of contribution according to the unique characteristics of VANET communication. Our scheme uses the weighted rewarding component to ensure fairness. Extensive simulation results are presented to support the effectiveness of our scheme.","Incentive schemes,
Ad hoc networks,
Spraying,
Vehicle driving,
Mobile communication,
Communications Society,
Computer science,
Automotive engineering,
Application software,
Throughput"
Analysis and comparison of sleeping posture classification methods using pressure sensitive bed system,"Pressure ulcers are common problems for bedridden patients. Caregivers need to reposition the sleeping posture of a patient every two hours in order to reduce the risk of getting ulcers. This study presents the use of Kurtosis and skewness estimation, principal component analysis (PCA) and support vector machines (SVMs) for sleeping posture classification using cost-effective pressure sensitive mattress that can help caregivers to make correct sleeping posture changes for the prevention of pressure ulcers.","Intelligent sensors,
Medical services,
Principal component analysis,
Algorithm design and analysis,
Bayesian methods,
Support vector machines,
Support vector machine classification,
USA Councils,
Cities and towns,
Protocols"
Automated Method for Improving System Performance of Computer-Aided Diagnosis in Breast Ultrasound,"The purpose of this research was to demonstrate the feasibility of a computerized auto-assessment method in which a computer-aided diagnosis (CADx) system itself provides a level of confidence for its estimate for the probability of malignancy for each radiologist-identified lesion. The computer performance was assessed within a leave-one-case-out protocol using a database of sonographic images from 542 patients (19% cancer prevalence). We investigated the potential of computer-derived confidence levels both as 1) an output aid to radiologists and 2) as an automated method to improve the computer classification performance-in the task of differentiating between cancerous and benign lesions for the entire database. For the former, the CADx classification performance was assessed within ranges of confidence levels. For the latter, the computer-derived confidence levels were used in the determination of the computer-estimated probability of malignancy for each actual lesion based on probabilities obtained from different views. The use of this auto-assessment method resulted in the modest but statistically significant increase in the area under the receiver operating characteristic (ROC) curve (AUC value) of 0.01 with respect to the performance obtained using the ldquotraditionalrdquo CADx approach, increasing the AUC value from 0.89 to 0.90 (p -value 0.03). We believe that computer-provided confidence levels may be helpful to radiologists who are using CADx output in diagnostic image interpretation as well as for automated improvement of the CADx classification for cancer.",
Human pose estimation using consistent max-covering,"We propose a novel consistent max-covering scheme for human pose estimation. Consistent max-covering formulates pose estimation as the covering of body part polygons on an object silhouette so that the body part tiles maximally cover the foreground, match local image features, and satisfy body linkage plan and color constraints. It uses high order constraints to anchor multiple body parts simultaneously; the hyper-edges in the part relation graph are essential for detecting complex poses. Because of using multiple clues in pose estimation, this method is resistant to cluttered foregrounds. We propose an efficient linear relaxation method to solve the consistent max-covering problem. Experiments on a variety of images and videos show that the proposed method is more robust than locally constrained methods for human pose estimation.","Humans,
Immune system,
Assembly,
Tiles,
Couplings,
Relaxation methods,
Videos,
Robustness,
Application software,
Learning systems"
ContextServ: A platform for rapid and flexible development of context-aware Web services,"Context-aware Web services are currently emerging as an important technology for building innovative context-aware applications. Unfortunately, context-aware Web services are still difficult to build. This paper describes ContextServ, a platform for rapid development of context-aware Web services. ContextServ adopts model-driven development where context-aware Web services are specified using ContextUML, a UML based modeling language. The platform also offers a set of automated tools for generating and deploying executable implementations of context-aware Web services. This paper presents the motivation, system design, implementation, and usage of ContextServ.","Context-aware services,
Web services,
Context awareness,
Context modeling,
Computer science,
Application software,
Intelligent sensors,
Computer networks,
Food technology,
Simple object access protocol"
Efficient large-scale model checking,"Model checking is a popular technique to systematically and automatically verify system properties. Unfortunately, the well-known state explosion problem often limits the extent to which it can be applied to realistic specifications, due to the huge resulting memory requirements. Distributed-memory model checkers exist, but have thus far only been evaluated on small-scale clusters, with mixed results. We examine one well-known distributed model checker, DiVinE, in detail, and show how a number of additional optimizations in its runtime system enable it to efficiently check very demanding problem instances on a large-scale, multi-core compute cluster. We analyze the impact of the distributed algorithms employed, the problem instance characteristics and network overhead. Finally, we show that the model checker can even obtain good performance in a high-bandwidth computational grid environment.","Large-scale systems,
Algorithm design and analysis,
State-space methods,
Performance analysis,
Computer science,
Clustering algorithms,
Explosions,
Multicore processing,
Distributed computing,
Distributed algorithms"
Mobility Effects in Mobile Ad Hoc Networks,,"Mobile ad hoc networks,
Routing protocols,
Throughput,
System testing,
Network topology,
Performance loss,
Mobile communication,
Informatics,
Computer networks,
Information science"
K-means Clustering Algorithm with Improved Initial Center,"In this paper we present a new clustering method based on k-means that have avoided alternative randomness of initial center. This paper focused on K-means algorithm to the initial value of the dependence of k selected from the aspects of the algorithm is improved. First,the initial clustering number is. Second, through the application of the sub-merger strategy the categories were combined.The algorithm does not require the user is given in advance the number of cluster. Experiments on synthetic datasets are presented to have shown significant improvements in clustering accuracy in comparison with the random k-means.","Clustering algorithms,
Partitioning algorithms,
Data mining,
Iterative algorithms,
Computer science,
Electronic mail,
Switches,
Clustering methods,
Convergence,
Algorithm design and analysis"
Traffic light detection with color and edge information,"The concern of the intelligent transportation system rises and many driver support systems have been developed. In this paper, a fast method of detecting a traffic light in a scene image is proposed. By converting the color space from RGB to normalized RGB, some regions are selected as candidates of a traffic light. Then a method based on the Hough transform is applied to detect an exact region. Experimental results using images including a traffic light verifies the effectiveness of the proposed method.",
Oscillation monitoring system using synchrophasors,"With the growing implementation of synchrophasors or PMUs across the power grid, it is now possible to observe and analyze system-wide dynamic phenomena in real-time. At Washington State University (WSU), we have been developing the framework of an Oscillation Monitoring System (OMS) for extracting the modal information of electromechanical oscillations from synchrophasors in real-time. A prototype version of OMS has already been implemented in real-time at Tennessee Valley Authority (TVA) and it is being tested and tuned. The paper summarizes the theory of OMS and highlights the implementation features at TVA.","Monitoring,
Phasor measurement units,
Engines,
Real time systems,
Shape measurement,
Damping,
Power measurement,
Power system dynamics,
Power system stability,
Flowcharts"
Memory-based Particle Filter for face pose tracking robust under complex dynamics,"A novel particle filter, the memory-based particle filter (M-PF), is proposed that can visually track moving objects that have complex dynamics. We aim to realize robustness against abrupt object movements and quick recovery from tracking failure caused by factors such as occlusions. To that end, we eliminate the Markov assumption from the previous particle filtering framework and predict the prior distribution of the target state from the long-term dynamics. More concretely, M-PF stores the past history of the estimated target states, and employs a random sampling from the history to generate prior distribution; it represents a novel PF formulation.Our method can handle nonlinear, time-variant, and non-Markov dynamics, which is not possible within existing PF frameworks. Accurate prior prediction based on proper dynamics model is especially effective for recovering lost tracks, because it can provide possible target states, which can drastically change since the track was lost. We target the face pose of seated humans in this paper. Quantitative evaluations with magnetic sensors confirm improved accuracy in face pose estimation and successful recovery from tracking loss. The proposed M-PF suggests a new paradigm for modeling systems with complex dynamics and so offers a various visual tracking applications.","Particle filters,
Particle tracking,
Robustness,
Target tracking,
History,
Filtering,
State estimation,
Sampling methods,
Predictive models,
Face"
Maturing Software Engineering Knowledge through Classifications: A Case Study on Unit Testing Techniques,"Classification makes a significant contribution to advancing knowledge in both science and engineering. It is a way of investigating the relationships between the objects to be classified and identifies gaps in knowledge. Classification in engineering also has a practical application; it supports object selection. They can help mature software engineering knowledge, as classifications constitute an organized structure of knowledge items. Till date, there have been few attempts at classifying in software engineering. In this research, we examine how useful classifications in software engineering are for advancing knowledge by trying to classify testing techniques. The paper presents a preliminary classification of a set of unit testing techniques. To obtain this classification, we enacted a generic process for developing useful software engineering classifications. The proposed classification has been proven useful for maturing knowledge about testing techniques, and therefore, SE, as it helps to: 1) provide a systematic description of the techniques, 2) understand testing techniques by studying the relationships among techniques (measured in terms of differences and similarities), 3) identify potentially useful techniques that do not yet exist by analyzing gaps in the classification, and 4) support practitioners in testing technique selection by matching technique characteristics to project characteristics.",
High efficiency battery management system for serially connected battery string,"This article presents an approach for balancing serially connected battery strings with high efficiency energy transferred. The efficiency of this balancing process is accomplished by channeling an excess energy through a DC Link Bus rather than done serially. Using this proposed technique with converter which was 80 percent efficiency, the equalized efficiency of balancing process is increased more than the conventional techniques at least 12.80 percent. By applied this proposed technique to the charge equalized system, the unbalance charging problem can be eliminated. The validity of this approach can be supported by computer simulation and experimental result.",Battery management systems
A High-Performance Computing Forecast: Partly Cloudy,"Cloud computing is emerging as an important computational resource allocation trend in commercial, academic, and industrial sectors. Yet, because the business model doesn't currently meet all the needs of high-performance computing (HPC)-the demands of capability computing, for example-the relationship between clouds and HPC suggests a partly cloudy forecast.","Cloud computing,
Distributed computing,
Resource management,
Computer networks,
Economies of scale,
Availability,
Costs,
Delay,
Computer industry,
Business"
Unsupervised learning of 3D object models from partial views,We present an algorithm for learning 3D object models from partial object observations. The input to our algorithm is a sequence of 3D laser range scans. Models learned from the objects are represented as point clouds. Our approach can deal with partial views and it can robustly learn accurate models from complex scenes. It is based on an iterative matching procedure which attempts to recursively merge similar models. The alignment between models is determined using a novel scan registration procedure based on range images. The decision about which models to merge is performed by spectral clustering of a similarity matrix whose entries represent the consistency between different models.,
Multiple-Antenna Interference Cancellation and Detection for Two Users Using Precoders,"We consider interference cancellation for a system with two users when users know each other channels. The goal is to utilize multiple antennas to cancel the interference without sacrificing the diversity or the complexity of the system. Before, in the literature, it was shown how a receiver with two receive antennas can completely cancel the interference of two users and provide a diversity of 2 for users with two transmit antennas. We propose a system to achieve the maximum possible diversity of 4 with low complexity. Then, we extend the results to two-user systems with any number of transmit and receive antennas. Our main idea is to design precoders, using the channel information, to make it possible for different users to transmit over orthogonal spaces. Then, using the orthogonality of the transmitted signals, the receiver can separate them and decode the signals independently. We analytically prove that the system provides full diversity to both users. In addition, we provide simulation results that confirm our analytical proof.","Interference cancellation,
Receiving antennas,
Transmitting antennas,
Maximum likelihood decoding,
Transmitters,
Antenna arrays,
Multiuser detection,
Block codes,
Maximum likelihood detection,
Feedback"
Approximation Algorithms for Data Broadcast in Wireless Networks,"Broadcasting is a fundamental operation in wireless networks and plays an important role in the communication protocol design. In multihop wireless networks, however, interference at a node due to simultaneous transmissions from its neighbors makes it non-trivial to design a minimum-latency broadcast algorithm, which is known to be NP-complete. We present a simple 12-approximation algorithm for the one-to-all broadcast problem that improves all previously known guarantees for this problem. We then consider the all-to-all broadcast problem where each node sends its own message to all other nodes. For the all-to-all broadcast problem, we present two algorithms with approximation ratios of 20 and 34, improving the best result available in the literature. Finally, we report experimental evaluation of our algorithms. Our studies indicate that our algorithms perform much better in practice than the worst-case guarantees provided in the theoretical analysis and achieve up to 37% performance improvement over existing schemes.","Approximation algorithms,
Broadcasting,
Wireless networks,
Delay,
Computer science,
Wireless application protocol,
Algorithm design and analysis,
Spread spectrum communication,
Interference,
Peer to peer computing"
A detailed delay path model for FPGAs,"A complete circuit-level description of a representative FPGA is presented in this paper, from which a simple RC delay model as a function of architectural and technology parameters is derived. Using this model, the expression for the optimal delay of any path through the FPGA can be formulated. We distill our model into being purely architecture dependent, and use it to capture new insight into how FPGA parameters can directly affect its delay. Several applications of this model are: (1) to gain better intuition of how architecture and process parameters affect the delay path in an FPGA, (2) for initial studies into new circuit designs and integrated circuit technologies, (3) in CAD tools for optimisation and sensitivity analysis. The technique described can be applied to arbitrary circuits, and simulations show that our closed form equations give delay values that are accurate to approximately 10% when compared to HSPICE simulation.","Delay,
Field programmable gate arrays,
Circuit simulation,
Integrated circuit modeling,
Circuit synthesis,
Integrated circuit technology,
Design automation,
Design optimization,
Sensitivity analysis,
Equations"
Multi Objective Higher Order Mutation Testing with Genetic Programming,"In academic empirical studies, mutation testing has been demonstrated to be a powerful technique for fault finding.However, it remains very expensive and the few valuable traditional mutants that resemble real faults are mixed in with many others that denote unrealistic faults.These twin problems of expense and realism have been a significant barrier to industrial uptake of mutation testing.Genetic programming is used to search the space of complex faults (higher order mutants). The space is much larger than the traditional first order mutation space of simple faults.However, the use of a search based approach makes this scalable, seeking only those mutants that challenge the tester,while the consideration of complex faults addresses the problem of fault realism; it is known that 90% of real faults are complex (i.e. higher order).We show that we are able to find examples that pose challenges totesting in the higher order space that cannot be represented in thefirst order space.",
Bit-Level Extrinsic Information Exchange Method for Double-Binary Turbo Codes,"Nonbinary turbo codes have many advantages over single-binary turbo codes, but their decoder implementations require much more memory, particularly for storing symbolic extrinsic information to be exchanged between two soft-input-soft-output (SISO) decoders. To reduce the memory size required for double-binary turbo decoding, this paper presents a new method to convert symbolic extrinsic information to bit-level information and vice versa. By exchanging bit-level extrinsic information, the number of extrinsic information values to be exchanged in double-binary turbo decoding is reduced to the same amount as that in single-binary turbo decoding. A double-binary turbo decoder is designed for the WiMAX standard to verify the proposed method, which reduces the total memory size by 20%.","Turbo codes,
WiMAX,
Quantization,
Iterative decoding,
Forward error correction,
Land mobile radio,
Digital video broadcasting,
Code standards,
Computer science education"
Secure Hierarchical Data Aggregation in Wireless Sensor Networks,"Communication in wireless sensor networks uses the majority of a sensor's limited energy. Using aggregation in wireless sensor network reduces the overall communication cost. Security in wireless sensor networks entails many different challenges. Traditional end-to-end security is not suitable for use with in-network aggregation. A corrupted sensor has access to the data and can falsify results. Additively homomorphic encryption allows for aggregation of encrypted values, with the result being the same as the result when unencrypted data was aggregated. Using public key cryptography, digital signatures can be used to achieve integrity. We propose a new algorithm using homomorphic encryption and additive digital signatures to achieve confidentiality, integrity and availability for in-network aggregation in wireless sensor networks. We prove that our digital signature algorithm which is based on the elliptic curve digital signature algorithm (ECDSA) is as secure as ECDSA.","Wireless sensor networks,
Cryptography,
Base stations,
Aggregates,
Data security,
Digital signatures,
Batteries,
Computer science,
Monitoring,
Spread spectrum communication"
Multi-robot coordination using generalized social potential fields,"We present a novel approach to compute collision-free paths for multiple robots subject to local coordination constraints. More specifically, given a set of robots, their initial and final configurations, and possibly some additional coordination constraints, our goal is to compute a collision-free path between the initial and final configuration that maintains the constraints. To solve this problem, our approach generalizes the social potential field method to be applicable to both convex and nonconvex polyhedra. Social potential fields are then integrated into a “physics-based motion planning” framework which uses constrained dynamics to solve the motion planning problem. Our approach is able to plan for over 200 robots while averaging about 110 ms per step in a variety of environments.","Robot kinematics,
Orbital robotics,
Motion planning,
Robotics and automation,
Computer science,
Robotic assembly,
Collision avoidance,
Multirobot systems,
Contracts,
Virtual manufacturing"
Blocking and Delay Analysis of Single Wavelength Optical Buffer With General Packet Size Distribution,"Buffers are essential components of any packet switch for resolving contentions among arriving packets. Currently, optical buffers are composed of fiber delay lines (FDL), whose blocking and delay behavior differ drastically from that of conventional RAM at least two-fold: 1) only multiples of discrete time delays can be offered to arriving packets; 2) a packet must be dropped if the maximum delay provided by optical buffer is not sufficient to avoid contention, this property is called balking. As a result, optical buffers only have finite time resolution, which may lead to excess load and prolong the packet delay. In this paper, a novel queueing model of optical buffer is proposed, and the closed-form expressions of blocking probability and mean delay are derived to explore the tradeoff between buffer performance and system parameters, such as the length of the optical buffer, the time granularity of FDLs, and to evaluate the overall impact of packet length distribution on the buffer performance.",
Activity-Driven Populace: A Cognitive Approach to Crowd Simulation,Simulating a natural-looking virtual populace requires modeling different behavioral levels to mimic how people choose and organize their activities. A multilayer behavior model for crowd simulation can help developers endow each entity with high-level objectives built on top of a reactive and cognitive decision system.,"Computational modeling,
Humans,
Physical layer,
Navigation,
Educational institutions,
Safety,
Pressing,
Collision avoidance,
Computational efficiency,
Virtual environment"
Onboard Science Processing Concepts for the HyspIRI Mission,"This paper presents the operational concept for onboard processing for the HysIRI mission which is an Earth observing mission that includes both thermal infrared instrument and a hyperspectral visible/shortwave infrared instrument, and that is being considered for launch in the next decade. This article describes the potential application of AI techniques for the HyspIRI mission-for both onboard processing and ground-based, automated mission planning.","Instruments,
Infrared heating,
Thermal management,
Artificial intelligence,
Volcanoes,
Monitoring,
Space technology,
Space heating,
Space missions,
Sea surface"
Evaluating the Accuracy of Fault Localization Techniques,"We investigate claims and assumptions made in several recent papers about fault localization (FL) techniques. Most of these claims have to do with evaluating FL accuracy. Our investigation centers on a new subject program having properties useful for FL experiments. We find that Tarantula (Jones et al.) works well on the program, and we show weak support for the assertion that coverage-based test suites help Tarantula to localize faults. Baudry et al. used automatically-generated mutants to evaluate the accuracy of an FL technique that generates many distinct scores for program locations. We find no evidence to suggest that the use of mutants for this purpose is invalid. However, we find evidence that the standard method for evaluating FL accuracy is unfairly biased toward techniques that generate many distinct scores, and we propose a fairer method of accuracy evaluation. Finally, Denmat et al. suggest that data mining techniques may apply to FL. We investigate this suggestion with the data mining tool Weka, using standard techniques for evaluating the accuracy of data mining classifiers. We find that standard classifiers suffer from the class imbalance problem. However, we find that adding cost information improves accuracy.","Data mining,
Humans,
Software engineering,
Computer science,
Costs,
Genetic mutations,
Data analysis,
Software testing,
Programming profession"
Automatic Detection of Pulmonary Embolism in CTA Images,"Pulmonary embolism (PE) is a common life-threatening disorder for which an early diagnosis is desirable. We propose a new system for the automatic detection of PE in contrast-enhanced CT images. The system consists of candidate detection, feature computation and classification. Candidate detection focusses on the inclusion of PE-even complete occlusions-and the exclusion of false detections, such as tissue and parenchymal diseases. Feature computation does not only focus on the intensity, shape and size of an embolus, but also on locations and the shape of the pulmonary vascular tree. Several classifiers have been tested and the results show that the performance is optimized by using a bagged tree classifier with two features based on the shape of a blood vessel and the distance to the vessel boundary. The system was trained on 38 CT data sets. Evaluation on 19 other data sets showed that the system generalizes well. The sensitivity of our system on the evaluation data is 63% at 4.9 false positives per data set, which allowed the radiologist to improve the number of detected PE by 22%.","Computed tomography,
Shape,
Blood vessels,
Biomedical imaging,
Arteries,
Diseases,
Classification tree analysis,
Testing,
X-ray imaging,
High-resolution imaging"
Problem Based Learning in the Software Engineering Classroom,"Software engineering lecturers are faced with the teaching of concepts which sometimes are not easy for inexperienced students to understand.  Therefore, it can be useful to consider and use non-traditional teaching methods which can improve students’ learning.  In this paper, we discuss problem-based learning and how its use can improve students’ understanding of concepts.   We present factors which should exist in ‘pure’ problem-based learning.  We then describe how one of the authors used problem-based learning in a class who were required to understand information flows through software engineering diagramming techniques, with the ultimate view to being able to analyze and design computerized information systems.  This problem-based learning class was observed and analyzed by the second author.  The analysis presented focuses on the problem-based learning factors, how they were implemented in class, and the strengths and weaknesses of the use of problem-based learning in this way.  In conclusion, the authors discuss how the teaching could be improved through modifying the teaching method for a future class in which problem-based learning will be used.  This modification is expected to enhance the students’ learning and their experience.","Software engineering,
Information systems,
Problem-solving,
Computer science education,
Systems engineering education,
Computer science,
Information analysis,
North America,
Technological innovation,
Concrete"
An AGPS-based elderly tracking system,"The design of an experimental AGPS-based (Assisted Global Positioning System) elderly tracking system is described. The system includes: a wearable AGPS terminal with HSPA two-way communication capability and designed for 10 days of continuous battery operation, a GPS assistance data server with reference GPS stations, location database and server, application server, and web server and client. Assistance data is retrieved by the wearable AGPS terminal using the SUPL protocol (Secured User Plane Location). This paper describes the design of each component based on key considerations such as accuracy, availability, battery life-time, and user behavior.","Senior citizens,
Global Positioning System,
Batteries,
Satellite broadcasting,
Frequency,
Databases,
Web server,
Information retrieval,
Protocols,
Prototypes"
Designing multi-leader-based Allgather algorithms for multi-core clusters,"The increasing demand for computational cycles is being met by the use of multi-core processors. Having large number of cores per node necessitates multi-core aware designs to extract the best performance. The Message Passing Interface (MPI) is the dominant parallel programming model on modern high performance computing clusters. The MPI collective operations take a significant portion of the communication time for an application. The existing optimizations for collectives exploit shared memory for intra-node communication to improve performance. However, it still would not scale well as the number of cores per node increase. In this work, we propose a novel and scalable multi-leader-based hierarchical Allgather design. This design allows better cache sharing for Non-Uniform Memory Access (NUMA) machines and makes better use of the network speed available with high performance interconnects such as InfiniBand. The new multi-leader-based scheme achieves a performance improvement of up to 58% for small messages and 70% for medium sized messages.","Algorithm design and analysis,
Clustering algorithms,
High performance computing,
Multicore processing,
Sockets,
Libraries,
Bandwidth,
Message passing,
Parallel programming,
Kernel"
CellMR: A framework for supporting mapreduce on asymmetric cell-based clusters,"The use of asymmetric multi-core processors with on-chip computational accelerators is becoming common in a variety of environments ranging from scientific computing to enterprise applications. The focus of current research has been on making efficient use of individual systems, and porting applications to asymmetric processors. In this paper, we take the next step by investigating the use of multi-core-based systems, especially the popular Cell processor, in a cluster setting. We present CellMR, an efficient and scalable implementation of the MapReduce framework for asymmetric Cell-based clusters. The novelty of CellMR lies in its adoption of a streaming approach to supporting MapReduce, and its adaptive resource scheduling schemes: Instead of allocating workloads to the components once, CellMR slices the input into small work units and streams them to the asymmetric nodes for efficient processing. Moreover, CellMR removes I/O bottlenecks by design, using a number of techniques, such as double-buffering and asynchronous I/O, to maximize cluster performance. Our evaluation of CellMR using typical MapReduce applications shows that it achieves 50.5% better performance compared to the standard nonstreaming approach, introduces a very small overhead on the manager irrespective of application input size, scales almost linearly with increasing number of compute nodes (a speedup of 6.9 on average, when using eight nodes compared to a single node), and adapts effectively the parameters of its resource management policy between applications with varying computation density.","Acceleration,
Processor scheduling,
Distributed computing,
Multicore processing,
Resource management,
Computer architecture,
High performance computing,
Parallel architectures,
Data processing,
Parallel programming"
Greedy routing with guaranteed delivery using Ricci flows,"Greedy forwarding with geographical locations in a wireless sensor network may fail at a local minimum. In this paper we propose to use conformal mapping to compute a new embedding of the sensor nodes in the plane such that greedy forwarding with the virtual coordinates guarantees delivery. In particular, we extract a planar triangulation of the sensor network with non-triangular faces as holes, by either using the nodes' location or using a landmark-based scheme without node location. The conformal map is computed with Ricci flow such that all the non-triangular faces are mapped to perfect circles. Thus greedy forwarding will never get stuck at an intermediate node. The computation of the conformal map and the virtual coordinates is performed at a preprocessing phase and can be implemented by local gossip-style computation. The method applies to both unit disk graph models and quasi-unit disk graph models. Simulation results are presented for these scenarios.","Routing,
Sensor phenomena and characterization,
Batteries,
Wireless sensor networks,
Monitoring,
Road transportation,
Telecommunication traffic,
Approximation algorithms,
Scheduling algorithm,
Accuracy"
Dissecting Self-* Properties,"The scale and complexity of distributed systems have steadily grown in the recent years. Management of this complexity has drawn attention towards systems that can automatically maintain themselves throughout different scenarios. These systems have been described with many terms, such as self-healing, self-stabilizing, self-organizing, self-adaptive, self-optimizing, self-protecting, and self-managing. These attributes are collectively referred to as self-* properties. Even with the increased focus on self-* research, there exists much ambiguity in the perceptions of the different self-* properties. In this paper, we propose to resolve the ambiguity by introducing a template for defining self-* properties, and use it to offer formal definitions of existing self-* terms. We then present some observations about the relationships among the different self-* properties. Finally, we propose two new self-* properties that are meaningful in this space.",
Exploiting In-Zone Broadcasts for Cache Sharing in Mobile Ad Hoc Networks,"The problem of cache sharing for supporting data access in mobile ad hoc networks is studied in this paper. The key to this problem is to discover a requested data item in an efficient manner. In the paper, we propose two caching protocols, IXP and DPIP, which distinguish themselves from the existing ones in that they fully exploit in-zone broadcasts to facilitate cache sharing operation. In particular, the DPIP protocol offers an implicit index push property, which is highly useful for enhancing cache hit ratio in the neighborhood of a data requester node. Moreover, our protocols also exploit the broadcasts to facilitate the design of a simple but efficient count-based cache replacement scheme. Performance study shows that the proposed protocols can significantly improve the performance of data access in a mobile ad hoc network.","Broadcasting,
Mobile ad hoc networks,
Access protocols,
Mobile computing,
Telecommunication traffic,
Information retrieval,
Wireless communication,
Base stations,
Bandwidth,
Communications technology"
Independent navigation of multiple mobile robots with hybrid reciprocal velocity obstacles,"We present an approach for smooth and collision-free navigation of multiple mobile robots amongst each other. Each robot senses its surroundings and acts independently without central coordination or communication with other robots. Our approach uses both the current position and the velocity of other robots to predict their future trajectory in order to avoid collisions. Moreover, our approach is reciprocal and avoids oscillations by explicitly taking into account that the other robots also sense their surroundings and change their trajectories accordingly. We build on prior work related to velocity obstacles and reciprocal velocity obstacles and introduce the concept of hybrid reciprocal velocity obstacles for collision avoidance that takes into account the kinematics of the robots and uncertainty in sensor data. We apply our approach to a set of iRobot Create robots using centralized sensing and show natural, direct, and collision-free navigation in several challenging scenarios.","Navigation,
Mobile robots,
Robot kinematics,
Robot sensing systems,
Mobile communication,
Collision avoidance,
Contracts,
Intelligent robots,
USA Councils,
Trajectory"
Effects of Different Imaging Models on Least-Squares Image Reconstruction Accuracy in Photoacoustic Tomography,"In the classic formulation of photoacoustic tomography (PAT), two distinct descriptions of the imaging model have been employed for developing reconstruction algorithms. We demonstrate that the numerical and statistical properties of unweighted least-squares reconstruction algorithms associated with each imaging model are generally very different. Specifically, some PAT reconstruction algorithms, including many of the iterative algorithms previously explored, do not work directly with the raw measured pressure wavefields, but rather with an integrated data function that is obtained by temporally integrating the photoacoustic wavefield. The integration modifies the statistical distribution of the data, introducing statistical correlations among samples. This change is highly significant for iterative algorithms, many of which explicitly or implicitly seek to minimize a statistical cost function. In this work, we demonstrate that iterative reconstruction by least-squares minimization yields better resolution-noise tradeoffs when working with the raw pressure data than with the integrated data commonly employed. In addition, we demonstrate that the raw-data based approach is less sensitive to certain deterministic errors, such as dc offset errors.","Image reconstruction,
Tomography,
Biomedical optical imaging,
Optical imaging,
Reconstruction algorithms,
Biomedical measurements,
Optical sensors,
Biomedical imaging,
Biomedical engineering,
Iterative algorithms"
Discriminative structure learning of hierarchical representations for object detection,"A variety of flexible models have been proposed to detect objects in challenging real world scenes. Motivated by some of the most successful techniques, we propose a hierarchical multi-feature representation and automatically learn flexible hierarchical object models for a wide variety of object classes. To that end we not only rely on automatic selection of relevant individual features, but go beyond previous work by automatically selecting and modeling complex, long-range feature couplings within this model. To achieve this generality and flexibility our work combines structure learning in conditional random fields and discriminative parameter learning of classifiers using hierarchical features. We adopt an efficient gradient based heuristic for model selection and carry it forward to discriminative, multidimensional selection of features and their couplings for improved detection performance. Experimentally we consistently outperform the currently leading method on all 20 classes of the PASCAL VOC 2007 challenge and achieve the best published results on 16 of 20 classes.",Object detection
Sourcerer: An internet-scale software repository,"Vast quantities of open source code are now available online, presenting a great potential resource for software developers. Yet the current generation of open source code search engines fail to take advantage of the rich structural information contained in the code they index. We have developed Sourcerer, an infrastructure for large-scale indexing and analysis of open source code. By taking full advantage of this structural information, Sourcerer provides a foundation upon which state of the art search engines and related tools easily be built. We describe the Sourcerer infrastructure, present the applications that we have built on top of it, and discuss how existing tools could benefit from using Sourcerer.","Internet,
Search engines,
Open source software,
Libraries,
Large-scale systems,
Indexing,
Java,
Information retrieval,
Computer architecture,
Feature extraction"
First steps towards a general SysML model for discrete processes in production systems,"In many areas of science, like computer science or electrical engineering, modeling languages have been established, however, this is not the case in the field of discrete processes (Weilkiens 2006). There are two reasons which motivate such a development: 1. Modeling languages allow realizing projects by the principles of systems engineering. So one obtains clearness even for large projects and reduces the discrepancy between model and reality. 2. Modeling languages are a central part of automatic code generation. In this paper, we present our first steps in developing a simulation-tool-independent description of production systems and first ideas on how to convert such a general model into simulation-tool-specific models.",
Error-Correction of Multidimensional Bursts,"We present several methods and constructions to generate binary codes for correction of a multidimensional cluster- error, whose shape can be a box-error, a Lee sphere error, or an error with an arbitrary shape. Our codes have very low redundancy, close to optimal, and a large range of parameters of arrays and clusters. Our main results are summarized as follows. 1) A construction of two-dimensional codes capable to correct a rectangular-error with considerably more flexible parameters from previously known constructions. This construction is easily generalized for D dimensions. 2) A novel method based on D colorings of the D -dimensional space for constructing D -dimensional codes correcting a D -dimensional cluster-error of various shapes. 3) A transformation of the D -dimensional space into another D -dimensional space in a way that a D -dimensional Lee sphere is transformed into a shape located in a D-dimensional box of a relatively small size. 4) Applying the coloring method to correct more efficiently a two-dimensional error whose shape is a Lee sphere. 5) A construction of D -dimensional codes capable to correct a D -dimensional cluster-error of size b in which the number of erroneous positions is relatively small compared to b. 6) We present a code which corrects a D -dimensional arbitrary cluster-error with relatively small redundancy.","Multidimensional systems,
Shape,
Redundancy,
Error correction codes,
Optical recording,
Holographic optical components,
Computer science,
Binary codes,
Holography"
On the Design and Implementation of a Cache-Aware Multicore Real-Time Scheduler,"Multicore architectures, which have multiple processing units on a single chip, have been adopted by most chip manufacturers. Most such chips contain on-chip caches that are shared by some or all of the cores on the chip. Prior work has presented methods for improving the performance of such caches when scheduling soft real-time workloads. Given these methods, two additional research issues arise: (1) how to automatically profile the cache behavior of real-time tasks within the scheduler; and (2) how to implement scheduling methods efficiently, so that scheduling overheads do not offset any cache-related performance gains. This paper addresses these two issues in an implementation of a cache-aware soft real-time scheduler within Linux, and shows that the use of this scheduler can result in performance improvements that directly result from a decrease in shared cache miss rates.","Multicore processing,
Job shop scheduling,
Real time systems,
Sun,
Processor scheduling,
Manufacturing processes,
Performance gain,
System performance,
Computer science,
Computer architecture"
Real-time distributed monitoring of electromagnetic pollution in urban environments,"In the last two decades, the wide diffusion of mobile phones and wireless technologies has brought many advantages in professional activities as well as in everyday life. In general, mobile communication networks are constituted by a limited number of base stations. The spatial distribution of the field radiated from base stations turns out to be non-uniform with variations both in time and space that can exceed the normative limits of electromagnetic emissions. In order to real-time and in a long-term monitor the electromagnetic emissions, an innovative low cost solution based on a wireless sensor network (WSN) is presented in this paper. Such a network consists of spatially distributed and wirelessly connected autonomous devices (called nodes) that use suitable sensor to cooperatively collect physical quantities. The proposed solution considers the deployment of a set of WSN nodes, equipped with a broadband field probe, to monitor the level of the electromagnetic radiation in a distributed and real-time fashion.","Monitoring,
Urban pollution,
Electromagnetic radiation,
Wireless sensor networks,
Space technology,
Base stations,
Mobile handsets,
Professional activities,
Mobile communication,
Space stations"
Language-Based Isolation of Untrusted JavaScript,"Web sites that incorporate untrusted content may use browser- or language-based methods to keep such content from maliciously altering pages, stealing sensitive information, or causing other harm. We study language-based methods for filtering and rewriting JavaScript code, using Yahoo! ADSafe and Facebook FBJS as motivating examples. We explain the core problems by describing previously unknown vulnerabilities and subtleties, and develop a foundation for improved solutions based on an operational semantics of the full ECMA-262 language. We also discuss how to apply our analysis to address the JavaScript isolation problems we discovered.","Java,
Facebook,
Filtering,
Advertising,
Social network services,
Computer security,
Educational institutions,
Computer science,
USA Councils,
Electronic mail"
Semantic segmentation of street scenes by superpixel co-occurrence and 3D geometry,"We present a novel approach for image semantic segmentation of street scenes into coherent regions, while simultaneously categorizing each region as one of the predefined categories representing commonly encountered object and background classes. We formulate the segmentation on small blob-based superpixels and exploit a visual vocabulary tree as an intermediate image representation. The main novelty of this generative approach is the introduction of an explicit model of spatial co-occurrence of visual words associated with super-pixels and utilization of appearance, geometry and contextual cues in a probabilistic framework. We demonstrate how individual cues contribute towards global segmentation accuracy and how their combination yields superior performance to the best known method on the challenging benchmark dataset which exhibits diversity of street scenes with varying viewpoints, large number of categories, captured in daylight and dusk.","Layout,
Geometry,
Image segmentation,
Vocabulary,
Object detection,
Face detection,
Labeling,
Context modeling,
Computer vision,
Shape"
Arm movements effect on ultra wideband on-body propagation channels and radio systems,This paper presents experimental investigation of ultra wideband on-body radio channel in both the anechoic chamber and indoor environments including effects of time varying movements of various body parts on the the channel characteristics. Measured data are used to extract radio propagation channel parameters and investigate the influence of body movements on derived channel models. These models are applied in conjunction with different modulation techniques commonly used for impulse radios to evaluate the system performance of on-body UWB radio systems. Bit error rate and signal-to-noise ratio studies show that careful considerations need to be taken when choosing the modulation technique for optimal ultra wideband body-centric systems.,"Ultra wideband technology,
Antennas and propagation,
Radio propagation,
Wrist,
Indoor environments,
Body sensor networks,
Performance evaluation,
Antenna measurements,
Anechoic chambers,
System performance"
File Fragment Classification-The Case for Specialized Approaches,"Increasingly advances in file carving, memory analysis and network forensics requires the ability to identify the underlying type of a file given only a file fragment. Work to date on this problem has relied on identification of specific byte sequences in file headers and footers, and the use of statistical analysis and machine learning algorithms taken from the middle of the file. We argue that these approaches are fundamentally flawed because they fail to consider the inherent internal structure in widely used file types such as PDF, DOC, and ZIP. We support our argument with a bottom-up examination of some popular formats and an analysis of TK PDF files. Based on our analysis, we argue that specialized methods targeted to each specific file type will be necessary to make progress in this area.",
A multi-objective approach to Redundancy Allocation Problem in parallel-series systems,"The Redundancy Allocation Problem (RAP) is a kind of reliability optimization problems. It involves the selection of components with appropriate levels of redundancy or reliability to maximize the system reliability under some predefined constraints. We can formulate the RAP as a combinatorial problem when just considering the redundancy level, while as a continuous problem when considering the reliability level. The RAP employed in this paper is that kind of combinatorial optimization problems. During the past thirty years, there have already been a number of investigations on RAP. However, these investigations often treat RAP as a single objective problem with the only goal to maximize the system reliability (or minimize the designing cost). In this paper, we regard RAP as a multi-objective optimization problem: the reliability of the system and the corresponding designing cost are considered as two different objectives. Consequently, we can utilize a classical Multi-objective Evolutionary Algorithm (MOEA), named Non-dominated Sorting Genetic Algorithm II (NSGA-II), to cope with this multi-objective redundancy allocation problem (MORAP) under a number of constraints. The experimental results demonstrate that the multi-objective evolutionary approach can provide more promising solutions in comparison with two widely used single-objective approaches on two parallel-series systems which are frequently studied in the field of reliability optimization.","Redundancy,
Reliability,
Cost function,
Design optimization,
Evolutionary computation,
Genetic algorithms,
Dynamic programming,
Availability,
Sorting,
Linear programming"
Space-Efficient Framework for Top-k String Retrieval Problems,"Given a set
={
d
1
,
d
2
,...,
d
D
}
of
D
strings of total length
n
, our task is to report the ""most relevant""strings for a given query pattern
P
. This involves somewhat more advanced query functionality than the usual pattern matching, as some notion of ""most relevant"" is involved. In information retrieval literature, this task is best achieved by using inverted indexes. However, inverted indexes work only for some predefined set of patterns. In the pattern matching community, the most popular pattern-matching data structures are suffix trees and suffix arrays. However, a typical suffix tree search involves going through all the occurrences of the pattern over the entire string collection, which might be a lot more than the required relevant documents. The first formal framework to study such kind of retrieval problems was given by [Muthukrishnan, 2002]. He considered two metrics for relevance: frequency and proximity. He took a threshold-based approach on these metrics and gave data structures taking
Undefined control sequence \logn
words of space. We study this problem in a slightly different framework of reporting the top
k
most relevant documents (in sorted order) under similar and more general relevance metrics. Our framework gives linear space data structure with optimal query times for arbitrary score functions. As a corollary, it improves the space utilization for the problems in [Muthukrishnan, 2002] while maintaining optimal query performance. We also develop compressed variants of these data structures for several specific relevance metrics.","Data structures,
Pattern matching,
Computer science,
Information retrieval,
USA Councils,
Tree data structures,
Frequency,
Extraterrestrial measurements,
Indexing,
Databases"
A Cognitive Radio Network Architecture without Control Channel,"The spectrum-agile cognitive radio has been developed to significantly increase spectrum utilization and relieve the spectrum exhaustion problem, by enabling secondary users to dynamically access the licensed spectrum bands. As such cognitive radio will be a key feature of future wireless technologies. In this paper, we propose an architecture for cognitive radio network (CRN). Our architecture uses one radio per node and does not need a common control channel, and is highly adaptable and resilient to maintain connectivity between neighboring nodes. In particular, we present a channel selection algorithm that not only spreads nodes into different channels to reduce cochannel interference, but also enables a node to easily compute the channel of a neighbor without the need to negotiate with the neighbor, which is highly desirable for CRN, as a channel may become inaccessible abruptly due to that the licensed user suddenly starts using it. Simulation results show that our CRN model and channel selection algorithm are highly adaptable and resilient to dynamic channels, and can achieve a close performance to the scheme that uses an extra radio and a static control channel to exchange channel information.","Cognitive radio,
Radio control,
Interchannel interference,
Wireless networks,
Communication system control,
Computer science,
Computer architecture,
Computational modeling,
Radio frequency,
Throughput"
A flat direct model for speech recognition,"We introduce a direct model for speech recognition that assumes an unstructured, i.e., flat text output. The flat model allows us to model arbitrary attributes and dependences of the output. This is different from the HMMs typically used for speech recognition. This conventional modeling approach is based on sequential data and makes rigid assumptions on the dependences. HMMs have proven to be convenient and appropriate for large vocabulary continuous speech recognition. Our task under consideration, however, is the Windows Live Search for Mobile (WLS4M) task [1]. This is a cellphone application that allows users to interact with web-based information portals. In particular, the set of valid outputs can be considered discrete and finite (although probably large, i.e., unseen events are an issue). Hence, a flat direct model lends itself to this task, making the adding of different knowledge sources and dependences straightforward and cheap. Using e.g. HMM posterior, m-gram, and spotter features, significant improvements over the conventional HMM system were observed.","Speech recognition,
Hidden Markov models,
Vocabulary,
Entropy,
Testing,
Detectors,
Computer science,
Cellular phones,
Portals,
Natural languages"
Distributed triangulation in the presence faulty and byzantine beacons in aircraft networks with ADS-B technology,"This paper explores two different algorithms designed for quick triangulation in the face of numerous incorrect measurements. The incorrect measurements can be randomly faulty or maliciously converging to an incorrect answer. Both algorithms require the number of correct measurements to exceed a user defined consensus threshold. Both algorithms will correctly terminate in an environment possessing more than 50% faulty beacons, as long as the number of correct measurements exceed the consensus threshold.","Broadcasting,
Air traffic control,
Aircraft navigation,
Robustness,
Global Positioning System,
Filters,
Computer science,
Least squares approximation,
Greedy algorithms,
Aerospace control"
Inertial-aided KLT feature tracking for a moving camera,"We propose a novel inertial-aided KLT feature tracking method robust to camera ego-motions. The conventional KLT uses images only and its working condition is inherently limited to small appearance change between images. When big optical flows are induced by a camera-ego motion, an inertial sensor attached to the camera can provide a good prediction to preserve the tracking performance. We use a low-grade MEMS-based gyroscope to refine an initial condition of the nonlinear optimization in the KLT. It increases the possibility for warping parameters to be in the convergence region of the KLT. For longer tracking with less drift, we use the affine photometric model and it can effectively deal with camera rolling and outdoor illumination change. Extra computational cost caused by this higher-order motion model is alleviated by restraining the Hessian update and GPU acceleration. Experimental results are provided for both indoor and outdoor scenes and GPU implementation issues are discussed.","Karhunen-Loeve transforms,
Cameras,
Optical sensors,
Robustness,
Employee welfare,
Nonlinear optics,
Optical variables control,
Image motion analysis,
Tracking,
Gyroscopes"
An adaptive learning particle swarm optimizer for function optimization,"Traditional particle swarm optimization (PSO) suffers from the premature convergence problem, which usually results in PSO being trapped in local optima. This paper presents an adaptive learning PSO (ALPSO) based on a variant PSO learning strategy. In ALPSO, the learning mechanism of each particle is separated into three parts: its own historical best position, the closest neighbor and the global best one. By using this individual level adaptive technique, a particle can well guide its behavior of exploration and exploitation. A set of 21 test functions were used including un-rotated, rotated and composition functions to test the performance of ALPSO. From the comparison results over several variant PSO algorithms, ALPSO shows an outstanding performance on most test functions, especially the fast convergence characteristic.","Particle swarm optimization,
Convergence,
Testing,
Learning systems,
Organisms,
Birds,
Marine animals,
Educational institutions,
Cognition,
Cultural differences"
Process variation aware thread mapping for Chip Multiprocessors,"With the increasing scaling of manufacturing technology, process variation is a phenomenon that has become more prevalent. As a result, in the context of Chip Multiprocessors (CMPs) for example, it is possible that identically-designed processor cores on the chip have non-identical peak frequencies and power consumptions. To cope with such a design, each processor can be assumed to run at the frequency of the slowest processor, resulting in wasted computational capability. This paper considers an alternate approach and proposes an algorithm that intelligently maps (and remaps) computations onto available processors so that each processor runs at its peak frequency. In other words, by dynamically changing the thread-to-processor mapping at runtime, our approach allows each processor to maximize its performance, rather than simply using chip-wide lowest frequency amongst all cores and highest cache latency. Experimental evidence shows that, as compared to a process variation agnostic thread mapping strategy, our proposed scheme achieves as much as 29% improvement in overall execution latency, average improvement being 13% over the benchmarks tested. We also demonstrate in this paper that our savings are consistent across different processor counts, latency maps, and latency distributions.With the increasing scaling of manufacturing technology, process variation is a phenomenon that has become more prevalent. As a result, in the context of Chip Multiprocessors (CMPs) for example, it is possible that identically-designed processor cores on the chip have non-identical peak frequencies and power consumptions. To cope with such a design, each processor can be assumed to run at the frequency of the slowest processor, resulting in wasted computational capability. This paper considers an alternate approach and proposes an algorithm that intelligently maps (and remaps) computations onto available processors so that each processor runs at its peak frequency. In other words, by dynamically changing the thread-to-processor mapping at runtime, our approach allows each processor to maximize its performance, rather than simply using chip-wide lowest frequency amongst all cores and highest cache latency. Experimental evidence shows that, as compared to a process variation agnostic thread mapping strategy, our proposed scheme achieves as much as 29% improvement in overall execution latency, average improvement being 13% over the benchmarks tested. We also demonstrate in this paper that our savings are consistent across different processor counts, latency maps, and latency distributions.","Yarn,
Frequency,
Delay,
Manufacturing processes,
Energy consumption,
Process design,
Computational intelligence,
Runtime,
Benchmark testing,
Pulp manufacturing"
Visual Exploration of Three-Dimensional Gene Expression Using Physical Views and Linked Abstract Views,"During animal development, complex patterns of gene expression provide positional information within the embryo. To better understand the underlying gene regulatory networks, the Berkeley Drosophila Transcription Network Project (BDTNP) has developed methods that support quantitative computational analysis of three-dimensional (3D) gene expression in early Drosophila embryos at cellular resolution. We introduce PointCloudXplore (PCX), an interactive visualization tool that supports visual exploration of relationships between different genes' expression using a combination of established visualization techniques. Two aspects of gene expression are of particular interest: 1) gene expression patterns defined by the spatial locations of cells expressing a gene and 2) relationships between the expression levels of multiple genes. PCX provides users with two corresponding classes of data views: 1) physical views based on the spatial relationships of cells in the embryo and 2) abstract views that discard spatial information and plot expression levels of multiple genes with respect to each other. Cell selectors highlight data associated with subsets of embryo cells within a View. Using linking, these selected cells can be viewed in multiple representations. We describe PCX as a 3D gene expression visualization tool and provide examples of how it has been used by BDTNP biologists to generate new hypotheses.","Gene expression,
Embryo,
Data visualization,
Computer science,
Animals,
Computer networks,
Spatial resolution,
Bioinformatics,
Biology computing,
Cellular networks"
A pervasive and preventive healthcare solution for medication noncompliance and daily monitoring,"Pervasive healthcare solution for medication noncompliance problem would help to save $177 billion annually in the United States. And the rapidly increasing demanding of daily monitoring with onsite diagnosis and prognosis is driving homecare solutions to integrate more and more sensing and data processing capacities. So a powerful system is needed not only to address the medication noncompliance but also to be used as a Pervasive Healthcare Station in home. In this paper, a pervasive and preventive healthcare solution for medication noncompliance and daily monitoring is proposed using an intelligent package sealed by Controlled Delamination Material (CDM) and controlled by Radio Frequency Identification (RFID). Onsite diagnosis and prognosis capacities for kinds of health parameters are supported due to scalable and intensive computing capacitance of the 2D-Mesh-NoC based multi-core architecture. Additionally, friendly human-machine interface is emphasized to make it usable for the elderly, disabled and patients due to enhanced multimedia performance. Experimental results of an implemented prototype confirmed the necessity of the multi-core architecture and approved the feasibility of the proposed intelligent package.","Medical services,
Monitoring,
Medical diagnostic imaging,
Packaging,
Medical control systems,
Radio control,
Radiofrequency identification,
Computer architecture,
Data processing,
Delamination"
Line Segmentation for Degraded Handwritten Historical Documents,We propose a novel approach for text line segmentation based on adaptive local projection profiles. Our algorithm is suitable for degraded documents with text lines written in large skew. The main novelty of our approach is applying the local algorithm in an incremental manner that adapts to the skew of each text line as it progresses. The proposed approach achieves very accurate results on a set of degraded documents with lines written in different skew angles and curvatures.,"Degradation,
Image segmentation,
Text analysis,
Image analysis,
Handwriting recognition,
Text recognition,
Computer science,
Noise robustness,
Ink"
Mining opinion from text documents: A survey,"Opinion Mining is a process, used for automatic extraction of knowledge from the opinion of others about some particular topic or problem. With the growing availability of online resources on web and popularity of fast and rich resources of opinion sharing such as online review sites and personal blogs, Opinion Mining has become an interesting area of research. World Wide Web is a fastest medium for opinion collection from users. Human perception and user opinion has greater potential for knowledge discovery and decision support. In this paper we have presented a survey which covers techniques and methods that promise to enable us to get opinion oriented information from text. This research effort deals with techniques and challenges related to sentiment analysis and Opinion Mining. We have followed systematic literature review process to conduct this survey. Our focus was mainly on machine learning techniques on the basis of their usage and importance for opinion mining. We have tried to identify most commonly used classification techniques for opinionated documents to assist future research in this area.",
Wideband 60GHz on-chip antenna with an artificial magnetic conductor,"A wideband 60-GHz on-chip antenna fabricated with a 0.18um CMOS process with an artificial magnetic conductor (AMC) is presented. A shield plane is patterned to create mesh and inserted between the on-chip antenna and the grounded lossy CMOS substrate. With this arrangement, a frequency selective surface is produced and the meshed shield plane provides high wave impedance over a certain bandwidth around 60 GHz. Meanwhile, the meshed shield plane behaves as an AMC and the reflected wave is in phase with the incident wave. Therefore, the loss induced by the CMOS substrate can be minimized. Both the AMC and the patch antenna are optimized to deliver a wideband operation covering 56–66 GHz. The size of the on-chip antenna including the AMC is 1.55mm by 1.55mm. Compared with the conventional on-chip antenna having a typical gain of −10dBi to −8dBi, the proposed on-chip antenna with an AMC can offer a gain of −2.5dBi to −1.5 dBi in the band. Measurement has verified the prediction.","Broadband antennas,
Conductors,
CMOS process,
Silicon,
Bandwidth,
Patch antennas,
Electromagnetic reflection,
Radio frequency,
CMOS technology,
Surface impedance"
Bregman Divergences and Surrogates for Learning,"Bartlett et al. (2006) recently proved that a ground condition for surrogates, classification calibration, ties up their consistent minimization to that of the classification risk, and left as an important problem the algorithmic questions about their minimization. In this paper, we address this problem for a wide set which lies at the intersection of classification calibrated surrogates and those of Murata et al. (2004). This set coincides with those satisfying three common assumptions about surrogates. Equivalent expressions for the members-sometimes well known-follow for convex and concave surrogates, frequently used in the induction of linear separators and decision trees. Most notably, they share remarkable algorithmic features: for each of these two types of classifiers, we give a minimization algorithm provably converging to the minimum of any such surrogate. While seemingly different, we show that these algorithms are offshoots of the same ldquomasterrdquo algorithm. This provides a new and broad unified account of different popular algorithms, including additive regression with the squared loss, the logistic loss, and the top-down induction performed in CART, C4.5. Moreover, we show that the induction enjoys the most popular boosting features, regardless of the surrogate. Experiments are provided on 40 readily available domains.","Minimization methods,
Particle separators,
Decision trees,
Boosting,
Supervised learning,
Convergence,
Logistics,
Calibration,
Risk management,
Additives"
SHIMMER: A new tool for temporal gait analysis,"Development of a flexible wireless sensor platform for measurement of biomechanical and physiological variables related to functional movement would be a vital step towards effective ambulatory monitoring and early detection of risk factors in the ageing population. The small form factor, wirelessly enabled SHIMMER platform has been developed towards this end. This study is focused assessing the utility of the SHIMMER for use in ambulatory human gait analysis. Temporal gait parameters derived from a tri-axial gyroscope contained in the SHIMMER are compared against those acquired simultaneously using the CODA motion analysis system. Results from a healthy adult male subject show excellent agreement (ICC(2, k) > 0.85) in stride, swing and stance time for 10 walking trials and 4 run trials. The mean differences using the Bland and Altman method for stance, stride and swing times were 0.0087, 0.0044 and -0.0061 seconds respectively. These results suggest that the SHIMMER is a versatile cost effective tool for use in temporal gait analysis.","Biomedical monitoring,
Legged locomotion,
Wireless sensor networks,
Motion measurement,
Laboratories,
Gyroscopes,
Motion analysis,
Kinematics,
Semiconductor device measurement,
Sensor systems"
Expression-insensitive 3D face recognition using sparse representation,"We present a face recognition method based on sparse representation for recognizing 3D face meshes under expressions using low-level geometric features. First, to enable the application of the sparse representation framework, we develop a uniform remeshing scheme to establish a consistent sampling pattern across 3D faces. To handle facial expressions, we design a feature pooling and ranking scheme to collect various types of low-level geometric features and rank them according to their sensitivities to facial expressions. By simply applying the sparse representation framework to the collected low-level features, our proposed method already achieves satisfactory recognition rates, which demonstrates the efficacy of the framework for 3D face recognition. To further improve results in the presence of severe facial expressions, we show that by choosing higher-ranked, i.e., expression-insensitive, features, the recognition rates approach those for neutral faces, without requiring an extensive set of reference faces for each individual to cover possible variations caused by expressions as proposed in previous work. We apply our face recognition method to the GavabDB and FRGC 2.0 databases and demonstrate encouraging results.","Face recognition,
Graphical models,
Emotion recognition,
Image sequences,
Independent component analysis,
Facial features,
Information science,
Computer science,
Information analysis,
Labeling"
RF-GPS: RFID assisted localization in VANETs,"Providing vehicles' position is essential in VANETs. Currently, GPS positioning is widely used, but the accuracy is not adequate for emerging safety applications. In order to provide accurate positioning, this paper proposes RF-GPS, a RFID-assisted localization system that reliably supports lane-level position accuracy. It improves accuracy of the GPS system by employing a DGPS-like concept. It also allows vehicles without GPS to compute their position by contacting GPS equipped neighbors. We evaluate the performance of the proposed localization system via simulation.","Radiofrequency identification,
Global Positioning System,
Accidents,
Vehicle driving,
Ad hoc networks,
Collision avoidance,
Internet,
Road vehicles,
Vehicle safety,
Vehicle detection"
Kernel Granger Causality Mapping Effective Connectivity on fMRI Data,"Although it is accepted that linear Granger causality can reveal effective connectivity in functional magnetic resonance imaging (fMRI), the issue of detecting nonlinear connectivity has hitherto not been considered. In this paper, we address kernel Granger causality (KGC) to describe effective connectivity in simulation studies and real fMRI data of a motor imagery task. Based on the theory of reproducing kernel Hilbert spaces, KGC performs linear Granger causality in the feature space of suitable kernel functions, assuming an arbitrary degree of nonlinearity. Our results demonstrate that KGC captures effective couplings not revealed by the linear case. In addition, effective connectivity networks between the supplementary motor area (SMA) as the seed and other brain areas are obtained from KGC.","Kernel,
Marine technology,
Educational technology,
Magnetic resonance imaging,
Brain modeling,
Electroencephalography,
Spatial resolution,
Hilbert space,
Couplings,
Neuroimaging"
Justifying and Generalizing Contrastive Divergence,"We study an expansion of the log likelihood in undirected graphical models such as the restricted Boltzmann machine (RBM), where each term in the expansion is associated with a sample in a Gibbs chain alternating between two random variables (the visible vector and the hidden vector in RBMs). We are particularly interested in estimators of the gradient of the log likelihood obtained through this expansion. We show that its residual term converges to zero, justifying the use of a truncation—running only a short Gibbs chain, which is the main idea behind the contrastive divergence (CD) estimator of the log-likelihood gradient. By truncating even more, we obtain a stochastic reconstruction error, related through a mean-field approximation to the reconstruction error often used to train autoassociators and stacked autoassociators. The derivation is not specific to the particular parametric forms used in RBMs and requires only convergence of the Gibbs chain. We present theoretical and empirical evidence linking the number of Gibbs steps k and the magnitude of the RBM parameters to the bias in the CD estimator. These experiments also suggest that the sign of the CD estimator is correct most of the time, even when the bias is large, so that CD-k is a good descent direction even for small k.",
"Modeling interaction between individuals, social networks and public policy to support public health epidemiology","Human behavior, social networks, and civil infrastructure are closely intertwined. Understanding their co-evolution is critical for designing public policies. Human behaviors and day-to-day activities of individuals create dense social interactions that provide a perfect fabric for fast disease propagation. Conversely, people's behavior in response to public policies and their perception of the crisis can dramatically alter normally stable social interactions. Effective planning and response strategies must take these complicated interactions into account. The basic problem can be modeled as a coupled co-evolving graph dynamical system and can also be viewed as partially observable Markov decision process. As a way to overcome the computational hurdles, we describe an High Performance Computing oriented computer simulation to study this class of problems. Our method provides a novel way to study the co-evolution of human behavior and disease dynamics in very large, realistic social networks with over 100 Million nodes and 6 Billion edges.","Computational modeling,
Diseases,
Social network services,
Markov processes,
Mathematical model,
Public healthcare,
Public policy"
"SINGO: A single-end-operative and genderless connector for self-reconfiguration, self-assembly and self-healing","Flexible and reliable connection is critical for self-reconfiguration, self-assembly, or self-healing. However, most existing connection mechanisms suffer from a deficiency that a connection would seize itself if one end malfunctions or is out of service. To mitigate this limitation on self-healing, this paper presents a new SINGO connector that can establish or disengage a connection even if one end of the connection is not operational. We describe the design and the prototype of the connector and demonstrate its performance by both theoretical analysis and physical experimentations.",
Complexity of the short-term heart-rate variability,"This work has proposed a methodology based on the concept of entropy rates to study the complexity of the short-term heart-rate variability (HRV) for improving risk stratification to predict sudden cardiac death (SCD) of patients with established ischemic-dilated cardiomyopathy (IDC). The short-term HRV was analyzed during daytime and nighttime by means of RR series. An entropy rate was calculated on the RR series, previously transformed to symbol sequences by means of an alphabet. A statistical analysis permitted to stratify high- and low-risk patients of suffering SCD, with a specificity (SP) of 95% and sensitivity (SE) of 83.3%. To get a better characterization of short-term HRV, the study has also considered the adjustment of the parameters involved in the proposed methodology. Finally, a statistical analysis was applied to recognize valid prognostic markers.",
Game-Based Learning with Ubiquitous Technologies,"E-learning communities have recently started to emphasize the need to make e-learning systems more practical rather than more powerful. Related studies indicate that the appeal of learning content as well as participation in learning activities are primary concerns. The authors propose using advanced ubiquitous technologies to construct a location-aware, digital game-based learning environment. This framework presents the notion of a ubiquitous game-based learning model from technical and experimental viewpoints. An example of the courseware for a class on Tamsui, Taiwan's historical culture demonstrates the learning scenario's viability in the proposed environment.","Electronic learning,
Games,
Virtual environment,
Space technology,
Global Positioning System,
Geographic Information Systems,
Radiofrequency identification,
Courseware,
Pervasive computing,
Computer science education"
Alattin: Mining Alternative Patterns for Detecting Neglected Conditions,"To improve software quality, static or dynamic verification tools accept programming rules as input and detect their violations in software as defects. As these programming rules are often not well documented in practice, previous work developed various approaches that mine programming rules as frequent patterns from program source code. Then these approaches use static defect-detection techniques to detect pattern violations in source code under analysis. These existing approaches often produce many false positives due to various factors. To reduce false positives produced by these mining approaches, we develop a novel approach, called Alattin, that includes a new mining algorithm and a technique for detecting neglected conditions based on our mining algorithm. Our new mining algorithm mines alternative patterns in example form ""P1 or P2"", where P1 and P2 are alternative rules such as condition checks on method arguments or return values related to the same API method. We conduct two evaluations to show the effectiveness of our Alattin approach. Our evaluation results show that (1) alternative patterns reach more than 40% of all mined patterns for APIs provided by six open source libraries; (2) the mining of alternative patterns helps reduce nearly 28% of false positives among detected violations.","Software quality,
Dynamic programming,
Computer science,
USA Councils,
Software tools,
Pattern analysis,
Programming profession,
Software engineering,
Automatic programming,
Software libraries"
"Appearance contrast for fast, robust trail-following","We describe a framework for finding and tracking “trails” for autonomous outdoor robot navigation. Through a combination of visual cues and ladar-derived structural information, the algorithm is able to follow paths which pass through multiple zones of terrain smoothness, border vegetation, tread material, and illumination conditions. Our shape-based visual trail tracker assumes that the approaching trail region is approximately triangular under perspective. It generates region hypotheses from a learned distribution of expected trail width and curvature variation, and scores them using a robust measure of color and brightness contrast with flanking regions. The structural component analogously rewards hypotheses which correspond to empty or low-density regions in a groundstrike-filtered ladar obstacle map. Our system's performance is analyzed on several long sequences with diverse appearance and structural characteristics. Ground-truth segmentations are used to quantify performance where available, and several alternative algorithms are compared on the same data.","Robustness,
Shape,
Laser radar,
Navigation,
Intelligent robots,
Road vehicles,
Unmanned aerial vehicles,
USA Councils,
Vegetation mapping,
Lighting"
A Framework for Haptic Broadcasting,This article presents a comprehensive exploration of the issues underlying haptic multimedia broadcasting. It also describes the implementation of a prototype system as a proof of concept.,"Haptic interfaces,
Multimedia communication,
Broadcast technology,
Digital multimedia broadcasting,
Displays,
Virtual environment,
Streaming media,
Layout,
MPEG 4 Standard,
Motion pictures"
Atomicity Analysis of Service Composition across Organizations,"Atomicity is a highly desirable property for achieving application consistency in service compositions. To achieve atomicity, a service composition should satisfy the atomicity sphere, a structural criterion for the backend processes of involved services. Existing analysis techniques for atomicity sphere generally assume complete knowledge of all involved backend processes. Such an assumption is invalid when some service providers do not release all details of their backend processes to service consumers outside the organizations. To address this problem, we propose a process algebraic framework to publish atomicity-equivalent public views from the backend processes. These public views extract relevant task properties and reveal only partial process details that service providers need to expose. Our framework enables the analysis of atomicity sphere for service compositions using these public views instead of their backend processes. This allows service consumers to choose suitable services such that their composition satisfies the atomicity sphere without disclosing the details of their backend processes. Based on the theoretical result, we present algorithms to construct atomicity-equivalent public views and to analyze the atomicity sphere for a service composition. Two case studies from supply chain and insurance domains are given to evaluate our proposal and demonstrate the applicability of our approach.","Web services,
Privacy,
System recovery,
Supply chains,
Protection,
Application software,
Algorithm design and analysis,
Insurance,
Algebra,
Internet"
Tracking of cell populations to understand their spatio-temporal behavior in response to physical stimuli,"We have developed methods for segmentation and tracking of cells in time-lapse phase-contrast microscopy images. Our multi-object Bayesian algorithm detects and tracks large numbers of cells in presence of clutter and identifies cell division. To solve the data association problem, the assignment of current measurements to cell tracks, we tested various cost functions with both an optimal and a fast, suboptimal assignment algorithm. We also propose metrics to quantify cell migration properties, such as motility and directional persistence, and compared our findings of cell migration with the standard random walk model. We measured how cell populations respond to the physical stimuli presented in the environment, for example, the stiffness property of the substrate. Our analysis of hundreds of spatio-temporal cell trajectories revealed significant differences in the behavioral response of fibroblast cells to changes in hydrogel conditions.","Microscopy,
Cost function,
Image segmentation,
Current measurement,
Fibroblasts,
Image analysis,
Shape,
Tracking,
Iterative algorithms,
Motion analysis"
Multiview point cloud kernels for semisupervised learning [Lecture Notes],"In semisupervised learning (SSL), a predictive model is learn from a collection of labeled data and a typically much larger collection of unlabeled data. These paper presented a framework called multi-view point cloud regularization (MVPCR), which unifies and generalizes several semisupervised kernel methods that are based on data-dependent regularization in reproducing kernel Hilbert spaces (RKHSs). Special cases of MVPCR include coregularized least squares (CoRLS), manifold regularization (MR), and graph-based SSL. An accompanying theorem shows how to reduce any MVPCR problem to standard supervised learning with a new multi-view kernel.","Clouds,
Kernel,
Semisupervised learning,
Estimation error,
Approximation error,
Convergence,
Signal processing algorithms,
Support vector machines,
Hilbert space"
A joint power and subchannel allocation scheme maximizing system capacity in dense femtocell downlink systems,"Femtocell system is expected to bring significant improvement of system performance with low cost. However, when a number of femto BSs are installed by users without cell-planning, the femtocell system will be in dense environment, in which many femto co-cells exist in a small region and a great portion of the femto cell is overlapped by other femto cells. Femtocell system in dense environment is exposed to strong inter-cell interference problem which is critical to system capacity. In this paper, we derive a joint power and subchannel allocation scheme in dense environment. The simulation and numerical results show that our proposed scheme simply finds a better solution compared to the conventional scheme in dense environment.","Downlink,
Interference,
Resource management,
Mobile communication,
Radio spectrum management,
System performance,
Costs,
Power system management,
DSL,
Crosstalk"
Vision-tactile-force integration and robot physical interaction,"This paper presents an approach for integrating vision, tactile and force sensors in a robotic manipulation framework. Having an initial estimation of the object pose in the environment, a position-based visual servoing loop controls the hand for task execution, based on the input received from a model-based articular pose estimator following the Virtual Visual Servoing approach. The visual control is combined with another control signal obtained from tactile feedback, through a set of selection matrices that can be modified at runtime in order to select the best modality for a given cartesian degree of freedom. The result of the preliminary integration is modified by an impedance force controller, in charge of performing the task motion along the task direction, at the same time that forces are regulated on the rest of directions. The design of the controller allows to perform the task even if a sensor is not available or provides inaccurate data. Several experiments are performed, first by considering only force feedback, and then adding vision and, finally, tactile information. Errors in the estimation of the object initial position are manually introduced in the experiments, and results show how the vision-tactile-force combination is able to deal with them, performing much better than the vision-force and force-alone approaches.","Robot vision systems,
Robot sensing systems,
Force sensors,
Visual servoing,
Force control,
Runtime,
Impedance,
Motion control,
Force feedback,
Estimation error"
A design and analysis tool for underactuated compliant hands,"Highly underactuated and passively adaptive robotic hands have shown great promise for robust performance in unstructured settings. In order to fully realize this potential, efficient tools are needed to analyze the execution of a grasp when using this class of devices. Along this line, this paper introduces a quasistatic analysis method for underactuated hands. First, we predict whether initial contacts between the fingers and the object are stable throughout the execution of a grasp, or the fingers will slip as the hand closes. Second, we compute the unbalanced forces applied to the object during the grasping process. Finally, once the grasp is complete, we analyze its stability as actuator forces are increased. These computations are performed in 3D, allow arbitrary kinematic structure of the fingers or geometry of the target object and take into account frictional constraints. We discuss applications of this method focusing on both on-line computation to execute a specific grasping task and off-line optimization to increase the range of grasps that can be performed using a given hand model.","Fingers,
Grasping,
Design optimization,
Intelligent robots,
Actuators,
Kinematics,
Robot sensing systems,
Couplings,
Geometry,
USA Councils"
Multi-modal speaker diarization of real-world meetings using compressed-domain video features,Speaker diarization is originally defined as the task of determining “who spoke when” given an audio track and no other prior knowledge of any kind. The following article shows a multi-modal approach where we improve a state-of-the-art speaker diarization system by combining standard acoustic features (MFCCs) with compressed domain video features. The approach is evaluated on over 4.5 hours of the publicly available AMI meetings dataset which contains challenges such as people standing up and walking out of the room. We show a consistent improvement of about 34% relative in speaker error rate (21% DER) compared to a state-of-the-art audio-only baseline.,"Video compression,
Cameras,
Loudspeakers,
Speech,
Mouth,
Computer science,
Ambient intelligence,
Legged locomotion,
Error analysis,
Density estimation robust algorithm"
A bug you like: A framework for automated assignment of bugs,"Assigning bug reports to individual developers is typically a manual, time-consuming, and tedious task. In this paper, we present a framework for automated assignment of bug-fixing tasks. Our approach employs preference elicitation to learn developer predilections in fixing bugs within a given system. This approach infers knowledge about a developer's expertise by analyzing the history of bugs previously resolved by the developer. We apply a vector space model to recommend experts for resolving bugs. When a new bug report arrives, the system automatically assigns it to the appropriate developer considering his or her expertise, current workload, and preferences. We address the task allocation problem by proposing a set of heuristics that support accurate assignment of bug reports to the developers.","Computer bugs,
History,
Artificial intelligence,
Data mining,
Information retrieval,
Computer science,
Digital cameras,
Automatic control,
Machine learning"
Heterogeneous feature machines for visual recognition,"With the recent efforts made by computer vision researchers, more and more types of features have been designed to describe various aspects of visual characteristics. Modeling such heterogeneous features has become an increasingly critical issue. In this paper, we propose a machinery called the Heterogeneous Feature Machine (HFM) to effectively solve visual recognition tasks in need of multiple types of features. Our HFM builds a kernel logistic regression model based on similarities that combine different features and distance metrics. Different from existing approaches that use a linear weighting scheme to combine different features, HFM does not require the weights to remain the same across different samples, and therefore can effectively handle features of different types with different metrics. To prevent the model from overfitting, we employ the so-called group LASSO constraints to reduce model complexity. In addition, we propose a fast algorithm based on co-ordinate gradient descent to efficiently train a HFM. The power of the proposed scheme is demonstrated across a wide variety of visual recognition tasks including scene, event and action recognition.","Kernel,
Computer vision,
Shape measurement,
Support vector machines,
Character recognition,
Laboratories,
Statistics,
Machinery,
Logistics,
Layout"
Source Camera Identification Using Large Components of Sensor Pattern Noise,,"Image sensors,
Digital images,
Forensics,
Fingerprint recognition,
Data mining,
Digital cameras,
Computational complexity,
Dark current,
Sensor phenomena and characterization,
Chaos"
Design and control of a high performance SCARA type robotic arm with rotary hydraulic actuators,"This study proposes a Selective Compliant Assembly Robotic Arm (SCARA) with two revolute joints for poultry deboning. The joints of the arm are based on two high performance rotary type hydraulic actuators. These actuators are operated by servo valves, which control hydraulic fluid flow and direction. A PID based independent joint control system is considered for controlling the position of each joint. The system was modelled using the MATLAB - SIMULINK toolbox. The simulation results show that the arm was capable of covering a work envelope of 0.9 m × 0.9 m, reaching controlled velocities of up to 7.5 m/s with an average of 5.8 m/s. Obtaining such high speeds and torques would be a difficult task with electrical actuators of the capacity as the hydraulic counterparts considered here.","Robots,
Hydraulic actuators,
Fluid flow control,
Robotic assembly,
Control systems,
Servomechanisms,
Valves,
Fluid flow,
Three-term control,
Mathematical model"
Research on Distributed Architecture Based on SOA,"In essence, Service Oriented Architecture (SOA) is distributed and shall not have any centralized control point. Any application can access an SOA network for interaction as long as it complies with the corresponding standards. A centralized SOA architecture can also be regarded as a part of SOA. This paper gives a general overview of the fundamental principles, features, and core technologies of distributed SOA, analyzes the integration of distributed object technologies and Web technology, and describes the difference between distributed SOA and centralized SOA. With the previous as a basis, this paper discusses the function and implementation of SOA in the distributed architecture, proposes an SOA-based distributed multi-layer architecture and an application integration frame realized via J2EE. This application integration frame can connect any distributed application without restriction and support data and function sharing among applications,thus simplifying the implementation of the functions at various layers and providing better maintainability and scalability.","Semiconductor optical amplifiers,
Service oriented architecture,
Computer architecture,
Application software,
Computer science,
Petroleum,
Centralized control,
Scalability,
Paper technology,
Internet"
Mining temporal medical data using adaptive fuzzy cognitive maps,"In this paper, we present our research on mining temporal medical data. In particular, we investigate the possibility of applying fuzzy cognitive maps (FCMs) to discover temporal dependencies between medical concepts. We consider two types of simple concepts, i.e., medical interventions (such as prescribing drugs) and health effects expressed by changes of patients' conditions. The long-term goal of the research is the application of the discovered FCM to medical decision support and planning the therapy of patients. Finally, we present the results of experiments performed on temporal diabetes data.","Data mining,
Fuzzy cognitive maps,
Medical diagnostic imaging,
Diseases,
Medical treatment,
Ontologies,
Drugs,
Knowledge representation,
Computer science,
Diabetes"
CIM and IEC 61850 integration issues: Application to power systems,"Common Information Model (CIM) is emerging as a standard for information modelling for power control centers. While, IEC 61850 by International Electrotechnical Commission (IEC) is emerging as a standard for achieving interoperability and automation at the substation level. In future, once these two standards are well adopted, the issue of integration of these standards becomes imminent. Some efforts reported towards the integration of these standards have been surveyed. This paper describes a possible approach for the integration of IEC 61850 and CIM standards based on mapping between the representation of elements of these two standards. This enables seamless data transfer from one standard to the other. Mapping between the objects of IEC 61850 and CIM standards both in the static and dynamic models is discussed. A CIM based topology processing application is used to demonstrate the design of the data transfer between the standards. The scope and status of implementation of CIM in the Indian power sector is briefed.","Computer integrated manufacturing,
IEC standards,
Power systems,
Power system modeling,
Substation automation,
Automatic control,
Communication system control,
Power system dynamics,
SCADA systems,
Power control"
Trabecular Bone Analysis in CT and X-Ray Images of the Proximal Femur for the Assessment of Local Bone Quality,"Currently, conventional X-ray and CT images as well as invasive methods performed during the surgical intervention are used to judge the local quality of a fractured proximal femur. However, these approaches are either dependent on the surgeon's experience or cannot assist diagnostic and planning tasks preoperatively. Therefore, in this work a method for the individual analysis of local bone quality in the proximal femur based on model-based analysis of CT- and X-ray images of femur specimen will be proposed. A combined representation of shape and spatial intensity distribution of an object and different statistical approaches for dimensionality reduction are used to create a statistical appearance model in order to assess the local bone quality in CT and X-ray images. The developed algorithms are tested and evaluated on 28 femur specimen. It will be shown that the tools and algorithms presented herein are highly adequate to automatically and objectively predict bone mineral density values as well as a biomechanical parameter of the bone that can be measured intraoperatively.","Cancellous bone,
Image analysis,
Computed tomography,
X-ray imaging,
Surgery,
Surges,
Shape,
Testing,
Minerals,
Density measurement"
Convex optimization for multi-class image labeling with a novel family of total variation based regularizers,"We introduce a linearly weighted variant of the total variation for vector fields in order to formulate regularizers for multi-class labeling problems with non-trivial interclass distances. We characterize the possible distances, show that Euclidean distances can be exactly represented, and review some methods to approximate non-Euclidean distances in order to define novel total variation based regularizers. We show that the convex relaxed problem can be efficiently optimized to a prescribed accuracy with optimality certificates using Nesterov's method, and evaluate and compare our approach on several synthetical and real-world examples.","Labeling,
Image segmentation,
Costs,
Spatial coherence,
TV,
Jacobian matrices,
Pattern analysis,
Human computer interaction,
Mathematics,
Computer science"
Robust on-line model-based object detection from range images,"A mobile robot that accomplishes high level tasks needs to be able to classify the objects in the environment and to determine their location. In this paper, we address the problem of online object detection in 3D laser range data. The object classes are represented by 3D point-clouds that can be obtained from a set of range scans. Our method relies on the extraction of point features from range images that are computed from the point-clouds. Compared to techniques that directly operate on a full 3D representation of the environment, our approach requires less computation time while retaining the robustness of full 3D matching. Experiments demonstrate that the proposed approach is even able to deal with partially occluded scenes and to fulfill the runtime requirements of online applications.","Robustness,
Object detection,
Clouds,
Feature extraction,
Layout,
Mobile robots,
Data mining,
Service robots,
Simultaneous localization and mapping,
Intelligent robots"
Concurrent Event Detection for Asynchronous consistency checking of pervasive context,"Contexts, the pieces of information that capture the characteristics of computing environments, are often inconsistent in the dynamic and uncertain pervasive computing environments. Various schemes have been proposed to check context consistency for pervasive applications. However, existing schemes implicitly assume that the contexts being checked belong to the same snapshot of time. This limitation makes existing schemes do not work in pervasive computing environments, which are characterized by the asynchronous coordination among computing devices. The main challenge imposed on context consistency checking by asynchronous environments is how to interpret and detect concurrent events. To this end, we propose in this paper the Concurrent Events Detection for Asynchronous consistency checking (CEDA) algorithm. An analytical model, together with corresponding numerical results, is derived to study the performance of CEDA. We also conduct extensive experimental evaluation to investigate whether CEDA is desirable for context-aware applications. Both theoretical analysis and experimental evaluation show that CEDA accurately detects concurrent events in time in asynchronous pervasive computing environments, even with dynamic changes in message delay, duration of events and error rate of context collection.","Event detection,
Pervasive computing,
Context,
Concurrent computing,
Delay,
Embedded computing,
Computer science,
Internet,
Mobile computing,
Analytical models"
Two-stage Approach for Word-wise Script Identification,"A two-stage approach for word-wise identification of English (Roman), Devnagari and Bengali (Bangla) scripts is proposed. This approach balances the tradeoff between recognition accuracy and processing speed. The 1st stage allows identifying scripts with high speed, yet less accuracy when dealing with noisy data. The advanced 2nd stage processes only those samples that yield low recognition confidence in the first stage. For both stages a rough character segmentation is performed and features are computed on segmented character components. Features used in the 1st stage are a 64-dimensional chain-code-histogram feature, while 400-dimensional gradient features are used in the 2nd stage. Final classification of a word to a particular script is done via majority voting of each recognized character component of the word. Extensive experiments with various confidence scores were conducted and reported here. The overall recognition accuracy and speed is remarkable. Correct classification of 98.51% on 11,123 test words is achieved, even when the recognition-confidence is as high as 95% at both stages.","Optical character recognition software,
Character recognition,
Natural languages,
Voting,
Text analysis,
Information analysis,
Information security,
Computer science,
Educational institutions,
Automatic testing"
A probabilistic call admission control algorithm for WLAN in heterogeneous wireless environment,"In an integrated WLAN and cellular network, if all mobile users whose connections originate in the cellular network migrate to the WLAN whenever they enter the double coverage area, the WLAN will be severely congested and its users will suffer from performance degradation. Therefore, we propose a Call Admission Control (CAC) algorithm that allows the WLAN to limit downward Vertical Handovers (VHOs) from the cellular network to reduce unnecessary VHO processing. Numerical and simulation results demonstrate that our CAC scheme reduces the unnecessary VHO processing while keeping the DVHO blocking rate within acceptable limits and maintaining reasonable throughput in the WLAN.","Call admission control,
Wireless LAN,
Land mobile radio cellular systems,
NIST,
Throughput,
Wireless networks,
Costs,
Degradation,
Numerical simulation,
Cellular networks"
FPGA vs. GPU for sparse matrix vector multiply,"Sparse matrix-vector multiplication (SpMV) is a common operation in numerical linear algebra and is the computational kernel of many scientific applications. It is one of the original and perhaps most studied targets for FPGA acceleration. Despite this, GPUs, which have only recently gained both general-purpose programmability and native support for double precision floating-point arithmetic, are viewed by some as a more effective platform for SpMV and similar linear algebra computations. In this paper, we present an analysis comparing an existing GPU SpMV implementation to our own, novel FPGA implementation. In this analysis, we describe the challenges faced by any SpMV implementation, the unique approaches to these challenges taken by both FPGA and GPU implementations, and their relative performance for SpMV.","Field programmable gate arrays,
Sparse matrices,
Computer architecture,
Kernel,
Acceleration,
Biology computing,
Vectors,
Linear algebra,
Scientific computing,
Biological system modeling"
Compressed sensing based UWB receiver: Hardware compressing and FPGA reconstruction,"A low sampling rate approach for recovering ultra wide band (UWB) signals is proposed, using Distributed Amplifiers (DAs) and low speed Analog-to-Digital Converters (ADCs) and based on the theory of compressed sensing. A microwave circuit consisting of a bank of DAs, followed by a bank of ADCs, is designed to implement analog compressing, where the elements of measurement matrix are realized by picosecond delay tap and flexible gain coefficients in DAs. Numerical simulation shows that a bank of eight DAs and ADCs with 500MHz sampling rate can almost perfectly recover a 100ps-resolution UWB echo signal in the noiseless case. For recovering the UWB signals in a real-time way, issues in field programmable gate array (FPGA) implementation are discussed.","Compressed sensing,
Hardware,
Field programmable gate arrays,
Sampling methods,
Ultra wideband technology,
Distributed amplifiers,
Analog-digital conversion,
Microwave circuits,
Flexible printed circuits,
Microwave measurements"
Local interference pricing for distributed beamforming in MIMO networks,"We study a distributed algorithm for adjusting beamforming vectors in a peer-to-peer wireless network with multiple-input multiple-output (MIMO) channels. Each transmitter precoding matrix has rank one, and a linear minimum mean squared error (MMSE) filter is applied at each receiver. Our objective is to maximize the total utility summed over all users, where each user's utility is a function of the received signal-to-interference-plus-noise ratio (SINR). Given all users' beamforming vectors and receive filters, each receiver announces an interference price, representing the marginal cost of interference from other users. A particular transmitter updates its beamforming vector to maximize its utility minus the interference cost to other users. We show that if the utility functions satisfy certain concavity conditions, then the total utility is non-decreasing with each update. We also present numerical results that illustrate the effect of ignoring interference prices from all but the closest users, and relaxing requirements on the frequency of beam and price updates.",
Piecewise-consistent color mappings of images acquired under various conditions,"Many applications in computer vision require comparisons between two images of the same scene. Comparison applications usually assume that corresponding regions in the two images have similar colors. However, this assumption is not always true. One way to deal with this problem is to apply a color mapping to one of the images. In this paper we address the challenge of computing color mappings between pairs of images acquired under different acquisition conditions, and possibly by different cameras. For images taken from different viewpoints, our proposed method overcomes the lack of pixel correspondence. For images taken under different illumination, we show that no single color mapping exists, and we address and solve a new problem of computing a minimal set of piecewise color mappings. When both viewpoint and illumination vary, our method can only handle planar regions of the scene. In this case, the scene planar regions are simultaneously co-segmented in the two images, and piecewise color mappings for these regions are calculated. We demonstrate applications of the proposed method for each of these cases.","Statistics,
Statistical distributions,
Pixel,
Image denoising,
Markov random fields,
Application software,
Computer science,
Educational institutions,
Probability distribution,
Random number generation"
Real-time vision-based multiple vehicle detection and tracking for nighttime traffic surveillance,"This study presents an effective system for detecting and tracking moving vehicles in nighttime traffic scene for traffic surveillance. The proposed method identifies vehicles based on detecting and locating vehicle headlights and taillights by using the techniques of image segmentation and pattern analysis. First, to effectively extract bright objects of interest, a fast bright-object segmentation process based on automatic multilevel histogram thresholding is applied on the nighttime road-scene images. This automatic multilevel thresholding approach can provide robustness and adaptability for the detection system to be operated well under various illumination conditions at night. The extracted bright objects are processed by a spatial clustering and tracking procedure by locating and analyzing the spatial and temporal features of vehicle light patterns, and then identifying and classifying the moving cars and motorbikes in the traffic scenes. Experimental results demonstrate that the proposed approach is feasible and effective for vehicle detection and identification in various nighttime environments for traffic surveillance.","Vehicle detection,
Surveillance,
Vehicles,
Layout,
Image segmentation,
Pattern analysis,
Histograms,
Robustness,
Lighting,
Motorcycles"
A preclinical PET/MR insert for a human 3T MR scanner,"We describe the design of a prototype preclinical PET/MR system capable of scanning small animals up to the size of a rabbit. The scanner uses highly sensitive and fast LSO/silicon-photomultiplier arrays as detector elements, direct digitization with a dedicated ASIC and integrated digital singles processing. Special measures are taken to improve the MR-compatibility of this design. Measurements of a prototype detector were performed inside 1.5T and 3T MRI-systems. Even under high gradient-slew-rates and RF-pulses disturbance energy- and coincidence timing-resolution of ~18% and 530ps (FWHM) respectively have been measured. The presented work is part of the EU FP7 project HYPERImage, Grant Agreement N° 201651, [1].","Positron emission tomography,
Humans,
Prototypes,
Detectors,
Animals,
Rabbits,
Sensor arrays,
Application specific integrated circuits,
Performance evaluation,
Energy measurement"
JigDFS: A secure distributed file system,"Ubiquitous connectivity and availability of P2P resources creates opportunities for building new services. This paper describes Jigsaw Distributed File System (JigDFS) which can be used to securely store and retrieve files on a P2P network anonymously. JigDFS is designed to provide strong encryption and a certain level of plausible deniability. Files in JigDFS are sliced into small segments using an Information Dispersal Algorithm (IDA) and distributed onto different nodes recursively to increase fault tolerance against node failures. Moreover, layered encryption is applied to each file with keys produced by a hashed-key chain algorithm, so that data (file segments) and keys reside on different hosts. In such a scheme, if an attacker compromises a host and retrieves the data, the attacker will still need the correct key to decipher the data. Furthermore, recursive IDA and layered encryption ensure users' anonymity. It is difficult for an adversary to identify who owns a file, even who has retrieved a file in JigDFS. Often, a strong adversary may have the power to monitor the network or even force a user to give up the password. Design of JigDFS provides users with plausible deniability which enhances privacy. When being questioned, a JigDFS user can simply argue that he/she is merely a relaying node, rather than the file owner. Moreover, a user, when forced, can give up a valid, however, incorrect encryption key. There is no way for an adversary to verify either correctness of a key or the identity of file owner. JigDFS is developed using platform independent Java technologies and is envisioned to utilize mobile computing elements such as PDAs and smart phones.","File systems,
Cryptography,
Personal digital assistants,
Availability,
Buildings,
Fault tolerance,
Information retrieval,
Monitoring,
Privacy,
Relays"
Online Stochastic Matching: Beating 1-1/e,"We study the online stochastic bipartite matching problem, in a form motivated by display ad allocation on the Internet. In the online, but adversarial case, the celebrated result of Karp, Vazirani and Vazirani gives an approximation ratio of
1−
1
e
≃0.632
, a very familiar bound that holds for many online problems; further, the bound is tight in this case. In the online, stochastic case when nodes are drawn repeatedly from a known distribution, the greedy algorithm matches this approximation ratio, but still, no algorithm is known that beats the
1−
1
e
bound.Our main result is a
0.67
-approximation online algorithm for stochastic bipartite matching, breaking this
1−
1
e
barrier. Furthermore, we show that no online algorithm can produce a
1−ϵ
approximation for an arbitrarily small
ϵ
for this problem. Our algorithms are based on computing an optimal offline solution to the expected instance, and using this solution as a guideline in the process of online allocation. We employ a novel application of the idea of the power of two choices from load balancing: we compute two disjoint solutions to the expected instance, and use both of them in the online algorithm in a prescribed preference order. To identify these two disjoint solutions, we solve a max flow problem in a boosted flow graph, and then carefully decompose this maximum flow to two edge-disjoint (near-) matchings. In addition to guiding the online decision making, these two offline solutions are used to characterize an upper bound for the optimum in any scenario. This is done by identifying a cut whose value we can bound under the arrival distribution. At the end, we discuss extensions of our results to more general bipartite allocations that are important in a display ad application.","Stochastic processes,
Displays,
Approximation algorithms,
Internet,
Greedy algorithms,
Guidelines,
Load management,
Flow graphs,
Decision making,
Upper bound"
A Knowledge-Based Approach to Soft Tissue Reconstruction of the Cervical Spine,"For surgical planning in spine surgery, the segmentation of anatomical structures is a prerequisite. Past efforts focussed on the segmentation of vertebrae from tomographic data, but soft tissue structures have, for the most part, been neglected. Only sparse research work has been done for the spinal cord and the trachea. However, as far as the author is aware, there is no work on segmenting intervertebral discs. Therefore, a totally automatic reconstruction algorithm for the most relevant cervical structures is presented. It is implemented as a straightforward process, using anatomical knowledge which is, in concept, transferrable to other tissues of the human body. No seed points are required since the discs, as initial landmarks, are located via an object recognition approach. The spinal musculature is reconstructed by surface analysis on already segmented vertebrae, thus it can be taken into account in a biomechanical simulation. The segmentation results of our approach showed 91% accordance with expert segmentations and the computation time is less than 1 min on a standard PC. Since the presented system follows some general concepts this approach may also be considered as a step towards full body segmentation of the human.","Biological tissues,
Surgery,
Spine,
Humans,
Anatomical structure,
Tomography,
Spinal cord,
Reconstruction algorithms,
Object recognition,
Surface reconstruction"
A cascade of artificial neural networks to predict transformers oil parameters,"In this paper artificial neural networks have been constructed to predict different transformers oil parameters. The prediction is performed through modeling the relationship between the insulation resistance measured between distribution transformers high voltage winding, low voltage winding and the ground and the breakdown strength, interfacial tension acidity and the water content of the transformers oil. The process of predicting these oil parameters statuses is carried out using various configurations of neural networks. First, a multilayer feed forward neural network with a back-propagation learning algorithm was implemented. Subsequently, a cascade of these neural networks was deemed to be more promising, and four variations of a three stage cascade were tested. The first configuration takes four inputs and outputs four parameter values, while the other configurations have four neural networks, each with two or three inputs and a single output; the output from some networks are pipelined to some others to produce the final values. Both configurations are evaluated using real-world training and testing data and the accuracy is calculated across a variety of hidden layer and hidden neuron combinations. The results indicate that even with a lack of sufficient data to train the network, accuracy levels of 84% for breakdown voltage, 95% for interfacial tension, 56% for water content, and 75% for oil acidity predictions were obtained by the cascade of neural networks.","Artificial neural networks,
Oil insulation,
Neural networks,
Breakdown voltage,
Low voltage,
Petroleum,
Multi-layer neural network,
Testing,
Performance evaluation,
Predictive models"
A low-radix and low-diameter 3D interconnection network design,"Interconnection plays an important role in performance and power of CMP designs using deep sub-micron technology. The network-on-chip (NoCs) has been proposed as a scalable and high-bandwidth fabric for interconnect design. The advent of the 3D technology has provided further opportunity to reduce on-chip communication delay. However, the design of the 3D NoC topologies has important distinctions from 2D NoCs or off-chip interconnection networks. First, current 3D stacking technology allows only vertical inter-layer links. Hence, there cannot be direct connections between arbitrary nodes in different layers — the vertical connection topology are essentially fixed. Second, the 3D NoC is highly constrained by the complexity and power of routers and links. Hence, low-radix routers are preferred over high-radix routers for lower power and better heat dissipation. This implies long network latency due to high hop counts in network paths. In this paper, we design a low-diameter 3D network using low-radix routers. Our topology leverages long wires to connect remote intra-layer nodes. We take advantage of the start-of-the-art one-hop vertical communication design and utilize lateral long wires to shorten network paths. Effectively, we implement a small-to-medium sized clique network in different layers of a 3D chip. The resulting topology generates a diameter of 3-hop only network, using routers of the same radix as 3D mesh routers. The proposed network shows up to 29% of network latency reduction, up to 10% throughput improvement, and up to 24% energy reduction, when compared to a 3D mesh network.","Multiprocessor interconnection networks,
Network-on-a-chip,
Network topology,
Delay,
Wires,
Fabrics,
Stacking,
Mesh generation,
Throughput,
Mesh networks"
Capturing order in social interactions [Social Sciences],"As humans appear to be literally wired for social interaction, it is not surprising to observe that social aspects of human behavior and psychology attract interest in the computing community as well. The gap between social animal and unsocial machine was tolerable when computers were nothing else than improved versions of old tools (e.g., word processors replacing typewriters), but today computers go far beyond that simple role. Now computers are the natural means for a wide spectrum of new, inherently social activities like remote communication, distance learning, online gaming, social networking, information seeking and sharing, and training in virtual worlds. In this new context, computers must integrate human-human interaction as seamlessly as possible and deal effectively with spontaneous social behaviors of their users. In concise terms, computers need to become socially intelligent.","Humans,
Artificial intelligence,
Psychology,
Animals,
Ear,
Mirrors,
Microphones,
Cameras,
Intelligent sensors,
Face detection"
Imitation learning with generalized task descriptions,"In this paper, we present an approach that allows a robot to observe, generalize, and reproduce tasks observed from multiple demonstrations. Motion capture data is recorded in which a human instructor manipulates a set of objects. In our approach, we learn relations between body parts of the demonstrator and objects in the scene. These relations result in a generalized task description. The problem of learning and reproducing human actions is formulated using a dynamic Bayesian network (DBN). The posteriors corresponding to the nodes of the DBN are estimated by observing objects in the scene and body parts of the demonstrator. To reproduce a task, we seek for the maximum-likelihood action sequence according to the DBN. We additionally show how further constraints can be incorporated online, for example, to robustly deal with unforeseen obstacles. Experiments carried out with a real 6-DoF robotic manipulator as well as in simulation show that our approach enables a robot to reproduce a task carried out by a human demonstrator. Our approach yields a high degree of generalization illustrated by performing a pick-and-place and a whiteboard cleaning task.","Robots,
Humans,
Layout,
Manipulators,
Cleaning,
Robotics and automation,
Bayesian methods,
Contracts,
Maximum likelihood estimation,
Robustness"
A context aware wireless body area network (BAN),"In monitoring a patient's real-time vital signs through Body Area Networks (BAN), rich data sources are communicated to medical practitioners. The benefit of BANs may be negated if medical practitioners are overloaded with streams of BAN data. It is essential that data is delivered in a timely context aware manner. In this paper a BAN designed for falls assessment among elder patients (65+ years) is presented, with an emphasis on the communication scheme chosen. The FrameComm MAC protocol described in this paper employs three data management techniques, 1) message priority, 2) opportunistic aggregation and 3) an adaptive duty cycle, all of which are designed to ensure that patient vital signs (i.e. data packets) are delivered under a variety of network loads. The protocol is evaluated using a small laboratory network, initially configured to communicate Beat-to-Beat (continuous blood pressure) readings when a patient goes from a sitting to a standing position and then with added ECG (ElectroCardioGram) readings.","Body sensor networks,
Context awareness,
Biomedical monitoring,
Patient monitoring,
Blood pressure,
Laboratories,
Educational institutions,
Protocols,
Electrocardiography,
Wireless sensor networks"
A Novel Approach to Reduce Line Harmonic Current for a Three-phase Diode Rectifier-fed Electrolytic Capacitor-less Inverter,This paper describes a three-phase high power factor rectifier for an electrolytic capacitor-less inverter based on the current injection scheme. The rectifier injects the machine side zero sequence current into the grid side through three bi-directional switches. This paper also introduces the modified circuit topology which is suitable for an electrolytic capacitor-less inverter. A bulky dc or ac reactor can be removed due to small capacitance of the proposed circuit topology. The principle of current injection and a proper current control method based on a feed-forward control concept have been described in this paper. The validity of the proposed circuit topology has been confirmed by simulation and experimental results.,"Diodes,
Inverters,
Circuit topology,
Rectifiers,
Reactive power,
Bidirectional control,
Switches,
Inductors,
Capacitance,
Current control"
Fault isolation for device drivers,"This work explores the principles and practice of isolating low-level device drivers in order to improve OS dependability. In particular, we explore the operations drivers can perform and how fault propagation in the event a bug is triggered can be prevented. We have prototyped our ideas in an open-source multiserver OS (MINIX 3) that isolates drivers by strictly enforcing least authority and iteratively refined our isolation techniques using a pragmatic approach based on extensive software-implemented fault-injection (SWIFI) testing. In the end, out of 3,400,000 common faults injected randomly into 4 different Ethernet drivers using both programmed I/O and DMA, no fault was able to break our protection mechanisms and crash the OS. In total, we experienced only one hang, but this appears to be caused by buggy hardware.","Kernel,
Computer crashes,
Operating systems,
Vehicle crash testing,
Linux,
Lab-on-a-chip,
Computer science,
Software prototyping,
Open source software,
Software testing"
A runtime relocation based workflow for self dynamic reconfigurable systems design,"A self, partial and dynamic approach to reconfiguration makes it possible to obtain higher flexibility and better performance with respect to simpler approaches; however, the price for this improvement lies in the increased difficulties in the reconfigurable system creation and management, which become significantly more complex. An automated or semiautomated way to support this kind of systems would simplify the problem by raising the level of abstraction at which the designer has to operate. The aim of this work is the creation of a complete workflow to help the designer in the creation and management of self partially and dynamically reconfigurable systems: the designer should only specify the application, the reconfigurable device, the reconfiguration model (1D vs 2D) and the type of communication infrastructure, and the automated flow will deal with the subsequent steps down to the final architecture implementation. Among other aspects, the provided support includes the definition of area constraint for cores, the creation of an efficient runtime solution for core allocation management and the generation of a solution to obtain internal and fast relocation of cores.","Runtime,
Environmental management,
Innovation management,
Computer science,
Artificial intelligence,
Laboratories,
Technology management,
Field programmable gate arrays,
Energy management,
Fabrics"
Polymer microring resonators for high-sensitivity and wideband photoacoustic imaging,"Polymer microring resonators have been exploited for high-sensitivity and wideband photoacoustic imaging. To demonstrate high-sensitivity ultrasound detection, highfrequency photoacoustic imaging of a 49-μm-diameter black bead at an imaging depth of 5 mm was imaged photoacoustically using a synthetic 2-D array with 249 elements and a low laser fluence of 0.35 mJ/cm2. A bandpass filter with a center frequency of 28 MHz and a bandwidth of 16 MHz was applied to all element data but without signal averaging, and a signalto- noise ratio of 16.4 dB was obtained. A wideband detector response is essential for imaging reconstruction of multiscale objects, e.g., various sizes of tissues, by using a range of characteristic acoustic wavelengths. A simulation of photoacoustic tomography of beads shows that objects with their boundaries characteristic of high spatial frequencies and the inner structure primarily of low spatial frequency components can be faithfully reconstructed using such a detector. Photoacoustic tomography experiments of 49- and 301-μm-diameter beads were presented. A high resolution of 12.5 μm was obtained. The boundary of a 301-μm bead was imaged clearly. The results demonstrated that the high sensitivity and broadband response of polymer microring resonators have potential for high resolution and high-fidelity photoacoustic imaging.",
Efficient Bit-Parallel GF(2^m) Multiplier for a Large Class of Irreducible Pentanomials,"This work studies efficient bit-parallel multiplication in GF(2m) for irreducible pentanomials, based on the so-called shifted polynomial bases (SPBs). We derive a closed expression of the reduced SPB product for a class of polynomials xm + xk s + xk s-1+ hellip + xk-1 + 1, with ks - k1 les m+1/ 2. Then, we apply the above formulation to the case of pentanomials. The resulting multiplier outperforms, or is as efficient as the best proposals in the technical literature, but it is suitable for a much larger class of pentanomials than those studied so far. Unlike previous works, this property enables the choice of pentanomials optimizing different field operations (for example, inversion), yet preserving an optimal implementation of field multiplication, as discussed and quantitatively proved in the last part of the paper.","Polynomials,
Equations,
Hardware,
Galois fields,
Complexity theory,
Elliptic curve cryptography,
Optimization"
Enhancement of the Krylov Subspace Regularization for Microwave Biomedical Imaging,"Although Krylov subspace methods provide fast regularization techniques for the microwave imaging problem, they cannot preserve the edges of the object being imaged and may result in an oscillatory reconstruction. To suppress these spurious oscillations and to provide an edge-preserving regularization, we use a multiplicative regularizer which improves the reconstruction results significantly while adding little computational complexity to the inversion algorithm. We show the inversion results for a real human forearm assuming the 2-D transverse magnetic illumination and a cylindrical object assuming the 2-D transverse electric illumination.","Biomedical imaging,
Microwave imaging,
Image reconstruction,
Microwave theory and techniques,
Permittivity measurement,
Computational complexity,
Lighting,
Gaussian processes,
Magnetic field measurement,
Electric variables measurement"
Model-Driven approach to Software Architecture design,"Software Architecture (SA) allows for early assessment of and design for quality attributes of a software system, and it plays a critical role in current software development. However, there is no consensus on fundamental issues such as design methods and representation organization and languages, and current proposals lack specificity and preciseness. Thus, it is extremely difficult to build a complete and appropriate software architecture, even though it is recognized as a fundamental artifact. In this paper we define an architecture design method that enables the systematic and assisted construction of the SA of Enterprise Applications, taking into account major quality attributes involved in this family of systems. We apply Model-Driven Engineering techniques to achieve this goal. The architecture is treated as a mega-model (a model composed of related models) and the application of design decisions is encoded in terms of model transformations. The architectural rationale is explicitly registered as the set of transformations that yields the complete SA from scratch. We illustrate the application of the approach by designing the SA of a case study from the literature.","Software architecture,
Software design,
Design methodology,
Design for quality,
Software systems,
Programming,
Proposals,
Computer architecture,
Application software,
Model driven engineering"
Constrained Registration of the Wrist Joint,"Comparing wrist shapes of different individuals requires alignment of these wrists into the same pose. Unconstrained registration of the carpal bones results in anatomically nonfeasible wrists. In this paper, we propose to constrain the registration using the shapes of adjacent bones, by keeping the width of the gap between adjacent bones constant. The registration is formulated as an optimization involving two terms. One term aligns the wrist bones by minimizing the distances between corresponding bone surfaces. The second term constrains the registration by minimizing the distances between adjacent sliding surfaces. The registration is based on the Iterative Closest Point algorithm. All bones are registered concurrently so that no bias is introduced towards any of the bones. The proposed registration method delivers anatomically correct configurations of the bones. The registration errors are in the order of the voxel size of the acquired CT data (0.3 times 0.3 times 0.3 mm3). The standard deviation in the widths of gaps between adjacent bones is in the order of 10% with an insignificant bias. This is a large improvement over the standard deviations of 30%-80% encountered in unconstrained registration. The value of this method is its capability of accurately registering joints in varying poses resulting in physiological joint configurations.","Wrist,
Bones,
Shape,
Biomedical imaging,
Medical diagnostic imaging,
Joints,
Pathology,
Surgery,
Iterative closest point algorithm,
Computed tomography"
Domain adaptive semantic diffusion for large scale context-based video annotation,"Learning to cope with domain change has been known as a challenging problem in many real-world applications. This paper proposes a novel and efficient approach, named domain adaptive semantic diffusion (DASD), to exploit semantic context while considering the domain-shift-of-context for large scale video concept annotation. Starting with a large set of concept detectors, the proposed DASD refines the initial annotation results using graph diffusion technique, which preserves the consistency and smoothness of the annotation over a semantic graph. Different from the existing graph learning methods which capture relations among data samples, the semantic graph treats concepts as nodes and the concept affinities as the weights of edges. Particularly, the DASD approach is capable of simultaneously improving the annotation results and adapting the concept affinities to new test data. The adaptation provides a means to handle domain change between training and test data, which occurs very often in video annotation task. We conduct extensive experiments to improve annotation results of 374 concepts over 340 hours of videos from TRECVID 2005-2007 data sets. Results show consistent and significant performance gain over various baselines. In addition, the proposed approach is very efficient, completing DASD over 374 concepts within just 2 milliseconds for each video shot on a regular PC.","Large-scale systems,
Testing,
Detectors,
Airplanes,
Gunshot detection systems,
Computer vision,
Videoconference,
Training data,
Computer science,
Application software"
Binary causal-adversary channels,"In this work we consider the communication of information in the presence of a causal adversarial jammer. In the setting under study, a sender wishes to communicate a message to a receiver by transmitting a codeword x = (x1, …, xn) bit-by-bit over a communication channel. The adversarial jammer can view the transmitted bits xi one at a time, and can change up to a p-fraction of them. However, the decisions of the jammer must be made in an online or causal manner. Namely, for each bit xi the jammer's decision on whether to corrupt it or not (and on how to change it) must depend only on xj for j ≤ i. This is in contrast to the “classical” adversarial jammer which may base its decisions on its complete knowledge of x. We present a non-trivial upper bound on the amount of information that can be communicated. We show that the achievable rate can be asymptotically no greater than min{1 - H(p), (1 - 4p)+}. Here H(.) is the binary entropy function, and (1 - 4p)+ equals 1 - 4p for p ≤ 0.25, and 0 otherwise.","Jamming,
Upper bound,
Communication channels,
Codes,
Computer science,
Entropy,
Laboratories,
Computer interfaces,
Decoding"
Survey and evaluation of real-time fall detection approaches,"As we grow old, our desire for independence does not diminish; yet our health increasingly needs to be monitored. Injuries such as falling can be a serious problem for the elderly. If a person falls and is not able to get assistance within an hour, casualties arising from that fall can result in fatalities as early as 6 months later [1]. It would seem then that a choice between safety and independence must be made. Fortunately, as health care technology advances, simple devices can be made to detect or even predict falls in the elderly, which could easily save lives without too much intrusion on their independence. Much research has been done on the topic of fall detection and fall prediction. Some have attempted to detect falls using a variety of sensors such as: cameras, accelerometers, gyroscopes, microphones, or a combination of the like. This paper is aimed at reporting which existing methods have been found effective by others, as well as documenting the findings of our own experiments. The combination of which will assist in the progression towards a safe, unobtrusive monitoring system for independent seniors.","Senior citizens,
Acceleration,
Accelerometers,
Medical diagnostic imaging,
Biomedical monitoring,
Computer science,
Computerized monitoring,
Injuries,
Safety,
Medical services"
Process-Variation-Aware Adaptive Cache Architecture and Management,"Fabricating circuits that employ ever-smaller transistors leads to dramatic variations in critical process parameters. This in turn results in large variations in execution/access latencies of different hardware components. This situation is even more severe for memory components due to minimum-sized transistors used in their design. Current design methodologies that are tuned for the worst case scenarios are becoming increasingly pessimistic from the performance angle, and thus, may not be a viable option at all for future designs. This paper makes two contributions targeting on-chip data caches. First, it presents an adaptive cache management policy based on nonuniform cache access. Second, it proposes a latency compensation approach that employs several circuit-level techniques to change the access latency of select cache lines based on the criticalities of the load instructions that access them. Our experiments reveal that both these techniques can recover significant amount of the lost performance due to worst case designs.","Delay,
Pipelines,
Timing,
Benchmark testing,
Threshold voltage,
Clocks,
Transistors"
Kd-Jump: a Path-Preserving Stackless Traversal for Faster Isosurface Raytracing on GPUs,"Stackless traversal techniques are often used to circumvent memory bottlenecks by avoiding a stack and replacing return traversal with extra computation. This paper addresses whether the stackless traversal approaches are useful on newer hardware and technology (such as CUDA). To this end, we present a novel stackless approach for implicit kd-trees, which exploits the benefits of index-based node traversal, without incurring extra node visitation. This approach, which we term Kd-Jump, enables the traversal to immediately return to the next valid node, like a stack, without incurring extra node visitation (kd-restart). Also, Kd-Jump does not require global memory (stack) at all and only requires a small matrix in fast constant-memory. We report that Kd-Jump outperforms a stack by 10 to 20% and kd-restar t by 100%. We also present a Hybrid Kd-Jump, which utilizes a volume stepper for leaf testing and a run-time depth threshold to define where kd-tree traversal stops and volume-stepping occurs. By using both methods, we gain the benefits of empty space removal, fast texture-caching and realtime ability to determine the best threshold for current isosurface and view direction.","Isosurfaces,
Data visualization,
Hardware,
Acceleration,
Layout,
Computer science,
Foot,
Aneurysm,
Skull,
Testing"
Coexistence with malicious nodes: A game theoretic approach,"In this paper, we use game theory to study the interactions between a malicious node and a regular node in wireless networks with unreliable channels. Since the malicious nodes do not reveal their identities to others, it is crucial for the regular nodes to detect them through monitoring and observation. We model the malicious node detection process as a Bayesian game with imperfect information and show that a mixed strategy perfect Bayesian Nash Equilibrium (also a sequential equilibrium) is attainable. While the equilibrium in the detection game ensures the identification of the malicious nodes, we argue that it might not be profitable to isolate the malicious nodes upon detection. As a matter of fact, malicious nodes and regular nodes can co-exist as long as the destruction they bring is less than the contribution they make. To show how we can utilize the malicious nodes, a post-detection game between the malicious and regular nodes is formalized. Solution to this game shows the existence of a subgame perfect Nash Equilibrium and the conditions that achieve the equilibrium. Simulation results and their discussions are also provided to illustrate the properties of the derived equilibria.","Game theory,
Monitoring,
Bayesian methods,
Intrusion detection,
Wireless networks,
Nash equilibrium,
Computer science,
Laboratories,
Routing protocols,
Uncertainty"
Exploiting fast carry-chains of FPGAs for designing compressor trees,"Fast carry chains featuring dedicated adder circuitry is a distinctive feature of modern FPGAs. The carry chains bypass the general routing network and are embedded in the logic blocks of FPGAs for fast addition. Conventional intuition is that such carry chains can be used only for implementing carry-propagate addition; state-of-the-art FPGA synthesizers can only exploit the carry chains for these specific circuits. This paper demonstrates that the carry chains can be used to build compressor trees, i.e., multi-input addition circuits used for parallel accumulation and partial product reduction for parallel multipliers implemented in FPGA logic. The key to our technique is to program the lookup tables (LUTs) in the logic blocks to stop the propagation of carry bits along the carry chain at appropriate points. This approach improves the area of compressor trees significantly compared to previous methods that synthesized compressor trees solely on LUTs, without compromising the performance gain over trees built from ternary carry-propagate adders.","Field programmable gate arrays,
Adders,
Table lookup,
Arithmetic,
Network synthesis,
Delay,
Routing,
Logic circuits,
Digital signal processing,
Application specific integrated circuits"
Fingerprinting Blank Paper Using Commodity Scanners,"We develop a novel technique for authenticating physical documents by using random, naturally occurring imperfections in paper texture. To this end, we devised a new method for measuring the three-dimensional surface of a paper without modifying the document in any way, using only a commodity scanner. From this physical feature, we generate a concise fingerprint that uniquely identifies the document. Our method is secure against counterfeiting, robust to harsh handling, and applicable even before any content is printed on a page. It has a wide range of applications, including detecting forged currency and tickets, authenticating passports, and halting counterfeit goods. On a more sinister note, document identification could be used to de-anonymize printed surveys and to compromise the secrecy of paper ballots.","Fingerprint recognition,
Robustness,
Surface texture,
Counterfeiting,
Authentication,
Computer science,
Printing,
Forgery,
Shape measurement,
Computer security"
Real-time enabled IEEE 802.15.4 sensor networks in industrial automation,"Sensor networks have been investigated in many scenarios and a good number of protocols have been developed. With the standardization of the IEEE 802.15.4 protocol, sensor networks became also an interesting topic in industrial automation. Here, the main focus is on real-time capabilities and reliability. We analyzed the IEEE 802.15.4 standard both in a simulation environment and analytically to figure out to which degree the standard fulfills these specific requirements. Our results can be used for planning and deploying IEEE 802.15.4 based sensor networks with specific performance demands. Furthermore, we clearly identified specific protocol limitations that prevent its applicability for delay bounded real-time applications. We therefore propose some protocol modifications that enable real-time operation based on standard IEEE 802.15.4 compliant sensor hardware.","Delay,
Wireless sensor networks,
Computer industry,
Manufacturing automation,
Media Access Protocol,
Energy efficiency,
Wireless application protocol,
Analytical models,
Standardization,
Aerospace industry"
Annotation and Image Markup: Accessing and Interoperating with the Semantic Content in Medical Imaging,"The annotation and image markup project makes large distributed collections of medical images in cyberspace and hospital information systems accessible using an information model of image content and ontologies. Interest in applying semantic Web technologies to the life sciences continues to accelerate. Biomedical research is increasingly an online activity as scientists combine and explore different types of data in cyberspace, putting together complementary views on problems that lead to new insights and discoveries. An e-Science paradigm is thus emerging; the biomedical community is looking for tools to help access, query, and analyze a myriad of data in cyberspace. Specifically, the biomedical community is beginning to embrace technologies such as ontologies to integrate scientific knowledge, standard syntaxes, and semantics to make biomedical knowledge explicit, and the semantic Web to establish virtual collaborations.","Biomedical imaging,
Cancer,
Anatomy,
Abdomen,
Ontologies,
Semantic Web,
Bioinformatics,
Intelligent systems,
Intelligent structures,
Terminology"
Metric Functional Dependencies,"When merging data from various sources, it is often the case that small variations in data format and interpretation cause traditional functional dependencies (FDs) to be violated, without there being an intrinsic violation of semantics. Examples include differing address formats, or different reported latitude/longitudes for a given address. In this paper, we define metric functional dependencies, which strictly generalize traditional FDs by allowing small differences (controlled by a metric) in values of the consequent attribute of an FD. We present efficient algorithms for the verification problem: determining whether a given metric FD holds for a given relation. We experimentally demonstrate the validity and efficiency of our approach on various data sets that lie in multidimensional spaces.","USA Councils,
Merging,
Databases,
Clocks,
Data engineering,
Computer science,
Cities and towns,
Multidimensional systems,
Process design,
Robustness"
Formal Modeling and Verification of Safety-Critical Software,"Rigorous quality demonstration is important when developing safety-critical software such as a reactor protection system (RPS) for a nuclear power plant. Although using formal methods such as formal modeling and verification is strongly recommended, domain experts often reject formal methods for four reasons: there are too many candidate techniques, the notations appear complex, the tools often work only in isolation, and output is often too difficult for domain experts to understand. A formal-methods-based process that supports development, verification and validation, and safety analysis can help domain experts overcome these obstacles. Nuclear engineers can also use CASE tools to apply formal methods without having to know details of the underlying formalism. The authors spent more than seven years working with nuclear engineers in developing RPS software and applying formal methods. The engineers and regulatory personnel found the process effective and easy to apply with the integrated tool support.","Software safety,
Programmable control,
Failure analysis,
US Department of Transportation,
Computer aided software engineering,
Logic testing,
Embedded software,
Software tools,
Software testing,
Control systems"
Understanding Social Robots,"Research on social robots is mainly comprised of research into algorithmic problems in order to expand a robot´s capabilities to improve communication with human beings. Also, a large body of research concentrates on the appearance, i.e. aesthetic form of social robots. However, only little reference to their definition is made. In this paper we argue that form, function, and context have to be taken systematically into account in order to develop a model to help us understand social robots. Therefore, we address the questions: What is a social robot, what are the interdisciplinary research aspects of social robotics, and how are these different aspects interlinked? In order to present a comprehensive and concise overview of the various aspects we present a framework for a definition towards social robots.","Robotics and automation,
Robotic assembly,
Human robot interaction,
Biological system modeling,
Context modeling,
Automata,
Manipulators,
Computer science,
Laboratories,
Anthropomorphism"
Tomographic Reconstruction of Three-Dimensional Volumes Using the Distorted Born Iterative Method,"Although real imaging problems involve objects that have variations in three dimensions, a majority of work examining inverse scattering methods for ultrasonic tomography considers 2-D imaging problems. Therefore, the study of 3-D inverse scattering methods is necessary for future applications of ultrasonic tomography. In this work, 3-D reconstructions using different arrays of rectangular elements focused on elevation were studied when reconstructing spherical imaging targets by producing a series of 2-D image slices using the 2-D distorted Born iterative method (DBIM). The effects of focal number f/#, speed of sound contrast Deltac, and scatterer size were considered. For comparison, the 3-D wave equation was also inverted using point-like transducers to produce fully 3-D DBIM image reconstructions. In 2-D slicing, blurring in the vertical direction was highly correlated with the transmit/receive elevation point-spread function of the transducers for low Deltac. The eventual appearance of overshoot artifacts in the vertical direction were observed with increasing Deltac. These diffraction-related artifacts were less severe for smaller focal number values and larger spherical target sizes. When using 3-D DBIM, the overshoot artifacts were not observed and spatial resolution was improved. However, results indicate that array configuration in 3-D reconstructions is important for good image reconstruction. Practical arrays were designed and assessed for image reconstruction using 3-D DBIM.","Tomography,
Iterative methods,
Image reconstruction,
Ultrasonic imaging,
Inverse problems,
Three dimensional displays,
Acoustic scattering,
Partial differential equations,
Acoustic transducers,
Diffraction"
Towards a unique FPGA-based identification circuit using process variations,"A compact chip identification (ID) circuit with improved reliability is presented. Ring oscillators are used to measure the spatial process variation and the ID is based on their relative speeds. A novel averaging and postprocessing scheme is employed to accurately determine the faster of two similar-frequency ring oscillators in the presence of noise. Using this scheme, the average number of unstable bits i.e. bits which can change in value between readings, measured on an FPGA is shown to be reduced from 5.3% to 0.9% at 20°C. Within the range 20 – 60°C, the percentage of unstable bits is within 2.8%. An analysis of the effectiveness of the scheme and the distribution of the errors is given over different temperature ranges and FPGA chips.","Circuits,
Field programmable gate arrays,
Temperature distribution,
Radiofrequency identification,
Intrusion detection,
Semiconductor device measurement,
Nonvolatile memory,
Voltage,
Ring oscillators,
Semiconductor lasers"
Security considerations for the WirelessHART protocol,"WirelessHART is a secure and reliable communication standard for industrial process automation. The WirelessHART specifications are well organized in all aspects except security: there are no separate specifications of security requirements or features. Rather, security mechanisms are described throughout the documentation. This hinders implementation of the standard and development of applications since it requires profound knowledge of all the core specifications on the part of the developer. In this paper we provide a comprehensive overview of WirelessHART security: we analyze the provided security mechanisms against well known threats in the wireless medium, and propose recommendations to mitigate shortcomings. Furthermore, we elucidate the specifications of the security manager, its placement in the network, and interaction with the network manager.","Wireless application protocol,
Data security,
Communication system security,
Automation,
Wireless sensor networks,
Monitoring,
Resource management,
Communication standards,
Standards development,
IEC standards"
Shape priors and discrete MRFs for knowledge-based segmentation,"In this paper we introduce a new approach to knowledge-based segmentation. Our method consists of a novel representation to model shape variations as well as an efficient inference procedure to fit the model to new data. The considered shape model is similarity-invariant and refers to an incomplete graph that consists of intra and intercluster connections representing the inter-dependencies of control points. The clusters are determined according to the co-dependencies of the deformations of the control points within the training set. The connections between the components of a cluster represent the local structure while the connections between the clusters account for the global structure. The distributions of the normalized distances between the connected control points encode the prior model. During search, this model is used together with a discrete Markov random field (MRF) based segmentation, where the unknown variables are the positions of the control points in the image domain. To encode the image support, a Voronoi decomposition of the domain is considered and regional based statistics are used. The resulting model is computationally efficient, can encode complex statistical models of shape variations and benefits from the image support of the entire spatial domain.","Shape control,
Image segmentation,
Biomedical imaging,
Interpolation,
Context modeling,
Deformable models,
Shape measurement,
Computer science,
Radiology,
Markov random fields"
A Deep Non-linear Feature Mapping for Large-Margin kNN Classification,"KNN is one of the most popular data mining methods for classification, but it often fails to work well with inappropriate choice of distance metric or due to the presence of numerous class-irrelevant features. Linear feature transformation methods have been widely applied to extract class-relevant information to improve kNN classification, which is very limited in many applications. Kernels have also been used to learn powerful non-linear feature transformations, but these methods fail to scale to large datasets. In this paper, we present a scalable non-linear feature mapping method based on a deep neural network pretrained with Restricted Boltzmann Machines for improving kNN classification in a large-margin framework, which we call DNet-kNN. DNet-kNN can be used for both classification and for supervised dimensionality reduction. The experimental results on two benchmark handwritten digit datasets and one newsgroup text dataset show that DNet-kNN has much better performance than large-margin kNN using a linear mapping and kNN based on a deep autoencoder pretrained with Restricted Boltzmann Machines.","Data mining,
Kernel,
Computer science,
Nearest neighbor searches,
Genetics,
Neural networks,
High performance computing,
Principal component analysis,
Graphical models,
Power generation"
Mobile human-robot teaming with environmental tolerance,"We demonstrate that structured light-based depth sensing with standard perception algorithms can enable mobile peer-to-peer interaction between humans and robots. We posit that the use of recent emerging devices for depth-based imaging can enable robot perception of non-verbal cues in human movement in the face of lighting and minor terrain variations. Toward this end, we have developed an integrated robotic system capable of person following and responding to verbal and non-verbal commands under varying lighting conditions and uneven terrain. The feasibility of our system for peer-to-peer HRI is demonstrated through two trials in indoor and outdoor environments.","Humans,
Cameras,
Speech recognition,
Robot vision systems,
Hidden Markov models"
Throughput optimal task allocation under thermal constraints for multi-core processors,"It is known that temperature gradients and thermal hotspots affect the reliability of microprocessors. Temperature is also an important constraint when maximizing the performance of processors. Although DVFS and DFS can be used to extract higher performance from temperature and power constrained single core processors, the full potential of multi-core performance cannot be exploited without the use of thread migration or task-to-core allocation schemes. In this paper, we formulate the problem of throughput-optimal task allocation on thermally constrained multi-core processors, and present a novel solution that includes optimal speed throttling. We show that the algorithms are implementable in real time and can be implemented in operating system's dynamic scheduling policy. The method presented here can result in a significant improvement in throughput over existing methods (5X over a naive scheme).","Throughput,
Multicore processing,
Temperature,
Thermal management,
Optimization,
Thermal engineering,
Upper bound,
Frequency,
Yarn,
Algorithm design and analysis"
TIGUAN: Thread-Parallel Integrated Test Pattern Generator Utilizing Satisfiability ANalysis,"We present the automatic test pattern generator TIGUAN based on a thread-parallel SAT solver. Due to a tight integration of the SAT engine into the ATPG algorithm and a carefully chosen mix of various optimization techniques, multi-million-gate industrial circuits are handled without aborts. TIGUAN supports both conventional single-stuck-at faults and sophisticated conditional multiple stuck-at faults which allows to generate patterns for non-standard fault models.",
"A survey on wireless security protocols (WEP, WPA and WPA2/802.11i)","Wireless technology has been gaining rapid popularity for some years. Adaptation of a standard depends on the ease of use and level of security it provides. In this case, contrast between wireless usage and security standards show that the security is not keeping up with the growth paste of end user's usage. Current wireless technologies in use allow hackers to monitor and even change the integrity of transmitted data. Lack of rigid security standards has caused companies to invest millions on securing their wireless networks. There are three major types of security standards in wireless. In our previous papers which registered in ICFCC 2009 Malaysia and ICCDA 2009 Singapore [1] [2], we explained the structure of WEP and WPA as first and second wireless security protocols and discussed all their versions, problems and improvements. Now, we try to explain WPA2 versions, problems and enhancements that have done solve the WPA major weakness. Finally we make a comparison among WEP and WPA and WPA2 as all wireless security protocols in Wi-Fi technology. In the next phase we hope that we will publish a complete comparison among wireless security techniques by add the WiMax security technique and make a whole comparison among all security protocols in this area.","Communication system security,
Wireless application protocol,
Cryptography,
Data security,
Protection,
Access protocols,
Media Access Protocol,
Knowledge management,
Data privacy,
Wireless LAN"
A Secure Implementation of White-Box AES,,"Billets,
Cryptography,
Protection,
Encoding,
Information analysis,
Manipulator dynamics,
Software algorithms,
Resists,
Table lookup,
Security"
An adaptive neuro-endocrine system for robotic systems,"We present an adaptive artificial neural-endocrine (AANE) system that is capable of learning “on-line” and exploits environmental data to allow for adaptive behaviour to be demonstrated. Our AANE is capable of learning associations between sensor data and actions, and affords systems the ability to cope with sensor degradation and failure. We have tested our system in real robotic units and demonstrate adaptive behaviour over prolonged periods of time. This work is another step towards creating a robotic control system that affords “homeostasis” for prolonged autonomy.","Adaptive systems,
Robots"
Dynamic Service and Data Migration in the Clouds,"Cloud computing is an emerging computation paradigm. To support successful cloud computing, service oriented architecture (SOA) should play a major role. Due to the nature of widely distributed service providers in clouds, the service performance could be impacted when the network traffic is congested. This can be a major barrier for tasks with real-time requirements. In clouds, this problem can be solved by migrating services to different platforms such that the communication cost can be minimized. In this paper, we consider the problem of service selection and migration in clouds. We develop a framework to facilitate service migration and design a cost model and the decision algorithm to determine the tradeoffs on service selection and migration.","Cloud computing,
Service oriented architecture,
Costs,
Hardware,
Resource management,
Distributed computing,
Application software,
Computer science,
Web services,
Delay"
Phonetic aspects of content design in AESOP (Asian English Speech cOrpus Project),"This research is part of the ongoing multinational collaboration “Asian English Speech cOrpus Project” (AESOP), whose aim is to build up an Asian English speech corpus representing the varieties of English spoken in Asia. AESOP is an international consortium of linguists, speech scientists, psychologists and educators from Japan, Taiwan, Hong Kong, China, Thailand, Indonesia and Mongolia. Its primary aim is to collect and compare Asian English speech corpora from the countries listed above in order to derive a set of core properties common to all varieties of Asian English, as well as to discover features that are particular to individual varieties. Each research team will use a common recording setup and share an experimental task set, and will develop a common, open-ended annotation system. Moreover, AESOP-collected corpora will be an open resource, available to the research community at large. The initial stage of the phonetics aspect of this project will be devoted to designing spoken-language tasks which will elicit production of a large range of English segmental and suprasegmental characteristics. These data will be used to generate a catalogue of acoustic characteristics particular to individual varieties of Asian English, which will then be compared with the data collected by other AESOP members in order to determine areas of overlap between L1 and L2 English as well as differences among varieties of Asian English.","Natural languages,
Collaboration,
Asia,
Psychology,
Speech analysis,
Laboratories,
Production,
Character generation,
Educational technology,
Communications technology"
Simulation of the Dynamics of Nonplayer Characters' Emotions and Social Relations in Games,"One of the main challenges faced by the video game industry is to give life to believable nonplayer characters (NPCs). Research shows that emotions play a key role in determining the behavior of individuals. In order to improve the believability of NPCs' behavior, we propose in this paper a model of the dynamics of emotions taking into account the personality and the social relations of the character. First, we present work from the literature on emotions, personality, and social relations in computer science and in human and social sciences. We focus on the influence of personality on the triggering of emotions, and the influence of emotions on the dynamics of social relations. Based on this work, we propose a dynamic model of the socioemotional state and its implementation as part of a tool for game programmers. This tool aims at the simulation of the evolution of emotions and social relations of NPCs based on their personality and roles.","Games,
Artificial intelligence,
Intelligent agent,
Avatars,
Computational intelligence,
Computational modeling,
Computer industry,
Industrial relations,
Toy industry,
Computer science"
Combinatorial Constructions for Optimal Two-Dimensional Optical Orthogonal Codes,"Optical orthogonal codes (OOCs) have been designed for OCDMA. A one-dimensional (1-D) optical orthogonal code (1-D OOC) is a set of one-dimensional binary sequences having good auto and cross-correlations. One limitation of 1-D OOC is that the length of the sequence increases rapidly when the number of users or the weight of the code is increased, which means large bandwidth expansion is required if a big number of codewords is needed. To lessen this problem, two-dimensional (2-D) coding (also called multiwavelength OOCs) was invested. A two dimensional (2-D) optical orthogonal code (2-D OOC) is a set of utimesv matrices with (0, 1) elements having good auto and cross-correlations. Recently, many researchers are working on constructions and designs of 2-D OOCs. In this paper, we shall reveal the combinatorial properties of 2-D OOCs and give an equivalent combinatorial description of a 2-D OOC. Based on this, we are able to use combinatorial methods to obtain many optimal 2-D OOCs.","Two dimensional displays,
Optical design,
Binary sequences,
Bandwidth,
Mathematics,
Multiaccess communication,
Optical fibers,
Asynchronous communication,
Communication system security,
Computer science"
"Extensions to the Method of Multiplicities, with Applications to Kakeya Sets and Mergers","We extend the ""method of multiplicities"" to get the following results, of interest in combinatorics and randomness extraction. (A) We show that every Kakeya set (a set of points that contains a line in every direction) in
Undefined control sequence \F
must be of size at least
q
n
/
2
n
. This bound is tight to within a
2+o(1)
factor for every
n
as
q→∞
, compared to previous bounds that were off by exponential factors in
n
. (B) We give improved randomness extractors and ""randomness mergers"". Mergers are seeded functions that take as input
Λ
(possibly correlated) random variables in
{0,1
}
N
and a short random seed and output a single random variable in
{0,1
}
N
that is statistically close to having entropy
(1−δ)⋅N
when one of the
Λ
input variables is distributed uniformly. The seed we require is only
(1/δ)⋅logΛ
-bits long, which significantly improves upon previous construction of mergers. (C) Using our new mergers, we show how to construct randomness extractors that use logarithmic length seeds while extracting
1−o(1)
fraction of the min-entropy of the source. The ""method of multiplicities"", as used in prior work, analyzed subsets of vector spaces over finite fields by constructing somewhat low degree interpolating polynomials that vanish on every point in the subset {\em with high multiplicity}. The typical use of this method involved showing that the interpolating polynomial also vanished on some points outside the subset, and then used simple bounds on the number of zeroes to complete the analysis. Our augmentation to this technique is that we prove, under appropriate conditions, that the interpolating polynomial vanishes {\em with high multiplicity} outside the set. This novelty leads to significantly tighter analyses.","Corporate acquisitions,
Polynomials,
Computer science,
Application software,
Combinatorial mathematics,
Random variables,
Entropy,
Functional analysis,
Input variables,
Galois fields"
Assisted-freehand ultrasound elasticity imaging,"Good-quality elasticity imaging requires highly controlled compressions of the breast, which are often challenging to obtain with freehand, even by an experienced radiologist. This paper presents assisted-freehand ultrasound (AFUSON): a fusion of freehand and automated ultrasound systems designed to assisted elasticity imaging acquisition while remaining as flexible as freehand. In the form of a hand-held device, this semi-automatic solution delivers both increased acquisition precision and control. Compared with freehand acquisitions, it reduces out-of-plane motion decorrelation by one-half and lateral motion by one-third, increases within-scan repeatability by 50%, and does so across operators.","Ultrasonic imaging,
Elasticity,
Breast,
Protocols,
Probes,
Engines,
Algorithm design and analysis,
Automatic control,
Workstations,
Graphical user interfaces"
"Performance Comparison of AODV, DSR and ZRP Routing Protocols in MANET'S","This paper aims to compare performance of some routing protocols for Mobile Ad-Hoc networks (MANET’s). A Mobile Ad-Hoc Network (MANET) is a collection of wireless mobile nodes forming a temporary network without using any centralized access point, infrastructure, or centralized administration. Data transmission between two nodes in MANET’s , requires multiple hops as nodes transmission range is limited. Mobility of the different nodes makes the situation even more complicated. Multiple routing protocols especially for these conditions have been developed during the last years, to find optimized routes from a source to some destination. This paper presents performance evaluation of three different routing Protocols (AODV, DSR and ZRP) in variable pause times . We have used QualNet Simulator [1] from Scalable Networks to perform the simulations. Performance evaluation of AODV, DSR and ZRP is evaluated based on Average end to end delay, TTL based hop count and Packet delivery ratio.","Routing protocols,
Mobile ad hoc networks,
Computer networks,
Ad hoc networks,
Broadcasting,
Telecommunication computing,
Communication system control,
Telecommunication control,
Intelligent networks,
Computer science"
Radio-Controlled Cyborg Beetles: A Radio-Frequency System for Insect Neural Flight Control,"We present the first report of radio control of a cyborg beetle in free-flight. The microsystem (Figs. 1,2) consisted of a radio-frequency receiver assembly, a micro battery and a live giant flower beetle platform (Mecynorhina polyphemus or Mecynorhina torquata). The assembly had six electrode stimulators implanted into the left and right optic lobes, brain, posterior pronotum (counter electrode), right and left basalar flight muscles. Initiation and cessation of flight were accomplished by optic lobe stimulation while muscular stimulation of either right or left basalar flight muscles (referenced to the posterior pronotum electrode) elicited left or right turns, respectively. Flight commands were wirelessly transferred to the beetle-mounted system (running BeetleBrain v1.0 code) via an RF transmitter operated by a laptop running custom software (BeetleCo mmander v1.0) through a USB/Serial interface.","Radio control,
Radio frequency,
Insects,
Aerospace control,
Optical receivers,
Electrodes,
Optical transmitters,
Assembly,
Stimulated emission,
Muscles"
Software defined radio in the electrical and computer engineering curriculum,The development of Software Defined Radio systems and their extension to Cognitive Radio Systems and Smart Radio Systems have introduced a plethora of topics and examples that can be included in the curriculum. The design of these software defined radio systems has less in common with traditional radio design and more in common with the design of Embedded Systems and Software Engineering. This purpose of this paper is to overview software defined radio from the simplest form to its most complicated form and giveexamples on how software defined radio concepts can be used as examples and exercises in a variety of Electrical Engineering and Computer Engineering courses and labs.,"Software radio,
Electrical engineering computing,
Digital signal processing,
Radio transmitters,
Receivers,
Field programmable gate arrays,
Application software,
Frequency,
Hardware,
Software engineering"
Image annotation using multi-label correlated Green's function,"Image annotation has been an active research topic in the recent years due to its potentially large impact on both image understanding and web/database image search. In this paper, we target at solving the automatic image annotation problem in a novel semi-supervised learning framework. A novel multi-label correlated Green's function approach is proposed to annotate images over a graph. The correlations among labels are integrated into the objective function which improves the performance significantly. We also propose a new adaptive decision boundary method for multi-label assignment to deal with the difficulty of label assignment in most of the existing rank-based multi-label classification algorithms. Instead of setting the threshold heuristically or by experience, our method principally compute it upon the prior knowledge in the training data. We perform our methods on three commonly used image annotation testing data sets. Experimental results show significant improvements on classification performance over four other state-of-the-art methods. As a general semi-supervised learning framework, other local feature based image annotation methods could be easily incorporated into our framework to improve the performance.","Green's function methods,
Semisupervised learning,
Image retrieval,
Training data,
Information retrieval,
Content based retrieval,
Computer vision,
Pairwise error probability,
Computer science,
Data engineering"
Searching for Stability in Interdomain Routing,"The border gateway protocol (BGP) handles the task of establishing routes between the autonomous systems (ASes) that make up the Internet. It is known that it is possible for a group of ASes to define local BGP policies that lead to global BGP protocol oscillations. We close a long standing open question by showing that, for any network, if two stable routing outcomes exist then persistent BGP route oscillations are possible. This is the first non-trivial necessary condition for BGP safety. It shows that BGP safety must always come at the price of severe restrictions on ASes' expressiveness in their choice of routing policies. The technical tools used in our proof may be helpful in the detection of potential route oscillations and their debugging. We also address the question of how long it takes BGP to converge to a stable routing outcome. We analyze a formal measure of the convergence time of BGP for the policy class defined by Gao and Rexford, which is said to accurately depict the business structure underlying the Internet. We prove that, even for this restricted class of preferences, the convergence time might be linear in the size of the network. However, we show a much more reasonable bound if the network structure is similar to the current Internet: we prove that the number of phases required for convergence is bounded by approximately twice the depth of the customer-provider hierarchy.","Stability,
Safety,
Internet,
Convergence,
Computer science,
Routing protocols,
Debugging,
Sufficient conditions,
Communications Society,
USA Councils"
Similarity metrics for categorization: From monolithic to category specific,"Similarity metrics that are learned from labeled training data can be advantageous in terms of performance and/or efficiency. These learned metrics can then be used in conjunction with a nearest neighbor classifier, or can be plugged in as kernels to an SVM. For the task of categorization two scenarios have thus far been explored. The first is to train a single “monolithic” similarity metric that is then used for all examples. The other is to train a metric for each category in a 1-vs-all manner. While the former approach seems to be at a disadvantage in terms of performance, the latter is not practical for large numbers of categories. In this paper we explore the space in between these two extremes. We present an algorithm that learns a few similarity metrics, while simultaneously grouping categories together and assigning one of these metrics to each group. We present promising results and show how the learned metrics generalize to novel categories.","Machine learning,
Training data,
Kernel,
Computer vision,
Nearest neighbor searches,
Support vector machines,
Support vector machine classification,
Space exploration,
Object recognition,
Focusing"
Mental models of privacy and security,"The mental models approach could significantly improve risk communication in the case of computer security. The particular mental models that will be discussed here are: physical, medical, criminal, warfare, and market models. Our strongest conclusion is that mental models can be used to improve risk communication. The second, untested, conclusion is that the best model may be the medical model.","Cognitive science,
Privacy,
Computer security,
Decision making,
Packaging,
Humans,
Uncertainty,
Information security,
Birth disorders,
Pregnancy"
An FPGA-based stream processor for embedded real-time vision with Convolutional Networks,"Many recent visual recognition systems can be seen as being composed of multiple layers of convolutional filter banks, interspersed with various types of non-linearities. This includes Convolutional Networks, HMAX-type architectures, as well as systems based on dense SIFT features or Histogram of Gradients. This paper describes a highly-compact and low power embedded system that can run such vision systems at very high speed. A custom board built around a Xilinx Virtex-4 FPGA was built and tested. It measures 70 × 80 mm, and the complete system-FPGA, camera, memory chips, flash-consumes 15 watts in peak, and is capable of more than 4 × 109 multiply-accumulate operations per second in real vision application. This enables real-time implementations of object detection, object recognition, and vision-based navigation algorithms in small-size robots, micro-UAVs, and hand-held devices. Real-time face detection is demonstrated, with speeds of 10 frames per second at VGA resolution.","Robot vision systems,
Filter bank,
Histograms,
Embedded system,
Machine vision,
Field programmable gate arrays,
Testing,
Semiconductor device measurement,
Cameras,
Object detection"
Tracking multiple objects using particle filters and digital elevation maps,"Tracking multiple objects has always been a challenge, and is a crucial problem in the field of driving assistance systems. The particle filter-based trackers have the theoretical possibility of tracking multiple hypotheses, but in practice the particles will cluster around the stronger one. This paper proposes a two-level approach to the multiple object tracking problem. One particle filter-based tracker will search the whole state space for new hypotheses, and when a hypothesis becomes strong enough, it will be passed to an individual object tracker, which will track it until the object is lost. The initialization tracker and the individual object trackers use the same state models and the same measurement technique, based on stereovision-generated elevation maps, and differ only in their use of the estimation results. The proposed solution is a simple and robust one, adaptable to different types of object models and to different types of sensors.","Particle tracking,
Particle filters,
Image edge detection,
Stereo image processing,
Probability density function,
State-space methods,
Computer science,
Filtering,
Detection algorithms,
Image reconstruction"
An Analysis of a Neural Dynamical Approach to Solving Optimization Problems,"Recently, a neural dynamical approach to solving linearly constrained variational inequality problems is presented, and its stability and convergence are conjectured by simulation. This technical note analyzes the global stability and convergence of the neural dynamical approach. Theoretically, it is shown that the neural dynamical approach is convergent globally to a solution when the nonlinear mapping is monotone at the solution. Unlike existing convergence results of neural dynamical methods for solving linearly or nonlinearly variational inequalities, our main results don't assume the differentiability condition of the nonlinear mapping. Therefore, the neural dynamical approach can be further guaranteed to solve linearly constrained monotone variational inequality problems with a non-smooth mapping. Comparsions and examples illustrative significance of the obtained results on non-smooth mapping.","Automatic control,
Filtering,
State estimation,
Nonlinear filters,
Control systems,
Riccati equations,
Observers,
Stability analysis,
Sufficient conditions,
Stochastic systems"
Automatic Transcription of Handwritten Medieval Documents,"The automatic transcription of historical documents is vital for the creation of digital libraries. In order to make images of valuable old documents amenable to browsing, a transcription of high accuracy is needed. In this paper, two state-of-the art recognizers originally developed for modern scripts are applied to medieval documents. The first is based on Hidden Markov Models and the second uses a Neural Network with a bidirectional Long Short-Term Memory. On a dataset of word images extracted from a medieval manuscript of the 13th century, written in Middle High German by several writers, it is demonstrated that a word accuracy of 93.32% is achievable. This is far above the word accuracy of 77.12% achieved with the same recognizers for unconstrained modern scripts written in English. These results encourage the development of real world systems for automatic transcription of historical documents with a view to image and text browsing in digital libraries.","Handwriting recognition,
Hidden Markov models,
Software libraries,
Writing,
Neural networks,
Computer science,
Multimedia systems,
Mathematics,
Artificial intelligence,
Vocabulary"
Design for Soft Error Resiliency in Internet Core Routers,"This paper describes the modeling, analysis and verification methods used to achieve a reliability target set for transient outages in equipment used to build the backbone routing infrastructure of the Internet. We focus on the ASIC design and analysis techniques that were undertaken to achieve the targeted behavior using 65 nm technology. Considerable attention was paid to Single Event Upset in flip-flops and their potential to produce network impacting events that are not systematically detected and controlled. Using random fault injection in large scale RTL simulations, and slack time distributions from static timing analysis, estimates of functional and temporal soft error masking effects were applied to a system soft error model to drive decisions on interventions such as the use of larger resilient flip-flops, parity protection of registers groupings, and designed responses to detected upsets.",
In-Network Coherence Filtering: Snoopy coherence without broadcasts,"With transistor miniaturization leading to an abundance of on-chip resources and uniprocessor designs providing diminishing returns, the industry has moved beyond single-core microprocessors and embraced the many-core wave. Scalable cache coherence protocol implementations are necessary to allow fast sharing of data among various cores and drive the many-core revolution forward. Snoopy coherence protocols, if realizable, have the desirable property of having low storage overhead and not adding indirection delay to cache-to-cache accesses. There are various proposals, like Token Coherence (TokenB), Uncorq, Intel QPI, INSO and Timestamp Snooping, that tackle the ordering of requests in snoopy protocols and make them realizable on unordered networks. However, snoopy protocols still have the broadcast overhead because each coherence request goes to all cores in the system. This has substantial network bandwidth and power implications. In this work, we propose embedding small in-network coherence filters inside on-chip routers that dynamically track sharing patterns among various cores. This sharing information is used to filter away redundant snoop requests that are traveling towards unshared cores. Filtering these useless messages saves network bandwidth and power and makes snoopy protocols on many-core systems truly scalable. Our in-network coherence filters are able to reduce the total number of snoops in the system on an average by 41.9%, thereby reducing total network traffic by 25.4% on 16-processor chip multiprocessor (CMP) systems running parallel applications. For 64-processor CMP systems, our filtering technique on an average achieves 46.5% reduction in total number of snoops that ends up reducing the total network traffic by 27.3%, on an average.","Filtering,
Broadcasting,
Coherence,
Access protocols,
Filters,
Bandwidth,
Telecommunication traffic,
Microprocessors,
Cache storage,
Added delay"
Microclimate real-time monitoring based on ZigBee sensor network,"Monitoring microenvironment at the farm level has recently become one of the hottest topics in precision agriculture. Zigbee technology is then the most prospective candidate for wirelessly networking those field sensors due to its low cost and power consumption and flexible architecture. The microclimate monitoring system in this research is a suit of equipments based on Zigbee networking to measure the air temperature and humidity in a vast area. Every sensor node works on a cluster tree topology which extends the point-to-point distance up to 1 mile (line of sight), allowing this system to cover large farm using less sensor nodes. A sensor node consists of a micro-controller unit connected with air temperature and humidity sensor chips which are packed in a cylindrical louvered housing to prevent fault air temperature and humidity data from solar radiation. Every sensor node uses energy from a solar cell charged by a charger circuit to a battery package that stores power for use during night time. An energy management scheme was implemented to optimize power use for sending and receiving data. The data from every node were sent to the receiver every 8–30 minutes, depending on backup energy status at each node. The humidity and temperature data are stored on a data-logging PC and only current data are displayed on website","Monitoring,
ZigBee,
Temperature sensors,
Humidity,
Temperature measurement,
Sensor systems,
Agriculture,
Costs,
Energy consumption,
Area measurement"
Hierarchical Bayesian Models for Collaborative Tagging Systems,"Collaborative tagging systems with user generated content have become a fundamental element of websites such as Delicious, Flickr or CiteULike. By sharing common knowledge, massively linked semantic data sets are generated that provide new challenges for data mining. In this paper, we reduce the data complexity in these systems by finding meaningful topics that serve to group similar users and serve to recommend tags or resources to users. We propose a well-founded probabilistic approach that can model every aspect of a collaborative tagging system. By integrating both user information and tag information into the well-known Latent Dirichlet Allocation framework, the developed models can be used to solve a number of important information extraction and retrieval tasks.",
MapReduce for the Cell Broadband Engine Architecture,"MapReduce is a simple and flexible parallel programming model proposed by Google for large-scale distributed data processing. In this paper, we present a design and prototype implementation of MapReduce for the Cell Broadband Engine® Architecture (CBEA). The MapReduce model provides a simple machine abstraction that shields users from parallelization and other distributed programming complications. The goal of this paper is to describe the tradeoffs in the design of the runtime and demonstrate the potential for high performance. We study the basic characteristics of the MapReduce model and identify three types of MapReduce applications: map dominated, partition dominated, and sort dominated. We evaluate our runtime performance, scalability, and efficiency for microbenchmarks representing each of these application types as well as for complete applications. We find that map-dominated applications map well to the CBEA and that our prototype sustains high performance on these applications. For partition-dominated and sort-dominated applications, we analyze runtime performance, identify sources of inefficiency, and propose several future enhancements to significantly improve performance. Overall, we find that the simplicity and efficiency of the model make it an attractive tool for programming Cell Broadband Engine processor-based platforms.",
Real-time learning of accurate patch rectification,"Recent work showed that learning-based patch rectification methods are both faster and more reliable than affine region methods. Unfortunately, their performance improvements are founded in a computationally expensive offline learning stage, which is not possible for applications such as SLAM. In this paper we propose an approach whose training stage is fast enough to be performed at run-time without the loss of accuracy or robustness. To this end, we developed a very fast method to compute the mean appearances of the feature points over sets of small variations that span the range of possible camera viewpoints. Then, by simply matching incoming feature points against these mean appearances, we get a coarse estimate of the viewpoint that is refined afterwards. Because there is no need to compute descriptors for the input image, the method is very fast at run-time. We demonstrate our approach on tracking-by-detection for SLAM, real-time object detection and pose estimation applications.","Simultaneous localization and mapping,
Runtime,
Robustness,
Cameras,
Object detection"
On Allocating Goods to Maximize Fairness,"We consider the Max-Min Allocation problem: given a set of m agents and a set of n items, where agent A has utility u(A, i) for item i, our goal is to allocate items to agents so as to maximize fairness. Specifically, the utility of an agent is the sum of its utilities for the items it receives, and we seek to maximize the minimum utility of any agent. While this problem has received much attention recently, its approximability has not been well-understood thus far. The best known approximation algorithm achieves a roughly O(\sqrt m}-approximation, and in contrast, the best known hardness of approximation stands at 2. Our main result is an algorithm that achieves a \tilde{O}(n^{\eps})-approximation in time n^{O(1/\eps)} for any \eps=\Omega(log log n/log n). In particular, we obtain a poly-logarithmic approximation in quasi-polynomial time, and for every constant \eps ≫ 0, we obtain an n^{\eps}-approximation in polynomial time. Our algorithm also yields a quasi-polynomial time m^{\eps}-approximation algorithm for any constant \eps ≫ 0. An interesting technical aspect of our algorithm is that we use as a building block a linear program whose integrality gap is \Omega(\sqrt m). We bypass this obstacle by iteratively using the solutions produced by the LP to construct new instances with significantly smaller integrality gaps, eventually obtaining the desired approximation. We also investigate a special case of the problem, where every item has a non-zero utility for at most two agents. This problem is hard to approximate to within any factor better than 2. We give a factor 2-approximation algorithm.","Approximation algorithms,
Polynomials,
Upper bound,
Computer science,
Information science,
Iterative algorithms,
Resource management"
A cross-input adaptive framework for GPU program optimizations,"Recent years have seen a trend in using graphic processing units (GPU) as accelerators for general-purpose computing. The inexpensive, single-chip, massively parallel architecture of GPU has evidentially brought factors of speedup to many numerical applications. However, the development of a high-quality GPU application is challenging, due to the large optimization space and complex unpredictable effects of optimizations on GPU program performance. Recently, several studies have attempted to use empirical search to help the optimization. Although those studies have shown promising results, one important factor—program inputs—in the optimization has remained unexplored. In this work, we initiate the exploration in this new dimension. By conducting a series of measurement, we find that the ability to adapt to program inputs is important for some applications to achieve their best performance on GPU. In light of the findings, we develop an input-adaptive optimization framework, namely G-ADAPT, to address the influence by constructing cross-input predictive models for automatically predicting the (near-)optimal configurations for an arbitrary input to a GPU program. The results demonstrate the promise of the framework in serving as a tool to alleviate the productivity bottleneck in GPU programming.","Parallel architectures,
Computer architecture,
Program processors,
Yarn,
Computer science,
Educational institutions,
Computer graphics,
Application software,
Predictive models,
Productivity"
On the security of non-invertible fingerprint template transforms,"Many transformation functions have been proposed for generating revocable or non-invertible biometric templates. However, their security analysis either ignores the distribution of biometric features or uses inefficient feature matching. This usually leads to unrealistic estimates of security. In this paper we introduce a new measure of non-invertibility, called the Coverage-Effort (CE) curve which measures the number of guesses (Effort) required by an adversary to recover a certain fraction (Coverage) of the original biometric data. In addition to utilizing the feature distribution, the CE curve allows estimation of security against partial recovery of biometric features. We analyze the CE curves obtained using different instances of a mixture of Gaussians based feature transform for fingerprint templates. Our analysis shows that knowledge of the fingerprint minutiae distribution reduces the effort required to obtain a specified coverage.","Fingerprint recognition,
Biometrics,
Authentication,
Data security,
Bioinformatics,
Protection,
Cryptography,
Information security,
Humans,
Computer security"
"On the multiple unicast network coding, conjecture","In this paper, we study the multiple unicast network communication problem on undirected graphs. It has been conjectured by Li and Li [CISS 2004] that, for the problem at hand, the use of network coding does not allow any advantage over standard routing. Loosely speaking, we show that under certain (strong) connectivity requirements the advantage of network coding is indeed bounded by 3.","Unicast,
Network coding,
Routing,
Encoding,
Broadcasting,
Computer science,
Laboratories,
Polynomials,
Bipartite graph"
Detecting Insider Theft of Trade Secrets,"MITRE researchers designed a prototype system for identifying insider threats, which prompted a team of engineers and social scientists to experimentally study how malicious insiders use information differently from a benign baseline group. This research provides new insight into how malicious insiders behave.","Prototypes,
Design engineering"
Autonomous indoor helicopter flight using a single onboard camera,"We consider the problem of autonomously flying a helicopter in indoor environments. Navigation in indoor settings poses two major challenges. First, real-time perception and response is crucial because of the high presence of obstacles. Second, the limited free space in such a setting places severe restrictions on the size of the aerial vehicle, resulting in a frugal payload budget. We autonomously fly a miniature RC helicopter in small known environments using an on-board light-weight camera as the only sensor. We use an algorithm that combines data-driven image classification with optical flow techniques on the images captured by the camera to achieve real-time 3D localization and navigation. We perform successful autonomous test flights along trajectories in two different indoor settings. Our results demonstrate that our method is capable of autonomous flight even in narrow indoor spaces with sharp corners.","Helicopters,
Cameras,
Aircraft navigation,
Indoor environments,
Remotely operated vehicles,
Space vehicles,
Payloads,
Image classification,
Optical sensors,
Image motion analysis"
An Automatic Hand Gesture Recognition System Based on Viola-Jones Method and SVMs,"In this paper we present an automatic hand gesture recognition system operating on video stream. The system consists of two modules: hand gesture detection module and hand gesture recognition module. The detection module could accurately locate the hand regions with a blue rectangle; this is mainly based on Viola-Jones method, which is currently considered the fastest and most accurate learning-based method for object detection. In the recognition module, the Hu invariant moments feature vectors of the detected hand gesture are extracted and a Support Vector Machines (SVMs) classifier is trained for final recognition, due to its high generalization performance without the need to add a priori knowledge. The performance of the proposed system is tested through a series of experiments and a simple Human-Computer Interaction application based on hand gesture recognition method is finally developed.","Object detection,
Support vector machines,
Support vector machine classification,
Feature extraction,
Computer science,
Educational institutions,
Information science,
Streaming media,
Learning systems,
System testing"
Congestion games with resource reuse and applications in spectrum sharing,"In this paper we consider an extension to the classical definition of congestion games (CG) in which multiple users share the same set of resources and their payoff for using any resource is a function of the total number of users sharing it. The classical congestion games enjoy some very appealing properties, including the existence of a Nash equilibrium and that every improvement path is finite and leads to such a NE (also called the finite improvement property or FIP), which is also a local optimum to a potential function. On the other hand, this class of games does not model well the congestion or resource sharing in a wireless context, a prominent feature of which is spatial reuse. What this translates to in the context of a congestion game is that a user's payoff for using a resource (interpreted as a channel) is a function of the its number of its interfering users sharing that channel, rather than the total number among all users. This makes the problem quite different. We will call this the congestion game with resource reuse (CG-RR). In this paper we study intrinsic properties of such a game; in particular, we seek to address under what conditions on the underlying network this game possesses the FIP or NE. We also discuss the implications of these results when applied to wireless spectrum sharing.","Interference,
Wireless communication,
Character generation,
Nash equilibrium,
Resource management,
Cost function,
Collaborative work,
Delay,
Context modeling,
Routing"
Mobile imaging and Spectroscopic Threat Identification (MISTI): System overview,"The Mobile Imaging and Spectroscopic Threat Identification (MISTI) system developed to locate radiological threats in urban and rural environments is currently undergoing characterization activities. MISTI is a mobile source detection and imaging system designed to identify and localize a radiological source to within +/- 10 m in range. This requirement is based on a 1 mCi Cs-137 source at 100 m in 20s, while maintaining a false alarm rate of less than one per day. MISTI utilizes the cost effective collection power of NaI for imaging and the sensitivity of high resolution HPGe for spectroscopy. MISTI's data acquisition system was developed with the latest commercially availed hardware that met MISTI's requirements. The performance of crucial software and hardware components is presented along with overall system performance. A synopsis and example of the initial characterization results are presented here.","Spectroscopy,
Telephony,
Optical imaging,
High-resolution imaging,
Instruments,
Graphical user interfaces,
Costs,
Image resolution,
Hardware,
Nuclear imaging"
Perceptual compressive sensing for image signals,"Human eyes have different sensitivity to different frequency components of image signals, typically, low frequency components are relatively more crucial to the perceptual quality of images than high frequency components. Based on this observation, we propose a novel sampling scheme for compressive sensing framework by designing a weighting scheme for the sampling matrix. By adjusting the weighting coefficients, we can tune the structure of the sampling matrix to favor the frequency components that are important to human perception, so that those components could be more precisely recovered in the reconstruction procedure. Experimental results reveal that our proposed scheme can greatly enhance the performance of compressive sensing framework in both PSNR and visual quality without increasing the complexity of the framework structure or computational procedure.","Image coding,
Image sampling,
Signal sampling,
Humans,
Frequency,
Sampling methods,
Image reconstruction,
Sparse matrices,
Reconstruction algorithms,
Length measurement"
Distance Oracles for Spatial Networks,"The popularity of location-based services and the need to do real-time processing on them has led to an interest in performing queries on transportation networks, such as finding shortest paths and finding nearest neighbors. The challenge is that these operations involve the computation of distance along a spatial network rather than ""as the crow flies."" In many applications an estimate of the distance is sufficient, which can be achieved by use of an oracle. An approximate distance oracle is proposed for spatial networks that exploits the coherence between the spatial position of vertices and the network distance between them. Using this observation, a distance oracle is introduced that is able to obtain the epsilon-approximate network distance between two vertices of the spatial network. The network distance between every pair of vertices in the spatial network is efficiently represented by adapting the well-separated pair technique to spatial networks. Initially, use is made of an epsilon-approximate distance oracle of size O( n / epsilon^d ) that is capable of retrieving the approximate network distance in O(logn) time using a B-tree. The retrieval time can be theoretically reduced further to O(1) time by proposing another e-approximate distance oracle of size O(n log n / epsilon^d) that uses a hash table. Experimental results indicate that the proposed technique is scalable and can be applied to sufficiently large road networks. A 10%-approximate oracle (epsilon = 0.1) on a large network yielded an average error of 0.9% with 90% of the answers making an error of 2% or less and an average retrieval timeof 68µ seconds. Finally, a strategy for the integration of the distance oracle into any relational database system as well as using it to perform a variety of spatial queries such as region search, k-nearest neighbor search, and spatial joins on spatial networks is discussed.","Computer networks,
Spatial coherence,
Data engineering,
Automation,
Computer science,
Educational institutions,
Transportation,
Nearest neighbor searches,
Roads,
Relational databases"
An adaptive coevolutionary Differential Evolution algorithm for large-scale optimization,"In this paper, we propose a new algorithm, named JACC-G, for large scale optimization problems. The motivation is to improve our previous work on grouping and adaptive weighting based cooperative coevolution algorithm, DECC-G [1], which uses random grouping strategy to divide the objective vector into subcomponents, and solve each of them in a cyclical fashion. The adaptive weighting mechanism is used to adjust all the subcomponents together at the end of each cycle. In the new JACC-G algorithm: (1) A most recent and efficient Differential Evolution (DE) variant, JADE [2], is employed as the subcomponent optimizer to seek for a better performance; (2) The adaptive weighting is time-consuming and expected to work only in the first few cycles, so a detection module is added to prevent applying it arbitrarily; (3) JADE is also used to optimize the weight vector in adaptive weighting process instead of using a basic DE in previous DECC-G. The efficacy of the proposed JACC-G algorithm is evaluated on two sets of widely used benchmark functions up to 1000 dimensions.","Large-scale systems,
Optimization methods,
Evolutionary computation,
Scalability"
Message passing for GPGPU clusters: CudaMPI,"We present and analyze two new communication libraries, cudaMPI and glMPI, that provide an MPI-like message passing interface to communicate data stored on the graphics cards of a distributed-memory parallel computer. These libraries can help applications that perform general purpose computations on these networked GPU clusters. We explore how to efficiently support both point-to-point and collective communication for either contiguous or noncontiguous data on modern graphics cards. Our software design is informed by a detailed analysis of the actual performance of modern graphics hardware, for which we develop and test a simple but useful performance model.","Message passing,
Computer graphics,
Computer interfaces,
Concurrent computing,
Distributed computing,
Software libraries,
Application software,
Computer networks,
Software design,
Performance analysis"
M-government: Opportunities and Challenges to Deliver Mobile Government Services in Developing Countries,"Mobile Government (m-Government) is the use of mobile technologies within the government administration to deliver public services to citizens and firms. It is quickly emerging as the new frontier of service delivery, and transforming government by making public services more accessible to citizens. Governments in developing countries are increasingly making efforts to provide more access to information and services for citizens, businesses, and civil servants through wireless devices. However, providing strategically high impact m-services is beset with numerous challenges- complexity of different mobile technologies, creating secured networks to deliver reliable service, and identifying the types of services that can be easily provided on mobile devices. The scope of this study, therefore, is to identify and discuss some of the challenges as well as the opportunities for improving government performance. The objectives of this paper are, to discuss the main challenges and opportunities of m-government and to identify m-services that m-Government initiatives present for developing countries.","Electronic government,
Personal digital assistants,
Mobile computing,
Communication system security,
Communications technology,
Business communication,
Web and internet services,
Mobile handsets,
Wireless networks,
Context-aware services"
Architectural styles for runtime software adaptation,"Runtime software adaptability — the ability to change an application's behavior during runtime — is an increasingly important capability for systems, both to support continuous operation and to support a good user experience. Achieving such adaptability may be very hard or easy; the degree of difficulty will largely reflect choices made in a system's architecture. Some architectural styles are much more supportive of dynamic change than others. This paper examines a range of styles and assesses them with respect to a four-element evaluation framework, called BASE. The framework considers how a style supports changes to behavior, state, its execution context, and supports asynchrony of change. Styles considered include REST, event-based, service-oriented, and peer-to-peer.","Runtime,
Peer to peer computing,
Software systems,
Operating systems,
Context modeling,
Engines,
Probes,
Computer science,
Software engineering,
Security"
Most salient region tracking,"In this paper, we introduce a cognitive approach for object tracking from a mobile platform. The approach is based on a biologically motivated attention system which is able to detect regions of interest in images based on concepts of the human visual system. A top-down guided visual search module of the system enables to especially favor features which fit to a previously learned target object. Here, the appearance of an object is learned online within the first image in which it is detected. In subsequent images, the attention system searches for the target features and builds a top-down, target-related saliency map. This enables to focus on the most relevant features of especially this object in especially this scene without knowing anything about a particular object model or scene in advance. The system is able to operate in real-time and to cope with the requirements of real-world tasks such as illumination variations and other moving objects.","Radar tracking,
Real time systems,
Mobile robots,
Cameras,
Lighting,
Robot vision systems,
Target tracking,
Humans,
Object detection,
Layout"
Router microarchitecture and scalability of ring topology in on-chip networks,"On-chip networks are critical to the scaling of future multicore processors. Recent multicore processors have adopted ring topologies because of its simplicity and high bandwidth. In this paper, we first describe a bufferless router microarchitecture for an on-chip network ring topology. We propose to extend the bufferless router with an extra buffer entry to create a lightweight router microarchitecture. The proposed microarchitecture approaches ideal latency by reducing the microarchitecture complexity through minimizing the amount of buffers and simplifying switch allocation. We describe how the proposed lightweight microarchitecture does not need additional virtual channels to break routing deadlock. The scalability of the ring topology is presented as the network size increase. Although the ring topology has larger hop count with larger network diameter, lower per-hop router latency and no serialization latency results in lower latency for the ring topology compared to a 2D mesh topology. However, the wide channels in a ring topology creates bandwidth fragmentation which results in poor bandwidth utilization for short packets, compared to a 2D mesh topology, and can limit the scalability of the ring topology.","Microarchitecture,
Scalability,
Network topology,
Network-on-a-chip,
Delay,
Bandwidth,
Multicore processing,
Switches,
Routing,
System recovery"
Performance of Different Mobile Payment Service Concepts Compared with a NFC-Based Solution,"The paper compares the performance of different traditional mobile payment service concepts with a state of the art NFC-based mobile payment solution. The goal is to evaluate the different mobile payment concepts, not their software implementation, from a performance and end-to-end service duration time point of view. Overall, there have been five different mobile payment services developed, implemented and benchmarked for the concept comparison, namely: Interactive Voice Response, Short Message Service, Wireless Application Protocol, One Time Password Generator as well as a solution based on Near Field Communication.","Authentication,
Credit cards,
Mobile computing,
Wireless application protocol,
Message service,
Mobile communication,
Social network services,
Web and internet services,
Application software,
Computer science"
Large displacement optical flow computation withoutwarping,"We propose an algorithm for large displacement optical flow estimation which does not require the commonly used coarse-to-fine warping strategy. It is based on a quadratic relaxation of the optical flow functional which decouples data term and regularizer in such a way that the non-linearized variational problem can be solved by an alternation of two globally optimal steps, one imposing optimal data consistency, the other imposing discontinuity-preserving regularity of the flow field. Experimental results confirm that the proposed algorithmic implementation outperforms the traditional warping strategy, in particular for the case of large displacements of small scale structures.","Optical computing,
Image motion analysis,
Computer vision,
Motion estimation,
Biomedical optical imaging,
Computer science,
Computer applications,
Robustness,
Computer graphics,
Image reconstruction"
Control of a 9-DoF Wheelchair-mounted robotic arm system using a P300 Brain Computer Interface: Initial experiments,"A wheelchair-mounted robotic arm (WMRA) system was designed and built to meet the needs of mobilityimpaired persons with limitations of upper extremities, and to exceed the capabilities of current devices of this type. The control of this 9-degree-of-freedom system expands upon conventional control methods and combines the 7-DoF robotic arm control with the 2-degree-of-freedom power wheelchair control. The 3- degrees of redundancy are optimized to effectively perform activities of daily living and overcome singularities, joint limits and some workspace limitations. The control system is designed for teleoperated or autonomous coordinated Cartesian control, which offers expandability for future research. A P300 Brain Computer Interface (BCI), the BCI2000, was implemented to control the WMRA system. The control is done by recording and analysing the brain activity through an electrode cap while providing visual stimulation to the user via a visual matrix. The visual matrix contains a symbolic or an alphabetic array corresponding to the motion of the WMRA. By recognizing online and in real-time, which element in the matrix elicited a P300, the BCI system can identify which element the user chose to communicate. The chosen element is then communicated to the controller of the WMRA system. The speed and accuracy of the BCI system was tested. This paper gives details of the WMRA's integration with the BCI2000 and documents the experimental results of the BCI and the WMRA in simulation.","Control systems,
Mobile robots,
Brain computer interfaces,
Robot kinematics,
Extremities,
Robot control,
Wheelchairs,
Electrodes,
Real time systems,
System testing"
Architecture-driven self-adaptation and self-management in robotics systems,"We describe an architecture-centric design and implementation approach for building self-adapting and self-managing robotics systems. The basis of our approach is the concept of meta-level components, which facilitate adaptation and management of application-level components. Our approach applies two key enhancements to the traditional usage of meta-level components: (1) we utilize three distinct, specialized meta-level components for the three fundamental activities of a robotics system: sensing, computation, and control, and (2) we allow meta-level components to themselves be monitored, managed and adapted by other (higher layer) meta-level components. In this way, our approach flexibly supports adaptive layered architectures of arbitrary depth, the specification of arbitrary system adaptation policies, and the provision of intelligent facilities for constructing adaptation plans on-the-fly. We showcase our approach using a team of autonomous mobile robots that engage in a leader-follower scenario and experience a wide variety of failures, activating distinct recovery mechanisms.","Computer architecture,
Adaptive systems,
Monitoring,
Control systems,
Intelligent robots,
Mobile robots,
Software architecture,
Computer science,
Buildings,
Robot sensing systems"
Pipelined data parallel task mapping/scheduling technique for MPSoC,"In this paper, we propose a multi-task mapping/scheduling technique for heterogeneous and scalable MPSoC. To utilize the large number of cores embedded in MPSoC, the proposed technique considers temporal and data parallelisms as well as task parallelism. We define a multi-task mapping/scheduling problem with all these parallelisms and propose a QEA(quantum-inspired evolutionary algorithm)-based heuristic. Compared with an ILP (Integer Linear Programming) approach, experiments with real-life examples show the feasibility and the efficiency of the proposed technique.","Dynamic scheduling,
Adaptive scheduling,
Processor scheduling,
Runtime,
Timing,
Degradation,
Scheduling algorithm,
Availability,
Computer science,
Reliability engineering"
Fast and robust photomapping with an Unmanned Aerial Vehicle (UAV),"A fast and robust method for visual odometry based on the Fourier-Mellin Invariant (FMI) descriptor is presented. It extends previous FMI based approaches in two ways. First, a logarithmic representation of the spectral magnitude of the FMI descriptor is used. Second, a filter on the frequency where the shift is supposed to appear is applied. It is shown with experiments with an Unmanned Aerial Vehicle that this improved Fourier-Mellin Invariant (iFMI) method is is indeed an advancement and well suited for online visual odometry to generate large photo maps.","Unmanned aerial vehicles,
Robustness,
Frequency,
Matched filters,
Intelligent robots,
USA Councils,
Motion estimation,
Computer vision,
Pixel,
Karhunen-Loeve transforms"
A new wavelet based algorithm for estimating respiratory motion rate using UWB radar,"UWB signals have become attractive for their particular advantage of having narrow pulse width which makes them suitable for remote sensing of vital signals. In this paper a novel approach to estimate periodic motion rates, using Ultra Wide Band (UWB) signals is proposed. The proposed algorithm which is based on wavelet transform is used as a non-contact tool for measurement of respiration motion rate. Compared with traditional contact measurement devices, experimental results utilizing a 3.2 GHz bandwidth transceiver, demonstrate 99% similar results. The standard deviation of the proposed algorithm for 30 independent experiments has obtained 19% for respiration motion.","Motion estimation,
Ultra wideband radar,
Antenna measurements,
Signal processing algorithms,
Wavelet transforms,
Transceivers,
Ultra wideband antennas,
Patient monitoring,
Filter bank,
Bandwidth"
A Trojan-resistant system-on-chip bus architecture,"Communications systems are increasingly reliant on system-on-chip (SoC) solutions. As the complexity and size of SoCs continues to grow, so does the risk of ¿Trojan¿ attacks, in which an integrated circuit (IC) design is surreptitiously and maliciously altered at some point during the design or manufacturing process. Despite the risks that such an attack entail, relatively little attention has been given in the literature to methods enabling detection of and response to run-time Trojan attacks. In the present paper, we present a Trojan-resistant system bus architecture suitable across a wide range of SoC bus systems. The system detects malicious bus behaviors associated Trojan hardware, protects the system and system bus from them and reports the malicious behaviors to the CPU. We show that use of this bus and associated embedded software is highly effective in reducing IC Trojan vulnerabilities without loss of bus performance.","System-on-a-chip,
System buses,
Process design,
Manufacturing processes,
Runtime,
Computer architecture,
Hardware,
Protection,
Embedded software,
Performance loss"
Direct Reconstruction of Pharmacokinetic-Rate Images of Optical Fluorophores From NIR Measurements,"In this paper, we present a new method to form pharmacokinetic-rate images of optical fluorophores directly from near infra-red (NIR) boundary measurements. We first derive a mapping from spatially resolved pharmacokinetic rates to NIR boundary measurements by combining compartmental modeling with a diffusion based NIR photon propagation model. We express this mapping as a state-space equation. Next, we introduce a spatio-temporal prior model for the pharmacokinetic-rate images and combine it with the state-space equation. We address the image formation problem using the extended Kalman filtering framework. We analyze the computational complexity of the resulting algorithms and evaluate their performance in numerical simulations. An important feature of our approach is that the reconstruction of fluorescence concentrations and compartmental modeling are combined into a single step 1) to take advantage of the inherent temporal correlations in dynamic NIR measurements, and 2) to incorporate spatio-temporal a priori information on pharmacokinetic-rate images. Simulation results show that the resulting algorithms are more robust and lead to higher signal-to-noise ratio as compared to existing approaches where the reconstruction of concentrations and compartmental modeling are treated separately. Additionally, we reconstructed pharmacokinetic-rate images using in vivo data obtained from three patients with breast tumors. The reconstruction results show that the pharmacokinetic rates of indocyanine green are higher inside the tumor region as compared to the surrounding tissue.","Image reconstruction,
Optical filters,
Equations,
Infrared imaging,
Spatial resolution,
Optical propagation,
Kalman filters,
Filtering,
Performance analysis,
Algorithm design and analysis"
Petri nets and programming: A survey,"Petri nets and related models have been used for specification, analysis, and synthesis of programs. The paper contains a survey of several literature approaches and an examination of their relationship to Petri net modeling and supervisory control. The discussion is restricted to Petri net models in the class of place/transitions nets and the supervisory control of this class of models.","Petri nets,
Supervisory control,
Application software,
Vectors,
Computer science,
Control systems,
Discrete event systems,
Automata,
Sections,
Concurrent computing"
High bit rate ultrasonic communication through metal channels,"As low cost, low power wireless networking technologies continue to gain popularity in industrial control and remote sensing applications, greater demand is being placed on network reliability and robustness. The numerous metallic objects found in many industrial environments can make reliable RF coverage difficult to obtain. In cases where system components are physically isolated from one another by metallic barriers (e.g. bulkheads or storage tank walls), direct RF communication between components is not possible. Prior research into the use of ultrasonic signaling as a means of passing data across metallic barriers has proven successful, but it has been observed that acoustic echoing in the channel leads to significant intersymbol interference (ISI) when symbol rate approaches the hundred kilosymbol/second range. An echo cancelation technique was developed to partially suppress these echoes, but its performance was limited due to simplicity of the channel model used. In this paper, we develop a more accurate channel model and use it as the basis for constructing an improved echo cancelation pulse. The new pulse suppresses echoes to a level comparable to the RMS noise amplitude of the channel, greatly reducing ISI. The resulting transceiver is capable of transmitting data at over 5 Mbps using simple pulse amplitude modulation (PAM). This technique thus represents a data rate increase by a factor of five over prior work.","Bit rate,
Intersymbol interference,
Telecommunication network reliability,
Power system reliability,
Radio frequency,
Costs,
Wireless sensor networks,
Isolation technology,
Industrial control,
Remote sensing"
Year,,
Morphodynamic Analysis of Cerebral Aneurysm Pulsation From Time-Resolved Rotational Angiography,"This paper presents a technique to estimate and model patient-specific pulsatility of cerebral aneurysms over one cardiac cycle, using 3D rotational X-ray angiography (3DRA) acquisitions. Aneurysm pulsation is modeled as a time varying B-spline tensor field representing the deformation applied to a reference volume image, thus producing the instantaneous morphology at each time point in the cardiac cycle. The estimated deformation is obtained by matching multiple simulated projections of the deforming volume to their corresponding original projections. A weighting scheme is introduced to account for the relevance of each original projection for the selected time point. The wide coverage of the projections, together with the weighting scheme, ensures motion consistency in all directions. The technique has been tested on digital and physical phantoms that are realistic and clinically relevant in terms of geometry, pulsation and imaging conditions. Results from digital phantom experiments demonstrate that the proposed technique is able to recover subvoxel pulsation with an error lower than 10% of the maximum pulsation in most cases. The experiments with the physical phantom allowed demonstrating the feasibility of pulsation estimation as well as identifying different pulsation regions under clinical conditions.","Aneurysm,
Angiography,
Motion estimation,
Imaging phantoms,
Biomedical computing,
Computer networks,
Biomedical imaging,
Computational modeling,
Biomedical engineering,
Electronic mail"
A Practical Acceleration Algorithm for Real-Time Imaging,A practical acceleration algorithm for real-time magnetic resonance imaging (MRI) is presented. Neither separate training scans nor embedded training samples are used. The Kalman filter based algorithm provides a fast and causal reconstruction of dynamic MRI acquisitions with arbitrary readout trajectories. The algorithm is tested against abrupt changes in the imaging conditions and offline reconstructions of in vivo cardiac MRI experiments are presented.,"Acceleration,
Magnetic resonance imaging,
Image reconstruction,
Kalman filters,
Spatial resolution,
Pixel,
Testing,
In vivo,
Magnetic separation,
Filtering"
Spatiotemporal stereo via spatiotemporal quadric element (stequel) matching,"Spatiotemporal stereo is concerned with the recovery of the 3D structure of a dynamic scene from a temporal sequence of multiview images. This paper presents a novel method for computing temporally coherent disparity maps from a sequence of binocular images through an integrated consideration of image spacetime structure and without explicit recovery of motion. The approach is based on matching spatiotemporal quadric elements (stequels) between views, as it is shown that this matching primitive provides a natural way to encapsulate both local spatial and temporal structure for disparity estimation. Empirical evaluation with laboratory based imagery with ground truth and more typical natural imagery shows that the approach provides considerable benefit in comparison to alternative methods for enforcing temporal coherence in disparity estimation.","Spatiotemporal phenomena,
Stereo vision,
Layout,
Motion estimation,
Image motion analysis,
Optical filters,
Laboratories,
Pattern matching,
Spatial resolution,
Filter bank"
Joint calibration of multiple sensors,"Many calibration methods calibrate a pair of sensors at a time. For robotic systems with many sensors, they are often time-consuming to use, and can also lead to inaccurate results. In this paper, we combine a number of ideas in the literature to derive a unified framework that jointly calibrates many sensors a large system. Key to our approach are (i) grouping sensors to produce 3D data, thereby providing a unifying formalism that allows us to jointly calibrate all of the groups at the same, (ii) using a variety of geometric constraints to perform the calibration, and (iii) sharing sensors between groups to increase robustness. We show that this gives a simple method that is easily applicable to calibrating large systems. Our experiments show that this method not only reduces calibration error, but also requires less human time.","Calibration,
Robot sensing systems,
Sensor systems,
Cameras,
Intelligent sensors,
Robot vision systems,
Robustness,
Humans,
Optimization methods,
Intelligent robots"
A performance evaluation of scientific I/O workloads on Flash-based SSDs,"Flash-based solid state disks (SSDs) are an alternative form of storage device that promises to deliver higher performance than the traditional mechanically rotating hard drives. While SSDs have seen utilization in embedded, consumer, and server computer systems, there has been little understanding of its performance effects with scientific I/O workloads. This paper provides a trace driven performance evaluation of scientific I/O workloads on SSDs. We find that SSDs only provide modest performance gains over mechanical hard drives due to the write-intensive nature of many scientific workloads. Other workloads (like read-mostly web servers) would likely see much larger gains. Additionally, we observe that the concurrent I/O (when multiple parallel processes simultaneously access a single storage device) may significantly affect the SSD performance. However, such effects appear to be dependent on specific SSD implementation features and they are hard to predict in a general fashion. These results suggest that abundant cautions are needed when supporting high-performance scientific I/O workloads on Flash-based SSDs.","Solid state circuits,
Performance gain,
Network servers,
File systems,
Computer science,
Embedded computing,
Web server,
Delay,
Disk drives,
Power generation economics"
BarterCast: A practical approach to prevent lazy freeriding in P2P networks,"A well-known problem in P2P systems is freeriding, where users do not share content if there is no incentive to do so. In this paper, we distinguish lazy freeriders that are merely reluctant to share but follow the protocol, versus die-hard freeriders that employ sophisticated methods to subvert the protocol. Existing incentive designs often provide theoretically attractive resistance against die-hard freeriding, yet are rarely deployed in real networks because of practical infeasibility. Meanwhile, real communities benefit greatly from prevention of lazy freeriding, but have only centralized technology available to do so. We present a lightweight, fully distributed mechanism called BARTERCAST that prevents lazy freeriding and is deployed in practice. BarterCast uses a maxflow reputation algorithm based on a peer's private history of its data exchanges as well as indirect information received from other peers. We assess different reputation policies under realistic, trace-based community conditions and show that our mechanism is consistent and effective, even when significant fractions of peers spread false information. Furthermore, we present results of the deployment of BarterCast in the BitTorrent-based Tribler network which currently has thousands of users worldwide.","Protocols,
Intelligent networks,
Computer science,
History,
Peer to peer computing,
Proposals,
Privacy,
Particle measurements"
Design Rule Hierarchies and Parallelism in Software Development Tasks,"As software projects continue to grow in scale, being able to maximize the work that developers can carry out in parallel as a set of concurrent development tasks, without incurring excessive coordination overhead, becomes increasingly important. Prevailing design models, however, are not explicitly conceived to suggest how development tasks on the software modules they describe can be effectively parallelized. In this paper, we present a design rule hierarchy based on the assumption relations among design decisions. Software modules located within the same layer of the hierarchy suggest independent, hence parallelizable, tasks. Dependencies between layers or within a module suggest the need for coordination during concurrent work. We evaluate our approach by investigating the source code and mailing list of Apache Ant. We observe that technical communication between developers working on different modules within the same hierarchy layer, as predicted, is significantly less than communication between developers working across layers.","Programming,
Large-scale systems,
Parallel processing,
Project management,
Software engineering,
Software design,
Computer science,
USA Councils,
Context,
Predictive models"
Using clouds for metagenomics: A case study,"Cutting-edge sequencing systems produce data at a prodigious rate; and the analysis of these datasets requires significant computing resources. Cloud computing provides a tantalizing possibility for on-demand access to computing resources. However, many open questions remain. We present here a performance assessment of BLAST on real metagenomics data in a cloud setting in order to determine the viability of this approach. BLAST is one of the premier applications in bioinformatics and computational biology and is assumed to consume the vast majority of resources in that area.","Cloud computing,
DNA,
Bioinformatics,
Genomics,
Biology computing,
Sequences,
Organisms,
Data analysis,
Costs,
Laboratories"
Submodular Function Minimization under Covering Constraints,"This paper addresses the problems of minimizing nonnegative submodular functions under covering constraints, which generalize the vertex cover, edge cover, and set cover problems. We give approximation algorithms for these problems exploiting the discrete convexity of submodular functions. We first present a rounding 2-approximation algorithm for the submodular vertex cover problem based on the half-integrality of the continuous relaxation problem, and show that the rounding algorithm can be performed by one application of submodular function minimization on a ring family. We also show that a rounding algorithm and a primal-dual algorithm for the submodular cost set cover problem are both constant factor approximation algorithms if the maximum frequency is fixed. In addition, we give an essentially tight lower bound on the approximability of the submodular edge cover problem.",Computer science
Vehicle traffic congestion management in vehicular ad-hoc networks,"Vehicle traffic congestion is reflected as delays while traveling. Traffic congestion has a number of negative effects and is a major problem in today's society. Several techniques have been deployed to deal with this problem. In this paper, we have proposed an innovative approach to deal with the problem of traffic congestion using the characteristics of vehicular ad-hoc networks (VANET). We have used the Adaptive Proportional Integral rate controller, a congestion control algorithm designed for the Internet, to deal with the problem of vehicle traffic congestion in vehicular networks. The adaptive PI rate controller is a rate based controller that employs control theory to manage the problem of data traffic congestion in computer networks. Using simulations we have demonstrated the applicability of the algorithm in the domain of vehicle traffic congestion in a VANET.","Vehicles,
Telecommunication traffic,
Ad hoc networks,
Communication system traffic control,
Traffic control,
Programmable control,
Adaptive control,
Pi control,
Proportional control,
Computer network management"
Rapid method for finding faulty elements in antenna arrays using far field pattern samples,"A simple and fast technique that allows a diagnosis of faulty elements in antenna arrays, that only needs to consider a small number of samples of its degraded far-field pattern is described. The method tabulates patterns radiated by the array with 1 faulty element only. Then the pattern corresponding to the configuration of failed/unfailed elements under test is calculated using the error-free pattern and the patterns with 1 faulty element. The configuration with the lowest difference between the calculated and the degraded patterns is selected. Comparison of the performance of this method using an exhaustive search and a genetic algorithm for an equispaced linear array of 100 lambda/2-dipoles is shown. Mutual coupling as well as noise/measurement errors in the pattern samples was considered in the numerical analysis.","Antenna arrays,
Genetic algorithms,
Degradation,
Antenna radiation patterns,
Fault diagnosis,
Testing,
Radar antennas,
Planar arrays,
Physics,
Computer science"
A linear formulation of shape from specular flow,"When a curved mirror-like surface moves relative to its environment, it induces a motion field—or specular flow— on the image plane that observes it. This specular flow is related to the mirror's shape through a non-linear partial differential equation, and there is interest in understanding when and how this equation can be solved for surface shape. Existing analyses of this ‘shape from specular flow equation’ have focused on closed-form solutions, and while they have yielded insight, their critical reliance on externally-provided initial conditions and/or specific motions makes them difficult to apply in practice. This paper resolves these issues. We show that a suitable reparameterization leads to a linear formulation of the shape from specular flow equation. This formulation radically simplifies the reconstruction process and allows, for example, both motion and shape to be recovered from as few as two specular flows even when no externally-provided initial conditions are available. Our analysis moves us closer to a practical method for recovering shape from specular flow that operates under arbitrary, unknown motions in unknown illumination environments and does not require additional shape information from other sources.","Shape,
Partial differential equations,
Nonlinear equations,
Motion analysis,
Differential equations,
Image motion analysis,
Closed-form solution,
Image reconstruction,
Information analysis,
Lighting"
Input Classes for Identifiability of Bilinear Systems,"This paper asks what classes of input signals are sufficient in order to completely identify the input/output behavior of generic bilinear systems. The main results are that step inputs are not sufficient, nor are single pulses, but the family of all pulses (of a fixed amplitude but varying widths) do suffice for identification.","Nonlinear systems,
Linear systems,
Signal processing,
Space vector pulse width modulation,
Cells (biology),
Steady-state,
Drugs,
Testing,
Mathematics,
Computer science"
On properties of the widely linear MSE filter and its LMS implementation,"Widely linear filters have been receiving much attention lately and have been proposed for many signal processing applications where the traditional circularity assumptions on the complex data do not hold. In this paper, we study the properties of the mean-square-error (MSE) widely linear filter and its least mean squares (LMS) adaptive implementation. We show that in certain cases, widely linear filter does not provide any additional advantage compared to the linear filter even with highly noncircular data. On the other hand, we show examples of cases where it can lead to important performance gains even when the input is circular. We also show that its performance can slow down significantly with highly noncircular inputs when it is implemented using an LMS type gradient descent algorithm thus making recursive least squares type adaptive implementations more desirable for an adaptive widely linear filter.","Nonlinear filters,
Least squares approximation,
Adaptive filters,
Signal processing algorithms,
Signal processing,
Adaptive signal processing,
Magnetic separation,
Convergence,
Computer science,
Application software"
A Necessary and Sufficient Timing Assumption for Speed-Independent Circuits,"This paper presents a proof that the adversary path timing assumption is both necessary and sufficient for correct SI circuit operation. This assumption requires that the delay of a wire on one branch of a fork be less than the delay through a gate sequence beginning at another branch in the same fork. Both the definition of the timing assumption and the proof build on a general, formal notion of computation given with respect to production rule sets. This underlying framework can be used for a variety of proof efforts or as a basis for defining other useful notions involving asynchronous computation.","Timing,
Delay,
Asynchronous circuits,
Production,
Computer science,
USA Councils,
Wire,
Hazards,
Switching circuits,
Design methodology"
"A 0.8 V, 2.6 mW, 88 dB Dual-Channel Audio Delta-Sigma D/A Converter With Headphone Driver","A 0.8 V third-order multibit DeltaSigma DAC with headphone driver is described. It is the first sub-1 V audio DeltaSigma DAC with integrated on-chip headphone driver. The dual-channel operation in digital section was efficiently realized with a time-interleaved single-channel hardware and extra registers. Using novel DAC architecture suitable for low-voltage operation, the analog section requires only one opamp per channel for the D/A conversion, low-pass filtering, and driving the headphone. Two prototype ICs (separate digital and analog chips), implemented in a 0.35 mum CMOS process, achieved an 88 dB dynamic range, while consuming 2.6 mW from a 0.8 V supply.","Headphones,
Noise reduction,
1f noise,
Voltage,
Switches,
Low pass filters,
Digital filters,
Power supplies,
Computer science,
Signal to noise ratio"
HERO: Online Real-Time Vehicle Tracking,"Intelligent transportation systems have become increasingly important for the public transportation in Shanghai. In response, ShanghaiGrid (SG) project aims to provide abundant intelligent transportation services to improve the traffic condition. A challenging service in SG is to accurately locate the positions of moving vehicles in real time. In this paper, we present an innovative scheme, hierarchical exponential region organization (HERO), to tackle this problem. In SG, the location information of individual vehicles is actively logged in local nodes which are distributed throughout the city. For each vehicle, HERO dynamically maintains an advantageous hierarchy on the overlay network of local nodes to conservatively update the location information only in nearby nodes. By bounding the maximum number of hops the query is routed, HERO guarantees to meet the real-time constraint associated with each vehicle. A small-scale prototype system implementation and extensive simulations based on the real road network and trace data of vehicle movements from Shanghai demonstrate the efficacy of HERO.",
Fast 3D mapping by matching planes extracted from range sensor point-clouds,"This article addresses fast 3D mapping by a mobile robot in a predominantly planar environment. It is based on a novel pose registration algorithm based entirely on matching features composed of plane-segments extracted from point-clouds sampled from a 3D sensor. The approach has advantages in terms of robustness, speed and storage as compared to the voxel based approaches. Unlike previous approaches, the uncertainty in plane parameters is utilized to compute the uncertainty in the pose computed by scan-registration. The algorithm is illustrated by creating a full 3D model of a multi-level robot testing arena.",
The salvage cache: A fault-tolerant cache architecture for next-generation memory technologies,"There has been much work on the next generation of memory technologies such as MRAM, RRAM and PRAM. Most of these are non-volatile in nature, and compared to SRAM, they are often denser, just as fast, and have much lower energy consumption. Using 3-D stacking technology, it has been proposed that they can be used instead of SRAM in large level 2 caches prevalent in today's microprocessors. However, one of the key challenges in the use of these technologies, such as MRAM, is their higher fault probabilities arising from the larger process variation, defects in its fabrication, and the fact that the cache is much larger. This seriously affect yield. In this paper, we propose a fault resilient set associative cache architecture which we called the salvage cache. In the salvage cache, a faulty cache block is sacrificed and used to repair faults found in other blocks. We will describe in detail the architecture of the salvage cache as well as provide results of yield simulations that show that a much higher yield can be achieved viz-a-viz other fault tolerant techniques. We will also show the performance savings that arise from the use of a large next-generation L2 cache.","Fault tolerance,
Memory architecture,
Random access memory,
Read-write memory,
Phase change random access memory,
Nonvolatile memory,
Energy consumption,
Stacking,
Magnetoresistance,
Computer architecture"
On the effectiveness of structural detection and defense against P2P-based botnets,"Recently, peer-to-peer (P2P) networks have emerged as a covert communication platform for malicious programs known as bots. As popular distributed systems, they allow bots to communicate easily while protecting the botmaster from being discovered. Existing work on P2P-based botnets mainly focuses on measurement-based studies of botnet behaviors. In this work, through simulation, we study extensively the structure of P2P networks running Kademlia, one of a few widely used P2P protocols in practice. Our simulation testbed not only incorporates the actual code of a real Kademlia client software to achieve high realism, but also applies distributed event-driven simulation techniques to achieve high scalability. Using this testbed, we analyze the scaling, clustering, reachability, and various centrality properties of P2P-based botnets from a graph-theoretical perspective. We further demonstrate experimentally and theoretically that monitoring bot activities in a P2P network is difficult, suggesting that the P2P mechanism indeed helps botnets hide their communication effectively. Finally, we evaluate the effectiveness of some potential mitigation techniques, such as content poisoning, sybil-based and eclipse-based mitigation. Conclusions drawn from this work shed light on the structure of P2P botnets, how to monitor bot activities in P2P networks, and how to mitigate botnet operations effectively.","Bluetooth,
Computer worms,
Cellular phones,
Mathematical model,
Laboratories,
Personal digital assistants,
Sequential analysis,
IP networks,
Statistical analysis,
Information security"
A robust video based traffic light detection algorithm for intelligent vehicles,"Recently, researches on intelligent vehicles which can drive in urban environment autonomously become more popular. Traffic lights are common in cities and are important cues for the path planning of intelligent vehicles. In this paper, a robust and efficient algorithm to detect traffic lights based on video sequences captured by a low cost off-the-shelf video camera is proposed. The algorithm models the hue and saturation according to Gaussian distributions and learns their parameters with training images. From learned models, candidate regions of the traffic lights in the test images can be extracted. Post processing method which takes account of the shape information is applied to the candidate regions. Furthermore, detection results of the previous image frames are aggregated in order to provide a more robust result. Experimental results on several video sequences captured in typical urban environment prove the effectiveness of the proposed algorithm.","Robustness,
Detection algorithms,
Intelligent vehicles,
Traffic control,
Video sequences,
Cities and towns,
Path planning,
Costs,
Cameras,
Gaussian distribution"
Software Clustering Using Dynamic Analysis and Static Dependencies,"Decomposing a software system into smaller, more manageable clusters is a common approach to support the comprehension of large systems. In recent years, researchers have focused on clustering techniques to perform such architectural decomposition, with the most predominant clustering techniques relying on the static analysis of source code. We argue that these static structural relationships are not sufficient for software clustering due to the increased complexity and behavioral aspects found in software systems. In this paper, we present a novel software clustering approach that combines dynamic and static analysis to identify component clusters. We introduce a two-phase clustering technique that combines software features to build a core skeleton decomposition with structural information to further refine these clusters. A case study is presented to evaluate the applicability and effectiveness of our approach.","Software maintenance,
Skeleton,
Data mining,
Software systems,
Computer science,
Software engineering,
Conference management,
Engineering management,
Performance analysis,
Computer architecture"
Community Detection in Large-Scale Bipartite Networks,"Community detection in networks receives much attention recently. Most of the previous works are for unipartite networks composed of only one type of nodes. In real world situations, however, there are many bipartite networks composed of two types of nodes. In this paper, we propose a fast algorithm called LP&BRIM for community detection in large-scale bipartite networks. It is based on a joint strategy of two developed algorithms -- label propagation (LP), a very fast community detection algorithm, and BRIM, an algorithm for generating better community structure by recursively inducing divisions between the two types of nodes in bipartite networks. Through experiments, we demonstrate that this new algorithm successfully finds meaningful community structures in large-scale bipartite networks in reasonable time limit.","Large-scale systems,
Intelligent agent,
Intelligent networks,
Detection algorithms,
Conferences,
Computer science,
Information science,
Electronic mail,
Complex networks,
Particle measurements"
HiRA: A methodology for deadlock free routing in hierarchical networks on chip,"Complexity of designing large and complex NoCs can be reduced/managed by using the concept of hierarchical networks. In this paper, we propose a methodology for design of deadlock free routing algorithms for hierarchical networks, by combining routing algorithms of component subnets. Specifically, our methodology ensures reachability and deadlock freedom for the complete network if routing algorithms for subnets are deadlock free. We evaluate and compare the performance of hierarchical routing algorithms designed using our methodology with routing algorithms for corresponding flat networks. We show that hierarchical routing, combining best routing algorithm for each subnet, has a potential for providing better performance than using any single routing algorithm. This is observed for both synthetic as well as traffic from real applications. We also demonstrate, by measuring jitter in throughput, that hierarchical routing algorithms leads to smoother flow of network traffic. A router architecture that supports scalable table-based routing is briefly outlined.","System recovery,
Routing,
Network-on-a-chip,
Algorithm design and analysis,
Scalability,
Network topology,
Computer networks,
Design engineering,
Telecommunication computing,
Design methodology"
Optimal Fast Hashing,"This paper is about designing optimal high-throughput hashing schemes that minimize the total number of memory accesses needed to build and access an hash table. Recent schemes often promote the use of multiple-choice hashing. However, such a choice also implies a significant increase in the number of memory accesses to the hash table, which translates into higher power consumption and lower throughput. In this paper, we propose to only use choice when needed. Given some target hash table overflow rate, we provide a lower bound on the total number of needed memory accesses. Then, we design and analyze schemes that provably achieve this lower bound over a large range of target overflow values. Further, for the multilevel hash table scheme, we prove that the optimum occurs when its sub table sizes decrease in a geometric way, thus formally confirming a heuristic rule-of-thumb.","Energy consumption,
Throughput,
High-speed networks,
Communications Society,
Computer science,
Counting circuits,
Data structures,
Computer aided manufacturing,
CADCAM,
Random access memory"
A Review of Digital Watermarking Techniques for Text Documents,"Copyright protection of plain text while traveling over the internet is very crucial. Digital watermarking provides the complete copyright protection solution for this problem. Text being the most dominant medium travelling over the internet needs absolute protection. Text watermarking techniques have been developed in past to protect the text from illegal copying, redistribution and to prevent copyright violations. This paper presents a review of some of the recent research in watermarking techniques for plain text documents. The reviewed approaches are classified into three categories, the image based approach, the syntactic approach and the semantic approach. This paper discusses the main contributions, advantages and drawbacks of different methods used for text watermarking in past.","Watermarking,
Copyright protection,
Internet,
Cryptography,
Steganography,
Computer science,
Information security,
Intellectual property,
Data mining,
Law"
Stereo vision and terrain modeling for quadruped robots,"Legged robots offer the potential to navigate highly challenging terrain, and there has recently been much progress in this area. However, a great deal of this recent work has operated under the assumption that either the robot has complete knowledge of its environment or that its environment is suitably regular so as to be navigated with only minimal perception, an unrealistic assumption in many real-world domains. In this paper we present an integrated perception and control system for a quadruped robot that allows it to perceive and traverse previously unseen, rugged terrain that includes large, irregular obstacles. A key element of the system is a novel terrain modeling algorithm, used for filling in the occluded models resulting from on-board vision systems. We apply our approach to the LittleDog robot, and show that it allows the robot to walk over challenging terrain using only on-board perception.","Stereo vision,
Robot vision systems,
Legged locomotion,
Mobile robots,
Robot sensing systems,
Navigation,
Robotics and automation,
Control systems,
Vehicle dynamics,
Computer vision"
Centralized PSM: An AP-centric power saving Mode for 802.11 infrastructure networks,"Energy management in a wireless LAN is an important problem, as the viability of wireless devices depends very much on their battery life. In this paper, we propose a centralized power saving mode (C-PSM), an AP-centric PSM for 802.11 infrastructure networks. Having the AP select optimal PSM parameters, such as the beacon and listen intervals, CPSM is able to maximize the total energy efficiency for all clients. Moreover, C-PSM provides a first-wake-up schedule to further increase the energy efficiency by reducing clients' simultaneous wake-ups. Extensive simulation experiments show that C-PSM outperforms the standard PSM by a very significant margin. In our set of experiments, C-PSM reduces power consumption and increases energy efficiency by as much as 76% and 320%, respectively. As a side benefit, C-PSM also decreases the frame buffering delay at the AP by 88%. The wake-up schedule can save clients' energy consumption by 22% at most. Moreover, the improvement increases with the number of clients.","Energy efficiency,
Energy consumption,
Bismuth,
Computer networks,
Wireless LAN,
Telecommunication traffic,
Protocols,
Computer science,
Battery management systems,
Computer network management"
Trust Inference in Complex Trust-Oriented Social Networks,"Many social networking platforms have emerged on theWeb, such as MySpace and Facebook. In those networks,most participants are usually physically unknown in reallife and have no prior direct interactions with each other.Hence, it is necessary to form complex trust-oriented socialnetworks with more trust related information and infertrust values between participants. The inferred trust resultsprovide important indications for some applications, suchas introducing products to trustworthy people or recruitingtrustworthy employees from social networks. Traditionaltrust inference mechanisms are based on simple trust networksand can hardly deliver realistic trust results. In thispaper, we first propose a complex trust-oriented social networkstructure containing trust values, social relations andrecommendation roles. We also propose a novel Bayesiannetwork based trust inference mechanism taking trust values,social relations and recommendation roles into account.Experiments demonstrate that our proposed trust inferencemechanism performs well in complex trust-oriented socialnetworks.","Social network services,
Inference mechanisms,
Computer networks,
MySpace,
Facebook,
Recruitment,
Manufacturing,
Psychology,
Computer science,
Bayesian methods"
Evaluating AAM fitting methods for facial expression recognition,"The human face is a rich source of information for the viewer and facial expressions are a major component in judging a person's affective state, intention and personality. Facial expressions are an important part of human-human interaction and have the potential to play an equally important part in human-computer interaction. This paper evaluates various Active Appearance Model (AAM) fitting methods, including both the original formulation as well as several state-of-the-art methods, for the task of automatic facial expression recognition. The AAM is a powerful statistical model for modelling and registering deformable objects. The results of the fitting process are used in a facial expression recognition task using a region-based intermediate representation related to Action Units, with the expression classification task realised using a Support Vector Machine. Experiments are performed for both person-dependent and person-independent setups. Overall, the best facial expression recognition results were obtained by using the Iterative Error Bound Minimisation method, which consistently resulted in accurate face model alignment and facial expression recognition even when the initial face detection used to initialise the fitting procedure was poor.","Active appearance model,
Face recognition,
Deformable models,
Face detection,
Humans,
Information resources,
Fitting,
Support vector machines,
Support vector machine classification,
Iterative methods"
An analytical design of Fractional Order Proportional Integral and [Proportional Integral] controllers for robust velocity servo,"In this paper, two tuning methods of Fractional Order Proportional Integral (FOPI) controller and Fractional Order [Proportional Integral] (FO[PI]) controller for the typical first-order velocity servo system are discussed. Using the Integer Order Proportional, Integral and Derivative (IOPID) controller, the controller can not be designed to follow the proposed tuning idea to achieve the robustness requirement. However, the FOPI and FO[PI] controllers designed by the proposed tuning methods can improve the performance and robustness of the first order velocity servo system. Furthermore, following the proposed systematic and analytical design schemes, the FO[PI] controller outperforms the FOPI controller. Simulation results are presented to validate the proposed tuning methods.","Velocity control,
Pi control,
Proportional control,
Robust control,
Servomechanisms,
Control systems,
Motion control,
Frequency,
Transfer functions,
PD control"
A QoS-Driven Approach for Semantic Service Composition,"Semantic information, which is well-regulated and easy to be retrieved, has greatly enriched the expressive ability of the Web. These advantages can be applied in Web Services to meet the increasing complexity of Web applications. In this paper, we propose a service composition approach. It combines the large-scaled Web Services and semantic information which is described in WSC’09. Besides, QoS has become a critical issue to evaluate the performance of Web applications. Being different from improving the QoS of single services, our approach focuses on the overall QoS of the service composition. The algorithm shows that the semantic information based and QoS driven approach improves the efficiency and QoS performance of service composition.","Web services,
Quality of service,
XML,
Information retrieval,
Semantic Web,
OWL,
Interconnected systems,
Delay,
Business,
Computer science"
New Techniques for Improving the Performance of the Lockstep Architecture for SEEs Mitigation in FPGA Embedded Processors,"The growing availability of embedded processors inside FPGAs provides unprecedented flexibility for system designers. The use of such devices for space or mission critical applications, however, is being delayed by the lack of effective low cost techniques to mitigate radiation induced errors. In this paper a non invasive approach for the implementation of fault tolerant systems based on COTS processors embedded in FPGAs, using lockstep in conjunction with checkpoint and rollback recovery, is presented. The proposed approach does not require modifications in the processor architecture or in the application software. The experimental validation of this approach through fault injection is described, the corresponding results are discussed, and the addition of a write history table as a means to reduce the performance overhead imposed by previous implementations is proposed and evaluated.","Field programmable gate arrays,
Availability,
Space missions,
Mission critical systems,
Delay effects,
Costs,
Fault tolerant systems,
Computer architecture,
Application software,
History"
Simulation Algorithm for Energy-Efficient Train Control under Moving Block System,"Energy consumption is an important part among railway operational costs. This paper explores the simulation problem on energy-efficient multi-train control under Moving Block System. The model for energy-efficient multi-train control is founded according to the operation rules and habits. The way to calculate the train’s time interval under Moving Block System is introduced. The simulation model suits for the locomotives with discrete levels of control on uneven rail. It can clearly describe the impact on the following train by other trains. Based on energy-efficient control strategy, new algorithm is discussed about how to determine the control modes and handles of the following train for better headway. At last, it gives some simulation results to show that the methods of energy-efficient train control can avoid unnecessary braking and reduce the energy consumption¿","Energy efficiency,
Rail transportation,
Electric variables control,
Energy consumption,
Costs,
Communication system signaling,
Communication system control,
Computer science,
Railway engineering,
Power engineering and energy"
Integrating facial expressions into user profiling for the improvement of a multimodal recommender system,"Over the years, recommender systems have been systematically applied in both industry and academia to assist users in dealing with information overload. One of the factors that determine the performance of a recommender system is user feedback, which has been traditionally communicated through the application of explicit and implicit feedback techniques. In this paper, we propose a novel video search interface that predicts the topical relevance of a video by analysing affective aspects of user behaviour. We, furthermore, present a method for incorporating such affective features into user profiling, to facilitate the generation of meaningful recommendations, of unseen videos. Our experiment shows that multimodal interaction feature is a promising way to improve the performance of recommendation.","Recommender systems,
Feedback,
Aggregates,
Support vector machines,
Support vector machine classification,
Face recognition,
Emotion recognition,
Computer industry,
Books,
Motion pictures"
A high-performance hardwired CABAC decoder for ultra-high resolution video,"Context-Based Binary Arithmetic Coding (CABAC) is one of two entropy coders used in H.264/AVC, which achieves a high compression ratio at the expense of high computational complexity. For real-time decoding of ultra-high resolution video, we propose a high-throughput hardwired CABAC decoder subsystem. By analyzing the distribution of different types of syntax elements (SE), we propose a Two-Bin arithmetic decoding engine (Two-Bin AE) to generate two bins in one cycle for the most frequent SEs. In order to boost the utilization of the proposed engine, we employ a Prediction-Based Parallel Processing Method to perform decoding and context index calculation in parallel. Furthermore, we propose a Context Table Reallocation Scheme, which can shorten the critical path delay of a Two- Bin AE circuit by 18%. Experimental results show that our decoder on average takes only 118 clock cycles to decode one macroblock (MB) at Main Profile, Level 4.0. For QFHD, i.e., 4x1080 HD, sequences, it can run effectively at 110 MHz. The design has been successfully integrated into an H.264/AVC QFHD video decoder in an SOC system.","Decoding,
Video compression,
Arithmetic,
Automatic voltage control,
Engines,
Entropy,
Computational complexity,
Parallel processing,
Delay,
Circuits"
Bounded Dataflow Networks and Latency-Insensitive circuits,"We present a theory for modular refinement of Synchronous Sequential Circuits (SSMs) using Bounded Dataflow Networks (BDNs). We provide a procedure for implementing any SSM into an LI-BDN, a special class of BDNs with some good compositional properties. We show that the Latency-Insensitive property of LI-BDNs is preserved under parallel and iterative composition of LI-BDNs. Our theory permits one to make arbitrary cuts in an SSM and turn each of the parts into LI-BDNs without affecting the overall functionality. We can further refine each constituent LI-BDN into another LI-BDN which may take different number of cycles to compute. If the constituent LI-BDN is refined correctly we guarantee that the overall behavior would be cycle-accurate with respect to the original SSM. Thus one can replace, say a 3-ported register file in an SSM by a one-ported register file without affecting the correctness of the SSM. We give several examples to show how our theory supports a generalization of previous techniques for Latency-Insensitive refinements of SSMs.","Field programmable gate arrays,
Timing,
Delay,
Clocks,
Wire,
Sequential circuits,
Registers,
Computer networks,
Computer science,
Artificial intelligence"
Local Graph Partitions for Approximation and Testing,"We introduce a new tool for approximation and testing algorithms called partitioning oracles. We develop methods for constructing them for any class of bounded-degree graphs with an excluded minor, and in general, for any hyperfinite class of bounded-degree graphs. These oracles utilize only local computation to consistently answer queries about a global partition that breaks the graph into small connected components by removing only a small fraction of the edges. We illustrate the power of this technique by using it to extend and simplify a number of previous approximation and testing results for sparse graphs, as well as to provide new results that were unachievable with existing techniques. For instance:1. We give constant-time approximation algorithms for the size of the minimum vertex cover, the minimum dominating set, and the maximum independent set for any class of graphs with an excluded minor.2. We show a simple proof that any minor-closed graph property is testable in constant time in the bounded degree model.3. We prove that it is possible to approximate the distance to almost any hereditary property in any bounded degree hereditary families of graphs. Hereditary properties of interest include bipartiteness, k-colorability, and perfectness.",
Automated detection of drusen in the macula,"Age related macular degeneration (AMD) is a condition of the retina that occurs with individuals over 50. AMD is characterized by the formation of drusen in the macula. This condition leads to a deterioration of foveal vision and eventually functional blindness. Automatically screening atrisk individuals may allow the detection of intermediate stage AMD where it is still treatable using anti-VEGH therapy. One of the difficulties in detecting and locating drusen is that their aspect (shape, texture, color, extent) varies significantly, and because of this it is often difficult to build a classifier. To address this difficulty we use a two pronged approach based on (a) multiscale analysis and (b) kernel based anomaly detection. We show experimental results on examples of fundus images taken from healthy and affected patients.","Retina,
Medical treatment,
Biomedical imaging,
Blindness,
Humans,
Physics,
Laboratories,
Computer science,
Shape,
Kernel"
Degrees of freedom of multi-source relay networks,"We study a multi-source Gaussian relay network consisting of K source-destination pairs having K unicast sessions. We assume M layers of relays between the sources and the destinations. We find achievable degrees of freedom of the network. Our schemes are based on interference alignment at the transmitters and symbol extension and opportunistic interference cancellation at the relays. For K-L-K networks, i.e., 2-hop network with L relays, we show min{K,K/2 + L/(2(K − 1))} degrees of freedom are achievable. For K-hop networks with K relays in each layer, we show the full K degrees of freedom are achievable provided that K is even and the channel distribution satisfies a certain symmetry.","Relays,
Interference cancellation,
Interference channels,
Signal to noise ratio,
MIMO,
Computer science,
Unicast,
Transmitters,
Information theory,
Computer networks"
Distance Oracles for Sparse Graphs,"Thorup and Zwick, in their seminal work, introduced the approximate distance oracle, which is a data structure that answers distance queries in a graph. For any integer k, they showed an efficient algorithm to construct an approximate distance oracle using space O(kn^{1+1/k}) that can answer queries in time O(k) with a distance estimate that is at most 2k-1 times larger than the actual shortest distance (this ratio is called the stretch).They proved that, under a combinatorial conjecture, their data structure is optimal in terms of space: if a stretch of at most 2k-1 is desired, then the space complexity is at least n^{1+1/k}. Their proof holds even if infinite query time is allowed: it is essentially an ""incompressibility"" result. Also, the proof only holds for dense graphs, and the best bound it can prove only implies that the size of the data structure is lower bounded by the number of edges of the graph. Naturally, the following question arises: what happens for sparse graphs? In this paper we give a new lower bound for approximate distance oracles in the cell-probe model. This lower bound holds even for sparse (polylog(n)-degree) graphs, and it is not an ""incompressibility"" bound: we prove a three-way tradeoff between space, stretch, and query time. We show that when the query time is t and the stretch is a, then the space S must be at least n^{1+1/Omega(a*t)} / lg n. This lower bound follows by a reduction from lopsided set disjointness to distance oracles, based on and motivated by recent work of Patrascu. Our results in fact show that for any high-girth regular graph, an approximate distance oracle that supports efficient queries for all subgraphs of G must obey this tradeoff. We also prove some lemmas that count sets of paths in high-girth regular graphs and high-girth regular expanders, which might be of independent interest.",
Edge Anonymity in Social Network Graphs,"Edges in social network graphs may represent sensitive relationships. In this paper, we consider the problem of edges anonymity in graphs. We propose a probabilistic notion of edge anonymity, called graph confidence, which is general enough to capture the privacy breach made by an adversary who can pinpoint target persons in a graph partition based on any given set of topological features of vertexes. We consider a special type of edge anonymity problem which uses vertex degree to partition a graph. We analyze edge disclosure in real-world social networks and show that although some graphs can preserve vertex anonymity, they may still not preserve edge anonymity. We present three heuristic algorithms that protect edge anonymity using edge swap or edge deletion. Our experimental results, based on three real-world social networks and several utility measures, show that these algorithms can effectively preserve edge anonymity yet obtain anonymous graphs of acceptable utility.","Social network services,
Privacy,
Algorithm design and analysis,
Computer networks,
Computer science,
Heuristic algorithms,
Partitioning algorithms,
Protection,
Collaboration,
Filtering"
Discovering the best web service: A neural network-based solution,"Differentiating between Web services that share similar functionalities is becoming a major challenge into the discovery of Web services. In this paper we propose a framework for enabling the efficient discovery of Web services using Artificial Neural Networks (ANN) best known for their generalization capabilities. The core of this framework is applying a novel neural network model to Web services to determine suitable Web services based on the notion of the Quality of Web Service (QWS). The main concept of QWS is to assess a Web service's behaviour and ability to deliver the requested functionality. Through the aggregation of QWS for Web services, the neural network is capable of identifying those services that belong to a variety of class objects. The overall performance of the proposed method shows a 95% success rate for discovering Web services of interest. To test the robustness and effectiveness of the neural network algorithm, some of the QWS features were excluded from the training set and results showed a significant impact in the overall performance of the system. Hence, discovering Web services through a wide selection of quality attributes can considerably be influenced with the selection of QWS features used to provide an overall assessment of Web services.","Web services,
Neural networks,
Quality of service,
Artificial neural networks,
Service oriented architecture,
Monitoring,
Cybernetics,
USA Councils,
Computer networks,
Information science"
Multi-objective Optimization of Graph Partitioning Using Genetic Algorithms,"Graph partitioning is a NP-hard problem with multiple conflicting objectives. The graph partitioning should minimize the inter-partition relationship while maximizing the intra-partition relationship. Furthermore, the partition load should be evenly distributed over the respective partitions. Therefore this is a multi-objective optimization problem. There are two approaches to multi-objective optimization using genetic algorithms: weighted cost functions and finding the Pareto front. We have used the Pareto front method to find the suitable curve of non-dominated solutions, composed of a high number of solutions. The proposed methods of this paper used to improve the performance are injecting best solutions of previous runs into the first generation of next runs and also storing the non-dominated set of previous generations to combine with later generation's non-dominated set. These improvements prevent the GA from getting stuck in the local optima and make the search more efficient and increase the probability of finding more optimal solutions. Finally, a simulation research is carried out to investigate the effectiveness of the proposed algorithm. The simulation results confirm the effectiveness of the proposed multi-objective GA method.","Genetic algorithms,
Pareto optimization,
Cost function,
Computational modeling,
Optimization methods,
Genetic engineering,
Computer applications,
Application software,
NP-hard problem,
Partitioning algorithms"
Grouping-Based Fine-Grained Job Scheduling in Grid Computing,"Grid computing is the technology for building Internet-wide computing environment in which distributed and heterogeneous resources are integrated. However, in Grid environment, job scheduling is confronted with a great challenge. This paper focuses on lightweight jobs scheduling in Grid Computing. An Adaptive Fine-grained Job Scheduling (AFJS) algorithm is proposed. Compared with other fine-grained job scheduling algorithms based on grouping, AFJS can outperform them according to the experimental results.","Processor scheduling,
Grid computing,
Scheduling algorithm,
Distributed computing,
Resource management,
Educational technology,
Adaptive scheduling,
Computer networks,
Analytical models,
Computer science education"
Basis function adaptation methods for cost approximation in MDP,"We generalize a basis adaptation method for cost approximation in Markov decision processes (MDP), extending earlier work of Menache, Mannor, and Shimkin. In our context, basis functions are parametrized and their parameters are tuned by minimizing an objective function involving the cost function approximation obtained when a temporal differences (TD) or other method is used. The adaptation scheme involves only low order calculations and can be implemented in a way analogous to policy gradient methods. In the generalized basis adaptation framework we provide extensions to TD methods for nonlinear optimal stopping problems and to alternative cost approximations beyond those based on TD.","Cost function,
Function approximation,
Optimization methods,
Differential equations,
Vectors,
Computer science,
Laboratories,
Gradient methods,
Design optimization,
Process design"
Comparison of decentralized time slot allocation strategies for asymmetric traffic in TDD systems,"Recently, wireless multimedia services have been growing because of the spread of various wireless applications. Hence, time division duplex (TDD) systems and their crossed-slot interference problems in a cellular environment have been attracting growing interests. Considering large signaling overhead between cells, decentralized time slot allocation (TSA) strategy is suitable for practical implementation. Thus, Fixed-TSA strategy which fixes the same ratio of uplink and downlink time slots in all cells is adopted in commercial WiBro systems (IEEE802.16e Mobile WiMax) to mitigate the crossed-slot interference. However, the Fixed-TSA strategy reduces the flexibility in time slot allocation due to a strong constraint set by a predefined boundary. Moreover, the mathematical derivation of the optimal value for the predefined boundary has not been presented yet. In this paper, we propose two decentralized TSA strategies. The first one is Enhanced Fixed-TSA strategy, which dynamically adapts the predefined boundary of the conventional Fixed-TSA strategy according to traffic conditions. The second one is region-based Decentralized Time Slot Allocation (RED-TSA) strategy, which utilizes partial location information of MSs to reduce crossedslot interference. We fully analyze the proposed TSA strategies in terms of new call blocking probability, average bit error probability, and the overall system throughput. Numerical results show that the proposed RED-TSA strategy provides the highest system throughput by compromising both new call blocking performance and average bit error performance, whereas it requires additional location information and complicated computation. On the contrary, the proposed Enhanced Fixed-TSA strategy requires reasonable computational complexity while it provides almost the same system throughput with that of the proposed RED-TSA strategy. In a nutshell, the proposed Enhanced Fixed- TSA strategy is more appropriate for the practical systems. Moreover, it can be directly applied to the current commercial WiMax/WiBro systems with minimum changes.","Downlink,
WiMAX,
Throughput,
Wireless communication,
Computer science,
Bandwidth,
Communication standards,
Interference constraints,
Error probability,
High performance computing"
RAS-M: Resource Allocation Strategy Based on Market Mechanism in Cloud Computing,"Resource management is one of the main issues in Cloud Computing. In order to improve resource utilization of large Data Centers while delivering services with higher QoS to Cloud Clients, a resource allocation strategy based on market (RAS-M) is proposed. Firstly, the architecture and the market model of RAS-M are constructed, in which a QoS-refection utility function is designed according to different resource requirements of the Cloud Client, the equilibrium state of RAS-M is defined and the proof of its optimality is given. Secondly, GA-based price adjusted algorithm is introduced to deal with the problem of achieving the equilibrium state of RAS-M. Finally, RAS-M is implemented upon Xen to reallocate the VM’s weight. Experiments results obtained by setting different parameters show that RAS-M can achieve the equilibrium state approximately, that is, demand and supply is balanced nearly, which validates RAS-M is effective and practicable, and is capable of achieving its goal.","Resource management,
Cloud computing,
Concurrent computing,
Distributed computing,
Virtual manufacturing,
Grid computing,
Computational modeling,
Computer science,
Conference management,
Technology management"
Serious Games for Training Occupants of a Building in Personal Fire Safety Skills,"To survive a fire, occupants of a building have to be able to evacuate the structure before the situation becomes unsustainable. Evacuation time is thus a critical factor, but lack of knowledge about the basics of fire safety can dangerously increase this time and also result in various forms of unsafe behavior. In this paper, we propose serious games as a tool to acquire personal fire safety skills, also discussing a specific game we have developed.","Fires,
Railway safety,
Game theory,
Buildings,
Stress,
Design engineering,
Guidelines,
Application software,
Human computer interaction,
Computer science"
Animatronic Shader Lamps Avatars,"Applications such as telepresence and training involve the display of real or synthetic humans to multiple viewers. When attempting to render the humans with conventional displays, non-verbal cues such as head pose, gaze direction, body posture, and facial expression are difficult to convey correctly to all viewers. In addition, a framed image of a human conveys only a limited physical sense of presence—primarily through the display's location. While progress continues on articulated robots that mimic humans, the focus has been on the motion and behavior of the robots. We introduce a new approach for robotic avatars of real people: the use of cameras and projectors to capture and map the dynamic motion and appearance of a real person onto a humanoid animatronic model. We call these devices animatronic Shader Lamps Avatars (SLA).We present a proof-of-concept prototype comprised of a camera, a tracking system, a digital projector, and a life-sized styrofoam head mounted on a pan-tilt unit. The system captures imagery of a moving, talking user and maps the appearance and motion onto the animatronic SLA, delivering a dynamic, real-time representation of the user to multiple viewers.","Animation,
Lamps,
Avatars,
Humans,
Displays,
Robot sensing systems,
Head,
Robot vision systems,
Cameras,
Rendering (computer graphics)"
A novel SoC architecture on FPGA for ultra fast face detection,"Face detection is the cornerstone of a wide range of applications such as video surveillance, robotic vision and biometric authentication. One of the biggest challenges in face detection based applications is the speed at which faces can be accurately detected. In this paper, we present a novel SoC (System on Chip) architecture for ultra fast face detection in video or other image rich content. Our implementation is based on an efficient and robust algorithm that uses a cascade of Artificial Neural Network (ANN) classifiers on AdaBoost trained Haar features. The face detector architecture extracts the coarse grained parallelism by efficiently overlapping different computation phases while taking advantage of the finegrained parallelism at the module level. We provide details on the parallelism extraction achieved by our architecture and show experimental results that portray the efficiency of our face detection implementation. For the implementation and evaluation of our architecture we used the Xilinx FX130T Virtex5 FPGA device on the ML510 development board. Our performance evaluations indicate that a speedup of around 100X can be achieved over a SSE-optimized software implementation running on a 2.4GHz Core-2 Quad CPU. The detection speed reaches 625 frames per sec (fps).","Field programmable gate arrays,
Face detection,
Computer architecture,
Parallel processing,
Artificial neural networks,
Video surveillance,
Robot vision systems,
Biometrics,
Authentication,
System-on-a-chip"
Document Clustering Using Concept Space and Cosine Similarity Measurement,"Document clustering is related to data clustering concept which is one of data mining tasks and unsupervised classification. It is often applied to the huge data in order to make a partition based on their similarity. Initially, it used for Information Retrieval in order to improve the precision and recall from query. It is very easy to cluster with small data attributes which contains of important items. Furthermore, document clustering is very useful in retrieve information application in order to reduce the consuming time and get high precision and recall. Therefore, we propose to integrate the information retrieval method and document clustering as concept space approach. The method is known as Latent Semantic Index (LSI) approach which used Singular Vector Decomposition (SVD) or Principle Component Analysis (PCA). The aim of this method is to reduce the matrix dimension by finding the pattern in document collection with refers to concurrent of the terms. Each method is implemented to weight of term-document in vector space model (VSM) for document clustering using fuzzy c-means algorithm. Besides reduction of term-document matrix, this research also uses the cosine similarity measurement as replacement of Euclidean distance to involve in fuzzy c-means. And as a result, the performance of the proposed method is better than the existing method with f-measure around 0.91 and entropy around 0.51.","Extraterrestrial measurements,
Information retrieval,
Clustering algorithms,
Data mining,
Large scale integration,
Euclidean distance,
Clustering methods,
Space technology,
Information science,
Principal component analysis"
Preference learning for affective modeling,There is an increasing trend towards personalization of services and interaction. The use of computational models for learning to predict user emotional preferences is of significant importance towards system personalization. Preference learning is a machine learning research area that aids in the process of exploiting a set of specific features of an individual in an attempt to predict her preferences. This paper outlines the use of preference learning for modeling emotional preferences and shows the methodology's promise for constructing accurate computational models of affect.,"Computational modeling,
Protocols,
Predictive models,
Machine learning,
Humans,
Interactive systems,
Instruments,
Control systems,
Gaussian processes,
Neural networks"
Efficiency of SiC JFET-Based Inverters,"The state-of-the-art SiC JFETs are characterized. Three-phase full-bridge inverter power loss models based on experimental data are established and used to estimate inverter efficiency. The impact of load power, temperature, and switching frequency on inverter efficiency is analyzed and demonstrated. The efficiency of the SiC JFET inverters based on present device quality is above 98% with full load current, and more efficient than most conventional Si inverters, especially at high temperature and high frequency.","Silicon carbide,
Inverters,
JFETs,
Testing,
Schottky diodes,
Intrusion detection,
Manufacturing,
Switching frequency,
Temperature distribution,
Frequency estimation"
Temporally variable multi-aspect auditory morphing enabling extrapolation without objective and perceptual breakdown,"A generalized framework of auditory morphing based on the speech analysis, modification and resynthesis system STRAIGHT is proposed that enables each morphing rate of representational aspects to be a function of time, including the temporal axis itself. Two types of algorithms were derived: an incremental algorithm for real-time manipulation of morphing rates and a batch processing algorithm for off-line post-production applications. By defining morphing in terms of the derivative of mapping functions in the logarithmic domain, breakdown of morphing resynthesis found in the previous formulation in the case of extrapolations was eliminated. A method to alleviate perceptual defects in extrapolation is also introduced.","Extrapolation,
Electric breakdown,
Time frequency analysis,
Spectrogram,
Speech analysis,
Speech processing,
Speech synthesis,
Auditory system,
Music,
Controllability"
Modeling and control of the monopedal robot Thumper,"A hybrid controller that induces stable running gaits on a monopedal robot is developed. The robot features a rigid leg with a revolute knee and a heavy torso with center of mass located far from the hip. The torso houses a novel powertrain that provides series compliance in the compression direction of the leg. The proposed control law is developed within the hybrid zero dynamics framework and it acts on two levels. On the first level, continuous within-stride control asymptotically imposes (virtual) holonomic constraints reducing the dynamics of the robot to a lower-dimensional hybrid subsystem. On the second level, event-based control stabilizes the resulting hybrid subsystem. The controller achieves the dual objectives of working harmoniously with the system's natural dynamics and inducing provably exponentially stable running motions, while all relevant physical constraints are respected.","Robots,
Torso,
Leg,
Hip,
Legged locomotion,
Robotics and automation,
Knee,
Mechanical power transmission,
Thigh,
Automatic control"
Reliability-aware scalability models for high performance computing,"Scalability models are powerful analytical tools for evaluating and predicting the performance of parallel applications. Unfortunately, existing scalability models do not quantify failure impact and therefore cannot accurately account for application performance in the presence of failures. In this study, we extend two well-known models, namely Amdahl's law and Gustafson's law, by considering the impact of failures and the effect of fault tolerance techniques on applications. The derived reliability-aware models can be used to predict application scalability in failure-present environments and evaluate fault tolerance techniques. Trace-based simulations via real failure logs demonstrate that the newly developed models provide a better understanding of application performance and scalability in the presence of failures.",
Optimizing Speed of a True Random Number Generator in FPGA by Spectral Analysis,"Security and speed are two important properties of today's communication systems. In order to generate initialization vectors and keys for such communication fast enough, a true random number generator (TRNG) with a high bit rate is needed. In this paper an FPGA implementation of a TRNG based on several equal length oscillator rings that achieves a high bit rate is analyzed by using spectral analysis. The design is examined by defining a MatLab model of the TRNG and by investigating the frequency spectrum at different locations in order to find the speed increasing potential of the TRNG. Experiments performed on an Altera Cyclone II FPGA have shown that a TRNG, whose parameters were optimized by means of such a model, achieves a bit rate of 300 Mbit/s. Experiments with repeated restarts from a known state have shown that the output of the TRNG contains true randomness and not only pseudo randomness.","Random number generation,
Field programmable gate arrays,
Spectral analysis,
Bit rate,
Cryptography,
Ring oscillators,
Mathematical model,
Frequency,
Throughput,
Jitter"
"Unsupervised Learning of Probabilistic Object Models (POMs) for Object Classification, Segmentation, and Recognition Using Knowledge Propagation","We present a method to learn probabilistic object models (POMs) with minimal supervision, which exploit different visual cues and perform tasks such as classification, segmentation, and recognition. We formulate this as a structure induction and learning task and our strategy is to learn and combine elementary POMs that make use of complementary image cues. We describe a novel structure induction procedure, which uses knowledge propagation to enable POMs to provide information to other POMs and ldquoteach themrdquo (which greatly reduces the amount of supervision required for training and speeds up the inference). In particular, we learn a POM-IP defined on interest points using weak supervision [1], [2] and use this to train a POM-mask, defined on regional features, which yields a combined POM that performs segmentation/localization. This combined model can be used to train POM-edgelets, defined on edgelets, which gives a full POM with improved performance on classification. We give detailed experimental analysis on large data sets for classification and segmentation with comparison to other methods. Inference takes five seconds while learning takes approximately four hours. In addition, we show that the full POM is invariant to scale and rotation of the object (for learning and inference) and can learn hybrid objects classes (i.e., when there are several objects and the identity of the object in each image is unknown). Finally, we show that POMs can be used to match between different objects of the same category, and hence, enable objects recognition.","Unsupervised learning,
Image segmentation,
Inference algorithms,
Object detection,
State estimation,
Data analysis,
Object recognition,
Computational modeling,
Humans,
Decision theory"
Transmit beams adapted to reverberation noise suppression using dual-frequency SURF imaging,"A method that uses dual-frequency pulse complexes of widely separated frequency bands to suppress noise caused by multiple scattering or multiple reflections in medical ultrasound imaging is presented. The excitation pulse complexes are transmitted to generate a second order ultrasound field (SURF) imaging synthetic transmit beam. This beam has reduced amplitude near the transducer, which illustrates the multiple scattering suppression ability of the imaging method. Field simulations solving a nonlinear wave equation are used to calculate SURF imaging beams, which are compared with beams for pulse inversion (PI) and fundamental imaging. In addition, a combined SURF and PI beam generation is described and compared with the beams mentioned above. A quality ratio, relating the energy within the near-field to that within the imaging region, is defined and used to score the multiple scattering and multiple reflection suppression abilities when imaging with the different beams. The realized combined SURF-PI beam scores highest, followed by SURF, PI (that score equally well), and the fundamental. The amplitude in the imaging region and therefore also the SNR is highest for the fundamental followed by SURF, PI, and SURF-PI. The work hence indicates that when substituting PI for SURF, one may trade increased SNR into use of increased imaging frequencies without loss of multiple scattering and multiple reflection noise suppression.","Reverberation,
Ultrasonic imaging,
Scattering,
Optical reflection,
Frequency,
Acoustic reflection,
Signal to noise ratio,
Biomedical imaging,
Pulse generation,
Biomedical transducers"
New insight into Fermi-level unpinning on GaAs: Impact of different surface orientations,"We have systematically studied NMOSFETs, MOSCAPs, and the interfacial chemistry on GaAs (100), (110), (111)A and (111)B-four different crystalline surfaces with direct ALD Al2O3. We found that a much higher drain current on GaAs(111)A NMOSFET can be achieved compared to that obtained on the other 3 surfaces. Also, the results of MOS-CAPs and the interfacial chemistry obtained on the (111)A surface are very different from those others. These experimental results conclusively demonstrate that Fermi-level on the GaAs (111)A surface is indeed unpinned and Fermi-level pinning is not an intrinsic property of GaAs, but is orientation dependent thus related to surface chemistry.","Gallium arsenide,
MOSFET circuits,
Aluminum oxide,
Photonic band gap,
Intrusion detection,
Chemistry,
Dielectric substrates,
High-K gate dielectrics,
Frequency,
Crystallization"
Experimental evaluation of a WSN platform power consumption,"Critical characteristics of wireless sensor networks, as being autonomous and comprising small or miniature devices are achieved at the expense of very strict available energy related limitations. Therefore, it is apparent that optimal resource management is among the most important challenges in WSNs development and success.","Wireless sensor networks,
Energy consumption,
Processor scheduling,
Exponential distribution,
Degradation,
Computer science,
Educational institutions,
Information technology,
Parallel machines,
Mesh networks"
A distributed MAC design for data collision-free wireless USB home networks,"USB has been used at an enormous number of USB devices as a universal interface. Advances in USB and wireless communication technologies have led USB-IF (USB Implementers Forum) to establishment of WUSB(Wireless USB) standard 1.0 that has wired USB's speed and compatibility in addition to wireless technology's convenience. The wireless USB provides convenience. However, due to mobility of WUSB devices in multi-hop environment, DRP (Distributed Reservation Protocol) reservation conflicts happen frequently among devices with three-hop distance. To solve this problem, we propose a new DRP reservation scheme using a new 2-hop range DRP Availability IE for three-hop mobility support in WUSB networks with ultrawideband (UWB) technology. It is shown by simulation results that throughputs of devices at frequent three-hop range DRP reservation conflicts are largely increased.","Universal Serial Bus,
Home automation,
Ultra wideband technology,
Wireless application protocol,
Multimedia systems,
Physical layer,
Computer science education,
Educational programs,
Wireless communication,
Communications technology"
Correlation of sleep EEG frequency bands and heart rate variability,"Sleep apnoea is a sleep breathing disorder which causes changes in cardiac and neuronal activity and discontinuities in sleep pattern when observed via electrocardiogram (ECG) and electroencephalogram (EEG). This paper presents a pilot study result of assessing the correlation between EEG frequency bands and ECG heart rate variability (HRV) in normal and sleep apnoea human clinical patients at different sleep stages. In sleep apnoea patients, the results have shown that EEG delta, sigma and beta bands exhibited a strong correlation with cardiac HRV parameters at different sleep stages.","Electroencephalography,
Heart rate variability,
Sleep apnea,
Hafnium,
Australia,
Electrocardiography,
Resonant frequency,
Hospitals,
Electrodes,
Electromyography"
Topology and Memory Effect on Convention Emergence,"Social conventions are useful self-sustaining protocols for groups to coordinate behavior without a centralized entity enforcing coordination. We perform an in-depth study of different network structures, to compare and evaluate the effects of different network topologies on the success and rate of emergence of social conventions. While others have investigated memory for learning algorithms, the effects of memory or history of past activities on the reward received by interacting agents have not been adequately investigated. We propose a reward metric that takes into consideration the past action choices of the interacting agents. The research question to be answered is what effect does the history based reward function and the learning approach have on convergence time to conventions in different topologies. We experimentally investigate the effects of history size, agent population size and neighborhood size the emergence of social conventions.","History,
Intelligent agent,
Network topology,
Artificial intelligence,
Protocols,
Humans,
Conferences,
Computer science,
USA Councils,
Performance evaluation"
Independent Double-Gate Fin SONOS Flash Memory Fabricated With Sidewall Spacer Patterning,"Fin silicon-oxide-nitride-oxide-semiconductor (SONOS) flash memories having independent double gates are fabricated and characterized. This device has two sidewall gates sharing one Si fin. To achieve narrow Si fin width over the photolithography limitation, sidewall spacer patterning is adopted. Specific fabrication processes for the fin SONOS flash memory having independent double gates are described. Electrical properties related to the opposite gate dependence are characterized. Measurement results of the paired cell interference are delivered.",
Sensor node deployment in wireless sensor networks based on improved particle swarm optimization,"Wireless sensor networks (WSNs) is always consist of stationary and mobile sensor nodes, sensor node deployment is one of the key topics addressed in the researches of WSNs, traditional virtual force(VF) algorithm is presented. This paper proposes a method of improved particle swarm optimization to solve the problem. The simulation results show that the improved particle swarm optimization has better performance on the sensor node deployment problem and reduce the network energy consumption and increase the whole coverage ratio.","Wireless sensor networks,
Particle swarm optimization,
Sensor phenomena and characterization,
Sensor systems,
Convergence,
Costs,
Clustering algorithms,
Computer networks,
Resource management,
Intelligent sensors"
Relative entropy and score function: New information-estimation relationships through arbitrary additive perturbation,"This paper establishes new information-estimation relationships pertaining to models with additive noise of arbitrary distribution. In particular, we study the change in the relative entropy between two probability measures when both of them are perturbed by a small amount of the same additive noise. It is shown that the rate of the change with respect to the energy of the perturbation can be expressed in terms of the mean squared difference of the score functions of the two distributions, and, rather surprisingly, is unrelated to the distribution of the perturbation otherwise. The result holds true for the classical relative entropy (or Kullback-Leibler distance), as well as two of its generalizations: Rényi's relative entropy and the f-divergence. The result generalizes a recent relationship between the relative entropy and mean squared errors pertaining to Gaussian noise models, which in turn supersedes many previous information-estimation relationships. A generalization of the de Bruijn identity to non-Gaussian models can also be regarded as consequence of this new result.",
Ac-ac dual active bridge converter for solid state transformer,This work investigates the application of an ac-ac dual active bridge converter for solid state transformer. The proposed converter topology consists of two active H-bridges and one high frequency transformer. Four-quadrant switch cells are used to ensure bi-directional power flow. The advantages of direct ac-ac conversion include fewer power conversion stages and minimized passive components. The ac-ac dual active bridge converter is controlled with phase shift modulation. Operating modes for both power flow directions are described and zero-voltage switching criteria are analyzed. One design example is presented. Simulation results verify the theoretical analysis.,"Bridges,
Solid state circuits,
Switches,
Load flow,
Topology,
Frequency conversion,
Bidirectional control,
Power conversion,
Phase modulation,
Zero voltage switching"
Fully Redundant Decimal Arithmetic,"Hardware implementation of all the basic radix-10 arithmetic operations is evolving as a new trend in the design and implementation of general purpose digital processors. Redundant representation of partial products and remainders is common in the multiplication and division hardware algorithms, respectively. Carry-free implementation of the more frequent add/subtract operations, with the byproduct of enhancing the speed of multiplication and division, is possible with redundant number representation. However, conversion of redundant results to conventional representations entails slow carry propagation that can be avoided if the results are kept in redundant format for later use as operands of other arithmetic operations. Given that redundant decimal representations, contrary to redundant binary, do not necessarily require extra storage, we are motivated to develop a framework for fully redundant decimal arithmetic, where all operands and results belong to the same redundant decimal number system and can be stored and later used as operands of further decimal operations. In this paper, we present a new faster decimal signed digit add/sub unit and show how it can be efficiently used in the design of decimal multipliers and dividers, where all operands and results are represented with the same redundant digit set [–7, 7].","Digital arithmetic,
Hardware,
Encoding,
CMOS technology,
Computer science,
Internet,
Application software,
Commercialization,
Frequency conversion"
Particle Swarm Optimization Algorithm with Exponent Decreasing Inertia Weight and Stochastic Mutation,The paper gives an improved particle swarm optimal algorithm in which a kind of exponent decreasing inertia weights is given to improve the convergence speed and a kind of stochastic mutations is used to improve the diversity of the swarm in order to overcome the disadvantage of premature convergence and later period oscillatory occurrences. It is shown by five representative benchmarks function’s test that the improved algorithm is better than both a particle swarm optimization with linear decreasing inertia weight and a particle swarm optimization with exponent decreasing inertia weight in global searching and performance.,"Particle swarm optimization,
Stochastic processes,
Genetic mutations,
Convergence,
Optimization methods,
Mathematics,
Computer science,
Benchmark testing,
Birds,
Fuzzy systems"
On Concise 3-D Simple Point Characterizations: A Marching Cubes Paradigm,"The centerlines of tubular structures are useful for medical image visualization and computer-aided diagnosis applications. They can be effectively extracted by using a thinning algorithm that erodes an object layer by layer until only a skeleton is left. An object point is ldquosimplerdquo and can be safely deleted only if the resultant image is topologically equivalent to the original. Numerous characterizations of 3-D simple points based on digital topology already exist. However, little work has been done in the context of marching cubes (MC). This paper reviews several concise 3-D simple point characterizations in a MC paradigm. By using the Euler characteristic and a few newly observed properties in the context of connectivity-consistent MC, we present concise and more self-explanatory proofs. We also present an efficient method for computing the Euler characteristic locally for MC surfaces. Performance evaluations on different implementations are conducted on synthetic data and multidetector computed tomography examination of virtual colonoscopy and angiography.","Biomedical imaging,
Medical diagnostic imaging,
Visualization,
Computer aided diagnosis,
Application software,
Skeleton,
Topology,
Computed tomography,
Virtual colonoscopy,
Angiography"
Coordination Planning: Applying Control Synthesis Methods for a Class of Distributed Agents,"This brief proposes a new multi-agent planning approach to logical coordination synthesis that views a class of distributed agents as discrete-event processes. The coordination synthesis problem involves finding a coordination module for every agent, using which their coordinated interactions would never violate some specified inter-agent constraint. This brief first shows explicitly that, though conceptually different, the well-researched problem of supervision in control science and the problem of distributed agent coordination planning in computer agents science are mathematically related. This basic result enables the application of the vast body of knowledge and associated synthesis tools already founded in discrete-event control theory for automatic coordination synthesis of distributed agents. Within this logical framework, a basic planning methodology applying the discrete-event control synthesis methods is proposed and illustrated using TCT, a software design tool implementing these methods. A simple example demonstrates how it supports formal synthesis of coordination modules for distributed agents. Discussions in relation to previous work examine the relative significance of the new multi-agent planning framework.","Automatic control,
Control system synthesis,
Distributed computing,
Discrete event systems,
Multiagent systems,
Automata,
Process planning,
Application software,
Control theory,
Software design"
Background subtraction in varying illuminations using an ensemble based on an enlarged feature set,"Image sequences with dynamic backgrounds often cause false classification of pixels. In particular, varying illuminations cause significant changes in the representation of a scene in different color spaces, which in turn results in the high levels of failure in such conditions. Because mapping to alternate color spaces has largely failed to solve this problem, a solution of using alternate image features is proposed in this paper. In particular, the use of gradient and texture features along with the original color intensities are used in an ensemble of mixture of Gaussians background classifiers. A clear improvement is shown when using this method compared to the Mixture of Gaussians algorithm using only color intensities. In addition, this work shows that performing background subtraction using only gradient magnitude as an image feature performs at a much higher rate in varying illuminations then using color intensities. Results are generated on three separate datasets, each with unique, dynamic, illumination conditions.","Lighting,
Gaussian distribution,
Gaussian processes,
Image segmentation,
Computer science,
Image sequences,
Layout,
Iterative algorithms,
Target tracking,
Equations"
Information Hiding in Dual Images with Reversibility,"Secret information protection has received increasing attention from the information technology community.Proposing a novel information hiding scheme to effectively conceal secret information in various applications is encouraged.In this paper, we proposed a novel reversible information hiding scheme by embedding secret information into dual image. The proposed scheme first converts the secret bit stream into secret digits in the base-5 numeral system. Then, two secret digits are embedded into one pixel pair at a time by distributing them into a dual image (i.e. two stego images). The experimental results show that the proposed scheme achieves high embedding capacity and good visual quality of stego images.","Computer science,
Pixel,
Digital images,
Detectors,
Protection,
Steganography,
Image coding,
Decoding,
Data mining,
Robustness"
Minimum Energy coding in CDMA Wireless Sensor Networks,"A theoretical framework is proposed for accurate comparison of minimum energy coding in coded division multiple access (CDMA) wireless sensor networks (WSNs). Energy consumption and reliability are analyzed for two coding schemes: minimum energy coding (ME), and modified minimum energy coding (MME). A detailed model of consumed energy is described as function of the coding, radio transmit power, the characteristics of the transceivers, and the dynamics of the wireless channel. Since CDMA is strongly limited by multi-access interference, the system model includes all the relevant characteristics of wireless propagation. A distributed and asynchronous algorithm, which minimizes the total energy consumption by controlling the radio power, is developed. Numerical results are presented to validate the theoretical analysis and show under which conditions MME outperforms ME with respect to energy consumption and bit error rate. It is concluded that MME is more energy efficient than ME only for short codewords.","Multiaccess communication,
Wireless sensor networks,
Energy consumption,
Power system modeling,
Interference,
Bit error rate,
Power control,
Computer networks,
Wireless communication,
Modulation coding"
Cluster-based channel assignment in multi-radio multi-channel wireless mesh networks,"In a typical Wireless Mesh Network (WMN), the interfering links can broadly be classified as coordinated and non-coordinated links, depending upon the geometric relationship. It is known that compared to coordinated interference, the non-coordinated interference result in significantly lower throughput and an unfair capacity distribution amongst the links. However, identification of non-coordinated interference relationships requires that each node is aware of the precise location of its neighbours, which is impractical. In this paper, we propose a novel two-phase Cluster-Based Channel Assignment Scheme (CCAS) that minimizes both non-coordinated as well as coordinated interference without requiring the nodes to be aware of the location of its neighbours. CCAS logically partitions the network into non-overlapping clusters. The links within each cluster operate on a common channel which is orthogonal to that used in neighbouring clusters, thus eliminating non-coordinated interference. The inter-cluster links are assigned channels such that any non-coordinated interference that they introduce is minimized. The second phase of CCAS minimizes the coordinated interference by exploiting the channel diversity to sub-divide each cluster into multiple interference domains, thereby increasing the capacity of individual links. Simulation-based evaluations demonstrate that CCAS can achieve twice the aggregate network goodput as compared to existing channel assignment schemes, while ensuring a fair distribution of capacity amongst the links.","Wireless mesh networks,
Protocols,
Computer science,
Multiaccess communication,
Computer networks,
Australia,
Throughput,
Interference elimination,
Aggregates,
Frequency"
BEST 2009 : Thai word segmentation software contest,"This is a non-technical paper describing how and why we organized BEST 2009, the first contest in the series of “Benchmark for Enhancing the Standard of Thai language processing”, which is expected to help accelerate the progress of the Natural Language Processing technology in Thailand by assembling 3 essential components: common standards, resources and researchers. The BEST 2009 : Thai Word Segmentation Software Contest is the first shared task on Thai NLP that exercised this assemblage and aimed to find the best algorithms that could correctly divide Thai non-segmented script into words according to the guidelines previously prepared by experts from several research institutes and universities. Thai word-segmented corpora of 5 million words have been developed as a training set, another 600K as a test set. The evaluation procedure and protocol have been designed. The process and the results of the contest are reported.","Natural language processing,
Software standards,
Assembly,
Educational institutions,
Testing,
Acceleration,
Paper technology,
Software algorithms,
Guidelines,
Protocols"
Avoiding cache thrashing due to private data placement in last-level cache for manycore scaling,"Without high-bandwidth broadcast, large numbers of cores require a scalable point-to-point interconnect and a directory protocol. In such cases, a shared, inclusive last level cache (LLC) can improve data sharing and avoid three-way communication for shared reads. However, if inclusion encompasses thread-private data, two problems arise with the shared LLC. First, current memory allocators align stack bases on page boundaries, which emerges as a source of severe conflict misses for large numbers of threads on data-parallel applications. Second, correctness does not require the private data to reside in the shared directory or the LLC. This paper advocates stack-base randomization that eliminates the major source of conflict misses for large numbers of threads. However, when capacity becomes a limitation for the directory or last-level cache, this is not sufficient. We then propose non-inclusive, semi-coherent cache organization (NISC) that removes the requirement for inclusion of private data and reduces capacity misses. Our data-parallel benchmarks show that these limitations prevent scaling beyond 8 cores, while our techniques allow scaling to at least 32 cores for most benchmarks. At 8 cores, stack randomization provides a mean speedup of 1.2X, but stack randomization with 32 cores gives a speedup of 2.7X over the best baseline configuration. Comparing to conventional performance with a 2 MB LLC, our technique achieves similar performance with a 256 KB LLC, suggesting LLCs may be typically overprovisioned. When very limited LLC resources are available, NISC can further improve system performance by 1.8X.","Yarn,
Broadcasting,
Protocols,
Hardware,
Large-scale systems,
Computer science,
System performance,
Throughput,
Multithreading,
Sun"
A hierarchical approach to unsupervised shape calibration of microphone array networks,"Microphone arrays represent the basis for many challenging acoustic sensing tasks. The accuracy of techniques like beamforming directly depends on a precise knowledge of the relative positions of the sensors used. Unfortunately, for certain use cases manually measuring the geometry of an array is not feasible due to practical constraints. In this paper we present an approach to unsupervised shape calibration of microphone array networks. We developed a hierarchical procedure that first performs local shape calibration based on coherence analysis and then employs SRP-PHAT in a network calibration method. Practical experiments demonstrate the effectiveness of our approach especially for highly reverberant acoustic environments.","Microphone arrays,
Calibration,
Sensor arrays,
Acoustic arrays,
Acoustic sensors,
Array signal processing,
Source separation,
Acoustic measurements,
Shape measurement,
Planar arrays"
Cube: A 512-FPGA cluster,"Cube, a massively-parallel FPGA-based platform is presented. The machine is made from boards each containing 64 FPGA devices and eight boards can be connected in a cube structure for a total of 512 FPGA devices. With high bandwidth systolic inter-FPGA communication and a flexible programming scheme, the result is a low power, high density and scalable supercomputing machine suitable for various large scale parallel applications. A RC4 key search engine was built as an demonstration application. In a fully implemented Cube, the engine can perform a full search on the 40-bit key space within 3 minutes, this being 359 times faster than a multi-threaded software implementation running on a 2.5GHz Intel Quad-Core Xeon processor.","Field programmable gate arrays,
Pipelines,
Hardware,
Costs,
Application software,
Large-scale systems,
Search engines,
Radio access networks,
Parallel programming,
Prototypes"
Contention-polling duality coordination function for IEEE 802.11 WLAN family,"The contention-based DCF suffers from collision seriously under heavy load conditions. In this paper, a novel medium access control protocol called contention-polling duality coordination function (CPDCF) is proposed to enhance the DCF-based MAC protocol. A polling ACK mechanism is introduced to admit the designated station to transmit without performing any contention process. By eliminating collisions and preventing waste from idle backoff slots, the system capacity is increased effectively. A mathematical model is developed to analyze the saturation throughput of the CPDCF with various CP combinations. The performance in terms of fairness is studied as well. The model accuracy is verified via extensive simulations. Numerical results show that the well-managed CPDCF can achieve a significantly higher system capacity than the legacy DCF does. The CPDCF is also employed to enhance the performance of IEEE 802.11e EDCA. We conclude that the proposed CPDCF is quite feasible, effective, and can be easily integrated with the widespread DCF-based devices.","Wireless LAN,
Access protocols,
Media Access Protocol,
Web and internet services,
Wireless application protocol,
Throughput,
Multiaccess communication,
Computer science,
Delay,
Mathematical model"
Optically transparent ultra-wideband antenna,"A novel optically transparent ultra-wideband (UWB) disc monopole antenna is proposed. The antenna is fed by a 50 coplanar waveguide and its operational bandwidth is measured from 1 to 8.5 GHz. Gain and radiation patterns are also presented. The antenna exhibits omnidirectional and monopole-like radiation patterns at low frequencies for the H-plane and E-plane, respectively. The proposed antenna uses the highly conductive AgHT-4 transparent film, which makes the antenna suitable for inter-vehicle communication.","ultra wideband antennas,
antenna radiation patterns,
monopole antennas"
Evolution versus Temporal Difference Learning for learning to play Ms. Pac-Man,"This paper investigates various factors that affect the ability of a system to learn to play Ms. Pac-Man. For this study Ms. Pac-Man provides a game of appropriate complexity, and has the advantage that in recent years there have been many other papers published on systems that learn to play this game. The results indicate that Temporal Difference Learning (TDL) performs most reliably with a tabular function approximator, and that the reward structure chosen can have a dramatic impact on performance. When using a multi-layer perceptron as a function approximator, evolution outperforms TDL by a significant margin. Overall, the best results were obtained by evolving multi-layer perceptrons.","Evolution (biology),
Evolutionary computation,
Multilayer perceptrons,
State estimation,
Testing,
Genetic programming,
Parallel programming,
Genetic algorithms,
Artificial neural networks,
Topology"
Robust shadow and illumination estimation using a mixture model,"Illuminant estimation from shadows typically relies on accurate segmentation of the shadows and knowledge of exact 3D geometry, while shadow estimation is difficult in the presence of texture. These can be onerous requirements; in this paper we propose a graphical model to estimate the illumination environment and detect the shadows of a scene with textured surfaces from a single image and only coarse 3D information. We represent the illumination environment as a mixture of von Mises-Fisher distributions. Then, each shadow pixel becomes the combination of samples generated from this illumination environment. We integrate a number of low-level, illumination-invariant 2D cues in a graphical model to detect and estimate cast shadows on textured surfaces. Both 2D cues and approximate 3D reasoning are combined to infer a set of labels that identify the shadows in the image and estimate the positions, shapes and intensities of the light sources. Our results demonstrate that the probabilistic combination of multiple cues, unlike prior approaches, manages to differentiate both hard and soft shadows from the underlying surface texture even when we can only coarsely anticipate the effect of 3D geometry. We also experimentally demonstrate how correct estimation of the sharpness and shape of the light sources improves the augmented reality results.","Robustness,
Lighting,
Light sources,
Layout,
Shape,
Surface texture,
Geometry,
Reflectivity,
Object detection,
Image segmentation"
Towards privacy-sensitive participatory sensing,"The ubiquity of mobile devices has brought forth the concept of participatory sensing, whereby ordinary citizens can now contribute and share information from the urban environment. However, such applications introduce a key research challenge: preserving the location privacy of the individuals contributing data. In this paper, we propose the use of microaggregation, a concept used for protecting privacy in databases, as a solution to this problem. We compare microaggregation with tessellation, the current state-of-the-art, and demonstrate that each technique has its advantage in certain mutually exclusive situations. We propose a hybrid scheme called, Hybrid Variable-Size Maximum Distance to Average Vector (V-MDAV), which combines the positive aspects of both these techniques. Our evaluations based on real-world data traces show that hybrid V-MDAV improves the percentage of positive identifications made by the application server by up to 100% and decreases the information loss by about 40%. Furthermore, our studies show that perturbing user locations with random Gaussian noise can provide users with an extra layer of protection with very little impact on the system performance.","Tiles,
Protection,
Roads,
Australia,
Mobile computing,
Data privacy,
Mobile handsets,
Pricing,
Computer science,
Databases"
Integrality Gaps for Strong SDP Relaxations of UNIQUE GAMES,"With the work of Khot and Vishnoi (FOCS 2005) as a starting point, we obtain integrality gaps for certain strong SDP relaxations of unique games. Specifically, we exhibit a gap instance for the basic semidefinite program strengthened by all valid linear inequalities on the inner products of up to
exp(Ω(loglog n
)
1/4
)
vectors. For stronger relaxations obtained from the basic semidefinite program by
R
rounds of Sherali--Adams lift-and-project, we prove a unique games integrality gap for
R=Ω(loglog n
)
1/4
.By composing these SDP gaps with UGC-hardness reductions, the above results imply corresponding integrality gaps for every problem for which a UGC-based hardness is known. Consequently, this work implies that including any valid constraints on up to
exp(Ω(loglog n
)
1/4
)
vectors to natural semidefinite program, does not improve the approximation ratio for any problem in the following classes: constraint satisfaction problems, ordering constraint satisfaction problems and metric labeling problems over constant-size metrics. We obtain similar SDP integrality gaps for balanced separator, building on Devanur et al. (STOC 2006). We also exhibit, for explicit constants
γ,δ≫0
, an n-point negative-type metric which requires distortion
Ω(loglogn
)
γ
to embed into
ℓ
1
, although all its subsets of size
exp(Ω(loglog n
)
δ
)
embed isometrically into
ℓ
1
.","User-generated content,
Labeling,
Approximation algorithms,
Computer science,
Vectors,
Particle separators"
Analysis of Penalized Likelihood Image Reconstruction for Dynamic PET Quantification,"Quantification of tracer kinetics using dynamic positron emission tomography (PET) provides important information for understanding the physiological and biochemical processes in humans and animals. A common procedure is to reconstruct a sequence of dynamic images first, and then apply kinetic analysis to the time activity curve of a region of interest derived from the reconstructed images. Obviously, the choice of image reconstruction method and its parameters affect the accuracy of the time activity curve and hence the estimated kinetic parameters. This paper analyzes the effects of penalized likelihood image reconstruction on tracer kinetic parameter estimation. Approximate theoretical expressions are derived to study the bias, variance, and ensemble mean squared error of the estimated kinetic parameters. Computer simulations show that these formulae predict correctly the changes of these statistics as functions of the regularization parameter. It is found that the choice of the regularization parameter has a significant impact on kinetic parameter estimation, indicating proper selection of image reconstruction parameters is important for dynamic PET. A practical method has been developed to use the theoretical formulae to guide the selection of the regularization parameter in dynamic PET image reconstruction.","Image analysis,
Image reconstruction,
Positron emission tomography,
Kinetic theory,
Parameter estimation,
Humans,
Animals,
Image sequence analysis,
Computer errors,
Computer simulation"
A QoS-Aware Model for Web Services Discovery,"With the increasing number of Web Services on the web, the service consumers are always presented with a group of services offering the similar functions. How to discover and find out the appropriate one among the large numbers of Web Services is the problem put on the desk. A service discovery mechanism in view of the functional and nonfunctional properties (i.e. QoS) is essential for Web Services consumers. Base on the analyses of the known models, a QoS-aware model for Web Services discovery is proposed, by introducing QoS Broker. The merit of this model is that QoS description is introduced from the beginning of the Web Service providers publishing their services, and it does not need to modify the standard UDDI interface. So the client side software can transparently plug on without any extra modification. In the meantime, the model can discovery seemly Web Services with real-time, fair and authentic QoS information by its monitoring and valuation mechanism.","Web services,
Quality of service,
Availability,
Publishing,
Monitoring,
Cost accounting,
Computer science education,
Educational technology,
Computer science,
Mechanical factors"
Exploring multispectral iris recognition beyond 900nm,"Most iris recognition systems acquire images of the eye in the 700nm–900nm range of the electromagnetic spectrum. In this work, the iris is examined at wavelengths beyond 900nm. The purpose is to understand the iris structure at longer wavelengths and to determine the possibility of performing cross-spectral iris matching. An acquisition system is first designed for imaging the iris at narrow spectral bands in the 950 nm–1650 nm range. Next, the left and right images of the iris are acquired from 25 subjects in order to conduct the analysis. Finally, the possibility of performing cross-spectral matching and multispectral fusion at the match score level is investigated. Experimental results suggest: (a) the feasibility of acquiring iris images in wavelengths beyond 900nm using InGaAs detectors; (b) the possibility of observing different structures in the iris anatomy at various wavelengths; and (c) the potential of performing cross-spectral matching and multispectral fusion for enhanced iris recognition.","Iris recognition,
Image analysis,
Image coding,
Detectors,
Biometrics,
Fingerprint recognition,
Electromagnetic spectrum,
Indium gallium arsenide,
Anatomy,
Stability"
Network coding-based wireless media transmission using POMDP,"We consider the problem of joint network coding and packet scheduling for multimedia transmission from the Access Point (AP) to multiple receivers in 802.11 networks. The state of receivers is described by a hidden Markov model and the AP acts as a decision maker which employs a partially observable Markov decision process (POMDP) to optimize the media transmission. Importantly, we introduce a simulation-based dynamic programming algorithm as a solution tool for our POMDP abstract. Our simulation-based algorithm simplifies the modeling process as well as reduces the computational complexity of the solution process. Our simulation results demonstrate that the proposed scheme provides higher performance than the network coding scheme without using optimization techniques and traditional retransmission scheme.","Network coding,
Hidden Markov models,
Streaming media,
Scheduling algorithm,
Computational modeling,
State feedback,
Dynamic programming,
Processor scheduling,
Wireless networks,
Heuristic algorithms"
High-speed 10-bit LCD column driver with a split DAC and a class-AB output buffer,"We propose a high-speed rail-to-rail 10-bit column driver with a reduced die area for LCD-TV applications. This column driver combines an 8-bit resistor-string digital-to-analog converter (R-DAC), constructed using a hybrid-type decoder to reduce the RC time delay and die area, with a 2-bit interpolation DAC. In the output buffer, error amplifiers drive a column line so as to realize a highspeed rail-to-rail drive. Gamma-corrected output voltages are generated by the resistor string of R-DAC, which contains resistors of unequal values that match the inverse of the liquid crystal transmittance-voltage characteristic. A prototype 10- bit LCD column driver was designed and fabricated using a 0.3 mum CMOS technology, and has a settling time of within 2 mus and a quiescent current of 5.4 muA per channel.","Driver circuits,
Resistors,
CMOS technology,
Digital-analog conversion,
Decoding,
Delay effects,
Interpolation,
Rail to rail amplifiers,
Rail to rail outputs,
Voltage"
Low power real-time seizure detection for ambulatory EEG,"Ambulatory Electroencephalograph (AEEG) technology is becoming popular because it facilitates the continuous monitoring of epilepsy patients without interrupting their routine life. As long term monitoring requires low power processing on the device, a low power real time seizure detection algorithm suitable for AEEG devices is proposed herein. The performance of various classifiers was tested and the most effective was found to be the Linear Discriminant Analysis classifier (LDA). The algorithm presented in this paper provides 87.7 (100–70.2)% accuracy with 94.2 (100–78)% sensitivity and 77.9 (100–52.1)% specificity in patient dependent experiments. It provides 76.5 (79.0–73.3)% accuracy with 90.9 (96.2–85.8)% sensitivity and 59.5 (70.9–52.6)% specificity in patient independent experiments. We also suggest how power can be saved at the lost of a small amount of accuracy by applying different techniques. The algorithm was simulated on a DSP processor and on an ASIC and the power estimation results for both implementations are presented. Seizure detection using the presented algorithm is approximately 100% more power efficient than other AEEG processing methods. The implementation using an ASIC can reduce power consumption by 25% relative to the implementation on a DSP processor with reduction of only 1% of accuracy.","Electroencephalography,
Patient monitoring,
Linear discriminant analysis,
Digital signal processing,
Application specific integrated circuits,
Epilepsy,
Detection algorithms,
Testing,
Brain modeling,
Energy consumption"
Analysis and improvements of cloud models for propagation studies,"Two cloud models currently in use in propagation and remote sensing simulations in the presence of nonprecipitating clouds were analyzed. A new cloud model is also proposed: a modification of a humidity threshold to better identify clouds is suggested, as is a new cloud density function for computing cloud liquid and ice content within a cloud. The performances of the threshold functions were examined at the Atmospheric Radiation Measurement (ARM) Program's Southern Great Plaints (SGP) site in Oklahoma, USA, by using radiosonde and ceilometer data. The new threshold showed an improvement in the cloud detection (15%) and a reduction of false cloud identification in clear-sky conditions (26%). Next, the cloud density models were evaluated in the brightness temperature (Tb) domain, by comparing simulated Tb values in cloudy conditions with those measured by dual-channel microwave radiometers at several ARM sites. The new model provided good results in comparison with the radiometer measurements, with overall root mean square (RMS) differences of 3.10 K, reducing the RMS by about 16% with respect to the best of the other models. Improvements can be noticed in particular at SGP (20%), and in the tropics (37%).","Clouds,
Atmospheric modeling,
Mathematical model,
Analytical models,
Computational modeling,
Humidity,
Liquids"
A global optimization framework for meeting summarization,"We introduce a model for extractive meeting summarization based on the hypothesis that utterances convey bits of information, or concepts. Using keyphrases as concepts weighted by frequency, and an integer linear program to determine the best set of utterances, that is, covering as many concepts as possible while satisfying a length constraint, we achieve ROUGE scores at least as good as a ROUGE-based oracle derived from human summaries. This brings us to a critical discussion of ROUGE and the future of extractive meeting summarization.","Computer science,
Data mining,
Ambient intelligence,
Frequency,
Humans,
Integer linear programming"
Guiding medical needles using single-point tissue manipulation,"This paper addresses the use of robotic tissue manipulation in medical needle insertion procedures to improve targeting accuracy and to help avoid damaging sensitive tissues. To control these multiple, potentially competing objectives, we present a phased controller that operates one manipulator at a time using closed-loop imaging feedback. We present an automated procedure planning technique that uses tissue geometry to select the needle insertion location, manipulation locations, and controller parameters. The planner uses a stochastic optimization of a cost function that includes tissue stress and robustness to disturbances. We demonstrate the system on 2D tissues simulated with a mass-spring model, including a simulation of a prostate brachytherapy procedure. It can reduce targeting errors from more than 2cm to less than 1mm, and can also shift obstacles by over 1cm to clear them away from the needle path.","Needles,
Automatic control,
Biomedical imaging,
Robot sensing systems,
Medical robotics,
Robotics and automation,
Manipulators,
Feedback,
Geometry,
Stochastic processes"
Effects of using two neighborhood structures on the performance of cellular evolutionary algorithms for many-objective optimization,"Cellular evolutionary algorithms usually use a single neighborhood structure for local selection. When a new solution is to be generated by crossover and/or mutation for a cell, a pair of parent solutions is selected from its neighbors. The current solution at the cell is replaced with the newly generated offspring if the offspring has the higher fitness value than the current one. That is, the “replace-if-better” policy is used for the replacement of the current solution. Local selection, crossover, mutation and replacement are iterated at every cell in cellular algorithms. A recently proposed multiobjective evolutionary algorithm called MOEA/D by Zhang and Li (2007) can be viewed as a cellular algorithm where each cell has its own scalarizing fitness function with a different weight vector. We can introduce a spatial structure to MOEA/D by the Euclidean distance between weight vectors. Its main difference from standard cellular algorithms is that a newly generated offspring for a cell is compared with not only the current solution of the cell but also its neighbors for local replacement in MOEA/D. In this paper, we examine the effect of local replacement on the search ability of a cellular version of MOEA/D. Whereas the same neighborhood structure was used for local selection and local replacement in the original MOEA/D, we examine the use of different neighborhood structures for local selection and local replacement. It is shown through computational experiments on multiobjective 0/1 knapsack problems with two, four and six objectives that local replacement plays an important role in MOEA/D especially for many-objective optimization problems.","Evolutionary computation,
Genetic mutations,
Euclidean distance,
Computer science,
Intelligent systems,
Testing"
CoScribe: Integrating Paper and Digital Documents for Collaborative Knowledge Work,"This paper presents CoScribe, a concept and prototype system for the combined work with printed and digital documents, which supports a large variety of knowledge work settings. It integrates novel pen-and-paper-based interaction techniques that enable users to collaboratively annotate, link and tag both printed and digital documents. CoScribe provides for a very seamless integration of paper with the digital world, as the same digital pen and the same interactions can be used on paper and displays. As our second contribution, we present empirical results of three field studies on learning at universities. These motivated the design of CoScribe and were abstracted to a generic framework for the design of intuitive pen-and-paper user interfaces. The resulting interaction design comprising collaboration support and multiuser visualizations has been implemented and evaluated in user studies. The results indicate that CoScribe imposes only minimal overhead on traditional annotation processes and provides for a more efficient structuring and retrieval of documents.","Collaboration,
User interfaces,
Computer science,
Tagging,
Joining processes,
Media,
Educational institutions"
Implementing the design of smart home and achieving energy conservation,"The idea of smart home originated from the concept of home automation, and this goal was based on the availability of a smart home network for devices to communicate. Even though solutions for the home network have been discussed for years, and networking technologies over different media such as power line, phone line and Ethernet have been tested and found functional satisfaction, the applications of smart home were not found popular. This is because a gap remaining between users' habits and the services offered by the so called ‘smart appliances’, including information and communication devices and network-enabled white goods. In this article, a proposal to integrate different groups of smart appliances and deliver more value-added services will be introduced. More details about the development and implementation of the key technologies for the smart home network, smart appliances and value-added services will be described.","Smart homes,
Energy conservation,
Decision support systems,
Virtual reality"
Botnet Command and Control Mechanisms,"Botnet is most widespread and occurs commonly in today's cyber attacks, resulting in serious threats to our network assets and organization's properties. Botnets are collections of compromised computers (Bots) which are remotely controlled by its originator (BotMaster) under a common Commond-and-Control (C & C) infrastructure. They are used to distribute commands to the Bots for malicious activities such as distributed denial-of-service (DDoS) attacks, sending large amount of SPAM and other nefarious purposes. Understanding the Botnet C & C channels is a critical component to precisely identify, detect, and mitigate the Botnets threats. Therefore, in this paper we provide a classification of Botnets C & C channels and evaluate well-known protocols (e.g. IRC, HTTP, and P2P) which are being used in each of them.","Command and control systems,
Turing machines,
Internet,
Computer science,
Information systems,
Electronic mail,
Computer crime,
Unsolicited electronic mail,
Distributed computing,
Protection"
Using Anonymized Data for Classification,"In recent years, anonymization methods have emerged as an important tool to preserve individual privacy when releasing privacy sensitive data sets. This interest in anonymization techniques has resulted in a plethora of methods for anonymizing data under different privacy and utility assumptions. At the same time, there has been little research addressing how to effectively use the anonymized data for data mining in general and for distributed data mining in particular. In this paper, we propose a new approach for building classifiers using anonymized data by modeling anonymized data as uncertain data. In our method, we do not assume any probability distribution over the data. Instead, we propose collecting all necessary statistics during anonymization and releasing these together with the anonymized data. We show that releasing such statistics does not violate anonymity. Experiments spanning various alternatives both in local and distributed data mining settings reveal that our method performs better than heuristic approaches for handling anonymized data.","Data mining,
Data privacy,
Euclidean distance,
USA Councils,
Statistical distributions,
Drugs,
Classification algorithms,
Data engineering,
Computer science,
Probability distribution"
Multi-camera track-before-detect,We present a novel multi-camera multi-target fusion and tracking algorithm for noisy data. Information fusion is an important step towards robust multi-camera tracking and allows us to reduce the effect of projection and parallax errors as well as of the sensor noise. Input data from each camera view are projected on a top-view through multi-level homographic transformations. These projected planes are then collapsed onto the top-view to generate a detection volume. To increase track consistency with the generated noisy data we propose to use a track-before-detect particle filter (TBD-PF) on a 5D state-space. TBD-PF is a Bayesian method which extends the target state with the signal intensity and evaluates each image segment against the motion model. This results in filtering components belonging to noise only and enables tracking without the need of hard thresholding the signal. We demonstrate and evaluate the proposed approach on real multi-camera data from a basketball match.,"Target tracking,
Particle tracking,
Noise robustness,
Sensor fusion,
Noise reduction,
Cameras,
Noise generators,
Particle filters,
Bayesian methods,
Image segmentation"
System design of CDMA2000 femtocells,"Femtocells extend the cellular network coverage and provide high speed data service inside homes and enterprises for mobiles supporting existing cellular radio communication techniques. They also provide additional system capacity by offloading macro network traffic. This article reviews the characteristics of cdma2000-based femtocell systems. It discusses design and deployment aspects such as carrier allocation, access control, efficient support for femtocell discovery by idle mobiles, and active call hand-in from macrocell to femtocell. The evolution of the cdma2000 standard for optimizing performance and enriching user experience with femtocells is also discussed.","Femtocells,
Multiaccess communication,
Radio frequency,
Radiofrequency interference,
Macrocell networks,
Bandwidth,
Semiconductor device measurement,
Interchannel interference,
Monitoring,
Base stations"
Prophet: A Speculative Multi-threading Execution Model with Architectural Support Based on CMP,"Speculative Multithreading (SpMT) has been proposed as a perspective method for sequential programs to benefit from the increasing computing resources provided by Chip Multiprocessors (CMP). This paper analyzes the extraction of ihread-level parallelism from general-purpose programs and presents a speculative multi-threading execution model, Prophet. The architectural support for Prophet execution model is designed based on CMP. In Prophet the inter-thread data dependences are reduced by precomputation slice (p-slice). Multi-versioning Cache system along with thread state control mechanism are designed for buffering the speculative data and also a snooping bus based cache coherence protocol is used to detect data dependence violation. The experiment results show that Prophet system could achieve significant speedup for general-purpose programs.","Yarn,
Parallel processing,
Embedded computing,
Runtime,
Hardware,
Program processors,
Registers,
Computer science,
Multithreading,
Data mining"
Brightness-Based Selection and Edge Detection-Based Enhancement Separation Algorithm for Low-Resolution Metal Transfer Images,"Next-generation gas metal arc welding (GMAW) machines require the rapid metal transfer process be accurately monitored using a high-speed vision system and be feedback controlled. However, the necessity for high frame rate reduces the resolution achievable and bright welding arc makes it difficult to clearly image the metal transfer process. Processing of images for real-time monitoring of metal transfer process is thus challenging. To address this challenge, the authors analyzed the characteristics of metal transfer images in a novel modification of GMAW, referred to as double-electrode GMAW, and proposed an algorithm consisting of a system of effective steps to extract the needed droplet feedback information from high frame rate low-resolution metal transfer images. Experimental results verified the effectiveness of the proposed algorithm in automatically locating the droplet and computing the droplet size with an adequate accuracy.","Image edge detection,
Welding,
Condition monitoring,
Machine vision,
Adaptive control,
Image resolution,
Image analysis,
Information analysis,
Algorithm design and analysis,
Data mining"
Structure and ferroelectric properties of Bi(Zn1/2Ti1/2)O3-(Bi1/2K1/2)TiO3 perovskite solid solutions,"Lead-free piezoelectric ceramics based on chiBi(Zn1/2Ti1/2)O3-(1-chi)(Bi1/2K1/2)TiO3 were obtained via solid state processing techniques. A single perovskite phase with tetragonal symmetry was obtained for Bi(Zn1/2Ti1/2)O3 (BZT) substitutions up to 20 mol%. The maximum density was 97.1% at the composition of chi = 0.1. The dielectric measurement indicated that the transition temperature decreased linearly with increasing BZT content. The P-E loops revealed an increase in remanent polarization (Pr) with the addition of BZT. The maximum planar coupling coefficient, kappar, for the chi = 0.1 composition was 21.6 and the piezoelectric coefficient, d33, for chi = 0, chi = 0.05, and chi = 0.1 was 108, 185, and 235 pm/V, respectively. Overall, the dielectric and piezoelectric properties showed significant improvement when BZT was added.","Ferroelectric materials,
Ceramics,
Dielectric loss measurement,
Dielectric constant,
Materials science and technology,
Temperature sensors,
Solids,
Chemical technology,
Optical sensors,
Pattern analysis"
A Task Abstraction and Mapping Approach to the Shimming Problem in Scientific Workflows,"Recently, there has been an increasing need in scientific workflows to solve the shimming problem, the use of a special kind of adaptors, called shims, to link related but incompatible workflow tasks. However, existing techniques produce scientific workflows that are cluttered with many visible shims, which distract a scientist’s focus on functional components. Moreover, these techniques do not address a new type of shimming problem that occurs due to the incompatibility between the ports of a task and the inputs/outputs of its internal task component. To address these issues, 1) we propose a task template model which encapsulates the composition and mapping of shims and functional task component within a task interface; 2) we design an XML based task specification language, called TSL, to realize the proposed task template model; 3) we propose a service oriented architecture for task management to enable the distributed execution of shims and functional components; and 4) we implement the proposed model, language and architecture and present a case study to validate them. Our technique uniquely addresses both types of shimming problems. To our best knowledge, this is the first shimming technique that makes shims invisible at the workflow level, resulting in scientific workflows that are more elegant and readable.","Computer science,
Specification languages,
Acceleration,
Feeds"
An Efficient Approach for Service Process Reconfiguration in SOA with End-to-End QoS Constraints,"Using SOA, service processes can be composed statically or dynamically using services provided by different service providers. Some services may become faulty at runtime and cause the service process to violate the end-to-end quality of service (QoS) constraint. We propose an efficient approach for replacing faulty services to ensure that the reconfigured service process still meets the original end-to-end QoS constraint. We use an iterative algorithm to identify reconfiguration regions that have a small number of services, some faulty and some healthy, in order to have more service selection options. By reconfiguring services in these selected regions rather than the whole service process, the computational complexity is significantly reduced. Simulation study has shown that our approach is efficient as most service processes can be repaired by replacing only a small number of services.","Service oriented architecture,
Quality of service,
Computational modeling,
Web services,
Delay,
Algorithm design and analysis,
Business,
USA Councils,
Computer science,
Runtime"
A joint model of complex wavelet coefficients for texture retrieval,We present a Copula-based statistical model of complex wavelet coefficient magnitudes for color texture image retrieval. Our model is based on two-parameter Weibull distributions and a multivariate Student t Copula. For similarity measurement we employ a Monte- Carlo approach to approximate the Kullback-Leibler divergence between two models. The experimental retrieval results show that the incorporation of the dependency structure between subbands significantly improves retrieval accuracy compared to previous approaches.,
SeGCom: Secure Group Communication in VANETs,"In this paper, we propose a novel scheme to achieve secure, and efficient vehicular communication. In particular, SegCom provides two mechanisms to perform successive authentication of the vehicle with the road-side infrastructure units to expedite authentication for vehicle-to-infrastructure (V2I) communication. Furthermore, to enhance the efficiency of vehicle-to-vehicle (V2V) communication, SeGCom permits the vehicles to form group, which are also used for performing multi-hop V2V communication without any assistance from a trusted authority. Comparison with other existing schemes in the literature has been performed to show the efficiency and applicability of our scheme.",
On the controllability of fixed-wing perching,"The ability of birds to perch robustly and effectively is a powerful demonstration of the capabilities of nature's control systems. Their apparent robustness to gust disturbances is particularly remarkable because when the airspeed approaches zero just before acquiring a perch, the influence of aerodynamic forces, and therefore potentially the control authority, is severely compromised. In this paper we present a simplified closed-form model for a fixed-wing aircraft which closely agrees with experimental indoor perching data. We then carefully examine the LTV controllability along an optimized perching trajectory for three different actuation scenarios - a glider (no powerplant), a fixed propeller, and a propeller with thrust vectoring. The results reveal that while all three vehicles are LTV controllable along the trajectory, the additional actuators allow the perch to be more easily acquired with less control surface deflections. However, in all three cases, disturbances experienced just before reaching the perch cannot be effectively rejected.","Controllability,
Robust control,
Propellers,
Birds,
Control systems,
Aerodynamics,
Force control,
Power system modeling,
Aircraft,
Vehicles"
A fast feature extraction in object recognition using parallel processing on CPU and GPU,"Due to the advents of multi-core CPU and GPU, various parallel processing techniques have been widely applied to many application fields including computer vision. This paper presents a parallel processing technique for realtime feature extraction in object recognition by autonomous mobile robots, which utilizes both CPU and GPU by combining OpenMP, SSE (Streaming SIMD Extension) and CUDA programming. Firstly, the algorithms and codes for feature extraction are optimized and implemented in parallel processing. After the parallel algorithms are assured to maintain the same level of performance, the process for extracting key points and obtaining dominant orientation with respect to the key points is parallelized. Following the extraction is the construction of a parallel descriptor via SSE instructions. Finally, the GPU version of SIFT is also implemented using CUDA. The experiments have shown that the CPU version of SIFT is almost five times faster than the original SIFT while maintaining robust performance. Further, the GPU-Parallel descriptor achieves acceleration up to five times higher than the CPU-Parallel descriptor at a cost of a bit lower performance.","Feature extraction,
Object recognition,
Parallel processing,
Application software,
Computer vision,
Mobile robots,
Parallel programming,
Robot programming,
Parallel algorithms,
Robustness"
"Cell Contour Tracking and Data Synchronization for Real-Time, High-Accuracy Micropipette Aspiration","This paper presents an automated cell contour visual measurement technique and a data synchronization mechanism for real-time, high-accuracy mechanical characterization of individual cells with micropipette aspiration. A computer vision tracking algorithm is developed for automatically measuring cell deformation parameters in real time (30 Hz) with a resolution down to 0.21 pixel, significantly enhancing the accuracy and efficiency of micropipette aspiration. To achieve a high characterization accuracy, the cell deformations and applied pressure changes are precisely synchronized using a data synchronization mechanism. Experimental results on both solid-like cells (interstitial cells) and liquid-like cells (neutrophils) quantitatively demonstrate that the visual tracking algorithm is capable of significantly increasing the efficiency and accuracy of micropipette aspiration. Among several characterized mechanical parameters, the viscoelastic properties of porcine aortic valve interstitial cells were, for the first time, quantified in this study.","Mechanical factors,
Force measurement,
Mechanical variables measurement,
Cells (biology),
Biological cells,
Sun,
Viscosity,
Stress measurement,
Atomic measurements,
Atomic force microscopy"
Blackbox Polynomial Identity Testing for Depth 3 Circuits,"We study depth three arithmetic circuits with bounded top fanin. We give the first deterministic polynomial time blackbox identity test for depth three circuits with bounded top fanin over the field of rational numbers, thus resolving a question posed by Klivans and Spielman (STOC 2001). Our main technical result is a structural theorem for depth three circuits with bounded top fanin that compute the zero polynomial. In particular we show that if a circuit C with real coefficients is simple, minimal and computes the zero polynomial, then the rank of C can be upper bounded by a function only of the top fanin. This proves a weak form of a conjecture of Dvir and Shpilka (STOC 2005) on the structure of identically zero depth three arithmetic circuits. Our blackbox identity test follows from this structural theorem by combining it with a construction of Karnin and Shpilka (CCC 2008). Our proof of the structure theorem exploits the geometry of finite point sets in R^n. We identify the linear forms appearing in the circuit C with points in R^n. We then show how to apply high dimensional versions of the Sylvester--Gallai Theorem, a theorem from incidence-geometry, to identify a special linear form appearing in C, such that on the subspace where the linear form vanishes, C restricts to a simpler circuit computing the zero polynomial. This allows us to build an inductive argument bounding the rank of our circuit. While the utility of such theorems from incidence geometry for identity testing has been hinted at before, our proof is the first to develop the connection fully and utilize it effectively.","Circuit testing,
Polynomials,
Computer science,
Digital arithmetic,
Information geometry,
Interpolation,
Upper bound,
Galois fields,
Modular construction"
Multi-camera activity correlation analysis,"We propose a novel approach for modelling correlations between activities in a busy public space captured by multiple non-overlapping and uncalibrated cameras. In our approach, each camera view is automatically decomposed into semantic regions, across which different spatio-temporal activity patterns are observed. A novel Cross Canonical Correlation Analysis (xCCA) framework is formulated to detect and quantify temporal and causal relationships between regional activities within and across camera views. The approach accomplishes three tasks: (1) estimate the spatial and temporal topology of the camera network; (2) facilitate more robust and accurate person re-identification; (3) perform global activity modelling and video temporal segmentation by linking visual evidence collected across camera views. Our approach differs from the state of the art in that it does not rely on either intra or inter camera tracking. It therefore can be applied to even the most challenging video surveillance settings featured with severe occlusions and extremely low spatial and temporal resolutions. Its effectiveness is demonstrated using 153 hours of videos from 8 cameras installed in a busy underground station.","Cameras,
Member and Geographic Activities,
Network topology,
Layout,
Spatial resolution,
Object detection,
Robustness,
Video surveillance,
Monitoring,
Event detection"
Supporting interoperability using the Discrete-event Modeling Ontology (DeMO),"In modeling and simulation, the need for interoperability can be between simulation models or, more broadly, within simulation environments. For example, simulation of biochemical pathways for glycan biosynthesis will need access to glycomics knowledge bases such as the GlycO, EnzyO and ReactO ontologies and bioinformatics resource/databases. Traditionally, developers have studied these information sources and written custom simulation code with hardlinks into, for example, databases. Our research explores a technique which allows developers to create a conceptual model using domain ontologies, and then use alignment and mapping information between the domain ontologies and the Discrete-event Modeling Ontology (DeMO) to create DeMO instances which represent a model that conforms to a particular simulation world view. Once the DeMO instances have been created, a code generator can be used to produce an executable simulation model. This paper discusses several situations in which DeMO can support interoperability but focuses primarily on interoperability between domain ontologies and DeMO.","Ontologies,
Computational modeling,
Computer simulation,
Discrete event simulation,
OWL,
Databases,
Standards development,
Humans,
Silver,
Computer science"
A SVD-Based Method to Assess the Uniqueness and Accuracy of SPECT Geometrical Calibration,"Geometrical calibration is critical to obtaining high resolution and artifact-free reconstructed image for SPECT and CT systems. Most published calibration methods use analytical approach to determine the uniqueness condition for a specific calibration problem, and the calibration accuracy is often evaluated through empirical studies. In this work, we present a general method to assess the characteristics of both the uniqueness and the quantitative accuracy of the calibration. The method uses a singular value decomposition (SVD) based approach to analyze the Jacobian matrix from a least-square cost function for the calibration. With this method, the uniqueness of the calibration can be identified by assessing the nonsingularity of the Jacobian matrix, and the estimation accuracy of the calibration parameters can be quantified by analyzing the SVD components. A direct application of this method is that the efficacy of a calibration configuration can be quantitatively evaluated by choosing a figure-of-merit, e.g., the minimum required number of projection samplings to achieve desired calibration accuracy. The proposed method was validated with a slit-slat SPECT system through numerical simulation studies and experimental measurements with point sources and an ultra-micro hot-rod phantom. The predicted calibration accuracy from the numerical studies was confirmed by the experimental point source calibrations at ~ 0.1&nbsp;mm for both the center of rotation (COR) estimation of a rotation stage and the slit aperture position (SAP) estimation of a slit-slat collimator by an optimized system calibration protocol. The reconstructed images of a hot rod phantom showed satisfactory spatial resolution with a proper calibration and showed visible resolution degradation with artificially introduced 0.3 mm COR estimation error. The proposed method can be applied to other SPECT and CT imaging systems to analyze calibration method assessment and calibration protocol optimization.","Calibration,
Image resolution,
Image reconstruction,
Computed tomography,
Jacobian matrices,
Imaging phantoms,
Protocols,
Spatial resolution,
Singular value decomposition,
Cost function"
"A Continuous STAPLE for Scalar, Vector, and Tensor Images: An Application to DTI Analysis","The comparison of images of a patient to a reference standard may enable the identification of structural brain changes. These comparisons may involve the use of vector or tensor images (i.e., 3-D images for which each voxel can be represented as an RN vector) such as diffusion tensor images (DTI) or transformations. The recent introduction of the Log-Euclidean framework for diffeomorphisms and tensors has greatly simplified the use of these images by allowing all the computations to be performed on a vector-space. However, many sources can result in a bias in the images, including disease or imaging artifacts. In order to estimate and compensate for these sources of variability, we developed a new algorithm, called continuous STAPLE, that estimates the reference standard underlying a set of vector images. This method, based on an expectation-maximization method similar in principle to the validation method STAPLE, also estimates for each image a set of parameters characterizing their bias and variance with respect to the reference standard. We demonstrate how to use these parameters for the detection of atypical images or outliers in the population under study. We identified significant differences between the tensors of diffusion images of multiple sclerosis patients and those of control subjects in the vicinity of lesions.","Tensile stress,
Diffusion tensor imaging,
Image analysis,
Jacobian matrices,
Radiology,
Multiple sclerosis,
Roentgenium,
Hospitals,
Statistics,
Diseases"
Towards adaptive interpolative reasoning,"Fuzzy interpolative reasoning has been extensively studied due to its ability to enhance the robustness of fuzzy systems and to reduce system complexity. However, during the interpolation process, it is possible that multiple object values for a common variable are inferred which may lead to inconsistency in interpolated results. Such inconsistencies may result from defective interpolated rules or incorrect interpolative transformations. This paper presents a novel approach for identification and correction of defective rules in transformations, thereby removing the inconsistencies. In particular, an assumption-based truth maintenance system (ATMS) is used to record dependencies between reasoning results and interpolated rules, while the underlying technique that the general diagnostic engine (GDE) employs for fault localization is adapted to isolate possible faulty interpolated rules and their associated interpolative transformations. From this, an algorithm is introduced to allow for the modification of the original linear interpolation to become first-order piecewise linear. The approach is applied to a carefully chosen practical problem to illustrate the potential in strengthening the power of interpolative reasoning.","Interpolation,
Fuzzy reasoning,
Fuzzy sets,
Fuzzy systems,
Robustness,
Engines,
Piecewise linear techniques,
Piecewise linear approximation,
Extrapolation,
Artificial intelligence"
Detection and removal of chromatic moving shadows in surveillance scenarios,"Segmentation in the surveillance domain has to deal with shadows to avoid distortions when detecting moving objects. Most segmentation approaches dealing with shadow detection are typically restricted to penumbra shadows. Therefore, such techniques cannot cope well with umbra shadows. Consequently, umbra shadows are usually detected as part of moving objects. In this paper we present a novel technique based on gradient and colour models for separating chromatic moving cast shadows from detected moving objects. Firstly, both a chromatic invariant colour cone model and an invariant gradient model are built to perform automatic segmentation while detecting potential shadows. In a second step, regions corresponding to potential shadows are grouped by considering “a bluish effect” and an edge partitioning. Lastly, (i) temporal similarities between textures and (ii) spatial similarities between chrominance angle and brightness distortions are analysed for all potential shadow regions in order to finally identify umbra shadows. Unlike other approaches, our method does not make any a-priori assumptions about camera location, surface geometries, surface textures, shapes and types of shadows, objects, and background. Experimental results show the performance and accuracy of our approach in different shadowed materials and illumination conditions.","Surveillance,
Object detection,
Vehicle dynamics,
Computer vision,
Shape,
Lighting,
Layout,
Brightness,
Surface texture,
Image segmentation"
Experiments in place recognition using gist panoramas,"In this paper we investigate large scale view based localization in urban areas using panoramic images. The presented approach utilizes global gist descriptor computed for portions of panoramic images and a simple similarity measure between two panoramas, which is robust to changes in vehicle orientation, while traversing the same areas in different directions. The global gist feature [14] has been demonstrated previously to be a very effective conventional image descriptor, capturing the basic structure of different types of scenes in a very compact way. We present an extensive experimental validation of our panoramic gist approach on a large scale Street View data set of panoramic images for place recognition or topological localization.","Large-scale systems,
Image recognition,
Urban areas,
Vocabulary,
Global Positioning System,
Layout,
Visual databases,
Image databases,
Spatial databases,
Vehicles"
AdaptSens: An Adaptive Data Collection and Storage Service for Solar-Powered Sensor Networks,"In this paper, we present AdaptSens: a reliable data collection and storage system for solar-powered sensor networks. Unlike battery-operated devices, solar-powered systems have a less predictable energy supply and their ability to harvest energy depends on past spending, thereby creating incentives for adaptive matching of energy supply and demand. Our storage system is novel in its layered architecture and its incremental layer activation mechanism. AdaptSens provides a set of functions, in separate layers, such as sensory data collection, replication (to prevent failure-induced data loss), and storage balancing (to prevent depletion-induced data loss). The mechanism utilizes surplus energy when available by activating more layers, and resorts to progressively more energy-efficient (partial hibernation) modes when energy is scarce. Best reliability is achieved when all layers are active but meaningful intermediate modes allow different degrees of energy conservation. The efficacy of AdaptSens in trading off reliability for energy is tested on both an outdoor system and an indoor testbed. Evaluation results show that AdaptSens minimizes the sum of all data losses when combining the energy, storage and node failure factors.","Sensor systems,
Energy storage,
Batteries,
Computer networks,
System testing,
Load management,
Real time systems,
Computer science,
USA Councils,
Computer network reliability"
User-Agent Cooperation in Multiagent IVUS Image Segmentation,"Automated interpretation of complex images requires elaborate knowledge and model-based image analysis, but often needs interaction with an expert as well. This research describes expert interaction with a multiagent image interpretation system using only a restricted vocabulary of high-level user interactions. The aim is to minimize inter- and intra-observer variability by keeping the total number of interactions as low and simple as possible. The multiagent image interpretation system has elaborate high-level knowledge-based control over low-level image segmentation algorithms. Agents use contextual knowledge to keep the number of interactions low but, when in doubt, present the user with the most likely interpretation of the situation. The user, in turn, can correct, supplement, and/or confirm the results of image-processing agents. This is done at a very high level of abstraction such that no knowledge of the underlying segmentation methods, parameters or agent functioning is needed. High-level interaction thereby replaces more traditional contour correction methods like inserting points and/or (re)drawing contours. This makes it easier for the user to obtain good results, while inter- and intra-observer variability are kept minimal, since the image segmentation itself remains under control of image-processing agents. The system has been applied to intravascular ultrasound (IVUS) images. Experiments show that with an average of 2-3 high-level user interactions per correction, segmentation results substantially improve while the variation is greatly reduced. The achieved level of accuracy and repeatability is equivalent to that of manual drawing by an expert.","Image segmentation,
Image processing,
Biomedical imaging,
Active shape model,
Radiology,
Image analysis,
Vocabulary,
Control systems,
Ultrasonic imaging,
Knowledge based systems"
"Stochastic differential equations for modeling, estimation and identification of mobile-to-mobile communication channels","Mobile-to-mobile networks are characterized by node mobility that makes the propagation environment time varying and subject to fading. As a consequence, the statistical characteristics of the received signal vary continuously, giving rise to a Doppler power spectral density (DPSD) which varies from one observation instant to the next. The current models do not capture and track the time varying characteristics. This paper is concerned with dynamical modeling of time varying mobile-to-mobile channels, parameter estimation and identification from received signal measurements. The evolution of the propagation environment is described by stochastic differential equations, whose parameters can be determined by approximating the band-limited DPSD using the Gauss-Newton method. However, since the DPSD is not available online, we propose to use a filter-based expectation maximization algorithm and Kalman filter to estimate the channel parameters and states, respectively. The scheme results in a finite dimensional filter which only uses the first and second order statistics. The algorithm is recursive allowing the inphase and quadrature components and parameters to be estimated online from received signal measurements. The algorithms are tested using experimental data collected from moving sensor nodes in indoor and outdoor environments demonstrating the method's viability.","Stochastic processes,
Differential equations,
Communication channels,
Parameter estimation,
Recursive estimation,
Fading,
Signal processing,
Time measurement,
Newton method,
Least squares methods"
Mining for Gold Farmers: Automatic Detection of Deviant Players in MMOGs,"Gold farming refers to the illicit practice of gathering and selling virtual goods in online games for real money. Although around one million gold farmers engage in gold farming related activities, to date a systematic study of identifying gold farmers has not been done. In this paper we use data from the massively-multiplayer online role-playing game (MMORPG) EverQuest II to identify gold farmers. We perform an exploratory logistic regression analysis to identify salient descriptive statistics followed by a machine learning binary classification problem to identify a set of features for classification purposes. Given the cost associated with investigating gold farmers, we also give criteria for evaluating gold farming detection techniques, and provide suggestions for future testing and evaluation techniques.","Gold,
Computer science,
Costs,
Subscriptions,
Weapons,
Environmental economics,
Logistics,
Regression analysis,
Statistical analysis,
Machine learning"
SBC for motion assist using neural oscillator,"In this paper we propose a framework for synchronization based control (SBC) using neural oscillators for motion assist. A neural oscillator is used to accomplish synchronization and entrainment between periodic motions by the human and robot. The mutual joint torque between the human and robot is used as an external input signal for the neural oscillator, which generates the desired trajectory of a robot joint angle, so that the robot motion synchronizes with the external mutual joint torque. The validity and feasibility of the proposed method is examined from three points of view. The first is whether synchronization of action between human and robot can be realized. The second is whether the assist effect can be obtained, and the third is whether the proposed method has an acceptable level of usability for the user. We explored those three points of view by conducting computer simulations on a human-motion assist system and experiments with a joint torque sensing robot suit.","Oscillators,
Humans,
Legged locomotion,
Force control,
Motion control,
Torque,
Control systems,
Exoskeletons,
Robot sensing systems,
Service robots"
Automatic speech recognition for Bangla digits,"In this paper, we introduce a system for Bangla digit automatic speech recognition (ASR). Though Bangla is one of the largely spoken languages in the world, only a few works on Bangla ASR can be found in the literature, especially on Bangladeshi accented Bangla. In this work, the corpus is collected from natives in Bangladesh. Mel-frequency cepstral coefficients (MFCCs) based features and hidden Markov model (HMM) based classifiers are used for recognition. Experimental results show comparatively high recognition performance (more than 95%) for first six digits (0 – 5) and low performance (less than 90%) for the next four digits (6 – 9). We notice two confused pairs of digits: one with (6) and (9), and the other with (7) and (8), in the experiments. We also find that different dialects in Bangladesh have a greater role on this confusion.","Automatic speech recognition,
Hidden Markov models,
Natural languages,
Speech recognition,
Speech synthesis,
Databases,
Artificial neural networks,
Information technology,
Educational institutions,
Cepstral analysis"
Analysis of text entry performance metrics,"Researchers have proposed many text entry systems to enable users to perform this frequent task as quickly and precise as possible. Unfortunately the reported data varies widely and it is difficult to extract meaningful average entry speeds and error rates from this body of work. In this article we collect data from well-designed and well-reported experiments for the most important text entry methods, including those for handheld devices. Our survey results show that thumb keyboard is the fastest text entry method after the standard QWERTY keyboard, and that Twiddler is fastest amongst non-QWERTY methods. Moreover, we survey how text entry errors were handled in these studies. Finally, we conducted a user study to detect which effect different error-handling methodologies have on text entry performance metrics. Our study results show that the way human errors are handled has indeed a significant effect on all frequently used error metrics.","Performance analysis,
Measurement,
Error correction,
Keyboards,
Data mining,
Error analysis,
Handheld computers,
Thumb,
Computer science,
Humans"
Wyner-Ziv to H.264 video transcoder for low cost video encoding,"This paper proposes a Wyner-Ziv/H.264 transcoder that enables low cost video applications. The proposed solution supports video encoding on resource constrained devices such as disposable video cameras, network camcorders and low cost video encoders. This approach is based on reducing encoding resource requirements on a device by using Wyner-Ziv video encoding. The system shifts the burden of complexity away from the encoder, for example to a network node, where a transcoder efficiently converts WZ encoded video to H.264 by reusing the information from the WZ decoding stage. The transcoded H.264 video is requires fewer resources than WZ decoding and therefore reduces the complexity of decoding. The complexity of encoding and playback ends of video applications is thus reduced enabling new class of consumer application. The paper is focused on reducing the complexity of the macro-block mode coding decision process carried out in H.264 encoding stage of the transcoder. Based on a data mining process, the approach replaces the high complexity H.264 mode decision algorithm by a faster decision tree. The proposed architecture reduces the battery consumption of the end-user devices and the transcoding time is reduced by 86% with negligible rate-distortion loss.","Costs,
Encoding,
Decoding,
Cameras,
Video equipment,
Data mining,
Decision trees,
Batteries,
Transcoding,
Rate-distortion"
A semi-closed-form solution to optimal decentralized beamforming for two-way relay networks,"In this paper, we investigate the design of a distributed beamforming technique for two-way relay networks consisting of two transceivers and multiple relay nodes. We consider the problem of the total transmit power minimization subject to two constraints that maintain the transceivers' SNRs above given thresholds γ1 and γ2. For this problem, we obtain a semi-closed-form solution for the beamforming weight vector. Our solution is based on the observation that beamforming weight vector depends on the sum of SNR thresholds γ1 + γ2. As long as this sum does not change, the beamforming weight vector will not change. Hence, a two-way relay vector can be equivalently modeled as a one-way relay network where γ1 and γ2 are replaced with 0 and γ1 + γ2, respectively. Based on this equivalent one-way relaying scheme, the beamforming weight vector is obtained in a closed form if a certain intermediate parameter is given. This parameter is the transmit power of the source in the equivalent one-way relaying scheme and can be obtained efficiently using a simple bisection method. Once this parameter is obtained, the beamforming weight vector can be obtained in a closed form.","Array signal processing,
Relays,
Transceivers,
Wireless networks,
Quality of service,
Vectors,
Conferences,
Computer networks,
Distributed computing,
Design engineering"
Enhancing E-Learning Through Teacher Support: Two Experiences,"Teachers in e-learning play a crucial role as facilitators of the students' learning experiences. To this end, a teacher needs to monitor, understand and evaluate the activity of the students in the course. What is more, e-learning can be enhanced if tools for supporting teachers in this task are provided. In this paper, two experiences are presented to show the convenience of providing teachers with such tools. These experiences proved that providing support to teachers allowed them to assess the students more closely. As a result, the students' dropout rate in a postgraduate course was reduced and the number of students who passed a physics course in secondary education was increased.",
Fuzzy edge detection based on pixel's gradient and standard deviation values,"This paper presents a new fuzzy based edge detection algorithm. Each different edge detection method has its own advantages and disadvantages. For example each method detects part of real edges and also some unreal edges. To reduce this effect we have used two different source of information and a fuzzy system to decide about whether each pixel is edge or not. First both gradient and standard deviation values are computed, form two set of edges, utilized as inputs for our fuzzy system. Then fuzzy system decides on each pixel according to fuzzy rules. Finally we have compared results of the proposed algorithm with other algorithms such as Sobel, Robert, and Prewitt. Experimental results show the ability and high performance of proposed algorithm.",Image edge detection
Insights on Fault Interference for Programs with Multiple Bugs,"Multiple faults in a program may interact with each other in a variety of ways. A test case that fails due to a fault may not fail when another fault is added, because the second fault may mask the failure-causing effect of the first fault. Multiple faults may also collectively cause failure on a test case that does not fail due to any single fault alone. Many studies try to perform fault localization on multi-fault programs and several of them seek to match a failed test to its causative fault. It is therefore, important to better understand the interference between faults in a multi-fault program, as an improper assumption about test case failure may lead to an incorrect matching of failed test to fault, which may in turn result in poor fault localization. This paper investigates such interference and examines if one form of interference holds more often than another, and uniformly across all conditions. Empirical studies on the Siemens suite suggest that no one form of interference holds unconditionally and that observation of failure masking is a more frequent event than observation of a new test case failure.","Interference,
Computer bugs,
Software reliability,
Reliability engineering,
Computer science,
Performance evaluation,
Software testing,
Design for experiments,
Software debugging"
Combating side-channel attacks using key management,"Embedded devices are widely used in military and civilian operations. They are often unattended, publicly accessible, and thus vulnerable to physical capture. Tamper-resistant modules are popular for protecting sensitive data such as cryptographic keys in these devices. However, recent studies have shown that adversaries can effectively extract the sensitive data from tamper-resistant modules by launching semi-invasive side-channel attacks such as power analysis and laser scanning. This paper proposes an effective key management scheme to harden embedded devices against side-channel attacks. This technique leverages the bandwidth limitation of side channels and employs an effective updating mechanism to prevent the keying materials from being exposed. This technique forces attackers to launch much more expensive and invasive attacks to tamper embedded devices and also has the potential of defeating unknown semi-invasive side-channel attacks.","Hardware,
Data mining,
Wireless sensor networks,
Information security,
Laboratories,
Protection,
Power lasers,
Optical materials,
Sensor systems,
Control systems"
ID repetition in Kad,"ID uniqueness is essential in DHT-based systems as peer lookup and resource searching rely on ID-matching. Many previous works and measurements on Kad do not take into account that IDs among peers may not be unique. We observe that a significant portion of peers, 19.5% of the peers in routing tables and 4.5% of the active peers (those who respond to Kad protocol), do not have unique IDs. These repetitions would mislead the measurements of Kad network. We further observe that there are a large number of peers that frequently change their UDP ports, and there are a few IDs that repeat for a large number of times and all peers with these IDs do not respond to Kad protocol. We analyze the effects of ID repetitions under simplified settings and find that ID repetition degrades Kad's performance on publishing and searching, but has insignificant effect on lookup process. These measurement and analysis are useful in determining the sources of repetitions and are also useful in finding suitable parameters for publishing and searching.","Peer to peer computing,
Routing,
Intrusion detection,
Publishing,
Computer science,
Protocols,
Performance analysis,
Degradation,
Certification,
Crawlers"
Gaming On and Off the Social Graph: The Social Structure of Facebook Games,"Games built on Online Social Networks (OSNs) have become a phenomenon since 3rd party developer tools were released by OSNs such as Facebook. However, apart from their explosive popularity, little is known about the nature of the social networks that are built during play. In this paper, we present the findings of a network analysis study carried out on two Facebook applications, in comparison with a similar but stand-alone game. We found that games built both on and off a social graph exhibit similar social properties. Specifically, the distribution of player-to-player interactions decays as a power law with a similar exponent for the majority of players. For games built on the social network platform however, we find that the networks are characterised by a sharp cut-off, compared with the classically scale-free nature of the social network for the game not built on an existing social graph.","Facebook,
Social network services,
Explosives,
MySpace,
Computer networks,
Motion pictures,
Mood,
Time factors,
Videos,
YouTube"
Evaluating Network Security With Two-Layer Attack Graphs,"Attack graphs play important roles in analyzing network security vulnerabilities, and previous works have provided meaningful conclusions on the generation and security measurement of attack graphs. However, it is still hard for us to understand attack graphs in a large network, and few suggestions have been proposed to prevent inside malicious attackers from attacking networks. To address these problems, we propose a novel approach to generate and describe attack graphs. Firstly, we construct a two-layer attack graph, where the upper layer is a hosts access graph and the lower layer is composed of some host-pair attack graphs. Compared with previous works, our attack graph has simpler structures, and reaches the best upper bound of computation cost in O(N2). Furthermore, we introduce the adjacency matrix to efficiently evaluate network security, with overall evaluation results presented by gray scale images vividly. Thirdly, by applying prospective damage and important weight factors on key hosts with crucial resources, we can create prioritized lists of potential threatening hosts and stepping stones, both of which can help network administrators to harden network security. Analysis on computation cost shows that the upper bound computation cost of our measurement methodology is O(N3 ), which could also be completed in real time. Finally, we give some examples to show how to put our methods in practice.","Computational efficiency,
Computer security,
Upper bound,
Computer networks,
Application software,
Laboratories,
Educational technology,
Computer science education,
Software measurement,
Scalability"
Six Conjectures in Quantum Physics and Computational Neuroscience,"A paradox on Hilbert’s problem 6 is identified. To avoid the paradox, equilibrium-based YinYang bipolar sets and bipolar dynamic logic (BDL) are introduced. Bipolar quantum entanglement is defined. BDL leads to a bipolar  axiomatization for physics. Applicability of BDL is discussed. Six conjectures in quantum physics and computational neuro¬science are posted.",
3D environment reconstruction using modified color ICP algorithm by fusion of a camera and a 3D laser range finder,"In this paper, we propose a system which reconstructs the environment with both color and 3D information. We perform extrinsic calibration of a camera and a LRF (laser range finder) to fuse 3D information and color information of objects. We also formularize an equation to measure the result of the calibration. Moreover, we acquire 3D data by rotating 2D LRF with camera, and use ICP (iterative closest point) algorithm to combine data acquired in other places. We use the SIFT (scale invariant feature transform) matching for the initial estimation of ICP algorithm. It offers accurate and stable initial estimation robust to motion change compare to odometry. We also modify the ICP algorithm using color information. Computation time of ICP algorithm can be reduced by using color information.","Iterative closest point algorithm,
Cameras,
Laser fusion,
Iterative algorithms,
Calibration,
Fuses,
Equations,
Transforms,
Motion estimation,
Robustness"
Towards Text-based Emotion Detection A Survey and Possible Improvements,"This paper presents an overview of the emerging field of emotion detection from text and describes the current generation of detection methods that are usually divided into the following three main categories: keyword-based, learning-based, and hybrid recommendation approaches. Limitations of current detection methods are examined, and possible solutions are suggested to improve emotion detection capabilities in practical systems, which emphasize on human-computer interactions. These solutions include extracting keywords with semantic analysis, and ontology design with emotion theory of appraisal. Furthermore, a case-based reasoning architecture is proposed to combine these solutions.","Biology computing,
Computer science,
Psychology,
Blogs,
Application software,
Ontologies,
Computer architecture,
Information management,
Chaos,
Management information systems"
In situ image-based modeling,"We present an interactive image-based modelling method for generating 3D models within an augmented reality system. Applying real time camera tracking, and high-level automated image analysis, enables more powerful modelling interactions than have previously been possible. The result is an immersive modelling process which generates accurate three dimensional models of real objects efficiently and effectively. In demonstrating the modelling process on a range of indoor and outdoor scenes, we show the flexibility it offers in enabling augmented reality applications in previously unseen environments.","Layout,
Power system modeling,
Cameras,
Augmented reality,
Geometry,
Shape,
Image analysis,
Information analysis,
Solid modeling,
Parameter estimation"
Probabilistic occlusion boundary detection on spatio-temporal lattices,"In this paper, we present an algorithm for occlusion boundary detection. The main contribution is a probabilistic detection framework defined on spatio-temporal lattices, which enables joint analysis of image frames. For this purpose, we introduce two complementary cost functions for creating the spatio-temporal lattice and for performing global inference of the occlusion boundaries, respectively. In addition, a novel combination of low-level occlusion features is discriminatively learnt in the detection framework. Simulations on the CMU Motion Dataset provide ample evidence that proposed algorithm outperforms the leading existing methods.",
It's all in the game: Towards an affect sensitive and context aware game companion,"Robot companions must be able to display social, affective behaviour. As a prerequisite for companionship, the ability to sustain long-term interactions with users requires companions to be endowed with affect recognition abilities. This paper explores application-dependent user states in a naturalistic scenario where an iCat robot plays chess with children. In this scenario, the role of context is investigated for the modelling of user states related both to the task and the social interaction with the robot. Results show that contextual features related to the game and the iCat's behaviour are successful in helping to discriminate among the identified states. In particular, state and evolution of the game and display of facial expressions by the iCat proved to be the most significant: when the user is winning and improving in the game her feeling is more likely to be positive and when the iCat displays a facial expression during the game the user's level of engagement with the iCat is higher. These findings will provide the foundation for a rigorous design of an affect recognition system for a game companion.","Context awareness,
Robot sensing systems,
Context modeling,
Computer science,
Computer displays,
Information analysis,
Emotion recognition,
Prototypes,
Information resources,
Senior citizens"
Operation-based conflict detection and resolution,Models are in wide-spread use in the software development lifecycle and model-driven development even promotes them from an abstraction of the system to the description the system is generated from. Therefore it is increasingly important to collaborate on models. These models can range from requirements models over UML models to project management models such as schedules. Tool support for collaboration on models is therefore crucial. Traditionally software configuration management (SCM) systems such as RCS [9] or Subversion [10] have supported this task for textual artifacts such as source code on the granularity of files and textual lines. They do not work well for graph-like models with many links since the granularity needed to support them is on the level of model elements and their attributes. For the design of a novel SCM system addressing these requirements it is essential to define how conflicts on models are detected and how they can be resolved. In this paper we present an approach to conflict detection and resolution on models. We employ operation-based change tracking and therefore detect conflicts based on operations. For conflict resolution we propose an integration of SCM with techniques from rational management to effectively resolve conflicts.,
Extracting Correlations,"Motivated by applications in cryptography, we consider a generalization of randomness extraction and the related notion of privacy amplification to the case of two correlated sources. We introduce the notion of {\em correlation extractors}, which extract nearly perfect independent instances of a given joint distribution from imperfect, or ""leaky,'' instances of the same distribution. More concretely, suppose that Alice holds a
and Bob holds b
, where (a, b)
are obtained by taking n
independent samples from a joint distribution (X, Y)
and letting a
include all X
instances and b
include all Y
instances. An adversary Eve obtains partial information about (a, b)
by choosing a function L
with output length t
and learning L(a, b)
.The goal is to design a protocol between Alice and Bob which may use additional fresh randomness, such that for every L
as above the following holds. In the end of the interaction, Alice outputs a'
and Bob outputs b'
such that (a', b')
are statistically indistinguishable from m
independent instances of (X, Y)
even when conditioned on Eve's view, and {\em even when conditioned on the joint view of Eve together with either Alice or Bob}.The standard questions of privacy amplification and randomness extraction correspond to the case where X
and Y
are identical random bits. In this work we address this question for other types of correlations. A central special case is that of {\em OT extractors}, which are correlation extractors for the correlation (X, Y)
corresponding to the cryptographic primitive of oblivious transfer. Our main result is that for any finite joint distribution (X, Y)
there is an explicit correlation extractor which extracts m=\Omega(n)
instances using O(n)
bits of communication, even when t=\Omega(n)
bits of information can be leaked to Eve. We present several applications which motivate the concept of correlation extractors and our main result. These include:\begin{itemize} \item Protecting certain cryptographic protocols against side-channel attacks. \item A protocol which realizes $m$ instances of oblivious transfer by communicating only $O(m)$ bits. The security of the protocol relies on a number-theoretic intractability assumption. \item A {\em constant-rate} unconditionally secure construction of oblivious transfer (for semi-honest parties) from {\em any nontrivial channel}. This establishes constant-rate equivalence of any two nontrivial finite channels.\end{itemize}","Cryptography,
Data mining,
Computer science,
Privacy,
Cryptographic protocols,
Application software,
Protection,
Information security,
Mathematics,
Technological innovation"
Inertial parameter estimation of floating base humanoid systems using partial force sensing,"Recently, several controllers have been proposed for humanoid robots which rely on full-body dynamic models. The estimation of inertial parameters from data is a critical component for obtaining accurate models for control. However, floating base systems, such as humanoid robots, incur added challenges to this task (e.g. contact forces must be measured, contact states can change, etc.) In this work, we outline a theoretical framework for whole body inertial parameter estimation, including the unactuated floating base. Using a least squares minimization approach, conducted within the null-space of unmeasured degrees of freedom, we are able to use a partial force sensor set for full-body estimation, e.g. using only joint torque sensors, allowing for estimation when contact force measurement is unavailable or unreliable (e.g. due to slipping, rolling contacts, etc.). We also propose how to determine the theoretical minimum force sensor set for full body estimation, and discuss the practical limitations of doing so.","Parameter estimation,
Torque,
Humanoid robots,
Force sensors,
Biological system modeling,
Joints,
Force control,
Motion control,
Service robots,
Force measurement"
Verification and synthesis for secrecy in discrete-event systems,"Keeping a property of system behaviors secret from an observer (who has a partial observation of any executed behavior) requires that the execution of any property-satisfying or property-violating behavior must not become known to the observer. When an observer does not know the exact behaviors of a system it observes, a weaker notion of secrecy can be defined, which we introduce in this paper. We present an algorithm for verifying the properties of secrecy as well as its weaker version. When a given system does not possess a secrecy property, we consider restricting the behaviors of the system by means of supervisory control so as to ensure that the controlled system satisfies the desired secrecy property. We show the existence of a maximally permissive supervisor to ensure secrecy or its weaker version, and present algorithms for their synthesis.","Discrete event systems,
Feature extraction,
Data mining,
Discrete Fourier transforms,
Signal analysis,
Vehicle safety,
Wheels,
Fourier transforms,
Frequency,
Wavelet transforms"
The price of selfishness in network coding,"We introduce a game theoretic framework for studying a restricted form of network coding in a general wireless network. The network is fixed and known, and the system performance is measured as the number of wireless transmissions required to meet n unicast demands. Game theory is here employed as a tool for improving distributed network coding solutions. We propose a framework that allows each unicast session to independently adjust his routing decision in response to local information. Specifically, we model the interactions of the unicast sessions as a noncooperative game. This approach involves designing both local cost functions and decision rules for the unicast sessions so that the resulting collective behavior achieves a desirable system performance in a shared network environment. We propose a family of cost functions and compare the performance of the resulting distributed algorithms to the best performance that could be found and implemented using a centralized controller. We focus on the performance of stable solutions - where stability here refers to a form of Nash equilibrium defined below. Results include bounds on the bestand worst-case stable solutions as compared to the optimal centralized solution. Results in learning in games prove that the best-case stable solution can be learned by self-interested players with probability approaching 1.","Network coding,
Unicast,
Game theory,
System performance,
Cost function,
Wireless networks,
Routing,
Distributed algorithms,
Centralized control,
Stability"
A weakly consistent scheme for IMS presence service,"IP multimedia core network subsystem (IMS) provides presence service for universal mobile telecommunications system (UMTS). In IMS, the presence server is responsible for notifying an authorized watcher of the updated presence information. If the updates occur more frequently than the accesses of the watcher, the presence server will generate many notifications. This paper uses a weakly consistent scheme (called delayed update) to reduce the notification traffic. In this scheme, a delayed timer is defined to control the notification rate. We propose an analytic model and simulation experiments to investigate the performance of delayed update. The study indicates that delayed update can effectively reduce the notification traffic without significantly degrading the valid access probability.","3G mobile communication,
Network servers,
Traffic control,
Computer science,
Multimedia systems,
Performance analysis,
Subscriptions,
Analytical models,
Delay effects,
Communication system traffic control"
Determination of optimal call admission control policy in wireless networks,"This paper investigates the optimal call admission control (CAC) policy for non-priority scheme (NPS) and reserved channel scheme (RCS) in wireless networks, respectively. Both new call and handoff call arrival processes are assumed to be Poisson processes, and the call holding times are exponentially distributed with different rate for new call and handoff calls. Admitting each call would bring a reward to the network provider but holding each call in the system would also incur some expenses (or cost) to the provider. We concentrate on the optimization problems of when to admit or reject a call in order to achieve the maximum total expected discounted reward. By establishing a discounted semi Markov decision process (SMDP) model, we verify that the optimal policies are state-related control limit policies for both NPS and RCS. Our numerical results explained in both tables and diagrams are consistent with our theoretic results.","Call admission control,
Wireless networks,
Optimal control,
Costs,
Telecommunication control,
Quality of service,
Student members,
Process control,
Computer science,
State-space methods"
Extended maximizing unavailability interval (eMUI): maximizing energy saving in IEEE 802.16e for mixing type I and type II PSCs,"To conserve energy in IEEE 802.16e, earlier, we proposed maximizing unavailability interval (MUI) for type II power saving class (PSC). MUI is guaranteed to find the maximum unavailability interval. Therefore, energy consumption can be reduced significantly. In this paper, we further extend MUI for the mixture of Type I and Type II PSCs. The proposed eMUI still can achieve maximum unavailability interval. The simulation results show that the proposed eMUI reduces energy consumption and average packet response time.","Energy consumption,
Computer science,
Delay,
WiMAX,
Energy efficiency,
Wireless networks,
Data communication,
Downlink,
Councils,
Transceivers"
Incorporating Human Contrast Sensitivity in Model Observers for Detection Tasks,"Contrast sensitivity of the human visual system is a characteristic that can adversely affect human performance in detection tasks. In this paper, we propose a method for incorporating human contrast sensitivity in anthropomorphic model observers. In our method, we model human contrast sensitivity using the Barten model with the mean luminance of a region of interest centered at the signal location. In addition, one free parameter is varied to control the effect of the contrast sensitivity on the model observer's performance. We investigate our model of human contrast sensitivity in a channelized-Hotelling observer (CHO) with difference-of-Gaussian channels. We call the CHO incorporating the contrast sensitivity a contrast-sensitive CHO (CS-CHO). The human data from a psychophysical study by Park are used for comparing the performance of the CS-CHO to human performance. That study used Gaussian signals with six different signal intensities in non-Gaussian lumpy backgrounds. A value of the free parameter is chosen to match the performance of the CS-CHO to the mean human performance only at the strongest signal. Results show that the CS-CHO with the chosen value of the free parameter predicts the mean human performance at the five lower signal intensities. Our results show that the CS-CHO predicts human performance well as a function of signal intensity.","Humans,
Biomedical imaging,
Image quality,
Visual system,
Anthropomorphism,
Diseases,
Mathematics,
Drugs,
Psychology,
Medical signal detection"
An improved perturbation and observation MPPT method of photovoltaic generate system,"An efficient Maximum Power Point Tracking (MPPT) algorithm is important to increase the output efficiency of a photovoltaic (PV) generate system. The conventional perturbation and observation (PO) MPPT algorithm is impossible to quickly acquire the maximum power point (MPP), and the tracking course is very difficulty under veil weather conditions, and the essential reason is not known the actual values of the n and Io. It is well-known that the different solar cells have different n and Io. Theoretical and simulative results show that a piece of solar cell has same photocurrent under different n and Io conditions. A new combined perturb and observe (PO) method is described in order to acquire the actual n and I. Then an improved PO maximum power point tracking method is described which based on the actual n and Io, and the tracking bound is reduced, and the tracking speed is rapid to compare with the conventional PO method. Furthermore, it is simple and can be easily implemented in digital signal processor (DSP). The simulation results verified the correctness and validity of MPPT method.","Photovoltaic systems,
Solar power generation,
Photovoltaic cells,
Diodes,
Photoconductivity,
Power generation,
Power system modeling,
Signal processing algorithms,
Kelvin,
Computer science"
Computational multiscale modeling in protein--ligand docking,"The term multiscale modeling usually refers to solving physical problems along multiple spatial or temporal scales. This definition can also be extended to include other nonorthogonal descriptive scales, to allow a hierarchical approach to accurate and efficient problem solving. Therefore, the scales proposed in this article are not the traditional scales currently used in life sciences. We deal with computational rather than experimental multi- scales, and we extend this language to describe the problem of protein-ligand docking.","Computational modeling,
Proteins,
Biochemistry,
Cardiovascular diseases,
Cardiac disease,
Inhibitors,
Lead compounds,
Chemical compounds,
Geometry,
Biological system modeling"
Enhanced delegation-based authentication protocol for PCSs,"Lee and Yeh recently presented a delegation-based authentication protocol for portable communication systems (PCSs), which is claimed to provide non-repudiation in on-line authentication. This investigation indicates that their protocol has a weakness in that a malicious visited location register can forge the authentication messages in off-line authentication processes, preventing mobile users from obtaining non-repudiation in such processes. This study also presents an enhanced protocol, which not only has the same security properties as the original protocol but also avoids the weakness in the original scheme and reduces the computational cost.",
Analysis of Parallel Algorithms for Energy Conservation in Scalable Multicore Architectures,"This paper analyzes energy characteristics of parallel algorithms executed on scalable multicore processors. Specifically, we provide a methodology for evaluating energy scalability of parallel algorithms while satisfying performance requirements. Four parallel algorithms are analyzed to illustrate our method. We study the sensitivity of our analysis to changes in parameters such as the ratio of power required for computation versus power required for communication. The results suggest that power and performance scalability of a parallel algorithm can be quite different. Our method can be used to determine how many cores to use in order to minimize energy consumption.","Algorithm design and analysis,
Parallel algorithms,
Energy conservation,
Multicore processing,
Scalability,
Computer architecture,
Frequency,
Computer science,
Energy consumption,
Parallel processing"
Evolving novel image features using Genetic Programming-based image transforms,"In this paper, we use Genetic Programming (GP) to define a set of transforms on the space of greyscale images. The motivation is to allow an evolutionary algorithm means of transforming a set of image patterns into a more classifiable form. To this end, we introduce the notion of a transform-based evolvable feature (TEF), a moment value extracted from a GP-transformed image, used in a classification task. Unlike many previous approaches, the TEF allows the whole image space to be searched and augmented. TEFs are instantiated through Cartesian Genetic Programming, and applied to a medical image classification task, that of detecting muscular dystrophy-indicating inclusions in cell images. It is shown that the inclusion of a single TEF allows for significantly superior classification relative to predefined features alone.","Image databases,
Spatial databases,
Genetic programming,
Image classification,
Pattern recognition,
Machine learning,
Pixel,
Evolutionary computation,
Image recognition,
Measurement standards"
RankBoost with l1 regularization for facial expression recognition and intensity estimation,"Most previous facial expression analysis works only focused on expression recognition. In this paper, we propose a novel framework of facial expression analysis based on the ranking model. Different from previous works, it not only can do facial expression recognition, but also can estimate the intensity of facial expression, which is very important to further understand human emotion. Although it is hard to label expression intensity quantitatively, the ordinal relationship in temporal domain is actually a good relative measurement. Based on this observation, we convert the problem of intensity estimation to a ranking problem, which is modeled by the RankBoost. The output ranking score can be directly used for intensity estimation, and we also extend the ranking function for expression recognition. To further improve the performance, we propose to introduce l1 based regularization into the Rankboost. Experiments on the Cohn-Kanade database show that the proposed method has a promising performance compared to the state-of-the-art.","Face recognition,
Pattern recognition,
Pattern analysis,
Humans,
Psychology,
Gold,
Computer science,
Laboratories,
Emotion recognition,
Image databases"
A Telehealth Architecture for Networked Embedded Systems: A Case Study in In Vivo Health Monitoring,"The improvement in processor performance through continuous breakthroughs in transistor technology has resulted in the proliferation of lightweight embedded systems. Advances in wireless technology and embedded systems have enabled remote healthcare and telemedicine. While medical examinations could previously extract only localized symptoms through snapshots, now continuous monitoring can discretely analyze how a patient's lifestyle affects his/her physiological conditions and if additional symptoms occur under various stimuli. We demonstrate how medical applications in particular benefit from a hierarchical networking scheme that will improve the quantity and quality of ubiquitous data collection. Our Telehealth networking infrastructure provides flexibility in terms of functionality and the type of applications that it supports. We specifically present a case study that demonstrates the effectiveness of our networked embedded infrastructure in an in vivo pressure application. Experimental results of the in vivo system demonstrate how it can wirelessly transmit pressure readings measuring from 0 to 1.5 lbf/in2 with an accuracy of 0.02 lbf/in2. The challenges in biocompatible packaging, transducer drift, power management, and in vivo signal transmission are also discussed. This research brings researchers a step closer to continuous, real-time systemic monitoring that will allow one to analyze the dynamic human physiology.","Embedded system,
In vivo,
Medical services,
Biomedical monitoring,
Transistors,
Telemedicine,
Patient monitoring,
Condition monitoring,
Remote monitoring,
Biomedical equipment"
Safe-commit analysis to facilitate team software development,"Software development teams exchange source code in shared repositories. These repositories are kept consistent by having developers follow a commit policy, such as “Program edits can be committed only if all available tests succeed.” Such policies may result in long intervals between commits, increasing the likelihood of duplicative development and merge conflicts. Furthermore, commit policies are generally not automatically enforceable. We present a program analysis to identify committable changes that can be released early, without causing failures of existing tests, even in the presence of failing tests in a developer's local workspace. The algorithm can support relaxed commit policies that allow early release of changes, reducing the potential for merge conflicts. In experiments using several versions of a non-trivial software system with failing tests, 3 newly enabled commit policies were shown to allow a significant percentage of changes to be committed.",
Performance Comparison of Secure Comparison Protocols,"Secure Multiparty Computation (SMC) has gained tremendous importance with the growth of the Internet and E-commerce, where mutually untrusted parties need to jointly compute a function of their private inputs. However, SMC protocols usually have very high computational complexities, rendering them practically unusable. In this paper, we tackle the problem of comparing two input values in a secure distributed fashion. We propose efficient secure comparison protocols for both the homomorphic encryption and secret sharing schemes. We also give experimental results to show their practical relevance.","Protocols,
Cryptography,
Sliding mode control,
Arithmetic,
Databases,
Expert systems,
Application software,
Mathematics,
Computer science,
Internet"
Physical layer security with artificial noise: Secrecy capacity and optimal power allocation,"We consider the problem of secure communication in wireless fading channels in the presence of non-colluding passive eavesdroppers. The transmitter has multiple antennas and is able to simultaneously transmit an information bearing signal to the intended receiver and artificial noise to the eavesdroppers. We obtain an analytical closed-form lower bound for secrecy capacity, which is used as the objective function to optimize transmit power allocation between the information signal and the artificial noise. Our analytical and numerical results show that equal power allocation is a simple and generic strategy which achieves near optimal capacity performance. We also find that adaptive power allocation based on each channel realization provides no or insignificant capacity improvement over equal power allocation.","Physical layer,
Information security,
Communication system security,
Wireless communication,
Fading,
Transmitters,
Receiving antennas,
Transmitting antennas,
Information analysis,
Signal analysis"
Bandwidth efficient multicast routing in multi-channel multi-radio wireless mesh networks,"Multi-channel multi-radio (MCMR) wireless mesh networking is an emerging technology that enables high-throughput networking capability using multiple channels and multiple radios per mesh router. Traditional multicast routing algorithms such as shortest path trees and minimum Steiner trees do not consider the wireless broadcast advantage or the underlying channel assignments (i.e., channel diversity) in a MCMR wireless mesh network (WMN). In this paper, we propose a multicast routing algorithm for MCMR WMNs that takes into account the wireless broadcast advantage and channel diversity in order to minimize the amount of network bandwidth consumed by the routing tree. The algorithm does so by minimizing the number of transmissions required to deliver one packet from the source to all the destinations of a multicast group. Experimental results show that the proposed algorithm constructs routing trees having the least number of transmissions when compared with traditional trees such as shortest path trees, minimum Steiner trees, and minimum number of forwarders trees.","Bandwidth,
Routing,
Wireless mesh networks,
Multicast algorithms,
Throughput,
Broadcasting,
Spread spectrum communication,
Spine,
Aggregates,
Computer science"
Interference alignment and the generalized degrees of freedom of the X channel,"We study the sum capacity of the X channel generalization of the symmetric 2-user interference channel. In this X channel, there are 4 independent messages, one from each transmitter to each receiver. We characterize the sum capacity of a deterministic version of this channel, and obtain the generalized degrees of freedom characterization for the Gaussian version. The regime where the X channel outperforms the underlying interference channel is explicitly identified, and an interesting interference alignment scheme based on a cyclic decomposition of the signal space is shown to be optimal in this regime.","Interference channels,
Transmitters,
Wireless networks,
Computer science,
Signal processing,
Linear code,
Gaussian channels,
Equations"
A Novel Data Dissemination Method for Vehicular Networks with Rateless Codes,"Overcoming problems associated with network dynamicity and unreliable channels has been a challenge for data dissemination protocols in vehicular networks. In this paper, we present an overview on the most interesting solutions that have been proposed to perform data dissemination in this environment. Starting from this analysis, we present a novel approach that can efficiently address a reliable communication even in high dynamic networks. The new approach is based on the exploitation of a peculiar characteristic of rateless codes. In particular, the proposed method uses the orthogonality of the encoded sets of symbols generated by different random seeds. In this way, portions of the information can be disseminated even if this has not been decoded yet. In fact, an easy management of the communication of these sets among nodes enhances the reliability of the communication as well as the speed of the information dissemination. In this work, we present the idea of this innovative approach and we provide results that show the advantages of using it over other solutions.","Electronic mail,
Protocols,
Computer science,
Decoding,
Telecommunication network reliability,
Vehicle safety,
Road safety,
Communications Society,
Production systems,
Application software"
Use of Student Experiments for Teaching Embedded Software Development Including HW/SW Co-Design,"Embedded systems have been applied widely, not only to consumer products and industrial machines, but also to new applications such as ubiquitous or sensor networking. The increasing role of software (SW) in embedded system development has caused a great demand for embedded SW engineers, and university education for embedded SW engineering has become important. The embedded software engineers should learn system architecture design and hardware (HW) technologies as well as SW technologies. However, only a few universities offer education courses for embedded software engineering that include system architecture design and HW technologies. This paper proposes a student experiment method that is designed to nurture embedded SW engineers by teaching these technologies. The proposed method includes an experiment for embedded SW development, an experiment for embedded HW development and a HW/SW co-design experiment, which help students learn system architecture design skills such as system modeling, HW/SW tradeoff design, and SW and HW module design. A model of each experiment was developed and evaluated.","Education,
Embedded system,
Field programmable gate arrays,
Computer architecture,
Transform coding,
Discrete cosine transforms,
Student experiments"
Digital image encryption algorithm based on chaos and improved DES,"In recent years, encryption technology has been developed quickly and many image encryption methods have been put forward. Chaos based image encryption technique is a new encryption technique for images. It utilizes chaos random sequence to encrypt image, which is an efficient way to deal with the intractable problem of fast and highly secure image encryption. However, the Chaos based image encryption technique has some deficiencies, such as the limited accuracy problem. This paper researches on the chaotic encryption, DES encryption and a combination of image encryption algorithm, and simulate these algorithms, through analysis of the algorithm to find the gaps. And on this basis, the algorithm has been improved. The new encryption scheme realizes the digital image encryption through the chaos and improving DES. Firstly, new encryption scheme uses the Logistic chaos sequencer to make the pseudo-random sequence, carries on the RGB with this sequence to the image chaotically, then makes double time encryptions with improvement DES, displays they respective merit. Theoretical analysis and the simulation indicate that this plan has the high starting value sensitivity, and enjoys high security and the encryption speed. In addition it also keeps the neighboring RGB relevance close to zero. The algorithm can be used in the actual image encryption.","Digital images,
Cryptography,
Chaos,
Analytical models,
Random sequences,
Image analysis,
Algorithm design and analysis,
Logistics,
Displays,
Security"
Execution leases: A hardware-supported mechanism for enforcing strong non-interference,"High assurance systems such as those found in aircraft controls and the financial industry are often required to handle a mix of tasks where some are niceties (such as the control of media for entertainment, or supporting a remote monitoring interface) while others are absolutely critical (such as the control of safety mechanisms, or maintaining the secrecy of a root key). While special purpose languages, careful code reviews, and automated theorem proving can be used to help mitigate the risk of combining these operations onto a single machine, it is difficult to say if any of these techniques are truly complete because they all assume a simplified model of computation far different from an actual processor implementation both in functionality and timing. In this paper we propose a new method for creating architectures that both (a) makes the complete information-flow properties of the machine fully explicit and available to the programmer and (b) allows those properties to be verified all the way down to the gate-level implementation the design. The core of our contribution is a new call-and-return mechanism, Execution Leases, that allows regions of execution to be tightly quarantined and their side effects to be tightly bounded. Because information can flow through untrusted program counters, stack pointer or other global processor state, these and other states are leased to untrusted environments with an architectural bound on both the time and memory that will be accessible to the untrusted code. We demonstrate through a set of novel micro-architectural modifications that these leases can be enforced precisely enough to form the basis for information-flow bounded function calls, table lookups, and mixed-trust execution. Our novel architecture is a significant improvement in both flexibility and performance over the initial Gate-Level Information Flow Tracking architectures, and we demonstrate the effectiveness of the resulting design through the development of a new language, compiler, ISA, and synthesizable prototype.","Automatic control,
Control systems,
Aerospace control,
Aerospace industry,
Electrical equipment industry,
Industrial control,
Remote monitoring,
Air safety,
Computational modeling,
Timing"
A new channel hopping MAC protocol for mobile ad hoc networks,"Although multiple channels are supported in the physical layer, the IEEE 802.11 MAC layer mechanism is designed for using a single channel. Exploiting multiple channels enhances spatial reuse and reduces transmission collisions and thus improves network throughput. Designing a multi-channel MAC protocol is much more difficult than designing a singlechannel one. New challenges, such as the channel allocation problem and the missing receiver problem, must be overcome. Existing multi-channel MAC protocols suffer from either higher hardware cost (because of applying multiple transceivers) or lower channel utilization (due to limited transmission opportunity). In this paper, a fully distributed channel hopping solution, the Cyclic-Quorum-based Multi-channel (CQM) MAC protocol, is proposed. We use the cyclic quorum in a novel way and the proposed protocol has several attractive features. First, only a single transceiver is needed for each node. Second, any sender is guaranteed to meet its receiver in a short time. Third, each node's channel hopping sequence is derived from its node ID. This avoids exchanging control messages, such as each node's hopping sequence or available channel list. Fourth, multiple transmission pairs can accomplish handshaking simultaneously. The proposed protocol is simple and efficient. Simulations results verify that our mechanism is a promising multi-channel MAC protocol for mobile ad hoc networks.","Media Access Protocol,
Mobile ad hoc networks,
Transceivers,
Switches,
Channel allocation,
Costs,
Throughput,
Hardware,
Data communication,
Chaos"
A comparison of MIP-based decomposition techniques and VNS approaches for batch scheduling problems,"This research is motivated by a scheduling problem found in the diffusion and oxidation areas of semiconductor wafer fabrication facilities. With respect to some practical motivated process constraints, like equipment dedication and unequal batch-sizes, we model the problem as unrelated parallel batch machines problem with incompatible job families and unequal ready times of the jobs. Our objective is to minimize the total weighted tardiness (TWT) of the jobs. Given that the problem is NP-hard, we propose two different solution approaches. The first approach works with a time window-based mixed integer programming (MIP) decomposition. The second approach uses a variable neighbourhood search (VNS). Using randomly generated test instances, we show that the proposed algorithms outperform common dispatching rules that cannot deal with the given constraints effectively.","Job shop scheduling,
Dispatching,
Oxidation,
Manufacturing,
Furnaces,
Electronics packaging,
Laboratories,
Fabrication,
Semiconductor device modeling,
Linear programming"
Span Programs and Quantum Query Complexity: The General Adversary Bound Is Nearly Tight for Every Boolean Function,"The general adversary bound is a semi-definite program (SDP) that lower-bounds the quantum query complexity of a function. We turn this lower bound into an upper bound, by giving a quantum walk algorithm based on the dual SDP that has query complexity at most the general adversary bound, up to a logarithmic factor. In more detail, the proof has two steps, each based on ""span programs,"" a certain linear-algebraic model of computation. First, we give an SDP that outputs for any boolean function a span program computing it that has optimal ""witness size."" The optimal witness size is shown to coincide with the general adversary lower bound. Second, we give a quantum algorithm for evaluating span programs with only a logarithmic query overhead on the witness size. The first result is motivated by a quantum algorithm for evaluating composed span programs. The algorithm is known to be optimal for evaluating a large class of formulas. The allowed gates include all constant-size functions for which there is an optimal span program. So far, good span programs have been found in an ad hoc manner, and the SDP automates this procedure. Surprisingly, the SDP's value equals the general adversary bound. A corollary is an optimal quantum algorithm for evaluating ""balanced"" formulas over any finite boolean gate set. The second result extends span programs' applicability beyond the formula-evaluation problem. We extend the analysis of the quantum algorithm for evaluating span programs. The previous analysis shows that a corresponding bipartite graph has a large spectral gap, but only works when applied to the composition of constant-size span programs. We show generally that properties of eigenvalue-zero eigenvectors in fact imply an ""effective"" spectral gap around zero. A strong universality result for span programs follows. A good quantum query algorithm for a problem implies a good span program, and vice versa. Although nearly tight, this equivalence is nontrivial. Span programs are a promising model for developing more quantum algorithms.","Boolean functions,
Quantum computing,
Computer science,
Computational modeling,
Upper bound,
Algorithm design and analysis,
Bipartite graph"
Mixed reality simulation for mobile robots,"Mobile robots are increasingly entering the real and complex world of humans in ways that necessitate a high degree of interaction and cooperation between human and robot. Complex simulation models, expensive hardware setup, and a highly controlled environment are often required during various stages of robot development. There is a need for robot developers to have a more flexible approach for conducting experiments and to obtain a better understanding of how robots perceive the world. Mixed Reality (MR) presents a world where real and virtual elements co-exist. By merging the real and the virtual in the creation of an MR simulation environment, more insight into the robot behaviour can be gained, e.g. internal robot information can be visualised, and cheaper and safer testing scenarios can be created by making interactions between physical and virtual objects possible. Robot developers are free to introduce virtual objects in an MR simulation environment for evaluating their systems and obtain a coherent display of visual feedback and realistic simulation results. We illustrate our ideas using an MR simulation tool constructed based on the 3D robot simulator Gazebo.",
EOG and EMG based virtual keyboard: A brain-computer interface,"This paper discusses a brain-computer interface through electrooculogram (EOG) and electromyogram (EMG) signals. In situations of disease or trauma, there may be inability to communicate with others through means such as speech or typing. Eye movement tends to be one of the last remaining active muscle capabilities for people with neurodegenerative disorders, such as amyotrophic lateral sclerosis (ALS) also known as Lou Gehrig's disease. Thus, there is a need for eye movement based systems to enable communication. To meet this need, we proposed a system to accept eye-gaze controlled navigation of a particular letter and EMG based click to enter the letter. Eye -gaze direction (angle) is obtained from EOG signals and EMG signal is recorded from eyebrow muscle activity. A virtual screen keyboard may be used to examine the usability of the proposed system.","Electrooculography,
Electromyography,
Keyboards,
Brain computer interfaces,
Diseases,
Muscles,
Speech,
Communication system control,
Control systems,
Navigation"
Analyzing the Evolution of the Source Code Vocabulary,"Source code is a mixed software artifact, containing information for both the compiler and the developers. While programming language grammar dictates how the source code is written, developers have a lot of freedom in writing identifiers and comments. These are intentional in nature and become means of communication between developers.The goal of this paper is to analyze how the source code vocabulary changes during evolution, through an exploratory study of two software systems. Specifically, we collected data to answer a set of questions about the vocabulary evolution, such as: How does the size of the source code vocabulary evolve over time? What do most frequent terms refer to? Are new identifiers introducing new terms? Are there terms shared between different types of identifiers and comments? Are new and deleted terms in a type of identifiers mirrored in other types of identifiers or in comments?","Vocabulary,
Software maintenance,
Writing,
Software systems,
Information analysis,
Computer science,
Computer languages,
Knowledge management,
Guidelines,
Software engineering"
Prioritizing Legal Requirements,"Requirements prioritization is used in the early phases of software development to determine the order in which requirements should be implemented. Requirements are not all equally important to the final software system because time constraints, expense, and design can each raise the urgency of implementing some requirements before others. Laws and regulations can make requirements prioritization particularly challenging due to the high costs of noncompliance and the substantial amount of domain knowledge needed to make prioritization decisions. In the context of legal requirements, implementation order ideally should be influenced by the laws and regulations governing a given software system. In this paper, we present a prioritization technique for legal requirements. We apply our technique on a set of 63 functional requirements for an open-source electronic health records system that must comply with the U.S. Health Insurance Portability and Accountability Act.","Law,
Legal factors,
Software systems,
Open source software,
Insurance,
Privacy,
Medical services,
Computer science,
Programming,
Time factors"
Performance analysis of Optical Packet Switches enhanced with electronic buffering,"Optical networks with Wavelength Division Multiplexing (WDM), especially Optical Packet Switching (OPS) networks, have attracted much attention in recent years. However, OPS is still not yet ready for deployment, which is mainly because of its high packet loss ratio at the switching nodes. Since it is very difficult to reduce the loss ratio to an acceptable level by only using all-optical methods, in this paper, we propose a new type of optical switching scheme for OPS which combines optical switching with electronic buffering. In the proposed scheme, the arrived packets that do not cause contentions are switched to the output fibers directly; other packets are switched to shared receivers and converted to electronic signals and will be stored in the buffer until being sent out by shared transmitters. We focus on performance analysis of the switch, and with both analytical models and simulations, we show that to dramatically improve the performance of the switch, for example, reducing the packet loss ratio from 10−2 to close to 10−6, very few receivers and transmitters are needed to be added to the switch. Therefore, we believe that the proposed switching scheme can greatly improve the practicability of OPS networks.","Performance analysis,
Optical packet switching,
Optical buffering,
Switches,
Optical transmitters,
Optical receivers,
Packet switching,
Wavelength division multiplexing,
Optical losses,
Buffer storage"
Enhanced EMG signal processing for simultaneous and proportional myoelectric control,"A new signal processing scheme is presented for extracting neural control information from the multi-channel surface electromyographic signal (sEMG). The extracted information can be used to proportionally control a multi-degree of freedom (DOF) prosthesis. Four time-domain (TD) features were extracted from the multi-channel sEMG during a series of anisotonic, isometric wrist contractions, which involved simultaneous activations of the three DOF of the wrist. The forces produced at the three wrist DOFs during these contractions were also collected using a customized force sensor. The extracted features and the recorded force signals, as input/target pairs, were then used to train a multilayer perceptron (MLP) neural network. A five-fold cross-validation training/testing method was applied. The resulting performance is a significant improvement over a previously proposed sEMG processing method for the proportional, multi-DOF myoelectric control task.","Electromyography,
Signal processing,
Proportional control,
Data mining,
Wrist,
Feature extraction,
Force sensors,
Neural prosthesis,
Time domain analysis,
Multilayer perceptrons"
A Delaunay triangulation approach for segmenting clumps of nuclei,"Cell-based fluorescence imaging assays have the potential to generate massive amount of data, which requires detailed quantitative analysis. Often, as a result of fixation, labeled nuclei overlap and create a clump of cells. However, it is important to quantify phenotypic read out on a cell-by-cell basis. In this paper, we propose a novel method for decomposing clumps of nuclei using high-level geometric constraints that are derived from low-level features of maximum curvature computed along the contour of each clump. Points of maximum curvature are used as vertices for Delaunay triangulation (DT), which provides a set of edge hypotheses for decomposing a clump of nuclei. Each hypothesis is subsequently tested against a constraint satisfaction network for a near optimum decomposition. The proposed method is compared with other traditional techniques such as the watershed method with/without markers. The experimental results show that our approach can overcome the deficiencies of the traditional methods and is very effective in separating severely touching nuclei.","Cells (biology),
Image segmentation,
Laboratories,
Computer science,
Nuclear electronics,
Voting,
Fluorescence,
Image analysis,
Testing,
Biosensors"
Which landmark is useful? Learning selection policies for navigation in unknown environments,"In general, a mobile robot that operates in unknown environments has to maintain a map and has to determine its own location given the map. This introduces significant computational and memory constraints for most autonomous systems, especially for lightweight robots such as humanoids or flying vehicles. In this paper, we present a novel approach for learning a landmark selection policy that allows a robot to discard landmarks that are not valuable for its current navigation task. This enables the robot to reduce the computational burden and to carry out its task more efficiently by maintaining only the important landmarks. Our approach applies an unscented Kalman filter for addressing the simultaneous localization and mapping problems and uses Monte-Carlo reinforcement learning to obtain the selection policy. Based on real world and simulation experiments, we show that the learned policies allow for efficient robot navigation and outperform handcrafted strategies. We furthermore demonstrate that the learned policies are not only usable in a specific scenario but can also be generalized towards environments with varying properties.",
Multiobjective quantum-inspired evolutionary algorithm for fuzzy path planning of mobile robot,"This paper proposes a multiobjective quantum-inspired evolutionary algorithm (MQEA) to design efficient fuzzy path planner of mobil robot. MQEA employs the probabilistic mechanism inspired by the concept and principles of quantum computing. As the probabilistic individuals are updated by referring to nondominated solutions in the archive, population converges to Pareto-optimal solution set. In order to evaluate the performance of proposed MQEA, robot soccer system is utilized as a mobile robot system. Three objectives such as elapsed time, heading direction and posture angle errors are designed to obtain robust fuzzy path planner in the robot soccer system. Simulation results show the effectiveness of the proposed MQEA from the viewpoint of the proximity to the Pareto-optimal set. Moreover, various trajectories by the obtained solutions from the proposed MQEA are shown to verify the performance and to see its applicability.",
An empirical study of the effects of personality in pair programming using the five-factor model,"Pair Programming (PP) has been long researched in industry and academia. Although research evidence about its usefulness is somewhat inconclusive, previous studies showed that its use in an academic environment can benefit students in programming and design courses. In our study, we investigated the “human” aspect of PP; in particular the effects that personality attributes may have on PP's effectiveness as a pedagogical tool. We conducted a formal experiment at the University of Auckland to investigate the influence of personality differences among paired students using the Five-Factor Model as a personality measurement framework. The aim of our study was to improve the implementation of PP as a pedagogical tool through understanding the impact the variation in the personality profile of paired students has towards their academic performance. Our findings showed that differences in personality traits did not significantly affect the academic performance of students who pair programmed.","Software engineering,
Computer science,
Humans,
Programming profession,
Psychology,
Software measurement,
Navigation,
Electric variables measurement,
Engineering management,
Computer industry"
Kalman filter-based channel estimation for amplify and forward relay communications,"We propose an autoregressive model for the combined amplify and forward time-varying relay channel and derive a causal iterative channel estimation method using Kalman filter. This formulation enables us to study and compare two widely-used pilot transmission strategies in terms of the channel estimation errors. We provide a single-letter formula for the power allocation between the source and relay to achieve near optimal bit error rate (BER) performance in dual-hop communications. For cooperative communications, we show that the relay speed has a significant impact on the BER performance, and hence is important to be considered in practical system design.","Kalman filters,
Channel estimation,
Relays,
Power system relaying,
Bit error rate,
Tin,
Time division multiplexing,
Wireless communication,
Fading,
Educational institutions"
A Floating-Point Unit for 4D Vector Inner Product with Reduced Latency,"This paper presents the algorithm and implementation of a new high-performance functional unit for floating-point four-dimensional vector inner product (4D dot product; DP4), which is most frequently performed in 3D graphics application. The proposed IEEE-compliant DP4 unit computes Z = AB + CD + EF + GH in one path and keeps the intermediate rounding by IEEE-754 rounding to nearest even. The intermediate rounding is merged with shift alignment, and intermediate carry-propagated addition and normalization are omitted to reduce latency in the proposed architecture. The proposed DP4 unit is implemented with 0.18-mum CMOS technology and has 12.8-ns critical path delay, which is reduced by 45.5 percent compared to a previous DP4 implementation using discrete multipliers and adders. The proposed DP4 unit also reduces the cycle time of 3D graphics applications by 12.4 percent on the average compared to the usual 3D graphics FPU based on four-way multiply-add-fused units.","Adders,
Graphics,
Three dimensional displays,
Delay,
Computer architecture,
Hardware,
Merging"
Secrecy capacity region of Gaussian broadcast channel,"In this paper, we first consider a scenario where a source node wishes to broadcast two confidential messages for two respective receivers, while a wire-taper also receives the transmitted signal. We assume that the signals are transmitted over additive white Gaussian noise channels. We characterize the secrecy capacity region of this channel. Our achievable coding scheme is based on superposition coding and the random binning. We refer to this scheme as Secret Superposition Coding. The converse proof combines the converse proof for the conventional Gaussian broadcast channel and the perfect secrecy constraint. This capacity region matches the capacity region of the broadcast channel without security constraint. It also matches the secrecy capacity of the wire-tap channel. Based on the rate characterization of the secure Gaussian broadcast channel, we then use a multilevel coding approach for the slowly fading wire-tap. We assume that the transmitter only knows the eavesdropper's channel. In this approach, source node sends secure layered coding and the receiver viewed as a continuum ordered users. We derive optimum power allocation for the layers which maximizes the total average rate.","Broadcasting,
Fading,
Transmitters,
Security,
Degradation,
Laboratories,
Additive white noise,
Communication systems,
Councils,
Entropy"
A Traffic Engineering Approach for Placement and Selection of Network Services,"Network services are provided by means of dedicated service gateways, through which traffic flows are directed. Existing work on service gateway placement has been primarily focused on minimizing the length of the routes through these gateways. Only limited attention has been paid to the effect these routes have on overall network performance. We propose a novel approach for the service placement problem, which takes into account traffic engineering considerations. Rather than trying to minimize the length of the traffic flow routes, we take advantage of these routes in order to enhance the overall network performance. We divide the problem into two subproblems: finding the best location for each service gateway, and selecting the best service gateway for each flow. We propose efficient algorithms for both problems and study their performance. Our main contribution is showing that placement and selection of network services can be used as effective tools for traffic engineering.","Telecommunication traffic,
Communication system traffic control,
Reliability engineering,
Routing,
Computer science,
Performance loss,
Web and internet services,
Video compression,
Access protocols,
Communication system control"
Switching Supervisory Control Using Calibrated Forecasts,"In this paper, we approach supervisory control as an online decision problem. In particular, we introduce ldquocalibrated forecastsrdquo as a mechanism for controller selection in supervisory control. The forecasted quantity is a candidate controller's performance level, or reward, over finite implementation horizon. Controller selection is based on using the controller with the maximum calibrated forecast of the reward. The proposed supervisor does not perform a pre-routed search of candidate controllers and does not require the presence of exogenous inputs for excitation or identification. Assuming the existence of a stabilizing controller within the set of candidate controllers, we show that under the proposed supervisory controller, the output of the system remains bounded for any bounded disturbance, even if the disturbance is chosen in an adversarial manner. The use of calibrated forecasts enables one to establish overall performance guarantees for the supervisory scheme even though non-stabilizing controllers may be persistently selected by the supervisor because of the effects of initial conditions, exogenous disturbances, or random selection. The main results are obtained for a general class of system dynamics and specialized to linear systems.",
Tunable features of magnetoelectric transformers,"We have found that magnetostrictive FeBSiC alloy ribbons laminated with piezoelectric Pb(Zr,Ti)O3 fiber can act as a tunable transformer when driven under resonant conditions. These composites were also found to exhibit the strongest resonant magnetoelectric voltage coefficient of 750 V/ cm-Oe. The tunable features were achieved by applying small dc magnetic biases of -5 les Hdc les 5 Oe. The features include 1) a high voltage gain of -55 les Vgain les 55; and 2) a large current-to-voltage conversion of -2000 les alphaI-V les 2000 (V/A). The tunable transformer features can be attributed to large changes in the piezomagnetic coefficient and permeability of the magnetostrictive phase under Hdc.","Transformers,
Magnetic resonance,
Magnetostriction,
Permeability"
Processing Area Queries in Wireless Sensor Networks,"Area query processing is significant for various applications of wireless sensor networks. No previous study has specifically addressed this issue. We can adopt a naive method, which is to send all data to Base Station for centralized processing. However, this method wastes a large amount of energy for reporting useless data. This motivates us to propose an energy-efficient in-network area query processing scheme. In our scheme, the whole monitored area is partitioned into grids, and a gray code is used to represent a Grid ID (GID), which is a smart way to describe an area. Furthermore, a reporting tree is constructed to process merging areas and aggregations. Based on the properties of GIDs, useless data can be dropped and areas can be merged as early as possible. Incremental update is used to continuously generate query results. In essence, all of these strategies are pivots to conserve energy consumption. With a thorough simulation study, it is shown that our scheme is energy-efficient.","Wireless sensor networks,
Query processing,
Monitoring,
Computer science,
Energy efficiency,
Sensor phenomena and characterization,
Mobile computing,
Base stations,
Reflective binary codes,
Merging"
An Active Pixel CMOS separable transform image sensor,"This paper presents a 128×128 charge-mode CMOS imaging sensor that computes separable transforms directly on the focal plane. The pixel is a unique extension of the widely reported Active Pixel Sensor (APS) cell. By capacitively coupling across an array of such cells onto switched capacitor circuits, computation of any unitary 2-D transform that is separable into inner and outer products is possible. This includes the Walsh, Hadamard and Haar basis functions. This scheme offers several advantages including multiresolution imaging, inherent de-noising, compressive sampling and lower integration voltage and faster readout. The chip was implemented on a 0:5µm CMOS process and measures 9mm2 in MOSIS' submicron design rules.","Pixel,
CMOS image sensors,
Image sensors,
Charge-coupled image sensors,
Coupling circuits,
Switched capacitor circuits,
Image resolution,
Noise reduction,
Sampling methods,
Voltage"
Evaluating Retraining Rules for Semi-Supervised Learning in Neural Network Based Cursive Word Recognition,"Training a system to recognize handwritten words is a task that requires a large amount of data with their correct transcription. However, the creation of such a training set, including the generation of the ground truth, is tedious and costly. One way of reducing the high cost of labeled training data acquisition is to exploit unlabeled data, which can be gathered easily. Making use of both labeled and unlabeled data is known as semi-supervised learning. One of the most general versions of semi-supervised learning is self-training, where a recognizer iteratively retrains itself on its own output on new, unlabeled data. In this paper we propose to apply semi-supervised learning, and in particular self-training, to the problem of cursive, handwritten word recognition. The special focus of the paper is on retraining rules that define what data are actually being used in the retraining phase. In a series of experiments it is shown that the performance of a neural network based recognizer can be significantly improved through the use of unlabeled data and self-training if appropriate retraining rules are applied.","Semisupervised learning,
Neural networks,
Handwriting recognition,
Text recognition,
Pattern recognition,
Text analysis,
Computer science,
Mathematics,
Costs,
Training data"
A combinatorial approach to building navigation graphs for dynamic web applications,"Modeling the navigation structure of a dynamic web application is a challenging task because of the presence of dynamic pages. In particular, there are two problems to be dealt with: (1) the page explosion problem, i.e., the number of dynamic pages may be huge or even infinite; and (2) the request generation problem, i.e., many dynamic pages may not be reached unless appropriate user requests are supplied. As a user request typically consists of multiple parameter values, the request generation problem can be further divided into two problems: (1) How to select appropriate values for individual parameters? (2) How to effectively combine individual parameter values to generate requests? This paper presents a combinatorial approach to building a navigation graph. The novelty of our approach is two-fold. First, we use an abstraction scheme to control the page explosion problem. In this scheme, pages that are likely to have the same navigation behavior are grouped together, and are represented as a single node in a navigation graph. Grouping pages reduces and bounds the size of a navigation graph for practical applications. Second, assuming that values of individual parameters are supplied by using other techniques or generated manually by the user, we combine parameter values in a way that achieves a well-defined combinatorial coverage called pairwise coverage. Using pairwise coverage can significantly reduce the number of requests that have to be submitted while still achieving effective coverage of the navigation structure. We report a prototype tool called Tansuo, and apply the tool to five open source web applications. Our empirical results indicate that Tansuo can efficiently generate web navigation graphs for these applications.","Navigation,
Explosions,
Testing,
Web pages,
HTML,
Uniform resource locators,
Application software,
Computer science,
Information technology,
Laboratories"
Effective Presentation Technique of Scent Using Small Ejection Quantities of Odor,"Trials on the transmission of olfactory information together with audio/visual information are currently underway. However, a problem exists in that continuous emission of scent leaves scent in the air causing human olfactory adaptation. To resolve this problem, we aimed at minimizing the quantity of scent ejected using an ink-jet olfactory display developed. Following the development of a breath sensor for breath synchronization, we next developed an olfactory ejection system to present scent on each inspiration. We then measured human olfactory characteristics in order to determine the most suitable method for presenting scent on an inspiration. Experiments revealed that the intensity of scent perceived by the user was altered by differences in the presentation method even when the quantity of scent was unchanged. We present here a method of odor presentation that most effectively minimizes the ejection quantities.",
On the NP-Hardness of Checking Matrix Polytope Stability and Continuous-Time Switching Stability,"Motivated by questions in robust control and switched linear dynamical systems, we consider the problem checking whether all convex combinations of k matrices in Rntimesn are stable. In particular, we are interested whether there exist algorithms which can solve this problem in time polynomial in n and k. We show that if k=nd for any fixed real d > 0, then the problem is NP-hard, meaning that no polynomial-time algorithm in n exists provided that P ne NP, a widely believed conjecture in computer science. On the other hand, when k is a constant independent of n, then it is known that the problem may be solved in polynomial time in n. Using these results and the method of measurable switching rules, we prove our main statement: verifying the absolute asymptotic stability of a continuous-time switched linear system with more than nd matrices Ai isin Rntimesn satisfying 0 ges Ai + Ai T is NP-hard.","Polynomials,
Robust stability,
Testing,
Laboratories,
Linear matrix inequalities,
Robust control,
Computer science,
Asymptotic stability,
Linear systems,
Control systems"
Empathizing with robots: Fellow feeling along the anthropomorphic spectrum,A long-standing question within the robotics community is about the degree of human-likeness robots ought to have when interacting with humans. We explore an unexamined aspect of this problem: how people empathize with robots along the anthropomorphic spectrum. We conducted a web-based experiment (n = 120) that measured how people empathized with four different robots shown to be experiencing mistreatment by humans. Our results indicate that people empathize more strongly with more human-looking robots and less with mechanical looking robots. We also found that a person's general ability to empathize has no predictive value for expressed empathy toward robots.,"Anthropomorphism,
Humanoid robots,
Human robot interaction,
Robot sensing systems,
Psychology,
Brain modeling,
Testing,
Laboratories,
Autism,
Footwear"
Ad-hoc wireless network coverage with networked robots that cannot localize,"We study a fully distributed, reactive algorithm for deployment and maintenance of a mobile communication backbone that provides an area around a network gateway with wireless network access for higher-level agents. Possible applications of such a network are distributed sensor networks as well as communication support for disaster or military operations. The algorithm has minimalist requirements on the individual robotic node and does not require any localization. This makes the proposed solution suitable for deployment of large numbers of comparably cheap mobile communication nodes and as a backup solution for more capable systems in GPS-denied environments. Robots keep exploring the configuration space by random walk and stop only if their current location satisfies user-specified constraints on connectivity (number of neighbors). Resulting deployments are robust and convergence is analyzed using both kinematic simulation with a simplified collision and communication model as well as a probabilistic macroscopic model. The approach is validated on a team of 9 iRobot Create robots carrying wireless access points in an indoor environment.","Wireless networks,
Mobile communication,
Robot sensing systems,
Orbital robotics,
Space exploration,
Spine,
Military communication,
Legged locomotion,
Robustness,
Kinematics"
Survey of the Visual Exploration and Analysis of Perfusion Data,"Dynamic contrast-enhanced image data (perfusion data) are used to characterize regional tissue perfusion. Perfusion data consist of a sequence of images, acquired after a contrast agent bolus is applied. Perfusion data are used for diagnostic purposes in oncology, ischemic stroke assessment or myocardial ischemia. The diagnostic evaluation of perfusion data is challenging, since the data is complex and exhibits various artifacts, e.g., motion artifacts. We provide an overview on existing methods to analyze, and visualize CT and MR perfusion data. The integrated visualization of several 2D parameter maps, the 3D visualization of parameter volumes and exploration techniques are discussed. An essential aspect in the diagnosis of perfusion data is the correlation between perfusion data and derived time-intensity curves as well as with other image data, in particular with high resolution morphologic image data. We discuss visualization support with respect to the three major application areas: ischemic stroke diagnosis, breast tumor diagnosis and the diagnosis of coronary heart disease.","Data analysis,
Data visualization,
Computed tomography,
Cardiac disease,
Blood flow,
Neoplasms,
Morphology,
Pathology,
Spatial resolution,
Biomedical imaging"
P-packSVM: Parallel Primal grAdient desCent Kernel SVM,"It is an extreme challenge to produce a nonlinear SVM classifier on very large scale data. In this paper we describe a novel P-packSVM algorithm that can solve the Support Vector Machine (SVM) optimization problem with an arbitrary kernel. This algorithm embraces the best known stochastic gradient descent method to optimize the primal objective, and has 1/¿ dependency in complexity to obtain a solution of optimization error ¿. The algorithm can be highly parallelized with a special packing strategy, and experiences sub-linear speed-up with hundreds of processors. We demonstrate that P-packSVM achieves accuracy sufficiently close to that of SVM-light, and overwhelms the state-of-the-art parallel SVM trainer PSVM in both accuracy and efficiency. As an illustration, our algorithm trains CCAT dataset with 800k samples in 13 minutes and 95% accuracy, while PSVM needs 5 hours but only has 92% accuracy. We at last demonstrate the capability of P-packSVM on 8 million training samples.","Kernel,
Support vector machines,
Support vector machine classification,
Stochastic processes,
Optimization methods,
Costs,
Data mining,
Physics,
Asia,
Computer science"
Multiphase geometric couplings for the segmentation of neural processes,"The ability to constrain the geometry of deformable models for image segmentation can be useful when information about the expected shape or positioning of the objects in a scene is known a priori. An example of this occurs when segmenting neural cross sections in electron microscopy. Such images often contain multiple nested boundaries separating regions of homogeneous intensities. For these applications, multiphase level sets provide a partitioning framework that allows for the segmentation of multiple deformable objects by combining several level set functions. Although there has been much effort in the study of statistical shape priors that can be used to constrain the geometry of each partition, none of these methods allow for the direct modeling of geometric arrangements of partitions. In this paper, we show how to define elastic couplings between multiple level set functions to model ribbon-like partitions. We build such couplings using dynamic force fields that can depend on the image content and relative location and shape of the level set functions. To the best of our knowledge, this is the first work that shows a direct way of geometrically constraining multiphase level sets for image segmentation. We demonstrate the robustness of our method by comparing it with previous level set segmentation methods.","Image segmentation,
Level set,
Deformable models,
Shape,
Layout,
Solid modeling,
Information geometry,
Biomedical imaging,
Biomembranes,
Computer science"
GCS: High-performance gate-level simulation with GPGPUs,"In recent years, the verification of digital designs has become one of the most challenging, time consuming and critical tasks in the entire hardware development process. Within this area, the vast majority of the verification effort in industry relies on logic simulation tools. However, logic simulators deliver limited performance when faced with vastly complex modern systems, especially synthesized netlists. The consequences are poor design coverage, delayed product releases and bugs that escape into silicon. Thus, we developed a novel GPU-accelerated logic simulator, called GCS, optimized for large structural netlists. By leveraging the vast parallelism offered by GP-GPUs and a novel netlist balancing algorithm tuned for the target architecture, we can attain an order-of-magnitude performance improvement on average over commercial logic simulators, and simulate large industrial-size designs, such as the OpenSPARC processor core design.","Logic design,
Computational modeling,
Circuit simulation,
Graphics,
Hardware,
Discrete event simulation,
Parallel processing,
Computer simulation,
Job shop scheduling,
Data structures"
An initial memory model for virtual and robot companions supporting migration and long-term interaction,"This work proposes an initial memory model for a long-term artificial companion, which migrates among virtual and robot platforms based on the context of interactions with the human user. This memory model enables the companion to remember events that are relevant or significant to itself or to the user. For other events which are either ethically sensitive or with a lower long-term value, the memory model supports forgetting through the processes of generalisation and memory restructuring. The proposed memory model draws inspiration from the human short-term and long-term memories. The short-term memory will support companions in focusing on the stimuli that are relevant to their current active goals within the environment. The long-term memory will contain episodic events that are chronologically sequenced and derived from the companion's interaction history both with the environment and the user. There are two key questions that we try to address in this work: 1) What information should the companion remember in order to generate appropriate behaviours and thus smooth the interaction with the user? And, 2) What are the relevant aspects to take into consideration during the design of memory for a companion that can have different types of virtual and physical bodies? Finally, we show an implementation plan of the memory model, focusing on issues of information grounding, activation and sensing based on specific hardware platforms.","Cognitive robotics,
Human robot interaction,
Artificial intelligence,
Competitive intelligence,
Context modeling,
Robot sensing systems,
History,
Grounding,
Hardware,
Delay"
Stiffness discrimination with visual and proprioceptive cues,"This study compares the Weber fraction for human perception of stiffness among three conditions: vision, proprioceptive motion feedback, and their combination. To make comparisons between these feedback conditions, a novel haptic device was designed that senses the spring behavior through encoder and force measurements, and implements a controller to render linear virtual springs so that the stimuli displayed haptically could be compared with their visual counterparts. The custom-designed, torque-controlled haptic interface non-invasively controls the availability of proprioceptive motion feedback in unimpaired individuals using a virtual environment. When proprioception is available, the user feels an MCP joint rotation that is proportional to his or her finger force. When proprioception is not available, the actual finger is not allowed to move, but a virtual finger displayed graphically moves in proportion to the user's applied force. Visual feedback is provided and removed by turning on and off this graphical display. Weber fractions were generated from an experiment in which users examined pairs of springs and attempted to identify the spring with higher stiffness. To account for slight trial-to-trial variations in the relationship between force and position in the proprioceptive feedback conditions, our analysis uses measurements of the actual rendered stiffness, rather than the commanded stiffness. Results for 10 users give average Weber fractions of 0.056 for vision, 0.036 for proprioception, and 0.039 for their combination, indicating that proprioception is important for stiffness perception for this experimental setup. The long-term goal of this research is to motivate and develop methods for proprioception feedback to wearers of dexterous upper-limb prostheses.","Springs,
Force feedback,
Force measurement,
Fingers,
Haptic interfaces,
Rendering (computer graphics),
Humans,
Linear feedback control systems,
Force control,
Motion control"
SVEET! a scalable virtualized evaluation environment for TCP,"The ability to establish an objective comparison between high-performance TCP variants under diverse networking conditions and to obtain a quantitative assessment of their impact on the global network traffic is essential to a community-wide understanding of various design approaches. Small-scale experiments are insufficient for a comprehensive study of these TCP variants. We propose a TCP performance evaluation testbed, called SVEET, on which real implementations of the TCP variants can be accurately evaluated under diverse network configurations and workloads in large-scale network settings. This testbed combines real-time immersive simulation, emulation, machine and time virtualization techniques. We validate the testbed via extensive experiments and assess its capabilities through case studies involving real web services.","Testing,
Telecommunication traffic,
Large-scale systems,
Emulation,
Web services"
Code-modulated path-sharing multi-antenna receivers: theory and analysis,"Conventional multi-antenna receiver front-ends require multiple RF/baseband chains and analog-to-digital converters (ADC). This increases power consumption and chip area substantially. In this letter, we introduce a new Code-Modulated Path-Sharing Multi-Antenna (CPMA) receiver architecture suitable for any multi-antenna scheme including spatial multiplexing, spatial diversity, and beamforming. The receiver uses code modulation to distinguish the antenna signals before combining them in the analog domain. The combined signal propagates through shared-path blocks and all the original signals are later recovered in the digital domain for further processing. Due to the spread spectrum nature of code modulation, a larger bandwidth is needed for the blocks in the shared path. To alleviate this effect, the use of non-orthogonal coding is examined. An effective channel matrix is derived and the system capacity is evaluated in terms of the cross-correlation between signature codes. Implementation and code selection issues are discussed. Analysis and simulation results indicate that by properly selecting non-orthogonal code sets, the spreading factor, and therefore, the overall analog signal bandwidth is reduced while incurring minimal performance degradation.","Modulation coding,
Signal processing,
Bandwidth,
Radio frequency,
Baseband,
Analog-digital conversion,
Energy consumption,
Diversity reception,
Array signal processing,
Receiving antennas"
Interactive Visual Analysis of Complex Scientific Data as Families of Data Surfaces,"The widespread use of computational simulation in science and engineering provides challenging research opportunities. Multiple independent variables are considered and large and complex data are computed, especially in the case of multi-run simulation. Classical visualization techniques deal well with 2D or 3D data and also with time-dependent data. Additional independent dimensions, however, provide interesting new challenges. We present an advanced visual analysis approach that enables a thorough investigation of families of data surfaces, i.e., datasets, with respect to pairs of independent dimensions. While it is almost trivial to visualize one such data surface, the visual exploration and analysis of many such data surfaces is a grand challenge, stressing the users' perception and cognition. We propose an approach that integrates projections and aggregations of the data surfaces at different levels (one scalar aggregate per surface, a 1D profile per surface, or the surface as such). We demonstrate the necessity for a flexible visual analysis system that integrates many different (linked) views for making sense of this highly complex data. To demonstrate its usefulness, we exemplify our approach in the context of a meteorological multi-run simulation data case and in the context of the engineering domain, where our collaborators are working with the simulation of elastohydrodynamic (EHD) lubrication bearing in the automotive industry.","Computational modeling,
Data visualization,
Context modeling,
Automotive engineering,
Cognition,
Aggregates,
Meteorology,
Data engineering,
Collaborative work,
Lubrication"
Driver's cognitive distraction detection using physiological features by the adaboost,"Effects of driver's states adaptive driving support systems is highly expected for the prevention of traffic accidents. In order to create this constituent technology, detecting driver's psychosomatic states which occurs just before a traffic accident is essential. Therefore driver's distraction is thought as one of important factors. This study focused on detecting driver's cognitive distraction, a state which can easily lead to a traffic accident. We reproduced the cognitive distraction by imposing conversation or arithmetic loads to the subjects on a driving simulator. A stereo camera system were used as the means to track a subject's eyes, and head movements, which were set as classification features for pattern recognition on the Support Vector Machine (hereafter, SVM) basis used in the previous study of the AIDE project, a part of EU 6th Framework Programme. Diameter of pupil as well as the interval between heart R-waves (hereafter, heart rate RRI) from an ECG (electrocardiogram) were added for classification features to further improve the accuracy of driver's cognitive distraction detection. Based on this study, we established the methodology for more precise and faster driver's cognitive detection by using the AdaBoost.","Computer vision,
Road accidents,
Support vector machines,
Support vector machine classification,
Adaptive systems,
Psychology,
Arithmetic,
Cameras,
Tracking,
Eyes"
Using modulation spectra for voice pathology detection and classification,"In this paper, we consider the use of Modulation Spectra for voice pathology detection and classification. To reduce the high-dimensionality space generated by Modulation spectra we suggest the use of Higher Order Singular Value Decomposition (SVD) and we propose a feature selection algorithm based on the Mutual Information between subjective voice quality and computed features. Using SVM with a radial basis function (RBF) kernel as classifier, we conducted experiments on a database of sustained vowel recordings from healthy and pathological voices. For voice pathology detection, the suggested approach achieved a detection rate of 94.1% and an Area Under the Curve (AUC) score of 97.8%. For voice pathology classification, an average detection rate and AUC of 88.6% and 94.8%, respectively, was achieved in classifying polyp against keratosis leukoplakia, adductor spasmodic dysphonia and vocal nodules.","Pathology,
Speech,
Support vector machines,
Support vector machine classification,
Hidden Markov models,
Jitter,
Acoustic noise,
Noise level,
Frequency estimation,
Filters"
MASS: A Malay language LVCSR corpus resource,"This paper presents the development of the speech, text and pronunciation dictionary resources required to build a large vocabulary speech recognizer for the Malay language. This project is a collaboration project among three universities: USM, MMU from Malaysia and NTU from Singapore. The Malay speech corpus consists of read speech (speaker independent/ dependent and accent independent/ dependent) and broadcast news. To date, 90 speakers have been recorded which is equal to a total of nearly 70 hours of read speech, and 10 hours of broadcast news from local TV stations in Malaysia was transcribed. The text corpus consists of 700Mbytes of data extracted from Malaysia's local news web pages from 1998–2008 and a rule based G2P tool is develop to generate the pronunciation dictionary.","Dictionaries,
TV broadcasting,
Vocabulary,
Speech recognition,
Text recognition,
Natural languages,
Collaboration,
Educational institutions,
Data mining,
Web pages"
Control analysis of an active power filter using Lyapunov candidate,"To reduce harmonic distortion in the power lines and to improve the system power quality, the applications of active power filter (APF) has attracted a lot of attentions. A Lyapunov-based current control method for APF system is proposed. The control law is derived from directly applying the Lyapunov stability theory to the integral controller, so that the closed-loop systems possess the better harmonic current elimination. The control strategy based on Lyapunov candidate is discussed in detail and it is shown that the proposed control method provides a general design framework for the model-based harmonic current elimination and reactive power compensation. The main advantage of this approach is that Lyapunov stability analysis can be used to obtain a set of proper parameters for the integral controller. The proposed controller is validated by carrying out some experiments. Computer simulations are presented to confirm the effectiveness of the proposed control strategy and the validity of the simulation technique. Experimental results are also presented to verify the theoretical and simulation studies.","power system stability,
active filters,
closed loop systems,
control system analysis,
electric current control,
Lyapunov methods,
power harmonic filters,
power supply quality"
Provably Secure Certificateless Two-Party Authenticated Key Agreement Protocol without Pairing,"Key agreement is one of the fundamental cryptographic primitives in public key cryptography. So far several certificateless two-party authenticated key agreement (CL-T-AKA) protocols have been proposed. However, all these protocols are based on bilinear map and most of them are without formal security proof. In this paper, we present a new formal security model of CL-T-AKA protocols and bring forward the first two-party key agreement protocol without the computation of expensive bilinear map. Our protocol is secure under the security model assuming the Gap-DH problem is intractable. With respect to efficiency, our protocol requires a single round of communication in which each party sends only one group element, and needs only five modular exponentiation computations. In addition, we point out that an existing certificateless two-party key agreement protocol cannot resist man-in-the-middle attack.",
Ontology-Based Reasoning in Requirements Elicitation,"This paper introduces an ontology-based reasoning method for requirements elicitation. We start with an ontology structure contains knowledge of functional requirements and relations among them. We then propose a framework to elicit requirements using ontology: First we map initial requirements to functions in domain ontology. After that, we use rules and relations among functions to reason for errors and potential requirements. Using that result, analysts can generate questions to customers and correctly and efficiently revise requirements. We have been developing an ontology-based checking tool for requirements elicitation based on our method.","Ontologies,
Computer errors,
Redundancy,
Software engineering,
Computer science,
Writing,
Contracts"
Communicating the sum of sources in a 3-sources/3-terminals network,"We consider the network communication scenario in which a number of sources si each holding independent information Xi wish to communicate the sum ∑Xi to a set of terminals tj. In this work we consider directed acyclic graphs with unit capacity edges and independent sources of unit-entropy. The case in which there are only two sources or only two terminals was considered by the work of Ramamoorthy [ISIT 2008] where it was shown that communication is possible if and only if each source terminal pair si/tj is connected by at least a single path. In this work we study the communication problem in general, and show that even for the case of three sources and three terminals, a single path connecting source/terminal pairs does not suffice to communicate ∑Xi. We then present an efficient encoding scheme which enables the communication of ∑Xi for the three sources, three terminals case, given that each source terminal pair is connected by two edge disjoint paths. Our encoding scheme includes a structural decomposition of the network at hand which may be found useful for other network coding problems as well.","Network coding,
Arithmetic,
Encoding,
Source coding,
Computer science,
Joining processes,
Routing,
Galois fields,
Entropy"
Concentration based feature construction approach for spam detection,"Inspired by human immune system, a concentration based feature construction (CFC) approach which utilizes a two-element concentration vector as the feature vector is proposed for spam detection in this paper. In the CFC approach, ‘self’ and ‘non-self’ concentrations are constructed by using ‘self’ and ‘non-self’ gene libraries, respectively, and subsequently are used to form a vector with two elements of concentrations for characterizing the e-mail efficiently. As a result, the design of classifier actually amounts to establishing a mapping between two real-value inputs and one binary output. The classification of the e-mail is considered as an optimization problem aiming at minimizing a formulated cost function. A clonal particle swarm optimization (CPSO) algorithm proposed by the leading author is also employed for this purpose. Several classifiers including linear discriminant, multi-layer neural networks and support vector machine are used to verify the effectiveness and robustness of the CFC approach. Experimental results demonstrate that the proposed CFC approach not only has a very much fast speed but also gives 97% and 99% of accuracy just using a two-element concentration feature vector on corpus PU1 and Ling, respectively.",
A Framework for Cost Sensitive Assessment of Intrusion Response Selection,"In recent years, cost-sensitive intrusion response has gained significant interest, mainly due to its emphasis on the balance between potential damage incurred by the intrusion and cost of the response. However, one of the challenges in applying this approach is defining a consistent and adaptable measurement of these cost factors on the basis of system requirements and policy. In this paper,we present a host-based framework for the cost-sensitive assessment and selection of intrusion response. Specifically,we introduce a set of measurements that characterize the potential costs associated with the intrusion handling process, and propose an intrusion response evaluation method with respect to the risk of potential intrusion damage, the effectiveness of the response action and the response cost for a system. We provide an implementation of the proposed solution as an IDS-independent plugin tool and demonstrate its advantages on the several attack examples.",
"Metasynthesis: M-Space, M-Interaction, and M-Computing for Open Complex Giant Systems","The studies of complex systems have been recognized as one of the greatest challenges for current and future science and technology. Open complex giant systems (OCGSs) are a family of specially complex systems with system complexities such as openness, human involvement, societal characteristic, and intelligence emergence. They greatly challenge multiple disciplines such as system sciences, system engineering, cognitive sciences, information systems, artificial intelligence, and computer sciences. As a result, traditional problem-solving methodologies can help deal with them but are far from a mature solution methodology. The theory of qualitative-to-quantitative metasynthesis has been proposed as a breakthrough and effective methodology for the understanding and problem solving of OCGSs. In this paper, we propose the concepts of M-Interaction, M-Space, and M-Computing which are three key components for studying OCGS and building problem-solving systems. M-Interaction forms the main problem-solving mechanism of qualitative-to-quantitative metasynthesis; M-Space is the OCGS problem-solving system embedded with M-Interactions, while M-Computing consists of engineering approaches to the analysis, design, and implementation of M-Space and M-Interaction. We discuss the theoretical framework, problem-solving process, social cognitive evolution, intelligence emergence, and pitfalls of certain types of cognitions in developing M-Space and M-Interaction from the perspectives of cognitive sciences and social cognitive interaction. These can help one understand complex systems and develop effective problem-solving methodologies.","Problem-solving,
Humans,
Artificial intelligence,
Internet,
Systems engineering and theory,
Information systems,
Buildings,
Design engineering,
Cognition,
Cybernetics"
Discrete tracking of parametrized curves,"A novel scheme for deformable tracking of curvilinear structures in image sequences is presented. The approach is based on B-spline snakes defined by a set of control points whose optimal configuration is determined through efficient discrete optimization. Each control point is associated with a discrete random variable in a MAP-MRF formulation where a set of labels captures the deformation space. In such a context, generic terms are encoded within this MRF in the form of pairwise potentials. The use of pairwise potentials along with the B-spline representation offers nearly perfect approximation of the continuous domain. Efficient linear programming is considered to recover the approximate optimal solution. The method is successfully applied to the tracking of guide-wires in fluoroscopic X-ray sequences of several hundred frames which requires extremely robust techniques.",
Mean square stabilization of multi-input systems over stochastic multiplicative channels,"This paper deals with the mean square stabilization problem for multi-input networked systems via single packet or multiple packets transmission, where the unreliability of input channels is modeled by a multiplicative white noise. For the single packet case, the critical value (lower bound) of mean square capacity for ensuring mean square stabilization is given by adopting the bisection technique. For the m-parallel multiple packets transmission strategy, a necessary and sufficient condition on overall mean square capacity for mean square stabilization in terms of the Mahler measure or topological entropy of the plant is presented, under the assumption that the given network resource can be allocated among all the input channels. Applications in erasure-type channel and channel with stochastic sector-bounded uncertainty are provided to demonstrate the results.",
Power-Law Distributions of Component Size in General Software Systems,"This paper begins by modeling general software systems using concepts from statistical mechanics which provide a framework for linking microscopic and macroscopic features of any complex system. This analysis provides a way of linking two features of particular interest in software systems: first the microscopic distribution of defects within components and second the macroscopic distribution of component sizes in a typical system. The former has been studied extensively, but the latter much less so. This paper shows that subject to an external constraint that the total number of defects is fixed in an equilibrium system, commonly used defect models for individual components directly imply that the distribution of component sizes in such a system will obey a power-law Pareto distribution. The paper continues by analyzing a large number of mature systems of different total sizes, different implementation languages, and very different application areas, and demonstrates that the component sizes do indeed appear to obey the predicted power-law distribution. Some possible implications of this are explored.","Software systems,
Power system modeling,
Joining processes,
Microscopy,
Thermodynamics,
Temperature distribution,
Predictive models,
Assembly systems,
Software standards,
Equations"
Practical Energy-Aware Scheduling for Real-Time Multiprocessor Systems,"Energy-aware real-time multiprocessor scheduling has been studied extensively so far. However, some of the constraints associated with the practical DVS applications have been ignored for simplicity. These constraints include discrete speed, idle power, inefficient speed, and application-specific power characteristics etc. This work targets energy-aware scheduling of periodic real-time tasks on the DVS-equipped multiprocessor systems with practical constraints. An adaptive minimal bound first-fit (AMBFF) algorithm with consideration of these realistic constraints is proposed for both dynamic-priority and fixed-priority multiprocessor scheduling. Simulation results on three commercial processor models show that our algorithm can save significantly more energy than existing algorithms.","Real time systems,
Multiprocessing systems,
Processor scheduling,
Energy consumption,
Voltage control,
Scheduling algorithm,
Frequency,
Computer applications,
Embedded computing,
Information science"
Lexicon Bad Smells in Software,"We introduce the notion of ""lexicon bad smell"", which parallels that of ""code smell"" and indicates some potential lexicon construction problems that can be addressed through refactoring (e.g., renaming). We created a catalog of lexicon bad smells and we developed a publicly available suite of detectors to locate them. The paper presents a case study in which we used the detectors on two open-source systems. The study revealed the main challenges faced in detecting the lexicon bad smells.","Detectors,
Face detection,
Documentation,
Reverse engineering,
Computer science,
Open source software,
Terminology,
Programming environments,
Programming profession,
Speech"
Pattern Matching with Independent Wildcard Gaps,"Pattern matching is fundamental in applications such as biological sequence analysis and text indexing. A wildcard gap matches any subsequence with a length between two user specified integers, therefore introducing much adaptability to patterns. However, most existing works require that gaps in a pattern be the same. In this paper, we define a new pattern matching problem where gaps are independently specified. The objective is to compute the number of all matches. Since this number is exponential with respect to the maximal gap flexibility and the pattern length, counting matches one by one is computationally infeasible. We develop an efficient algorithm, named Pattern mAtching with Independent wildcard Gaps (PAIG) for this problem, and propose two approaches to enhance its performance further. For the final version, the time complexity is O(Ll2W2), where L is the sequence length, l is the pattern length, and W is the maximal gap flexibility. The space complexity is O(lW), making PAIG easy to run in a Java Applet. Experimental results validate the efficiency of PAIG and confirm our analysis about its different versions.","Pattern matching,
Sequences,
Computer science,
USA Councils,
DNA,
Data mining,
Biology computing,
Application software,
Indexing,
Java"
Experimental study of gate oxide early-life failures,"Large-scale experimental data from 90nm test chips consisting of 49,152 transistors, and experiments on 90nm test chips containing inverter chains are used to establish: 1. A gate-oxide early-life failure (ELF, also called infant mortality) candidate transistor produces gradually degraded drive currents over time; 2. A digital circuit path consisting of a gate-oxide ELF candidate transistor experiences gradual delay shifts over time before the circuit produces functional failures. These results may be utilized to effectively overcome ELF challenges in scaled CMOS technologies.","Ground penetrating radar,
Geophysical measurement techniques,
Circuit testing,
Stress,
Degradation,
Delay,
Inverters,
Large-scale systems,
Voltage,
Digital circuits"
Task-space trajectories via cubic spline optimization,"We consider the task of planning smooth trajectories for robot motion. In this paper we make two contributions. First we present a method for cubic spline optimization; this technique lets us simultaneously plan optimal task-space trajectories and fit cubic splines to the trajectories, while obeying many of the same constraints imposed by a typical motion planning algorithm. The method uses convex optimization techniques, and is therefore very fast and suitable for real-time re-planning and control. Second, we apply this approach to the tasks of planning foot and body trajectory for a quadruped robot, the “LittleDog,” and show that the proposed approach improves over previous work on this robot.","Spline,
Trajectory,
Motion planning,
Robot motion,
Constraint optimization,
Foot,
Robotics and automation,
Computer science,
Optimization methods,
Stochastic processes"
Robust Initial Detection of Landmarks in Film-Screen Mammograms Using Multiple FFDM Atlases,"Automated analysis of mammograms requires robust methods for pectoralis segmentation and nipple detection. Locating the nipple is especially important in multiview computer aided detection systems, in which findings are matched across images using the nipple-to-finding distance. Segmenting the pectoralis is a key preprocessing step to avoid false positives when detecting masses due to the similarity of the texture of mammographic parenchyma and the pectoral muscle. A multi-atlas algorithm capable of providing very robust initial estimates of the nipple position and pectoral region in digitized mammograms is presented here. Ten full-field digital mammograms, which are easily annotated attributed to their excellent contrast, are robustly registered to the target digitized film-screen mammogram. The annotations are then propagated and fused into a final nipple position and pectoralis segmentation. Compared to other nipple detection methods in the literature, the system proposed here has the advantages that it is more robust and can provide a reliable estimate when the nipple is located outside the image. Our results show that the change in the correlation between nipple-to-finding distances in craniocaudal and mediolateral oblique views is not significant when the detected nipple positions replace the manual annotations. Moreover, the pectoralis segmentation is acceptable and can be used as initialization for a more complex algorithm to optimize the outline locally. A novel aspect of the method is that it is also capable of detecting and segmenting the pectoralis in craniocaudal views.","Robustness,
Image segmentation,
Cancer detection,
Mammography,
Optical films,
Optical noise,
Muscles,
Change detection algorithms,
Breast cancer,
Data mining"
Getting back to basics: Promoting the use of a traceability information model in practice,"It is widely assumed that following a process is a good thing if you want to achieve and exploit the benefits of traceability on a software development project. A core component of any such process is the definition and use of a traceability information model. Such models provide guidance as to those software development artifacts to collect and those relations to establish, and are designed to ultimately support required project analyses. However, traceability still tends to be undertaken in rather ad hoc ways in industry, with unpredictable results. We contend that one reason for this situation is that current software development tools provide little support to practitioners for building and using customized project-specific traceability information models, without which even the simplest of processes are problematic to implement and gain the anticipated benefits from. In this paper, we highlight the typical decisions involved in creating a basic traceability information model, suggest a simple UML-based representation for its definition, and illustrate its central role in the context of a modeling tool. The intent of this paper is to re-focus attention on very practical ways to apply traceability information models in practice so as to encourage wider adoption.","Programming,
Context modeling,
Prototypes,
Tagging,
Unified modeling language,
Software systems,
Computer science,
Industrial relations,
Buildings,
Certification"
Computing with Curvelets: From Image Processing to Turbulent Flows,"The curvelet transform allows an almost optimal nonadaptive sparse representation for curve-like features and edges. The authors describe some recent applications involving image processing, seismic data exploration, turbulent flows, and compressed sensing.","Image processing,
Frequency domain analysis,
Geometry,
Digital images,
Image restoration,
Noise reduction,
Image coding,
Wavelet domain,
Anisotropic magnetoresistance,
Isosurfaces"
Pilot-symbol-assisted detection scheme for distributed orthogonal space-time block coding,"In this letter, we investigate the effect of imperfect channel estimation on the performance of distributed space-time block codes (DSTBCs) with amplify-and-forward relaying. Exploiting the orthogonality of the underlying code, we derive a maximum likelihood metric conditioned on the channel estimate acquired through the insertion of pilot symbols. For a large number of pilot symbols, we demonstrate that the proposed decoding rule coincides with the so-called mismatched receiver. On the other hand, as the number of pilot symbols decreases, the proposed decoder converges to a non-coherent detector. Through Monte-Carlo simulations, we further demonstrate that the performance of the proposed scheme lies within 0.8 dB of the genie receiver performance bound.",
A selfish approach to coalition formation among unmanned air vehicles in wireless networks,"Autonomous agents such as unmanned aerial vehicles (UAVs) have a great potential for deployment in next generation wireless networks. While current literature has been mainly focused on the use of UAVs for connectivity enhancement and routing in military ad hoc networks, this paper proposes a novel usage model for UAVs in wireless communication networks. In the proposed model, a number of UAVs are required to collect data from a number of randomly located tasks and transmit this data wirelessly to a common receiver (such as the central command). Each task represents a queue of packets that require collection and transmission to the central receiver. This problem is modeled as a hedonic coalition formation game between the UAVs and the tasks that interact in order to form disjoint coalitions. Each formed coalition is modeled as a polling system consisting of a number of UAVs, designated as collectors, which act as a single server that moves between the different tasks present in the coalition, collects and transmits the packets to a common receiver. Within each coalition, some UAVs might also take the role as a relay for improving the packet success rate of the transmission. The proposed coalition formation algorithm allows the tasks and the UAVs to take local selfish decisions to join or leave a coalition, based on the achieved benefit, in terms of effective throughput, and the cost in terms of delay. Simulation results show how the proposed algorithm allows the UAVs and tasks to self-organize into independent coalitions, while improving the performance, in terms of average player (UAV or task) payoff, of at least 30.26% relatively to a scheme that allocates nearby tasks equally among UAVs.","Unmanned aerial vehicles,
Wireless networks,
Autonomous agents,
Next generation networking,
Routing,
Military communication,
Ad hoc networks,
Wireless communication,
Relays,
Throughput"
Autonomic cardiovascular modulation,"The aim of the study was twofold: first, to validate symbolic analysis as a tool capable of assessing autonomic cardiovascular regulation in rats and, second, to investigate neural cardiovascular regulation during the progression of ischemic induced chronic heart failure (CHF) in two groups of rats, a control group and a group treated with chronic administration of the mineralocorticoid receptor antagonist SP, a drug playing a protective role over cardiovascular regulation.","Cardiology,
Rats,
Animals,
Pulse modulation,
Spectral analysis,
Blood pressure,
Analytical models,
Heart rate,
Cardiovascular diseases,
Myocardium"
A locally global approach to stereo correspondence,"A novel approach to deal with the stereo correspondence problem induced by the implicit assumptions made by cost aggregation (CA) strategies is proposed. CA relies on the implicit assumption that disparity varies smoothly within neighboring points except at depth discontinuities and state-of-the-art CA strategies adapt their support to image content by classifying each pixel based on geometric and photometric constraints. Our proposal explicitly models this behavior from a different perspective, by gathering for each point, multiple assumptions that locally would be made by a hypothetical variable CA strategy. This framework enables to derive a function that locally captures the plausibility of the underlying geometric and photometric constraints independently enforced by supports of neighboring points. Experimental results confirm the effectiveness of our proposal.","Stereo vision,
Proposals,
Photometry,
Layout,
Cost function,
Computer vision,
Computational efficiency,
Hardware,
Conferences,
Filters"
Knowledge Acquisition and Insider Threat Prediction in Relational Database Systems,"This paper investigates the problem of knowledge acquisition by an unauthorized insider using dependencies between objects in relational databases. It defines various types of knowledge. In addition, it introduces the Neural Dependency and Inference Graph (NDIG), which shows dependencies among objects and the amount of knowledge that can be inferred about them using dependency relationships. Moreover, it introduces an algorithm to determine the knowledgebase of an insider and explains how insiders can broaden their knowledge about various relational database objects to which they lack appropriate access privileges. In addition, it demonstrates how NDIGs and knowledge graphs help in assessment of insider threats and what security officers can do to avoid such threats.","Knowledge acquisition,
Relational databases,
Protection,
Data security,
Data engineering,
Knowledge engineering,
Inference algorithms,
Information security,
Computer science,
Communication system security"
Controllable radio interference for experimental and testing purposes in Wireless Sensor Networks,"We address the problem of generating customized, controlled interference for experimental and testing purposes in wireless sensor networks. The known coexistence problems between electronic devices sharing the same ISM radio band drive the design of new solutions to mitigate interference. The validation of these techniques and the assessment of protocols under external interference require the creation of reproducible and well-controlled interference patterns on real nodes, a nontrivial and time-consuming task. In this paper, we study methods to generate a precisely adjustable level of interference on a specific channel, with lowcost equipment and rapid calibration. We focus our work on the platforms carrying the CC2420 radio chip. We show that, by setting the CC2420 in special mode, we can easily generate repeatable and precise patterns of interference. We show how this method is extremely useful for researchers to quickly investigate the behaviour of sensor network protocols and applications under different patterns of interference. We further evaluate the performance of our proposed method.",
A New Image Watermarking Scheme Using Saliency Based Visual Attention Model,"In this paper, a new spatial domain adaptive image watermarking scheme is proposed which embeds watermark information to the least salient pixels of the image. Watermarked image thus produced has less perceptual error with respect to human visual system (HVS). In proposed scheme least salient pixels are determined using the well known Itti-Koch model. Experimental results reveal that proposed scheme has less perceptual error than existing spatial domain embedding scheme.",
Quality based rank-level fusion in multibiometric systems,"Multibiometric systems fuse evidences from multiple biometric sources typically resulting in better recognition accuracy. These systems can consolidate information at various levels. For systems operating in the identification mode, rank level fusion presents a viable option. In this paper, several simple but powerful modifications are suggested to enhance the performance of rank-level fusion schemes in the presence of weak classifiers or low quality input images. These modifications do not require a training phase, therefore making them suitable in a wide range of applications. Experiments conducted on a multimodal database consisting of a few hundred users indicate that the suggested modifications to the highest rank and Borda count methods significantly enhance the rank-1 accuracy. Experiments also reveal that including image quality in the fusion scheme enhances the Borda count rank-1 accuracy by ~40%.",
Design and implementation of software-managed caches for multicores with local memory,"Heterogeneous multicores, such as Cell BE processors and GPGPUs, typically do not have caches for their accelerator cores because coherence traffic, cache misses, and latencies from different types of memory accesses add overhead and adversely affect instruction scheduling. Instead, the accelerator cores have internal local memory to place their code and data. Programmers of such heterogeneous multicore architectures must explicitly manage data transfers between the local memory of a core and the globally shared main memory. This is a tedious and errorprone programming task. A software-managed cache (SMC), implemented in local memory, can be programmed to automatically handle data transfers at runtime, thus simplifying the task of the programmer. In this paper, we propose a new software-managed cache design, called extended set-index cache (ESC). It has the benefits of both set-associative and fully associative caches. Its tag search speed is comparable to the set-associative cache and its miss rate is comparable to the fully associative cache. We examine various line replacement policies for SMCs, and discuss their trade-offs. In addition, we propose adaptive execution strategies that select the optimal cache line size and replacement policy for each program region at runtime. To evaluate the effectiveness of our approach, we implement the ESC and other SMC designs on the Cell BE architecture, and measure their performance with 8 OpenMP applications. The evaluation results show that the ESC outperforms other SMC designs. The results also show that our adaptive execution strategies work well with the ESC. In fact, our approach is applicable to all cores with access to both local and global memory in a multicore architecture.","Multicore processing,
Sliding mode control,
Programming profession,
Memory management,
Hardware,
Delay,
Computer architecture,
Processor scheduling,
Runtime,
Computer science"
Static virtual channel allocation in oblivious routing,"Most virtual channel routers have multiple virtual channels to mitigate the effects of head-of-line blocking. When there are more flows than virtual channels at a link, packets or flows must compete for channels, either in a dynamic way at each link or by static assignment computed before transmission starts. In this paper, we present methods that statically allocate channels to flows at each link when oblivious routing is used, and ensure deadlock freedom for arbitrary minimal routes when two or more virtual channels are available. We then experimentally explore the performance trade-offs of static and dynamic virtual channel allocation for various oblivious routing methods, including DOR, ROMM, Valiant and a novel bandwidth-sensitive oblivious routing scheme (BSORM). Through judicious separation of flows, static allocation schemes often exceed the performance of dynamic allocation schemes.","Channel allocation,
Routing,
Virtual colonoscopy,
Switches,
Buffer storage,
System recovery,
Computer science,
Artificial intelligence,
Laboratories,
Organizing"
Segmentation of Arabic Handwriting Based on both Contour and Skeleton Segmentation,"We propose a new algorithm for segmentation of off-line handwritten Arabic words. The algorithm segments the connected letters to smaller segments each of which contains no more than three letters. Each letter may be segmented to at most five pieces. In addition to improving the recognition of Arabic words, another potential application of the proposed segmentation method is to build lexicon of small size, consisting of no more than three letter combinations. Generally, it is very hard to generate lexicon for recognition of unconstraint handwritten Arabic documents due to the large number of words of Arabic language.The algorithm has been tested on over 6300 words from 45 different documents written by 18 writers. The system is able to segment more than 93% of the words into segments, each containing at most one letter, 6% of the words into segments that contains two letters and 3% of the words into segments that contains three letters.","Skeleton,
Robustness,
Handwriting recognition,
Shape,
Character recognition,
Text analysis,
Algorithm design and analysis,
Venus,
Computer science,
Testing"
"Providing route directions: Design of robot's utterance, gesture, and timing","Providing route directions is a complicated interaction. Utterances are combined with gestures and pronounced with appropriate timing. This study proposes a model for a robot that generates route directions by integrating three important crucial elements: utterances, gestures, and timing. Two research questions must be answered in this modeling process. First, is it useful to let robot perform gesture even though the information conveyed by the gesture is given by utterance as well? Second, is it useful to implement the timing at which humans speaks? Many previous studies about the natural behavior of computers and robots have learned from human speakers, such as gestures and speech timing. However, our approach is different from such previous studies. We emphasized the listener's perspective. Gestures were designed based on the usefulness, although we were influenced by the basic structure of human gestures. Timing was not based on how humans speak, but modeled from how they listen. The experimental result demonstrated the effectiveness of our approach, not only for task efficiency but also for perceived naturalness.","Timing,
Humans,
Educational robots,
Speech,
Computational modeling,
Atmospheric measurements"
Visual category recognition using Spectral Regression and Kernel Discriminant Analysis,"Visual category recognition (VCR) is one of the most important tasks in image and video indexing. Spectral methods have recently emerged as a powerful tool for dimensionality reduction and manifold learning. Recently, Spectral Regression combined with Kernel Discriminant Analysis (SR-KDA) has been successful in many classification problems. In this paper, we adopt this solution to VCR and demonstrate its advantages over existing methods both in terms of speed and accuracy. The distinctiveness of this method is assessed experimentally using an image and a video benchmark: the PASCAL VOC Challenge 08 and the Mediamill Challenge. From the experimental results, it can be derived that SR-KDA consistently yields significant performance gains when compared with the state-of-the art methods. The other strong point of using SR-KDA is that the time complexity scales linearly with respect to the number of concepts and the main computational complexity is independent of the number of categories.","Kernel,
Image recognition,
Linear discriminant analysis,
Layout,
Histograms,
Image representation,
Speech recognition,
Speech analysis,
Video recording,
Vector quantization"
"Execution Strategies for PTIDES, a Programming Model for Distributed Embedded Systems","We define a family of execution policies for a programming model called PTIDES (Programming Temporally Integrated Distributed Embedded Systems). A PTIDES application (factory automation, for example) is given as a discrete-event (DE) model of a distributed real-time system that includes sensors and actuators. The time stamps of DE events are bound to physical time at the sensors and actuators, turning the DE model into an executable specification of the system with explicit real-time constraints. This paper first defines a general execution strategy that conforms to the DE semantics, and then specializes this strategy to give practical, implementable and distributed policies. Our policies leverage network time synchronization to eliminate the need for null messages, allow independent events to be processed out of time stamp order, thus increasing concurrency and making more models feasible (w.r.t. real-time constraints), and improve fault isolation in distributed systems. The policies are given in terms of a safe to process predicate on events that depends on the time stamp of the events and the local notion of physical time. In a simple case we show how to statically check whether program execution satisfies timing constraints.","Embedded system,
Actuators,
Real time systems,
Application software,
Sensor systems,
Computational modeling,
Computer science,
Manufacturing automation,
Instruments,
Distributed computing"
Hierarchical video summarization in reference subspace,"In this paper, a hierarchical video structure summarization approach using Laplacian Eigenmap is proposed, where a small set of reference frames is selected from the video sequence to form a reference subspace to measure the dissimilarity between two arbitrary frames. In the proposed summarization scheme, the shot-level key frames are first detected from the continuity of inter-frame dissimilarity, and the sub-shot level and scene level representative frames are then summarized by using k-mean clustering. The experiment is carried on both test videos and movies, and the results show that in comparison with a similar approach using latent semantic analysis, the proposed approach using Laplacian Eigenmap can achieve a better recall rate in keyframe detection, and gives an efficient hierarchical summarization at sub shot, shot and scene levels subsequently.","Video sharing,
Layout,
Laplace equations,
Gunshot detection systems,
Multimedia databases,
Image segmentation,
Video sequences,
Testing,
Motion pictures,
Explosions"
Scalable learning for object detection with GPU hardware,"We consider the problem of robotic object detection of such objects as mugs, cups, and staplers in indoor environments. While object detection has made significant progress in recent years, many current approaches involve extremely complex algorithms, and are prohibitively slow when applied to large scale robotic settings. In this paper, we describe an object detection system that is designed to scale gracefully to large data sets and leverages upward trends in computational power (as exemplified by Graphics Processing Unit (GPU) technology) and memory. We show that our GPU-based detector is up to 90 times faster than a well-optimized software version and can be easily trained on millions of examples. Using inexpensive off-the-shelf hardware, it can recognize multiple object types reliably in just a few seconds per frame.","Object detection,
Hardware,
Detectors,
Intelligent robots,
Graphics,
Robot sensing systems,
Moore's Law,
Clocks,
USA Councils,
Indoor environments"
Face recognition using PCA and SVM,"Automatic recognition of people has received much attention during the recent years due to its many applications in different fields such as law enforcement, security applications or video indexing. Face recognition is an important and very challenging technique to automatic people recognition. Up to date, there is no technique that provides a robust solution to all situations and different applications that face recognition may encounter. In general, we can make sure that performance of a face recognition system is determined by how to extract feature vector exactly and to classify them into a group accurately. It, therefore, is necessary for us to closely look at the feature extractor and classifier. In this paper, Principle Component Analysis (PCA) is used to play a key role in feature extractor and the SVMs are used to tackle the face recognition problem. Support Vector Machines (SVMs) have been recently proposed as a new classifier for pattern recognition. We illustrate the potential of SVMs on the Cambridge ORL Face database, which consists of 400 images of 40 individuals, containing quite a high degree of variability in expression, pose, and facial details. The SVMs that have been used included the Linear (LSVM), Polynomial (PSVM), and Radial Basis Function (RBFSVM) SVMs. We provide experimental evidence which show that Polynomial and Radial Basis Function (RBF) SVMs performs better than Linear SVM on the ORL Face Dataset when both are used with one against all classification. We also compared the SVMs based recognition with the standard eigenface approach using the Multi-Layer Perceptron (MLP) Classification criterion.","Face recognition,
Principal component analysis,
Support vector machines,
Feature extraction,
Support vector machine classification,
Polynomials,
Law enforcement,
Security,
Indexing,
Robustness"
Gaming the jammer: Is frequency hopping effective?,"Frequency hopping has been the most popularly considered approach for alleviating the effects of jamming attacks. In this paper, we provide a novel, measurement-driven, game theoretic framework that captures the interactions between a communication link and an adversarial jammer, possibly with multiple jamming devices, in a wireless network employing frequency hopping (FH). The framework can be used to quantify the efficacy of FH as a jamming countermeasure. Our model accounts for two important factors that affect the aforementioned interactions: (a) the number of orthogonal channels available for use and (b) the frequency separation between these orthogonal bands. If the latter is small, then the energy spill over between two adjacent channels (considered orthogonal) is high; as a result a jammer on an orthogonal band that is adjacent to that used by a legitimate communication, can be extremely effective. We account for both these factors and using our framework we provide bounds on the performance of proactive frequency hopping in alleviating the impact of a jammer. The main contributions of our work are: (a) Construction of a measurement driven game theoretic framework which models the interactions between a jammer and a communication link that employ FH. (b) Extensive experimentation on our indoor testbed in order to quantify the impact of a jammer in a 802.11a/g network. (c) Application of our framework to quantify the efficacy of proactive FH across a variety of 802.11 network configurations. (d) Formal derivation of the optimal strategies for both the link and the jammer in 802.11 networks. Our results demonstrate that frequency hopping is largely inadequate in coping with jamming attacks in current 802.11 networks. In particular, we show that if current systems were to support hundreds of additional channels, FH would form a robust jamming countermeasure1.","Jamming,
Game theory,
Spread spectrum communication,
Frequency measurement,
Testing,
Wireless networks,
Transmitters,
Computer science,
Robustness,
Performance analysis"
3D Face Recognition Using Multiview Keypoint Matching,"A novel algorithm for 3D face recognition based point cloud rotations, multiple projections, and voted keypoint matching is proposed and evaluated. The basic idea is to rotate each 3D point cloud representing an individual's face around the x, y or z axes, iteratively projecting the 3D points onto multiple 2.5D images at each step of the rotation. Labeled keypoints are then extracted from the resulting collection of 2.5D images, and this much smaller set of keypoints replaces the original face scan and its projections in the face database. Unknown test faces are recognized firstly by performing the same multiview keypoint extraction technique, and secondly, the application of a new weighted keypoint matching algorithm. In an extensive evaluation using the GavabDB 3D face recognition dataset (61 subjects, 9 scans per subject), our method achieves up to 95% recognition accuracy for faces with neutral expressions only, and over 90% accuracy for face recognition where expressions (such as a smile or a strong laugh) and random face-occluding gestures are permitted.","Face recognition,
Clouds,
Testing,
Face detection,
Voting,
Surveillance,
Computer science,
Iterative algorithms,
Image databases,
Performance evaluation"
COCAST: Multicast mobile ad hoc networks using cognitive radio,"Many multicast protocols have been proposed for ad hoc networks. ODMRP is one of the most popular protocols due to its robustness in highly mobile wireless networks. However, ODMRP is unscalable in terms of the number of sources. We improve the scalability of ODMRP using cognitive radio technology, which diversifies channel usage for multicast members with multiple cognitive channels. We design a channel allocation and distribution scheme to build a multicast tree for each group. In addition, we implement cooperative sensing by multicast members using Join Query and Join Reply messages. In simulation experiments, our protocol, CoCast, demonstrates a superior delivery ratio and throughput performance to conventional ODMRP.","Mobile ad hoc networks,
Cognitive radio,
Multicast protocols,
Ad hoc networks,
Wireless application protocol,
Robustness,
Wireless networks,
Scalability,
Channel allocation,
Throughput"
Robustness evaluation of stereo algorithms on long stereo sequences,"This paper presents an approach to test stereo algorithms against long stereo sequences (say, 100+ image pairs). Stereo sequences of this length have not been quantitatively evaluated in the past, even though they are the input data of a vision-based driver assistance system. Using stereo sequences allows one to exploit the temporal information, which is, in general, not well used currently. The presented approach focuses on evaluating the robustness of algorithms against differing noise parameters (Gaussian noise, brightness differences, and blurring).","Stereo vision,
Testing,
Gaussian noise,
Noise robustness,
Performance evaluation,
Brightness,
Cost function,
Error correction,
Layout,
Computer science"
YinYang bipolar dynamic logic (BDL) and equilibrium-based computational neuroscience,"It is argued that without mental equilibrium mental disorder would be “big bang” from nowhere. Based on this argument, YinYang bipolar dynamic logic (BDL) and bipolar dynamic fuzzy logic (BDFL) are introduced; a theory of equilibrium-based computational neuroscience is presented with a 3-tier architecture. Agents, adaptivity, bipolar mental equilibrium, and bipolar disorders are mathematically characterized with bipolar relativity. It is shown that BDL and BDFL enable the unification of neural biological networks with bio-electromagnetic or quantum fields, equilibrium-based logical computation with “illogical” bipolar mental disorders, and hypothesis driven exploratory knowledge discovery with brain, behavior, and agents.","Neuroscience,
Quantum computing,
Fuzzy logic,
Biology computing,
Computer networks,
Mental disorders,
Physics,
Bioinformatics,
Computer applications,
Biological neural networks"
Designing of Robust Image Steganography Technique Based on LSB Insertion and Encryption,"This paper discusses the design of a Robust image steganography technique based on LSB (Least Significant Bit) insertion and RSA encryption technique. Steganography is the term used to describe the hiding of data in images to avoid detection by attackers. Steganalysis is the method used by attackers to determine if images have hidden data and to recover that data. The application discussed in this paper ranks images in a users library based on their suitability as cover objects for some data. By matching data to an image, there is less chance of an attacker being able to use steganalysis to recover the data. Before hiding the data in an image the application first encrypts it. The steganography method proposed in this paper and illustrated by the application is superior to that used by current steganography tools.","Robustness,
Steganography,
Cryptography,
Data engineering,
Design engineering,
Protection,
Communications technology,
Computer science,
Libraries,
Data encapsulation"
Distance transform templates for object detection and pose estimation,"We propose a new approach for detecting low textured planar objects and estimating their 3D pose. Standard matching and pose estimation techniques often depend on texture and feature points. They fail when there is no or only little texture available. Edge-based approaches mostly can deal with these limitations but are slow in practice when they have to search for six degrees of freedom. We overcome these problems by introducing the distance transform templates, generated by applying the distance transform to standard edge based templates. We obtain robustness against perspective transformations by training a classifier for various template poses. In addition, spatial relations between multiple contours on the template are learnt and later used for outlier removal. At runtime, the classifier provides the identity and a rough 3D pose of the distance transform template, which is further refined by a modified template matching algorithm that is also based on the distance transform. We qualitatively and quantitatively evaluate our approach on synthetic and real-life examples and demonstrate robust real-time performance.","Object detection,
Robustness,
Image edge detection,
Runtime,
Computer science,
Laboratories,
Visualization,
Layout,
Least squares methods,
Pixel"
Caveat eptor: A comparative study of secure device pairing methods,"“Secure Device Pairing” is the process of bootstrapping a secure channel between two previously unassociated devices over a (usually wireless) human-imperceptible communication channel. Lack of prior security context and common trust infrastructure open the door for Man-in-the-Middle (also known as Evil Twin) attacks. Mitigation of these attacks requires user involvement in the device pairing process. Prior research yielded a number of interesting methods utilizing various auxiliary human-perceptible channels, e.g., visual, acoustic or tactile. These methods engage the user in authenticating information exchanged over human-imperceptible channels, thus mitigating MiTM attacks and forming the basis for secure pairing. We present the first comprehensive comparative evaluation of notable secure device pairing methods. Our results identify methods best-suited for a given combination of devices and human abilities. This work is both important and timely, since it sheds light on usability in one of the very few settings where a wide range of users (not just specialists) are confronted with security techniques.","Communication system security,
Humans,
Wireless communication,
Usability,
Bluetooth,
Computer science,
Personal digital assistants,
Cameras,
Wireless sensor networks,
Hardware"
Efficiently Extracting Operational Profiles from Execution Logs Using Suffix Arrays,An important software reliability engineering tool is operational profiles. In this paper we propose a cost effective automated approach for creating second generation operational profiles using execution logs of a software product. Our algorithm parses the execution logs into sequences of events and produces an ordered list of all possible subsequences by constructing a suffix-array of the events. The difficulty in using execution logs is that the amount of data that needs to be analyzed is often extremely large (more than a million records per day in many applications). Our approach is very efficient. We show that our approach requires O(N) in space and time to discover all possible patterns in N events. We discuss a practical implementation of the algorithm in the context of the logs from a large cloud computing system.,"Software reliability,
Frequency,
Reliability engineering,
Computer science,
Cloud computing,
Humans,
Software systems,
Laboratories,
Costs,
Application software"
Stochastic Properties and Application of City Section Mobility Model,"Node mobility has a direct impact on the performance evaluation of various network mobility protocols. Unfortunately, most of the analysis on mobility protocols used Random Waypoint mobility model which does not represent real-world movement patterns of mobile nodes. In this paper, we have analyzed City Section mobility model, a realistic mobility model for the movement in the city streets. We have developed an analytical model to derive certain stochastic properties, such as, expected epoch length, expected epoch time, expected number of subnet crossings and subnet residence time of this model. Finally, we have applied the model to calculate the signaling cost of NEMO BSP and compared it with Random Waypoint mobility model. Results show that the use of realistic mobility model leads to better estimation of the signaling cost of network mobility protocol.",
SAR image despeckling via bilateral filtering,"Bilateral filtering (BF) can realise both smoothing images and preserving edges, whereas its filtering results are always influenced since its two parameters are difficult to configure to the optimum. In this reported work, the application of BF is extended to synthetic aperture radar (SAR) image despeckling, and the despeckling evaluation indexes, including the equivalent number of looks and the edge save index, are used to estimate the parameters. After BF with estimated parameters imposed on a normalised SAR image, further processing can achieve both despeckling and edge preservation simultaneously. Experimental results show that the visual quality and evaluation indexes of the proposed algorithm outperform the classical Lee filtering.",
An Image Based Auto-Focusing Algorithm forDigital Fundus Photography,"In fundus photography, the task of fine focusing the image is demanding and lack of focus is quite often the cause of suboptimal photographs. The introduction of digital cameras has provided an opportunity to automate the task of focusing. We have developed a software algorithm capable of identifying best focus. The auto-focus (AF) method is based on an algorithm we developed to assess the sharpness of an image. The AF algorithm was tested in the prototype of a semi-automated nonmydriatic fundus camera designed to screen in the primary care environment for major eye diseases. A series of images was acquired in volunteers while focusing the camera on the fundus. The image with the best focus was determined by the AF algorithm and compared to the assessment of two masked readers. A set of fundus images was obtained in 26 eyes of 20 normal subjects and 42 eyes of 28 glaucoma patients. The 95% limits of agreement between the readers and the AF algorithm were -2.56 to 2.93 and -3.7 to 3.84 diopter and the bias was 0.09 and 0.71 diopter, for the two readers respectively. On average, the readers agreed with the AF algorithm on the best correction within less than 3/4 diopter. The intraobserver repeatability was 0.94 and 1.87 diopter, for the two readers respectively, indicating that the limit of agreement with the AF algorithm was determined predominantly by the repeatability of each reader. An auto-focus algorithm for digital fundus photography can identify the best focus reliably and objectively. It may improve the quality of fundus images by easing the task of the photographer.","Photography,
Focusing,
Eyes,
Digital cameras,
Software algorithms,
Testing,
Software prototyping,
Prototypes,
Algorithm design and analysis,
Diseases"
A Framework of Testing as a Service,"This paper presents a framework of Testing as a Service (TaaS) which a new model to improve the efficiency of software quality assurance. We exploit the related key technical issues, and build a reference architecture of TaaS based on ontology, process automation and SOA techniques. A prototype for unit testing is implemented to demonstrate the validity of the framework.","Logic testing,
Electronic equipment testing,
Software testing,
Ontologies,
Software quality,
Partial response channels,
Web services,
Publishing,
Computer architecture,
Automation"
Evolutionary method combining particle swarm optimization and genetic algorithms using fuzzy logic for decision making,"We describe in this paper a new hybrid approach for mathematical function optimization combining Particle Swarm Optimization (PSO) and Genetic Algorithms (GAs) using Fuzzy Logic to integrate the results. The new evolutionary method combines the advantages of PSO and GA to give us an improved PSO+GA hybrid method. Fuzzy Logic is used to combine the results of the PSO and GA in the best way possible. The new hybrid PSO+GA approach is compared with the PSO and GA methods with a set of benchmark mathematical functions. The new hybrid PSO+GA method is shown to be superior that the individual evolutionary methods. The mathematical functions were evaluated with 2, 4, 8 and 32 variables to validate this approach.","Particle swarm optimization,
Genetic algorithms,
Fuzzy logic,
Decision making,
Optimization methods,
Power engineering and energy,
Fuzzy systems,
Computational modeling,
Helium,
Process design"
Precision Time Synchronization Using IEEE 1588 for Wireless Sensor Networks,"Wireless sensor networks are evolving from relatively undemanding applications to applications which have stronger requirements. The coordination of distributed entities and events requires time synchronization. Although a number of methods have been studied for WSNs, some applications require high precision time synchronization. Precision time synchronization enables a variety of extensions of applications. The IEEE 1588 precision time protocol (PTP) provides a standard method to synchronize devices in a network with sub-microsecond precision. This paper deals with precision time synchronization using IEEE 1588 over wireless sensor networks. Precision time synchronization using IEEE 1588 provides compatibility between heterogeneous systems in WSNs. This paper also presents experiments and performance evaluation of precision time synchronization in WSNs. Our result established a method for nodes in a network to maintain their clocks to within a 200 nanosecond offset from the reference clock of a master node.","Wireless sensor networks,
Synchronization,
Clocks,
Protocols,
Master-slave,
Computer networks,
Application software,
Delay effects,
Network-on-a-chip,
Marine technology"
Laser-based perception for door and handle identification,"In this paper, we present a laser-based approach for door and handle identification. The approach builds on a 3D perception pipeline to annotate doors and their handles solely from sensed laser data, without any a priori model learning. In particular, we segment the parts of interest using robust geometric estimators and statistical methods applied on geometric and intensity distribution variations in the scan. We present experimental results on a mobile manipulation platform (PR2) intended for indoor manipulation tasks. We validate the approach by generating trajectories that position the robot end-effector in front of door handles and grasp the handle. The robustness of our approach is demonstrated by real world experiments conducted on a large set of doors.","Robot sensing systems,
Laser theory,
Indoor environments,
Clouds,
Robustness,
Intelligent robots,
Laser modes,
Fixtures,
Orbital robotics,
Tactile sensors"
Statistical Based Impulsive Noise Removal in Digital Radiography,"A new filter to restore radiographic images corrupted by impulsive noise is proposed. It is based on a switching scheme where all the pulses are first detected and then corrected through a median filter. The pulse detector is based on the hypothesis that the major contribution to image noise is given by the photon counting process, with some pixels corrupted by impulsive noise. Such statistics is described by an adequate mixture model. The filter is also able to reliably estimate the sensor gain. Its operation has been verified on both synthetic and real images; the experimental results demonstrate the superiority of the proposed approach in comparison with more traditional methods.","Radiography,
Nonlinear filters,
Low pass filters,
Circuit noise,
Statistics,
Filtering,
Pixel,
Adaptive filters,
Computer science,
Digital filters"
Controller for TORCS created by imitation,"This paper is an initial approach to create a controller for the game TORCS by learning how another controller or humans play the game. We used data obtained from two controllers and from one human player. The first controller is the winner of the WCCI 2008 Simulated Car Racing Competition, and the second one is a hand coded controller that performs a complete lap in all tracks. First, each kind of controller is imitated separately, then a mix of the data is used to create new controllers. The imitation is performed by means of training a feed forward neural network with the data, using the backpropagation algorithm for learning.","Humans,
Games,
Artificial intelligence,
Computational modeling,
Computer simulation,
Artificial neural networks,
Backpropagation algorithms,
Internet,
Feeds,
Neural networks"
Quadratic Regularization Design for 2-D CT,"Statistical methods for tomographic image reconstruction have improved noise and spatial resolution properties that may improve image quality in X-ray computed tomography (CT). Penalized weighted least squares (PWLS) methods using conventional quadratic regularization lead to nonuniform and anisotropic spatial resolution due to interactions between the weighting, which is necessary for good noise properties, and the regularizer. Previously, we addressed this problem for parallel-beam emission tomography using matrix algebra methods to design data-dependent, shift-variant regularizers that improve resolution uniformity. This paper develops a fast angular integral mostly analytical (AIMA) regularization design method for 2-D fan-beam X-ray CT imaging, for which parallel-beam tomography is a special case. Simulation results demonstrate that the new method for regularization design requires very modest computation and leads to nearly uniform and isotropic spatial resolution in transmission tomography when using quadratic regularization.","Computed tomography,
Spatial resolution,
X-ray imaging,
Design methodology,
Statistical analysis,
Image reconstruction,
Image quality,
Least squares methods,
Anisotropic magnetoresistance,
Matrices"
A Partial-Protection Approach Using Multipath Provisioning,"We study the problem of reliably provisioning traffic using multipath routing in a mesh network. Traditional approaches handled reliability requirements using full-protection schemes. Although full-protection approaches offer high assurance, this assurance can be costly. We take a less expensive approach to maintain reliability by offering partial-protection. Specifically, our approach guarantees part of the requested bandwidth, rather than the full amount, in the event of a link failure. We first show that the amount of partial-protection that can be guaranteed is limited by the topology of the network and the bandwidth requirement of a connection request. We then propose an effective multipath algorithm that attempts to provision bandwidth requests while guaranteeing the maximum partial-protection possible. Results show that by effectively selecting paths that limit edge overuse, our algorithm achieves very low bandwidth blocking probability. Our algorithm also serves significantly more requested bandwidth than the protection approach.","Bandwidth,
Protection,
Network topology,
Computer network reliability,
Mesh networks,
Costs,
Communications Society,
Computer science,
Telecommunication network reliability,
Telecommunication traffic"
An Open Grid Services Architecture Primer,"To expand the use of distributed computer infrastructures as well as facilitate grid interoperability, OGSA has developed standards and specifications that address a range of scenarios, including high-throughput computing, federated data management, and service mobility.",
Adaptive Contour Features in oriented granular space for human detection and segmentation,"In this paper, a novel feature named adaptive contour feature (ACF) is proposed for human detection and segmentation. This feature consists of a chain of a number of granules in oriented granular space (OGS) that is learnt via the AdaBoost algorithm. Three operations are defined on the OGS to mine object contour feature and feature co-occurrences automatically. A heuristic learning algorithm is proposed to generate an ACF that at the same time define a weak classifier for human detection or segmentation. Experiments on two open datasets show that the ACF outperform several well-known existing features due to its stronger discriminative power rooted in the nature of its flexibility and adaptability to describe an object contour element.","Humans,
Face detection,
Space technology,
Detectors,
Object detection,
Image edge detection,
Heuristic algorithms,
Robustness,
Boosting,
Shape"
Unit Testing Approaches for BPEL: A Systematic Review,"Service-Oriented Architecture (SOA) is a new architectural style for developing distributed business applications. Nowadays, those applications are realized through web services, which are later grouped as web service compositions. Web service compositions language, like the BPELWS 2.0 standard, are extensions of imperative programming languages. Additionally, it presents a challenge for traditional white-box testing, due to its inclusion of specific instructions, concurrency, fault compensation and dynamic service discovery and invocation. In fact, there is a lack of unit testing approaches and tools, which has resulted in inefficient practices in testing and debugging of automated business processes. Therefore, we performed a systematic review study to analyze 27 different studies for unit testing approaches for BPEL. This paper aims to focus on a comprehensive review to identify a categorization, a description of test case generation approaches, empirical evidence, current trends in BPEL studies, and finally to end with future work for other researchers.",
Vision-Based Automated Single-Cell Loading and Supply System,"Automated continuous individual cell transfer is a critical step in single-cell applications using microfluidic devices. Cells must be aspirated gently from a buffer before transferring to operation zone so as not to artificially perturb their biostructures. Vision-based manipulation is a key technique for allowing nondestructive cell transportation. In this paper, we presented a design for an automated single-cell loading and supply system that can be integrated with complex microfluidic applications for examining or processing one cell at a time such as the current nuclear transplantation method. The aim of the system is to automatically transfer mammalian donor (~ 15 ¿m) or oocyte (~ 100 ¿ m) cells one by one from a container to a polydimethylsiloxane (PDMS) microchannel and then transport them to other modules. The system consists of two main parts: a single-cell suction module, and a PDMS-based microfluidic chip controlled by an external pump. The desired number of vacuumed cells can be directed into the microfluidic chip and stored in a docking area. From the batch, they can be moved to next module by activating pneumatic pressure valves located on two sides of the chip. The entire mechanism is combined with monitoring systems that perform detection/tracking and control.",
Profiling Java programs for parallelism,"One of the biggest challenges imposed by multi-core architectures is how to exploit their potential for legacy systems not built with multiple cores in mind. By analyzing dynamic data dependences of a program run, one can identify independent computation paths that could have been handled by individual cores. Our prototype computes dynamic dependences for Java programs and recommends locations to the programmer with the highest potential for parallelization. Such measurements can also provide starting points for automatic, speculative parallelization.","Java,
Parallel processing,
Concurrent computing,
Computer hacking,
Computer architecture,
Computer science,
Data analysis,
Prototypes,
Programming profession,
Scalability"
Efficient scale space auto-context for image segmentation and labeling,"The conditional random fields (CRF) model, using patch-based classification bound with context information, has been widely adopted for image segmentation/ labeling. In this paper, we propose three components for improving the speed and accuracy, and illustrate them on a developed auto-context algorithm: (1) a new coding scheme for multiclass classification, named data-assisted output code (DAOC); (2) a scale-space approach to make it less sensitive to geometric scale change; and (3) a region-based voting scheme to make it faster and more accurate at object boundaries. The proposed multiclass classifier, DAOC, is general and particularly appealing when the number of class becomes large since it needs a minimal number of [log2 k] binary classifiers for k classes. We show advantages of the DAOC classifier over the existing algorithms on several Irvine repository datasets, as well as vision applications. Combining DAOC, the scale-space approach, and the region-based voting scheme for autocontext, the overall algorithm is significantly faster (5 ~ 10 times) than the original auto-context, with improved accuracy over many of the existing algorithms on theMSRC and VOC 2007 datasets.","Image segmentation,
Labeling,
Testing,
Voting,
Context modeling,
Pixel,
Neuroimaging,
Nervous system,
Computer science,
Large-scale systems"
Optimal quarantining of wireless malware through power control,"The topic of malware propagation in mobile wireless networks is gaining momentum among the research community, as actual vulnerabilities are revealed through recent outbreaks of worms. We introduce a defense strategy that quarantines the malware by reducing the communication range. This counter-measure faces us to a trade-off: reducing the communication range suppresses the spread of the malware, however, it also negatively affects the performance of the network as the end-to-end communication delay increases. We model the propagation of the malware as a deterministic epidemic. Using an optimal control framework, we select the optimal communication range that captures the above trade-off by minimizing a global cost function. Using Pontryagin's Maximum Principle, we derive structural characteristics of the optimal communication range as a function of time for two different cost functions.",
Guided analysis of hurricane trends using statistical processes integrated with interactive parallel coordinates,"This paper demonstrates the promise of augmenting interactive multivariate representations with information from statistical processes in the domain of weather data analysis. Statistical regression, correlation analysis, and descriptive statistical calculations are integrated via graphical indicators into an enhanced parallel coordinates system, called the Multidimensional Data eXplorer (MDX). These statistical indicators, which highlight significant associations in the data, are complemented with interactive visual analysis capabilities. The resulting system allows a smooth, interactive, and highly visual workflow. The system's utility is demonstrated with an extensive hurricane climate study that was conducted by a hurricane expert. In the study, the expert used a new data set of environmental weather data, composed of 28 independent variables, to predict annual hurricane activity. MDX shows the Atlantic Meridional Mode increases the explained variance of hurricane seasonal activity by 7–15% and removes less significant variables used in earlier studies. The findings and feedback from the expert (1) validate the utility of the data set for hurricane prediction, and (2) indicate that the integration of statistical processes with interactive parallel coordinates, as implemented in MDX, addresses both deficiencies in traditional weather data analysis and exhibits some of the expected benefits of visual data analysis.","Hurricanes,
Weather forecasting,
Data visualization,
Data analysis,
Multidimensional systems,
Statistical analysis,
Computer interfaces,
Physics computing,
Regression analysis,
Performance analysis"
Adaptive bayesian filtering for vibration-based terrain classification,"Outdoor robots are faced with a variety of terrain types each possessing different characteristics. To ensure a safe traversal a robot has to infer the current ground surface from sensor readings. Recent techniques generate a model which predicts the terrain class from single vibration signals disregarding the temporal coherence between consecutive measurements. In this paper, we present a novel approach in which the final classification relies on the analysis of not only one, but several recent observations. Therefore, the probabilistic framework of the Bayes filter is adopted to the problem of terrain classification. We propose an adaptive approach which automatically adjusts its parameters according to the history of observations. To demonstrate the performance of our method we further describe and compare another technique based on temporal coherence. The evaluation using data collected from our RWI ATRV-Jr robot shows that our approach is both reactive and stable enough to detect fast terrain transitions and selective misclassifications.","Bayesian methods,
Adaptive filters,
Filtering,
Robot sensing systems,
Vibration measurement,
Sensor phenomena and characterization,
Signal generators,
Predictive models,
Coherence,
History"
Microwave non-invasive sensing of respiratory tidal volume,This paper describes the use of Doppler radar to measure respiration rate and air volume. The respiratory volume is measured indirectly via chest wall position. Calibration of displacement to airflow prior to subject measurements and accurate chest wall position information enable mean differences of less than 10 ml; with standard deviation of the difference of 20 ml between radar and reference measurements.,"Position measurement,
Volume measurement,
Displacement measurement,
Doppler radar,
Calibration,
Measurement standards,
Radar measurements"
Protein Synthesis in Giant Liposomes Using the In Vitro Translation System of Thermococcus kodakaraensis,"An in vitro translation system, based on cell components of the hyperthermophilic archaeon, Thermococcus kodakaraensis, has previously been developed. The system has been optimized and applied for protein production at high temperatures (60-65°C). In this paper, we have examined the possibilities to utilize this system at a lower temperature range using green fluorescence protein (GFP) as the reporter protein. By optimizing the composition of the reaction mixture, and adding chaperonins from the mesophilic Escherichia coli, the yield of protein production at 40°C was increased by fivefold. For liposome encapsulation of the optimized system, water-in-oil cell-sized emulsions were prepared by adding the translation system/GFP mRNA mixture to mineral oil supplemented with 1,2-dioleoyl-sn -glycero-3-phosphatidylcholine (DOPC). Giant liposomes were formed when these emulsions passed across a water/oil interface occupied with DOPC. The liposomes were incubated at 40°C for 90 min, and fluorescence was examined by laser confocal microscopy. A significant increase in average fluorescence intensity was observed in liposomes with GFP mRNA, but not in those without mRNA. Our results indicate that the T. kodakaraensis in vitro translation system is applicable for protein production within giant liposomes, and these artificial cell models should provide the methodology to reconstitute various cell functions from a constitutional biology approach.","Proteins,
In vitro,
Fluorescence,
Cells (biology),
Production systems,
Petroleum,
Hyperthermia,
Temperature distribution,
Encapsulation,
Minerals"
A Novel Self-Optimizing Handover Mechanism for Multi-service Provisioning in LTE-Advanced,"The 3GPP is defining the next generation radio access network which introduces advanced self- optimization to reduce the operational efforts and complexity. It is also of interest to minimize operational effort by introducing self-optimizing mechanisms. Optimization of the handover parameters is an important goal of LTE-Advanced. In this paper, a new self- optimizing handover algorithm based on the estimation of the number of cell-boundary crossings is proposed. The algorithm adjusts the handover parameters, such as Time- to-Trigger, measurement interval, and hysteresis, by comparing the number of cell-boundary crossings and the number of handovers UE performed in a period. To optimize the handover performance further, an adaptive L3 filter based on the UE’s velocity is proposed in this paper. A typical application scenario of this algorithm is high/medium speed environment under multi-service provisioning. The simulation results shows that the algorithms proposed in this paper have a better performance in the rate of ping-pong handovers and the average number of handovers.","Radio access networks,
Hysteresis,
Filters,
Quality of service,
3G mobile communication,
Vehicles,
Long Term Evolution,
Computer science,
Next generation networking,
Performance evaluation"
Web Service Substitution Based on Preferences Over Non-functional Attributes,"In many applications involving composite Web services, one or more component services may become unavailable. This presents us with the problem of identifying other components that can take their place, while maintaining the overall functionality of the composite service. Given a choice of candidate substitutions that offer the desired functionality, it is often necessary to select the most preferred substitution based on non-functional attributes of the service, e.g., security, reliability, etc. We propose an approach to this problem using preference networks for representing and reasoning about preferences over non-functional properties. We present algorithms for solving several variants of this problem: a) when the choice of the preferred substitution is independent of the other constituents of the composite service; b) when the choice of the preferred substitution depends on the other constituents of the composite service; and c) when multiple constituents of a composite service need to be replaced simultaneously. The proposed solutions to the service substitution problem based on preferences over non-functional properties are independent of the specific formalism used to represent functional requirements of a composite service as well as the specific algorithm used to assemble the composite service.","Web services,
Security,
Assembly,
Costs,
Application software,
Maintenance,
Context-aware services,
Computer science,
USA Councils,
Distributed computing"
EMI prediction in switched power supplies by full-wave and non-linear circuit co-simulation,"This paper treats the problem of electromagnetic interference in switched mode power supplies using co-simulation. Co-simulation combines full-wave EM solution with non linear SPICE circuit. Voltages, currents (at different nodes in the circuit) that drive the EMI can be simulated. Different co-simulation strategies are discussed along with their pros and cons. Also different commercial software tools have been evaluated for this simulation technique and promising results have been compared to measurements of voltages, TEM cell coupling and coupling to an antenna. The circuit investigated is a DC-DC buck converter.","Electromagnetic interference,
Switching circuits,
Power supplies,
Switched-mode power supply,
Voltage,
Circuit simulation,
Coupling circuits,
SPICE,
Software tools,
Antenna measurements"
Shared Kernel Information Embedding for discriminative inference,"Latent variable models (LVM), like the shared-GPLVM and the spectral latent variable model, help mitigate over-fitting when learning discriminative methods from small or moderately sized training sets. Nevertheless, existing methods suffer from several problems: (1) complexity; (2) the lack of explicit mappings to and from the latent space; (3) an inability to cope with multi-modality; and (4) the lack of a well-defined density over the latent space. We propose a LVM called the shared kernel information embedding (sKIE). It defines a coherent density over a latent space and multiple input/output spaces (e.g., image features and poses), and it is easy to condition on a latent state, or on combinations of the input/output states. Learning is quadratic, and it works well on small datasets. With datasets too large to learn a coherent global model, one can use sKIE to learn local online models. sKIE permits missing data during inference, and partially labelled data during learning. We use sKIE for human pose inference.","Kernel,
Humans,
Computer science,
Graphical models,
Computer vision,
Shape,
Semisupervised learning,
Gaussian processes,
Topology"
On Traffic Characteristics of a Broadband Wireless Internet Access,"Internet traffic measurements and traffic characterization are essential for managing and optimizing network infrastructures. The increasing number of wireless Internet users and the changing application demands require consecutive traffic measurements. Therefore, we have performed measurements of home users at a broadband wireless access service provider in order to reflect the current traffic characteristics. In this paper, we present the results of these measurements like application distributions as well as changing traffic characteristics caused by user demands and new services. The results could be used by a network service provider to optimize its network performance in order to give quality of service (QoS) guarantees for home users in its fixed wireless network.","Internet,
Telecommunication traffic,
Performance evaluation,
Bandwidth,
Streaming media,
Monitoring,
Quality of service,
IP networks,
Computer science,
Computer network management"
A novel approach to path planning for multiple robots in bi-connected graphs,"This paper addresses a problem of path planning for multiple robots. An abstraction where the environment for robots is modeled as an undirected graph with robots placed in its vertices is used (this abstraction is also known as the problem of pebble motion on graphs). A class of the problem with bi-connected graph and at least two unoccupied vertices is defined. A novel polynomial-time solution algorithm for this class of problem is proposed. It is shown in the paper that the new algorithm significantly outperforms the existing state-of-the-art techniques applicable to the problem. Moreover, the performed experimental evaluation indicates that the new algorithm scales up well which make it suitable for practical problem solving.","Path planning,
Robot kinematics,
Orbital robotics,
Robotics and automation,
Intelligent robots,
Topology,
Polynomials,
Performance evaluation,
Problem-solving,
Helium"
Expansion segmentation for visual collision detection and estimation,"Collision detection and estimation from a monocular visual sensor is an important enabling technology for safe navigation of small or micro air vehicles in near earth flight. In this paper, we introduce a new approach called expansion segmentation, which simultaneously detects “collision danger regions” of significant positive divergence in inertial aided video, and estimates maximum likelihood time to collision (TTC) in a correspondenceless framework within the danger regions. This approach was motivated from a literature review which showed that existing approaches make strong assumptions about scene structure or camera motion, or pose collision detection without determining obstacle boundaries, both of which limit the operational envelope of a deployable system. Expansion segmentation is based on a new formulation of 6-DOF inertial aided TTC estimation, and a new derivation of a first order TTC uncertainty model due to subpixel quantization error and epipolar geometry uncertainty. Proof of concept results are shown in a custom designed urban flight simulator and on operational flight data from a small air vehicle.","Maximum likelihood detection,
Uncertainty,
Vehicle detection,
Navigation,
Vehicle safety,
Earth,
Maximum likelihood estimation,
Layout,
Cameras,
Envelope detectors"
"Aqua-Net: An underwater sensor network architecture: Design, implementation, and initial testing","Underwater sensor networks (UWSNs) have emerged as a powerful technique for a wide range of aquatic applications. New challenges require novel research at every layer of the protocol stack. To efficiently compare various design, algorithms, and protocols, there is a pressing need for a generic architecture for underwater sensor networks. In this paper, we present such an architecture, called Aqua-Set, for various underwater solutions and applications. Aqua-Net follows a layered structure and meanwhile could support cross-layer optimization. It is flexible and easy to use. We have implemented several protocols in Aqua-Net and tested in a lab environment. Specifically, in this paper we present the design, implementation, and testing of a MAC protocol, called UW-Aloha, as a case study. The initial results show that our analysis, and lab testing match very well. We consider Aqua-Net a groundwork and valuable tool for future advances of the underwater sensor network research.","Testing,
Media Access Protocol,
Acoustic sensors,
Computer architecture,
Application software,
Underwater acoustics,
Sensor phenomena and characterization,
Access protocols,
Electromagnetic scattering,
Computer science"
Feature selection for person-independent 3D facial expression recognition using NSGA-II,"In this paper, the problem of person-independent facial expression recognition from 3D facial features is investigated. We propose a methodology for the selection of features that uses a multi-objective genetic algorithm where the number of features is optimized to improve classification accuracy. The facial feature selection aims to derive a set of features from the original expression images, which minimizes the within-class separability and maximizes the between-class separability. We used non-dominated sorted genetic algorithm II (NSGA II) which is one of the latest genetic algorithms developed for resolving problems of multi-objective aspects with more accuracy and higher convergence speed. The proposed methodology is evaluated using 3D facial expression database BU-3DFE. Facial expressions such as anger, sadness, surprise, joy, disgust, fear and neutral are successfully recognized with an average recognition rate of 88.18%.","Face recognition,
Genetic algorithms,
Facial features,
Optimization methods,
Spatial databases,
Pattern recognition,
Feature extraction,
Electronic mail,
Handwriting recognition,
Volume measurement"
Integrating symbolic and geometric planning for mobile manipulation,"Mobile manipulation requires to solve multiple subproblems. One is planning in high-dimensional configuration spaces, that we approach in this work. We decompose the manipulation problem into a symbolic and a geometric part. The symbolic part is implemented as a classical symbolic planner that tightly integrates a geometric planner enabling us to efficiently generate correct plans. A probabilistic roadmap planner constitutes the geometric part. During the computation of the roadmap we utilize proximity queries to determine non-colliding configurations and to verify collision-free paths between configurations accurately and efficiently. We demonstrate experiments in two scenarios, one of these being the manipulator dexterity test scenario that was used in NIST's response robot evaluation in Disaster City.","Manipulators,
Robot kinematics,
Testing,
Cities and towns,
Orbital robotics,
Mobile computing,
NIST,
Layout,
Control systems,
Computer science"
Hyper-learning for population-based incremental learning in dynamic environments,"The population-based incremental learning (PBIL) algorithm is a combination of evolutionary optimization and competitive learning. Recently, the PBIL algorithm has been applied for dynamic optimization problems. This paper investigates the effect of the learning rate, which is a key parameter of PBIL, on the performance of PBIL in dynamic environments. A hyper-learning scheme is proposed for PBIL, where the learning rate is temporarily raised whenever the environment changes. The hyper-learning scheme can be combined with other approaches, e.g., the restart and hypermutation schemes, for PBIL in dynamic environments. Based on a series of dynamic test problems, experiments are carried out to investigate the effect of different learning rates and the proposed hyper-learning scheme in combination with restart and hypermutation schemes on the performance of PBIL. The experimental results show that the learning rate has a significant impact on the performance of the PBIL algorithm in dynamic environments and that the effect of the proposed hyper-learning scheme depends on the environmental dynamics and other schemes combined in the PBIL algorithm.","Testing,
Constraint optimization,
Heuristic algorithms,
Evolutionary computation,
Convergence,
Computer science,
Councils,
Genetics,
Statistics,
Performance analysis"
Laguerre neural network-based smart sensors for wireless sensor networks,"A wireless sensor network comprises of several nodes (also called motes). A mote communicates with other nodes based on the information collected through the sensor module attached with multiple sensors, e.g., accelerometer, pressure, temperature and humidity sensors. It is important that the sensors provide accurate readout of the physical quantity that they sense, especially when the motes are operated in harsh environments. In this paper we propose intelligent sensors for the sensor module using a computationally efficient Laguerre neural networks (LaNN) to auto-compensate for the associated nonlinearity and environmental dependence, and provide linearized sensor readout even when the motes are operated in harsh environments. By taking an example of a capacitive pressure sensor, through computer simulations we have shown that the LaNN-based sensor model can provide highly linearized sensor output. The performance of the LaNN sensor model is compared with a multilayer perceptron-based sensor model, and it is observed that the former model is superior in terms of computational efficiency while providing similar linearity performance.",
Synthesis of compromise sum-difference arrays through time-modulation,"In this study, time-modulation is exploited for the synthesis of monopulse sub-arrayed antennas. The solution of the compromise sum-difference problem is obtained by setting the set of static excitations to an optimal sum set and synthesising the `best compromise` difference pattern through a contiguous partition method based approach. The array elements are aggregated into sub-arrays controlled by means of radio frequency (RF) switches with optimised `on` time-durations. The switch-on instants of the pulse sequences are then computed by means of a particle swarm optimiser to reduce the interferences caused by the sideband radiations. A selected set of numerical results is reported to assess the potentialities of time-modulation in dealing with the synthesis problem at hand.",
Image transmissions with security enhancement based on region and path diversity in wireless sensor networks,"Transmissions of large sized images can be a bottleneck for a wireless sensor network (WSN) due to its limited resources. Security can be another concern. This paper proposes a collaborative transmission scheme for image sensors to utilize inter-sensor correlations to decide the transmission and security sharing patterns based on the path diversities. Our proposed approach for secret image sharing on multiple node-disjoint paths for image delivery is to achieve high security without any key distribution and management, and thus the key management related problems do not exist. The energy efficiency is another major contribution made in this paper. This scheme does not only allow each image sensor to transmit optimal fractions of overlapped images through appropriate transmission paths in an energy-efficient way, but also provides unequal protection to overlapped image regions by path selections and adaptive bit error rate (BER) requirement. The simulation results show that the proposed scheme can achieve considerable gains in terms of network lifetime extension, image transmission security enhancement, image quality improvement, and energy efficiency for wireless sensor networks.",
An efficient implementation of 1-D median filter,"This paper presents a new architecture and circuit implementation of 1-D median filter. The proposed circuit belongs to the class of non-recursive sorting network architectures that process the input samples sequentially in the word-based manner. In comparison to the related schemes, it maintains sorting of samples from the previous position of the sliding window, positioning only the incoming sample to the correct rank. Unlike existing 1-D filter implementations, the circuit has linear hardware complexity, minimal latency and achieves throughput of 1/2 of the sampling rate. Experimental evaluation and comparisons show high efficiency of our design.","Hardware,
Sorting,
Circuits,
Filtering,
Delay,
Sampling methods,
Computer architecture,
Throughput,
Digital filters,
Signal processing"
"Joint power control, scheduling and routing for multicast in multihop energy harvesting sensor networks","We consider the problem of joint power control, scheduling and routing in energy harvesting sensor networks allowing for multicast of data generated at the sensor nodes to a set of sink nodes. In this setup we exploit broadcast nature of the channels and network coding to show performance improvement. We also develop computationally efficient suboptimal algorithms and study their performance.","Power control,
Routing,
Spread spectrum communication,
Network coding,
Wireless sensor networks,
Processor scheduling,
Energy consumption,
Time sharing computer systems,
Actuators,
Throughput"
"Integrating indoor mobility, object manipulation, and intuitive interaction for domestic service tasks","Domestic service tasks require three main skills from autonomous robots: robust navigation, mobile manipulation, and intuitive communication with the users. Most robot platforms, however, support only one or two of the above skills. In this paper we present Dynamaid, a new robot platform for research on domestic service applications. For robust navigation, Dynamaid has a base with four individually steerable differential wheel pairs, which allow omnidirectional motion. For mobile manipulation, Dynamaid is additionally equipped with two anthropomorphic arms that include a gripper, and with a trunk that can be lifted as well as twisted. For intuitive multimodal communication, the robot has a microphone, stereo cameras, and a movable head. Its humanoid upper body supports natural interaction. It can perceive persons in its environment, recognize and synthesize speech. We developed software for the tests of the RoboCup@Home competitions, which serve as benchmarks for domestic service robots. With Dynamaid and our communication robot Robotinho, our team Nimbro@Home took part in the RoboCup German Open 2009 and RoboCup 2009 competitions in which we came in second and third, respectively. We also won the innovation award for innovative robot design, empathic behaviors, and robot-robot cooperation.","Mobile robots,
Robustness,
Navigation,
Mobile communication,
Speech synthesis,
Wheels,
Anthropomorphism,
Arm,
Grippers,
Microphones"
Reducing power consumption in wired networks,"Over 500 million host computers, three billion PCs and mobile devices consume over a billion kilowatts of electricity. As part of this “system” computer networks consume an increasing amount of energy, and help reduce energy expenditure from other sources through E-Work, E-Commerce and E-Learning. Traditionally, network design seeks to minimise network cost and maximise quality of service (QoS). This paper examines some approaches for dynamically managing wired packet networks to minimise energy consumption while meeting users' QoS needs, by automatically turning link drivers and/or routers on/off in response to changes in network load.",
A novel Qos-aware MAC scheme using optimal retransmission for wireless networks,"This paper proposes a novel medium access control scheme for low cost, single-hop wireless networks where the source nodes have a transmitter module but no receiver module and hence they can only transmit data to a sink but cannot receive any control signals, like an ACK or NAK, from any other node. The goal of the proposed scheme is to provide QoS (in terms of packet delivery probability) to the nodes in such a network, where the existing schemes like polling or scheduled transmissions, CSMA and ARQ will be ineffective because of the unavailability of a receiver module at the nodes. The proposed scheme uses distributed control and allows the nodes to transmit each packet an optimal number of times at random instants in time within the packet generation interval. We define two optimization problems based on minimizing total network traffic and maximizing the delivery probability of the class of nodes requiring the highest QoS, respectively, and develop mathematical formulae and efficient algorithms to solve them. Numerical analysis and simulation results show that our scheme can provide high QoS to networks of different sizes.","Wireless networks,
Media Access Protocol,
Costs,
Transmitters,
Multiaccess communication,
Automatic repeat request,
Distributed control,
Random number generation,
Communication system traffic control,
Traffic control"
REST2SOAP: A framework to integrate SOAP services and RESTful services,"Nowadays, the mainstream of Web 2.0 website services is in the REST style called RESTful web service. RESTful services have been widely accepted by the public because of the usability and simplicity. Meanwhile, web service technologies realize service-oriented architecture (SOA) successfully and are exploited in both industry and academia. Notably, service composition that can aggregate existing services into a new one delivers the most benefits of SOA through BPEL standard. However, BPEL cannot support RESTful services and integrate both Web 2.0 and SOA-based resources in a composite service directly. It means that it is costly or time-consuming to utilize RESTful services in an SOA-based application. Therefore, this paper presents a framework, namely REST2SOAP, to integrate SOAP services and RESTful services. REST2SOAP leverages WADL specification, and can wrap RESTful services into SOAP services semi-automatically. Using REST2SOAP, developers can create a BPEL service that combines SOAP, RESTful services and user interfaces simultaneously.","Simple object access protocol,
Web services,
Service oriented architecture,
Mashups,
Computer science,
Oceans,
Usability,
Marine technology,
Aggregates,
User interfaces"
Joint visual attention modeling for naturally interacting robotic agents,"This paper elaborates on mechanisms for establishing visual joint attention for the design of robotic agents that learn through natural interfaces, following a developmental trajectory not unlike infants. We describe first the evolution of cognitive skills in infants and then the adaptation of cognitive development patterns in robotic design. A comprehensive outlook for cognitively inspired robotic design schemes pertaining to joint attention is presented for the last decade, with particular emphasis on practical implementation issues. A novel cognitively inspired joint attention fixation mechanism is defined for robotic agents.","Robots,
Text categorization,
Support vector machines,
Linear discriminant analysis,
Support vector machine classification,
Frequency,
Graphical models,
Induction generators,
Performance gain,
Statistics"
A note on optimal support recovery in compressed sensing,"Recovery of the support set (or sparsity pattern) of a sparse vector from a small number of noisy linear projections (or samples) is a “compressed sensing” problem that arises in signal processing and statistics. Although many computationally efficient recovery algorithms have been studied, the optimality (or gap from optimality) of these algorithms is, in general, not well understood. In this note, approximate support recovery under a Gaussian prior is considered, and it is shown that optimal estimation depends on the recovery metric in general. By contrast, it is shown that in the SNR limits, there exist uniformly near-optimal estimators, namely, the ML estimate in the high SNR case, and a computationally trivial thresholding algorithm in the low SNR case.","Compressed sensing,
Signal processing algorithms,
Vectors,
Statistics,
Maximum likelihood estimation,
Algorithm design and analysis,
Computational complexity,
Indexing,
Distortion measurement,
Signal sampling"
Genetic algorithm for DAG scheduling in Grid environments,"Complex applications are describing using work-flows. Execution of these workflows in Grid environments require optimized assignment of tasks on available resources according with different constrains. This paper presents a decentralized scheduling algorithm based on genetic algorithms for the problem of DAG scheduling. The genetic algorithm presents a powerful method for optimization and could consider multiple criteria in optimization process. Also, we describe in this paper the integration platform for the proposed algorithm in Grid systems. We make a comparative evaluation with other existing DAG scheduling solution: Cluster ready Children First, Earliest Time First, Highest Level First with Estimated Times, Improved Critical Path with Descendant Prediction) and Hybrid Remapper. We carry out our experiments using a simulation tool with various scheduling scenarios and with heterogeneous input tasks and computation resources. We present several experimental results that offer a support for near-optimal algorithm selection.","Genetic algorithms,
Processor scheduling,
Scheduling algorithm,
Application software,
Constraint optimization,
Optimization methods,
Computational modeling,
Resource management,
Costs,
Automatic control"
(De) focusing on global light transport for active scene recovery,"Most active scene recovery techniques assume that a scene point is illuminated only directly by the illumination source. Consequently, global illumination effects due to inter-reflections, sub-surface scattering and volumetric scattering introduce strong biases in the recovered scene shape. Our goal is to recover scene properties in the presence of global illumination. To this end, we study the interplay between global illumination and the depth cue of illumination defocus. By expressing both these effects as low pass filters, we derive an approximate invariant that can be used to separate them without explicitly modeling the light transport. This is directly useful in any scenario where limited depth-of-field devices (such as projectors) are used to illuminate scenes with global light transport and significant depth variations. We show two applications: (a) accurate depth recovery in the presence of global illumination, and (b) factoring out the effects of defocus for correct direct-global separation in large depth scenes. We demonstrate our approach using scenes with complex shapes, reflectances, textures and translucencies.","Layout,
Lighting,
Shape,
Light scattering,
Low pass filters,
Kernel,
Optical scattering,
Robots,
Computer science,
Convolution"
Learning semantic scene models by object classification and trajectory clustering,"Activity analysis is a basic task in video surveillance and has become an active research area. However, due to the diversity of moving objects category and their motion patterns, developing robust semantic scene models for activity analysis remains a challenging problem in traffic scenarios. This paper proposes a novel framework to learn semantic scene models. In this framework, the detected moving objects are first classified as pedestrians or vehicles via a co-trained classifier which takes advantage of the multiview information of objects. As a result, the framework can automatically learn motion patterns respectively for pedestrians and vehicles. Then, a graph is proposed to learn and cluster the motion patterns. To this end, trajectory is parameterized and the image is cut into multiple blocks which are taken as the nodes in the graph. Based on the parameters of trajectories, the primary motion patterns in each node (block) are extracted via Gaussian mixture model (GMM), and supplied to this graph. The graph cut algorithm is finally employed to group the motion patterns together, and trajectories are clustered to learn semantic scene models. Experimental results and applications to real world scenes show the validity of our proposed method.",
Model space visualization for multivariate linear trend discovery,"Discovering and extracting linear trends and correlations in datasets is very important for analysts to understand multivariate phenomena. However, current widely used multivariate visualization techniques, such as parallel coordinates and scatterplot matrices, fail to reveal and illustrate such linear relationships intuitively, especially when more than 3 variables are involved or multiple trends coexist in the dataset. We present a novel multivariate model parameter space visualization system that helps analysts discover single and multiple linear patterns and extract subsets of data that fit a model well. Using this system, analysts are able to explore and navigate in model parameter space, interactively select and tune patterns, and refine the model for accuracy using computational techniques. We build connections between model space and data space visually, allowing analysts to employ their domain knowledge during exploration to better interpret the patterns they discover and their validity. Case studies with real datasets are used to investigate the effectiveness of the visualizations.","Data mining,
Data visualization,
Scattering,
Pattern analysis,
Data analysis,
Navigation,
Predictive models,
Extraterrestrial phenomena,
Computer science,
User interfaces"
Beyond pretty pictures: Examining the benefits of code visualization for Open Source newcomers,"Joining an Open Source project is not easy. Newcomers often experience a steep learning curve dealing with technical complexity, lack of domain knowledge, and the amount of project information available for starters. This paper looks at the information needs of newcomers and the potential benefits of information visualization in supporting newcomers through a controlled experiment. Our results show that current OSS environments and development tools are lacking in support for the information needs of newcomers, and that existing visualization tools and techniques can help. We also discuss the potential problems and pitfalls associated with the inappropriate use of code visualization tools.","Visualization,
Computer bugs,
Computer science,
Open source software,
Writing,
Software debugging,
Joining IEEE,
History,
Joining processes"
Accelerated Learning (?),"We wish to pose accelerated learning as a challenge for intelligent systems technology. Research on intelligent tutoring systems has proved that accelerated learning is possible. The Sherlock tutor for electronics troubleshooting, for example, condensed four years of on-the-job training to approximately 25 hours, compressing the duration of the experience-feedback-learning cycle. But accelerated learning should refer to more than the hastening of basic proficiency. It reaches across the proficiency scale to the question of how to accelerate the achievement of expertise, and whether that is even possible. Paralleling this question are practical issues, including the military's need to conduct training at a rapid pace, and the issues of workforce and loss of expertise. Many organizations such as the US Department of Defense, NASA, and the electric utilities are at risk because of the imminent retirement of domain practitioners who handle the most difficult and mission-critical challenges. To accelerate proficiency, we must facilitate the acquisition of extensive, highly organized knowledge. We must also accelerate the acquisition of expert-level reasoning skills and strategies. But that's just the beginning of the challenge.","Acceleration,
Cognitive science,
Employee welfare,
Feedback,
Problem-solving,
Medical services,
Weather forecasting,
Human factors,
Learning systems,
Lead"
Wireless power recharging for implantable bladder pressure sensor,"This paper presents a wireless power recharging system design for implantable bladder pressure chronic monitoring application. The power recharging system consists of an external 4-turn 15-cm-diameter powering coil and a silicone-encapsulated implantable spiral coil with a dimension of 7 mm × 17 mm × 2.5 mm and 18 turns, which further encloses an ASIC with a programmable charging current and logic control capability, a 3-mm-diameter 12-mm-long rechargeable battery, and two ferrite rods. The ferrite rods are employed to improve the quality factor of the implantable coil. For a constant charging current of 100 µA, an RF power of 2.4 mW needs to be coupled into the implantable microsystem through tuned coil loops. With the two coils aligned coaxially or with a tilting angle up to 30°, an external RF power of 7W or 25W is required, respectively, for a large coupling distance of 20 cm at an optimal frequency of 3 MHz.","Wireless sensor networks,
Bladder,
Coils,
Ferrites,
Radio frequency,
Monitoring,
Spirals,
Application specific integrated circuits,
Programmable control,
Control systems"
Improving bug tracking systems,"It is important that information provided in bug reports is relevant and complete in order to help resolve bugs quickly. However, often such information trickles to developers after several iterations of communication between developers and reporters. Poorly designed bug tracking systems are partly to blame for this exchange of information being stretched over time. Our paper addresses the concerns of bug tracking systems by proposing four broad directions for enhancements. As a proof-of-concept, we also demonstrate a prototype interactive bug tracking system that gathers relevant information from the user and identifies files that need to be fixed to resolve the bug.","Computer bugs,
Prototypes,
Computer science,
Laboratories,
Testing,
Delay effects,
Relational databases,
Usability,
Decision trees,
Data mining"
Energy-efficient distributed spectrum sensing with convex optimization,"We consider the problem of distributed spectrum sensing in cognitive radio networks with a central fusion center, from an energy efficiency viewpoint. In our scheme, each cognitive radio adopts a combination of sleeping and censoring to obtain a sensing result based on energy detection, while the fusion center combines all the sensing results using an OR decision rule. Our goal is to minimize the network energy consumption, given constraints on the global probabilities of detection and false-alarm. We show that the underlying optimization problem can be solved as a convex optimization problem. We then show the energy efficiency of our scheme via simulations using a ZigBee transceiver model.","Energy efficiency,
Cognitive radio,
Energy consumption,
Constraint optimization,
Cost function,
Conferences,
Computer networks,
Distributed computing,
Europe,
Mathematics"
Hypervolume approximation using achievement scalarizing functions for evolutionary many-objective optimization,"This paper proposes an idea of approximating the hypervolume of a non-dominated solution set using a number of achievement scalarizing functions with uniformly distributed weight vectors. Each achievement scalarizing function with a different weight vector is used to measure the distance from the reference point of the hypervolume to the attainment surface of the non-dominated solution set along its own search direction specified by its weight vector. Our idea is to approximate the hypervolume by the average distance from the reference point to the attainment surface over a large number of uniformly distributed weight vectors (i.e., over various search directions). We examine the effect of the number of weight vectors (i.e., the number of search directions) on the approximation accuracy and the computation time of the proposed approach. As expected, experimental results show that the approximation accuracy is improved by increasing the number of weight vectors. It is also shown that the proposed approach needs much less computation time than the exact hypervolume calculation for a six-objective knapsack problem even when we use about 100,000 weight vectors.",Optimization methods
A Novel Visual Cryptography Scheme,"Visual Cryptography is a new cryptographic technique which allows visual information (pictures, text, etc.) to be encrypted in such a way that the decryption can be performed by human, without any decryption algorithm. Here we propose a Data hiding in halftone images using conjugate ordered dithering (DHCOD) algorithm which is a modified version of Data hiding in halftone images using conjugate error diffusion technique (DHCED). We use this DHOCD algorithm for proposing a new three phase visual cryptography scheme. DHCOD technique is used to hide an binary visual pattern in two or more ordered dither halftone images, which can be from the same or different multi-tone images. In proposed scheme we shall generate the shares using basic visual cryptography model and then embed them into a cover image using a DHCOD technique, so that the shares will be more secure and meaningful.",
Low Power Reconfiguration Technique for Coarse-Grained Reconfigurable Architecture,"Coarse-grained reconfigurable architectures (CGRAs) require many processing elements (PEs) and a configuration memory unit (configuration cache) for reconfiguration of its PE array. Although this structure is meant for high performance and flexibility, it consumes significant power. Specially, power consumption by configuration cache is explicit overhead compared to other types of intellectual property (IP) cores. Reducing power is very crucial for CGRA to be more competitive and reliable processing core in embedded systems. In this paper, we propose a reusable context pipelining (RCP) architecture to reduce power-overhead caused by reconfiguration. It shows that the power reduction can be achieved by using the characteristics of loop pipelining, which is a multiple instruction stream, multiple data stream (MIMD)-style execution model. RCP efficiently reduces power consumption in configuration cache without performance degradation. Experimental results show that the proposed approach saves much power even with reduced configuration cache size. Power reduction ratio in the configuration cache and the entire architecture are up to 86.33% and 37.19%, respectively, compared to the base architecture.","Reconfigurable architectures,
Energy consumption,
Pipeline processing,
Application software,
Application specific integrated circuits,
Power system reliability,
Embedded system,
Hardware,
Computer science,
Intellectual property"
Graph-based submodular selection for extractive summarization,"We propose a novel approach for unsupervised extractive summarization. Our approach builds a semantic graph for the document to be summarized. Summary extraction is then formulated as optimizing submodular functions defined on the semantic graph. The optimization is theoretically guaranteed to be near-optimal under the framework of submodularity. Extensive experiments on the ICSI meeting summarization task on both human transcripts and automatic speech recognition (ASR) outputs show that the graph-based submodular selection approach consistently outperforms the maximum marginal relevance (MMR) approach, a concept-based approach using integer linear programming (ILP), and a recursive graph-based ranking algorithm using Google's PageRank.",
Adaptive Locks: Combining Transactions and Locks for Efficient Concurrency,"Transactional memory is being advanced as an alternative to traditional lock-based synchronization for concurrent programming. Transactional memory simplifies the programming model and maximizes concurrency. At the same time, transactions can suffer from interference that causes them to often abort, from heavy overheads for memory accesses, and from expressiveness limitations (e.g., for I/O operations). In this paper we propose an adaptive locking technique that dynamically observes whether a critical section would be best executed transactionally or while holding a mutex lock. The critical new elements of our approach include the adaptivity logic and cost-benefit analysis, a low overhead implementation of statistics collection and adaptive locking in a full C compiler, and an exposition of the effects on the programming model. In experiments with both microand macro-benchmarks we found adaptive locks to consistently match or out perform the better of the two component mechanisms (mutexes or transactions). Compared to either mechanism alone, adaptive locks often provide 3-to-10x speedups. Additionally, adaptive locks simplify the programming model by reducing the need for fine-grained locking: with adaptive locks, the programmer can specify coarse-grained locking annotations and often achieve fine-grained locking performance due to the transactional memory mechanisms.","Concurrent computing,
Yarn,
Programming profession,
System recovery,
Interference,
Logic programming,
Parallel architectures,
Information science,
Jacobian matrices,
Computer science"
The capacity region of a class of deterministic Z channels,"We characterize the capacity region of a class of the deterministic Z channels. We show that, interestingly, Han-Kobayashi type rate-splitting is not required in the optimal achievable scheme for the class of channels considered.","Interference channels,
Broadcasting,
Relays,
Computer science,
Communication networks,
Information theory,
Fading,
Auxiliary transmitters"
Using Subfiling to Improve Programming Flexibility and Performance of Parallel Shared-file I/O,"There are two popular parallel I/O programming styles used by modern scientific computational applications: unique-file and shared-file. Unique-file I/O usually gives satisfactory performance, but its major drawback is that managing a large number of files can overwhelm the task of post-simulation data processing. Shared-file I/O produces fewer files and allows arrays partitioned among processes to be saved in the canonical order. As the number of processors on modern parallel machines increases into thousands and more, the problem size and in turn the global array size also increase proportionally. It is not practical to manage files of size each larger than a few hundreds of GB. Hence, to seek a middle ground between these two I/O styles, we propose a subfiling scheme that divides a large multi-dimensional global array into smaller subarrays, each saved in a smaller file, named subfile. Subfiling is implemented on top of MPI-IO. We also incorporate it into the parallel netCDF library in order to preserve the partitioning information in the netCDF file header, so that the global array can later be reconstructed. In addition, since the subfiling scheme decreases the number of processes sharing a file, it can reduce the overhead of file system's data consistency control. Our experimental results with several I/O benchmarks show that subfiling can provide improved I/O performance.",
Energy-efficient multi-hop transmission in Body Area Networks,"Compared to Wireless Sensor Network (WSN), Body Area Network (BAN) has its own unique requirements and further, it is more difficult to equip the medical sensors with replaceable batteries as this reduces the comport of the person wearing them. In this respect, a lot of efforts are being made by 802.15.6 WG to standardize a more efficient Medium Access Control (MAC) for BAN. Thus, in this paper, we propose an energy-efficient MAC scheme designed for BAN. The main idea is to allow for body sensors to transmit their data to the coordinator using multi-hop transmission in the BAN with the aim to maximize the lifetime of BAN. The performance of the proposed MAC scheme is evaluated by computer simulations in terms of lifetime and resource utilization.","Energy efficiency,
Spread spectrum communication,
Body area networks,
Body sensor networks,
Wireless sensor networks,
Batteries,
Media Access Protocol,
Resource management,
Computer simulation,
Computer science"
Design collaborative systems with multiple AF-relays for asynchronous frequency-selective fading channels,"Distributed systems with multiple amplify-andforward (AF) relays are very appealing, due to their ease of implementing space diversity. Although their performance on synchronous flat-fading channels was well understood, the corresponding design and optimization in asynchronous frequency selective fading (AFSF) channels remains unsolved. In this paper, we tackle the problem in the information-theoretic framework, revealing that multi-relay amplify-and-forward (MR-AF) systems over AFSF channels can be better understood through the concept of virtual sub-channels. Each relay node virtually performs two functions, appropriately amplifying sub-channel signals on one hand and serving as a local switching center on the other. System design, therefore, reduces to the determination of optimal amplification factors, switching matrices, and power allocation among the source, relays and relevant sub-channels. The optimization is implemented in a layered structure. The effects of asynchronism and knowledge of channel information on mutual information are also investigated.","Collaboration,
Frequency-selective fading channels,
Power system relaying,
Relays,
Mutual information,
Frequency,
Sensor arrays,
MIMO,
Antenna arrays,
Wireless LAN"
Direct geolocation of stationary wideband radio signal based on time delays and Doppler shifts,"Contrary to the suboptimal (two-step) geolocation procedures, we propose a maximum likelihood estimation for the position of a stationary transmitter which its delayed and Doppler shifted signal is observed by moving receivers. The position is estimated based on the same data used in common methods. However, it is performed in a single step by maximizing a cost function that depends on the unknown position only.","Wideband,
Delay effects,
Doppler shift,
Delay estimation,
Radio transmitters,
Maximum likelihood estimation,
Receivers,
Frequency estimation,
Mathematics,
Computer science"
Relational WordNet model for semantic search in Holy Quran,"The Holy Quran, due to its unique style and allegorical nature, needs special attention about searching and information retrieval issues. The legacy keyword searching techniques are incapable of retrieving semantically relevant verses. In this paper, we address the deficiencies of key word based searching and the issues related to semantic search in the Holy Quran, and propose a model that is capable of performing semantic search. The model exploits WordNet relationships in relational database model. The implementation of this model has been carried out in latest tools and Surah Al-Baqrah, the largest chapter of the Holy Quran, has been taken as sample text. The precision of our model's prototype implementation is far better than simple key word searching.","Keyword search,
Computer science,
Information retrieval,
Books,
Surface acoustic waves,
Relational databases,
Software prototyping,
Prototypes,
Sun,
Oceans"
Practical fuzzy decision trees,"Decision-tree algorithms are one of the most popular applications in machine learning. The ID3 algorithm is an efficient method for building decision trees that form the basis for many decision tree programs. Fuzzy ID3 is an extension of the existing ID3 algorithm; it integrates fuzzy set theory and ID3 to overcome the effects of spurious precision in the data, to treat uncertainties in the data and to reduce the decision tree sensitivity to small changes in attribute values. In this paper, we introduce a modified version of fuzzy ID3 algorithm that integrates information gain and classification ambiguity to select the test attribute. The modified algorithm achieves better accuracy than the original Fuzzy ID3 as well as crisp programs such C4.5 on a wide range of datasets. We also introduce a new machine learning software tool based on fuzzy decision trees.","Decision trees,
Testing,
Machine learning algorithms,
Fuzzy sets,
Machine learning,
Classification tree analysis,
Fuzzy set theory,
Fuzzy reasoning,
Training data,
Cancer"
People Reidentification in a Camera Network,"This paper presents an approach to the object reidentification problem in a camera network system. The reidentification or reacquisition problem consists essentially on the matching process of images acquired from different cameras. This work is applied in a monitored environment by cameras. This application is important to modern security systems, in which the targets presence identification in the environment expands the capacity of action by security agents in real time and provides important parameters like localization for each target. We used target's interest points and target's color with features for reidentification. The satisfactory results were obtained from real experiments in public video datasets and synthetic images with noise.","Cameras,
Surveillance,
Charge-coupled image sensors,
Event detection,
Humans,
Computer networks,
Computer science,
Computerized monitoring,
Application software,
Data security"
Robust Sybil Detection for MANETs,"In this research, we propose a robust Sybil attack detection framework for MANETs based on cooperative monitoring of network activities. We do not require designated and honest monitors to perform the Sybil attack detection. Each mobile node in the network observes packets passing through it and periodically exchanges its observations in order to determine the presence of an attack. Malicious nodes fabricating false observations will be detected and rendered ineffective. Our framework requires no centralized authority and, thus, is scalable in expanding network size. Privacy of each mobile node is also a consideration of our framework. Our preliminary experimental results yield above 80% accuracy (true positives) and about 10% error rate (false positives).","Robustness,
Mobile ad hoc networks,
Telecommunication traffic,
Computer science,
Computerized monitoring,
Computer displays,
Privacy,
Error analysis,
Counterfeiting,
Intrusion detection"
Early spatiotemporal grouping with a distributed oriented energy representation,"Spatiotemporal data is associated with vast amounts of raw samples. Given the limited computational resources typically available, an initial organization of this data supporting semantically meaningful lines of inquiry would facilitate efficient processing. In this paper, a new representation for grouping raw image data into a set of coherent spacetime regions is proposed. Unique in this proposal is that coherency is related to a richer description of local spacetime structure than generally considered. In particular, the representation describes the presence of particular oriented spacetime structures in a distributed manner. A key advantage of this representation is its ability to signal the presence of multiple oriented structures at a given spacetime location. More generally, the abstraction allows for the description and grouping of motion and non-motion-related patterns in a uniform manner. Empirical evaluation of the grouping method on synthetic and challenging natural imagery suggests its efficacy.","Spatiotemporal phenomena,
Brightness,
Image sequences,
Layout,
Filters,
Computer science,
Data engineering,
Power engineering and energy,
Proposals,
Stochastic processes"
ATPG-based grading of strong fault-secureness,"Robust circuit design has become a major concern for nanoscale technologies. As a consequence, for design validation, not only the functionality of a circuit has to be considered, but also its robustness properties have to be analyzed. In this work we propose a method to verify the strong fault-secureness by use of constrained SAT-based ATPG. Strongly fault-secure circuits can be seen as the widest class of circuits achieving the totally self-checking (TSC) goal, which requires that every fault be detected the first time it manifests itself as an error at the outputs. As the strongly fault-secure property guarantees to achieve the TSC goal even in the case of fault accumulation, the effects of all possible fault sequences have to be taken into consideration to verify this property. To speed up the complex analysis of multiple faults we develop rules to derive detectability or redundancy information for multiple faults from the respective information for single faults. For the case of not strongly fault-secure circuits our method provides measures to grade the “extent” of strong fault-secureness given by the implementation.","Circuit faults,
Redundancy,
Circuit synthesis,
Automatic test pattern generation,
Electrical fault detection,
Fault detection,
Robustness,
Computer errors,
Circuit simulation,
Logic design"
Utility of algebraic connectivity metric in topology design of survivable networks,"In studies of survivable networks, it is important to be able to differentiate network topologies by means of a robust numerical measure that indicates the levels of immunity of these topologies to failures of their nodes and links. Ideally, such a measure should be sensitive to the existence of nodes or links which are more important than others, for example, if their failures cause the network's disintegration. In this paper, we suggest using an algebraic connectivity metric, adopted from spectral graph theory, namely the 2nd smallest eigenvalue of the Laplacian matrix of the network topology, instead of the average nodal degree that is usually used to characterize network connectivity in studies of the spare capacity allocation problem. Extensive simulation studies confirm that this metric is a more informative and more accurate parameter than the average nodal degree for characterizing network topologies in survivability studies.","Network topology,
Protection,
Eigenvalues and eigenfunctions,
Laplace equations,
Robustness,
Next generation networking,
Computer networks,
Design engineering,
Computer science,
Software engineering"
New Digisonde for research and monitoring applications,"The new Digisonde-4D, while preserving the basic principles of the Digisonde family, introduces important hardware and software changes that implement the latest capabilities of new digital radio frequency (RF) circuitry and embedded computers. The “D” refers to digital transmitters and receivers in which no analog circuitry is used for conversion between the baseband and the RF. In conjunction with the new hardware design, new software solutions offer significantly enhanced measurement flexibility, enhanced signal selectivity, and new types of data, e.g., the complete set of time domain samples of all four antenna signals suitable for independent scientific analysis. With the new method of mitigating in-band RF interference, the ionogram running time can be made as short as a couple of seconds. The h'(f) precision ranging technique with an accuracy of better than 1 km can be used on a routine basis. The 4D model runs the new ARTIST-5 ionogram autoscaling software which reports in real time the required data for assimilation in ionospheric models. The paper highlights technical advances of the new Digisonde for research and monitoring applications.","Receivers,
Radio frequency,
Monitoring,
Hardware,
Digital signal processing,
Software,
Sensors"
Temperature-aware dynamic frequency and voltage scaling for reliability and yield enhancement,"A novel oscillation-based on-chip thermal sensing architecture for dynamically adjusting supply voltage and clock frequency in System-on-Chip (SoC) is proposed. It is shown that the oscillation frequency of a ring oscillator reduces linearly as the temperature rises, and thus provides a good on-chip temperature sensing mechanism. An efficient Dynamic Frequency-to-Voltage Scaling (DF2VS) algorithm is proposed to dynamically adjust supply voltage according to the oscillation frequencies of the ring oscillators distributed in SoC so that thermal sensing can be carried at all potential hot spots. An on-chip Dynamic Voltage Scaling or Dynamic Voltage and Frequency Scaling (DVS or DVFS) monitor selects the supply voltage level and clock frequency according to the outputs of all thermal sensors. Experimental results on SoC benchmark circuits show the effectiveness of the algorithm that a 10% reduction in supply voltage alone can achieve about 20% power reduction (DVS scheme), and nearly 50% reduction in power is achievable if the clock frequency is also scaled down (DVFS scheme). The chip temperature is reduced accordingly.","Frequency,
Dynamic voltage scaling,
Clocks,
Temperature sensors,
System-on-a-chip,
Ring oscillators,
Voltage control,
Voltage-controlled oscillators,
Monitoring,
Thermal sensors"
An Information-Theoretic Characterization of Weighted alpha-Proportional Fairness,"This paper provides a novel characterization of fairness concepts in network resource allocation problems from the viewpoint of information theory. The fundamental idea adopted in this paper is to characterize the utility functions used in optimization problems, which motivate fairness concepts, based on a trade-off between user and system satisfaction. Here, user satisfaction is evaluated using information divergence measures that were originally used in information theory to evaluate the difference between two probability distributions. In this paper, information divergence measures are applied to evaluate the difference between the implemented resource allocation and a requested resource allocation. The requested resource allocation is assumed to be ideal in some sense from the user's point of view. Also, system satisfaction is evaluated based on the efficiency of the implemented resource utilization, which is defined as the total amount of resources allocated to each user. The results discussed in this paper indicate that the well-known fairness concept called weighted alpha-proportional fairness can be characterized using the alpha-divergence measure, which is a general class of information divergence measures, as an equilibrium of the trade-off described above. In the process of obtaining these results, we also obtained a new utility function that has a parameter to control the trade-off. This new function is then applied to typical examples to solve resource allocation problems in simple network models such as those for two-link networks and wireless LANs.","Resource management,
Information theory,
Communications Society,
Paper technology,
Computer science,
USA Councils,
Probability distribution,
Wireless LAN,
Degradation,
Throughput"
Analytical transient response and propagation delay model for nanoscale CMOS inverter,"This paper presents a new analytical propagation delay model for nanoscale CMOS inverters. By using a non-saturation current model, the analytical input-output transfer responses and propagation delay model are derived. The model is used for calculating inverter delays for different input transition times, load capacitances and supply voltages. Delays predicted by the proposed model are in good agreement with those of transistor level simulation results from SPICE, with accuracy of 3% or better.",
Fault tolerant reversible logic synthesis: Carry look-ahead and carry-skip adders,"Irreversible logic circuits dissipate heat for every bit of information that is lost. Information is lost when the input vector cannot be recovered from its corresponding output vector. Reversible logic circuit naturally takes care of heating because it implements only the functions that have one-to-one mapping between its input and output vectors. Therefore reversible logic design becomes one of the promising research directions in low power dissipating circuit design in the past few years and has found its application in low power CMOS design, digital signal processing and nanotechnology. This paper presents the efficient approaches for designing reversible fast adders that implement carry look-ahead and carry-skip logic. The proposed 16-bit high speed reversible adder will include IG gates for the realization of its basic building block. The IG gate is universal in the sense that it can be used to synthesize any arbitrary Boolean-functions. The IG gate is parity preserving, that is, the parity of the inputs matches the parity of the outputs. It allows any fault that affects no more than a single signal readily detectable at the circuit's primary outputs. Therefore, the proposed high speed adders will have the inherent opportunity of detecting errors in its output side. It has also been demonstrated that the proposed design offers less hardware complexity and is efficient in terms of gate count, garbage outputs and constant inputs than the existing counterparts.","Fault tolerance,
Adders,
Circuit synthesis,
Logic circuits,
Logic design,
CMOS logic circuits,
Heating,
CMOS digital integrated circuits,
CMOS process,
Process design"
Physical Interference Modeling for Transmission Scheduling on Commodity WiFi Hardware,"The demand for capacity in WiFi networks is driving a new look at transmission scheduling-based link layers. One basic issue here is the use of accurate interference models to drive transmission scheduling algorithms. However, experimental work in this space has been limited. In this work, we use commodity WiFi hardware (specifically, 802.11a) for a comprehensive study of interference modeling for transmission scheduling on a mesh network setup. We focus on the well-known physical interference model for its realism. We propose use of the ""graded"" version of the model where feasibility of a link is probabilistic, as opposed to using the more traditional ""thresholded"" version, where feasibility is binary. We show experimentally that the graded model is significantly more accurate (80 percentile error 0.2 vs. 0.55 for thresholded model). We develop transmission scheduling experiments using greedy scheduling algorithms for the evacuation model for both interference models. We also develop similar experiments for optimal scheduling performance for the simplified one-shot scheduling. The scheduling experiments demonstrate clearly superior performance for the graded model, often by a factor of 2. We conclude by promoting use of this model for scheduling studies.","Interference,
Hardware,
Optimal scheduling,
Signal to noise ratio,
Processor scheduling,
Scheduling algorithm,
Bit error rate,
Physical layer,
Computer science,
Wireless networks"
"{K}
-Space and Image-Space Combination for Motion-Induced Phase-Error Correction in Self-Navigated Multicoil Multishot DWI","Motion during diffusion encodings leads to different phase errors in different shots of multishot diffusion-weighted acquisitions. Phase error incoherence among shots results in undesired signal cancellation when data from all shots are combined. Motion-induced phase error correction for multishot diffusion-weighted imaging (DWI) has been studied extensively and there exist multiple phase error correction algorithms. A commonly used correction method is the direct phase subtraction (DPS). DPS, however, can suffer from incomplete phase error correction due to the aliasing of the phase errors in the high spatial resolution phases. Furthermore, improper sampling density compensation is also a possible issue of DPS. Recently, motion-induced phase error correction was incorporated in the conjugate gradient (CG) image reconstruction procedure to get a nonlinear phase correction method that is also applicable to parallel DWI. Although the CG method overcomes the issues of DPS, its computational requirement is high. Further, CG restricts to sensitivity encoding (SENSE) for parallel reconstruction. In this paper, a new time-efficient and flexible k-space and image-space combination (KICT) algorithm for rigid body motion-induced phase error correction is introduced. KICT estimates the motion-induced phase errors in image space using the self-navigated capability of the variable density spiral trajectory. The correction is then performed in k -space. The algorithm is shown to overcome the problem of aliased phase errors. Further, the algorithm preserves the phase of the imaging object and receiver coils in the corrected k -space data, which is important for parallel imaging applications. After phase error correction, any parallel reconstruction method can be used. The KICT algorithm is tested with both simulated and in vivo data with both multishot single-coil and multishot multicoil acquisitions. We show that KICT correction results in diffusion-weighted images with higher signal-to-noise ratio (SNR) and fractional anisotropy (FA) maps with better resolved fiber tracts as compared to DPS. In peripheral-gated acquisitions, KICT is comparable to the CG method.","Error correction,
Character generation,
Encoding,
Image reconstruction,
High-resolution imaging,
Spatial resolution,
Image sampling,
Motion estimation,
Phase estimation,
Spirals"
Characterizing comment spam in the blogosphere through content analysis,"Spams are no longer limited to emails and webpages. The increasing penetration of spam in the form of comments in blogs and social networks has started becoming a nuisance and potential threat. In this work, we explore the challenges posed by this type of spam in the blogosphere with substantial generalization regarding other social media. Thus, we investigate the characteristics of comment spam in blogs based on their content. The framework uses some of the previously explored methods developed to effectively extract the features of the blog spam and also introduces a novel method of active learning from the raw data without requiring training instances. This makes the approach more flexible and realistic for such applications. We also incorporate the concept of co-training for supervised learning to get accurate results. The preliminary evaluation of the proposed framework shows promising results.","Blogs,
Search engines,
Electronic mail,
Social network services,
Web pages,
Unsolicited electronic mail,
Statistics,
Writing,
Machine learning algorithms,
Bayesian methods"
Optical flow on a flapping wing robot,"Optical flow sensing techniques are promising for obstacle avoidance, distance regulation, and moving target tracking, particularly for small mobile robots with limited power and payload constraints. Most optical flow sensing experimental work has been done on mobile platforms which are relatively steady in rotation, unlike the pitching motion expected on flapping wing flyers. In order to assess the feasibility of using optical flow to control an indoor flapping flyer, an 7 gram commercially available ornithopter airframe was equipped with on-board camera and CPU module with mass of 2.5 grams and 2.6 gram battery. An experiment was conducted capturing optical flow information during flapping and gliding flight on the same platform. As expected, flapping introduced substantial systematic bias to the direction estimates to the point of flipping the true direction periodically. Nonetheless, since the optical flow results oscillated at the same frequency as the flapping wings, it is envisioned that one could disambiguate the jittering optic flow measurements by correlating these with real-time feedback from the motor current.","Image motion analysis,
Optical feedback,
Optical sensors,
Optical control,
Robot sensing systems,
Target tracking,
Mobile robots,
Payloads,
Weight control,
Cameras"
Error-Correcting Codes for Automatic Control,"Systems with automatic feedback control may consist of several remote devices, connected only by unreliable communication channels. It is necessary in these conditions to have a method for accurate, real-time state estimation in the presence of channel noise. This problem is addressed, for the case of polynomial-growth-rate state spaces, through a new type of error-correcting code that is online and computationally efficient. This solution establishes a constructive analog, for some applications in estimation and control, of the Shannon coding theorem.","Error correction codes,
Automatic control,
Communication system control,
Base stations,
Mathematics,
Computer science,
State estimation,
Space exploration,
Space technology,
Velocity control"
iCFP: Tolerating all-level cache misses in in-order processors,"Growing concerns about power have revived interest in in-order pipelines. In-order pipelines sacrifice single-thread performance. Specifically, they do not allow execution to flow freely around data cache misses. As a result, they have difficulties overlapping independent misses with one another.","Pipeline processing,
Registers,
Merging,
Buffer storage,
Information science,
Parallel processing,
Processor scheduling,
Out of order,
Delay,
Proposals"
Protecting SIP server from CPU-based DoS attacks using history-based IP filtering,"Voice over IP (VoIP) telephony is vulnerable to a range of attacks, since its operation relies on the underlying IP network. The centralized design of the major VoIP signalling protocols such as the Session Initiation Protocol (SIP) makes the registration server a target for CPU-based denial of service (DoS) attacks. In this paper, we propose a history-based IP filtering layer to defeat these DoS attacks by blocking the SIP packets from previously unseen sources. Our empirical evaluation shows that our approach achieves significant improvement in CPU utilization under DoS attacks.","Protection,
Computer crime,
Authentication,
Filtering,
Internet telephony,
Protocols,
Network servers,
Web server,
IP networks,
Signal design"
Automatic Detection of Anatomical Landmarks in Uterine Cervix Images,"The work focuses on a unique medical repository of digital cervicographic images (ldquoCervigramsrdquo) collected by the National Cancer Institute (NCI) in longitudinal multiyear studies. NCI, together with the National Library of Medicine (NLM), is developing a unique Web-accessible database of the digitized cervix images to study the evolution of lesions related to cervical cancer. Tools are needed for automated analysis of the cervigram content to support cancer research. We present a multistage scheme for segmenting and labeling regions of anatomical interest within the cervigrams. In particular, we focus on the extraction of the cervix region and fine detection of the cervix boundary; specular reflection is eliminated as an important preprocessing step; in addition, the entrance to the endocervical canal (the ldquoosrdquo), is detected. Segmentation results are evaluated on three image sets of cervigrams that were manually labeled by NCI experts.","Cervical cancer,
Biomedical imaging,
Medical diagnostic imaging,
Biomedical engineering,
Cancer detection,
Image segmentation,
Software libraries,
Medical treatment,
Image databases,
Lesions"
ITACA: An integrated toolbox for the automatic composition and adaptation of Web services,"Adaptation is of utmost importance in systems developed by assembling reusable software services accessed through their public interfaces. This process aims at solving, as automatically as possible, mismatch cases which may be given at the different interoperability levels among interfaces by synthesizing a mediating adaptor. In this paper, we present a toolbox that fully supports the adaptation process, including: (i) different methods to construct adaptation contracts involving several services; (ii) simulation and verification techniques which help to identify and correct erroneous behaviours or deadlocking executions; and (iii) techniques for the generation of centralized or distributed adaptor protocols based on the aforementioned contracts. Our toolbox relates our models with implementation platforms, starting with the automatic extraction of behavioural models from existing interface descriptions, until the final adaptor implementation is generated for the target platform.","Web services,
Contracts,
System recovery,
Access protocols,
Irrigation,
Computer science,
Assembly systems,
Software reusability,
Software tools,
Quality of service"
Performance Evaluation of Link Quality Estimation Metrics for Static Multihop Wireless Sensor Networks,"The lossy nature of wireless communication leads to many challenges while designing multihop networks. As an integral part of reliable communication in wireless networks, effective link estimation is essential for routing protocols. Recent studies have shown that link reliability-based metrics like ETX have better performance than traditional metrics such as hop count or latency. Usually, such metrics employ techniques like blacklisting, involving thresholds during the link estimation process. In this paper, we conduct a detailed performance analysis of three commonly used link-quality metrics in wireless sensor networks: ETX, 4Bit, and RNP. We study the interplay between these metrics and CTP, a tree-based routing protocol provided by TinyOS. The objectives of our experiment are two fold. First, by applying different link-quality metrics to the same routing protocol, we provide extensive evaluation on ETX, 4Bit and RNP with insights on their performance under different criteria. Second, we study the impact of the presence or absence of a blacklisting policy when using these link quality estimation metrics. As to our knowledge, this paper is the first to compare the performance between these link quality based metrics with networks of different qualities under realistic conditions.","Spread spectrum communication,
Wireless sensor networks,
Routing protocols,
Signal to noise ratio,
Delay,
Performance loss,
Telecommunication network reliability,
Physical layer,
Communications Society,
Computer science"
Multi-input multi-output (MIMO) indoor optical wireless communications,"Indoor optical wireless systems often use arrays of transmitters and receivers to achieve high data rates. MIMO offers the potential to use these components to transmit data in parallel between multiple sources and detectors. The slowly varying incoherent communications channel can be measured using straightforward techniques, and resulting channel matrix used to recover data that is transmitted in parallel from a number of sources. In this paper we report results from several preliminary indoor communications experiments. A four channel MIMO system that uses white light LEDs for communication (and associated illumination) is described, as well as experiments in a diffuse environment, using infra-red sources. Design issues, as well as future opportunities and challenges are also discussed.","MIMO,
Wireless communication,
Optical receivers,
Optical transmitters,
Optical arrays,
Detectors,
Communication channels,
Indoor communication,
Light emitting diodes,
Lighting"
Component Based Software Reuse Key Technology Research and Design,"The paper Researches on the component-based software development (CBSD) method, brings forward and analyses several key technologies of component-based software reuse. This paper first designs the architecture fits for the CBSD method, explains the development process fits for the architecture, then designs and analyses the component, which is the basic part of the CBSD method, and analyses the requirements and constraints of the component. From this paper we can realize the advantage of CBSD method with these technologies.","Computer architecture,
Software quality,
Programming,
Information technology,
Application software,
Design engineering,
Assembly systems,
Nonhomogeneous media,
Software libraries,
Information science"
An agent-based personalized E-learning environment: Effort prediction perspective,"The various components of By virtue of the Information and Communication Technology (ICT), E-Learning becomes an asset in the field of training and education. Among many factors to facilitate the learning process, personalization role to provide the E-Learning Environments oriented towards learners. Personalization encompasses activities over learning content and learning sequence activities. As a new orientation of personalization this paper proposes an agent based personalized intra circle e-Learning Environment for interdisciplinary studies. The proposed system brings out another dimension for the personalization in the E-Learning Environments by providing domain specific content to the learner, especially for the post graduate and dual degree disciplinary courses. Generally the interdisciplinary courses provide their own difficulties to the learner from other discipline of courses. This system fades out the difficulties of these courses in the form of generating domain specific course content based on effort prediction to the learner. The system encompasses two processes for developing the course content and it personalizes the course content in three perspectives such as system's perspective, learner's perspective and teacher's perspective. To be best of our knowledge, this is the first attempt from the different perspective of personalization and of course this will lead a new direction of research in the field of E-Learning Environment.","Electronic learning,
Computer science,
Communications technology,
Computer science education,
Computer aided instruction,
Intelligent agent,
Adaptive systems,
Fluctuations,
Semantic Web,
Runtime"
Diagnosability analysis of unbounded Petri nets,"In this paper we consider the property of diagnosability for labeled unbounded Petri nets, namely Petri nets where the number of tokens in one or more places can grow indefinitely. We give necessary and sufficient conditions for diagnosability and we present a test to study diagnosability based on the analysis of the coverability graph of a particular net, called verifier net, that is built starting from the initial system. To the best of our knowledge, this is the first available test for diagnosability analysis of labeled unbounded Petri nets. We also discuss existing methods to perform diagnosis of unbounded Petri nets.","Petri nets,
Sufficient conditions,
Automata,
System testing,
Fault diagnosis,
Discrete event systems,
State-space methods,
Fault detection,
Performance analysis,
Performance evaluation"
Validating cascading of crossbar circuits with an integrated device-circuit exploration,"We present an integrated approach that combines 3D modeling of nanodevice electrostatics and operations with extensive circuit level validation and evaluation. We simulate crossed nanowire field-effect transistor (xnwFET) structures, extract electrical characteristics, and create behavioral models for circuit level validations. Our experiments show that functional cascaded dynamic circuits can be achieved by optimal selection of device level parameters such as VTH. Furthermore, VTH tuning is achieved through substrate biasing and source and drain junction underlap, which does not pose difficult manufacturability and customization challenges. Circuit level simulations of up to forty cascaded stages show correct propagation of data and adequate noise margins.","Circuits,
Educational technology,
Automation,
Computer aided instruction,
Automatic control,
Military computing,
Computer science,
Computer science education,
Instruments,
Application software"
"1/f
Noise in Diffuse Optical Imaging and Wavelet-Based Response Estimation","In diffuse optical imaging (DOI) data analysis, the functional response is contaminated with physiological noise as in functional magnetic resonance imaging (fMRI). In this work we extend a previously proposed method for fMRI to estimate the parameters of a linear model of DOI time series. The regression is performed in the wavelet domain to infer drift coefficients at different scales and to estimate the strength of the hemodynamic response function (HRF). This multiresolution approach benefits from the whitening property of the discrete wavelet transform (DWT), which approximately decorrelates long-memory noise processes. We also show that a more accurate estimation is obtained by removing some regressors correlating with the protocol. Moreover, we observe that this improvement is related to a quantitative measure of 1/f noise. The performances of the method are first evaluated against a standard spline-cosine drift approach with simulated HRF and real background physiology. Lastly, the technique is applied to experimental event-related data acquired by near-infrared spectroscopy (NIRS).","Discrete wavelet transforms,
Optical imaging,
Data analysis,
Magnetic noise,
Optical noise,
Magnetic resonance imaging,
Parameter estimation,
Wavelet domain,
Hemodynamics,
Decorrelation"
"Play, belief and stories about robots: A case study of a pleo blogging community","We present an analysis based on user-provided content collected from online blogs and forums about the robotic artifact Pleo. Our primary goal is to explore stories about how human-robot interaction would manifest themselves in actual real-world contexts. To be able to assess these types of communicative media we are using a method based on virtual ethnography that specifically addresses underlying issues in how the data is produced and should be interpreted. Results indicate that generally people are staging, performing and have a playful approach to the interaction. This is further emphasized by the way people communicate their stories through the blogging practice. Finally we argue that these resources are indeed essential for understanding and designing long-term human-robot relationships.",
Effect of transmit correlation on the sum-rate capacity of two-user broadcast channels,"This letter investigates the effect of the transmit correlation on the sum-rate capacity of a two-user broadcast channel (BC) with the use of equal-power allocation in high signal-to-noise ratio (SNR) environments. It is shown using an upper-bound analysis that the sum-rate capacity of a correlated two-user BC at high SNR highly depends on the phase difference between the transmit correlation coefficients of two users as well as the magnitude of the transmit correlation coefficients, and it is maximized (or minimized) when the principal eigenvectors of the transmit correlation matrix of two users are orthogonal (or parallel) to each other. The analytic results are verified by computer simulation.","Broadcasting,
Receiving antennas,
Transmitting antennas,
Signal to noise ratio,
Computer simulation,
Channel capacity,
Base stations,
Antenna measurements,
MIMO"
An approximation scheme for energy-efficient scheduling of real-time tasks in heterogeneous multiprocessor systems,"As application complexity increases, modern embedded systems have adopted heterogeneous processing elements to enhance the computing capability or to reduce the power consumption. The heterogeneity has introduced challenges for energy efficiency in hardware and software implementations. This paper studies how to partition real-time tasks on a platform with heterogeneous processing elements (processors) so that the energy consumption can be minimized. The power consumption models considered in this paper are very general by assuming that the energy consumption with higher workload is larger than that with lower workload, which is true for many systems. We propose an approximation scheme to derive near-optimal solutions for different hardware configurations in energy/power consumption. When the number of processors is a constant, the scheme is a fully polynomial time approximation scheme (FPTAS) to derive a solution with energy consumption very close to the optimal energy consumption in polynomial-time/space complexity. Experimental results reveal that the proposed scheme is very effective in energy efficiency with comparison to the state-of-the-art algorithm.","Energy efficiency,
Real time systems,
Multiprocessing systems,
Energy consumption,
Hardware,
Polynomials,
Processor scheduling,
Application software,
Embedded system,
Embedded computing"
SUSurE: Speeded Up Surround Extrema feature detector and descriptor for realtime applications,"There has been significant research into the development of visual feature detectors and descriptors that are robust to a number of image deformations. Some of these methods have emphasized the need to improve on computational speed and compact representations so that they can enable a range of real-time applications with reduced computational requirements. In this paper we present modified detectors and descriptors based on the recently introduced CenSurE [1], and show experimental results that aim to highlight the computational savings that can be made with limited reduction in performance. The developed methods are based on exploiting the concept of sparse sampling which may be of interest to a range of other existing approaches.","Computer vision,
Detectors,
Filters,
Robustness,
Information services,
Web sites,
Internet,
Application software,
Kernel,
Image databases"
Competitive buffer management with packet dependencies,"We introduce the problem of managing a FIFO buffer of bounded space, where arriving packets have dependencies among them. Our model is motivated by the scenario where large data frames must be split into multiple packets, because maximum packet size is limited by data-link restrictions. A frame is considered useful only if sufficiently many of its constituent packets are delivered. The buffer management algorithm decides, in case of overflow, which packets to discard and which to keep in the buffer. The goal of the buffer management algorithm is to maximize throughput of useful frames. This problem has a variety of applications, e.g., Internet video streaming, where video frames are segmented and encapsulated in IP packets sent over the Internet. We study the complexity of the above problem in both the offline and online settings. We give upper and lower bounds on the performance of algorithms using competitive analysis.",
Capacity of a class of symmetric SIMO Gaussian interference channels within O(1),"The N +1 user, 1×N single input multiple output (SIMO) Gaussian interference channel where each transmitter has a single antenna and each receiver has N antennas is studied. The symmetric capacity within O(1) is characterized for the symmetric case where all direct links have the same signal-to-noise ratio (SNR) and all undesired links have the same interference-to-noise ratio (INR). The gap to the exact capacity is a constant which is independent of SNR and INR. On the achievability side, an interesting conclusion is that the generalized degrees of freedom (GDOF) regime where treating interference as noise is found to be optimal in the 2 user interference channel, does not appear in the N + 1 user, 1 × N SIMO case. On the converse side, new multi-user outer bounds emerge out of this work that do not follow directly from the 2 user case. We also provide an outer bound on the capacity region of the 3 user SIMO Gaussian interference channel with 2 antennas at each receiver. This outer bound directly leads to the capacity region of this channel if the channel vectors satisfy certain conditions.","Interference channels,
Signal to noise ratio,
Receiving antennas,
Transmitting antennas,
Computer science,
Transmitters,
Navigation,
Base stations"
A remote doctor for homecare and medical diagnoses on cardiac patients by an adaptive ECG analysis,"Today the telemedicine applications are experiencing rapid growth. The possibility of having portable devices to use comfortably at home for monitoring the health of the patient has involved many research areas. In this paper a remote doctor for homecare and medical diagnoses on cardiac patient is proposed. An original smart system has been projected in order to adapt itself to the patient and develop, in this way, a personalized diagnosis taking into account his personal data and his clinical history; a practical and economical device that can adapt to any type of patient. The system is based on a PDA and an ECG sensor which acquires, at a fixed sampling frequency, the heart electrical impulses. In details, a removable and updatable memory is used to store the clinical and personal patient data, while an additional internal and read-only memory stores information on the metrological status of the measurement system. On PDA display, it is possible to observe the ECG signal graph, the information relating to diagnosis and the measurement uncertainty. The patient also has the opportunity to print the results by a Bluetooth printer. Through build-in models the ECG waveform is analyzed in order to diagnose possible arrhythmias occurrences or the happening of a heart attack according to some parameters like age, sex and physical constitution of the patient, and information on the measurement uncertainty. The embedded information on the system metrological performances and patient data are used to configure the computing algorithm and so to reduce the happening of diagnosis faults.","Medical diagnosis,
Electrocardiography,
Personal digital assistants,
Measurement uncertainty,
Telemedicine,
Patient monitoring,
Biomedical monitoring,
Remote monitoring,
History,
Sensor systems"
Speech Emotion Recognition Using Both Spectral and Prosodic Features,"In this paper, we propose a speech emotion recognition system using both spectral and prosodic features. Most traditional systems have focused on spectral features or prosodic features. Since both the spectral and the prosodic features contain emotion information, it is believed that the combining of spectral features and prosodic features will improve the performance of the emotion recognition system. Therefore, we propose to use both spectral and prosodic features. For spectral features, a GMM super vector based SVM is applied with them. For prosodic features, a set of prosodic features that are clearly correlated with speech emotional states and SVM is also used for emotion recognition. The combination of both spectral features and prosodic features is posed as a data fusion problem to obtain the final decision. Experimental results show that the combining of both spectral features and prosodic features yields the emotion error reduction rate of 18.0% and 52.8%, over using only spectral and prosodic features.","Speech,
Emotion recognition,
Support vector machines,
Spatial databases,
Mel frequency cepstral coefficient,
Support vector machine classification,
Cepstral analysis,
Sun,
Acoustics,
Training data"
An Exponential Lower Bound for the Parity Game Strategy Improvement Algorithm as We Know it,"This paper presents a new lower bound for the discrete strategy improvement algorithm for solving parity games due to Voege and Jurdzinski. First, we informally show which structures are difficult to solve for the algorithm. Second, we outline a family of games on which the algorithm requires exponentially many strategy iterations, answering in the negative the long-standing question whether this algorithm runs in polynomial time. Additionally we note that the same family of games can be used to prove a similar result w.r.t. the strategy improvement variant by Schewe as well as the strategy iteration for solving discounted payoff games due to Puri.","Computer science,
Runtime,
Logic,
Polynomials,
Upper bound,
Stochastic processes,
Game theory,
Automata,
Ear"
The effect of layout on the comprehension of UML class diagrams: A controlled experiment,"The results of a controlled experiment assessing the effects of different layout strategies on the comprehension of UML class diagrams of two software systems is presented. Six different categories of software comprehension tasks, with varying degrees of difficulty, are used to assess the layouts. Each task consists of several questions aimed at measuring the comprehensibility of a layout. The study involved 45 participants of varied experience in software design and programming ability. A report on the quantitative analysis of accuracy, speed, confidence level and preference of solving the tasks is given. Results indicate that clustered layouts demonstrate significant improvement in subject accuracy and speed in solving the problems in a majority of tasks.","Unified modeling language,
Control systems,
Software design,
Software systems,
Design for experiments,
Visualization,
Software maintenance,
Displays"
Exploring the Energy Consumption of Data Sorting Algorithms in Embedded and Mobile Environments,"Most mobile and embedded devices are battery powered. Hence, their uptime depends on the energy consumption of the used components. Developers made severe effort to optimize hardware components in order to reduce their energy consumption. However, in this paper we show that one also has to consider energy awareness in terms of software. In this study we focus on sorting algorithms, which are not only used directly by the user of a device but also very often implicitly by other algorithms. Our experiments show,that different sorting algorithms consume different amounts of energy. In detail, the experiments show that there is no direct correlation between the time complexity of an algorithm and its energy consumption.","Energy consumption,
Sorting,
Energy management,
Hardware,
Algorithm design and analysis,
Memory management,
Java,
Software performance,
Computer science,
Routing"
A robust relay node placement heuristic for structurally damaged wireless sensor networks,"Wireless sensor networks (WSN) can increase the efficiency of many real-life applications through the collaboration of thousands of miniaturized sensors which can be deployed unattended in inhospitable environments. Due to the harsh surroundings and violent nature of the applications, the network sometimes suffers a large scale damage that involves many nodes and would thus create multiple disjoint partitions. This paper investigates a strategy for recovering from such damage through the placement of relay nodes and promotes a novel approach. The proposed approach opts to re-establish connectivity using the least number of relays while ensuring certain quality in the formed topology. Unlike contemporary schemes that form a minimum spanning tree among the isolated segments, the proposed approach establishes a topology that resembles a spider web, for which the segments are situated at the perimeter. Such a topology not only exhibits stronger connectivity than a minimum spanning tree but also achieves better sensor coverage and enables balanced distribution of traffic load among the employed relays. The simulation results demonstrate the effectiveness of the proposed recovery algorithm.",
Power and performance of read-write aware Hybrid Caches with non-volatile memories,"Caches made of non-volatile memory technologies, such as Magnetic RAM (MRAM) and Phase-change RAM (PRAM), offer dramatically different power-performance characteristics when compared with SRAM-based caches, particularly in the areas of static/dynamic power consumption, read and write access latency and cell density. In this paper, we propose to take advantage of the best characteristics that each technology has to offer through the use of read-write aware Hybrid Cache Architecture (RWHCA) designs, where a single level of cache can be partitioned into read and write regions, each of a different memory technology with disparate read and write characteristics. We explore the potential of hardware support for intra-cache data movement within RWHCA caches. Utilizing a full-system simulator that has been validated against real hardware, we demonstrate that a RWHCA design with a conservative setup can provide a geometric mean 55% power reduction and yet 5% IPC improvement over a baseline SRAM cache design across a collection of 30 workloads. Furthermore, a 2-layer 3D cache stack (3DRWHCA) of high density memory technology with the same chip footprint still gives 10% power reduction and boost performance by 16% IPC improvement over the baseline.",Nonvolatile memory
The Effect of Learning on Bursting,"We have studied the effect that learning a new stimulus-response (SR) relationship had within a neuronal network cultured on a multielectrode array. For training, we applied repetitive focal electrical stimulation delivered at a low rate (Lt1/s). Stimulation was withdrawn when a desired SR success ratio was achieved. It has been shown elsewhere, and we verified that this training algorithm, named conditional repetitive stimulation (CRS), can be used to strengthen an initially weak SR. So far, it remained unclear what the role of the rest of the network during learning was. We therefore studied the effect of CRS on spontaneously occurring network bursts. To this end, we made profiles of the firing rates within network bursts. We have earlier shown that these profiles change shape on a time base of several hours during spontaneous development. We show here that profiles of summed activity, called burst profiles, changed shape at an increased rate during CRS. This suggests that the whole network was involved in making the changes necessary to incorporate the desired SR relationship. However, a local (path-specific) component to learning was also found by analyzing profiles of single-electrode-activity phase profiles. Phase profiles that were not part of the SR relationship changed far less during CRS than the phase profiles of the electrodes that were part of the SR relationship. Finally, the manner in which phase profiles changed shape varied and could not be linked to the SR relationship.","Strontium,
Electrodes,
Biological neural networks,
Shape,
Electrical stimulation,
Neurons,
Biomedical measurements,
Protocols,
Delay,
Testing"
MAX-MIN Ant System on GPU with CUDA,We propose a parallel MAX-MIN Ant System (MMAS) algorithm that is suitable for an implementation on graphics processing units (GPUs). Multi ant colonies with respective parameter settings are whole offloaded to the GPU in parallel. We have implemented this GPU-based MMAS on the GPU with compute unified device architecture (CUDA). Some performance optimization means for kernel program of GPU are introduced. Experimental results that are based on simulations for the traveling salesperson problem are presented to evaluate the proposed techniques.,"Concurrent computing,
Graphics,
Clustering algorithms,
Computer architecture,
Computational modeling,
Ant colony optimization,
Control systems,
Educational institutions,
Computer science,
Laboratories"
An Analytical Model to Study Optimal Area Breakdown between Cores and Caches in a Chip Multiprocessor,"A key design issue for chip multiprocessors (CMPs) is how to exploit the finite chip area to get the best system throughput.The most dominant area-consuming components in a CMP are processor cores and caches today.There is an important trade-off between the number of cores and the amount of cache in a single CMP chip.If we have too few cores, the system throughput will be limited by the number of threads.If we have too small cache capacity, the system may perform poorly due to frequent cache misses.This paper presents a simple and effective analytical model to study the trade-off of the core count and the cache capacity in a CMP under a finite die area constraint.Our model differentiates shared, private, and hybrid cache organizations.Our work will complement more detailed yet time-consuming simulation approaches by enabling one to quickly study how key chip area allocation parameters affect the system performance.","Analytical models,
Electric breakdown,
Space exploration,
Throughput,
Yarn,
Computer Society,
Very large scale integration,
Computer science,
Design methodology,
Bandwidth"
Leveraging the web context for context-sensitive opinion mining,"Existing automated opinion mining methods either employ a static lexicon-based approach or a supervised learning approach. Nevertheless, the former method often fails to identify context-sensitive semantics of the opinion words, and the latter approach requires a large number of human labeled training examples. The main contribution of this paper is the illustration of a novel opinion mining method underpinned by context-sensitive text mining and inferential language modeling to improve the effectiveness of opinion mining. Our initial experiments show that the proposed the inferential opinion mining method outperforms the purely lexicon-based opinion finding method in terms of several benchmark measures. Our research opens the door to the development of more effective opinion mining method to discover business intelligence from the Web knowledge base.","Text mining,
Data mining,
Blogs,
Motion pictures,
Electronic mail,
Context modeling,
Statistical learning,
Web pages,
Information systems,
Information technology"
Towards Modeling of Cardiac Micro-Structure With Catheter-Based Confocal Microscopy: A Novel Approach for Dye Delivery and Tissue Characterization,"This work presents a methodology for modeling of cardiac tissue micro-structure. The approach is based on catheter-based confocal imaging systems, which are emerging as tools for diagnosis in various clinical disciplines. A limitation of these systems is that a fluorescent marker must be available in sufficient concentration in the imaged region. We introduce a novel method for the local delivery of fluorescent markers to cardiac tissue based on a hydro-gel carrier brought into contact with the tissue surface. The method was tested with living rabbit cardiac tissue and applied to acquire three-dimensional image stacks with a standard inverted confocal microscope and two-dimensional images with a catheter-based confocal microscope. We processed these image stacks to obtain spatial models and quantitative data on tissue microstructure. Volumes of atrial and ventricular myocytes were 4901plusmn1713 and 10299plusmn3598 mum3 (meanplusmnsd), respectively. Atrial and ventricular myocyte volume fractions were 72.4plusmn4.7% and 79.7plusmn2.9% (mean plusmn sd), respectively. Atrial and ventricular myocyte density was 165571plusmn55836 and 86957plusmn32280 cells/mm3 (mean plusmn sd), respectively. These statistical data and spatial descriptions of tissue microstructure provide important input for modeling studies of cardiac tissue function. We propose that the described methodology can also be used to characterize diseased tissue and allows for personalized modeling of cardiac tissue.","Cardiac tissue,
Fluorescence,
Optical microscopy,
Optical imaging,
Cities and towns,
Cardiac disease,
Microstructure,
Biomedical optical imaging,
Cardiology,
Biomedical engineering"
Cryptanalysis of DS-SIP Authentication Scheme Using ECDH,"Session Initiation Protocol (SIP) is widely used in the current Internet protocols such as Hyper Text Transport Protocol (HTTP) and Simple Mail Transport Protocol (SMTP). In 2005, and proposed an authentication scheme using Elliptic Curve Diffie-Hellman (ECDH) problem for SIP called DS-SIP. However, this paper demonstrates that DS-SIP authentication scheme is still vulnerable to Denning-Sacco attack which an attacker can easily find a legal user's secret password when SIP client or SIP server compromises an old shared session key. In addition, we show that it can not resist the stolen-verifier attack where if an attacker has the ability to get the stored password verifier someway then it can be used to masquerade as the original user. In addition, this paper also proposes an enhanced DS-SIP (called EDS-SIP) authentication scheme based on ECDH in order to overcome such a security problem. As a result, the proposed EDS-SIP authentication scheme is more secure and has same efficient compare with Durlanik and Sogukpinar's DS-SIP authentication scheme.","Authentication,
Transport protocols,
Elliptic curve cryptography,
Elliptic curves,
Law,
Legal factors,
Postal services,
Resists,
National security,
Computer science"
"Performance analysis of AODV, AODVUU, AOMDV and RAODV over IEEE 802.15.4 in wireless sensor networks","In this paper the focus is on the performance study of four routing protocols, namely AODV, AODVUU, RAODV and AOMDV. We call these protocols AODV family of protocols as all these protocols consider AODV as the base routing protocol upon which these protocols are improved. Even though AODV and AODVUU are not different protocols, we wanted to see if there is any improvement in using the AODVUU implementation for a sensor network environment. We have investigated whether a multiple path algorithm like AOMDV would result in more data delivery as compared to single path solutions like AODV in a sensor network. Also, the reverse route discovery mechanisms employed in RAODV is checked for a sensor network. There is a need to understand the versatile behavioral aspects of these routing protocols in a wireless sensor network with varying traffic loads and the number of sources. All these protocols are simulated using NS-2 over IEEE 802.15.4. We also claim that our work is the first of its kind to study and compare the performance of all these four routing protocols from a sensor network point of view by extensively using various performance metrics like packet delivery ratio, average network delay, network throughput and normalized routing load.","Performance analysis,
Wireless sensor networks,
Routing protocols,
Telecommunication traffic,
Delay,
Monitoring,
Computer science,
Measurement,
Throughput,
ZigBee"
"Vision-based, real-time retinal image quality assessment","Real-time medical image quality is a critical requirement in a number of healthcare environments, including ophthalmology where studies suffer loss of data due to unusable (ungradeable) retinal images. Several published reports indicate that from 10% to 15% of images are rejected from studies due to image quality. With the transition of retinal photography to lesser trained individuals in clinics, image quality will suffer unless there is a means to assess the quality of an image in real-time and give the photographer recommendations for correcting technical errors in the acquisition of the photograph. The purpose of this research was to develop and test a methodology for evaluating a digital image from a fundus camera in real-time and giving the operator feedback as to the quality of the image. By providing real-time feedback to the photographer, corrective actions can be taken and loss of data or inconvenience to the patient eliminated. The methodology was tested against image quality as perceived by the ophthalmologist. We successfully applied our methodology on over 2,000 images from four different cameras acquired through dilated and undilated imaging conditions. We showed that the technique was equally effective on uncompressed and compressed (JPEG) images. We achieved a 100 percent sensitivity and 96 percent specificity in identifying “rejected” images.",
Non-negative matrix factorization of partial track data for motion segmentation,"This paper addresses the problem of segmenting low-level partial feature point tracks belonging to multiple motions. We show that the local velocity vectors at each instant of the trajectory are an effective basis for motion segmentation. We decompose the velocity profiles of point tracks into different motion components and corresponding non-negative weights using non-negative matrix factorization (NNMF). We then segment the different motions using spectral clustering on the derived weights. We test our algorithm on the Hopkins 155 benchmarking database and several new sequences, demonstrating that the proposed algorithm can accurately segment multiple motions at a speed of a few seconds per frame. We show that our algorithm is particularly successful on low-level tracks from real-world video that are fragmented, noisy and inaccurate.","Tracking,
Motion segmentation,
Computer vision,
Clustering algorithms,
Videoconference,
Data engineering,
Benchmark testing,
Motion analysis,
Cameras,
Laboratories"
An empirical study on achievable throughputs of IEEE 802.11n devices,"The empirical performance studies on the emerging IEEE 802.11n technology by an independent and vendor-neutral party have not really been explored. In this paper, we conduct performance measurements for the IEEE 802.11n network using a mixture of commercially available IEEE 802.11n devices from various manufacturers. With the same standard 20-MHz channel width configuration, the results demonstrate that IEEE 802.11n significantly outperforms the IEEE 802.11g network. The performance improvements of IEEE 802.11n are measured to be roughly about 85% for the downlink UDP traffic, 68% for the downlink TCP traffic, 50% for the uplink UDP traffic, and 90% for the uplink TCP traffic. We also observe that the UDP throughputs are largely imbalanced for the uplink and downlink traffics in most test networks, while the downlink and uplink TCP throughput results are quite balanced for all test networks. In addition, the 40-MHz channel configurations only provide marginal performance improvements. Unlike other existing work, here we also capture and analyze the IEEE 802.11n packets transferred during the performance tests in order to technically explain the measured performance results. It is observed that when the frame aggregation and block acknowledgement mechanisms are utilized, the superior performance results are achieved. However, the decisions on how and when to use these mechanisms are very hardware dependent.","Throughput,
Telecommunication traffic,
Testing,
Measurement,
Downlink,
Computer science,
Pulp manufacturing,
Performance analysis,
Hardware,
Bandwidth"
Strategies for Network Motifs Discovery,"Complex networks from domains like Biology or Sociology are present in many e-Science data sets. Dealing with networks can often form a workflow bottleneck as several related algorithms are computationally hard. One example is detecting characteristic patterns or ""network motifs"" - a problem involving subgraph mining and graph isomorphism. This paper provides a review and runtime comparison of current motif detection algorithms in the field. We present the strategies and the corresponding algorithms in pseudo-code yielding a framework for comparison. We categorize the algorithms outlining the main differences and advantages of each strategy. We finally implement all strategies in a common platform to allow a fair and objective efficiency comparison using a set of benchmark networks. We hope to inform the choice of strategy and critically discuss future improvements in motif detection.","Computer networks,
Complex networks,
Biology computing,
Sociology,
Sequences,
Proteins,
Taxonomy,
Neuroscience,
Network address translation,
Runtime"
Efficient data collection in wireless sensor networks with path-constrained mobile sinks,"Recent work shows that sink mobility along a constrained path can improve the energy efficiency in wireless sensor networks. However, due to the path constraint, a mobile sink with constant speed has limited communication time to collect data from the sensor nodes deployed randomly. This poses significant challenges in simultaneously improving the amount of data collected and reduction in energy consumption. To address this issue, we propose a novel data collection scheme, called the Maximum Amount Shortest Path (MASP), that increases network throughput as well as conserves energy to optimize the assignment of sensor nodes. MASP is formulated as an integer linear programming problem and then solved with the help of a genetic algorithm. A two-phase communication protocol is designed to implement the MASP scheme. Simulations experiments using OMNET++ show that MASP outperforms the Shortest Path Tree (SPT) and static sink methods in terms of system throughput and energy efficiency.","Wireless sensor networks,
Energy efficiency,
Throughput,
Mobile communication,
Energy consumption,
Integer linear programming,
Genetic algorithms,
Protocols"
Popular channel concentration schemes for efficient channel navigation in internet protocol televisions,"Internet Protocol Television (IPTV) is becoming increasingly popular as an emerging Internet application due to its variety of contents and services. However, hundreds of IPTV channels make it difficult to find one¿s desired channel. To relieve this problem, this paper presents a popular channel concentration scheme for efficient channel navigation in IPTV. Based on the property that TV channel selections concentrate on several frequently selected channels, the proposed scheme reorganizes channel sequences by clustering popular channels. Specifically, channels are rearranged in a frequency interleaved way to minimize the seek distance. Simulation studies show that the proposed scheme reduces the seek time of IPTV channel navigation significantly when up-down channel selection interfaces are used.",
Hand movement recognition for Brazilian Sign Language: A study using distance-based neural networks,"In this paper, the vision-based hand movement recognition problem is formulated for the universe of discourse of the Brazilian Sign Language. In order to analyze this specific domain we have used the artificial neural networks models based on distance, including neural-fuzzy models. The experiments explored here show the usefulness of these models to extract helpful knowledge about the classes of movements and to support the project of adaptative recognizer modules for Libras-oriented computational tools. Using artificial neural networks architectures - Self Organizing Maps and (Fuzzy) Learning Vector Quantization, it was possible to understand the data space and to build models able to recognize hand movements performed for one or more than one specific Libras users.",
In Strangers We Trust? Findings of an Empirical Study of Distributed Teams,"Trust has long been a contentious issue in human endeavours. It is not readily given nor gained, more so when strangers are involved. It often becomes an issue during distributed development where individuals are expected to interact with strangers they may not “meet” during the project lifetime. Trust was spontaneously raised by respondents in an empirical study of practices within distributed development and is reported in this paper. A qualitative analysis of study data suggests that trust typically becomes an issue in large teams when developers are to deliver an innovative product. We also found that it is more likely to be an issue the greater the diversity (of culture, language, time zone…etc.) within the team. Finally the data also suggests that developers more readily trust an authoritative team member (e.g. team leader), even if remote. Data suggests these factors can act as positive and negative forces to influence trust within distributed teams. These forces are reported in this paper together with proposed approaches that can promote equilibrium of the net forces.",
Compact MR-brake with serpentine flux path for haptics applications,"This research explores a new approach to shape the magnetic flux path in a MR brake. Magnetically conductive and non-conductive elements were stacked to weave the magnetic flux through the rotor and the outer shell of the brake. This approach enabled design of a more compact and powerful MR brake. In addition, a ferro-fluidic sealing technique was developed to prevent the fluid from leaking and to reduce off-state friction. Experimental results showed that, when compared to a commercial MR brake, our 33% smaller prototype MR-brake could generate 2.7 times more torque (10.9 Nm). A 1-DOF haptic interface employing the brake enabled crisp virtual wall collision simulations. Significant reduction in the off-state torque was obtained by applying a reverse current pulse to collapse a residual magnetic field in the brake.","Haptic interfaces,
Torque,
Magnetic flux,
Prototypes,
Erbium,
Structural rings,
Friction,
Virtual reality,
Valves,
Magnetic liquids"
Iteratively re-weighted least squares for sparse signal reconstruction from noisy measurements,"Finding sparse solutions of under-determined systems of linear equations is a problem of significance importance in signal processing and statistics. In this paper we study an iterative reweighted least squares (IRLS) approach to find sparse solutions of underdetermined system of equations based on smooth approximation of the L0 norm and the method is extended to find sparse solutions from noisy measurements. Analysis of the proposed methods show that weaker conditions on the sensing matrices are required. Simulation results demonstrate that the proposed method requires fewer samples than existing methods, while maintaining a reconstruction error of the same order and demanding less computational complexity.","Least squares methods,
Signal reconstruction,
Equations,
Least squares approximation,
Signal processing,
Statistics,
Iterative methods,
Sparse matrices,
Computational modeling,
Computational complexity"
Improving SVM Classification on Imbalanced Data Sets in Distance Spaces,"Imbalanced data sets present a particular challenge to the data mining community. Often, it is the rare event that is of interest and the cost of misclassifying the rare event is higher than misclassifying the usual event. When the data is highly skewed toward the usual, it can be very difficult for a learning system to accurately detect the rare event. There have been many approaches in recent years for handling imbalanced data sets, from under-sampling the majority class to adding synthetic points to the minority class in feature space. Distances between time series are known to be non-Euclidean and nonmetric, since comparing time series requires warping in time. This fact makes it impossible to apply standard methods like SMOTE to insert synthetic data points in feature spaces. We present an innovative approach that augments the minority class by adding synthetic points in distance spaces. We then use Support Vector Machines for classification. Our experimental results on standard time series show that our synthetic points significantly improve the classification rate of the rare events, and in many cases also improves the overall accuracy of SVM.","Support vector machines,
Support vector machine classification,
Learning systems,
Data mining,
USA Councils,
Petroleum,
Sampling methods,
Computer science,
Costs,
Event detection"
Exploring mobile devices as Grid resources: Using an x86 virtual machine to run BOINC on an iPhone,"The increasing power and number of mobile devices makes them an attractive target for grid computing. To date, most research related to mobile devices and grid computing is focused on the access and use of grid resources. In this paper we propose the use of mobile devices themselves as grid computing nodes. We demonstrate the feasibility of this concept by implementing the BOINC client on an Apple iPhone. Work units are downloaded from a BOINC server and executed on the iPhone via a virtual machine emulating an x86 processor, and results are uploaded to the server. The world of mobile devices brings renewed challenges to the problem of grid client design in the areas of network bandwidth, processor capability, storage, and energy consumption. Using our prototype, we conduct initial studies evaluating performance, energy efficiency, and bandwidth on a cellular device against a more traditional grid computing node.",
Noise Injection for Search Privacy Protection,"To protect user privacy in the search engine context, most current approaches, such as private information retrieval and privacy preserving data mining, require a server-side deployment, thus users have little control over their data and privacy. In this paper we propose a user-side solution within the context of keyword based search. We model the search privacy threat as an information inference problem and show how to inject noise into user queries to minimize privacy breaches. The search privacy breach is measured as the mutual information between real user queries and the diluted queries seen by search engines. We give the lower bound for the amount of noise queries required by a perfect privacy protection and provide the optimal protection given the number of noise queries. We verify our results with a special case where the number of noise queries is equal to the number of user queries. The simulation result shows that the noise given by our approach greatly reduces privacy breaches and outperforms random noise. As far as we know, this work presents the first theoretical analysis on user side noise injection for search privacy protection.","Protection,
Search engines,
Data privacy,
Information retrieval,
Noise reduction,
Data engineering,
Computer science,
Mutual information,
Information analysis,
Data mining"
Restoring punctuation and capitalization in transcribed speech,"Adding punctuation and capitalization greatly improves the readability of automatic speech transcripts. We discuss an approach for performing both tasks in a single pass using a purely text-based n-gram language model. We study the effect on performance of varying the n-gram order (from n = 3 to n = 6) and the amount of training data (from 58 million to 55 billion tokens). Our results show that using larger training data sets consistently improves performance, while increasing the n-gram order does not help nearly as much.",
An Approach for Validation of Semantic Composability in Simulation Models,"Semantic composability aims to ensure that the composition of simulation components is meaningful in terms of their expressed behavior, and achieves the desired objective of the new composed model. Validation of semantic composability is a non-trivial problem because reused simulation components are heterogeneous in nature and validation must consider various orthogonal aspects including logical, temporal, and formal. In this paper, we propose a layered approach to semantic composability validation with increasing accuracy and complexity. The first layer exploits model checking for logical properties of component coordination including deadlock, safety, and liveness. Next, we address temporal properties by validating composition safety and liveness through simulation time. The third layer provides a formal composition validation guarantee by determining the behavioral equivalence between the composed model and a perfect model. In contrast to state-of-the-art approaches, we propose time-based formalisms to describe simulation components and compare the composition behaviors through time using semantically related composition states.","Safety,
Computational modeling,
System recovery,
Time measurement,
Conferences,
Computer science,
Drives,
Computer simulation,
Context modeling,
Bills of materials"
Improving arabic text categorization using decision trees,"This paper presents the results of classifying Arabic text documents using a decision tree algorithm. Experiments are performed over two self collected data corpus and the results show that the suggested hybrid approach of Document Frequency Thresholding using an embedded information gain criterion of the decision tree algorithm is the preferable feature selection criterion. The study concluded that the effectiveness of the improved classifier is very good and gives generalization accuracy about 0.93 for the scientific corpus and 0.91 for the literary corpus and we also conclude that the effectiveness of the decision tree classifier was increased as we increase the training size, and the nature of the corpus has such a influence on the classifier performance.","Text categorization,
Decision trees,
Classification tree analysis,
Testing,
Support vector machines,
Support vector machine classification,
Natural languages,
Computer science,
Frequency,
Performance gain"
On predicting the time taken to correct bug reports in open source projects,"Existing studies on the maintenance of open source projects focus primarily on the analyses of the overall maintenance of the projects and less on specific categories like the corrective maintenance. This paper presents results from an empirical study of bug reports from an open source project, identifies user participation in the corrective maintenance process through bug reports, and constructs a model to predict the corrective maintenance effort for the project in terms of the time taken to correct faults. Our study focuses on 72482 bug reports from over nine releases of Ubuntu, a popular Linux distribution. We present three main results 1) 95% of the bug reports are corrected by people participating in groups of size ranging from 1 to 8 people, 2) there is a strong linear relationship (about 92%) between the the number of people participating in a bug report and the time taken to correct it, 3) a linear model can be used to predict the time taken to correct bug reports.","Predictive models,
Computer science,
Fault diagnosis,
Linux,
Testing,
Particle measurements,
Feedback,
Software maintenance,
Lab-on-a-chip,
Open source software"
Blind Decomposition of Transmission Light Microscopic Hyperspectral Cube Using Sparse Representation,"In this paper, we address the problem of fully automated decomposition of hyperspectral images for transmission light microscopy. The hyperspectral images are decomposed into spectrally homogeneous compounds. The resulting compounds are described by their spectral characteristics and optical density. We present the multiplicative physical model of image formation in transmission light microscopy, justify reduction of a hyperspectral image decomposition problem to a blind source separation problem, and provide method for hyperspectral restoration of separated compounds. In our approach, dimensionality reduction using principal component analysis (PCA) is followed by a blind source separation (BSS) algorithm. The BSS method is based on sparsifying transformation of observed images and relative Newton optimization procedure. The presented method was verified on hyperspectral images of biological tissues. The method was compared to the existing approach based on nonnegative matrix factorization. Experiments showed that the presented method is faster and better separates the biological compounds from imaging artifacts. The results obtained in this work may be used for improving automatic microscope hardware calibration and computer-aided diagnostics.","Microscopy,
Hyperspectral imaging,
Blind source separation,
Principal component analysis,
Source separation,
Biomedical optical imaging,
Image decomposition,
Image restoration,
Optimization methods,
Biological tissues"
Improving Movie Gross Prediction through News Analysis,"Traditional movie gross predictions are based on numerical and categorical movie data from The Internet Movie Database (IMDB). In this paper, we use the quantitative news data generated by Lydia, our system for large-scale news analysis, to help people to predict movie grosses. By analyzing two different models (regression and k-nearest neighbor models), we find models using only news data can achieve similar performance to those using IMDB data. Moreover, we can achieve better performance by using the combination of IMDB data and news data. Further, the improvement is statistically significant.","Motion pictures,
Intelligent agent,
Predictive models,
Deductive databases,
Thumb,
Economic forecasting,
Conferences,
Computer science,
USA Councils,
Internet"
Traffic Measurement and Analysis of a Broadband Wireless Internet Access,"The increasing broadband wireless Internet usage and the limited wireless resources require a careful network management and optimization of the wireless Internet Service Providers (ISPs). Unfortunately, those providers often just have statistics about the overall usage and limited knowledge about the detailed application distribution as well as the traffic characteristics. In this paper we present user and traffic characteristics measured at a broadband wireless Internet access. The results show that the applications change quickly but the general characteristics like packet size and TCP/UDP percentage have not changed during the last years.","Telecommunication traffic,
IP networks,
Web and internet services,
Wireless LAN,
Monitoring,
Peer to peer computing,
Video sharing,
Computer science,
Computer network management,
Resource management"
Millimeter-Wave Spatial Multiplexing in an Indoor Environment,"A unique feature of communication at millimeter (mm) wave carrier frequencies is that spatial multiplexing is available for multiple-input multiple-output (MIMO) links with moderate antenna spacing even without a rich scattering environment. In this paper, we investigate the potential for exploiting this observation for increasing the spectral efficiency of indoor 60 GHz links. We begin by establishing limits on the spatial degrees of freedom available for linear antenna arrays of constrained length. A system architecture designed to exploit the available degrees of freedom, including beamforming as well as spatial multiplexing, is proposed. We evaluate the link capacity achievable by the proposed architecture when operating in a simple indoor environment. The results illustrate the relationship between between the channel quality and the relative positions of the transmit and receive nodes.","Indoor environments,
Text categorization,
Clustering algorithms,
Impurities,
Statistics,
Computer science,
Testing,
Data mining,
Conferences,
Availability"
Swarm Based Text Summarization,"The scoring mechanism of the text features is the unique way for determining the key ideas in the text to be presented as text summary. The treating of all text features with same level of importance can be considered the main factor causing creating a summary with low quality. In this paper, we introduced a novel text summarization model based on swarm intelligence. The main purpose of the proposed model is for scoring the sentences, emphasizing on dealing with the text features fairly based on their importance. The weights obtained from the training of the model were used to adjust the text features scores, which could play an important role in the selection process of the most important sentences to be included in the final summary. The results show that the human summaries H1 and H2 are 49% similar to each other. The proposed model creates summaries which are 43% similar to the manually generated summaries, while the summaries produced by Ms Word summarizer are 39% similar.","Particle swarm optimization,
Computer science,
Springs,
Machine learning,
Web pages,
Clustering algorithms,
Information systems,
Text categorization,
Data mining,
HTML"
Learning on the Fly: Font-Free Approaches to Difficult OCR Problems,"Despite ubiquitous claims that optical character recognition (OCR) is a ""solved problem,'' many categories of documents continue to break modern OCR software such as documents with moderate degradation or unusual fonts. Many approaches rely on pre-computed or stored character models, but these are vulnerable to cases when the font of a particular document was not part of the training set, or when there is so much noise in a document that the font model becomes weak. To address these difficult cases, we present a form of iterative contextual modeling that learns character models directly from the document it is trying to recognize. We use these learned models both to segment the characters and to recognize them in an incremental, iterative process. We present results comparable to those of a commercial OCR system on a subset of characters from a difficult test document.","Optical character recognition software,
Character recognition,
Context modeling,
Degradation,
Natural languages,
Hidden Markov models,
Text analysis,
Computer science,
System testing,
Iterative methods"
A Class of Zero-Correlation Zone Sequence Set Using a Perfect Sequence,"This letter introduces a novel method of sequence construction having a zero-correlation zone. The cross-correlation function and the side-lobe of the auto-correlation function of the proposed sequence set is zero for the phase shifts within the zero-correlation zone. The proposed zero-correlation zone sequence set can be generated from an arbitrary perfect ternary sequence, the length of which is the product of a pair of odd integers ( (2k+1)(2n+1) for k ges 1 and n ges 0). The width of the zero-correlation zone of the proposed sequence set can be longer than the length of the perfect sequence that is used for constructing the proposed sequence set. The proposed sequence construction can generate an optimal zero-correlation zone sequence set that satisfies the theoretical bound of the sequence member size for the zero-correlation zone and the sequence period.","Autocorrelation,
Communication systems,
Radar applications,
Radar imaging,
Radar theory,
Radar detection,
Ultrasonic imaging,
Computer science,
Cities and towns,
Equations"
Debugging of Toffoli networks,"Intensive research is performed to find post-CMOS technologies. A very promising direction based on reversible logic are quantum computers. While in the domain of reversible logic synthesis, testing, and verification have been investigated, debugging of reversible circuits has not yet been considered. The goal of debugging is to determine gates of an erroneous circuit that explain the observed incorrect behavior. In this paper we propose the first approach for automatic debugging of reversible Toffoli networks. Our method uses a formulation for the debugging problem based on Boolean satisfiability. We show the differences to classical (irreversible) debugging and present theoretical results. These are used to speed-up the debugging approach as well as to improve the resulting quality. Our method is able to find and to correct single errors automatically.","Debugging,
Error correction,
Quantum computing,
Logic circuits,
Circuit synthesis,
Circuit testing,
Transistors,
Computer science,
Logic testing,
Circuit simulation"
Modeling of path loss for ultrawide band body-centric wireless communications,"This paper presents the characterization of the path loss in ultrawide band body area networks. A Measurement campaign was performed in the anechoic chamber and in an indoor environment for comparison. The propagation along the front part of the body was analyzed, and the effect of the body movements on the signal strength was investigated. The results demonstrated that the normal distribution provides the best fit for modeling the path loss.","Wireless communication,
Antenna measurements,
Indoor environments,
Performance evaluation,
Frequency measurement,
Loss measurement,
Antennas and propagation,
Anechoic chambers,
Signal analysis,
Transmitting antennas"
Visualising image databases,"In this paper we explore different ways in which large collections of images can be visualised. We discuss the three principle visualisation techniques employed for this purpose, namely dimensionality reduced mappings, clustering-based visualisations and graph-based representations. Mapping-based techniques try to present the relationships between images described by high-dimensional features in a low-dimensional visualisation space. Clustered visualisations group similar images based on content, metadata or time stamp information, while in graph-based approaches links between images are exploited to arrive at an intuitive display of the dataset. We highlight advantages and disadvantages of the various approaches and emphasise the need for a benchmark which allows objective evaluation of these systems.","Visualization,
Image databases,
Principal component analysis,
Humans,
Image retrieval,
Feature extraction,
Bridges,
Spatial databases,
Visual databases,
Data engineering"
Describing story evolution from dynamic information streams,"Sources of streaming information, such as news syndicates, publish information continuously. Information portals and news aggregators list the latest information from around the world enabling information consumers to easily identify events in the past 24 hours. The volume and velocity of these streams causes information from prior days to quickly vanish despite its utility in providing an informative context for interpreting new information. Few capabilities exist to support an individual attempting to identify or understand trends and changes from streaming information over time. The burden of retaining prior information and integrating with the new is left to the skills, determination, and discipline of each individual. In this paper we present a visual analytics system for linking essential content from information streams over time into dynamic stories that develop and change over multiple days. We describe particular challenges to the analysis of streaming information and present a fundamental visual representation for showing story change and evolution over time.","Information analysis,
Graphics,
Visual analytics,
Joining processes,
Streaming media,
Data models,
Laboratories,
Portals,
Image analysis,
Indexing"
Continuous authentication by electrocardiogram data,"Authentication is the process of verifying the claimed identity of a user. Traditional authentication systems suffer from vulnerabilities that can break the security of the system. An example of such vulnerabilities is Replay Attack: An attacker can use a pre-saved password or an authentication credential to log into the system. Another issue with existing authentication systems is that the authentication process is done only at the beginning of a session: once the user is authenticated in the system, her identity is assumed to remain the same during the lifetime of the session. In real world, an attacker can masquerade as a legitimate user by physically controlling an authenticated machine. Therefore, there is a need to continuously monitor the user to determine if the user who is using the computer is the same person that logged onto the system. In this paper, we present a framework for continuous authentication of the user based on the electrocardiogram data collected from the user's heart signal. The electrocardiogram (ECG) data is used as a soft biometric to continuously authenticate the identity of the user; Experimental results demonstrate that electrocardiogram biometric trait can guarantee the safety of the system from illegal access.","Authentication,
Biometrics,
Electrocardiography,
Security,
Data privacy,
Fingerprint recognition,
Protection,
Access control,
Computerized monitoring,
Heart"
Investigating the Implications of Virtual Machine Introspection for Digital Forensics,"Researchers and practitioners in computer forensics currently must base their analysis on information that is either incomplete or produced by tools that may themselves be compromised as a result of the intrusion. Complicating these issues are the techniques employed by the investigators themselves. If the system is quiescent when examined, most of the information in memory has been lost. If the system is active, the kernel and programs used by the forensic investigators are likely to influence the results and as such are themselves suspect. Using virtual machines and a technique called virtual machine introspection can help overcome these limits, but it introduces its own research challenges. Recent developments in virtual machine introspection have led to the identification of four initial priority research areas in virtual machine introspection including virtual machine introspection tool development, applications of virtual machine introspection to non-quiescent virtual machines, virtual machine introspection covert operations, and virtual machine introspection detection.","Virtual machining,
Digital forensics,
Computer science,
Hard disks,
Information analysis,
Kernel,
Availability,
Computer security,
Cryptography,
Read-write memory"
Very short-term load forecasting: Multilevel wavelet neural networks with data pre-filtering,"Very short term load forecasting predicts the load over one hour into the future in five minute steps, and is important in resource dispatch and area generation control. Effective forecasting, however, is difficult in view of noisy real-time data gathering and complicated features of load. This paper presents a method based on multilevel wavelet neural networks with novel pre-filtering. The key idea is to use a data pre-filtering method to detect and eliminate spikes within load, apply the wavelet technique to decompose the load into several frequency components, perform appropriate transformation on each component, and feed it together with other appropriate input to a separate neural network. Numerical testing demonstrates the significant value of data pre-filtering and multilevel wavelet neural networks, and shows that our method provides accurate forecasting.",
Urdu compound Character Recognition using feed forward neural networks,"Urdu compound Character Recognition is a scarcely developed area and requires robust techniques to develop as Urdu being a family of Arabic script is cursive, right to left in nature and characters change their shapes and sizes when they are placed at initial, middle or at the end of a word. The developed system consists of two main modules segmentation and classification. In the segmentation phase pixels strength is measured to detect words in a sentence and joints of characters in a compound/connected word for segmentation. In the next phase these segmented characters are feeded to a trained Neural Network for classification and recognition, where Feed Forward Neural Network is trained on 56 different classes of characters each having 100 samples. The main purpose of the system is to test the algorithm developed for segmentation of compound characters. The prototype of the system has been developed in Matlab, currently achieves 70% accuracy on the average","Character recognition,
Feeds,
Neural networks,
Feedforward neural networks,
Robustness,
Shape,
Phase measurement,
Phase detection,
System testing,
Prototypes"
Design and analysis of asynchronous wakeup for wireless sensor networks,"In wireless sensor networks, scheduling the sleep duration of each node is one of the key elements for controlling critical performance metrics such as energy consumption and latency. Since the wakeup interval is a primary parameter for determining the sleeping schedule, how to tune the wakeup interval is crucial for the overall network performance. In this paper, we present an effective framework for tuning asynchronous wakeup intervals of IEEE 802.15.4 sensor networks from the energy consumption viewpoint. First, we derive an energy consumption model of each node as an explicit function of the wakeup interval, and empirically validate the derived model. Second, based on the proposed model, we formulate the problem of tuning the wakeup interval with the following two objectives: to minimize total energy consumption and to maximize network lifetime. We show that these two problems can be optimally solved by an iterative algorithm with global information by virtue of the convexity of the problem structure. Finally, as practical solutions, we further propose heuristic optimization algorithms that only exploit local information. In order to develop heuristic algorithms, we propose two broadcasting schemes, which are entitled as maximum wakeup interval broadcasting and efficient local maximum broadcasting. These broadcasting algorithms enable nodes in the network to have heterogeneous wakeup intervals.",
"System integration of FastSPECT III, a dedicated SPECT rodent-brain imager based on BazookaSPECT detector technology","FastSPECT III is a stationary, single-photon emission computed tomography (SPECT) imager designed specifically for imaging and studying neurological pathologies in rodent brain, including Alzheimer's and Parkinsons's disease. Twenty independent BazookaSPECT [1] gamma-ray detectors acquire projections of a spherical field of view with pinholes selected for desired resolution and sensitivity. Each BazookaSPECT detector comprises a columnar CsI(Tl) scintillator, image-intensifier, optical lens, and fast-frame-rate CCD camera. Data stream back to processing computers via firewire interfaces, and heavy use of graphics processing units (GPUs) ensures that each frame of data is processed in real time to extract the images of individual gamma-ray events. Details of the system design, imaging aperture fabrication methods, and preliminary projection images are presented.","Optical imaging,
Firewire,
Computed tomography,
Pathology,
Rodents,
Parkinson's disease,
Gamma ray detectors,
Optical sensors,
Lenses,
Charge coupled devices"
RBM: A Role Based Mobility Model for VANET,"Simulation is an important approach to evaluate protocols. Realistic simulations lead to accurate evaluations. However, most of evaluations in VANET recently were still using MANET mobility models or MANET models with a map bounder. Existing VANET mobility models, which treat all nodes identical, cannot reach most researchers' demand. In this paper, we develop a role based mobility model for VANET. On contrast to existing models, nodes are differentiated by their roles. And nodes playing different roles have different strategy on both macro and micro mobility scope. Simulation shows that this new model are more realistic than existing models and can lead to many daily traffic situations that is easily observed in real life.","Mobile ad hoc networks,
Computational modeling,
Traffic control,
Turning,
Road vehicles,
Mobile communication,
Mobile computing,
Computer science,
Computer simulation,
Protocols"
"Optimized data transfer for time-dependent, GPU-based glyphs","Particle-based simulations are a popular tool for researchers in various sciences. In combination with the availability of ever larger COTS clusters and the consequently increasing number of simulated particles the resulting datasets pose a challenge for real-time visualization. Additionally the semantic density of the particles exceeds the possibilities of basic glyphs, like splats or spheres and results in dataset sizes larger by at least an order of magnitude. Interactive visualization on common workstations requires a careful optimization of the data management, especially of the transfer between CPU and GPU. We propose a flexible benchmarking tool along with a series of tests to allow the evaluation of the performance of different CPU/GPU combinations in relation to a particular implementation. We evaluate different uploading strategies and rendering methods for point-based compound glyphs suitable for representing the aforementioned datasets. CPU and GPU-based approaches are compared with respect to their rendering and storage efficiency to point out the optimal solution when dealing with time-dependent datasets. The results of our research are of general interest since they can be transferred to other applications where CPU-GPU bandwidth and a high number of graphical primitives per dataset pose a problem. The employed tool set for streamlining the measurement process is made publicly available.",
Intelligent context-aware monitoring of hypertensive patients,"We present a decision-level data fusion technique for monitoring and reporting critical health conditions of a hypertensive patient at home. Variables associated to the patient (physiological and behavioral) and to the living environment are considered in the solution, contributing to improve the confidence on the system outputs. In the paper, we model the problem variables as fuzzy, aiming to capture their intrinsic essence, and draw rules based on medical recommendations to identify the health condition of the patient. This initiative move towards to build an abstract framework for context-aware telemonitoring applications. We also describe the relevant components of the framework and provide an initial evaluation of its decision component. Our results demonstrate that a principled choice of rules and variables may lead to a consistent identification of critical patient's conditions.","Patient monitoring,
Hypertension,
Biomedical monitoring,
Computerized monitoring,
Condition monitoring,
Medical diagnostic imaging,
Medical services,
Pervasive computing,
Intelligent sensors,
Hospitals"
On decomposing Boolean functions via extended cofactoring,"We investigate restructuring techniques based on decomposition/factorization, with the objective to move critical signals toward the output while minimizing area. A specific application is synthesis for minimum switching activity (or high performance), with minimum area penalty, where decompositions with respect to specific critical variables are needed (the ones of highest switching activity for example). In this paper we describe new types of factorization that extend Shannon cofactoring and are based on projection functions that change the Hamming distance of the original minterms and on appropriate don't care sets, to favor logic minimization of the component blocks. We define two new general forms of decomposition that are special cases of the pattern F = G(H(X),Y). The related implementations, called P-Circuits, show experimentally promising results in area with respect to Shannon cofactoring.","Boolean functions,
Minimization,
Energy consumption,
Power dissipation,
Delay,
Computer science,
Signal synthesis,
Hamming distance,
CMOS logic circuits,
CMOS technology"
Ontology-Based Smart Home Solution and Service Composition,"Given the diversity of home environment, appliances, and residents, the applications for smart homes must be configurable and adaptive. Instead of programming each household, we propose an ontology-based framework to facilitate the automatic composition of appropriate applications. The system composes appropriate services depending upon the available equipments in each individual household automatically. Meanwhile, it dynamically adjusts the environment parameters to match the customer needs and to encompass the available resource. With its supporting on customized function template editing, customers are able to specify their usual behavior templates as different mode.","Ontologies,
Smart homes,
Home appliances,
Embedded software,
Intelligent sensors,
Computer science,
Semantic Web,
Telecommunication computing,
Machine intelligence,
Resource description framework"
Harnessing bacterial power in microscale actuation,"This paper presents a systematic analysis of the motion of microscale structures actuated by flagellated bacteria. We perform the study both experimentally and theoretically. We use a blotting procedure to attach flagellated bacteria to a buoyancy-neutral plate called a microbarge. The motion of the plate depends on the distribution of the cells on the plate and the stimuli from the environment. We construct a stochastic mathematical model for the system, based on the assumption that the behavior of each bacterium is random and independent of that of its neighbors. The main finding of the paper is that the motion of the barge plus bacteria system is a function of a very small set of parameters. This reduced-dimensional model can be easily estimated using experimental data. We show that the simulation results obtained from the model show an excellent match with the experimentally-observed motion of the barge.",
"Notice of Violation of IEEE Publication Principles
A Survey on Complex Wormhole Attack in Wireless Ad Hoc Networks","Notice of Violation of IEEE Publication Principles

""A Survey on Complex Wormhole Attack in Wireless Ad Hoc Networks""
by M. Jain and H. Kandwal
in the Proceedings of the International Conference on Advances in Computing, Control, & Telecommunication Technologies, 2009. ACT '09. December 2009, pp. 555-558

After careful and considered review of the content and authorship of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.

This paper contains significant portions of original text from the paper cited below. The original text was copied without attribution (including appropriate references to the original author(s) and/or paper title) and without permission.

Due to the nature of this violation, reasonable effort should be made to remove all past references to this paper, and future references should be made to the following article:

""A Full Image of the Wormhole Attacks Towards Introducing Complex Wormhole Attacks in Wireless Ad Hoc Networks""
by Marianne A.Azer, Sherif M. El-Kassas, Magdy S. El-Soudani
in International Journal of Computer Science and Information Security, Special Issue, May 2009


Wireless communication faces several security risks. An attacker can easily inject bogus packets, impersonating another sender. We refer to this attack as a spoofing attack. An attacker can also easily eavesdrop on communication record packets, and replay the (potentially altered) packets. In this paper, we are concerned of a particularly severe security attack that affects the ad hoc networks routing protocols, it is called the wormhole attack. We can think of wormhole attack as a two phase process launched by one or several malicious nodes. In the first phase, these malicious nodes, called wormhole nodes, try to lure legitimate nodes to send data to other nodes via them. In the second phase, wormhole nodes could exploit the data in variety of ways. We will introduce the wormhole attack modes and classes, and point to its impact and threat on ad hoc networks. We finally summarize and conclude this paper.","Mobile ad hoc networks,
Ad hoc networks,
Encapsulation,
Communication system security,
Computer networks,
Telecommunication computing,
Communication system control,
Telecommunication control,
Information technology,
Wireless communication"
Long term electrical load forecasting via a neurofuzzy model,"Long-term forecasting of load demand is necessary for the correct operation of electric utilities. There is an on-going attention toward putting new approaches to the task. Recently, Neurofuzzy modeling has played a successful role in various applications over nonlinear time series prediction. This paper presents a neurofuzzy model for long-term load forecasting. This model is identified through Locally Linear Model Tree (LoLiMoT) learning algorithm. The model is compared to a multilayer perceptron and hierarchical hybrid neural model (HHNM). The models are trained and assessed on load data extracted from a North-American electric utility.","Load forecasting,
Predictive models,
Multilayer perceptrons,
Power industry,
Economic forecasting,
Multi-layer neural network,
Power system planning,
Power generation,
Neural networks,
Data mining"
Linux kernels as complex networks: A novel method to study evolution,"In recent years, many graphs have turned out to be complex networks. This paper presents a novel method to study Linux kernel evolution — using complex networks to understand how Linux kernel modules evolve over time. After studying the node degree distribution and average path length of the call graphs corresponding to the kernel modules of 223 different versions (V1.1.0 to V2.4.35), we found that the call graphs of the file system and drivers module are scale-free small-world complex networks. In addition, both of the file system and drivers module exhibit very strong preferential attachment tendency. Finally, we proposed a generic method that could be used to find major structural changes that occur during the evolution of software systems.","Linux,
Kernel,
Complex networks,
Evolution (biology),
Software systems,
Computer science,
File systems,
IP networks,
Open source software,
Social network services"
Autonomous altitude estimation of a UAV using a single onboard camera,"Autonomous estimation of the altitude of an Unmanned Aerial Vehicle (UAV) is extremely important when dealing with flight maneuvers like landing, steady flight, etc. Vision based techniques for solving this problem have been underutilized. In this paper, we propose a new algorithm to estimate the altitude of a UAV from top-down aerial images taken from a single on-board camera. We use a semi-supervised machine learning approach to solve the problem. The basic idea of our technique is to learn the mapping between the texture information contained in an image to a possible altitude value. We learn an over complete sparse basis set from a corpus of unlabeled images capturing the texture variations. This is followed by regression of this basis set against a training set of altitudes. Finally, a spatio-temporal Markov Random Field is modeled over the altitudes in test images, which is maximized over the posterior distribution using the MAP estimate by solving a quadratic optimization problem with L1 regularity constraints. The method is evaluated in a laboratory setting with a real helicopter and is found to provide promising results with sufficiently fast turnaround time.","Unmanned aerial vehicles,
Machine learning,
Helicopters,
Intelligent robots,
USA Councils,
Robot vision systems,
Smart cameras,
Computer science,
Machine learning algorithms,
Markov random fields"
Logic synthesis for better than worst-case designs,"In this paper we present a novel metric for measuring and optimizing the performance of circuits that operate with the clock period smaller than the worst-case delay. In particular, we developed an efficient logic optimization operation “balance” and a library mapping algorithm named BTWLibMap. Together they are able to reduce the probability of a timing error by 2.3X while only incurring a 4% area overhead.","Logic design,
Delay,
Clocks,
Latches,
Pipelines,
Logic circuits,
Design optimization,
Timing,
Circuit optimization,
Registers"
Reprogramming with Minimal Transferred Data on Wireless Sensor Network,"In Wireless Sensor Networks, the preloaded program code and data on sensor nodes often need to be updated due to changes in user requirements or environmental conditions. Sensor nodes are severely restricted by energy constraints. It is especially energy consuming for sensor nodes to update code through radio packages. To efficiently update code through wireless radio, we propose an algorithm, Reprogramming with Minimal Transferred Data (RMTD), to find the optimum combination of copying from the old code image and downloading from the host machine to minimize the number of bytes needed to be transferred from the host machine to a sensor node. Our experiments show that, for small code modifications, RMTD reduces the number of bytes transferred by 93.25% over the existing Rsync-based algorithm. For normal code changes, RMTD shows an improvement of 59.82% in average.","Wireless sensor networks,
Image sensors,
Computer science,
Image segmentation,
Hardware,
Helium,
Packaging machines,
Wildlife,
Power generation economics,
Environmental economics"
Space charge accumulation under effects of temperature gradient and applied voltage reversal on solid dielectric DC cable,"A well-known fact of the existence and accumulation of space charge within the insulating material poses threat to the reliability in the operation of dc power cables. When power cables are loaded under high voltage direct current (HVDC), temperature gradient is developed across the insulation. Results of space charge evolution in commercial ac XLPE power cables under an application of 80 kV dc supply at different temperature gradients and during external voltage reversal are discussed in this paper. The space charge distributions were measured across the insulation of the cable by means of a modified pulsed electroacoustic (PEA) system with a current transformer attached. Therefore, a replica of a power cable under load conditions could be obtained, which allows us to investigate the formation, migration and accumulation of space charges in a power cable both without and with different temperature gradients consideration across the bulk of the insulating material during voltage reversal. Discussion will be made thoroughly in order to understand the space charge phenomenon of power cable under its service temperature as space charge accumulation during polarity reversal plays an important factor on the electric field distribution within the insulation material.",
Planar Graph Isomorphism is in Log-Space,"Graph Isomorphism is the prime example of a computational problem with a wide difference between the best known lower and upper bounds on its complexity. There is a significant gap between extant lower and upper bounds for planar graphs as well. We bridge the gap for this natural and important special case by presenting an upper bound that matches the known log-space hardness [JKMT03]. In fact, we show the formally stronger result that planar graph canonization is in log-space. This improves the previously known upper bound of AC1 [MR91]. Our algorithm first constructs the biconnected component tree of a connected planar graph and then refines each biconnected component into a triconnected component tree. The next step is to log-space reduce the biconnected planar graph isomorphism and canonization problems to those for 3-connected planar graphs, which are known to be in log-space by [DLN08]. This is achieved by using the above decomposition, and by making significant modifications to Lindell’s algorithm for tree canonization, along with changes in the space complexity analysis. The reduction from the connected case to the biconnected case requires further new ideas, including a non-trivial case analysis and a group theoretic lemma to bound the number of automorphisms of a colored 3-connected planar graph. This lemma is crucial for the reduction to work in log-space.","Upper bound,
Tree graphs,
Computational complexity,
Bridges,
Algorithm design and analysis,
Polynomials,
Encoding,
Computer science,
Testing"
Design of dual-band rat-race couplers,"New designs of dual-band rat-race couplers are proposed. The desired dual-band operations are realised by adding tapped open stubs to the branches of the conventional rat-race coupler. It is found that these stubs can be implemented in two different ways to satisfy the conditions for dual-band applications, which results in two types of designs. The two different designs can be separately used to achieve low and high ratios between two operating bands, therefore providing enhanced design flexibility. The design formulas are developed using ABCD matrix and the even-odd mode analysis. On the basis of the derived design equations, a type I rat-race coupler working at 1 GHz/3.5 GHz and a type II coupler working at 2 GHz/5 GHz are devised and fabricated on Rogers's RO3006 printed circuit boards. Measurement results prove the design concepts.","waveguide couplers,
matrix algebra,
printed circuit design"
Performance of MAP Reconstruction for Hot Lesion Detection in Whole-Body PET/CT: An Evaluation With Human and Numerical Observers,"For positron emission tomography (PET) imaging, different reconstruction methods can be applied, including maximum likelihood (ML ) and maximum a posteriori (MAP) reconstruction. Postsmoothed ML images have approximately position and object independent spatial resolution, which is advantageous for (semi-) quantitative analysis. However, the complex object dependent smoothing obtained with MAP might yield improved noise characteristics, beneficial for lesion detection. In this contribution, MAP and postsmoothed ML are compared for hot spot detection by human observers and by the channelized Hotelling observer (CHO). The study design was based on the ldquomultiple alternative forced choicerdquo approach. For the MAP reconstruction, the relative difference prior was used. For postsmoothed ML, a Gaussian smoothing kernel was used. Both the human observers and the CHO performed slightly better on MAP images than on postsmoothed ML images. The average CHO performance was similar to the best human performance. The CHO was then applied to evaluate the performance of priors with reduced penalty for large differences. For these priors, a poorer detection performance was obtained.","Lesions,
Whole-body PET,
Humans,
Image reconstruction,
Smoothing methods,
Computed tomography,
Positron emission tomography,
Reconstruction algorithms,
Maximum likelihood detection,
Spatial resolution"
Iterative Off-Resonance and Signal Decay Estimation and Correction for Multi-Echo MRI,"Signal dephasing due to field inhomogeneity and signal decay due to transverse relaxation lead to perturbations of the Fourier encoding commonly applied in magnetic resonance imaging. Hence, images acquired with long readouts suffer from artifacts such as blurring, distortion, and intensity variation. These artifacts can be removed in reconstruction, usually based on separately collected information in form of field and relaxation maps. In this work, a recently proposed gridding-based algorithm for off-resonance correction is extended to also address signal decay. It is integrated into a new fixed-point iteration, which permits the joint estimation of an image and field and relaxation maps from multi-echo acquisitions. This approach is then applied in simulations and in vivo experiments and demonstrated to improve both images and maps. The rapid convergence of the fixed-point iteration in combination with the efficient gridding-based correction promises to render the running time of such a joint estimation acceptable.",
Human Behavior Analysis Based on a New Motion Descriptor,"Human behavior analysis is an important area of research in computer vision and is also driven by a wide spectrum of applications, such as smart video surveillance and human-computer interface. In this paper, we present a novel approach for human behavior analysis. Two research challenges, motion representation and behavior recognition, are addressed. A novel motion descriptor, which is an improved feature based on optical flow, is proposed for motion representation. Optical flow is improved with a motion filter, and feature fusion with the shape and trajectory information. To recognize the behavior, the support vector machine is employed to train the classifier where the concatenation of histograms is formed as the input features. Experimental results on the Weizmann behavior database and the Institute of Automation, Chinese Academy of Science real-world multiview behavior database demonstrate the robustness and effectiveness of our method.","Humans,
Motion analysis,
Optical filters,
Image motion analysis,
Spatial databases,
Computer vision,
Application software,
Video surveillance,
Information filtering,
Information filters"
Spatial Characterization of fMRI Activation Maps Using Invariant 3-D Moment Descriptors,"A novel approach is proposed for quantitatively characterizing the spatial patterns of activation statistics in functional magnetic resonance imaging (fMRI) activation maps. Specifically, we propose using 3-D invariant moment descriptors, as opposed to the traditionally-employed magnitude-based features such as mean voxel statistics or percentage of activated voxels, to characterize the task-specific spatial distribution of activation statistics within a given region of interest (ROI). The proposed method is applied to real fMRI data collected from 21 healthy subjects performing previously-learned right-handed finger tapping sequences that are either externally guided (EG) by a cue or internally guided (IG)-tasks expected to incur subtle differences in motor-related cortical and subcortical ROIs. Voxel-based activation statistics contrasting EG versus rest and IG versus rest are examined in multiple manually-drawn ROIs on unwarped brain images. Analyzing the activation statistics within each ROI using the proposed 3-D invariant moment descriptors detected significant group differences between the two tasks, thus quantitatively demonstrating that the spatial distribution of activation statistics within an ROI represent an important task-related attribute of brain activation. In contrast, conventional methods that solely rely on activation statistic magnitudes and disregard spatial information showed reduced discriminability. Normally, incorporating spatial information would merely increase inter-subject variability partly due to differences in brain size and subject's orientation in the scanner. Yet, our results suggest that the proposed spatial features, which are invariant to similarity transformations, can effectively account for such inter-subject variability, while enhancing the sensitivity in detecting task-specific activation. Thus, we argue that this novel quantitative description of the ldquo3-D texturerdquo of activation maps provides new directions to explore for ROI-based fMRI analysis.","Statistical distributions,
Magnetic resonance imaging,
Statistics,
Statistical analysis,
Magnetic analysis,
Biomedical computing,
Nervous system,
Biomedical imaging,
Fingers,
Brain"
Impact of Advertisements during Channel Zapping on Quality of Experience,"Nowadays various digital television services are available. However, the user of these services experiences longer delays than the traditional analog TV while switching from channel to channel. The digital TV operator usually displays a black screen with the channel number during zapping. However, it could be interesting for the TV viewer, if the operator displays a screen with information instead of just a black screen. This information may be an advertisement, information about the target channel, personalized content of the user etc. In this paper, we describe a subjective experiment where the Quality of Experience (QoE) of channel zapping was quantified, while displaying a random set of advertisement pictures during zapping. It is found that, for longer zapping times, advertisements give better QoE than the black screen. However, when zapping times are small,users prefer a black screen over a glance of an advertisement picture.","IPTV,
Testing,
Video on demand,
Digital TV,
Displays,
DSL,
Mathematics,
Computer science,
Communications technology,
Delay"
A 3-D High-Frequency Array Based 16 Channel Photoacoustic Microscopy System for In Vivo Micro-Vascular Imaging,"This paper discusses the design of a novel photoacoustic microscopy imaging system with promise for studying the structure of tissue microvasculature for applications in visualizing angiogenesis. A new 16 channel analog and digital high-frequency array based photoacoustic microscopy system (PAM) was developed using an Nd: YLF pumped tunable dye laser, a 30 MHz piezo composite linear array transducer, and a custom multichannel receiver electronics system. Using offline delay and sum beam- forming and beamsteering, phantom images were obtained from a 6 mum carbon fiber in water at a depth of 8 mm. The measured -6 dB lateral and axial spatial resolution of the system was 100 plusmn 5 mum and 45 plusmn 5 mum, respectively. The dynamic focusing capability of the system was demonstrated by imaging a composite carbon fiber matrix through a 12.5 mm imaging depth. Next, 2-D in vivo images were formed of vessels around 100 mum in diameter in the human hand. Three-dimensional in vivo images were also formed of micro-vessels 3 mm below the surface of the skin in two Sprague Dawley rats.","In vivo,
Optical arrays,
Visualization,
Electron microscopy,
Neodymium,
Laser excitation,
Tunable circuits and devices,
Fiber lasers,
Pump lasers,
Transducers"
Capacity Analysis of Wireless Mesh Networks with Omni or Directional Antennas,"In this paper we analyze the capacity of wireless mesh networks that use omni or directional antennas. The capacity in our analysis is the end-to-end per-node throughput. Our analysis is based on the assumption that there is only one gateway in the network and all end-users' traffics go through the gateway. Non-gateway nodes are uniformly distributed in a two- dimensional region centered at the gateway. The main results of our analysis can be summarized as: 1) The capacity is O(1/N) for both omni and directional antennas, where N is number of nodes in the network. 2) the capacity is O(1g m/thetas) for m = 2, and O(1g m/thetas2 1g (1/thetas)) for m > 2, where m is the number of antennas on each node, and thetas is the beamwidth of antennas.","Wireless mesh networks,
Directional antennas,
Throughput,
Peer to peer computing,
Directive antennas,
Ad hoc networks,
Base stations,
Computer science,
Telecommunication traffic,
Receiving antennas"
Mining search topics from a code search engine usage log,"We present a topic modeling analysis of a year long usage log of Koders, one of the major commercial code search engines. This analysis contributes to the understanding of what users of code search engines are looking for. Observations on the prevalence of these topics among the users, and on how search and download activities vary across topics, leads to the conclusion that users who find code search engines usable are those who already know to a high level of specificity what to look for. This paper presents a general categorization of these topics that provides insights on the different ways code search engine users express their queries. The findings support the conclusion that existing code search engines provide only a subset of the various information needs of the users when compared to the categories of queries they look at.","Search engines,
Open source software,
Information analysis,
Internet,
Crawlers,
Indexes,
Solids,
Linear discriminant analysis,
Relational databases"
Selective Deblurring for Improved Calcification Visualization and Quantification in Carotid CT Angiography: Validation Using Micro-CT,"Visualization and quantification of small structures with computed tomography (CT) is hampered by the limited spatial resolution of the system. Histogram-based selective deblurring (HiSD) is a deconvolution method that restores small high-density structures, i.e., calcifications, of a CT image, using the high-intensity voxel information of the deconvolved image, while preserving the original hounsfield Units (HUs) in the remaining tissues. In this study, high resolution micro-CT data are used to validate the potential of HiSD to improve calcium visualization and quantification in the carotid arteries on in vivo contrast-enhanced CTA data. The evaluation is performed qualitatively and quantitatively on 15 atherosclerotic plaques obtained from ten different patients. HiSD in combination with vessel segmentation significantly improves calcification visualization and quantification on in vivo contrast-enhanced CT images. Calcification blur is reduced, while avoiding noise amplification and edge-ringing artifacts in the surrounding tissues. Calcification quantification errors are reduced by 23.5% on average.","Angiography,
Computed tomography,
Spatial resolution,
Data visualization,
Deconvolution,
Image restoration,
Calcium,
Carotid arteries,
Performance evaluation,
Image segmentation"
Using Model-Based Traces as Runtime Models,"Software engineers typically use code-level tracing to capture a running system's behavior. An alternative is to generate and analyze model-based traces, which contain rich semantic information about the system's runs at the abstraction level that its design models define. A set of metrics and operators can aid such trace analysis.",
Low-power split-path data-driven dynamic logic,"Data-pre-charged dynamic logic, also known as data-driven dynamic logic (D3L), is very efficient when low-power constraints are mandatory. Differently from conventional dynamic domino logic, which exploits a clock signal, D3L uses a subset of the input data signals for pre-charging the dynamic node, thus avoiding the clock distribution network. Power consumption is significantly reduced, but the pre-charge propagation path delay affects the speed performances and limits the energy-delay product (EDP) improvements. This study presents a new dynamic logic named split-path D3L (SPD3L) that overcomes the speed limitations of D3L. When applied to a 16 times 16 bit booth multiplier realised with STMicroelectronics 65 nm IV CMOS technology, the proposed technique leads to an EDP 25 and 30% lower than standard dynamic domino logic and conventional D3L counterparts, respectively.","low-power electronics,
clocks,
CMOS logic circuits,
logic gates"
A novel RF sensing circuit using injection locking and frequency demodulation for cognitive radio applications,"A novel RF sensing circuit for a cognitive radio to sense spectral environment is proposed based on injection locking and frequency demodulation techniques. In the experiments, a prototype circuit is designed in the 2.4 GHz ISM band to cover a sensing frequency range from 2.4 to 2.484 GHz. The spectrum scanning over the 84-MHz bandwidth is accomplished in less than 1 ms. The experimental results show that the RF sensing circuit can sense the frequency and the power of frequency-modulated signals as well as digitally modulated signals with a constant envelope.","Radio frequency,
Injection-locked oscillators,
Radiofrequency integrated circuits,
Demodulation,
Cognitive radio,
Voltage-controlled oscillators,
FCC,
Frequency synthesizers,
RF signals,
Equations"
Singular Vectors of a Linear Imaging System as Efficient Channels for the Bayesian Ideal Observer,"The Bayesian ideal observer provides an absolute upper bound for diagnostic performance of an imaging system and hence should be used for the assessment of image quality whenever possible. However, computation of ideal-observer performance in clinical tasks is difficult since the probability density functions of the data required for this observer are often unknown in tasks involving realistic, complex backgrounds. Moreover, the high dimensionality of the integrals that need to be calculated for the observer makes the computation more difficult. The ideal observer constrained to a set of channels, which we call a channelized-ideal observer (CIO), can reduce the dimensionality of the problem. These channels are called efficient if the CIO can approximate ideal-observer performance. In this paper, we propose a method to choose efficient channels for the ideal observer based on a singular value decomposition of a linear imaging system. As a demonstration, we test our method on detection tasks using non-Gaussian lumpy backgrounds and signals of Gaussian and elliptical profiles. Our simulation results show that singular vectors associated with either the background or the signal are highly efficient for the ideal observer for detecting both types of signals. In addition, this CIO outperforms a channelized-Hotelling observer with the same channels.","Vectors,
Bayesian methods,
Image quality,
Biomedical imaging,
Mathematics,
Upper bound,
Probability density function,
Signal detection,
Drugs,
Singular value decomposition"
Speech rhythm guided syllable nuclei detection,"In this paper, we present a novel speech-rhythm-guided syllable-nuclei location detection algorithm. As a departure from conventional methods, we introduce an instantaneous speech rhythm estimator to predict possible regions where syllable nuclei can appear. Within a possible region, a simple slope based peak counting algorithm is used to get the exact location of each syllable nucleus. We verify the correctness of our method by investigating the syllable nuclei interval distribution in TIMIT dataset, and evaluate the performance by comparing with a state-of-the-art syllable nuclei based speech rate detection approach.",
An evaluative study on the effect of contention on message latencies in large supercomputers,"Significant theoretical research was done on interconnect topologies and topology aware mapping for parallel computers in the 80s. With the deployment of virtual cut-through, wormhole routing and faster interconnects, message latencies reduced and research in the area died down. This paper presents a study showing that with the emergence of very large supercomputers, typically connected as a 3D torus or mesh, topology effects have become important again. It presents an evaluative study on the effect of contention on message latencies on torus and mesh networks. The paper uses three MPI benchmarks to evaluate the effect of hops (links) traversed by messages, on their latencies. The benchmarks demonstrate that when multiple messages compete for network resources, link occupancy or contention can increase message latencies by up to a factor of 8 times. In other words, contention leads to increased message latencies and reduces effective available bandwidth for each message. This suggests that application developers should consider interconnect topologies when mapping tasks to processors in order to obtain the best performance. Results are shown for two parallel machines - ANL's Blue Gene/P and PSC's XT3.","Delay,
Supercomputers,
Routing,
Network topology,
Bandwidth,
Computer science,
Concurrent computing,
Mesh networks,
Parallel machines,
Equations"
oCast: Optimal multicast routing protocol for wireless sensor networks,"In this paper, we describe oCast, an energy-optimal multicast routing protocol for wireless sensor networks. The general minimum-energy multicast problem is NP-hard. Intermittent connectivity that results from duty-cycling further complicates the problem. Nevertheless, we present both a centralized and distributed algorithm that are provably optimal when the number of destinations is small. This model is motivated by scenarios where sensors report to a small number of base stations or where data needs to be replicated on a small number of other nodes. We further propose an extended version of oCast, called Delay Bounded oCast (DB-oCast), which can discover optimal multicast trees under a predefined delay bound. Finally, we demonstrate the advantages of our schemes through both theoretical analysis and simulations.","Multicast protocols,
Routing protocols,
Wireless sensor networks,
Computer science,
Base stations,
Delay,
Broadcasting,
Network topology,
Power engineering and energy,
Distributed algorithms"
A method for automatic detection and classification of stroke from brain CT images,"Computed tomographic (CT) images are widely used in the diagnosis of stroke. In this paper, we present an automated method to detect and classify an abnormality into acute infarct, chronic infarct and hemorrhage at the slice level of non-contrast CT images. The proposed method consists of three main steps: image enhancement, detection of mid-line symmetry and classification of abnormal slices. A windowing operation is performed on the intensity distribution to enhance the region of interest. Domain knowledge about the anatomical structure of the skull and the brain is used to detect abnormalities in a rotation- and translation-invariant manner. A two-level classification scheme is used to detect abnormalities using features derived in the intensity and the wavelet domain. The proposed method has been evaluated on a dataset of 15 patients (347 image slices). The method gives 90% accuracy and 100% recall in detecting abnormality at patient level; and achieves an average precision of 91% and recall of 90% at the slice level.",
On Spatially-Aware Channel Selection in Dynamic Spectrum Access Multi-Hop Inter-Vehicle Communications,"The use of dynamic spectrum access techniques has a great potential in future inter-vehicle communications, while it must cope with (i) temporal and spatial spectrum utilization changes introduced by the primary and secondary users (environmental changes); and (ii) topology changes due to movement of vehicles (spatial movement). In the present paper, along this line, dynamic per-hop channel switching in multi-hop vehicular ad hoc networking is investigated. After defining a set of simple metric-based dynamic channel selection schemes with/without spatial-awareness, the basic performance (the total communication duration and the amount of transmitted data) are evaluated. These spatially-aware schemes estimate the maximum communication period, and tend to select a channel with a large amount of possible data transmission within the period. The simulation results demonstrate the advantages of the proposed spatial-awareness, especially in the multi-hop and highly congested environments with high-speed mobility.","Vehicle dynamics,
Spread spectrum communication,
Ad hoc networks,
Network topology,
Road safety,
Road vehicles,
Relays,
Mobile ad hoc networks,
Routing,
Computer science"
Novel Item Recommendation by User Profile Partitioning,"Standard top-N collaborative recommendation algorithms are very poor at recommending relevant products to a user that are more novel than her average tastes. Our study shows that novel recommendation is difficult because standard similarity metrics measure the aggregate similarity to multiple items in the user profile and the influence of more novel items is lost in the aggregation. To better capture the user's range of tastes, we propose to partition the user profile into clusters of similar items and compose the recommendation list of items that match well with each cluster, rather than with the entire user profile. In this paper we evaluate a number of partitioning strategies in combination with a dimension reduction strategy. A new evaluation methodology is introduced to capture the system ability to diversify its recommendations across relevant items regardless of their novelty. By plotting concentration curves of novelty against accuracy, we show that this strategy succeeds in reducing the system bias towards similar items at a small cost to overall accuracy.","Clustering algorithms,
Intelligent agent,
Partitioning algorithms,
Computer science,
Informatics,
Educational institutions,
Costs,
Filtering,
Scalability,
Conferences"
Arithmetic unit design using 180nm TSV-based 3D stacking technology,"We describe the design of two three dimensional arithmetic units (a 3D adder and a 3D multiplier) that are implemented using through-silicon-via 3D stacking technology. Compared to their 2D counterparts, our 3D adder incurs 10.6–34.3% less delay and 11.0–46.1% less energy when the width increases from 12-bit to 72-bit; the 32×32 3D multiplier incurs 14.4% less delay and 6.8% less energy, according to the post place and route results. The prototype chip including the implementations of a 3D adder, a 3D multiplier, and simple test interface has been delivered for fabrication in MIT Lincoln Laboratory, using their 3-tier SOI based 3D technology.",
A study on the statistical properties of double hoyt fading channels,"This paper deals with a study on the statistical properties of narrowband amplify-and-forward relay fading channels for Hoyt multipath propagation environments. We consider the basic radio link topology, where only one fixed relay station is used for the amplification. In this case, the envelope of the signal received over the overall multipath channel is modeled by the so-called double Hoyt fading process. Considering this multipath propagation channel model, analytical expressions for the first and second order statistics are provided. Specifically, the mean value, variance, probability density function (PDF), level-crossing rate (LCR), and average duration of fades (ADF) of double Hoyt processes are derived. In addition, an expression for the PDF of the corresponding channel phase is also provided. Since the double Rayleigh fading channel is a special case of the double Hoyt model, it will be shown how the derived expressions can be reduced to the results known for the double Rayleigh fading channel. Moreover, the validity of the derived quantities is confirmed by computer simulations.","Fading,
Relays,
Narrowband,
Radio link,
Topology,
Signal processing,
Multipath channels,
Analytical models,
Statistical analysis,
Probability density function"
A theoretical and empirical study of EFSM dependence,"Dependence analysis underpins many activities in software maintenance such as comprehension and impact analysis. As a result, dependence has been studied widely for programming languages, notably through work on program slicing. However, there is comparatively little work on dependence analysis at the model level and hitherto, no empirical studies. We introduce a slicing tool for Extended Finite State Machines (EFSMs) and use the tool to gather empirical results on several forms of dependence found in ten EFSMs, including well-known benchmarks in addition to real-world EFSM models. We investigate the statistical properties of dependence using statistical tests for correlation and formalize and prove four of the empirical findings arising from our empirical study. The paper thus provides the maintainer with both empirical data and foundational theoretical results concerning dependence in EFSM models.","Software maintenance,
Benchmark testing,
Educational institutions,
Computer science,
Computer languages,
Automata,
Application software,
Software testing,
System testing,
Debugging"
Noise Estimation in Magnitude MR Datasets,"Estimating the noise parameter in magnitude magnetic resonance (MR) images is important in a wide range of applications. We propose an automatic noise estimation method that does not rely on a substantial proportion of voxels being from the background. Specifically, we model the magnitude of the observed signal as a mixture of Rice distributions with common noise parameter. The expectation-maximization (EM) algorithm is used to estimate all parameters, including the common noise parameter. The algorithm needs initializing values for which we provide some strategies that work well. The number of components in the mixture model also needs to be estimated en route to noise estimation and we provide a novel approach to doing so. Our methodology performs very well on a range of simulation experiments and physical phantom data. Finally, the methodology is demonstrated on four clinical datasets.","Magnetic noise,
Image segmentation,
Gaussian noise,
Magnetic resonance imaging,
Histograms,
Magnetic resonance,
Background noise,
Parameter estimation,
Imaging phantoms,
Equations"
Towards traceable test-driven development,"Key among the Grand Challenges in Traceability are those that lead to achieving traceability as a by-product of the natural software development life cycle. This position paper profiles test-driven development (TDD), an emerging software development practice, in which automated tests and code satisfying them are developed in rapid succession over multiple iterations. Our position is that the nature of TDD offers unique opportunities for collecting traceability information throughout the TDD life cycle and that the provision of traceability information to the software developers during TDD will improve the process and the resulting software. We discuss the opportunities, challenges, and plans for the synthesis of TDD and traceability.","Programming,
Software testing,
Computer science,
Automatic testing,
Software maintenance,
Software performance,
Maintenance engineering,
Mice,
Performance analysis,
ISO standards"
Scale-Up Strategies for Processing High-Rate Data Streams in System S,"High performance stream processing is critical in sense-and-respond application domains – from environmental monitoring to algorithmic trading. In this paper, we focus on language and runtime support for improving the performance of sense-and-respond applications in processing data from high rate streams. The central tenet of this work is the definition of a streaming architectural pattern for these application domains and the programming model and the code generation framework to support it. Using IBM Research's System S middleware and the SPADE language, we demonstrate how to scale up a financial trading application.","Data security,
Feeds,
Stock markets,
Application software,
Data engineering,
Computer science,
Computerized monitoring,
Runtime,
Middleware,
Large-scale systems"
Particle Swarm Optimization based Adaboost for face detection,"This paper proposes a PSOAdaBoost algorithm incorporating Particle Swarm Optimization within an AdaBoost framework for face detection applications. The basic component of an AdaBoost detector is a weak classifier, consisting of a feature, selected by an exhaustive search mechanism, and a decision threshold. The proposed PSOAdaBoost computes the best feature and optimizes the threshold in one optimization process. Experiments between the proposed algorithm and AdaBoost (with exhaustive feature selection) suggest that PSOAdaBoost has better performance in terms of much less training time and better classification accuracy.","Particle swarm optimization,
Face detection,
Image edge detection,
Computer vision,
Application software,
Eyes,
Nose,
Mouth,
Robustness,
Object detection"
A Comparative Study of Selected Classifiers with Classification Accuracy in User Profiling,"In recent years the use of personalized service provisioning applications has been very popular. However, effective personalization cannot be achieved without accurate user profiles. In literature a number of classification algorithms have been used to classify user related information to create accurate user profiles. Nevertheless, there is lack of comparison of these algorithms with classification accuracy of the user profile information. In our previous work [1], we compared four different classification algorithms which are; Naïve Bayesian (NB), Instance-Based Learner (IB1), Bayesian networks (BN) and Lazy Learning of Bayesian Rules (LBR) classifiers. According to our results NB and IB1 classifiers outperformed the BN and LBR classifiers with respect to classification accuracy. In this study we compare the performance of NB, IB1, Classification and Regression Tree (SimpleCART), Naïve Bayesian Tree (NBTree), Iterative Dichotomister Tree (Id3), J48 -a version of C4.5- and Sequential Minimal Optimization (SMO) algorithms with large user profile data. This study is aimed to find the best classification algorithm for user profiling process.Our simulation results show that, in general, the NBTree has the highest classification accuracy performance with the lowest error rate. On the other hand, we also found that the NBTree has one of the highest time requirements to build the classification model. Therefore, NBTree classification algorithm should be favoured over SMO, NB, IB1, J48, SimpleCART and Id3 classifiers in the personalization applications especially when the classification accuracy performance is important.","Niobium,
Classification algorithms,
Bayesian methods,
Iterative algorithms,
Classification tree analysis,
Regression tree analysis,
Intrusion detection,
Machine learning algorithms,
Support vector machines,
Support vector machine classification"
Leaving Flatland: Toward real-time 3D navigation,"We report our first experiences with Leaving Flatland, an exploratory project that studies the key challenges of closing the loop between autonomous perception and action on challenging terrain. We propose a comprehensive system for localization, mapping, and planning for the RHex mobile robot in fully 3D indoor and outdoor environments. This system integrates Visual Odometry-based localization with new techniques in real-time 3D mapping from stereo data. The motion planner uses a new decomposition approach to adapt existing 2D planning techniques to operate in 3D terrain. We test the map-building and motion-planning subsystems on real and synthetic data, and show that they have favorable computational performance for use in high-speed autonomous navigation.","Navigation,
Mobile robots,
Real time systems,
Terrain mapping,
Humans,
Artificial intelligence,
USA Councils,
Motion planning,
Robot sensing systems,
Robot vision systems"
Binary linear multicast network coding on acyclic networks: principles and applications in wireless communication networks,"Conventional linear multicast can be constructed on any acyclic network by increasing the order of the finite field to a sufficiently large amount over which the multicast is defined. In this paper, we first discuss the reciprocal theorem of the conventional linear multicast and design a linear multicast on any give acyclic network with constant finite field by extending the multicast dimension and relaxing the constraint on the information storage. In particular, we propose the binary linear multicast network coding and the linear multicast with binary coefficients. With the proposed method, the computation complexity for network coding at the intermediate nodes can be significantly reduced; therefore cheap network nodes can be deployed in a large scale due to their low cost for wireless communications. In addition, some applications of the proposed binary linear multicast network coding in wireless communication networks are illustrated and validated.",
Design and implementation of microcontroller based PWM technique for sine wave inverter,"A microcontroller based advanced technique of generating sine wave with minimized harmonics is implemented in this paper. The proposed technique “impulse-sine product” aims to design and implement a voltage regulated inverter with ripple free and glitch free output sine wave that can operate electronic devices efficiently. The output voltage of the inverter is regulated by a feedback loop using analog to digital protocol of PIC16f877 microcontroller. The design is essentially focused upon low power electronic appliances such as personal computers, chargers, television sets. The inverter output is regulated from 16-volt rms to 220-volt rms for a variation of load between 1-Ω to 180-Ω loads. The designed inverter provides a range of operation from 25 Watts to 250 Watts. The design is mathematically modeled which is simulated in Matlab, Proteus and finally the results are practically verified.","Pulse width modulation inverters,
Microcontrollers,
Voltage,
Feedback loop,
Protocols,
Low power electronics,
Home appliances,
Microcomputers,
TV,
Mathematical model"
Task Performance is Prioritized Over Energy Reduction,"The objective of this study was to characterize the temporal relationship between hand stiffness and task performance during adaptation to a brief contact task that required precision at the time of contact. The experiment required subjects to control the vertical position of a paddle on a computer display by grasping a robot's instrumented handle, with the goal of intercepting a virtual ball within 1 mm from the paddle center. A force transient was applied to the hand immediately after the ball-paddle impact to estimate the intrinsic hand impedance. There were two main results: 1) more trials were required for a brief contact task to find a low-energy strategy when compared with tasks that received feedback through the entire movement trajectory and 2) when the whole course of adaptation is long for brief contact tasks, viscoelastic forces were increased to achieve the task goal before the energy reduction initiated. Also, as the accuracy requirement was increased by changing the gain between handle and paddle motion through visual amplification, peak stiffness increased and occurred later, indicating that higher energy strategies are used for longer when the task's accuracy requirements were increased. These results indicated that task performance may be prioritized over energy reduction for a brief contact task.",
Performance evaluation of CNFET-based logic gates,"As the physical gate length of current devices is reduced to below 65 nm, effects (such as large parametric variations and increase in leakage current) have caused the I-V characteristics to be substantially depart from those commonly associated with traditional MOSFETs, thus impeding the efficient development and manufacturing of devices at deep submicro/nano scales. Carbon Nanotube Field Effect Transistors (CNFETs) have received widespread attention, as one of the promising technologies for replacing MOSFETs at the end of the Technology Roadmap. This paper presents a detailed simulation-based assessment of circuit performance of this technology and compares it to conventional MOSFETs; the designs of different logic gates and the full adder circuit are simulated under the same minimum gate length and different operational conditions. It is shown that the power-delay product (PDP) and the leakage power for the CNFET based gates are lower than the MOSFET based logic gates by 100 to 150 times, respectively. The CNFET based logic gates demonstrate good functionality even at a 0.3 V power supply (while MOSFET based gates fail at 0.5 V).","Logic gates,
MOSFETs,
Circuit simulation,
Logic devices,
Leakage current,
Impedance,
Manufacturing,
CNTFETs,
Circuit optimization,
Logic design"
A Virtual Environment for Teaching Social Skills: AViSSS,"For typical adolescents, developing appropriate social skills can be difficult. For those with Asperger's syndrome (AS), this task is much more difficult and can be a determining factor in school success. Recent studies indicate that students with AS, who are often visual learners, can learn social skills through computer-based tasks. To help individuals with AS deal with social challenges, researchers have developed the Animated Visual Supports for Social Skills (AViSSS) 3D virtual environment. In AViSSS, a rendering engine aids the rendering of animations, characters, objects, and environments. As students interact in the virtual environment, AViSSS presents them with various situations and possible responses. Each response delivers a different simulated result, along with an explanation of the choice.","Virtual environment,
Education,
Engines,
Educational institutions,
Animation,
Problem-solving,
Variable speed drives,
Second Life,
Logic,
Autism"
Improving Performance of Dynamic Programming via Parallelism and Locality on Multicore Architectures,"Dynamic programming (DP) is a popular technique which is used to solve combinatorial search and optimization problems. This paper focuses on one type of DP, which is called nonserial polyadic dynamic programming (NPDP). Owing to the nonuniform data dependencies of NPDP, it is difficult to exploit either parallelism or locality. Worse still, the emerging multi/many-core architectures with small on-chip memory make these issues more challenging. In this paper, we address the challenges of exploiting the fine grain parallelism and locality of NPDP on multicore architectures. We describe a latency-tolerant model and a percolation technique for programming on multicore architectures. On an algorithmic level, both parallelism and locality do benefit from a specific data dependence transformation of NPDP. Next, we propose a parallel pipelining algorithm by decomposing computation operators and percolating data through a memory hierarchy to create just-in-time locality. In order to predict the execution time, we formulate an analytical performance model of the parallel algorithm. The parallel pipelining algorithm achieves not only high scalability on the 160-core IBM Cyclops64, but portable performance as well, across the 8-core Sun Niagara and quad-cores Intel Clovertown.","Dynamic programming,
Multicore processing,
Parallel processing,
Computer architecture,
Pipeline processing,
Concurrent computing,
Performance analysis,
Algorithm design and analysis,
Analytical models,
Predictive models"
Linear optimization on modern GPUs,"Optimization algorithms are becoming increasingly more important in many areas, such as finance and engineering. Typically, real problems involve several hundreds of variables, and are subject to as many constraints. Several methods have been developed trying to reduce the theoretical time complexity. Nevertheless, when problems exceed reasonable sizes they end up being very computationally intensive. Heterogeneous systems composed by coupling commodity CPUs and GPUs are becoming relatively cheap, highly performing systems. Recent developments of GPGPU technologies give even more powerful control over them. In this paper, we show how we use a revised simplex algorithm for solving linear programming problems originally described by Dantzig for both our CPU and GPU implementations. Previously, this approach has showed not to scale beyond around 200 variables. However, by taking advantage of modern libraries such as ATLAS for matrix-matrix multiplication, and the NVIDIA CUDA programming library on recent GPUs, we show that we can scale to problem sizes up to at least 2000 variables in our experiments for both architectures. On the GPU, we also achieve an appreciable precision on large problems with thousands of variables and constraints while achieving between 2× and 2.5× speed-ups over the serial ATLAS-based CPU version. With further tuning of both the algorithm and its implementations, even better results should be achievable for both the CPU and GPU versions.","Linear programming,
Computer graphics,
Hardware,
Finance,
Libraries,
Optimization methods,
Design automation,
Information science,
Power engineering computing,
High performance computing"
Achieving controllability of plug-in electric vehicles,"The paper presents a conceptual framework for actively involving highly distributed loads in system control actions. The context for load control is established by first reviewing system control objectives, including economic dispatch, automatic generation control and spinning reserve. Also, prior work on load control is reviewed. The load control strategy discussed in the paper builds on the concept of a load aggregator. The aggregator acquires data from plug-in electric vehicle loads in its area, and builds a consolidated model that describes overall load availability. When control actions are required, the aggregator broadcasts a common message to all loads, with the response of individual loads dependent upon their interpretation of that message. The interface between the aggregator and the system controller should have a form that allows load control to be integrated seamlessly into the legacy system. The paper discusses the communications infrastructure required to support such a load control scheme.","Controllability,
Electric vehicles,
Load flow control,
Communication system control,
Control systems,
Power system reliability,
Power generation,
Mesh generation,
Power generation economics,
Costs"
Side Effect Prediction Using Cooperative Pathways,"Drugs and biological experiments are designed to affect a particular target gene or pathway. However, they might inadvertently activate other pathways and cause side effects.Because of the existence of complex cellular mechanisms responding to stimuli,it is difficult to detect the presence of such side effects.Therefore, identification of pathways that function together under identical conditions would greatly help in anticipating these side effects before conducting these experiments. We develop a novel method to enumerate ``cooperative pathways'' defined as pathways that functiontogether under identical conditionsby combining pathway networks withcomprehensive gene expression profiles.For finding cooperative pathways from whole pathways,we propose an efficient algorithm, CoopeRativE Pathway Enumerator (CREPE),which enumerates connected subpathways having common activate conditionsand selects combinations of the subpathways sharing the conditions.%We apply CREPE to a yeast stress dataset combined with the KEGG pathways.We observe that the starch and sucrose metabolism pathwaycooperates with the pyruvate metabolism under heat shock stresses. It cooperates with the tricarboxylic acid (TCA) cycle under the stationary phases.","Drugs,
Gene expression,
Bioinformatics,
Computer science,
Stress,
Biochemistry,
Itemsets,
Chemical compounds,
Biology,
Fungi"
Collaborative Scientific Workflows,"In recent years, a number of scientific workflow management systems (SWFMSs) have been developed to help domain scientists synergistically integrate distributed computations, datasets, and analysis tools to enable and accelerate scientific discoveries. As more scientific research projects become collaborative in nature, there is a compelling need of dedicated services to support collaborative scientific workflows on the Internet. This paper reviews the state of the art of the field of scientific workflows towards the support of collaborative scientific workflows, identifies critical research challenges, and presents our ongoing research work aiming to study how to create services supporting collaborative scientific workflows.","Collaborative work,
Cancer,
Web and internet services,
Data analysis,
Acceleration,
Large-scale systems,
Computer science,
USA Councils,
Workflow management software,
Distributed computing"
Dynamic Provision of Computing Resources from Grid Infrastructures and Cloud Providers,"Grid computing involves the ability to harness together the power of computing resources. In this paper we push forward this philosophy and show technologies enabling federation of grid infrastructures regardless of their interface. The aim is to provide the ability to build arbitrary complex grid infrastructure able to sustain the demand required by any given service. In this very same line, this paper also addresses mechanisms that potentially can be used to meet a given quality of service or satisfy peak demands this service may have. These mechanisms imply the elastic growth of the grid infrastructure making use of cloud providers, regardless of whether they are commercial, like Amazon EC2 and GoGrid, or scientific, like Globus Nimbus. Both these technologies of federation and dynamic provisioning are demonstrated in two experiments. The first is designed to show the feasibility of the federation solution by harnessing resources of the TeraGrid, EGEE and Open Science Grid infrastructures through a single point of entry. The second experiment is aimed to show the overheads caused in the process of offloading jobs to resources created in the cloud.","Cloud computing,
Grid computing,
Pervasive computing,
Quality of service,
Computer architecture,
Computer interfaces,
Standards development,
Computerized monitoring,
High performance computing,
Reservoirs"
Biocybernetic loop: From awareness to evolution,"Developing systems that support people in everyday life in a discrete and effective way is an ultimate goal of a new generation of technical systems. Physiological computing represents one means of creating a system to sense the user, analyse users' responses to system adaptation and respond dynamically. This process of adaptation is achieved by creating a biocybernetic loop that may operate on several, simultaneous timescales (minutes/hours/weeks/ months/years). In terms of architecture, it is argued that a “sense-analyse-react” system requires middleware with closed-loop control consisting of: (1) a tangible layer concerned with sensors and actuators, (2) a reflective layer containing a flexible representation of the user to guide system adaptation, and (3) an application layer representing application scenarios and the context for adaptation and evolution.","Cybernetics,
Biomedical monitoring,
Psychology,
Control systems,
Computerized monitoring,
Real time systems,
Human computer interaction,
Robotics and automation,
Biosensors,
Application software"
Collaborative filtering by sequential extraction of user-item clusters based on structural balancing approach,"This paper considers a new approach to user-item clustering for collaborative filtering problems that achieves personalized recommendation. When user-item relations are given by an alternative process, personalized recommendation is performed by finding user-item neighborhoods (co-clusters) from a rectangular relational data matrix, in which users and items have mutually positive relations. In the proposed approach, user-item clusters are extracted one by one in a sequential manner via a structural balancing technique, used in conjunction with the sequential fuzzy cluster extraction method.","Collaboration,
Data mining,
Predictive models,
Principal component analysis,
Clustering methods,
Information filtering,
Information filters,
Computer networks,
Relational databases,
Prototypes"
Spatial pyramids and two-layer stacking SVM classifiers for image categorization: A comparative study,"Recent research in image recognition has shown that combining multiple descriptors is a very useful way to improve classification performance. Furthermore, the use of spatial pyramids that compute descriptors at multiple spatial resolution levels generally increases the discriminative power of the descriptors. In this paper we focus on combination methods that combine multiple descriptors at multiple spatial resolution levels. A possible problem of the naive solution to create one large input vector for a machine learning classifier such as a support vector machine, is that the input vector becomes of very large dimensionality, which can increase problems of overfitting and hinder generalization performance. Therefore we propose the use of stacking support vector machines where at the first layer each support vector machine receives the input constructed by each single descriptor and is trained to compute the right output class. A second layer support vector machine is then used to combine the class probabilities of all trained first layer support vector models to learn the right output class given these reduced input vectors. We have performed experiments on 20 classes from the Caltech object database with 10 different single descriptors at 3 different resolutions. The results show that our 2-layer stacking approach outperforms the naive approach that combines all descriptors directly in a very large single input vector.",
Computer aided modelling of an interdigitated microelectrode array impedance biosensor for the detection of bacteria,"Electrostatic finite element modelling software and an ac equivalent circuit model have been used to investigate an impediometric microelectrode array biosensor for the detection of bacteria. The electrostatic model showed the capacitance of the biosensor to decrease with increasing numbers of bacteria trapped on the sensor's surface in a suspension of relatively high dielectric permittivity. Optimization of the model suggests that reducing the spatial wavelength of the biosensor's electrodes either through a decrease in electrode width or gap will improve the sensor's sensitivity. In addition, the model confirmed that the permittivity of the external medium had a significant effect on detection efficiency. Increased sensitivity in suspensions of lower relative dielectric permittivity was observed. The equivalent circuit model (ECM) was used to analyze the effect of high levels of immobilized bacteria at fixed signal frequencies (100 Hz and 1 MHz). It has been shown that the ECM discussed in this paper is able to successfully model the experimental data for the actual sensor in the low frequency ranges, allowing prediction of the sensor response and analysis of its performance. Overall, the modelling results obtained in the present paper are in general agreement with those from other published data and can be used in the development and optimization of impediometric biosensors for rapid and reliable detection of pathogenic bacteria.","Microelectrodes,
Impedance,
Biosensors,
Microorganisms,
Permittivity,
Electrostatics,
Equivalent circuits,
Dielectrics,
Electrodes,
Electrochemical machining"
Fractional order proportional integral (FOPI) and [proportional integral] (FO[PI]) controller designs for first order plus time delay (FOPTD) systems,"In this paper, systematic design schemes of fractional order proportional integral (FOPI) controller and fractional order [proportional integral] (FO[PI]) controller for the first order plus time delay (FOPTD) system are presented, respectively. For comparison between the fractional order and the integer order controllers, the integer order proportional integral derivative (IOPID) controller is also designed following the same proposed tuning specifications to achieve the robustness requirement. It is found that the three controllers designed by the proposed tuning methods not only make the system stable, but also improve the performance and robustness for the first order plus time delay (FOPTD) systems. Simulation results are presented to validate the proposed tuning schemes. Furthermore, from the simulation results, it can be seen that the FOPI controller outperforms the other two controllers.","Pi control,
Proportional control,
Control systems,
Delay effects,
Robust control,
Three-term control,
PD control,
Automatic control,
Transfer functions,
Industrial plants"
Google Scholar's ranking algorithm: The impact of citation counts (An empirical study),"Google Scholar is one of the major academic search engines but its ranking algorithm for academic articles is unknown. In a recent study we partly reverse-engineered the algorithm. This paper presents the results of our second study. While the previous study provided a broad overview, the current study focused on analyzing the correlation of an article's citation count and its ranking in Google Scholar. For this study, citation counts and rankings of 1,364,757 articles were analyzed. Some results of our first study were confirmed: Citation counts is the highest weighed factor in Google Scholar's ranking algorithm. Highly cited articles are found significantly more often in higher positions than articles that are cited less often. Therefore, Google Scholar seems to be more suitable for searching standard literature than for gems or articles by authors advancing a view different from the mainstream. However, interesting exceptions for some search queries occurred. In some cases no correlation existed; in others bizarre patterns were recognizable, suggesting that citation counts sometimes have no impact at all on articles' rankings.",
Multihop multi-channel scheduling for wireless control in WirelessHART networks,"The WirelessHART standard uses TDMA and channel hopping to control access to the network and to coordinate communication between network devices, in order to enhance reliability and to improve the throughput of the network. A problem in utilizing multiple channels is that current devices are usually equipped with a single transceiver. Thus, a node can only transmit or receive on one channel at a time. Moreover, contrary to today's wired control systems, if a single access point is used the communication becomes the bottle neck of the control system. Therefore this paper presents how one may schedule the WirelessHART communication using two access points. Furthermore the paper describes a scheduling algorithm managing a multihop multi-channel networked control system based on the WirelessHART standard. A simulation example of a multihop multi-channel network is also shown, using the fixed packet lost utility of the Matlab/Simulink-based tool TrueTime.","Spread spectrum communication,
Communication system control,
Control systems,
Communication standards,
Time division multiple access,
Telecommunication network reliability,
Throughput,
Transceivers,
Neck,
Scheduling algorithm"
Shear Modulus Decomposition Algorithm in Magnetic Resonance Elastography,"Magnetic resonance elastography (MRE) is an imaging modality capable of visualizing the elastic properties of an object using magnetic resonance imaging (MRI) measurements of transverse acoustic strain waves induced in the object by a harmonically oscillating mechanical vibration. Various algorithms have been designed to determine the mechanical properties of the object under the assumptions of linear elasticity, isotropic and local homogeneity. One of the challenging problems in MRE is to reduce the noise effects and to maintain contrast in the reconstructed shear modulus images. In this paper, we propose a new algorithm designed to reduce the degree of noise amplification in the reconstructed shear modulus images without the assumption of local homogeneity. Investigating the relation between the measured displacement data and the stress wave vector, the proposed algorithm uses an iterative reconstruction formula based on a decomposition of the stress wave vector. Numerical simulation experiments and real experiments with agarose gel phantoms and human liver data demonstrate that the proposed algorithm is more robust to noise compared to standard inversion algorithms and stably determines the shear modulus.","Magnetic resonance,
Iterative algorithms,
Magnetic resonance imaging,
Image reconstruction,
Mechanical factors,
Strain measurement,
Vibration measurement,
Algorithm design and analysis,
Noise reduction,
Stress"
Fast torque control system of PMSM based on model predictive control,"This paper describes a fast torque control system of permanent magnet synchronous motor (PMSM) based on model predictive control (MPC). This torque controller selects directly one of the voltage vector of voltage source PWM inverter considering voltage saturation explicitly. To obtain the fast torque response at the transient state and the stable current response at the steady state, the problem with selecting the voltage vector to output is formulated based on MPC. In this paper, real-time implementation method using a look-up table, which is designed beforehand, is discussed and the experimental results are shown.","Torque control,
Predictive models,
Predictive control,
Voltage control,
Permanent magnet motors,
Pulse width modulation inverters,
Table lookup,
Steady-state,
Switches,
Telephony"
Vision based hand gesture recognition using finite state machines and fuzzy logic,This paper proposes a finite state and fuzzy logic based approach to hand gesture learning and recognition. The location of 2D image positions of the hands of the user is obtained by Edge detection and vector extraction. These are used to identify the hand posture as well as the center of the hand of the user. We first learn the spatial information without data segmentation and alignment. Then the data is grouped into clusters that are associated with information for temporal alignment. The points thus obtained are clustered using Fuzzy c-mean clustering algorithm. These clusters of hand posture further determine the states of the Finite State Machine(s) through which the succeeding gesture has to be matched. To build a Gesture Recognizer (GR) the temporal information is integrated. Each hand gesture is defined to be an ordered sequence of states in spatial-temporal space i.e. FSM corresponding to it. The number of states/clusters in an FSM represents a trade-off between the accuracy of gesture recognizer and the amount of spatial-temporal data it stores.,"Automata,
Fuzzy logic,
Speech,
Handicapped aids,
Image edge detection,
Image segmentation,
Humans,
Eyes,
Tracking,
Thumb"
How Much Can The Internet Be Greened?,"The power consumption of the Internet is becoming more and more a key issue, and several projects are studying how to reduce its energy consumption. In this paper, we provide a first evaluation of the amount of redundant resources (nodes and links) that can be powered off from a network topology to reduce power consumption. We first formulate a theoretical evaluation that exploits random graph theory to estimate the fraction of devices that can be removed from the topology still guaranteeing connectivity. Then we compare theoretical results with simulation results using realistic Internet topologies. Results, although preliminary, show that large energy savings can be achieved by accurately turning off nodes and links, e.g., during off-peak time. We show also that the non-cooperative design of the current Internet severely impacts the possible energy saving, suggesting that a cooperative approach can be investigated further.","Internet,
Text categorization,
Clustering algorithms,
Impurities,
Statistics,
Computer science,
Testing,
Data mining,
Conferences,
Availability"
Evaluating Test-Driven Development in an Industry-Sponsored Capstone Project,"Test-Driven Development (TDD) is an agile development process wherein automated tests are created before production code is designed or constructed in short, rapid iterations. This paper discusses an experiment conducted with undergraduate students in a year-long software engineering capstone course. In this course the students designed, implemented, deployed, and maintained a software system to meet the requirements of an industry sponsor who served as the customer. The course followed an incremental process in which features were added incrementally under the direction of the industry sponsor and the professor. The fourteen students observed in the study were divided into three teams. Among the three teams were two experimental groups. One group consisted of two teams that applied a Test-First (TDD) methodology, while a control group applied a traditional Test-Last methodology. Unlike Test-First, the tests in Test-Last are written after the design and construction of the production code being tested. Results from this experiment differ from many previous studies. In particular, the Test-Last team was actually more productive and wrote more tests than their Test-First counterparts. Anecdotal evidence suggests that factors other than development approach such as individual ambition and team motivation may have more affect than the development approach applied. Although more students indicated a preference for the Test-First approach, concerns regarding learning and applying TDD with unfamiliar technologies are noted.","Software testing,
Production,
Automatic testing,
Computer industry,
Software quality,
Software engineering,
Software design,
Programming profession,
Information technology,
Software maintenance"
A cooperative MAC protocol with virtual-antenna array support in a multi-AP WLAN system,"In this paper we propose a cooperative-aware medium access control (MAC) protocol in the newly emerged multiple access point (MAP) WLAN system, where each user can associate with multiple APs. Leveraging the multiple association feature, the spectrum efficient virtual-antenna array (VA) cooperative strategy can be utilized. However, the APs selected for VA transmission cannot serve packet transmission individually, which leads to spatial reuse inefficiency. To balance the tradeoff between spatial reuse efficiency and cooperative gain, an interference model that describes interference among cooperative transmissions should be first established. In this paper, a virtual link model is proposed, where each virtual link is composed of one end user, one combination of VA APs and one data rate to represent one cooperative transmission. Based on this model, a multi-cell virtual link scheduling problem is formulated to achieve optimal system performance. Combined with local clique searching procedure and physical carrier sensing adaptation scheme, the dual decomposition of the scheduling problem can distributively achieve the near optimal performance. Finally a VA-based cooperative MAC (V-MAC) protocol is proposed to implement the cooperative scheduling scheme. Simulation result shows V-MAC significantly improves the system throughput meanwhile guarantees the system fairness.","Media Access Protocol,
Wireless LAN,
Phased arrays,
Access protocols,
Interference,
System performance,
Relays,
Antenna arrays,
Throughput,
Wireless networks"
Detection of forgery in paintings using supervised learning,"This paper examines whether machine learning and image analysis tools can be used to assist art experts in the authentication of unknown or disputed paintings. Recent work on this topic has presented some promising initial results. Our reexamination of some of these recently successful experiments shows that variations in image clarity in the experimental datasets were correlated with authenticity, and may have acted as a confounding factor, artificially improving the results. To determine the extent of this factor's influence on previous results, we provide a new ¿ground truth¿ data set in which originals and copies are known and image acquisition conditions are uniform. Multiple previously-successful methods are found ineffective on this new confounding-factor-free dataset, but we demonstrate that supervised machine learning on features derived from hidden-Markov-tree-modeling of the paintings' wavelet coefficients has the potential to distinguish copies from originals in the new dataset.","Forgery,
Painting,
Supervised learning,
Wavelet coefficients,
Image analysis,
Art,
Machine learning,
Image color analysis,
Hidden Markov models,
Statistics"
T-hive : Vibrotactile interface presenting spatial information on handle surface,"Many recent studies have explored the use of tactile cues, however they were confined to the unilateral display device. Although lots of bilateral haptic devices have been developed to provide a guiding force on an input handle, however, a vibrotactile stimuli has not been tried to present directional information on the handle. This research introduces an attempt to combine a tactile display with an input device. A new 6DOF bilateral haptic device, which provides a spatial sensation on the handle using vibrotactile display, is proposed in this research. The sphere-shape handle is specially designed to be covered with several pieces of vibrating panels. When a specific panel is activated, the user perceives the spatial location of the vibrotactile stimulus during an input operation. This paper introduces the design of the proposed device, including the selection guide of the dimension, location, and number of vibrotactile panels. The method for combination of vibrotactile stimulus and the way to achieve fine resolution with small number of tactors are discussed. Experimental results show that the users can reliably perceive the directional information using the proposed device. An application for teleoperation of a robot proves the effectiveness and the usefulness of the proposed bilateral device.","Displays,
Haptic interfaces,
Skin,
Torso,
Computer interfaces,
Cognitive robotics,
Marine technology,
Robotics and automation,
Navigation,
Testing"
An FPGA wave union TDC for time-of-flight applications,"An 18-channel time-of-flight (TOF) grade time-to-digit converter (TDC) has been implemented in a low cost FPGA device. The TDC has the following unique features. (1) The time recording structures of the TDC is based on the “wave union TDC” we developed in our previous work. A leading edge of the input hit launches a bit pattern, or wave union into the delay chain-register array structure which yields two usable measurements. The two measurements effectively sub-divide timing bins for each other especially the “ultra-wide bins” caused by the FPGA logic array block (LAB) structure and improves measurement precision both in terms of maximum bin width and RMS resolution. A coarser measurement on input signal trailing edge is also provided for time-over-threshold (TOT) applications. (2) The TDC supports advanced timing reference distribution schemes that are superior to conventional common start/stop schemes. The TDC has 16 regular measurement channels plus two channels for timing reference. The timing reference is established with multiple measurements rather than single shot common start/stop. An advanced scheme, the mean-timing approach even eliminates needs of high quality timing distribution media. (3) The ASIC-like encapsulation of the FPGA TDC significantly shorten the learning curve for potential users while maintain certain flexibility for various applications. Necessary digital post-processing functions including semi-continuous automatic calibration, data buffer, data link jam prevention logic etc. are integrated into the firmware to provide a turn-key solution for users.","Field programmable gate arrays,
Timing,
Logic arrays,
Costs,
Propagation delay,
Signal resolution,
Encapsulation,
Calibration,
Computer buffers,
Automatic logic units"
A Novel Split and Merge EM Algorithm for Gaussian Mixture Model,"As an extremely powerful probability model, Gaussian mixture model (GMM) has been widely used in fields of pattern recognition, information processing and data mining. If the number of the Gaussians in the mixture is pre-known, the well-known Expectation-Maximization (EM) algorithm could be used to estimate the parameters in the Gaussian mixture model. However, in many practical applications, the number of the components is not known.Then the Gaussian mixture modeling becomes a compound problem of the determination of number of Gaussian components and the parameter estimation for the mixture, which is rather difficult. In this paper, we propose a split and merge EM (SMEM) algorithm to decide the number of the components, which is referred to the model selection for the mixture. Based on the minimum description length (MDL) criterion, the proposed SMEM algorithm can avoid the local optimum drawback of the usual EM algorithm and determine the number of components in the Gaussian mixture model automatically. By splitting and merging the uncorrect components, the algorithm can converge to the maximization of the MDL criterion function and get a better parameter estimation of the Gaussian mixture with correct number of components in the mixture. It is demonstrated well by the experiments that the proposed split and merge EM algorithm can make both parameter learning and model selection efficiently for Gaussian mixture.","Parameter estimation,
Mathematical model,
Inference algorithms,
Probability,
Information processing,
Clustering algorithms,
Statistics,
Computers,
Information science,
Pattern recognition"
A Simple Cache Partitioning Approach in a Virtualized Environment,"Virtualization is often used in systems for the purpose of offering isolation among applications running in separate virtual machines (VM). Current virtual machine monitors (VMMs) have done a decent job in resource isolation in memory, CPU and I/O devices. However, when looking further into the usage of lower-level shared cache, we notice that one virtual machine’s cache behavior may interfere with another’s due to the uncontrolled cache sharing. In this situation, performance isolation cannot be guaranteed. This paper presents a cache partitioning approach which can be implemented in the VMM. We have implemented this mechanism in Xen VMM using the page coloring technique traditionally applied to the OS. Our VMM-based implementation is fully transparent to the guest OSes. It thus shows the advantages of simplicity and flexibility. Our evaluation shows that our cache partitioning method can work efficiently and improve the performance of co-scheduled applications running within different VMs. In the concurrent workloads selected from the SPEC CPU 2006 benchmarks, our technique achieves a performance improvement by up to 19% for the most sensitive workloads","Virtual machining,
Virtual manufacturing,
Voice mail,
Operating systems,
Virtual machine monitors,
Computer science,
Distributed processing,
Application software,
Application virtualization,
USA Councils"
Multi-a(ge)nt Graph Patrolling and Partitioning,"Abstract--- We introduce a novel multi agent patrolling algorithm inspired by the behavior of gas filled balloons. Very low capability ant-like agents are considered with the task of patrolling an unknown area modeled as a graph. While executing the proposed algorithm, the agents dynamically partition the graph between them using simple local interactions, every agent assuming the responsibility for patrolling his subgraph. Balanced graph partition is an emergent behavior due to the local interactions between the agents in the swarm. Extensive simulations on various graphs (environments) showed that the average time to reach a balanced partition is linear with the graph size. The simulations yielded a convincing argument for conjecturing that if the graph being patrolled contains a balanced partition, the agents will find it. However, we could not prove this. Nevertheless, we have proved that if a balanced partition is reached, the maximum time lag between two successive visits to any vertex using the proposed strategy is at most twice the optimal so the patrol quality is at least half the optimal. In case of weighted graphs the patrol quality is at least lmin/2lmax of the optimal where lmax(lmin) is the longest (shortest) edge in the graph.","Partitioning algorithms,
Intelligent agent,
Unmanned aerial vehicles,
Satellites,
Circuits,
Conferences,
Computer science,
Heuristic algorithms,
Particle swarm optimization,
Time measurement"
Significance of Strike Model in Circuit-Level Prediction of Charge Sharing Upsets,"When evaluating sub-100 nm circuits for hardness to Single Event Transients (SETs), the choice of strike model is shown to have a notable effect upon observed upsets. A method utilizing distributed charges to model strikes to adjacent devices is illustrated and utilized to compare the effect of strike kernel models in such Charge Sharing SETS (CSSETS). Bias-dependent models are shown to more accurately predict expected physical observations and Technology Computer Aided Design (TCAD) simulation, especially when such charge-sharing upsets must be considered.","Predictive models,
Circuit simulation,
SPICE,
Kernel,
Integrated circuit modeling,
Context modeling,
Physics computing,
Computational modeling,
Computer simulation,
Radiation hardening"
A Comparative Study of Medical Data Classification Methods Based on Decision Tree and Bagging Algorithms,"Medical data mining has been a popular data mining topic of late. Especially, diagnosing of the heart disease is one of the important issue and many researchers investigated to develop intelligent medical decision support systems to help the physicians. In this paper, we propose the use of decision tree C4.5 algorithm, bagging with decision tree C4.5 algorithm and bagging with Naïve Bayes algorithm to identify the heart disease of a patient and compare the effectiveness, correction rate among them. The data we study is collected from patients with coronary artery disease.","Classification tree analysis,
Decision trees,
Bagging,
Cardiac disease,
Coronary arteriosclerosis,
Medical diagnostic imaging,
Computer science,
Data mining,
Heart,
Databases"
Controlling an automated wheelchair via joystick/head-joystick supported by smart driving assistance,"With this work, we encourage the application of smart driving assistance algorithms to support the operator of an automated wheelchair in complex navigational situations. On the basis of an empirical study in which eight untrained subjects performed a given course using a conventional joystick and a proportional head-joystick respectively, we are able to prove benefits resulting from the application of a newly developed driving assistance module. Altering the translational and rotational velocities in situations where an obstacle blocks the user-commanded way, the driving assistance module significantly improves driver-performance by preventing all collisions along the way.","Automatic control,
Wheelchairs,
Wheels,
Navigation,
Intelligent sensors,
Medical services,
Foot,
Rehabilitation robotics,
Data processing,
Mobile robots"
Computing bounds for fault tolerance using formal techniques,"Continuously shrinking feature sizes result in an increasing susceptibility of circuits to transient faults, e.g. due to environmental radiation. Approaches to implement fault tolerance are known. But assessing the fault tolerance of a given circuit is a tough problem. Here, we propose the use of formal methods to assess the robustness of a digital circuit with respect to transient faults. Our formal model uses a fixed bound in time to cope with the complexity of the underlying sequential equivalence check. The result is a lower and an upper bound on the robustness. The underlying algorithm and techniques to improve the efficiency are presented. In experiments the method is evaluated on circuits with different fault detection mechanisms.","Fault tolerance,
Circuit faults,
Robustness,
Reachability analysis,
Upper bound,
Formal verification,
Computer science,
Digital circuits,
Electrical fault detection,
Circuit testing"
Tuning On-Air Signatures for Balancing Performance and Confidentiality,"In this paper, we investigate the trade off between performance and confidentiality in signature-based air indexing schemes for wireless data broadcast. Two metrics, namely, false drop probability and false guess probability, are defined to quantify the filtering efficiency and confidentiality loss of a signature scheme. Our analysis reveals that false drop probability and false guess probability share a similar trend as the tuning parameters of a signature scheme change and it is impossible to achieve a low false drop probability and a high false guess probability simultaneously. In order to balance the performance and confidentiality, we perform an analysis to provide a guidance for parameter settings of the signature schemes to meet different system requirements. In addition, we propose the jump pointer technique and the XOR signature scheme to further improve the performance and confidentiality. A comprehensive simulation has been conducted to validate our findings.","Indexing,
Filtering,
Satellite broadcasting,
Cryptography,
Performance analysis,
Watches,
Videos,
WiMAX,
Portable computers,
Games"
Proposal of a method to detect black hole attack in MANET,"Black hole attack is a serious threat in a mobile ad hoc network (MANET). In this attack, a malicious node injects a faked Route Reply message to deceive the source node so that the source node establishes a route to the malicious node and sends all the data packets to the malicious node. Every conventional method to detect such an attack has a defect of rather high rate of misjudgment in the detection. In order to overcome this defect, we propose a new detection method based on checking the sequence number in the Route Reply message by making use of a new message originated by the destination node and also by monitoring the messages relayed by the intermediate nodes in the route. Computer simulation results demonstrate that our method has a feature of much lower false positive and negative rates in detecting any number of malicious nodes than the conventional methods.","Proposals,
Mobile ad hoc networks,
Tin,
Relays,
Unicast,
Routing protocols,
Broadcasting,
Computerized monitoring,
Computer simulation,
Network topology"
Improvement of accuracy and speed of a commercial AFM using positive position feedback control,"The atomic force microscope (AFM) is a device capable of generating topographic images of sample surfaces with extremely high resolutions down to the atomic level. It is also being used in applications that involve manipulation of matter at a nanoscale. Early AFMs were operated in open loop. As a result, they were susceptible to piezoelectric creep, thermal drift, hysteresis nonlinearity and scan-induced vibration. These effects tend to distort the generated image. The distortions are often minimized by limiting the scanning speed and range of the AFMs. Recently a new generation of AFMs has emerged that utilizes position sensors to measure displacements of the scanner in three dimensions. These AFMs are equipped with feedback loops that work to minimize the adverse effects of hysteresis, piezoelectric creep and thermal drift on the obtained image using standard PI controllers. These feedback controllers are often not designed to deal with the highly resonant nature of an AFM's scanner, nor with the cross-coupling between various axes. In this paper we illustrate the drastic improvement in accuracy and imaging speed that can be obtained by proper design of a feedback controller. Such controllers can be incorporated into most modern AFMs with minimal effort since they can be implemented in software with the existing hardware.","Feedback control,
Atomic force microscopy,
Image generation,
Creep,
Hysteresis,
Adaptive control,
Force control,
Surface topography,
Image resolution,
Distortion measurement"
Inferring a probability distribution function for the pose of a sensor network using a mobile robot,In this paper we present an approach for localizing a sensor network augmented with a mobile robot which is capable of providing inter-sensor pose estimates through its odometry measurements. We present a stochastic algorithm that samples efficiently from the probability distribution for the pose of the sensor network by employing Rao-Blackwellization and a proposal scheme which exploits the sequential nature of odometry measurements. Our algorithm automatically tunes itself to the problem instance and includes a principled stopping mechanism based on convergence analysis. We demonstrate the favourable performance of our approach compared to that of established methods via simulations and experiments on hardware.,"Probability distribution,
Mobile robots,
Intelligent sensors,
Proposals,
Distributed computing,
Computer networks,
Simultaneous localization and mapping,
Convergence,
Motion estimation,
Robotics and automation"
OCSA: An algorithm for burst mapping in IEEE 802.16e mobile WiMAX networks,"Most of IEEE 802.16e resource allocation proposals only focus on how to allocate the resources to meet QoS parameters such as throughput, delay, and delay-jitter. As described in the standard, the mapping from the allocation into downlink subframe for each burst needs to be in a rectangular shape. The rectangular mapping problem is a variation of a bin or strip packing problem, which is known to be NP complete. However, the mapping decision needs to be made within a few milliseconds for each Mobile WiMAX frame. In this paper, we introduce a heuristic algorithm, called One Column Striping with non-increasing Area first mapping (OCSA). The algorithm is fast and simple to implement and minimizes the unused slots in the frame.","WiMAX,
Downlink,
Resource management,
Frequency conversion,
Bandwidth,
Quality of service,
Mobile communication,
Mobile computing,
Computer science,
Drives"
Developmental Stereo: Emergence of Disparity Preference in Models of the Visual Cortex,"How our brains develop disparity tuned V1 and V2 cells and then integrate binocular disparity into 3-D perception of the visual world is still largely a mystery. Moreover, computational models that take into account the role of the 6-layer architecture of the laminar cortex and temporal aspects of visual stimuli are elusive for stereo. In this paper, we present cortex-inspired computational models that simulate the development of stereo receptive fields, and use developed disparity sensitive neurons to estimate binocular disparity. Not only do the results show that the use of top-down signals in the form of supervision or temporal context greatly improves the performance of the networks, but also results in biologically compatible cortical maps-the representation of disparity selectivity is grouped, and changes gradually along the cortex. To our knowledge, this work is the first neuromorphic, end-to-end model of laminar cortex that integrates temporal context to develop internal representation, and generates accurate motor actions in the challenging problem of detecting disparity in binocular natural images. The networks reach a subpixel average error in regression, and 0.90 success rate in classification, given limited resources.",
Extending service model to build an effective service composition framework for cyber-physical systems,"Cyber-Physical Systems (CPSs) are combinations of physical entities controlled by software systems to accomplish specified tasks under stringent real-time and physical entity constraints. As more and more physical entities are equipped with embedded computers, they are becoming more and more intelligent. However, the problem of effectively composing the services provided by cyber and physical entities to achieve specific goals still remains a challenge. Traditional service-oriented models and composition techniques are insufficient for CPS. In this paper, we present a novel Physical-Entity (PE) service-oriented model to address the problem, including the concepts of PE-ontology and PE-SOA specifications. Based on the model, we develop a two-level compositional reasoning approach, which divides the process into abstract and physical levels to expedite the composition process. With the assistance of the PE-ontology and PE-SOA, abstract level reasoning is performed by hiding the physical level details. This separation greatly reduces the search space for both levels through a divide-and-conquer technique. The model and the composition approach are illustrated using a simplified emergency response case study system.","Service oriented architecture,
Physics computing,
Knowledge engineering,
Sensor fusion,
Intelligent sensors,
Concrete,
Computer science,
Control systems,
Software systems,
Real time systems"
PSO based neural network for time series forecasting,"Artificial neural networks are being widely used for time series forecasting. In recent years much effort has been made for the development of particle swarm algorithm for the optimization of neural networks. In this paper, the performance of two variants of particle swarm optimization algorithm (Trelea I and Trelea II) for training neural network has been examined with a real data for financial time series forecasting. Results clearly indicated the superiority of swarm based algorithms over the standard backpropagation training algorithm with respect to common performance measures across three forecasting horizons. In particular, with the Trelea II trained model, we obtained 92.48 %, 56.64 %, and 44.66 % decrease in terms of MSE over the standard back-propagation trained neural network for 10 days, 30 days and 60 days ahead forecasts respectively.","Neural networks,
Artificial neural networks,
Predictive models,
Signal processing algorithms,
Particle swarm optimization,
Backpropagation algorithms,
Genetic algorithms,
Computer networks,
Computer architecture,
Computational intelligence"
Linear embeddings in non-rigid structure from motion,"This paper proposes a method to recover the embedding of the possible shapes assumed by a deforming nonrigid object by comparing triplets of frames from an orthographic video sequence. We assume that we are given features tracked with no occlusions and no outliers but possible noise, an orthographic camera and that any 3D shape of a deforming object is a linear combination of several canonical shapes. By exploiting any repetition in the object motion and defining an ordering between triplets of frames in a generalized non-metric multi-dimensional scaling framework, our approach recovers the shape coefficients of the linear combination, independently from other structure and motion parameters. From this point, a good estimate of the remaining unknowns is obtained for a final optimization to perform full non-rigid structure from motion. Results are presented on synthetic and real image sequences and our method is found to perform better than current state of the art.","Utility programs,
Nonlinear distortion,
Cameras,
Surface reconstruction,
Optical reflection,
Image reconstruction,
Optical refraction,
Geometry,
Optical distortion,
Robustness"
A Distributed boundary detection algorithm for multi-robot systems,"We describe a distributed boundary detection algorithm suitable for use on multi-robot systems with dynamic network topologies. We assume that each robot has access to its local network geometry, which is the combination of a robot's network connectivity and the positions of its neighbors measured relative to itself. Our algorithm uses this information to classify robots as boundary or interior in one communications round, which is fast enough for rapidly changing networks. We use the local boundary classifications to create a robust boundary subgraph, and to determine if the boundary is an interior void or the exterior boundary. A proof of the key property of the boundary detection algorithm is provided, and all the algorithms are extensively tested on a swarm of 25#x2013;35 robots in rapidly changing network topologies.","Detection algorithms,
Multirobot systems,
Robot kinematics,
Robot sensing systems,
Computational geometry,
Distributed algorithms,
Position measurement,
Routing,
Network topology,
Solid modeling"
An empirical evaluation of the influence of human personality on exploratory software testing,"Exploratory testing is a kind of testing in which no documentation or scripting is used. Personality characteristics including intelligence and extroversion play a significant role in exploratory testing. The main hypothesis of this research is ‘personality traits influence exploratory testing’. This research hypothesis has been unfolded into six sub-hypotheses which have been tested individually. To test the research hypothesis, an Exploratory Testing Aptitude Test (ETAT) was designed. The subjects were also given Pre-ETAT, Standard Progressive Matrices (SPM), Myers-Briggs Type Indicator, and Post-ETAT tests. The results were analyzed using Statistical Package of Social Sciences (SPSS). The results indicate that there is a positive correlation between human personality types and the ability of testers in exploratory testing. We conclude that people having extrovert personality types are good exploratory testers. Moreover, good exploratory testers have higher IQ level which is directly proportional to result of ETAT.","Humans,
Software testing,
Materials testing,
System testing,
Computer science,
Psychology,
Risk management,
Performance evaluation,
Personnel,
Documentation"
On the Use of Textural Features for Writer Identification in Old Handwritten Music Scores,"Writer identification consists in determining the writer of a piece of handwriting from a set of writers. In this paper we present a system for writer identification in old handwritten music scores which uses only music notation to determine the author. The steps of the proposed system are the following. First of all, the music sheet is preprocessed for obtaining a music score without the staff lines. Afterwards, four different methods for generating texture images from music symbols are applied. Every approach uses a different spatial variation when combining the music symbols to generate the textures. Finally, Gabor filters and Grey-scale Co-ocurrence matrices are used to obtain the features. The classification is performed using a k-NN classifier based on Euclidean distance. The proposed method has been tested on a database of old music scores from the 17th to 19th centuries, achieving encouraging identification rates.","Feature extraction,
Text analysis,
Image generation,
Handwriting recognition,
Computer science,
Data mining,
Image recognition,
Computer vision,
Mathematics,
Gabor filters"
CDMA and FSCW surface acoustic wave temperature sensors for wireless operation at high temperatures,"This paper reports on the successful operation of surface acoustic wave (SAW) devices at high temperature, including wireless measurements of SAW devices up to 750°C. Two types of SAW devices and corresponding systems were investigated: (1) 15-bit code-division multiple-access (CDMA) SAW tags, interrogated using a device-specific direct sequence spread spectrum (DSSS) binary phase shift key (BPSK) code; and (2) multi-track reflective delay lines, interrogated using a frequency-stepped continuous-wave (FSCW) radar signal. The SAW devices used in this work were fabricated on langasite (LGS, La3Ga5SiO14) using Pt/Rh/ZrO2 electrodes, which can withstand temperatures up to 1000°C. A FEM/BEM simulation package using both the Pt/Rh/ZrO2 thin film and LGS properties was used to simulate the interdigital transducers (IDTs) used in the fabricated structures. The two classes of SAW sensors were first directly wired and characterized at high temperature. The FSCW system was then used to test the wireless operation of LGS SAW devices up to 750°C.","Temperature sensors,
Multiaccess communication,
Power capacitors,
Surface acoustic waves,
Acoustic waves,
Wireless sensor networks,
Surface acoustic wave devices,
Temperature measurement,
Spread spectrum radar,
Acoustic measurements"
Distributed Key Generation for the Internet,"Although distributed key generation (DKG) has been studied for some time, it has never been examined outside of the synchronous setting. We present the first realistic DKG architecture for use over the Internet. We propose a practical system model and define an efficient verifiable secret sharing scheme in it. We observe the necessity of Byzantine agreement for asynchronous DKG and analyze the difficulty of using a randomized protocol for it. Using our verifiable secret sharing scheme and a leader-based agreement protocol, we then design a DKG protocol for public-key cryptography. Finally, along with traditional proactive security, we also introduce group modification primitives in our system.","Internet,
Security,
Variable structure systems,
Cryptographic protocols,
Public key cryptography,
Synchronous generators,
Public key,
Distributed computing,
Computer science,
Computer architecture"
Prediction of Effective Permittivity of Diphasic Dielectrics as a Function of Frequency,"An analytical model based on an equivalent impedance circuit for effective permittivity of a composite dielectric as a function of frequency with complex-shaped inclusions is presented. The geometry of the capacitor containing this composite dielectric is discretized into partial impedance elements, the total equivalent impedance is calculated, and the effective permittivity of the composite dielectric is obtained from this equivalent impedance. An example application using this method is given for an individual cell of a diphasic dielectric consisting of a high-permittivity spherical inclusion enclosed in a low-permittivity parallelepiped. The capacitance and resistance for individual discretized elements in the composite cell are modeled as a function of an inclusion radius. The proposed approach is then extended to a periodic three-dimensional structure comprised of multiple individual cells. The equivalent impedance model is valid for both static and alternating applied electric fields, over the entire range of volume fraction of inclusions. The equivalent impedance model has a few advantages over existing effective medium theories, including no limitations on the shape of inclusions or their separation distance.",
Image matching in large scale indoor environment,"In this paper, we propose a data driven approach to first-person vision. We propose a novel image matching algorithm, named Re-Search, that is designed to cope with self-repetitive structures and confusing patterns in the indoor environment. This algorithm uses state-of-art image search techniques, and it matches a query image with a two-pass strategy. In the first pass, a conventional image search algorithm is used to search for a small number of images that are most similar to the query image. In the second pass, the retrieval results from the first step are used to discover features that are more distinctive in the local context. We demonstrate and evaluate the Re-Search algorithm in the context of indoor localization, with the illustration of potential applications in object pop-out and data-driven zoom-in.","Image matching,
Large-scale systems,
Indoor environments,
Image retrieval,
Machine vision,
Cameras,
Image databases,
Computer science,
Algorithm design and analysis,
Impedance matching"
An EHD gas pump utilizing a ring/needle electrode,"An electrohydrodynamic (EHD) gas pump, utilizing a ring/needle electrode, has been proposed and the effect of the ring electrode to the flow velocity and yield of flow generation of the EHD gas pump was investigated. The needle/ring is used as a corona discharge electrode and the mesh is used as the ion collecting electrode. It was observed that the proposed type of EHD gas pump can generate a higher flow velocity and yield of flow generation as compared with that from the same EHD gas pump without the ring electrode. As a result, a maximum flow velocity of VW=4.54 m/s from the proposed EHD gas pump can be obtained at V=15.0 kV, which is 1.2 times higher than that of VW =3.82 m/s of the same needle-to-mesh type EHD gas pump without the ring electrode. With the ring electrode, the yield of flow generation of YW=46.15 m/Ws can be obtained at V=11 kV, which is 2.5 times higher than that of the design without the ring electrode of YW=18.41 m/Ws. These enhancements, however, may be due to the effect of the ring electrode installed near the needle electrode.","Needles,
Electrodes,
Corona,
Fluid flow,
Air gaps,
Moon,
Computer science,
Electrohydrodynamics,
Wind speed,
Physics"
"YinYang Bipolar Relativity - A Unifying Theory of Nature, Agents and Life Science","YinYang bipolar relativity and a real world bipolar string theory are introduced as a unification of nature, agents, and life science. Nine axioms and 16 conjectures are posted for microscopic and macroscopic agent interaction, regulation, coordination, and exploratory scientific discovery in physical and social sciences. It is concluded that bipolar relativity constitutes an equilibrium-based axiomatization of physics – a partial but most general solution to Hilbert’s Problem 6.","Physics,
Microscopy,
Multidimensional systems,
Bioinformatics,
Systems biology,
Intelligent systems,
Intelligent agent,
Biology computing,
Computer science,
Educational institutions"
Emotion classification based on gamma-band EEG,"In this paper, we use EEG signals to classify two emotions-happiness and sadness. These emotions are evoked by showing subjects pictures of smile and cry facial expressions. We propose a frequency band searching method to choose an optimal band into which the recorded EEG signal is filtered. We use common spatial patterns (CSP) and linear-SVM to classify these two emotions. To investigate the time resolution of classification, we explore two kinds of trials with lengths of 3s and 1s. Classification accuracies of 93.5% ± 6.7% and 93.0%±6.2% are achieved on 10 subjects for 3s-trials and 1s-trials, respectively. Our experimental results indicate that the gamma band (roughly 30–100 Hz) is suitable for EEG-based emotion classification.","Electroencephalography,
Frequency,
Neuroscience,
Psychology,
Humans,
Computer displays,
Emotion recognition,
USA Councils,
Spatial resolution,
Signal resolution"
Multiservice on-demand routing in LEO satellite networks,"In this paper, a distributed on-demand routing protocol for Low Earth Orbit (LEO) satellite systems, named multiservice on-demand routing (MOR), is proposed and evaluated. The proposed protocol adjusts the routing procedure to the QoS requirements of different traffic classes. The performance of the MOR protocol is compared to the unique proposal for traffic class dependent routing in the literature and the good characteristics of the proposed scheme are corroborated by ample simulation experiments, where significant gains in performance are witnessed.",
Extended Use Case Points Method for Software Cost Estimation,"Software cost estimation is a key open issue for the software industry, which suffers from cost overruns frequently. As the most popular technique for object-oriented software cost estimation, use case points(UCP) method, however, has two major drawbacks: the uncertainty of the cost factors and the abrupt classification. To address these two issues, we propose the extended use case points (EUCP) method. With a probabilistic cost model constructed from integrating fuzzy set theory and Bayesian belief networks(BBNs) with the UCP method, EUCP provides a probability distribution of cost and a refined gradual classification, which mitigate the uncertainty of cost factors and improve the accuracy of classification. In this paper, we provides two case studies to demonstrate the effectiveness of EUCP in the real life.","Uncertainty,
Object oriented modeling,
Fuzzy set theory,
Probability distribution,
Cost function,
Risk management,
Fuzzy sets,
Educational institutions,
Computer science,
State estimation"
Parallel and distributed approach for processing large-scale XML datasets,"An emerging trend is the use of XML as the data format for many distributed scientific applications, with the size of these documents ranging from tens of megabytes to hundreds of megabytes. Our earlier benchmarking results revealed that most of the widely available XML processing toolkits do not scale well for large sized XML data. A significant transformation is necessary in the design of XML processing for scientific applications so that the overall application turn-around time is not negatively affected. We present both a parallel and distributed approach to analyze how the scalability and performance requirements of large-scale XML-based data processing can be achieved. We have adapted the Hadoop implementation to determine the threshold data sizes and computation work required per node, for a distributed solution to be effective. We also present an analysis of parallelism using our Piximal toolkit for processing large-scale XML datasets that utilizes the capabilities for parallelism that are available in the emerging multi-core architectures. Multi-core processors are expected to be widely available in research clusters and scientific desktops, and it is critical to harness the opportunities for parallelism in the middleware, instead of passing on the task to application programmers. Our parallelization approach for a multi-core node is to employ a DFA-based parser that recognizes a useful subset of the XML specification, and convert the DFA into an NFA that can be applied to an arbitrary subset of the input. Speculative NFAs are scheduled on available cores in a node to effectively utilize the processing capabilities and achieve overall performance gains. We evaluate the efficacy of this approach in terms of potential speedup that can be achieved for representative XML data sets.","Large-scale systems,
XML,
Parallel processing,
Process design,
Performance analysis,
Scalability,
Data processing,
Distributed computing,
Computer architecture,
Multicore processing"
Vehicle fleet as a distributed energy storage system for the power grid,"Experts in the field have agreed that the addition of an energy storage unit to vehicles and renewable energy sources benefits their performance, efficiency, cost, and reliability. In this paper, the notion of integration of vehicular with renewable systems so that they can both share the onboard battery unit of plug-in hybrid electric vehicles is discussed.","Energy storage,
Power grids,
Costs,
Batteries,
Wind energy,
Power generation,
Renewable energy resources,
Power system reliability,
Hybrid electric vehicles,
Wind energy generation"
Channel assignment in multi-radio wireless mesh networks : A graph-theoretic approach,"In this paper, we propose a load-based scheme for assigning channels to radio interfaces in multi-radio, multi-channel wireless mesh networks. We first construct a model for channel assignment as an optimization problem with the goal of minimizing the overall network interference. The problem is proven to be NP-Hard. We then apply the Lagrangian relaxation method to obtain lower bounds as well as near-optimal feasible solutions for large size networks. We further present a meta-heuristic based on genetic algorithms, which can yield good quality solutions for very large networks. With these two centralized approaches as the benchmark, we propose a fully distributed algorithm in order to tackle the channel assignment problem practically. Our extensive simulation experiments demonstrate that the distributed algorithm performs competitively and can serve as a practical and scalable solution to the channel assignment problem.","Wireless mesh networks,
Interference,
Distributed algorithms,
Lagrangian functions,
Relaxation methods,
Throughput,
Computer science,
Bandwidth,
Routing,
Telecommunication traffic"
SEU MITIGATION-using 1/3 rate convolution coding,"With the new emerging fabrication technology there has been scaling of components size, leading to reduction in the physical size of devices. This increases the frequency of radiation induced temporary faults also called soft errors, which corrupts the content of the memory system. This paper proposes a new technique to protect the data from these soft errors which occurs in form of bit flips in the memory. The technique introduced is based on simple convolution codes and is proven to give good performance.","Convolution,
Convolutional codes,
Single event upset,
Protection,
Redundancy,
Iterative decoding,
Encoding,
Error correction,
Bit error rate,
Delay"
Real-time estimation of the ECG-derived respiration (EDR) signal using a new algorithm for baseline wander noise removal,"Numerous methods have been reported for deriving respiratory information such as respiratory rate from the electrocardiogram (ECG). In this paper the authors present a real-time algorithm for estimation and removal of baseline wander (BW) noise and obtaining the ECG-derived respiration (EDR) signal for estimation of a patient’s respiratory rate. This algorithm utilizes a real-time “T-P knot” baseline wander removal technique which is based on the repetitive backward subtraction of the estimated baseline from the ECG signal. The estimated baseline is interpolated from the ECG signal at midpoints between each detected R-wave. As each segment of the estimated baseline signal is subtracted from the ECG, a “flattened” ECG signal is produced for which the amplitude of each R-wave is analyzed. The respiration signal is estimated from the amplitude modulation of R-waves caused by breathing. Testing of the algorithm was conducted in a pseudo real-time environment using MATLABTM, and test results are presented for simultaneously recorded ECG and respiration recordings from the PhysioNet/PhysioBank Fantasia database. Test data from patients were chosen with particularly large baseline wander components to ensure the reliability of the algorithm under adverse ECG recording conditions. The algorithm yielded EDR signals with a respiration rate of 4.4 breaths/min. for Fantasia patient record f2y10 and 10.1 breaths/min. for Fantasia patient record f2y06. These were in good agreement with the simultaneously recorded respiration data provided in the Fantasia database thus confirming the efficacy of the algorithm.","Electrocardiography,
Testing,
Amplitude estimation,
Signal processing algorithms,
Signal analysis,
Databases,
Amplitude modulation,
Computer languages,
Algorithm design and analysis,
Noise level"
An improved valence-arousal emotion space for video affective content representation and recognition,"To understand video affective content automatically, the primary task is to transform the abstract concept of emotion into the form which can be handled by the computer easily. An improved V-A emotion space is proposed to address this problem. It unifies the discrete and dimensional emotion model by introducing the typical fuzzy emotion subspace. Fuzzy C-mean clustering (FCM) algorithm is adopted to divide the V-A emotion space into the subspaces and Gaussian mixture model (GMM) is used to determine their membership functions. Based on the proposed emotion space, the maximum membership principle and the threshold principle are introduced to represent and recognize video affective content. A video affective content database is created to validate the proposed model. The experimental results show that the improved emotion space can be used as a solution to represent and recognize video affective content.","Emotion recognition,
Space technology,
Content based retrieval,
Indexing,
Sun,
Computer science,
Discrete transforms,
Clustering algorithms,
Databases,
Humans"
Increased discrimination in level set methods with embedded conditional random fields,"We propose a novel approach for improving level set segmentation methods by embedding the potential functions from a discriminatively trained conditional random field (CRF) into a level set energy function. The CRF terms can be efficiently estimated and lead to both discriminative local potentials and edge regularizers that take into account interactions among the labels. Unlike discrete CRFs, the use of a continuous level set framework allows the natural use of flexible continuous regularizers such as shape priors. We show promising experimental results for the method on two difficult medical image segmentation tasks.",
IPGroupRep: A Novel Reputation Based System for Anti-Spam,"While reputation systems have already been applied into the field of anti-spam, they still have some shortcomings,in terms of reputation database scale and vulnerable to be evaded by the adverse users. To solve these problems,we present a novel reputation system named IPGroupRep.The performance of this system is evaluated on some real world dataset, and compared to the existing reputation systems. The experimental results show that IPGroupRep could effectively separate spam from legitimate messages .",
BRICK: A Binary Tool for Run-Time Detecting and Locating Integer-Based Vulnerability,"Integer-based vulnerability is an extremely serious bug for programs written in languages such as C/C++. However,in practice, very few software security tools can efficiently detect and accurately locate such vulnerability. In addition, previous methods mainly depend on source code analysis and recompilation which are impractical when protecting the program without source code. In this paper,we present the design, implementation, and evaluation of BRICK (Binary Run-time Integer-based vulnerability Checker), a tool for run-time detecting and locating integer-based vulnerability. Given an integer-based vulnerability exploit, BRICK is able to catch the value which falls out of the range of its corresponding type, then find the root cause for this vulnerability, and finally locate the vulnerability code and give a warning, based on its checking scheme. BRICK is implemented on the dynamic binary instrumentation framework Valgrind and its type inference plug-in: Catchconv. Preliminary experimental results are quit promising: BRICK can detect and locate most of integer-based vulnerability in real software, and has very low false positives and negatives.","Protection,
Software tools,
Instruments,
Computer bugs,
Program processors,
Runtime library,
Availability,
Computer security,
Laboratories,
Computer science"
Vacant parking space detector for outdoor parking lot by using surveillance camera and FCM classifier,"The most prevailing approach now for parking lot vacancy detecting system is to use sensor-based techniques. The main impediments to the camera-based system in applying to parking lots on rooftop and outside building are the glaring sun light and dark shadows in the daytime, and low-light intensity and back-lighting in the nighttime. To date, no camera-based detecting systems for outdoor parking lots have been in practical use. A few engineering firms provide the camera-based system, which is only for underground and indoor parking lots. This paper reports on the new camera based system called ParkLotD for detecting vacancy/occupancy in parking lots. ParkLotD uses a classifier based on fuzzy c-means (FCM) clustering and hyper-parameter tuning by particle swarm optimization (PSO). The test result of the detection error rate for the indoor multi-story parking lot has improved by an order of magnitude compared to the current system based on the edge detection approach. ParkLotD demonstrates high detection performance and enables the camera-based system to achieve the practical use in outdoor parking lots.","Detectors,
Surveillance,
Cameras,
Image edge detection,
Light emitting diodes,
Sensor systems,
Displays,
Infrared sensors,
Impedance,
Sun"
Vision-based leader-follower formations with limited information,This paper presents a new vision-based leader-follower formation algorithm where the leader's trajectory is unknown to the robots which are following. Formation schemes in straight lines and diagonal formations are introduced which are both stable and observable in the presence of limited views. The algorithms are novel since they only use local image measurements through a pinhole camera to estimate the leader's position. This approach does not require specialized markings nor extensive robot communications. The algorithms are also decentralized. We apply an input-output feedback linearization for system stability and utilize an Extended Kalman Filter (EKF) for estimation. Simulations illustrate how the proposed formation controls work. Real experiments utilizing multiple miniature robots are also presented and illustrate the challenges associated with noisy images in real-world applications.,"Cameras,
Robot vision systems,
Communication system control,
Robot sensing systems,
Kinematics,
Centralized control,
Linear feedback control systems,
Stability,
Angular velocity,
Velocity control"
Separation principle for opportunistic spectrum access in unslotted primary systems,We consider the design of opportunistic spectrum access (OSA) strategies that allow secondary users to search for and exploit spectrum opportunities in unslotted primary systems. We formulate the joint design of OSA as a constrained partially observable Markov decision process (POMDP). A separation principle for the joint design of OSA is established under certain conditions on the false alarm probability of the spectrum sensor. This result extends the separation principle for OSA in slotted primary systems to unslotted primary systems.,
Survivability: Measuring and ensuring path diversity,"A novel criterion is introduced for assessing the diversity of a collection of paths or trajectories. The main idea is the notion of survivability, which measures the likelihood that numerous paths are obstructed by the same obstacle. This helps to improve robustness with respect to collision, which is an important challenge in the design of real-time planning algorithms. Efficient algorithms are presented for computing the survivability criterion and for selecting a subset of paths that optimize survivability from a larger collection. The algorithms are implemented and solutions are illustrated for two different systems. Chi-square tests are used to show uniform coverage obtained by using the computed paths in a simple breadth-first search. Random obstacle placement is used to show superior robustness of these primitives compared to uniform sampling of the control space.","Sampling methods,
Robustness,
Algorithm design and analysis,
Robust control,
Remotely operated vehicles,
Robotics and automation,
Computer science,
USA Councils,
Testing,
State-space methods"
Cross-sectional comparison of insulation degradation mechanisms and lifetime evaluation of power transmission equipment,"As the deregulation of the electric power industry moves forward, there is an increasing need for cost cutting and it is becoming exceedingly important to keep track of the residual life and identify the timing of replacement for power transmission equipment that has aged for several tens of years in the field. This paper, taking up the theme of ""insulation degradation phenomena and lifetime evaluation for power transmission equipment"", conducted cross-sectional comparison of different types of equipment although this kind of evaluation was made independently for these equipment types so far. This paper covered six types of equipment: gas insulated switchgear (GIS), gas insulated transformers, oil filled (OF) transformers, OF cables, crosslinked polyethylene (XLPE) cables, and power capacitors. As a result of examining the characteristics such as electric, thermal, mechanical, and hygroscopic degradations, each of these equipment types was found to have distinctive Voltage-time (V-t) and Voltage-number (V-N) characteristics, which sometimes greatly varied from those of others. Since thermal degradation is dominant for gas insulated transformers and oil filled transformers, their lifetimes depend on the reduction of mechanical strengths of Polyethylene Terephthalate (PET) or insulating paper. Therefore, it is effective to diagnose decomposition products such as CO, CO2 and furfural generated in the thermal degradation mechanism. On the other hand, the influence of electric degradation has more significant impact on the other equipment types except OF cables. Such GIS components as epoxy spacers, XLPE cables, and power capacitors for which long-term ac voltages are dominant, are commonly characterized by the long-term V-t characteristics with a large gradient (small n-value). In terms of insulation structures, there is a possibility of reducing the degree of degradation by improving solids or the interface between solids and electrodes.","Power transmission,
Power transformer insulation,
Gas insulation,
Thermal degradation,
Oil insulation,
Geographic Information Systems,
Petroleum,
Oil filled cables,
Polyethylene,
Power capacitors"
Dynamic fuzzy clustering using Harmony Search with application to image segmentation,"In this paper, a new dynamic clustering approach based on the Harmony Search algorithm (HS) called DCHS is proposed. In this algorithm, the capability of standard HS is modified to automatically evolve the appropriate number of clusters as well as the locations of cluster centers. By incorporating the concept of variable length in each harmony memory vector, DCHS is able to encode variable numbers of candidate cluster centers at each iteration. The PBMF cluster validity index is used as an objective function to validate the clustering result obtained from each harmony memory vector. The proposed approach has been applied onto well known natural images and experimental results show that DCHS is able to find the appropriate number of clusters and locations of cluster centers. This approach has also been compared with other metaheuristic dynamic clustering techniques and has shown to be very promising.","Image segmentation,
Clustering algorithms,
Partitioning algorithms,
Machine learning algorithms,
Pattern recognition,
Space exploration,
Application software,
Radiology,
Unsupervised learning,
Machine learning"
Key generation exploiting MIMO channel evolution: Algorithms and theoretical limits,"An emerging area of research in wireless communications is the generation of secret encryption keys based on the shared (or common) randomness of the wireless channel between two legitimate nodes in a communication network. However, to date, little work has appeared on methods to use the increased randomness available when the network nodes have multiple antennas. This paper provides theoretical performance bounds associated with using multi-antenna communications and proposes two practical methods for generating secret keys exploiting the increased randomness. Performance simulations reveal the efficiency of the methods.","MIMO,
Wireless communication,
Cryptography,
Performance analysis,
Mutual information,
Jacobian matrices,
Computer networks,
Communication networks,
Multipath channels,
Antennas and propagation"
Performance comparison of hybrid vehicle energy management controllers on real-world drive cycle data,"Hybrid vehicle fuel economy and drivability performance are very sensitive to the ldquoenergy managementrdquo controller that regulates power flow among the various energy sources and sinks. Many methods have been proposed for designing such controllers. Most analytical studies evaluate closed-loop performance on government test cycles. Moreover, there are few results that compare stochastic optimal control algorithms to the controllers employed in today's production hybrids. This paper studies controllers designed using shortest path stochastic dynamic programming (SPSDP). The controllers are evaluated on Ford Motor Company's highly accurate proprietary vehicle model over large numbers of real-world drive cycles, and compared to a controller developed by Ford for a prototype vehicle. Results show the SPSDP-based controllers yield 2-3% better performance than the Ford controller on real-world driving data, with even more improvement on a government test cycle. In addition, the SPSDP-based controllers can directly quantify tradeoffs between fuel economy and drivability.","Vehicle driving,
Energy management,
Fuel economy,
Government,
Testing,
Stochastic processes,
Optimal control,
Load flow,
Performance analysis,
Production"
Overcoming limitations of game-theoretic distributed control,"Recently, game theory has been proposed as a tool for cooperative control. Specifically, the interactions of a multi-agent distributed system are modeled as a non-cooperative game where agents are self-interested. In this work, we prove that this approach of non-cooperative control has limitations with respect to engineering multi-agent systems. In particular, we prove that it is not possible to design budget balanced agent utilities that also guarantee that the optimal control is a Nash equilibrium. However, it is important to realize that game-theoretic designs are not restricted to the framework of non-cooperative games. In particular, we demonstrate that these limitations can be overcome by conditioning each player's utility on additional information, i.e., a state. This utility design fits into the framework of a particular form of stochastic games termed state-based games and is applicable in many application domains.","Distributed control,
Nash equilibrium,
Wireless sensor networks,
Costs,
Resource management,
Game theory,
Control systems,
Multiagent systems,
Communication system control,
Systems engineering and theory"
Mining Cluster-Based Mobile Sequential Patterns in Location-Based Service Environments,"In recent years, a number of studies have been done on Location-Based Service (LBS) due to their wide range of potential applications. In this paper, we propose a novel data mining algorithm named Cluster-based Mobile Sequential Pattern Mine (CMSP-Mine) for efficiently discovering the Cluster-based Mobile Sequential Patterns (CMSPs) of users in LBS environments. In CMSP-Mine, we first propose a transaction similarity measurement named Location-Based Service Alignment (LBS-Alignment) to evaluate the similarity between two mobile transaction sequences. Then, we propose a transaction clustering algorithm named Cluster-Object based Smart Cluster Affinity Search Technique (CO-Smart-CAST) to form a user cluster model of the mobile transactions based on LBS-Alignment. Furthermore, we proposed the novel prediction strategy that utilizes the discovered CMSPs to precisely predict the next movement of mobile users. To our best knowledge, this is the first work on mining the mobile sequential patterns associated with moving path and user clusters in LBS environments. Finally, through a series of experiments, our proposed methods were shown to deliver excellent performance in terms of efficiency, accuracy andapplicability under various system conditions.","Data mining,
Clustering algorithms,
Mobile handsets,
Mobile computing,
Computer science,
Business,
Conference management,
Environmental management,
Engineering management,
Middleware"
A novel thermal optimization flow using incremental floorplanning for 3D ICs,"Thermal issue is a critical challenge in 3D IC design. To eliminate hotspots, physical layouts are always adjusted by shifting or duplicating hot blocks. However, these modifications may degrade the packing area as well as interconnect distribution greatly. In this paper, we propose some novel thermal-aware incremental changes to optimize these multiple objectives including thermal issue in 3D ICs. Furthermore, to avoid random incremental modification, which may be inefficient and need long runtime to converge, here potential gain is modeled for each candidate incremental change. Based on the potential gain, a novel thermal optimization flow to intelligently choose the best incremental operation is presented. We distinguish the thermal-aware incremental changes in three different categories: migrating computation, growing unit and moving hotspot. Mixed integer linear programming (MILP) models are devised according to these different incremental changes. Experimental results show that migrating computation, growing unit and moving hotspot can reduce max on-chip temperature by 7%, 13% and 15% respectively on MCNC/GSRC benchmarks. Still, experimental results also show that the thermal optimization flow can reduce max on-chip temperature by 14% compared to an existing 3D floorplan tool CBA, and achieve better area and total wirelength improvement than individual operations do.","Three-dimensional integrated circuits,
Temperature,
Thermal conductivity,
Design optimization,
Thermal management,
Thermal degradation,
Runtime,
Delay,
Constraint optimization,
Computer science"
Comparison of surface EMG monitoring electrodes for long-term use in rehabilitation device control,"In this paper different types of electrodes for long-term surface EMG recording are compared to a reference electrode that is established for clinical use. The electrode materials include four different polymers with conductive load and a fabric of threads coated by a conductive layer. Different criteria are used to evaluate surface EMG recording: the signal quality, including signal-to-noise (SNR) ratio, and impedance in long-term monitoring. The aim of the study is to find an EMG electrode that allows for both silicone liner and textile integration for control of rehabilitation devices for quadriplegics with a partial residual function of the upper limb and for multifunctional prosthetic hands. Besides electrical properties, the biocompatibility and the wearing comfort have to be considered to achieve a wide acceptance by the patients. Except for one evaluated electrode, the signal quality of the four different surface electrodes is comparable to commercial Ag/AgCl gel electrodes in long-term monitoring.","Electromyography,
Monitoring,
Electrodes,
Conducting materials,
Polymer films,
Fabrics,
Yarn,
Surface impedance,
Textiles,
Prosthetic hand"
Performance analysis of outdoor localization systems based on RSS fingerprinting,"In the recent years the Location Based Services (LBS) are grabbing the attention of telecommunication actors since they are sources of new services and new revenues. The main challenge in the LBS domain is the localization of the mobile terminals within a certain accuracy. To this end, several radio positioning techniques have been introduced, one of which is the Location Fingerprinting. Although location fingerprinting has been investigated in some previous works, however there are only few studies that analyze its performance according to physical parameters of the underlying environment. In this paper, we present a performance analysis of the outdoor location fingerprinting systems. We propose a radio propagation model which allows us to introduce some spatial correlations for the shadowing effect and to include the temporal fluctuations of the radio signal. Based on our model, we examine the impact of different physical parameters on the system performance, and thereby we provide a framework which may be useful in the design and implementation of fingerprinting positioning systems.","Performance analysis,
Fingerprint recognition,
Radio propagation,
GSM,
Global Positioning System,
Land mobile radio cellular systems,
Computer science,
Research and development,
Telecommunication standards,
Shadow mapping"
Towards a Taxonomy of Provenance in Scientific Workflow Management Systems,"Scientific Workflow Management Systems (SWfMS) have been helping scientists to prototype and execute in silico experiments. They can systematically collect provenance information for the derived data products to be later queried. Despite the efforts on building a standard Open Provenance Model (OPM), provenance is tightly coupled to SWfMS. Thus scientific workflow provenance concepts, representation and mechanisms are very heterogeneous, difficult to integrate and dependent on the SWfMS. To help comparing, integrating and analyzing scientific workflow provenance, this paper presents a taxonomy about provenance characteristics. Its classification enables computer scientists to distinguish between different perspectives of provenance and guide to a better understanding of provenance data in general. The analysis of existing approaches will assist us in managing provenance data from distributed heterogeneous workflow executions.","Taxonomy,
Workflow management software,
Prototypes,
Collaborative work,
Instruments,
Assembly,
Environmental management,
Distributed computing,
Virtual prototyping,
Computational modeling"
A framework for planning comfortable and customizable motion of an assistive mobile robot,"Assistive mobile robots that can navigate autonomously can greatly benefit people with mobility impairments. Since an assistive mobile robot transports a human user from one place to another, its motion should be comfortable for human users. Moreover, it should be possible for users to customize the motion according to their comfort. While there exists a large body of work on motion planning for mobile robots, very little attention has been paid to characterizing comfort and planning comfortable trajectories.","Motion planning,
Mobile robots,
Humans,
Boundary conditions,
Acceleration,
Navigation,
Trajectory,
Motion measurement,
Time measurement,
Kinematics"
An efficient reliable multicast protocol for 802.11-based wireless LANs,"Many applications are inherently multicast in nature. Such applications can benefit tremendously from reliable multicast support at the MAC layer because addressing reliability at the MAC level is much less expensive than handling errors at the upper layers. However, the IEEE 802.11 MAC layer does not support reliable multicast. This void in the MAC layer is a limiting factor in the efficacy of multicast applications. In this paper, we propose a Slot Reservation based Reliable Multicast protocol that adds a novel reliability component to the existing multicast protocol in the 802.11 MAC. Our protocol builds on the existing DCF support in the IEEE 802.11 MAC to seamlessly incorporate an efficient reliable multicast mechanism. Intelligent assignment of transmission slots, minimal control packet overhead and an efficient retransmission strategy form the basis of our protocol. We evaluate the performance of our protocol through extensive simulations. Our simulation results show that our protocol outperforms another reliable multicast protocol, Batch Mode Multicast MAC in terms of delivered throughput in various scenarios.","Multicast protocols,
Wireless LAN,
Media Access Protocol,
Communication system control,
Application software,
Throughput,
Broadcasting,
Bandwidth,
Multiaccess communication,
Computer science"
Relationship-based change propagation: A case study,"Software development is an evolutionary process. Requirements of a system are often incomplete or inconsistent, and hence need to be extended or modified over time. Customers may demand new services or goals that often lead to changes in the design and implementation of the system. These changes are typically very expensive. Even if only local modifications are needed, manually applying them is time-consuming and and error-prone. Thus, it is essential to assist users in propagating changes across requirements, design, and implementation artifacts. In this paper, we take a model-based approach and provide an automated algorithm for propagating changes between requirements and design models. The key feature of our work is explicating relationships between models at the requirements and design levels. We provide conditions for checking validity of these relationships both syntactically and semantically. We show how our algorithm utilizes the relationships between models at different levels to localize the regions that should be modified. We use the IBM Trade 6 case study to demonstrate our approach.","Programming,
Computer science,
Algorithm design and analysis,
Erbium,
Conferences"
Directional statistics BRDF model,"We introduce a novel parametric BRDF model that can accurately encode a wide variety of real-world isotropic BRDFs with a small number of parameters. The key observation we make is that a BRDF may be viewed as a statistical distribution on a unit hemisphere. We derive a novel directional statistics distribution, which we refer to as the hemispherical exponential power distribution, and model an isotropic BRDF with a mixture of it. The novel directional statistics BRDF model allows us to derive a canonical probabilistic method for estimating its parameters including the number of components. We show that the model captures the full spectrum of real-world isotropic BRDFs with accuracy comparable to non-parametric models but with a much more compact representation. We also experimentally show that the model achieves better accuracy with less measurements compared with such non-parametric models. We further demonstrate the advantages of the novel BRDF model by showing its use for reflection component separation and for exploring the space of isotropic BRDFs.","Statistical distributions,
Optical reflection,
Solid modeling,
Brain modeling,
Parametric statistics,
Parameter estimation,
Space exploration,
Computer vision,
Inverse problems,
Lighting"
SIFT features for face recognition,"Scale Invariant Feature Transform (SIFT) has shown to be very powerful for general object detection/recognition. And recently, it has been applied in face recognition. However, the original SIFT algorithm may not be optimal for analyzing face images. In this paper, we analyze the performance of SIFT and study its deficiencies when applied to face recognition. We propose two new approaches: Keypoints-Preserving-SIFT (KPSIFT) which keeps all the initial keypoints as features and Partial-Descriptor-SIFT (PDSIFT) where keypoints detected at large scale and near face boundaries are described by a partial descriptor. Furthermore, we compare the performances of holistic approaches: Fisherface (FLDA), the null space approach (NLDA) and Eigenfeature Regularization and Extraction (ERE) with feature based approaches: SIFT, KPSIFT and PDSIFT. Experimental results on ORL and AR databases show that our proposed approaches KPSIFT and PDSIFT can achieve better performance than the original SIFT. Moreover, the performance of PDSIFT is significantly better than FLDA and NLDA. And PDSIFT can achieve the same or better performance than the most successful holistic approach ERE.","Face recognition,
Face detection,
Object detection,
Image analysis,
Algorithm design and analysis,
Performance analysis,
Large-scale systems,
Null space,
Feature extraction,
Spatial databases"
Design of optimal switching surfaces for switched autonomous systems,"This paper presents a novel, computationally feasible procedure for computing optimal switching surfaces, i.e. optimal feedback controllers for switched autonomous nonlinear systems. Such systems are regulated by appropriately scheduling their operation modes over time. Given a finite mode sequence, the control task is to determine switching surfaces, which implicitly encode locally optimal switching times for a family of trajectories emerging from a pre-specified initial state set. Optimality of the switching times is assessed according to a nonlinear performance criterion.","Optimal control,
Feedback control,
Control systems,
Open loop systems,
Switching systems,
Switching converters,
Robots,
Switched systems,
Switches,
Sequences"
Analysis on mobile WiMAX security,"Security support is mandatory for any communication networks. For wireless systems, security support is even more important to protect the users as well as the network. Since wireless medium is available to all, the attackers can easily access the network and the network becomes more vulnerable for the user and the network service provider. In the existing research, there is lack of integrated presentation of solutions to all the security issues of mobile WiMAX network, which is important for researchers and practitioners. This paper discusses all the security issues in both point-to-multipoint and mesh network and discusses their solutions. In addition, a new recommendation is also proposed for one of the security issues.","WiMAX,
Authentication,
Data security,
Communication system security,
Cryptography,
Access protocols,
Mesh networks,
Portable media players,
Public key,
Computer security"
Minimizing total busy time in parallel scheduling with application to optical networks,"We consider a scheduling problem in which a bounded number of jobs can be processed simultaneously by a single machine. The input is a set of n jobs J = {J1, … , Jn}. Each job, Jj, is associated with an interval [sj, cj] along which it should be processed. Also given is the parallelism parameter g ≥ 1, which is the maximal number of jobs that can be processed simultaneously by a single machine. Each machine operates along a contiguous time interval, called its busy interval, which contains all the intervals corresponding to the jobs it processes. The goal is to assign the jobs to machines such that the total busy time of the machines is minimized. The problem is known to be NP-hard already for g = 2. We present a 4-approximation algorithm for general instances, and approximation algorithms with improved ratios for instances with bounded lengths, for instances where any two intervals intersect, and for instances where no interval is properly contained in another. Our study has important application in optimizing the switching costs of optical networks.","Optical fiber networks,
Computer science,
Processor scheduling,
Single machine scheduling,
Approximation algorithms,
Application software,
Educational institutions,
Parallel processing,
Cost function,
Parallel machines"
Robust MANET routing using adaptive path redundancy and coding,"Providing efficient networking services in MANETs is very challenging in presence of mobility, unpredictable radio channel and interference: a significant number of packets can be corrupted and/or lost. To increase reliability, various measures have been proposed. A popular approach is to use multiple paths and transmit identical copies of the packet on each path (i.e., path redundancy). A more efficient way is to use random Network Coding on top of the multiple paths and send differently coded packets on each path. Network Coding can improve delivery efficiency. But, it also increases delays and network overhead. If channel disruption is intermittent, it behooves us to turn “on and off” path redundancy and Network Coding depending on measured packet loss. In this paper we compare via simulation the performance of multipath routing with and without Network Coding (and with/without dynamic adaptation) for various motion and packet loss scenarios in terms of reliability, efficiency, robustness, and scalability.","Robustness,
Mobile ad hoc networks,
Routing,
Network coding,
Redundancy,
Forward error correction,
Error correction,
Automatic repeat request,
Interference,
Telecommunication network reliability"
Design for configurability: rethinking interdomain routing policies from the ground up,"Giving ISPs more fine-grain control over interdomain routing policies would help them better manage their networks and offer value-added services to their customers. Unfortunately, the current BGP route-selection process imposes inherent restrictions on the policies an ISP can configure, making many useful policies infeasible. In this paper, we present Morpheus, a routing control platform that is designed for configurability. Morpheus enables a single ISP to safely realize a much broader range of routing policies without requiring changes to the underlying routers or the BGP protocol itself. Morpheus allows network operators to: (1) make flexible trade-offs between policy objectives through a weighted-sum based decision process, (2) realize customer-specific policies by supporting multiple route-selection processes in parallel, and allowing customers to influence the decision processes, and (3) configure the decision processes through a simple and intuitive configuration interface based on the Analytic Hierarchy Process, a decision-theoretic technique for balancing conflicting objectives. We also present the design, implementation, and evaluation of Morpheus as an extension to the XORP software router.","Routing protocols,
IP networks,
Performance analysis,
Scalability,
Protection,
Internet,
Computer science,
Laboratories,
Computer architecture,
Service oriented architecture"
Application Level Interoperability between Clouds and Grids,"SAGA is a high-level programming interface which provides the ability to develop distributed applications in aninfrastructure independent way. In an earlier paper, we discussed how SAGA was used to develop a version of MapReduce which provided the user with the ability to control the relative placement of compute and data, whilst utilizing different distributed infrastructure. In this paper, we use the SAGA-based implementation of MapReduce, and demonstrate its interoperability across Clouds and Grids. We discuss how a range of cloud adaptors have beendeveloped for SAGA. The major contribution of this paper isthe demonstration – possibly the first ever, of interoperability between different Clouds and Grids, without any changes to the application. We analyse the performance of SAGA-MapReduce when using multiple, different, heterogeneous infrastructure concurrently for the same problem instance; However, we do not strive to provide a rigorous performance model, but to provide a proof-of-concept of application-level interoperabilityand illustrate its importance","Cloud computing,
Distributed computing,
Pervasive computing,
Application software,
Performance analysis,
Runtime,
Computer interfaces,
Grid computing,
Computer science,
Large-scale systems"
Prostate Brachytherapy Seed Reconstruction With Gaussian Blurring and Optimal Coverage Cost,"Intraoperative dosimetry in prostate brachytherapy requires localization of the implanted radioactive seeds. A tomosynthesis-based seed reconstruction method is proposed. A three-dimensional volume is reconstructed from Gaussian-blurred projection images and candidate seed locations are computed from the reconstructed volume. A false positive seed removal process, formulated as an optimal coverage problem, iteratively removes ldquoghostrdquo seeds that are created by tomosynthesis reconstruction. In an effort to minimize pose errors that are common in conventional C-arms, initial pose parameter estimates are iteratively corrected by using the detected candidate seeds as fiducials, which automatically ldquofocusesrdquo the collected images and improves successive reconstructed volumes. Simulation results imply that the implanted seed locations can be estimated with a detection rate of ges97.9% and ges99.3% from three and four images, respectively, when the C-arm is calibrated and the pose of the C-arm is known. The algorithm was also validated on phantom data sets successfully localizing the implanted seeds from four or five images. In a Phase-1 clinical trial, we were able to localize the implanted seeds from five intraoperative fluoroscopy images with 98.8% (STD=1.6) overall detection rate.","Brachytherapy,
Cost function,
Image reconstruction,
Dosimetry,
Reconstruction algorithms,
Gaussian processes,
Error correction,
Parameter estimation,
Imaging phantoms,
Clinical trials"
Cost of secure sensing in IEEE 802.15.4 networks,"In this paper, we consider interconnected IEEE 802.15.4 beacon enabled clusters with secure and reliable sensing. Clusters are interconnected via bridges implemented on cluster coordinators. Each cluster has to contribute a constant number of packets per second to the sink. Packets are protected with the Message Authentication Code which requires a secret key shared between each node and coordinator. We model network behavior under regular sensing traffic, power management, and periodic key exchanges. Three types of key exchange protocols of varying complexity are considered, and the impact of their communication and computation complexity on the network lifetime is evaluated. Our model and results can be used to determine network and security parameters in IEEE 802.15.4 based wireless sensor network when application requirements and known.","Costs,
Bridges,
Protection,
Message authentication,
Telecommunication traffic,
Traffic control,
Computer network management,
Energy management,
Protocols,
Computer networks"
Mining the coherence of GNOME bug reports with statistical topic models,"We adapt Latent Dirichlet Allocation to the problem of mining bug reports in order to define a new information-theoretic measure of coherence. We then apply our technique to a snapshot of the GNOME Bugzilla database consisting of 431,863 bug reports for multiple software projects. In addition to providing an unsupervised means for modeling report content, our results indicate substantial promise in applying statistical text mining algorithms for estimating bug report quality. Complete results are available from our supplementary materials website at http://sourcerer.ics.uci.edu/msr2009/gnome_coherence.html.","Coherence,
Linear discriminant analysis,
XML,
Information theory,
Databases,
Vocabulary,
Bayesian methods,
Computer science,
Text mining,
Software measurement"
Steady-state issues with finite control set model predictive control,"Finite Control Set Model Predictive Control (FCS-MPC) is a novel and promising control scheme for power converters and drives. Many practical and theoretical issues have been presented in the literature, showing good performance of this technique. The present work deals with one of the most relevant aspects of any controller, namely, the steady-state operation. As will be shown, basic FCS-MPC formulations can be enhanced to achieve a zero average steady-state error. We focus on a simple H-Bridge power converter with two complementary switches and discuss benefits and drawbacks of our proposal.","Steady-state,
Predictive models,
Predictive control,
Switches,
Switching converters,
Cost function,
Voltage control,
Proposals,
Sampling methods,
Pulse width modulation"
Reflecting on self-adaptive software systems,"Self-adaptability has been proposed as an effective approach to automate the complexity associated with the management of modern-day software systems. While over the past decade we have witnessed significant progress in the manner in which such systems are designed, constructed, and deployed, there is still a lack of consensus among the engineers on some of the fundamental underlying concepts. In this paper, we attempt to alleviate this issue by exploring the crucial role of computational reflection in the context of self-adaptive software systems. We show that computational reflection forms the foundation of a self-adaptive system, and an understanding of its properties is a prerequisite to intelligent and predictable construction of such systems. Examining several systems in light of computational reflection has helped us to identify a number of key challenges, which we report on and propose as avenues of future research.","Software systems,
Optical reflection,
Computer science,
Computational intelligence,
Guidelines,
Informatics,
Design engineering,
Adaptive systems,
Logic programming,
Computer languages"
Mining Hierarchical Scenario-Based Specifications,"Scalability over long traces, as well as comprehensibility and expressivity of results, are major challenges for dynamic analysis approaches to specification mining. In this work we present a novel use of object hierarchies over traces of inter-object method calls, as an abstraction/refinement mechanism that enables user-guided, top-down or bottom-up mining of layered scenario-based specifications, broken down by hierarchies embedded in the system under investigation. We do this using data mining methods that provide statistically significant sound and complete results modulo user-defined thresholds, in the context of Damm and Harel’s live sequence charts (LSC); a visual, modal, scenario-based, inter-object language. Thus, scalability, comprehensibility, and expressivity are all addressed. Our technical contribution includes a formal definition of hierarchical inter-object traces, and algorithms for ‘zoomingout’ and ‘zooming-in’, used to move between abstraction levels on the mined specifications. An evaluation of our approach based on several case studies shows promising results.","Scalability,
Data mining,
Concrete,
Software engineering,
Management information systems,
Conference management,
Information management,
Engineering management,
Computer science,
Mathematics"
Impact of duodenal image capturing techniques and duodenal regions on the performance of automated diagnosis of celiac disease,"Various techniques have been developed for an automated classification of endoscopic images. Besides the classical methods for endoscopic image capturing, new methods like the modified immersion technique have been devised and are in use. The impact of specific image capturing techniques for feature extraction and classification in automated diagnosis is unclear. This work applies several well tested methods for feature extraction and classification on images captured with the conventional and the modified immersion technique. We compare the classification rates and the impact on feature extraction of each specific capturing technique. We also compare the classification rates of different duodenal regions. Finally we advise an optimal combination of image capturing technique, duodenal region and feature extraction methods for automated celiac disease diagnosis.",
Security test generation using threat trees,"Software security issues have been a major concern to the cyberspace community, so a great deal of research on security testing has been performed, and various security testing techniques have been developed. Most of these techniques, however, have focused on testing software systems after their implementation is completed. To build secure and dependable software systems in a cost-effective way, however, it is necessary to put more effort upfront during the software development life cycle. In this paper, we provided a security testing approach that derives test cases from design-level artifacts. The security testing approach we consider consists of four activities: building threat trees from threat modeling; generating security tests from threat trees; generating test inputs including valid and invalid inputs; and assigning input values to parameters. We also conducted an empirical study to show feasibility of our approach.","Object oriented modeling,
Computer security,
Software testing,
Software systems,
Programming,
Information security,
Automatic testing,
Performance evaluation,
Buildings,
Application software"
Towards creative design using collaborative interactive genetic algorithms,"We present a computational model of creative design based on collaborative interactive genetic algorithms. We test our model on floorplanning. We guide the evolution of floorplans based on subjective and objective criteria. The subjective criteria consists of designers picking the floorplan they like the best from a population of floorplans, and the objective criteria consists of coded architectural guidelines. We support collaboration by allowing individual designers to view each others' designs during the evolutionary process and the sharing of designs via case injection. This methodology supports team design, and reflects the view of creativity that collaboration accounts for much of our intelligence and creativity. We present a description of the model and a comparative study of floorplans created individually versus collaboratively. Our results show that floorplans created collaboratively were considered to be more “revolutionary” and “original” than those created individually.",
LLC series resonant converter with auxiliary circuit for hold-up time,"Although LLC series resonant converter has advantages of wide operation range and high efficiency, hold up time requirement makes it difficult to optimal design at normal operating condition, results in increasing transformer size and low efficiency. In this paper, LLC series resonant converter with auxiliary circuit is proposed. Compare with conventional LLC series resonant converter, higher gain at the same frequency can be achieved because the proposed converter has a current boost up capability. Therefore, even if ac line is turned off, switching frequency range of LLC series resonant converter can be reduced. With the auxiliary circuit, LLC series resonant converter can achieve high power density and high efficiency at normal operation condition.","RLC circuits,
Switching frequency,
Switching converters,
Voltage,
Resonant frequency,
Magnetic resonance,
Frequency conversion,
DC-DC power converters,
Power systems,
Thermal management"
Using JML Runtime Assertion Checking to Automate Metamorphic Testing in Applications without Test Oracles,"It is challenging to test applications and functions for which the correct output for arbitrary input cannot be known in advance, e.g. some computational science or machine learning applications. In the absence of a test oracle, one approach to testing these applications is to use metamorphic testing: existing test case input is modified to produce new test cases in such a manner that, when given the new input, the application should produce an output that can be easily be computed based on the original output. That is, if input x produces output f(x), then we create input x' such that we can predict f(x') based on f(x); if the application or function does not produce the expected output, then a defect must exist, and either f(x) or f(x') (or both) is wrong. By using metamorphic testing, we are able to provide built-in ""pseudo-oracles"" for these so-called ""nontestable programs"" that have no test oracles.In this paper, we describe an approach in which a function's metamorphic properties are specified using an extension to the Java Modeling Language (JML), a behavioral interface specification language that is used to support the ""design by contract"" paradigm in Java applications. Our implementation, called Corduroy, pre-processes these specifications and generates test code that can be executed using JML runtime assertion checking, for ensuring that the specifications hold during program execution. In addition topresenting our approach and implementation, we also describe our findings from case studies in which we apply our technique to applications without test oracles.","Automatic testing,
Runtime,
Application software,
Software testing,
Machine learning,
Java,
Computer science,
Specification languages,
Contracts,
Data mining"
Distributed Projection Approximation Subspace Tracking based on consensus propagation,"We develop and investigate a distributed algorithm for signal subspace tracking with a wireless sensor network without the need for a fusion center, in order to improve the robustness and scalability. We assume that all sensor nodes may broadcast messages to sensors in their neighborhood defined by a finite (small) communication radius. To this aim, we start from projection approximation subspace tracking (PAST) which is a well-investigated algorithm suitable for implementation in a fusion center. We arrive at a distributed approximation of the PAST algorithm by letting each sensor broadcast its local observation variable xn(t) and a filtered observation vector y-n(t) to its neighborhood. Vice versa, the received messages at sensor node n from its neighborhood are fused by employing consensus propagation. Finally, we investigate the proposed distributed algorithm in simulation runs.","Signal processing algorithms,
Wireless sensor networks,
Approximation algorithms,
Laboratories,
Distributed algorithms,
Signal processing,
Broadcasting,
Sensor fusion,
Chemical industry,
Aggregates"
Prioritized optimization for task-space control,"We introduce an optimization framework called prioritized optimization control, in which a nested sequence of objectives are optimized so as not to conflict with higher-priority objectives. We focus on the case of quadratic objectives and derive an efficient recursive solver for this case. We show how task-space control can be formulated in this framework, and demonstrate the technique on three sample control problems. The proposed formulation supports acceleration, torque, and bilateral force constraints, while simplifying reasoning about task-space control. This scheme unifies prioritized task-space and optimization-based control. Our method computes control torques for all presented examples in real-time.","Automatic control,
Acceleration,
Force control,
Torque control,
Control systems,
Animals,
Intelligent robots,
USA Councils,
Humanoid robots,
Robot control"
An incremental conductance method with variable step size for MPPT: Design and implementation,In this paper the authors propose a novel method for tracking the maximum power point of a PV array. This method is derived by the well known incremental conductance method and the novelty consists in varying the duty cycle of the dc-dc switching converter used as MPPT tracker using a variable step size instead of a fixed step size. A practical way to determine the minimum and the maximum values of the variable step size is explained in detail. The laboratory results demonstrate the effectiveness of the proposed method both in terms of speed in MPP tracking and accuracy in the MPP individuation.,"Voltage,
Switching converters,
Photovoltaic systems,
DC-DC power converters,
Solar power generation,
Temperature dependence,
Renewable energy resources,
Sun,
Costs,
Switches"
Multipath multi-stream distributed reliable video delivery in Wireless Sensor Networks,"Reliably transporting captured video content over Wireless Sensor Networks (WSNs) poses several challenges due to the unique energy constraints presented by individual sensor nodes. The nature and amount of video content is another challenge that further complicates WSN data reliability problem. Motivated by the aforementioned factors, in this work, we propose an architecture that promises to enhance received video quality along-with sensor network lifetime maximization using a three prong strategy: splitting source coded video into prioritized streams, use path diversity to route video packets and partially and progressively decode the data packets as they traverse the multi-hop network. Further intrigued by quality enhancement for received video, we apply our proposed framework to an actual video sequence and compare the benefits that can be achieved using multipath multi-stream distributed reliability (MMDR) framework that we propose. We show that with little bit of processing within the network, significant enhancements in quality of video can be achieved for same level of energy spent to deliver video over the end-to-end path. We propose selective budgeting of network energy resources for processing within the network. We use low density parity check (LDPC) codes for channel coding the video sensor data. We show that the proposed architecture outperforms prevalent end-to-end channel coding schemes by considerable margins. Further, based on the results achieved, we discuss the associated rate-distortion tradeoffs in design of video sensor networks.","Wireless sensor networks,
Parity check codes,
Channel coding,
Energy capture,
Streaming media,
Decoding,
Spread spectrum communication,
Video sequences,
Energy resources,
Rate-distortion"
Semisupervised Multicategory Classification With Imperfect Model,"Semisupervised learning has been of growing interest over the past years and many methods have been proposed. While existing semisupervised methods have shown some promising empirical performances, their development has been based largely on heuristics. In this paper, we investigate semisupervised multicategory classification with an imperfect mixture density model. In the proposed model, the training data come from a probability distribution, which can be modeled imperfectly by an identifiable mixture distribution. Furthermore, we propose a semisupervised multicategory classification method and establish its generalization error bounds. The theoretical analysis illustrates that the proposed method can utilize unlabeled data effectively and can achieve fast convergence rate.","Mathematics,
Error analysis,
Probability distribution,
Convergence,
Semisupervised learning,
Computer science,
Training data,
Estimation error,
Data mining,
Computer science education"
Chirp rate estimation of speech based on a time-varying quasi-harmonic model,"The speech signal is usually considered as stationary during short analysis time intervals. Though this assumption may be sufficient in some applications, it is not valid for high-resolution speech analysis and in applications such as speech transformation and objective voice function assessment for detection of voice disorders. In speech, there are non stationary components, for instance time-varying amplitudes and frequencies, which may change quickly over short time intervals. In this paper, a previously suggested time-varying quasi-harmonic model is extended in order or to estimate the chirp rate for each sinusoidal component, thus successfully tracking fast variations in frequency and amplitude. The parameters of the model are estimated through linear Least Squares and the model accuracy is evaluated on synthetic chirp signals. Experiments on speech signals indicate that the new model is able to efficiently estimate the signal component chirp rates, providing means to develop more accurate speech models for high-quality speech transformations.","Chirp,
Speech analysis,
Signal analysis,
Transient analysis,
Harmonic analysis,
Least squares approximation,
Frequency estimation,
Speech processing,
Speech synthesis,
Speech coding"
Speaker identification with whispered speech based on modified LFCC parameters and feature mapping,"Much research recently in speaker recognition has been devoted to robustness due to microphone and channel effects. However, changes in vocal effort, especially whispered speech, present significant challenges in maintaining system performance. Due to the absence of any periodic excitation in whisper, the spectral structure in whisper and neutral speech will differ. Therefore, performance of speaker ID systems, trained mainly with high energy voiced phonemes, degrades when tested with whisper. This study considers a front-end feature compensation method for whispered speech to improve speaker recognition using a neutral trained system. First, an alternative feature vector with linear frequency cepstral coefficients (LFCC) is introduced based on spectral analysis from both speech modes. Next, for the first time a feature mapping is proposed for reducing whisper/neutral mismatch in speaker ID. Feature mapping is applied on a frame-by-frame basis between two speaker independent GMMs (Gaussian Mixture Models) of whispered and neutral speech. Text independent closed set speaker ID results show an absolute 20% improvement in accuracy when compared with a traditional MFCC feature based system. This result confirms a viable approach to improving speaker ID performance between neutral and whispered speech conditions.","Speech,
Speaker recognition,
Robustness,
Microphones,
System performance,
Periodic structures,
Degradation,
System testing,
Vectors,
Frequency"
A compact kick-and-bounce mobile robot powered by unidirectional impulse force generators,"In this paper, we propose a compact kick-and-bounce mobile robot powered by unidirectional impulse force generators. The unidirectional impulse force generator is a simple mechanical device for generating high-frequency impulse forces toward a certain direction unilaterally utilizing snap-through bucklings. The proposed kick-and-bounce robot has a pair of the unidirectional impulse force generators as the muscles of its biped legs. The robot moves forward rapidly by the repetition of the kicks and bounces to the ground. We show that the developed palm-top mobile robot whose weight is of only 67 [g] achieves the velocity of 0.8 [m/s] instantaneously.","Mobile robots,
Power generation,
Legged locomotion,
Leg,
Intelligent robots,
Muscles,
USA Councils,
Animals,
Nanobioscience,
Climbing robots"
"Psi-calculi: Mobile Processes, Nominal Data, and Logic","A psi-calculus is an extension of the pi-calculus with nominal data types for data structures and for logical assertions representing facts about data. These can be transmitted between processes and their names can be statically scoped using the standard pi-calculus mechanism to allow for scope migrations. Other proposed extensions of the pi-calculus can be formulated as psi-calculi; examples include the applied pi-calculus, the spi-calculus, the fusion calculus, the concurrent constraint pi-calculus, and calculi with polyadic communication channels or pattern matching. Psi-calculi can be even more general, for example by allowing structured channels, higher-order formalisms such as the lambda calculus for data structures, and a predicate logic for assertions. Our labelled operational semantics and definition of bisimulation is straightforward, without a structural congruence. We establish minimal requirements on the nominal data and logic in order to prove general algebraic properties of psi-calculi. The proofs have been checked in the interactive proof checker Isabelle. We are the first to formulate a truly compositional labelled operational semantics for calculi of this calibre. Expressiveness and therefore modelling convenience significantly exceeds that of other formalisms, while the purity of the semantics is on par with the original pi-calculus.","Data structures,
Calculus,
Communication channels,
Cryptography,
Computer science,
Mobile computing,
Information technology,
Pattern matching,
Logic functions,
Equations"
Space charge behavior in palm oil fatty acid ester (PFAE) by electro-optic field measurement,"For power transformer insulating oil, we focused on palm oil fatty acid ester (PFAE). It has satisfactory insulating performance, excellent cooling ability and superior biodegradability. Recently, investigations for application of PFAE to environment-friendly power transformers installations have been started. In this paper, the space charge behavior in PFAE was investigated from the measurement results of the electric field by using Kerr electro-optic method under dc voltages applications. For the field measurement, the Kerr constant of PFAE was identified first. Then, the electric field strength and its temporal change were measured in several PFAE / pressboard (PB) insulation systems. The electric field in PFAE was determined by the capacitive distribution at the moment of the voltage application, and decreased rapidly with time and reached the steady state determined by the resistive distribution. The transition time of the capacitive distribution to the resistive one and the strength of electric field in PFAE were much different from those in mineral oil. The influence of oil flow on the time transition of the electric field was also measured. Their differences were discussed with the charge behavior in liquids, based on electro-chemical properties of the oil. We discussed the difference of the flow electrification characteristics between PFAE and mineral oil, and its mechanism was discussed based on the charge behavior in oils.","Space charge,
Petroleum,
Current measurement,
Charge measurement,
Electric variables measurement,
Power transformers,
Power transformer insulation,
Minerals,
Oil insulation,
Cooling"
Using the Bhattacharyya parameter for design and analysis of cooperative wireless systems,"A simplified method of analysis and design based on the Bhattacharyya parameter (BP) in conjunction with the union bound and weight enumeration is presented for relay channels using coded cooperation. This method is particularly suitable for low-complexity relay systems employing demodulate-and-forward, focusing on the problems of relay selection and outage analysis. These applications are chosen to illustrate the use of the BP in scenarios where analytical solutions are otherwise unattainable. In terms of relay selection, it is shown that BP-based relay selection has essentially the same performance as density evolution, though with much lower complexity. It is further shown that BP-based relay selection can be applied to fractional cooperation, where each relay only forwards a fraction of the source codeword. In terms of analysis, it is shown that weight enumeration with BP can be used to provide a close approximate to the upper bound on the outage probability of fractional cooperation, again with much lower computational complexity than density evolution.","Relays,
Decoding,
Upper bound,
Wireless sensor networks,
Computational complexity,
Channel coding,
Cooperative systems,
Fading,
Strontium,
Computer science"
Biometric fusion: Does modeling correlation really matter?,"Sources of information in a multibiometric system are often assumed to be statistically independent in order to simplify the design of the fusion algorithm. However, the independence assumption may not be always valid. In this paper, we analyze whether modeling the dependence between match scores in a multibiometric system has any effect on the fusion performance. Our analysis is based on the likelihood ratio (LR) based fusion framework, which guarantees optimal performance if the match score densities are known. We show that the assumption of independence between matchers has a significant negative impact on the performance of the LR fusion scheme only when (i) the dependence characteristics among genuine match scores is different from that of the impostor scores and (ii) the individual matchers are not very accurate.","Biometrics,
Performance analysis,
Algorithm design and analysis,
Bioinformatics,
Information resources,
Fuses,
Fingerprint recognition,
Computer science,
Speech,
Fingers"
Restarting Particle Filters: An Approach to Improve the Performance of Dynamic Indoor Localization,"Particle filters have been found to be effective in tracking mobile targets in indoor environments. One frequently encountered problem in these settings occurs when the target's movement pattern changes unexpectedly; such as when the target turns around, enters a room from a corridor or turns left or right at an intersection. If the particle filter makes an incorrect prediction, it might not be able to recover using the normal techniques of prediction, weight update and resampling. We propose an approach to automatically restart the particle filter by sampling the latest trusted observation when the particle cloud diverges too much from the observations. The restart decision is based on Kullback-Leibler divergence between the probability surfaces associated with the current observation and the particle cloud. Through an experimental study we show that the restart algorithm allows the successful early recovery of stranded particle filters, in our scenarios providing a 36% average improvement in localization accuracy.","Particle filters,
Target tracking,
Clouds,
Sampling methods,
Particle tracking,
Indoor environments,
Robot sensing systems,
Robotics and automation,
Computer science,
Mobile computing"
On-vehicle receiver for distant visible light road-to-vehicle communication,"In this paper, we propose a road-to-vehicle visible communication system for ITS. In this system, a LED traffic light is used as transmitter and a photodiode is used as receiver. There are several problems associated with applying visible light communication to the field of ITS. It is necessary to receive information from long distance. And tracking the transmitter for a certain moving distance of the vehicle is also important. We applied an imaging optics to receive information over long distance, and two cameras are used to solve the relationship between the transmitter and the receiver position changes with time, and vibrational correction technique is also fixed to the system to minimize vibrational affections. We developed algorithms to track the transmitter. The experiments were conducted to confirm the proposals.","Light emitting diodes,
Optical transmitters,
Optical receivers,
Radio transmitters,
Communication system traffic control,
Vehicles,
Computer science,
Communication system control,
Intelligent systems,
Roads"
Using motor imagery based brain-computer interface for post-stroke rehabilitation,"There is now sufficient evidence that using a rehabilitation protocol involving motor imagery (MI) practice (or mental practice (MP)) in conjunction with physical practice (PP) of goal-directed rehabilitation tasks leads to enhanced functional recovery of paralyzed limbs among stroke sufferers. It is however difficult to ensure patient engagement during MP in the absence of any on-line measure of the MP. Fortunately in an EEG-based brain-computer interface (BCI), an on-line measure of MI activity is used to devise neurofeedback for the BCI user to help him/her focus better on the task. This paper reports a pilot study in which an EEG-based BCI system is used to provide neurofeedback to stroke participants during the MP part of the rehabilitation protocol. This helps patients to undertake the MP with stronger focus. The participants included five chronic stroke sufferers. The trial was undertaken for 12 sessions over a period of 6 weeks. A set of rehabilitation outcome measures including action research arm test (ARAT) and motricity index was made use of in assessing functional recovery. Moderate improvements approaching a minimal clinically important difference (MCID) were observed for the ARAT. Small positive improvements were also observed in other outcome measures. Participants appeared highly enthusiastic about participating in the study and regularly attended all the sessions. Although without a randomized control trial, it is difficult to ascertain whether the enhanced rehabilitation gain is primarily because of BCI neurofeedack, the positive gains in outcome measures demonstrate the potential and feasibility of using BCI for post-stroke rehabilitation.","Brain computer interfaces,
Neurofeedback,
Focusing,
Electroencephalography,
Protocols,
Gain measurement,
Neural engineering,
Intelligent systems,
Testing,
Pattern analysis"
A shadow zone aware routing protocol for acoustic underwater sensor networks,"Shadow zone and delay aware routing (SZODAR) scheme is a practical and distributed protocol that can find routes around shadow zones. In SZODAR, sensor nodes can raise or lower their acoustic transceivers to a depth such that shadow zones of the neighboring nodes are avoided. This paper presents the protocol working principle and preliminary calculations of the engineering parameters.","Routing protocols,
Underwater acoustics,
Acoustic sensors,
Ocean temperature,
Sea surface,
Wireless sensor networks,
Radio frequency,
Isothermal processes,
Transceivers,
Surveillance"
Reweighted Compressive Sampling for image compression,"Compressive Sampling (CS), is an emerging theory which points us a promising direction of designing novel efficient data compression techniques. However, the conventional CS adopts a non-discriminated sampling scheme which usually gives poor performance on realistic complex signals. In this paper we propose a reweighted Compressive Sampling for image compression. It introduces a weighting scheme into the conventional CS framework whose coefficients are determined in encoding side according to the statistics of image signals. Experimental results demonstrate that our proposed method notably outperforms the conventional Compressive Sampling framework in coding performance in the sense that the reconstruction quality is greatly enhanced with same number of measurements and computational complexity.",
Modeling and analysis of bandwidth-inhomogeneous swarms in BitTorrent,"A number of analytical models exists that capture various properties of the BitTorrent protocol. However, until now virtually all of these models have been based on the assumption that the peers in the system have homogeneous bandwidths. As this is highly unrealistic in real swarms, these models have very limited applicability. Most of all, these models implicitly ignore BitTorrent's most important property: peer selection based on the highest rate of reciprocity. As a result, these models are not suitable for understanding or predicting the properties of real BitTorrent networks. Furthermore, they are hardly of use in the design of realistic BitTorrent simulators and new P2P protocols. In this paper, we extend existing work by presenting a model of a swarm in BitTorrent where peers have arbitrary upload and download bandwidths. In our model we group peers with (roughly) the same bandwidth in classes, and then analyze the allocation of upload slots from peers in one class to peers in another class. We show that our model accurately predicts the bandwidth clustering phenomenon observed experimentally in other work, and we analyze the resulting data distribution in swarms. We validate our model with experiments using real BitTorrent clients. Our model captures the effects of BitTorrent's well-known ‘tit-for-tat’ mechanism in bandwidth-inhomogeneous swarms and provides an accurate mathematical description of the resulting dynamics.","Bandwidth,
Protocols,
Predictive models,
Mathematical model,
Peer to peer computing,
Computer science,
Internet,
Mechanical factors,
Fluid dynamics,
Analytical models"
Recursion in reconfigurable computing: A survey of implementation approaches,"Reconfigurable systems are widely used nowadays to increase performance of computationally intensive applications. There exist a lot of synthesis tools that automatically generate customized hardware circuits from specifications in both high-level and hardware description languages. However, such tools have a limited applicability because they are unable to handle recursive functions whereas it is known that recursion is a powerful problem-solving method widely used in computer science. Therefore a great deal of research effort is aimed at efficient implementation of recursion in reconfigurable hardware. This paper presents the state of the art in this area. The existing proposals are described, analyzed, and compared according to such criteria as level of parallelism supported, approach to concurrency, ease of use, availability of automated high-level synthesis tools, etc.","Computer applications,
Circuit synthesis,
Hardware design languages,
Problem-solving,
Computer science,
Proposals,
Parallel processing,
Concurrent computing,
Availability,
High level synthesis"
A wireless batteryless in vivo EKG and body temperature sensing microsystem with adaptive RF powering for genetically engineered mice monitoring,"A wireless, batteryless, and implantable EKG and core body temperature sensing microsystem with adaptive RF powering for untethered genetically engineered mice real-time monitoring is designed, implemented, and in vivo characterized. A packaged microsystem, exhibiting a total size of 9 mm x 7 mm x 3 mm with a weight of 400 mg including a pair of stainless steel EKG electrodes, is implanted in a mouse abdomen for real-time monitoring. A low power 2 mm x 2 mm ASIC, consisting of an EKG amplifier, a PTAT temperature sensor, an RF power sensing circuit, an RF-DC power converter, an 8-bit ADC, digital control circuitry, and a 433 MHz FSK transmitter, is powered by an adaptively controlled external RF energy source at 4 MHz to ensure a stable 2V supply with 156μA current driving capability for the overall microsystem.","Wireless sensor networks,
In vivo,
Temperature sensors,
Radio frequency,
Genetic engineering,
Mice,
Monitoring,
Circuits,
Packaging,
Steel"
Multi-class filter bank common spatial pattern for four-class motor imagery BCI,"This paper investigates the classification of multi-class motor imagery for electroencephalogram (EEG)-based Brain-Computer Interface (BCI) using the Filter Bank Common Spatial Pattern (FBCSP) algorithm. The FBCSP algorithm classifies EEG measurements from features constructed using subject-specific temporal-spatial filters. However, the FBCSP algorithm is limited to binary-class motor imagery. Hence, this paper proposes 3 approaches of multi-class extension to the FBCSP algorithm: One-versus-Rest, Pair-Wise and Divide-and-Conquer. These approaches decompose the multi-class problem into several binary-class problems. The study is conducted on the BCI Competition IV dataset IIa, which comprises single-trial EEG data from 9 subjects performing 4-class motor imagery of left-hand, right-hand, foot and tongue actions. The results showed that the multi-class FBCSP algorithm could extract features that matched neurophysiological knowledge, and yielded the best performance on the evaluation data compared to other international submissions.","Filter bank,
Electroencephalography,
Signal processing algorithms,
Tongue,
Frequency,
Foot,
Feature extraction,
Brain computer interfaces,
USA Councils,
Wideband"
Distance Learning in Discriminative Vector Quantization,"Discriminative vector quantization schemes such as learning vector quantization (LVQ) and extensions thereof offer efficient and intuitive classifiers based on the representation of classes by prototypes. The original methods, however, rely on the Euclidean distance corresponding to the assumption that the data can be represented by isotropic clusters. For this reason, extensions of the methods to more general metric structures have been proposed, such as relevance adaptation in generalized LVQ (GLVQ) and matrix learning in GLVQ. In these approaches, metric parameters are learned based on the given classification task such that a data-driven distance measure is found. In this letter, we consider full matrix adaptation in advanced LVQ schemes. In particular, we introduce matrix learning to a recent statistical formalization of LVQ, robust soft LVQ, and we compare the results on several artificial and real-life data sets to matrix learning in GLVQ, a derivation of LVQ-like learning based on a (heuristic) cost function. In all cases, matrix adaptation allows a significant improvement of the classification accuracy. Interestingly, however, the principled behavior of the models with respect to prototype locations and extracted matrix dimensions shows several characteristic differences depending on the data sets.",
Clustering patient length of stay using mixtures of Gaussian models and phase type distributions,Gaussian mixture distributions and Coxian phase type distributions have been popular choices model based clustering of patients' length of stay data. This paper compares these models and presents an idea for a mixture distribution comprising of components of both of the above distributions. Also a mixed distribution survival tree is presented. A stroke dataset available from the English Hospital Episode Statistics database is used as a running example.,"Hospitals,
Data engineering,
Statistical distributions,
Databases,
Probability distribution,
Distributed computing,
Computer science,
Aging,
Stochastic processes,
Probability density function"
"Period-Different
m
-Sequences With at Most Four-Valued Cross Correlation","This paper follows the recent work of Helleseth, Kholosha, Johansen, and Ness to study the cross correlation between an m -sequence of period 2m - 1 and the d-decimation of an m-sequence of a shorter period 2n - 1 for an even number m = 2n. Assuming that d satisfies d(2l + 1) = 2i (mod 2n - 1) for some l > 0 and i > 0, it is proved that the cross correlation takes on either exactly three or four values depending on whether I and n are coprime or not. The distribution of the cross-correlation values is also completely determined. Our results theoretically confirm the numerical data by Ness and Helleseth. It is conjectured that there are no other decimations that give at most four-valued cross correlation apart from the ones proved here.","Laboratories,
Mathematics,
Information security,
Equations,
Galois fields,
Councils,
Informatics,
Computer science,
Polynomials"
Using AR to support cross-organisational collaboration in dynamic tasks,"This paper presents a study where Augmented Reality (AR) technology has been used as a tool for supporting collaboration between the rescue services, the police and military personnel in a crisis management scenario. There are few studies on how AR systems should be designed to improve cooperation between actors from different organizations while at the same time support individual needs. In the present study an AR system was utilized for supporting joint planning tasks by providing organisation-specific views of a shared working. The study involved a simulated emergency event conducted in close to real settings with representatives from the organisations for which the system is developed. As a baseline, a series of trials without the AR system was carried out. Results show that the users were positive towards the AR system, and would like to use it in real work. They also experience some performance benefits of using the AR system compared to their traditional tools. Finally, the problem of designing for collaborative work as well as the benefits of using an iterative design processes is discussed.","Collaboration,
Collaborative work,
Collaborative tools,
Crisis management,
Command and control systems,
Augmented reality,
Personnel,
Terminology,
Paper technology,
Information science"
Automatic prosodic events detection using syllable-based acoustic and syntactic features,"Automatic prosodic event detection is important for both speech understanding and natural speech synthesis since prosody provides additional information over the short-term segmental features and lexical representation of an utterance. Similar to previous work, this paper focuses on automatic detection of coarse level representation of pitch accents, intonational phrase boundaries (IPB), and break indices. We exploit various classifiers and identify effective feature sets to improve performance of prosodic event detection according to acoustic, lexical, and syntactic evidence. our experiments on the Boston University Radio News Corpus show that the neural network classifier achieves the best performance for modeling acoustic evidence, and that support vector machines are more effective for the lexical and syntactic evidence. The combination of the acoustic and the syntactic models yields 89.8% accent detection accuracy, 93.3% IPB detection accuracy, and 91.1% break index detection accuracy. Compared with previous work, the IPB perfromance is similar, whereas the results for accent and break index detection are significantly better.",Event detection
Dynamic analysis and control design of optocoupler-isolated LLC series resonant converters with wide input and load variations,LLC series resonant converters for consumer or industrial electronics frequently encounter considerable changes in both input voltage and load current requirements. This paper presents theoretical and practical details involved with the dynamic analysis and control design of LLC series resonant dc-to-dc converters operating with wide input and load variations. The accuracy of dynamic analysis and validity of control design are confirmed with both computer simulations and experimental measurements.,"Control design,
Resonance,
Load management,
Voltage,
Industrial electronics,
DC-DC power converters,
Circuits,
Feedback,
Inverters,
Chromium"
Integrating sustainability in decision-making processes: A modelling strategy,"One of the most difficult problems that humanity currently faces is the sustainable development of our society, i.e. how to meet the needs of the present without compromising the ability of future generations to meet their own needs. Solving this challenge requires a multidisciplinary approach where researchers with different backgrounds combine their efforts. As software engineers, we can contribute to these efforts by offering existing software requirements modeling and analysis techniques as a means to integrate sustainability requirements in decision-making processes. In this paper, we use a popular language for modeling early requirements as a way to visualize the impact of alternative options on sustainability goals and to analyze the conflicts between sustainability and other problem-specific objectives. We apply this idea to the decision-making activities pertaining to the organization of the ICSE'09 conference itself.","Decision making,
Data analysis,
System testing,
Software measurement,
Software testing,
Humans,
Software systems,
Information analysis,
Aggregates,
Association rules"
Agent Smith: Towards an evolutionary rule-based agent for interactive dynamic games,"The goal of this project is to develop an agent to play the first-person shooter game Unreal Tournament 2004 [1], a fast-paced and dynamic environment that demands that the agent must be capable of making decisions quickly. An additional goal of this project is to explore evolutionary computation as a means for learning the rule sets used to control the game-playing agent. The agent's behavior is controlled by a rule-based system, which looks at multiple high-level conditions, such as whether the agent is weak, and determines a single high-level action, such as to head for the nearest known healing source. Using an evolutionary computation approach, in which the behavior is evolved over a number of generations, the agent learns increasingly better strategies for its environment. Through the work in this project, we are exploring several research questions, including the development of successful vocabulary of high-level conditions and actions for the rule set, the challenges of using the evolutionary process to hone a rule set, and the effects of using some expert knowledge in combination with the evolutionary process.","Evolutionary computation,
Games,
Knowledge based systems,
Avatars,
Control systems,
Vocabulary,
Toy industry,
Large-scale systems,
Humans,
Level set"
"A Fast and Precise Static Loop Analysis Based on Abstract Interpretation, Program Slicing and Polytope Models","A static loop analysis is a program analysis computing loop iteration counts. This information is crucial for different fields of applications. In the domain of compilers, the knowledge about loop iterations can be exploited for aggressive loop optimizations like Loop Unrolling. A loop analyzer also provides static information about code execution frequencies which can assist feedback-directed optimizations. Another prominent application is the static worst-case execution time (WCET) analysis which relies on a safe approximation of loop iteration counts.In this paper, we propose a framework for a static loop analysis based on Abstract Interpretation, a theory of a sound approximation of program semantics. To accelerate the analysis, we preprocess the analyzed code using Program Slicing, a technique that removes statements irrelevant forthe loop analysis. In addition, we introduce a novel polytope-based loop evaluation that further significantly reduces the analysis time. The efficiency of our loop analyzer is evaluated on a large number of benchmarks. Results show that 99% of the considered loops could be successfully analyzed in an acceptable amount of time. This study points out that our methodology is best suited for real-world problems.","Optimizing compilers,
Frequency,
Information analysis,
Program processors,
Computer science,
Embedded system,
Embedded computing,
Application software,
Acceleration,
Pipeline processing"
Integrating Co-design Practices into the Development of Mobile Science Collaboratories,"Scientific practices increasingly incorporate sensors for data capture, information visualization for data analysis, and low-cost mobile devices for field-based inquiries incorporating open web standards. While a broad range of design approaches for developing technology-enhanced learning has been used by researchers and practitioners for the last 15 years, significant challenges for educational use remain as new technologies and user experiences continually evolve outside the classroom. We focus on the specific design challenge of how to initiate the co-design process together with teachers, researchers, scientists, designers, and developers in order to devise and develop mobile science collaboratories that support open inquiry-based learning in ecology education. The outcomes presented in this paper point towards the need for additional methods to support co-design that take into consideration future user experiences needed for developing and implementing these types of learning activities","Collaboration,
Educational technology,
Collaborative work,
Environmental factors,
Data visualization,
Data analysis,
Technological innovation,
Educational products,
Process design,
Collaborative tools"
A hybrid FEC-ARQ protocol for low-delay lossless sequential data streaming,"Interactive Internet Applications that rely on sequential streams for lossless data exchange often use retransmission protocols (e.g. TCP) for reliability and the guarantee of sequential data ordering. More so than for bulk file transfer or media delivery, lossless sequential streaming poses an even greater challenge for the common problem cases of retransmission protocols, such as lossy links or long network paths, manifesting as significant latency in the interactive user experience. We propose a hybrid FEC-ARQ protocol built on a packet streaming code that reduces to a simple strategy over sending or resending original data packets or check packets combining undecoded packets, based on actual network conditions. Experimental results show that our proposed protocol can significantly improve the total delay over retransmission and other schemes that use FEC, under a range of bandwidth and loss scenarios.","Protocols,
Delay,
Internet,
Automatic repeat request,
Cloud computing,
Forward error correction,
Web server,
Throughput,
Decoding,
Computer science"
A brief survey on current RFID applications,"Radio Frequency Identification (RFID) is the next generation wireless communication technology applicable to a wide range of application areas. There are an increasing number of retailers, banks, traffic managements, exhibitions and logistic providers practicing this new technology to their products and services. Therefore, it brings both opportunities and challenges to RFID researchers. In this paper, we provide a brief survey on RFID applications and suggest some opportunities in intelligent RFID applications.","Radiofrequency identification,
RFID tags,
Application software,
Machine learning,
Cybernetics,
Computer applications,
Computer science,
Electronic mail,
Wireless communication,
Communications technology"
Online Anomaly Detection Using KDE,"Large backbone networks are regularly affected by a range of anomalies. This paper presents an online anomaly detection algorithm based on Kernel Density Estimates. The proposed algorithm sequentially and adaptively learns the definition of normality in the given application, assumes no prior knowledge regarding the underlying distributions, and then detects anomalies subject to a user-set tolerance level for false alarms. Comparison with the existing methods of Geometric Entropy Minimization, Principal Component Analysis and OneClass Neighbor Machine demonstrates that the proposed method achieves superior performance with lower complexity.","Kernel,
Machine learning algorithms,
Entropy,
Minimization methods,
Computer science,
Spine,
Detection algorithms,
Application software,
Principal component analysis,
High-speed networks"
View management of annotations for wearable augmented reality,"In annotation overlay applications using augmented reality (AR), view management is widely used for improving readability and intelligibility of the annotations. In order to recognize the visible portions of objects in the user's view, the positions, orientations, and shapes of the objects should be known in the case of conventional view management methods. However, it is difficult for a wearable AR system to obtain the positions, orientations and shapes of objects because the target object is usually moving or non-rigid. In this paper, we propose a view management method to overlay annotations of moving or non-rigid objects for networked wearable AR. The proposed method obtains positions and shapes of target objects via a network in order to estimate the visible portions of the target objects in the user's view. Annotations are located by minimizing penalties related to the overlap of an annotation, occlusion of target objects, length of a line between the annotation and the target object, and distance of the annotation in sequential frames. Through experiments, we have proven that the prototype system can correctly provide each user with annotations on multiple users of wearable AR systems.",
Year,,
Supporting Computer Science Curriculum: Exploring and Learning Linked Lists with iList,"We developed two versions of a system, called iList, that helps students learn linked lists, an important topic in computer science curricula. The two versions of iList differ on the level of feedback they can provide to the students, specifically in the explanation of syntax and execution errors. The system has been fielded in multiple classrooms in two institutions. Our results indicate that iList is effective, is considered interesting and useful by the students, and its performance is getting closer to the performance of human tutors. Moreover, the system is being developed in the context of a study of human tutoring, which is guiding the evolution of iList with empirical evidence of effective tutoring.","Computer science,
Data structures,
Humans,
Artificial intelligence,
Data mining,
Probability density function,
Heuristic algorithms"
Generalized multi-ethnic face age-estimation,"Age estimation from digital pictures of the face is a very promising research field that is now receiving wide attention. As with any good research problem, face age-estimation is wrought with many challenging interactions that cannot easily be separated out. In general, aging patterns are well understood for all humans, however, these patterns become confounded by intrinsic factors of genetics, gender differences, and ethnic deviations and, equally as important, extrinsic factors of the environment and behavior choices (i.e. sun exposure, drugs, cigarettes, etc). This novel work focuses on the development of a generalized multi-ethnic age-estimation technique — the first of its kind. In addition to the novelty of this approach, the system's overall performance measure (MAE) is “on par” with algorithms that are tuned for a specific ethnic group. Further, the proposed system performance proves to be far more stable across age than the best published results.","Active appearance model,
Face detection,
Aging,
Mathematics,
Statistics,
Input variables,
Human computer interaction,
Principal component analysis,
Shape,
Encoding"
System power consumption minimization for multichannel communications using cognitive radio,"Power consumption has been a significant issue for many mobile and wireless devices, especially those with high rate applications. This paper presents a methodology and framework to minimize system power consumption for multichannel communications using cognitive radio (CR) based on the application quality of service requirement, the channel condition, and the radio capabilities and characteristics. The CR framework enables an adaptation process that is aware of the radio (component) capabilities and characteristics. This paper mathematically formulates a system power consumption minimization problem under a rate constraint for multichannel communications and develops numerical solutions. Simulation results show that the knowledge of the radio capabilities and characteristics can help to reduce system power consumption significantly (e.g., up to 55% for a multichannel system with Class A power amplifiers).","Energy consumption,
Cognitive radio,
Chromium,
Quality of service,
MIMO,
Mobile communication,
Power amplifiers,
Helium,
Power engineering computing,
Power engineering and energy"
The Design of a Conceptual Framework and Technical Infrastructure for Model Management Language Engineering,"Model management is the discipline of managing artefacts used in Model-Driven Engineering (MDE). A model management framework defines and implements the operations (such as transformation or code generation) required to manipulate MDE artefacts. Modern approaches to model management generally implement these operations via domain-specific languages (DSLs). This paper presents and compares the principles behind three approaches to implementing DSLs for model management and identifies some of the key differences between DSL engineering in general and for model management. It then shows how theory relates to practice by illustrating how DSL design and implementation approaches have been used in practice to build working languages from the Epsilon model management framework. A set of questions for guiding the development of new model management DSLs is summarised, and data on development costs for the different approaches is presented.",
Malware detection using machine learning,"We propose a versatile framework in which one can employ different machine learning algorithms to successfully distinguish between malware files and clean files, while aiming to minimise the number of false positives. In this paper we present the ideas behind our framework by working firstly with cascade one-sided perceptrons and secondly with cascade kernelized one-sided perceptrons. After having been successfully tested on medium-size datasets of malware and clean files, the ideas behind this framework were submitted to a scaling-up process that enable us to work with very large datasets of malware and clean files.","Machine learning,
Testing,
Computer science,
Viruses (medical),
Learning systems,
Association rules,
Hidden Markov models,
Information technology,
Gas discharge devices,
Machine learning algorithms"
Explore/Exploit Schemes for Web Content Optimization,"We propose novel multi-armed bandit (explore/exploit) schemes to maximize total clicks on a content module published regularly on Yahoo! Intuitively, one can ``explore'' each candidate item by displaying it to a small fraction of user visits to estimate the item's click-through rate (CTR), and then ``exploit'' high CTR items in order to maximize clicks. While bandit methods that seek to find the optimal trade-off between explore and exploit have been studied for decades, existing solutions are not satisfactory for web content publishing applications where dynamic set of items with short lifetimes, delayed feedback and non-stationary reward (CTR) distributions are typical. In this paper, we develop a Bayesian solution and extend several existing schemes to our setting. Through extensive evaluation with nine bandit schemes, we show that our Bayesian solution is uniformly better in several scenarios. We also study the empirical characteristics of our schemes and provide useful insights on the strengths and weaknesses of each. Finally, we validate our results with a ``side-by-side'' comparison of schemes through live experiments conducted on a random sample of real user visits to Yahoo!",
UAV intelligent path planning for Wilderness Search and Rescue,"In the priority search phase1 of Wilderness Search and Rescue, a probability distribution map is created. Areas with higher probabilities are searched first in order to find the missing person in the shortest expected time. When using a UAV to support search, the onboard video camera should cover as much of the important areas as possible within a set time. We explore several algorithms (with and without set destination) and describe some novel techniques in solving this problem and compare their performances against typical WiSAR scenarios. This problem is NP-hard, but our algorithms yield high quality solutions that approximate the optimal solution, making efficient use of the limited UAV flying time.","Unmanned aerial vehicles,
Path planning,
Cameras,
Probability distribution,
Intelligent robots,
Computer science,
USA Councils,
Time factors,
Solids,
Object detection"
Lane boundary and curb estimation with lateral uncertainties,"This paper describes an algorithm for estimating lane boundaries and curbs from a moving vehicle using noisy observations and a probabilistic model of curvature. The primary contribution of this paper is a curve model we call lateral uncertainty, which describes the uncertainty of a curve estimate along the lateral direction at various points on the curve, and does not attempt to capture uncertainty along the longitudinal direction of the curve. Additionally, our method incorporates expected road curvature information derived from an empirical study of a real road network. Our method is notable in that it accurately captures the geometry of arbitrarily complex lane boundary curves that are not well approximated by straight lines or low-order polynomial curves. Our method operates independently of the direction of travel of the vehicle, and incorporates sensor uncertainty associated with individual observations. We analyze the benefits and drawbacks of the approach, and show results of our algorithm applied to real world data sets","Uncertainty,
Shape,
Geometry,
Remotely operated vehicles,
Computer vision,
Road vehicles,
Sensor systems,
Road safety,
Vehicle safety,
Mobile robots"
Model-Checking BNDC Properties in Cyber-Physical Systems,"In Cyber-physical systems, which are the integrations of computational and physical processes, it is hard to realize certain security properties. Fundamentally, physically observable behavior leads to violations of confidentiality. We focus on analyzing certain non-interference based security properties to ensure that interactions between the cyber and physical processes preserve confidentiality. A considerable barrier to this analysis is representing the physical system’s interactions. In this paper, these physical system properties are encoded into a discrete event system and the combined Cyber-physical system is described using the process algebra SPA. The model checker, CoPS shows BNDC (Bisimulation based Non Deducibility on Compositions) properties,which are a variant of non-interference properties, to check the system’s security against all high level potential interactions. We consider a model problem of invariant pipeline flow to examine the BNDC properties and their applicability for cyber-physical systems.","Physics computing,
Computer security,
Algebra,
Intelligent systems,
Computer networks,
Embedded computing,
Information security,
Computer applications,
Application software,
Computer science"
Adventures in archiving and using three years of webcam images,"Recent descriptions of algorithms applied to images archived from webcams tend to underplay the challenges in working with large data sets acquired from uncontrolled webcams in real environments. In building a database of images captured from 1000 webcams, every 30 minutes for the last 3 years, we observe that these cameras have a wide variety of failure modes. This paper details steps we have taken to make this dataset more easily useful to the research community, including (a) tools for finding stable temporal segments, and stabilizing images when the camera is nearly stable, (b) visualization tools to quickly summarize a years worth of image data from one camera and to give a set of exemplars that highlight anomalies within the scene, and (c) integration with LabelMe, allowing labels of static features in one image of a scene to propagate to the thousands of other images of that scene. We also present proof-of-concept algorithms showing how this data conditioning supports several problems in inferring properties of the scene from image data.","Layout,
Cameras,
Jacobian matrices,
Image databases,
Image segmentation,
Computer science,
Data engineering,
Data visualization,
Data mining,
Calibration"
Context assessment strategies for Ubiquitous Robots,"This paper presents an architecture for context-aware Ubiquitous Robotics applications, where mobile robots cooperate with intelligent environments to fulfill their tasks. Specifically, the work is focused on distributed knowledge representation issues and context assessment strategies, and introduces a technique for on-line context recognition in highly dynamic environments. Experimental validation, performed in a civillian hospital building, is described and discussed.","Intelligent robots,
Mobile robots,
Context awareness,
Robot sensing systems,
Intelligent sensors,
Intelligent actuators,
Cognitive robotics,
Robotics and automation,
Knowledge representation,
Context modeling"
Quantum Realization of Multiple-Valued Feynman and Toffoli Gates without Ancilla Input,"Multiple-valued Feynman and Toffoli gates are used in GFSOP based synthesis of quantum logic circuits. These gates are macro-level gates and need to be realized using technology dependent primitive gates. In this paper, we present ancilla input free architectures for realization of d-valued (d≫=3 ) Feynman and n-qudit (n≫=3 ) Toffoli gates on the top of liquid ion-trap realizable Muthukrishnan-Stroud gates. The proposed architectures can be used for any d if GF(d) can be constructed. We show realization examples of ternary, quaternary, and quinary Feynman gates, and 3 and 4-qudit Toffoli gates. The present realizations require either less or equal primitive gates than the previously reported realizations. Moreover, in contrast to the earlier realizations, the present Toffoli gate realizations do not require any ancilla input, which reduce the register width of a synthesized quantum logic circuit.",
"Mobile Robot Navigation Control in Moving Obstacle Environment Using Genetic Algorithm, Artificial Neural Networks and A* Algorithm","The pace of development and automation urge the need of robots controlling much of the work which used to be done mainly by humans. The modern technology has emphasized on the need to move a robot in an environment which is dynamically changing. An example of such an application may be the use of robots in industry to carry tools and other materials from one place to other. Since many robots would be working together, we need to ensure a collision free navigation plan for each of the robots.In this paper we find out the nearly most optimal path of the robot using Genetic, ANN and A* algorithms at each instant of time of robot travel. It may be used by the industry to send robots for surveys, data acquisition, doing specific work etc. The collision free movement of robot in a moving obstacle environment can be used to move robot in a world of robots.Results show that all 3 algorithms are able to move the robot without any collisions.",
Towards a Next-Generation Matrix Library for Java,"Matrices are essential in many fields of computer science, especially when large amounts of data must be handled efficiently. Despite this demand for matrix software, we were unable to find a Java library which was flexible enough to match all our needs. In this paper, we present the Universal Java Matrix Package (UJMP), an innovative software architecture in Java to store and process matrices with interfaces to external data sources such as Excel files or SQL-databases, allowing to handle data which would not fit into main memory.In contrast to all other approaches which we are aware of, our package is the only one to support very large matrices with up to 2^63 rows or columns. In addition, the use of variable argument lists provides a convenient way for accessing multi-dimensional data without the need to specify dimensionality at compile time. Arbitrary data types be handled through the use of Java Generics. Another key feature is the strict separation of interfaces and classes, making data storage implementation and math engine exchangeable, at runtime. This flexible architecture allows the user to decide whether operations should be optimized for speed or memory usage and makes the system easily extendable through existing libraries, incorporating their individual strengths.","Java,
Sparse matrices,
Packaging,
Software libraries,
Software packages,
Computer science,
Matrix decomposition,
Linear algebra,
Computer applications,
Application software"
Improving API Usage through Automatic Detection of Redundant Code,"Software projects often rely on third-party libraries made accessible through Application Programming Interfaces (APIs). We have observed many cases where APIs are used in ways that are not the most effective. We developed a technique and tool support to automatically detect such patterns of API usage in software projects. The main hypothesis underlying our technique is that client code imitating the behavior of an API method without calling it may not be using the API effectively because it could instead call the method it imitates. Our technique involves analyzing software systems to detect cases of API method imitations. In addition to warning developers of potentially re-implemented API methods, we also indicate how to improve the use of the API. Applying our approach on 10 Java systems revealed over 400 actual cases of potentially suboptimal API usage, leading to many improvements to the quality of the code we studied.","Software libraries,
Java,
Application software,
Software systems,
Software engineering,
Computer science,
Automatic programming,
Software tools,
Functional programming,
Degradation"
Transcranial Shear-Mode Ultrasound: Assessment of Imaging Performance and Excitation Techniques,"Transcranial ultrasound imaging is limited by poor acoustic windows and skull induced distortions to the beam. Shear waves in the skull have a better impedance match with longitudinal waves in water and thereby produce a more coherent focus inside the skull. This study presents work on an imaging technique that utilizes shear-wave propagation through the skull. The pulse-echo lateral distortion introduced by the skull was analyzed by imaging a point scatterer behind ex vivo human craniums at 1 MHz. Brightness images of the target obtained with either shear-mode or conventional longitudinal-mode transmission in the bone were assessed to quantify lateral resolution. As compared to longitudinal-mode transmission, it was found that the use of shear-mode resulted in improved localization along the propagation (depth) axis at the expense of degraded lateral resolution. The signal-to-noise ratio (SNR) limitations introduced by severe attenuation of shear-waves in the skull were overcome with frequency modulated (FM) coded excitations. This gain in SNR was exchanged with resolution and used for compensation of frequency-dependent attenuation in the skull, resulting in a greater than 20% improvement in lateral resolution for both modes of transcranial transmission. The results are an important step towards enhancing the quality of transcranial sonography.",
Interactive level set segmentation for image-guided therapy,"Image-guided therapy procedures require the patient to remain still throughout the image acquisition, data analysis and therapy. This imposes a tight time constraint on the over-all process. Automatic extraction of the pathological regions prior to the therapy can be faster than the customary manual segmentation performed by the physician. However, the image data alone is usually not sufficient for reliable and unambiguous computerized segmentation. Thus, the oversight of an experienced physician remains mandatory. We present a novel segmentation framework, that allows user feedback. A few mouse-clicks of the user, discrete in nature, are represented as a continuous energy term that is incorporated into a level-set functional. We demonstrate the proposed method on MR scans of uterine fibroids acquired prior to focused ultrasound ablation treatment. The experiments show that with a minimal user input, automatic segmentation results become practically identical to manual expert segmentation.","Level set,
Image segmentation,
Medical treatment,
Data analysis,
Time factors,
Data mining,
Pathology,
Feedback,
Focusing,
Ultrasonic imaging"
A flexible pressure monitoring system for pressure ulcer prevention,"Pressure ulcers are painful sores that arise from prolonged exposure to high pressure points, which restricts blood flow and leads to tissue necrosis. This is a common occurrence among patients with impaired mobility, diabetics and the elderly. In this work, a flexible pressure monitoring system for pressure ulcer prevention has been developed. The prototype consists of 99 capacitive pressure sensors on a 17-cm×22-cm sheet which is flexible in two dimensions. Due to its low cost, the sensor sheet can be disconnected from the reusable electronics and be disposed of after use, suitable for a clinical setting. Each sensor has a resolution of better than 2-mmHg and a range of 50-mmHg and offset is calibrated in software. Realtime pressure data is displayed on a computer. A maximum sampling rate of 12-Hz allows for continuous monitoring of pressure points.","Patient monitoring,
Blood flow,
Lead,
Diabetes,
Senior citizens,
Prototypes,
Capacitive sensors,
Costs,
Computer displays,
Sampling methods"
A novel and efficient unlinkable secret handshakes scheme,"An unlinkable secret handshakes is a two-party authentication protocol which protects the privacy of parties' affiliation and identity to everyone except for the intended parties. However, we find that the latest solution proposed by Jarecki et al. is not very efficient. In particular, each party requires O(log n) exponentiations where n is the number of parties affiliated with a single organization. In this paper, we propose a novel unlinkable secret handshakes protocol. Compared with Jarecki et al's scheme, our proposal is simple intuitionally and requires only constant exponentiations.",
Optimized assignment of developers for fixing bugs an initial evaluation for eclipse projects,"Decisions on “Who should fix this bug” have substantial impact on the duration of the process and its results. In this paper, optimized strategies for the assignment of the “right” developers for doing the “right” task are studied and the results are compared to manual (called ad hoc) assignment. The quality of assignment is measured by the match between requested (from bugs) and available (from developers) competence profile. Different variants of Greedy search with varying parameter of look-ahead time are studied. The quality of the results has been evaluated for nine milestones of the open source Eclipse JDT project. The optimized strategies with largest look ahead time are demonstrated to be substantially better than the ad hoc solutions in terms of the quality of the assignment and the number of bugs which can be fixed within the given time interval.","Computer bugs,
Software engineering,
Software measurement,
Resource management,
Computer science,
Human resource management,
Laboratories,
Scheduling,
Writing,
Software debugging"
Multi-layered friendship modeling for location-based Mobile Social Networks,"Location-based Mobile Social Networks (MSNs) are becoming increasingly popular given the success of Online Social Networks (OSNs), such as Facebook and MySpace, and recent availability of open mobile platforms, such as Apple iPhones and Google Android phones. MSNs extend existing OSNs by allowing a user to know when her friends are around and by providing the ability to meet new people who share her interests. There are few studies, however, on how users are connected through these emerging location-based MSNs. In this paper, we present analysis results of a commercial MSN for which we quantified the correlation between users' friendship with their mobility characteristics, social graph properties, and user profiles. The evaluation of the derived model from the empirical traces suggests that the model-based friend recommendation is effective, and its performance is better than well-known Naive Bayes classifier and J48 decision tree algorithms. To the best of our knowledge, this paper presents the first study that models the friendship connections over a real-world location-based MSN.",
Hierarchical On-line Arabic Handwriting Recognition,"In this paper, we present a multi-level recognizer for online Arabic handwriting. In Arabic script (handwritten and printed), cursive writing – is not a style – it is an inherent part of the script. In addition, the connection between letters is done with almost no ligatures, which complicates segmenting a word into individual letters. In this work, we have adopted the holistic approach and avoided segmenting words into individual letters. To reduce the search space, we apply a series of filters in a hierarchical manner. The earlier filters perform light processing on a large number of candidates, and the later filters perform heavy processing on a small number of candidates. In the first filter, global features and delayed strokes patterns are used to reduce candidate word-part models. In the second filter, local features are used to guide a dynamic time warping (DTW) classification. The resulting k top ranked candidates are sent for shape context based classifier, which determines the recognized word-part. In this work, we have modified the classic DTW to enable different costs for the different operations and control their behavior. We have performed several experimental tests and have received encouraging results.","Handwriting recognition,
Filters,
Writing,
Shape,
Computer science,
Delay,
Costs,
Testing,
Keyboards,
Personal digital assistants"
Language Model Integration for the Recognition of Handwritten Medieval Documents,"Building recognition systems for historical documents is a difficult task. Especially, when it comes to medieval scripts. The complexity is mainly affected by the poor quality and the small quantity of the data available. In this paper we apply an HMM based recognition system to medieval manuscripts from the 13th century written in Middle High German. The recognition system, which was originally developed for modern scripts, has been adapted to medieval scripts. Beside the data processing, one of the major challenges is to create a suitable language model. Because of the lack of appropriate independent text corpora for medieval languages, the language model has to be created on the base of a rather small number of manuscripts only. Due to the small size of the corpus, optimizing the language model parameters can quickly lead to the problem of overfitting. In this paper we describe a strategy to integrate all available information into the language model and to optimize the language model parameters without suffering from this problem.","Handwriting recognition,
Natural languages,
Hidden Markov models,
Software libraries,
Writing,
Text analysis,
Data processing,
Digital images,
Computer science,
Mathematics"
A clustering multi-objective evolutionary algorithm based on orthogonal and uniform design,"Designing efficient algorithms for difficult multi-objective optimization problems is a very challenging problem. In this paper a new clustering multi-objective evolutionary algorithm based on orthogonal and uniform design is proposed. First, the orthogonal design is used to generate initial population of points that are scattered uniformly over the feasible solution space, so that the algorithm can evenly scan the feasible solution space once to locate good points for further exploration in subsequent iterations. Second, to explore the search space efficiently and get uniformly distributed and widely spread solutions in objective space, a new crossover operator is designed. Its exploration focus is mainly put on the sparse part and the boundary part of the obtained non-dominated solutions in objective space. Third, to get desired number of well distributed solutions in objective space, a new clustering method is proposed to select the non-dominated solutions. Finally, experiments on thirteen very difficult benchmark problems were made, and the results indicate the proposed algorithm is efficient.","Evolutionary computation,
Algorithm design and analysis,
Scattering,
Clustering algorithms,
Design methodology,
Clustering methods,
Hypercubes,
Space exploration,
Computer science,
Design for experiments"
Depth reconstruction filter for depth coding,Depth images represent the distances of scene elements from a camera in 3D space; their efficient coding is crucial for emerging applications such as free-viewpoint TV and 3D video. An in-loop reconstruction filter that improves the depth-coding performance as well as the rendering quality of virtual views based upon the coded depth is proposed.,
Visualizing the Java heap to detect memory problems,"Many of the problems that occur in long-running systems involve the way that the system uses memory. We have developed a framework for extracting and building a model of the heap from a running Java system. Such a model is only useful if the programmer can extract from it the information they need to understand, find, and eventually fix memory-related problems in their system. This paper describes the visualization strategy we use for interactively displaying the model and related information to achieve these goals.","Visualization,
Java,
Programming profession,
Data mining,
Computer science,
Computer bugs"
Indexed Containers,"We show that the syntactically rich notion of inductive families can be reduced to a core type theory with a fixed number of type constructors exploiting the novel notion of indexed containers. Indexed containers generalize simple containers, capturing strictly positive families instead of just strictly positive types, without having to extend the core type theory. Other applications of indexed containers include data type-generic programming and reasoning about polymorphic functions. The construction presented here has been formalized using the Agda system.","Containers,
Algebra,
Reactive power,
Shape,
Programming,
Data mining,
Probability density function"
Classifying without discriminating,"Classification models usually make predictions on the basis of training data. If the training data is biased towards certain groups or classes of objects, e.g., there is racial discrimination towards black people, the learned model will also show discriminatory behavior towards that particular community. This partial attitude of the learned model may lead to biased outcomes when labeling future unlabeled data objects. Often, however, impartial classification results are desired or even required by law for future data objects in spite of having biased training data. In this paper, we tackle this problem by introducing a new classification scheme for learning unbiased models on biased training data. Our method is based on massaging the dataset by making the least intrusive modifications which lead to an unbiased dataset. On this modified dataset we then learn a non-discriminating classifier. The proposed method has been implemented and experimental results on a credit approval dataset show promising results: in all experiments our method is able to reduce the prejudicial behavior for future classification significantly without loosing too much predictive accuracy.",
Combinatorial Approach for Preventing SQL Injection Attacks,"A combinatorial approach for protecting Web applications against SQL injection is discussed in this paper, which is a novel idea of incorporating the uniqueness of Signature based method and auditing method. The major issue of web application security is the SQL Injection, which can give the attackers unrestricted access to the database that underlie Web applications and has become increasingly frequent and serious. From signature based method standpoint of view, it present a detection mode for SQL injection using pair wise sequence alignment of amino acid code formulated from wab application form parameter sent via web server. On the other hand from the Auditing based method standpoint of view, it analyzes the transaction to find out the malicious access. In signature based method It uses an approach called Hirschberg algorithm, it is a divide and conquer approach to reduce the time and space complexity. This system was able to stop all of the successful attacks and did not generate any false positives.","Data security,
Databases,
Protection,
Application software,
Space technology,
Computer science,
Cryptography,
Intrusion detection,
Counterfeiting,
Amino acids"
The impact of antenna orientation on wireless sensor network performance,"In this paper, we investigate the impact of antenna orientation on the performance of wireless sensor networks. In the experiments conducted using Crossbow MicaZ motes we realized that the antenna orientation has a significant impact on the received signal strength (RSSI). It thus becomes important to incorporate the effects on RSSI due to variations in antenna orientations while performing simulations and analyzing the performance of wireless sensor networks. We have successfully validated the experimental results with an analytical model. The significant deviations have been explained based on other factors that affect the performance of wireless sensor networks, viz. mulitpath, interference etc. However, it must be noticed that we do not generalize the results obtained, as it depends upon the type of antenna used, and in our case, it is a quarter wavelength tilt and swivel antenna that is usually used for MicaZ sensor motes. In order to determine the routing path from one node to the other, we propose Distance and Orientation based Protocol for Energy conservation, or in short DOPE. Using this protocol, the data is routed based on the least energy consuming path. Matlab simulation of sensor network yields that if the data is routed from one node to the other based on antenna orientation consideration, a lot of energy (on an average 30 percent) can be saved as compared to the networks where antenna orientation is not taken into consideration.","Wireless sensor networks,
Receiving antennas,
Transmitting antennas,
Antenna accessories,
Analytical models,
Testing,
Transmitters,
Performance analysis,
Interference,
Routing protocols"
Automated learning for parameter optimization of robotic assembly tasks utilizing genetic algorithms,"A challenge for automating mechanical assembly is that cumulative uncertainties typically exceed part clearances, which makes conventional position-based tactics unsuccessful. Force-based assembly strategies offer a potential solution, although such methods are still poorly understood and can be difficult to program. In this paper, we describe a force-based robotic assembly approach that uses fixed strategies with tunable parameters. A generic assembly strategy suitable for execution on an industrial robot is selected by the programmer. Parameters are then self-tuned empirically by the robot using a genetic-algorithm learning process that seeks to minimize assembly time subject to contact-force limits. Results are presented for two automotive part assembly examples using ABB robots with commercial force-control software, showing that the approach is highly effective and suitable for industrial use.","Robotic assembly,
Genetic algorithms,
Robotics and automation,
Service robots,
Force control,
Orbital robotics,
Industrial control,
Robot control,
Automatic control,
Manufacturing automation"
Building Prioritized Pairwise Interaction Test Suites with Ant Colony Optimization,"Interaction testing offers a stable cost-benefit ratio in identifying faults. But in many testing scenarios, the entire test suite cannot be fully executed due to limited time or cost. In these situations, it is essential to take the importance of interactions into account and prioritize these tests. To tackle this issue, the biased covering array is proposed and the Weighted Density Algorithm (WDA) is developed. To find a better solution, in this paper we adopt ant colony optimization (ACO) to build this prioritized pairwise interaction test suite (PITS). In our research, we propose four concrete test generation algorithms based on Ant System, Ant System with Elitist, Ant Colony System and Max-Min Ant System respectively. We also implement these algorithms and apply them to two typical inputs and report experimental results. The results show the effectiveness of these algorithms.",
A Confidence-Based Dominance Operator in Evolutionary Algorithms for Noisy Multiobjective Optimization Problems,"This paper describes a noise-aware dominance operator for evolutionary algorithms to solve the multiobjective optimization problems (MOPs) that contain noise in their objective functions. This operator takes objective value samples of given two individuals (or solution candidates), estimates the impacts of noise on the samples and determines whether it is confident enough to judge which one is superior/inferior between the two individuals. Since the proposed operator assumes no noise distributions a priori, it is well applicable to various MOPs whose objective functions follow unknown noise distributions. Experimental results show that it operates reliably in noisy MOPs and outperforms existing noise-aware dominance operators.","Evolutionary computation,
Artificial intelligence,
Computer science,
USA Councils,
Equations,
Degradation,
Genetic mutations"
Safety design for medical robots,"The use of robots in medicine is increasing, leading to the call for specific safety standards. This is a challenging endeavor, however, because the patient must usually be placed in the robot’s workspace and the medical staff must frequently interact with the robot. Although specific safety standards for medical robots do not yet exist, there are several medical device standards and well-established principles of risk analysis and safety design that can and should be applied. This paper presents a tutorial overview of safety design for medical robots, starting with a discussion of high-level safety requirements, followed by methods for risk assessment (or hazard analysis) and a brief discussion of some sample safety strategies.","Medical robotics,
Service robots,
Safety devices,
IEC standards,
Fault tolerant systems,
Error correction,
Risk analysis,
Risk management,
Surges,
Feedback control"
Alphabet SOUP: A framework for approximate energy minimization,"Many problems in computer vision can be modeled using conditional Markov random fields (CRF). Since finding the maximum a posteriori (MAP) solution in such models is NP-hard, much attention in recent years has been placed on finding good approximate solutions. In particular, graph-cut based algorithms, such as a-expansion, are tremendously successful at solving problems with regular potentials. However, for arbitrary energy functions, message passing algorithms, such as max-product belief propagation, are still the only resort. In this paper we describe a general framework for finding approximate MAP solutions of arbitrary energy functions. Our algorithm (called Alphabet SOUP for Sequential Optimization for Unrestricted Potentials) performs a search over variable assignments by iteratively solving subproblems over a reduced state-space. We provide a theoretical guarantee on the quality of the solution when the inner loop of our algorithm is solved exactly. We show that this approach greatly improves the efficiency of inference and achieves lower energy solutions for a broad range of vision problems.","Inference algorithms,
Iterative algorithms,
Image segmentation,
Computer vision,
Markov random fields,
Message passing,
Belief propagation,
Search methods,
Minimization methods,
Computer science"
On the Connected k-Coverage Problem in Heterogeneous Sensor Nets: The Curse of Randomness and Heterogeneity,"Coverage is an essential task in sensor deployment for the design of wireless sensor networks. While most existing studies on coverage consider homogeneous sensors, the deployment of heterogeneous sensors represents more accurately the network design for real-world applications. In this paper, we focus on the problem of connected k-coverage in heterogeneous wireless sensor networks. Precisely, we distinguish two deployment strategies, where heterogeneous sensors are either randomly or pseudo-randomly distributed in a field. While the first deployment approach considers a single layer of heterogeneous sensors, the second one proposes a multi-tier architecture of heterogeneous sensors to better address the problems introduced by pure randomness and heterogeneity.",
Does calling structure information improve the accuracy of fault prediction?,"Previous studies have shown that software code attributes, such as lines of source code, and history information, such as the number of code changes and the number of faults in prior releases of software, are useful for predicting where faults will occur. In this study of an industrial software system, we investigate the effectiveness of adding information about calling structure to fault prediction models. The addition of calling structure information to a model based solely on non-calling structure code attributes provided noticeable improvement in prediction accuracy, but only marginally improved the best model based on history and non-calling structure code attributes. The best model based on history and non-calling structure code attributes outperformed the best model based on calling and non-calling structure code attributes.","Accuracy,
History,
Predictive models,
Fault diagnosis,
Costs,
Computer science,
Computer industry,
Software systems,
Information analysis,
Performance analysis"
Some new directions in graph-based semi-supervised learning,"In this position paper, we first review the state-of-the-art in graph-based semi-supervised learning, and point out three limitations that are particularly relevant to multimedia analysis: (1) rich data is restricted to live on a single manifold; (2) learning must happen in batch mode; and (3) the target label is assumed smooth on the manifold. We then discuss new directions in semi-supervised learning research that can potentially overcome these limitations: (i) modeling data as a mixture of multiple manifolds that may intersect or overlap; (ii) online semi-supervised learning that learns incrementally with low computation and memory needs; and (iii) learning spectrally sparse but non-smooth labels with compressive sensing. We give concrete examples in each new direction. We hope this article will inspire new research that makes semi-supervised learning an even more valuable tool for multimedia analysis.","Semisupervised learning,
Laplace equations,
Data analysis,
Concrete,
Transducers,
Training data,
Web pages,
Euclidean distance,
Motion segmentation,
Computer vision"
Scalable RFID Pseudonym Protocol,"In this paper we address the issue of scalability in RFID pseudonym protocols. Many previously proposed protocols suffer from scalability issues because they require a linear search to identify or authenticate a tag. Some RFID protocols, however, only require constant time for tag identification, but, unfortunately, all previously proposed schemes of this type have serious shortcomings. We propose a novel RFID authentication protocol based on the Song-Mitchell protocol, that takes O(1) work to authenticate a tag, and meets the privacy, security and performance requirements identified here. The proposed scheme also supports tag delegation and ownership transfer in an efficient way.","Radiofrequency identification,
Protocols,
Decoding,
Video coding,
Feedback,
Motion estimation,
Computer science,
Computer errors,
Error correction codes,
Delay"
Combat with Black hole attack in AODV routing protocol in MANET,"A mobile ad hoc network (MANET) is an autonomous network that consists of mobile nodes that communicate with each other over wireless links. In the absence of a fixed infrastructure, nodes have to cooperate in order to provide the necessary network functionality. One of the principal routing protocols used in Ad hoc networks is AODV (Ad hoc on demand Distance Vector) protocol. The security of the AODV protocol is threaded by a particular type of attack called ‘Black Hole’ attack. In this attack a malicious node advertises itself as having the shortest path to the destination node. To combat with black hole attack, it is proposed to wait and check the replies from all the neighboring nodes to find a safe route but this approach suffers from high delay. In this paper, an approach is proposed to combat the Black hole attack by using negotiation with neighbors who claim to have a route to destination. the Simulation's results show that the proposed protocol provides better security and also better performance in terms of packet delivery than the conventional AODV in the presence of Black holes with minimal additional delay and Overhead.","Routing protocols,
Mobile ad hoc networks,
Ad hoc networks,
Computer networks,
Network topology,
Mobile computing,
Communication system security,
Added delay,
Wireless networks,
Base stations"
Movement direction decoding with spatial patterns of local field potentials,"We show that movement direction can be decoded with high accuracy using the spatial patterns extracted from multichannel local field potentials (LFPs). Two monkeys were trained to execute center-out movement in 8 directions. During the task the LFP activity was recorded with two 64 channel grids from the pre- and primary motor areas. The LFP signals were decomposed into 4 sub-band components in the 0–4 Hz, 4–10 Hz, 14–30 Hz and 48–200 Hz frequency ranges. The sub-band activity was post processed with regularized common spatial patterns algorithm and fed to linear discriminant analysis for final classification. Directions of movement were estimated using a redundant hierarchical classification strategy that tested groups of directions against diametrically opposite groups. The grouping of directions was based on the spatial correlation that we observed between LFP signals corresponding to neighboring movement directions which is similar to the cosine tuning profile of single neurons. We found that the decoding power for 8 directions was 80% and 92% for the two subjects, respectively, in 0–4Hz frequency band. Our best result of 92% nearly doubles the accuracy of the best results reported in the literature with similar set-ups. These results indicate that spatial patterns in LFP can be used to construct high accuracy brain computer interfaces.","Decoding,
Neurons,
Electroencephalography,
Frequency,
Brain,
USA Councils,
Tuning,
Prosthetics,
Signal to noise ratio,
Neural engineering"
Interpolation-sequence based model checking,SAT-based model checking is the most widely used method for verifying industrial designs against their specification. This is due to its ability to handle designs with thousands of state elements and more. The main drawback of using SAT-based model checking is its orientation towards ”bug-hunting” rather than full verification of a given specification. Previous works demonstrated how Unbounded Model Checking can be achieved using a SAT solver. In this work we present a novel SAT-based approach to full verification. The approach combines BMC with interpolation-sequence in order to imitate BDD-based Symbolic Model Checking. We demonstrate the usefulness of our method by applying it to industrial-size hardware designs from Intel. Our method compares favorably with McMillan's interpolation based model checking algorithm.,"Data structures,
Boolean functions,
Interpolation,
Safety,
Sliding mode control,
Computer architecture,
Computer industry,
Hardware,
Automata,
Logic"
An analysis of heterogeneous cooperative algorithms,"Most optimization algorithms suffer from a significant deterioration in performance as the dimensionality and complexity of the problem search space increases. Also these algorithms, given certain configurations, typically show markedly improved performance on a particular problem only to exhibit poor performance on another. The first issue could be resolved by using a cooperative algorithm to divide the problem complexity among its participating algorithms, making the problem easier to solve. The second issue could then be resolved with the use of differently configured participating algorithms within the overall cooperative algorithm. This paper investigates the possibility of combining different population-based algorithms within a cooperative algorithm. The aim is to take advantage of different algorithm characteristics regarding parameter settings, explorative/exploitative capacity, convergence speed and other behaviors in finding solutions to various optimization problems.","Algorithm design and analysis,
Evolution (biology),
Biological cells,
Genetic mutations,
Performance analysis,
Genetic algorithms,
Biological system modeling,
Stochastic processes,
Convergence,
Particle swarm optimization"
On the Impact of Heterogeneity and Back-End Scheduling in Load Balancing Designs,"Load balancing is a common approach for task assignment in distributed architectures. In this paper, we show that the degree of inefficiency in load balancing designs is highly dependent on the scheduling discipline used at each of the back-end servers. Traditionally, the back-end scheduler can be modeled as processor sharing (PS), in which case the degree of inefficiency grows linearly with the number of servers. However, if the back- end scheduler is changed to shortest remaining processing time (SRPT), the degree of inefficiency can be independent of the number of servers, instead depending only on the heterogeneity of the speeds of the servers. Further, switching the back-end scheduler to SRPT can provide significant improvements in the overall mean response time of the system as long as the heterogeneity of the server speeds is small.","Load management,
Processor scheduling,
Delay,
Parallel processing,
Information science,
Grid computing,
Web server,
Network servers,
Communications Society,
Computer science"
Cache Sharing Management for Performance Fairness in Chip Multiprocessors,"Resource sharing can cause unfair and unpredictable performance of concurrently executing applications in Chip-Multiprocessors (CMP). The shared last-level cache is one of the most important shared resources because off-chip request latency may take a significant part of total execution cycles for data intensive applications. Instead of enforcing performance fairness directly, prior work addressing fairness issue of cache sharing mainly focuses on the fairness metrics of cache miss numbers or miss rates. However, because of the variation of cache miss penalty, fairness on cache miss cannot guarantee performance fairness. Cache sharing management which directly addresses performance fairness is needed for CMP systems. This paper introduces a model to analyze the performance impact of cache sharing, and proposes a mechanism of cache sharing management to provide performance fairness for concurrently executing applications. The proposed mechanism monitors the actual penalty of all cache misses and dynamically estimates the cache misses with dedicated caches when the applications are actually running with a shared cache. The estimated relative slowdown for each core from dedicated environment to shared environment is used to guide cache sharing in order to guarantee performance fairness. The experiment results show that the proposed mechanism always improves the performance fairness metric, and can provide no worse throughput than the scenario without any management mechanism.",
Multichannel spectral pattern separation - An EEG processing application -,"A problem of information separation in multichannel recordings is important in engineering applications such as brain computer/machine interfaces (BCI/BMI). Whereas this problem is not entirely new, engineering approaches connecting the mental states of humans and the observed electroencephalography (EEG) recordings are still in their infancy, mostly due to problems with electrophysiological denoising. The electrophysiological signals captured in form of the EEG carry brain activity in form of the neurophysiological components which are usually embedded in much higher power electrical muscle activity components (electromyography - EMG; electrooculography - EOG; etc.). In this paper we present an approach to remove muscular interference caused by eye-movements from EEG recorded during auditory experiments in an eight channel recording setting. This is achieved by analyzing the correlation of the oscillatory modes within a multichannel signal in the Hilbert domain. Simulations in a real world auditory BCI setting support the analysis.","Electroencephalography,
Application software,
Electromyography,
Electrooculography,
Brain computer interfaces,
Computer interfaces,
Joining processes,
Humans,
Noise reduction,
Electrophysiology"
RSSI-based indoor localization using antenna diversity and plausibility filter,"Distance estimation by the evaluation of RSSI measurements is an easy method to predict the position of an unknown node. Therefore common and proven systems can be used for the infrastructure. For indoor environments the distance-pending path loss is affected by strong variations, especially appearing as frequency specific signal dropouts. A diversity concept with redundant data transmission in different frequency bands can reduce the dropout probability. When also space diversity and plausibility filtering are used, the Location Estimation Error can be reduced significantly. The investigations show that a good performance for precision and availability can also be reached with low infrastructural costs.","Filters,
Antenna measurements,
Position measurement,
Indoor environments,
Data communication,
Frequency diversity,
Filtering,
Estimation error,
Availability,
Costs"
Application of artificial neural network in detection of probing attacks,"The prevention of any type of cyber attack is indispensable because a single attack may break the security of computer and network systems. The hindrance of such attacks is entirely dependent on their detection. The detection is a major part of any security tool such as Intrusion Detection System (IDS), Intrusion Prevention System (IPS), Adaptive Security Alliance (ASA), check points and firewalls. Consequently, in this paper, we are contemplating the feasibility of an approach to probing attacks that are the basis of others attacks in computer network systems. Our approach adopts a supervised neural network phenomenon that is majorly used for detecting security attacks. The proposed system takes into account Multiple Layered Perceptron (MLP) architecture and resilient backpropagation for its training and testing. The system uses sampled data from Kddcup99 dataset, an attack database that is a standard for evaluating the security detection mechanisms. The developed system is applied to different probing attacks. Furthermore, its performance is compared to other neural networks' approaches and the results indicate that our approach is more precise and accurate in case of false positive, false negative and detection rate.","Artificial neural networks,
Intrusion detection,
Computer networks,
Neural networks,
Data security,
Application software,
Computer security,
Adaptive systems,
Computer architecture,
Backpropagation"
Mutual coupling reduction in MIMO antennas using artificial magnetic materials,"In this paper, artificial magnetic resonators are used in order to reduce mutual coupling in Multiple-input Multiple-Output (MIMO) systems. Split-ring resonator (SRR) inclusions are inserted between closely-spaced high-profile monopole antenna elements. It is shown that mutual coupling between the antenna elements can be reduced significantly by incorporating such magnetic inclusions. The magnetic materials work as insulators, and thus can be applied in a variety of antenna applications.","Mutual coupling,
MIMO,
Magnetic materials,
Electromagnetics,
Meetings"
Robust data placement in urgent computing environments,"Distributed urgent computing workflows often require data to be staged between multiple computational resources. Since these workflows execute in shared computing environments where users compete for resource usage, it is necessary to allocate resources that can meet the deadlines associated with time-critical workflows and can tolerate interference from other users. In this paper, we evaluate the use of robust resource selection and scheduling heuristics to improve the execution of tasks and workflows in urgent computing environments that are dependent on the availability of data resources and impacted by interference from less urgent tasks.","Robustness,
Resource management,
Processor scheduling,
Time factors,
Quality of service,
Computer applications,
Distributed computing,
Computer science,
Environmental management,
Interference"
The role of physical embodiment of a therapist robot for individuals with cognitive impairments,"This research focuses on studying the possible role of a socially interactive robot as a tool for monitoring and encouraging cognitive activities of the elderly and/or individuals suffering from dementia. One of the aims of this work is to show the benefits of the robot's physical embodiment in human-robot social interactions. The social therapist robot tries to provide customized cognitive stimulation by playing a music game with the user. The results of the 8-month pilot study depict a more efficient, natural, and preferred interaction with the robot rather than with the simulated robot.","Cognitive robotics,
Dementia,
Senior citizens,
Human robot interaction,
Rehabilitation robotics,
Medical services,
Alzheimer's disease,
Medical treatment,
Aging,
Monitoring"
Power-aware load balancing of large scale MPI applications,"Power consumption is a very important issue for HPC community, both at the level of one application or at the level of whole workload. Load imbalance of a MPI application can be exploited to save CPU energy without penalizing the execution time. An application is load imbalanced when some nodes are assigned more computation than others. The nodes with less computation can be run at lower frequency since otherwise they have to wait for the nodes with more computation blocked in MPI calls. A technique that can be used to reduce the speed is Dynamic Voltage Frequency Scaling (DVFS). Dynamic power dissipation is proportional to the product of the frequency and the square of the supply voltage, while static power is proportional to the supply voltage. Thus decreasing voltage and/or frequency results in power reduction. Furthermore, over-clocking can be applied in some CPUs to reduce overall execution time. This paper investigates the impact of using different gear sets , over-clocking, and application and platform propreties to reduce CPU power. A new algorithm applying DVFS and CPU over-clocking is proposed that reduces execution time while achieving power savings comparable to prior work. The results show that it is possible to save up to 60% of CPU energy in applications with high load imbalance. Our results show that six gear sets achieve, on average, results close to the continuous frequency set that has been used as a baseline.",
Sampling rate reduction for 60 GHz UWB communication using compressive sensing,"60 GHz ultra wide-band (UWB) communication is an emerging technology for high speed short range communications. However, the requirement of high-speed sampling increases the cost of receiver circuitry such as analog-to-digital converter (ADC). In this paper, we propose to use a compressive sensing framework to achieve a significant reduction of sampling rate. The basic idea is based on the observation that the received signals are sparse in the time domain due to the limited multipath effects at 60 GHz wireless transmission. According to the theory of compressive sensing, by carefully designing the sensing scheme, sub-Nyquist rate sampling of the sparse signal still enables exact recovery with very high probability. We discuss an implementation for a low-speed A/D converter for 60 GHz UWB received signal. Moreover, we analyze the bit error rate (BER) performance for BPSK modulation under RAKE reception. Simulation results show that in the single antenna pair system model, sampling rate can be reduced to 2.2% with 0.3dB loss of BER performance if the input sparsity is less than 1%. Consequently, the implementation cost of ADC is significantly reduced.","Sampling methods,
Ultra wideband communication,
Bit error rate,
Costs,
Ultra wideband technology,
Circuits,
Analog-digital conversion,
Wireless sensor networks,
Signal design,
Error analysis"
Feasible pattern generation method for humanoid robots,"This paper proposes a feasible pattern generation method for humanoid robots. One of the difficulties in pattern generation for humanoid robots is that generated patterns must satisfy many constraints such as physical limits, self-collision and so on to be feasible in addition to constraints to achieve a specified task. In reality, some of these constraints are not often taken into account during the pattern generation and they are just checked afterwards and unsatisfied constraints are fixed by hand. It is not easy to find a parameter set to get a feasible motion for humanoid robot and these pattern generators need to be used carefully when they are used online. The proposed method integrates the feasibility constraints into the pattern generation algorithm and enables to use it online more safely and releases human from parameter tuning. Moreover, a stiffness varying constraint is introduced to improve the feasibility.","Humanoid robots,
Joints,
Kinematics,
Legged locomotion,
Humans,
Robotics and automation,
Collision avoidance,
Motion detection,
Computer graphics,
Intelligent systems"
Parallel data-locality aware stencil computations on modern micro-architectures,"Novel micro-architectures including the Cell Broadband Engine Architecture and graphics processing units are attractive platforms for compute-intensive simulations. This paper focuses on stencil computations arising in the context of a biomedical simulation and presents performance benchmarks on both the Cell BE and GPUs and contrasts them with a benchmark on a traditional CPU system. Due to the low arithmetic intensity of stencil computations, typically only a fraction of the peak performance of the compute hardware is reached. An algorithm is presented, which reduces the bandwidth requirements and thereby improves performance by exploiting temporal locality of the data. We report on performance improvements over CPU implementations.","Concurrent computing,
Biomedical computing,
Computational modeling,
Central Processing Unit,
Engines,
Computer architecture,
Graphics,
Context modeling,
Arithmetic,
Hardware"
Tracking groups of people with a multi-model hypothesis tracker,"People in densely populated environments typically form groups that split and merge. In this paper we track groups of people so as to reflect this formation process and gain efficiency in situations where maintaining the state of individual people would be intractable. We pose the group tracking problem as a recursive multi-hypothesis model selection problem in which we hypothesize over both, the partitioning of tracks into groups (models) and the association of observations to tracks (assignments). Model hypotheses that include split, merge, and continuation events are first generated in a data-driven manner and then validated by means of the assignment probabilities conditioned on the respective model. Observations are found by clustering points from a laser range finder given a background model and associated to existing group tracks using the minimum average Hausdorff distance. Experiments with a stationary and a moving platform show that, in populated environments, tracking groups is clearly more efficient than tracking people separately. Our system runs in real-time on a typical desktop computer.",
The Effects of Nuclear Fragmentation Models on Single Event Effect Prediction,There is a need for improved physics models to correctly predict single event effects (SEEs) caused by nuclear reaction products from heavy ion radiation. Previous validations for nuclear fragmentation simulations are shown to be insufficient to support SEE analysis applications. A comparison of different physics models with experimental nuclear physics data coupled with energy deposition predictions is presented.,
The Complexity of Channel Scheduling in Multi-Radio Multi-Channel Wireless Networks,"The complexity of channel scheduling in Multi- Radio Multi-Channel (MR-MC) wireless networks is an open research topic. This problem asks for the set of edges that can support maximum amount of simultaneous traffic over orthogonal channels under a certain interference model. There exist two major interference models for channel scheduling, with one under the physical distance constraint, and one under the hop distance constraint. The complexity of channel scheduling under these two interference models serves as the foundation for many problems related to network throughput maximization. However, channel scheduling was proved to be NP-Hard only under the hop distance constraint for SR-SC wireless networks. In this paper, we fill the void by proving that channel scheduling is NP-Hard under both models in MR-MC wireless networks. In addition, we propose a polynomial-time approximation scheme (PTAS) framework that is applicable to channel scheduling under both interference models in MR-MC wireless networks. Furthermore, we conduct a comparison study on the two interference models and identify conditions under which these two models are equivalent for channel scheduling.","Wireless networks,
Interference constraints,
Telecommunication traffic,
Processor scheduling,
Computer science,
USA Councils,
Traffic control,
Throughput,
Polynomials,
Communications Society"
On the complexity and consistency of UKF-based SLAM,"This paper addresses two key limitations of the unscented Kalman filter (UKF) when applied to the simultaneous localization and mapping (SLAM) problem: the cubic, in the number of states, computational complexity, and the inconsistency of the state estimates. In particular, we introduce a new sampling strategy that minimizes the linearization error and whose computational complexity is constant (i.e., independent of the size of the state vector). As a result, the overall computational complexity of UKF-based SLAM becomes of the same order as that of the extended Kalman filter (EKF) when applied to SLAM. Furthermore, we investigate the observability properties of the linear-regression-based model employed by the UKF, and propose a new algorithm, termed the Observability-Constrained (OC)-UKF, that improves the consistency of the state estimates. The superior performance of the OC-UKF compared to the standard UKF and its robustness to large linearization errors are validated by extensive simulations.","Simultaneous localization and mapping,
Computational complexity,
Observability,
State estimation,
Sampling methods,
Performance gain,
Computational efficiency,
Robotics and automation,
Robustness,
Remotely operated vehicles"
Tri-Message: A Lightweight Time Synchronization Protocol for High Latency and Resource-Constrained Networks,"Existing terrestrial synchronization protocols including RBS, FTSP, TPSN, LTS and TSHL have already achieved high precision in radio networks, but none of them perform well in high latency networks like acoustic sensor networks. In this paper, we present tri-message: a lightweight time synchronization protocol for high latency and resource-constrained networks. As its name suggests, only three message exchanges are required in one synchronization process. Meanwhile, tri-message utilizes very simple mathematical operations to calculate the clock skew and offset. Specially, tri-message is feasible for many extremely long latency applications such as space exploration because it has an increasing synchronization precision with the increasement of distance.","Protocols,
Delay,
Frequency synchronization,
Clocks,
Acoustic sensors,
Radio network,
Space exploration,
Communications Society,
Paper technology,
Computer science"
Video semantic concept detection via associative classification,"Associative classification (AC) has been studied in the areas of content-based multimedia retrieval and semantic concept detection due to its high accuracy. The traditional AC algorithm discovers the association rules with the frequency count (minimum support) and ranking threshold (minimum confidence) while restricted to the concepts (class labels). In this paper, we propose a novel framework with a new associative classification algorithm which generates the classification rules based on the correlation between different feature-value pairs and the concept classes by using Multiple Correspondence Analysis (MCA). Experimenting with the high-level features and benchmark data sets from TRECVID, our proposed algorithm achieves promising performance and outperforms three well-known classifiers which are commonly used for performance comparison in the TRECVID community.","Association rules,
Testing,
Data mining,
Multimedia databases,
Event detection,
Feature extraction,
Classification tree analysis,
Support vector machines,
Support vector machine classification,
Content based retrieval"
Minimizing the number of bits needed for iris recognition via Bit Inconsistency and GRIT,"In this paper, we demonstrate how the concepts of Bit Inconsistency and Genetic Search can be used to minimize the number of iris code bits needed for iris recognition. In addition, we compare two systems: GRIT-I (Genetically Refined Iris Templates I) and GRIT-II. Our results show that GRIT-I (by evolving the bit mask of iris templates) was able to reduce the number of iris code bits needed by approximately 30% on average. GRIT-II by contrast optimizes the bit mask as well as the iris code bits that have 100% consistency and 100% coverage with respect to the training set. GRIT-II was able to reduce the number of iris code bits needed by approximately 89%.","Iris recognition,
Eyelids,
Kelvin,
Image analysis,
Eyelashes,
Genetic algorithms,
Machine learning,
Probability distribution,
Helium,
Predictive models"
High-order stencil computations on multicore clusters,"Stencil computation (SC) is of critical importance for broad scientific and engineering applications. However, it is a challenge to optimize complex, high-order SC on emerging clusters of multicore processors. We have developed a hierarchical SC parallelization framework that combines: (1) spatial decomposition based on message passing; (2) multithreading using critical section-free, dual representation; and (3) single-instruction multiple-data (SIMD) parallelism based on various code transformations. Our SIMD transformations include translocated statement fusion, vector composition via shuffle, and vectorized data layout reordering (e.g. matrix transpose), which are combined with traditional optimization techniques such as loop unrolling. We have thereby implemented two SCs of different characteristics—diagonally dominant, lattice Boltzmann method (LBM) for fluid flow simulation and highly off-diagonal (6-th order) finite-difference time-domain (FDTD) code for seismic wave propagation—on a Cell Broadband Engine (Cell BE) based system (a cluster of PlayStation3 consoles), a dual Intel quadcore platform, and IBM BlueGene/L and P. We have achieved high inter-node and intra-node (multithreading and SIMD) scalability for the diagonally dominant LBM: Weak-scaling parallel efficiency 0.978 on 131,072 BlueGene/P processors; strong-scaling multithreading efficiency 0.882 on 6 cores of Cell BE; and strong-scaling SIMD efficiency 0.780 using 4-element vector registers of Cell BE. Implementation of the high-order SC, on the contrary, is less efficient due to long-stride memory access and the limited size of the vector register file, which points out the need for further optimizations.","Multicore processing,
Multithreading,
Finite difference methods,
Time domain analysis,
Registers,
Message passing,
Parallel processing,
Matrix decomposition,
Lattice Boltzmann methods,
Fluid flow"
The Effectiveness of Automated Static Analysis Tools for Fault Detection and Refactoring Prediction,"Many automated static analysis (ASA) tools have been developed in recent years for detecting software anomalies. The aim of these tools is to help developers to eliminate software defects at early stages and produce more reliable software at a lower cost. Determining the effectiveness of ASA tools requires empirical evaluation. This study evaluates coding concerns reported by three ASA tools on two open source software (OSS) projects with respect to two types of modifications performed in the studied software CVS repositories: corrections of faults that caused failures, and refactoring modifications. The results show that fewer than 3% of the detected faults correspond to the coding concerns reported by the ASA tools. ASA tools were more effective in identifying refactoring modifications and corresponded to about 71% of them. More than 96% of the coding concerns were false positives that do not relate to any fault or refactoring modification.","Fault detection,
Software tools,
Open source software,
Costs,
Fault diagnosis,
Java,
Documentation,
Software testing,
Computer science,
Gas detectors"
On inference of network time constants from impulse response data: graph-theoretic Cramer-Rao bounds,"We examine the role played by a linear dynamical network's topology in inference of its eigenvalues from noisy impulse-response data. Specifically, for a canonical linear time-invariant network dynamics, we relate the Cramer-Rao bounds on eigenvalue estimator performance (from impulse-response data) to structural properties of the transfer function, and in turn to the network's topological structure. We focus especially on networks with a slow-coherence structure, in which case we find that stimulus and observation in each strongly-connected network component is needed for high-fidelity estimation.","Vehicle dynamics,
Parameter estimation,
Network topology,
Eigenvalues and eigenfunctions,
Biological control systems,
System identification,
Control systems,
Communication system traffic control,
Air traffic control,
Bayesian methods"
Prioritizing JUnit test cases in absence of coverage information,"Better orderings of test cases can detect faults in less time with fewer resources, and thus make the debugging process earlier and accelerate software delivery. As a result, test case prioritization has become a hot topic in the research of regression testing. With the popularity of using the JUnit testing framework for developing Java software, researchers also paid attention to techniques for prioritizing JUnit test cases in regression testing of Java software. Typically, most of them are based on coverage information of test cases. However, coverage information may need extra costs to acquire. In this paper, we propose an approach (named Jupta) for prioritizing JUnit test cases in absence of coverage information. Jupta statically analyzes call graphs of JUnit test cases and the software under test to estimate the test ability (TA) of each test case. Furthermore, Jupta provides two prioritization techniques: the total TA based technique (denoted as JuptaT) and the additional TA based technique (denoted as JuptaA). To evaluate Jupta, we performed an experimental study on two open source Java programs, containing 11 versions in total. The experimental results indicate that Jupta is more effective and stable than the untreated orderings and Jupta is approximately as effective and stable as prioritization techniques using coverage information at the method level.",
Modeling the Dose Rate Response and the Effects of Hydrogen in Bipolar Technologies,"A physical model describing the dose rate response and the effect of hydrogen in bipolar technologies is presented. The model uses electron-hole pair recombination and competing hydrogen reactions to explain the behaviors of bipolar devices and circuits at different dose rates. Dose-rate-dependent computer simulations based on the model were performed, and the results provide excellent qualitative agreement with the dose rate data taken on both gated lateral pnp bipolar test transistors and LM193 bipolar dual-voltage comparators. The model presented in this paper can be used to explain a variety of factors that can influence device dose rate response in bipolar technologies.","Hydrogen,
Circuits,
Space technology,
Spontaneous emission,
Electron traps,
Propulsion,
Laboratories,
Thermal stresses,
Packaging,
Computer simulation"
Precipitation-based conductor cooling model for Dynamic Thermal Rating systems,"This paper presents a precipitation-based conductor cooling model for use in power line ampacity rating applications. It is aimed at better modelling a conductor's temperature by incorporating line cooling resulting from precipitation falling on power lines. The improved calculations provide gains in additional line capacity for power transmission networks incorporating advanced Dynamic Thermal Rating systems. Depending on the precipitation rate and other atmospheric variables, the initial work presented in this paper suggests that line cooling gains between 1°C to over 20°C may be obtained. The precipitation based cooling model shows that the highest gains are observed for largest line loads, thus providing cooling where it is needed the most.","Conductors,
Cooling,
Thermal conductivity,
Temperature,
Power system modeling,
Atmospheric modeling,
Power transmission,
Power transmission lines,
Equations,
Meteorological factors"
Simultaneous local and global state estimation for robotic navigation,"Recent applications of robotics often demand two types of spatial awareness: 1) A fine-grained description of the robot's immediate surroundings for obstacle avoidance and planning, and 2) Knowledge of the robot's position in a large-scale global coordinate frame such as that provided by GPS. Although managing information at both of these scales is often essential to the robot's purpose, each scale has different requirements in terms of state representation and handling of uncertainty. In such a scenario, it can be tempting to pick either a body-centric coordinate frame or a globally fixed coordinate frame for all state representation. Although both choices have advantages, we show that neither is ideal for a system that must handle both global and local data. This paper describes an alternative design: a third coordinate frame that stays fixed to the local environment over short time-scales, but can vary with respect to the global frame. Careful management of uncertainty in this local coordinate frame makes it well-suited for simultaneously representing both locally and globally derived data, greatly simplifying system design and improving robustness. We describe the implementation of this coordinate frame and its properties when measuring uncertainty, and show the results of applying this approach to our 2007 DARPA Urban Challenge vehicle.","State estimation,
Navigation,
Robot kinematics,
Large-scale systems,
Global Positioning System,
Information management,
Robustness,
Coordinate measuring machines,
Measurement uncertainty,
Vehicles"
"SLAM in large indoor environments with low-cost, noisy, and sparse sonars","Simultaneous localization and mapping (SLAM) is a well-studied problem in mobile robotics. However, the majority of the proposed techniques for SLAMrely on the use of accurate and dense measurements provided by laser rangefinders to correctly localize the robot and produce accurate and detailed maps of complex environments. Little work has been done on the use of low-cost but noisy and sparse sonar sensors for SLAM in large indoor environments involving large loops. In this paper, we present our approach to SLAM with sonar sensors by applying particle filtering and a line-segment-based map representation with an orthogonality assumption to map indoor environments much larger and more challenging than those previously considered with sonar sensors. Results from robotic experiments demonstrate that it is possible to produce good maps of large indoor environments with large loops despite the inherent limitations of sonar sensors.","Simultaneous localization and mapping,
Indoor environments,
Working environment noise,
Robot sensing systems,
Mobile robots,
Sonar measurements,
Power lasers,
Sensor arrays,
Measurement by laser beam,
Filtering"
"Dependency Structure Matrix, Genetic Algorithms, and Effective Recombination","In many different fields, researchers are often confronted by problems arising from complex systems. Simple heuristics or even enumeration works quite well on small and easy problems; however, to efficiently solve large and difficult problems, proper decomposition is the key. In this paper, investigating and analyzing interactions between components of complex systems shed some light on problem decomposition. By recognizing three bare-bones interactions—modularity, hierarchy, and overlap, facet-wise models are developed to dissect and inspect problem decomposition in the context of genetic algorithms. The proposed genetic algorithm design utilizes a matrix representation of an interaction graph to analyze and explicitly decompose the problem. The results from this paper should benefit research both technically and scientifically. Technically, this paper develops an automated dependency structure matrix clustering technique and utilizes it to design a model-building genetic algorithm that learns and delivers the problem structure. Scientifically, the explicit interaction model describes the problem structure very well and helps researchers gain important insights through the explicitness of the procedure.","overlap,
Genetic algorithms,
dependency structure matrix,
problem decomposition,
modularity,
hierarchy"
Real-time cardiac MRI using prior spatial-spectral information,"Cardiac MRI performed while the patient is breathing is typically achieved using non-real-time techniques such as ECG triggering with respiratory gating; however, modern dynamic imaging techniques are beginning to enable this type of imaging in real-time. One of these dynamic imaging techniques is based on forming a Partially Separable Function (PSF) model of the data, but the model fitting process is known to be sensitive even when truncated SVD regularization is used. As a result, physiologically meaningless artifacts can appear in the dynamic images when the total number of measurements is limited. To address this issue, the dynamic imaging problem is formulated as a generalized Tikhonov regularization problem with the PSF model as a component of the forward data model, and a penalty function is used to introduce spatial-spectral prior information. This new method both reduces data acquisition requirements and improves stability relative to the original PSF based method when applied to cardiac MRI.","Magnetic resonance imaging,
Spatial resolution,
Image reconstruction,
Data acquisition,
Biomedical imaging,
Electrocardiography,
Stability,
High-resolution imaging,
Heart,
USA Councils"
Automated pavement distress detection using advanced image processing techniques,"In this paper, a novel, fast and self-adaptive image processing method is proposed for the extraction and connection of break points of cracks in pavement images. The algorithm first finds the initial point of a crack and then determines the crack's classification into transverse, longitudinal and alligator types. Different search algorithms are used for different types of cracks. Then the algorithm traces along the crack pixels to find the break point and then connect the identified crack point to the nearest break point in the particular search area. The nearest point then becomes the new initial point and the algorithm continues the process until reaching the end of the crack. The experimental results show that this connection algorithm is very effective in maximizing the accuracy of crack identification.","Image processing,
Wavelet transforms,
Inspection,
Image segmentation,
Filtering,
Sun,
Remuneration,
Civil engineering,
Robustness,
Statistics"
Five Recommended Practices for Computational Scientists Who Write Software,"Few software engineering techniques and approaches are specifically useful for computational scientists, and despite recent efforts, it could be many years before a consolidated handbook is available. Meanwhile, computational scientists can look to the practices of other scientists who write successful software.","User interfaces,
Military computing,
Data structures,
Computer interfaces,
Writing,
Software testing,
Best practices,
Guidelines,
Computer architecture,
Lifting equipment"
Security Implications of Virtualization: A Literature Study,"Server virtualization is a key technology for today's data centers, allowing dedicated hardware to be turned into resources that can be used on demand.However, in spite of its important role, the overall security impact of virtualization is not well understood.To remedy this situation, we have performed a systematic literature review on the security effects of virtualization. Our study shows that, given adequate management, the core virtualization technology has a clear positive effect on availability, but that the effect on confidentiality and integrity is less positive.Virtualized systems tend to lose the properties of location-boundedness, uniqueness and monotonicity.In order to ensure corporate and private data security, we propose to either remove or tightly manage non-essential features such as introspection, rollback and transfer.",
Estimation of drowsiness level based on eyelid closure and heart rate variability,"This paper presents a novel method that uses eyelid closure and heart rate variability to estimate the driver's drowsiness level. Laboratory experiments were conducted by using a proprietary driving simulator, which induced drowsiness among the test drivers. The purposes of these experiments were to obtain the electrocardiogram (ECG) and the eye-blink video sequences. Also the drivers were monitored through a video camera. The changes in facial expression of the drivers were used as a standard index of drowsiness level. Error-Correcting Output Coding (ECOC) was employed as a multi-class classifier to estimate the drowsiness level. We extended the ordinary ECOC using a loss function for decoding procedure to obtain class tendencies of each drowsiness level. We used the Loss-based Decoding ECOC (LD-ECOC) to classify the drowsiness level. As a result, we obtained an extraordinarily high accuracy for estimation of drowsiness level.","Eyelids,
Heart rate variability,
Decoding,
Road accidents,
Mouth,
Video sequences,
Humans,
Pattern recognition,
USA Councils,
Eyes"
Incremental SVM Model for Spam Detection on Dynamic Email Social Networks,"Recently, the huge number of email spams has caused serious problems in essential email communication. Traditional spam filters aim at analyzing email content to characterize the features that are commonly included in spams. However, it is observed that crafty tricks designed to avoid content-based filters will be endless owing to the economic benefits of sending spams. In view of this situation, there has been much research effort toward doing spam detection based on the reputation of senders rather than what is contained in emails. Motivated by the fact that spammers are prone to have unusual behavior and specific patterns of email communication, exploring email social networks to detect spams has received much attention. Nevertheless, previous works generally suffer from two problems: (1) the system is not robust in diverse environments, and (2) no update scheme is provided to catch the feature changes of evolving networks. In this paper, we propose an incremental support vector machine (SVM) model for spam detection on dynamic email social networks. A complete spam detection system MailNET is devised to better adjust to diverse networks. Several features of each user in the network are extracted to train an SVM model. Moreover, to catch the evolving nature of email communication, we present an incremental update scheme to efficiently re-train an SVM model. We evaluate MailNET on a live data set from a university-scale email server and show that the proposed model is efficient and effective, thus applicable to the real world.","Support vector machines,
Unsolicited electronic mail,
Social network services,
Electronic mail,
Filters,
Robustness,
Support vector machine classification,
Environmental economics,
Feature extraction,
Computer networks"
The ASSISTment Builder: Supporting the Life Cycle of Tutoring System Content Creation,"Content creation is a large component of the cost of creating educational software. Estimates are that approximately 200 hours of development time are required for every hour of instruction. We present an authoring tool designed to reduce this cost as it helps to refine and maintain content. The ASSISTment Builder is a tool designed to effectively create, edit, test, and deploy tutor content. The Web-based interface simplifies the process of tutor construction to allow users with little or no programming experience to develop content. We show the effectiveness of our Builder at reducing the cost of content creation to 40 hours for every hour of instruction. We describe new features that work toward supporting the life cycle of ITS content creation through maintaining and improving content as it is being used by students. The variabilization feature allows the user to reuse tutoring content across similar problems. The Student Comments feature provides a way to maintain and improve content based on feedback from users. The Most Common Wrong Answer feature provides a way to refine remediation based on the users' answers. This paper describes our attempt to support the life cycle of content creation.","Probability density function,
Data mining,
Programming profession,
Programming,
Artificial intelligence,
Electronic learning,
Computer science education"
An adaptive steganographic technique based on integer wavelet transform,"Steganography gained importance in the past few years due to the increasing need for providing secrecy in an open environment like the internet. With almost anyone can observe the communicated data all around, steganography attempts to hide the very existence of the message and make communication undetectable.","Steganography,
Wavelet transforms,
Discrete wavelet transforms,
Discrete Fourier transforms,
Data encapsulation,
Image coding,
Fourier transforms,
Discrete cosine transforms,
Transform coding,
Cryptography"
Fast checkpointing by Write Aggregation with Dynamic Buffer and Interleaving on multicore architecture,"Large scale compute clusters continue to grow to ever-increasing proportions. However, as clusters and applications continue to grow, the Mean Time Between Failures (MTBF) has reduced from days to hours. As a result, fault tolerance within the cluster has become imperative. MPI, the de-facto standard for parallel programming, is widely used on such large clusters. Many MPI implementations use Checkpoint/Restart schemes using the Berkeley Lab Checkpoint Restart (BLCR) Library to achieve some level of fault tolerance. However, the performance of the Checkpoint/Restart mechanism does not scale well with increasing job size. As a result, the deployment of Checkpoint/Restart mechanisms for large scale parallel applications is compromised. In our previous work, we proposed a technique to aggregate certain categories of checkpoint writes to reduce the checkpointing overhead. However, an application still experiences slow checkpoint writing because it is blocked waiting for its checkpoint file writes to complete. In this paper, we propose the Write Aggregation with Dynamic Buffer and Interleaving scheme to reduce the overhead related to checkpoint creation. By aggregating all checkpoint writes into a dynamic buffer pool and overlapping the application progress with the file writes, our algorithm is able to significantly reduce checkpoint creation overhead. In the experiments using 64 processor cores, our design demonstrates a speedup of 2.62 times in terms of checkpoint creation time when compared to the original BLCR design. Our scheme also reduces the impact of checkpointing on the application execution time from 20% to 6% when 3 checkpoints are taken during an application run.","Checkpointing,
Interleaved codes,
Multicore processing,
Large-scale systems,
Fault tolerance,
Computer architecture,
Parallel programming,
Libraries,
Aggregates,
Writing"
Federated and Shared Use of Sensor Networks through Security Middleware,"The lack of support for federation and sharing inhibits real-world deployment of wireless sensor networks (WSNs). In federated mode, multiple sensor networks belonging to separate administrative domains are used to achieve an application’s goal; in shared mode a single sensor or network provides its services to a range of applications. The lack of support for federation and sharing is particularly problematic with respect to enforcing security, not only at node or network level but also at the end-to-end application level. This paper highlights the missing links in state-of-the-art WSN security mechanisms, identifies the extra security requirements of shared and federated operations, and provides a high-level overview of security middleware and mechanisms to address the challenges. The resulting structured inventory of mechanisms provides building blocks to tackle the security problem and guides further research in this area.","Middleware,
Wireless sensor networks,
Hospitals,
Information security,
Medical control systems,
Data security,
Information technology,
Computer security,
Computer science,
Body sensor networks"
Randomizing RFID private authentication,"Privacy protection is increasingly important during authentications in Radio Frequency Identification (RFID) systems. In order to achieve high-speed authentication in large-scale RFID systems, researchers propose tree-based approaches, in which any pair of tags share a number of key components. Such designs, being efficient, often fail to achieve forward secrecy and resistance to attacks, such as compromising and desynchronization. Indeed, these attacks may still take effect even after a tag successfully finishes the authentication and key-updating procedure. To address the issue, we propose a lightweight RFID private authentication protocol, RWP, based on the random walk concept. RWP also provides the forward security and temporal resistance to the tracking attack. The analysis results show that RWP effectively enhances the security protection for RFID private authentication, and increases the authentication efficiency from O(logN) to O(1).","Radiofrequency identification,
Authentication,
Protocols,
Security,
Protection,
Databases,
Computer science,
Privacy,
Cryptography,
Large-scale systems"
S-Metric Calculation by Considering Dominated Hypervolume as Klee's Measure Problem,"The dominated hypervolume (or S-metric) is a commonly accepted quality measure for comparing approximations of Pareto fronts generated by multi-objective optimizers. Since optimizers exist, namely evolutionary algorithms, that use the S-metric internally several times per iteration, a fast determination of the S-metric value is of essential importance. This work describes how to consider the S-metric as a special case of a more general geometric problem called Klee's measure problem (KMP). For KMP, an algorithm exists with runtime O(n log n + nd/2 log n), for n points of d ≥ 3 dimensions. This complex algorithm is adapted to the special case of calculating the S-metric. Conceptual simplifications realize the algorithm without complex data structures and establish an upper bound of O(nd/2 log n) for the S-metric calculation for d ≥ 3. The performance of the new algorithm is studied in comparison to another state of the art algorithm on a set of academic test functions.",
An Inter-MAC architecture for heterogeneous gigabit home networks,"The home network of the near future will be a heterogeneous broadband network supporting the use of wired as well as wireless transmission technologies. The variety of services will range from HDTV via gaming to emergency services in the telemedicine area. In this paper, we introduce a technology-independent protocol layer called Inter-MAC which provides a common infrastructure to all home networking devices. The Inter-MAC can establish a connection via different transmission technologies while ensuring appropriate QoS. Key to this is the capability to correctly interpret technology-dependent PHY and MAC parameters for determining QoS when selecting initial and alternative paths, as well as the feature of admission control. Our simulations indicate clearly that the Inter-MAC approach can cope with varying loads, for instance, the time the jitter needed to stabilize was as short as 4 s. The reduced QoS was not noticeable by the user since the jitter introduced by handling additional HDTV flows was less than 0.2 ms for the existing and less than 0.7 ms for the new flow.",
Regularized Fitted Q-Iteration for planning in continuous-space Markovian decision problems,"Reinforcement learning with linear and non-linear function approximation has been studied extensively in the last decade. However, as opposed to other fields of machine learning such as supervised learning, the effect of finite sample has not been thoroughly addressed within the reinforcement learning framework. In this paper we propose to use L2 regularization to control the complexity of the value function in reinforcement learning and planning problems. We consider the Regularized Fitted Q-Iteration algorithm and provide generalization bounds that account for small sample sizes. Finally, a realistic visual-servoing problem is used to illustrate the benefits of using the regularization procedure.","Function approximation,
Computational modeling,
Supervised learning,
Machine learning,
Complex networks,
Computer networks,
Discrete event simulation,
Machine learning algorithms,
Error correction,
Automatic control"
New design for an endoesophageal sector- based array for the treatment of atrial fibrillation: a parametric simulation study,"Atrial fibrillation (AF) is the most frequent and sustained cardiac arrhythmia affecting humans. The electrical isolation by ablation of the pulmonary veins (PV) in the left atrium (LA) of the heart has proved to be an effective cure for the AF. The ablation consists mainly of the formation of a localized circumferential thermal coagulation of the cardiac tissue surrounding the PVs. In this article, a parametric study was carried out to establish an optimal configuration of endesophageal ultrasound phased arrays intended to treat the AF. The devices are spherical-surface sections truncated at 15 mm, with a depth of 4 mm, and they are cut in concentric-rings, each composed of independently driven sectors. The number of independent elements (Ne) was minimized for different values of ratio of pressure amplitude of the secondary lobe over the main lobe (n) of 0.35, 0.4, 0.45, and 0.5 inside a volume of interest (VOI). After assuming a Cartesian system with the origin in the center of the device, the VOI was defined as the prism enclosed by the coordinates (-12, 10, -9) mm and (12, 37, 9) mm. The VOI has its center at (0, 23.5, 0) mm and is large enough to contain all the targets identified in the Visible Human Project Male specimen. Operating at 1 MHz, n and Ne were calculated in function of the element size and focal length (F). Four devices for each value of n were found. After keeping values of F and normalized dimensions of the independent elements in terms of wavelength, higher frequencies were considered: 1.25 MHz, 1.5 MHz, and 2 MHz. In total, 16 device configurations were obtained. Realistic modeling of lesion formation in the heart chamber showed that the 16 configurations were able to produce the typical lesion used to treat the AF while preserving surrounding structures. At higher frequencies, lower power was required, and a greater number of array elements was required. For an exposure of 5 s and a maximum temperature of 70degC, the average (plusmns.d.) acoustical intensity at transducer surface varied from 22.3(plusmn5.8) W/cm2 for a device with F = 98 mm at 1 MHz to 5.8(plusmn1.2) W/cm2 for a device with F = 186 mm at 2 MHz, while requiring 319 and 2093 elements, respectively, and achieving values of n of 0.5 and 0.41, respectively. For the intended application, the selected devices implied a better focusing when compared with more traditional planar 2-D arrays, while requiring less power and fewer independent elements.",
The Oz of Wizard: Simulating the human for interaction research,"The Wizard of Oz experiment method has a long tradition of acceptance and use within the field of human-robot interaction. The community has traditionally downplayed the importance of interaction evaluations run with the inverse model: the human simulated to evaluate robot behavior, or “Oz of Wizard”. We argue that such studies play an important role in the field of human-robot interaction. We differentiate between methodologically rigorous human modeling and placeholder simulations using simplified human models. Guidelines are proposed for when Oz of Wizard results should be considered acceptable. This paper also describes a framework for describing the various permutations of Wizard and Oz states.",
A Cluster-Based Protocol to Enforce Integrity and Preserve Privacy in Data Aggregation,"Data fusion or information collection is one of the fundamental functions in the future cyber-physical systems. But, privacy concerns must be addressed and security must be assured in such systems. It is very challenging to achieve the synergy of privacy and integrity, because privacy preserving schemes try to hide or interfere with data, while integrity protection usually needs to enable peer monitoring or public access of the data. Therefore, privacy and integrity can be the conflicting requirements, one may barricade the implementation of the other.In this paper, we address both privacy of individual sensory data and integrity of aggregation result simultaneously by proposing a protocol called iCPDA, which piggybacks on a cluster-based privacy-preserving data aggregation protocol(CPDA). We implement the add-on feature to protect integrity of aggregation result. To show the efficacy and efficiency of the proposed scheme, we present simulation results. To the best of our knowledge, this paper is among the first protocols to preserve privacy and integrity in data aggregation.","Protocols,
Data privacy,
Protection,
Computer science,
Monitoring,
Wireless sensor networks,
Information security,
Distributed computing,
Conferences,
Helium"
On the Exploitation of CDF Based Wireless Scheduling,"Channel-aware scheduling strategies - such as the CDF Scheduler (CS) algorithm for the CDMA/HDR systems - provide an effective mechanism for utilizing the channel data rate for improving throughput performance in wireless data networks by exploiting channel fluctuations. A highly desired property of such a scheduling strategy is that its algorithm will be stable, in the sense that no user has incentive ""cheating"" the algorithm in order to increase his/her channel share (on the account of others). We present a scheme by which coordination allows a group of users to gain permanent increase in both their time slot share and in their throughput, on the expense of others, by misreporting their rates. We show that for large populations consisting of regular and coordinated users in equal numbers, the ratio of allocated time slots between a coordinated user and a regular one converges to e - 1 ap 1.7. Our scheme targets the very fundamental principle of CS (as opposed to just attacking implementation aspects), which bases its scheduling decisions on the cumulative distribution function (CDF) of the channel rates reported by users. Our scheme works both for the continuous channel spectrum and the discrete channel spectrum versions of the problem.","Throughput,
Resource management,
Processor scheduling,
Computer science,
Scheduling algorithm,
Fluctuations,
Distribution functions,
Wireless networks,
Channel capacity,
Communications Society"
A Filtered Backprojection Algorithm for Triple-Source Helical Cone-Beam CT,"Multisource cone-beam computed tomography (CT) is an attractive approach of choice for superior temporal resolution, which is critically important for cardiac imaging and contrast enhanced studies. In this paper, we present a filtered-backprojection (FBP) algorithm for triple-source helical cone-beam CT. The algorithm is both exact and efficient. It utilizes data from three inter-helix PI-arcs associated with the inter-helix PI-lines and the minimum detection windows defined for the triple-source configuration. The proof of the formula is based on the geometric relations specific to triple-source helical cone-beam scanning. Simulation results demonstrate the validity of the reconstruction algorithm. This algorithm is also extended to a multisource version for ( 2N + 1 ) -source helical cone-beam CT. With parallel computing, the proposed FBP algorithms can be significantly faster than our previously published multisource backprojection-filtration algorithms. Thus, the FBP algorithms are promising in applications of triple-source helical cone-beam CT.","Computed tomography,
Band pass filters,
Biomedical imaging,
Biomedical engineering,
Image resolution,
Image reconstruction,
Computational modeling,
Reconstruction algorithms,
Parallel processing"
Unifying visual saliency with HOG feature learning for traffic sign detection,"Traffic sign detection is important to a robotic vehicle that automatically drives on roads. In this paper, an efficient novel approach which is enlighten by the process of the human vision is proposed to achieve automatic traffic sign detection. The detection method combines bottom-up traffic sign saliency region with learning based top-down features of traffic sign guided search. The bottom-up stage could obtain saliency region of traffic sign and achieve computational parsimony using improved Model of Saliency-Based Visual Attention. The top-down stage searches traffic sign in these traffic sign saliency regions based on the feature Histogram of Oriented Gradient (HOG) and the classifier Support Vector Mechine (SVM). Experimental results show that, the proposed approach can achieve robustness to illumination, scale, pose, viewpoint change and even partial occlusion. The samllest detection size of traffic sign is 14×14, the average detection rate is 98.3% and the false positive rate is 5.09% in test image data set.","Traffic control,
Support vector machines,
Support vector machine classification,
Vehicle detection,
Robotics and automation,
Road vehicles,
Vehicle driving,
Humans,
Histograms,
Robustness"
Optical flow estimation on coarse-to-fine region-trees using discrete optimization,"In this paper, we propose a new region-based method for accurate motion estimation using discrete optimization. In particular, the input image is represented as a tree of over-segmented regions and the optical flow is estimated by optimizing an energy function defined on such a region-tree using dynamic programming. To accommodate the sampling-inefficiency problem intrinsic to discrete optimization compared to the continuous optimization based methods, both spatial and solution domain coarse-to-fine (C2F) strategies are used. That is, multiple region-trees are built using different over-segmentation granularities. Starting from a global displacement label discretization, optical flow estimation on the coarser level region-tree is used for defining region-wise finer displacement samplings for finer level region-trees. Furthermore, cross-checking based occlusion detection and correction and continuous optimization are also used to improve accuracy. Extensive experiments using the Middlebury benchmark datasets have shown that our proposed method can produce top-ranking results.","Image motion analysis,
Optimization methods,
Optical computing,
Motion estimation,
Sampling methods,
Minimization methods,
Dynamic programming,
Stereo vision,
Image segmentation,
Image representation"
PermaDAQ: A scientific instrument for precision sensing and data recovery in environmental extremes,"The PermaSense project has set the ambitious goal of gathering real-time environmental data for high-mountain permafrost in unattended operation over multiple years. This paper discusses the specialized sensing and data recovery architecture tailored to meet the precision, reliability and durability requirements of scientists utilizing the data for model validation. We present a custom sensor interface board including specialized sensors and redundancy features for end-to-end data validation. Aspects of high-quality data acquisition, design for reliability by strict separation of operating phases and analysis of energy efficiency are discussed. The system integration using the Dozer protocol scheme achieves a best-in-class average power consumption of 148µA considerably exceeding the lifetime requirement.",Instruments
Human computation,"This talk is about harnessing human brainpower to solve problems that computers cannot. Although computers have advanced dramatically over the last 50 years, they still do not possess basic conceptual intelligence or perceptual capabilities that most humans take for granted. By leveraging human abilities in a novel way, I solve large-scale computational problems and collect data to teach computers basic human talents. To this end, I treat human brains as processors in a distributed system, each performing a small part of a massive computation.","Humans,
Optical character recognition software,
Books,
Internet,
Computer displays,
Computer science,
Distributed computing,
Web sites,
Permission,
Computational intelligence"
Utility analysis for Internet-oriented server consolidation in VM-based data centers,"Server consolidation based on virtualization technology will simplify system administration, reduce the cost of power and physical infrastructure, and improve utilization in today's Internet-service-oriented enterprise data centers. How much power and how many servers for the underlying physical infrastructure are saved via server consolidation in VM-based data centers is of great interest to administrators and designers of those data centers. Various workload consolidations differ in saving power and physical servers for the infrastructure. The impacts caused by virtualization to those concurrent services are fluctuating considerably which may have a great effect on server consolidation. This paper proposes a utility analytic model for Internet-oriented server consolidation in VM-based data centers, modelling the interaction between server arrival requests with several QoS requirements, and capability flowing amongst concurrent services, based on the queuing theory. According to features of those services' workloads, this model can provide the upper bound of consolidated physical servers needed to guarantee QoS with the same loss probability of requests as in dedicated servers. At the same time, it can also evaluate the server consolidation in terms of power and utility of physical servers. Finally, we verify the model via a case study comprised of one e-book database service and one e-commerce Web service, simulated respectively by TPC-W and SPECweb2005 benchmarks. Our experiments show that the model is simple but accurate enough. The VM-based server consolidation saves up to 50% physical infrastructure, up to 53% power, and improves 1.7 times in CPU resource utilization, without any degradation of concurrent services' performance, running on Rainbow — our virtual computing platform.","Web server,
Costs,
Web and internet services,
Queueing analysis,
Upper bound,
Databases,
Web services,
Resource management,
Degradation,
Concurrent computing"
Active RFID System With Spread-Spectrum Transmission,"In this paper, we propose a new direct-sequence code division multiple access (DS-CDMA)-based transmission scheme for active, transmitter-only radio frequency identification (RFID) tags. We present the architecture of the system, as well as the mathematical analysis of detection performance and system capacity. Because of the good multiple-access properties of DS-CDMA transmission, the proposed system outperforms the classical ALOHA-based RFID system with transmitter-only tags. The obtained analytical results have been verified via computer simulations. Some implementation issues have also been discussed.","Radiofrequency identification,
Active RFID tags,
Spread spectrum communication,
Multiaccess communication,
RFID tags,
Computer simulation,
Wireless sensor networks,
Receivers,
Intrusion detection,
Pulse modulation"
Control and navigation of formations of car-like robots on a receding horizon,"The formulation and solution of a minimum time optimal control problem for a formation conformed by nonholonomic car-like mobile robots and a virtual leader reaching a target zone in an environment that includes dynamic and static obstacles with arbitrary shapes, is provided in this paper. The proposed approach for solving the formation to target zone minimum time problem, is formulated using receding horizon control methodologies. Simulation results using the proposed methodology are also reported.","Navigation,
Robot kinematics,
Control systems,
Orbital robotics,
Optimal control,
Shape control,
Mobile robots,
Vehicle dynamics,
Remotely operated vehicles,
Trajectory"
A Model of Job Scheduling with Deadline for Video-on-Demand System,"Scalability and high degree of playing continuity are key points to large-scale application of video-on-demand system. This paper presents a Peer-to-Peer(P2P) video-on-demand system CPVoD based on a cylindrical structure. It can enhance the efficiency of finding data by combining the accuracy and efficiency of DHT and the simplicity and practicability of Gossip protocol. A data scheduling strategy is introduced which starts from the perspective of reducing the expired data. By adding a small amount of local information to the data information exchange process based on Gossip protocol, it can help peers find the optimal download resource and finally enhance the playing continuity of system. A data downloading mathematical model is introduced, which converts the problem of improving the playing continuity of VoD system to the problem of finding the optimal solution for job scheduling with deadline, and a corresponding solution algorithm is also presented. The emulation experiments show that CPVoD can guarantee more than 99% of nodes joining the system normally and obtain a playing continuity more than 95% on the condition that server load is stable.",
A Wireless Sensor Network Clustering Algorithm Based on Energy and Distance,"Considering extremely limited energy of sensor nodes, how to utilize energy of the sensor nodes effectively to prolong survival time of wireless sensor network (WSN) is crucial. Aiming at monitoring application in particular regions, residual energy and distance constraint condition between cluster heads are taken into consideration. A new wireless sensor network clustering algorithm (EDL, Energy and Distance LEACH) is proposed to solve issues of finite energy and energy balance of WSN. Comparison of simulation tests between EDL clustering algorithm and LEACH algorithm was conducted. Results showed that the algorithm could make cluster head distribute in the actual confined area uniformly and avoid unnecessary energy loss which was caused by distance induced wireless interference.","Wireless sensor networks,
Clustering algorithms,
Attenuation,
Energy loss,
Circuits,
Power engineering and energy,
Energy consumption,
Energy efficiency,
Routing protocols,
Computer science"
Redundancy approach to increase the availability and reliability of radio communication in industrial automation,By the use of radio communication in industrial environments the influence of multi-path propagation is a fundamental problem especially for safety-related applications. The prevailing locally variation of field strength is hardly predictable and continuously changing. A conception with a wireless redundant data transmission was developed as an approach to reduce the problematic influences. A lot of preliminary investigations in real test environments with different ISM-Radio bands are necessary for this concept. A combination of space and frequency diversity was indicated as a most suitable solution. With this multiple radio channel concept a remarkable improvement of the availability of the radio link can be achieved.,"Redundancy,
Availability,
Radio communication,
Communication industry,
Automation,
Frequency diversity,
Fading,
Computer industry,
Radio link,
DC motors"
Robust CDN replica placement techniques,"Creating replicas of frequently accessed data objects across a read-intensive Content Delivery Network (CDN) can result in reduced user response time. Because CDNs often operate under volatile conditions, it is of the utmost importance to study replica placement techniques that can cope with uncertainties in the system parameters. We propose four CDN replica placement heuristics that guarantee a robust performance under the uncertainty of arbitrary CDN server failures. By robust performance we mean the solution quality that a heuristic guarantees given the uncertainties in system parameters. The simulation results reveal interesting characteristics of the studied heuristics. We report these characteristics with a detailed discussion on which heuristics to utilize for robust CDN data replication given a specific scenario.","Robustness,
Delay,
Uncertainty,
Resource management,
Degradation,
Mathematical model,
Costs,
Neodymium,
Computer science,
Active appearance model"
Dynamic Power Allocation Under Arbitrary Varying Channels - An Online Approach,"A major problem in wireless networks is coping with limited resources, such as bandwidth and energy. These issues become a major algorithmic challenge in view of the dynamic nature of the wireless domain. We consider in this paper the single-transmitter power assignment problem under time-varying channels, with the objective of maximizing the data throughput. It is assumed that the transmitter has a limited power budget, to be sequentially divided during the lifetime of the battery. We deviate from the classic work in this area, which leads to explicit ""water-filling"" solutions, by considering a realistic scenario where the channel state quality changes arbitrarily from one transmission to the other. The problem is accordingly tackled within the framework of competitive analysis, which allows for worst case performance guarantees in setups with arbitrarily varying channel conditions. We address both a ""discrete"" case, where the transmitter can transmit only at a fixed power level, and a ""continuous"" case, where the transmitter can choose any power level out of a bounded interval. For both cases, we propose online power-allocation algorithms with proven worst-case performance bounds. In addition, we establish lower bounds on the worst-case performance of any online algorithm, and show that our proposed algorithms are optimal.","Transmitters,
Batteries,
Resource management,
Wireless networks,
Bandwidth,
Throughput,
Communications Society,
Laboratories,
Computer science,
Time-varying channels"
Locality preserving constraints for super-resolution with neighbor embedding,"In this paper, we revisit the manifold assumption which has been widely adopted in the learning-based image super-resolution. The assumption states that point-pairs from the high-resolution manifold share the local geometry with the corresponding low-resolution manifold. However, the assumption does not hold always, since the one-to-multiple mapping from LR to HR makes neighbor reconstruction ambiguous and results in blurring and artifacts. To minimize the ambiguous, we utilize Locality Preserving Constraints (LPC) to avoid confusions through emphasizing the consistency of localities on both manifolds explicitly. The LPC are combined with a MAP framework, and realized by building a set of cell-pairs on the coupled manifolds. Finally, we propose an energy minimization algorithm for the MAP with LPC which can reconstruct high quality images compared with previous methods. Experimental results show the effectiveness of our method.",
Quantitative Model Checking of Continuous-Time Markov Chains Against Timed Automata Specifications,"We study the following problem: given a continuous-time Markov chain (CTMC) C, and a linear real-time property provided as a deterministic timed automaton (DTA) A, what is the probability of the set of paths of C that are accepted by A (C satisfies A)? It is shown that this set of paths is measurable and computing its probability can be reduced to computing the reachability probability in a piecewise deterministic Markov process (PDP). The reachability probability is characterized as the least solution of a system of integral equations and is shown to be approximated by solving a system of partial differential equations. For the special case of single-clock \DTA, the system of integral equations can be transformed into a system of linear equations where the coefficients are solutions of ordinary differential equations.","Automata,
Integral equations,
Biological system modeling,
Differential equations,
Clocks,
Markov processes,
Partial differential equations,
Stochastic systems,
Probabilistic logic,
Stochastic processes"
Teaching Digital Systems in the Context of the New European Higher Education Area: A Practical Experience,"This paper describes a practical experience of adapting the teaching of a course in Computer Technology (CT) to the new demands of the European Higher Education Area (EHEA). CT is a core course taught in the first year of the degree program Technical Engineering in Management Computing in the Faculty of Computer Science at the University of A Coruna (UDC), Spain. The contents of this course are mainly devoted to the design of digital systems. The main purpose of the adaptation has been to focus more on students, clearly defining the abilities they will develop during the course and suggesting activities that facilitate the development of those abilities. The aim of this work is to describe how this adaptation was performed, the materials and activities prepared, the difficulties encountered, the goals achieved and the response of students and teachers to these changes.","Curriculum development,
Computer science education,
Digital systems,
Education courses"
Design of haptic interface for brickout game,"This paper introduces a haptic interface for brick games. Conventionally the game user uses mouse or keyboard to play the brick game. However, these input devices do not provide intuitive interface for the game and any tactile feedback to the user. We use a haptic dial to add tactile feedback to enhance game effects in addition to visual and sound effects. The user changes the position of the paddle by spinning the dial knob and feels various tactile feedbacks according to the game context. Tactile feedbacks include friction, jog dial, barrier, detent, and any combinations of these effects which are programmed based on the amount, frequency, and direction of torque along the rotational path. These effects are used as either penalties or useful tools. The proposed haptic dial interface makes the game more fun and gives a very intuitive interface to the game user.","Haptic interfaces,
Games,
Torque,
Mice,
Keyboards,
Spinning,
Friction,
Senior citizens,
Force feedback,
Actuators"
A Hybrid System Using Symbolic and Numeric Knowledge for the Semantic Annotation of Sulco-Gyral Anatomy in Brain MRI Images,"This paper describes an interactive system for the semantic annotation of brain magnetic resonance images. The system uses both a numerical atlas and symbolic knowledge of brain anatomical structures depicted using the semantic Web standards. This knowledge is combined with graphical data, automatically extracted from the images by imaging tools. The annotations of parts of gyri and sulci, in a region of interest, rely on constraint satisfaction problem solving and description logics inferences. The system is run on a client-server architecture, using Web services and including a sophisticated visualization tool. An evaluation of the system was done using normal (healthy) and pathological cases. The results obtained so far demonstrate that the system produces annotations with high precision and quality.","Anatomy,
Magnetic resonance imaging,
Interactive systems,
Magnetic resonance,
Brain,
Anatomical structure,
Semantic Web,
Data mining,
Problem-solving,
Logic"
Detecting doubly compressed images based on quantization noise model and image restoration,"Since JPEG has been a popularly used image compression standard, forgery detection in JPEG images now plays an important role. Forgeries on compressed images often involve recompression and tend to erase those forgery traces existed in uncompressed images. We could, however, try to discover new traces caused by recompression and use these traces to detect the recompression forgeries. Quantization is the critical step in lossy compression which maps the DCT coefficients in an irreversible way under the quantization constraint set (QCS) theorem. In this paper, we first derive that a doubly compressed image no longer follows the QCS theorem and then propose a novel quantization noise model to characterize single and doubly compressed images. In order to detect double compression forgery, we further propose to approximate the uncompressed ground truth image using image restoration techniques. We conduct a series of experiments to demonstrate the validity of the proposed quantization noise model and also the effectiveness of the forgery detection method with the proposed image restoration techniques.","Image coding,
Quantization,
Image restoration,
Image retrieval,
Content based retrieval,
Feedback,
Engines,
Filters,
MPEG 7 Standard,
Testing"
The gigavision camera,"We propose a new image device called gigavision camera. The main differences between a conventional and a gigavision camera are that the pixels of the gigavision camera are binary and orders of magnitude smaller. A gigavision camera can be built using standard memory chip technology, where each memory bit is designed to be light sensitive. A conventional gray level image can be obtagray level image can be obtained from the binary gigavision image by low-ined from the binary gigavision image by low-pass filtering and sampling. The main advantage of the gigavision camera is that its response is non-linear and similar to a logarithmic function, which makes it suitable for acquiring high dynamic range scenes. The larger the number of binary pixels considered, the higher the dynamic range of the gigavision camera will be. In addition, the binary sensor of the gigavision camera can be combined with a lens array in order to realize an extremely thin camera. Due to the small size of the pixels, this design does not require deconvolution techniques typical of similar systems based on conventional sensors.","Cameras,
Sensor arrays,
Dynamic range,
Low pass filters,
Filtering,
Image sampling,
Layout,
Lenses,
Deconvolution,
Sensor systems"
Exact and fast L1 cache simulation for embedded systems,"In recent years, the gap between the cycle time of processors and memory access time has been increasing. One of the solutions to solve this problem is to use a cache. But just using a large cache may not reduce the total memory access time. We can have an optimal cache configuration which minimizes overall memory access time by varying the three cache parameters: a cache set size, a line size, and an associativity. In this paper, we propose two exact cache simulation algorithms: CRCB1 and CRCB2, based on Cache Inclusion Property. They realize exact cache simulation but increase simulation speed dramatically. By using our approach, the number of cache hit/miss judgments required for simulating all the cache configurations is reduced to 31.4%-93.6% compared to conventional approaches. As a result, our proposed approach totally runs an average of 1.8 times faster and a maximum of 3.3 times faster compared to the fastest approach proposed so far. Our proposed exact cache simulation approach achieves the world fastest L1 cache simulation.","Embedded system,
Analytical models,
Computational modeling,
Computer simulation,
Computer science"
A software solution for dynamic stack management on scratch pad memory,"In an effort to make processors more power efficient scratch pad memory (SPM) have been proposed instead of caches, which can consume majority of processor power. However, application mapping on SPMs remain a challenge. We propose a dynamic SPM management scheme for program stack data for processor power reduction. As opposed to previous efforts, our solution does not mandate any hardware changes, does not need profile information, and SPM size at compile-time, and seamlessly integrates support for recursive functions. Our technique manages stack frames on SPM using a scratch pad memory manager (SPMM), integrated into the application binary by the compiler. Our experiments on benchmarks from MiBench [15] show average energy savings of 37% along with a performance improvement of 18%.","Memory management,
Scanning probe microscopy,
Energy management,
Application software,
Hardware,
Logic arrays,
Engineering management,
Computer science,
Power engineering and energy,
Data analysis"
A memory optimization technique for software-managed scratchpad memory in GPUs,"With the appearance of massively parallel and inexpensive platforms such as the G80 generation of NVIDIA GPUs, more real-life applications will be designed or ported to these platforms. This requires structured transformation methods that remove existing application bottlenecks in these platforms. Balancing the usage of on-chip resources, used for improving the application performance, in these platforms is often non-intuitive and some applications will run into resource limits. In this paper, we present a memory optimization technique for the software-managed scratchpad memory in the G80 architecture to alleviate the constraints of using the scratchpad memory. We propose a memory optimization scheme that minimizes the usage of memory space by discovering the chances of memory reuse with the goal of maximizing the application performance. Our solution is based on graph coloring. We evaluated our memory optimization scheme by a set of experiments on an image processing benchmark suite in medical imaging domain using NVIDIA Quadro FX 5600 and CUDA. Implementations based on our proposed memory optimization scheme showed up to 37% decrease in execution time comparing to their naïve GPU implementations.","Yarn,
Application software,
Memory management,
Computer architecture,
Computer science,
Bandwidth,
Delay,
Runtime,
Parallel processing,
Central Processing Unit"
Mobile relay configuration in data-intensive Wireless Sensor Networks,"Recently, Wireless Sensor Networks (WSNs) have become increasingly available for data-intensive applications such as micro-climate monitoring, precision agriculture, and audio/video surveillance. A key challenge faced by data-intensive WSNs is to transmit the sheer amount of data generated within an application's lifetime to the base station despite the fact that sensor nodes have limited power supplies such as batteries or small solar panels. In this paper, we propose to use low-cost disposable mobile relays to reduce the energy consumption of data-intensive WSNs. Different from previous work, our approach does not require complex motion planning of mobile nodes, and hence can be implemented on a number of low-cost mobile sensor platforms. Moreover, we integrate the energy consumption due to both mobility and wireless transmissions into a holistic optimization framework. The optimal relay configuration is shown to depend on both the positions of nodes and the amount of data to be sent. We develop two algorithms that iteratively refine the configuration of mobile relays and converge to the optimal solution. These algorithms have efficient distributed implementations that do not require explicit synchronization. Our simulation results based on realistic energy models obtained from existing mobile and static sensor platforms show that our algorithms significantly outperform the best existing solutions.","Relays,
Wireless sensor networks,
Iterative algorithms,
Energy consumption,
Monitoring,
Agriculture,
Video surveillance,
Power generation,
Solar power generation,
Base stations"
Boosting Associated Pairing Comparison Features for pedestrian detection,"This paper proposes a novel approach to boost a set of Associated Pairing Comparison Features (APCFs) in Granular Space for pedestrian detection, in which Pairing Comparison of Color (PCC) and Pairing Comparison of Gradient (PCG) are two kinds of essential elements. A PCC is a Boolean color comparison of two granules and a PCG is a Boolean gradient comparison of two granules, which is motivated by animal vision system that using simple comparison information in both color and gradient modes for visual perception. Unlike previous works that describe object shape, our method is to find the symbiosis of colors or gradient orientations. Experiments on multi-view multi-pose pedestrian data demonstrate the efficacy of the proposed approach.","Boosting,
Computer vision,
Object detection,
Shape,
Space technology,
Machine vision,
Detectors,
Face detection,
Animals,
Conferences"
Irregular Sensing Range Detection Model for Coverage Based Protocols in Wireless Sensor Networks,"In order to optimize the performance of wireless sensor networks, based on the boolean sensing model (BSM) many coverage preserving protocols have been proposed; however, BSM model is too idealistic to be maintained in many applications. Sensing range could be changed by environment factors such as the obstacle and weather condition. BSM based protocols can not achieve their design goals because of the errors of coverage measurement. In this paper, we proposed our irregular sensing range detection model (RDM) which solves the range detection problem through estimating the sensing range by a revised Â¿-shape algorithm. A RDM based distributed algorithm for measuring the area coverage is proposed. Its complexity are analyzed and compared to the centralized Â¿-shape based area coverage algorithm. A set of simulation experiments is carried out to evaluate the performance of our algorithm in terms of coverage accuracy, coverage error and complexity. The result shows that the boolean sensing model will lead to errors when obstacles are injected into environment; however, our range detection model can detect the changes of environments and help protocols adapt to it.","Wireless application protocol,
Wireless sensor networks,
Surveillance,
Area measurement,
Monitoring,
Event detection,
Computer science,
Application software,
Distributed algorithms,
Algorithm design and analysis"
Hierarchical framework for direct gradient-based time-to-contact estimation,"The time-to-contact (TTC) estimation is a simple and convenient way to detect approaching objects, potential danger, and to analyze surrounding environment. TTC can be estimated directly from single camera though neither distance nor speed information can be estimated with single cameras. Traditional TTC estimation depends on “interesting feature points” or object boundaries, which is noisy and time consuming. In [13], we propose a direct “gradient-based” method to compute time-to-contact in three special cases that avoid feature points/lines and can take advantages of all related pixels for better computation. In this follow-up paper, we discuss the method to deal with the most general cases and propose a hierarchical fusion framework for direct gradient-based time-to-contact estimation. The new method enhances accuracy, robustness and is computationally efficient, which is important to provide fast response for vehicle applications.",
A taxonomy based semantic similarity of documents using the cosine measure,"In this paper, we present a new method for calculating semantic similarities between documents. This method is based on cosine similarity calculation between concept vectors of documents obtained from a taxonomy of words that captures IS-A relations. The calculation of semantic similarities between documents is a very time consuming task, since it is necessary first to calculate semantic similarities between each pair of words that appear on different documents. In this paper, we present a new method to calculate semantic similarities between documents which results in faster computational time. Both a taxonomy based semantic similarity and cosine similarity are employed. First, the concept vectors of documents are obtained by extending the terms in the document vectors with their corresponding IS-A concepts. Cosine similarity is then calculated between those concept vectors of documents. Thus, the overall similarity between documents is a combination of cosine similarity and semantic similarity. The proposed semantic similarity is tested in document clustering problem. The experimental results show that our method achieves a good performance.","Taxonomy,
Search engines,
Frequency,
Recommender systems,
Computational complexity,
Testing,
Web sites,
World Wide Web,
Internet,
Information retrieval"
Introduction to GPU Programming with GLSL,"One of the challenging advents in Computer Science in recent years was the fast evolution of parallel processors, specially the GPU – graphics processing unit. GPUs today play a major role in many computational environments, most notably those regarding real-time graphics applications, such as games. The digital game industry is one of the main driving forces behind GPUs, it persistently elevates the state-of-art in Computer Graphics, pushing outstanding realistic scenes to interactive levels. The evolution of photo realistic scenes consequently demands better graphics cards from the hardware industry. Over the last decade, the hardware has not only become a hundred times more powerful, but has also become increasingly customizable allowing programmers to alter some of previously fixed functionalities. This tutorial is an introduction to GPU programming using the OpenGL Shading Language – GLSL. It comprises an overview of graphics concepts and a walk-through the graphics card rendering pipeline. A thorough understanding of the graphics pipeline is extremely important when designing a program in GPU, known as a shader. Throughout this tutorial, the exposition of the GLSL language and GPU programming details are followed closely by examples ranging from very simple to more practical applications. It is aimed at an audience with no or little knowledge on the subject.","Computer graphics,
Layout,
Hardware,
Pipelines,
Computer science,
Application software,
Computer industry,
Toy industry,
Programming profession,
Rendering (computer graphics)"
Robust entrainment to natural oscillations of asymmetric systems arising from animal locomotion,"We consider a class of linear flexible mechanical systems arising from the dynamics of animal locomotion. A distinctive property of such systems is that the stiffness matrix is asymmetric. Extending the standard notion to this class, we define the natural oscillation as a free response under the damping compensation to achieve marginal stability. As a benchmark, a link chain system in a fluid environment is considered, and its natural oscillation is shown to exhibit travelling waves appropriate for undulatory swimming. Moreover, we propose nonlinear feedback controllers, inspired by neuronal dynamics, to achieve entrainment to the natural oscillation.","Robustness,
Animals,
Mechanical systems,
Damping,
Resonance,
Robot kinematics,
Control systems,
Fluid dynamics,
Vehicle dynamics,
Stability"
Combining Multiple Feature Selection Methods for Text Categorization by Using Rank-Score Characteristics,"Feature selection is an important method for improving the efficiency and accuracy of text categorization algorithmsby removing redundant and irrelevant terms from the corpus.Extensive researches have been done to improve the performance ofindividual feature selection methods, but not much on their combinations.In this paper, we propose a method of combining multiple feature selection methods by using the Combinatorial Fusion Analysis (CFA). A rank-score function and its graph, called rank-score graph,are adopted to measure the diversity of different feature selection methods.We have shown that a combination of multiple feature selection methods can outperform a single method only if each individual feature selection method has unique scoring behavior and relatively high performance. Moreover, it is shown that the rank-score function and rank-score graph are useful for the selection of a combination of feature selection methods.",
"Building a Responsibility Model Including Accountability, Capability and Commitment","This paper aims at building a responsibility model based on the concepts of Accountability, Capability and Commitment. The model's objectives are firstly to help organizations for verifying the organizational structure and detecting policy problems and inconsistency. Secondly, the paper brings up a conceptual framework to support organization for defining their corporate, security and access control policies. Our work provides a preliminary review of the researches performed in that field and proposes, based on the analyses, an UML responsibility model and a definition of all its concepts. Thereafter, to propose a formal representation of the model, we have selected the suitable language and logic system. The analyze highlights that an important variable is whether the responsibility is perceived at a user or at a company level.","Logic,
Access control,
Unified modeling language,
Testing,
Availability,
Computer security,
Computer science,
Buildings,
Performance analysis,
Business"
Design tests: An approach to programmatically check your code against design rules,"Assuring that a program conforms to its specification is a key concern in software quality assurance. Although there is substantial tool support to check whether an implementation complies to its functional requirements, checking whether it conforms to its design remains as an almost completely manual activity. In this paper, we present the concept of design tests, which are test-like programs that automatically check whether an implementation conforms to a specific design rule. Design rules are implemented directly in the target programming language in the form of tests. As a proof of concept, we present DesignWizard, an API developed to support design tests for Java programs as JUnit test cases. We applied design tests in two case studies and observed that our approach is suitable to check conformance automatically. Moreover, we observed that designers and programmers appreciate design tests as an executable documentation that can be easily kept up to date.","Software testing,
Automatic testing,
Java,
Programming profession,
Computer languages,
Documentation,
Software quality,
Algorithm design and analysis,
Computer science,
XML"
User selection in multiuser MIMO systems with secrecy considerations,"This paper investigates simple user selection strategies in a multiuser downlink system with a single transmitter, multiple legitimate receivers, and a single eavesdropper, where all nodes are equipped with multiple antennas. No information regarding the eavesdropper is presumed at the transmitter, and we examine the MIMO downlink channel with either single or multiple data streams per receiver. The transmitter uses linear beamforming techniques based on generalized zero-forcing with a corresponding minimum SINR or rate requirement per receiver. In both cases the information signal is transmitted with just enough power to guarantee the desired Quality-of-Service (QoS) at the desired receivers, while the remainder of the power is used to broadcast artificial noise that selectively degrades the passive eavesdropper's signal. Numerical simulations displaying the power consumption and the number of selected users provided by each selection scheme demonstrate the effectiveness of the proposed schedulers in ensuring physical-layer security.","MIMO,
Transmitters,
Downlink,
Receiving antennas,
Transmitting antennas,
Array signal processing,
Signal to noise ratio,
Quality of service,
Broadcasting,
Degradation"
Enhanced Security for Online Exams Using Group Cryptography,"While development of the Internet has contributed to the spread of online education, online exams have not been widely adopted. An online exam is defined here as one that takes place over the insecure Internet, and where no proctor is in the same location as the examinees. This paper proposes an enhanced secure online exam management environment mediated by group cryptography using remote monitoring and control of ports and input. The target domain of this paper is that of online exams for math or English contests in middle or high school, as well as exams in online university courses with students in remote locations.","Monitoring,
Security,
Computers,
Cryptography,
Internet,
Education,
Servers"
Building a transportation information system using only GPS and basic SMS infrastructure,"This work consists of two main components: (a) a longitudinal ethnographic study in Kyrgyzstan that demonstrates the importance of transportation resources in the developing world and how to plan for an appropriate ICT solution, and (b) the results of a proof-of-concept system engineered to create a bottom-up, transportation information infrastructure using only GPS and SMS. Transportation is a very important shared resource; enabling efficient and effective use of such resources aids overall development goals.","Transportation,
Information systems,
Global Positioning System,
GSM,
Systems engineering and theory,
Hardware,
Modems,
Vehicles,
Prediction algorithms,
System testing"
"Educational games (EG) design framework: Combination of game design, pedagogy and content modeling","Previous studies have found that educational games (EG) provide immersion, motivation, fun and high level of engagement. Studies also found that EG are able to teach 21st century skills. Thus, it is worth studying how this form of entertainment can be adapted into our teaching and learning strategies in order to develop our students' motivation and engagement, which is crucial in successful learning. However, designing and development of educational games with authentic learning content while maintaining the fun criteria can be challenging. Hence, a good design methodology and guideline is needed in assisting games development team to design an effective learning game. Our main attention is to produce a framework in educational games design for higher education. This paper analyses and compares a few frameworks available, then suggests a few criteria needed both from game design and pedagogy points of view. Hopefully it will shed some light in educational games design issues for higher education students.","Games,
Educational institutions,
Informatics,
Design methodology,
Guidelines,
Computer science education,
Magnetic heads,
Toy industry,
Home computing,
Software"
Resilient Cluster Leader Election for Wireless Sensor Networks,"Sensor nodes are often organized into clusters for efficiency and scalability purposes. Every sensor cluster is managed by a cluster leader during the network operation such as routing and data aggregation. Since managing a cluster consumes substantial energy, the cluster leader needs to be re-elected from time to time for load balancing. In hostile environments, it is critical to ensure the security of such leader election. This paper proposes an efficient, resilient, and fully distributed leader election protocol for sensor networks. It only uses efficient symmetric key operations and guarantees that (i) benign cluster members will elect the same leader as long as they are well- connected, and (ii) attackers cannot impact the leader election process to increase or decrease the chance of a benign member being elected as a cluster leader. In addition, the proposed method can quickly recover from message loss or malicious attacks. The evaluation results also demonstrate the efficiency and effectiveness of this approach.","Nominations and elections,
Wireless sensor networks,
Protocols,
Computer science,
Scalability,
Computer network management,
Routing,
Energy management,
Load management,
Computer crime"
Performance Evaluation of Power Saving Scheme with Dynamic Transmission Capacity Control,"Our major purpose in this study is to develop a method for achieving power conservation with a network itself. The proposed power saving approach takes traffic characteristics of network into consideration. If network switch can adapt its processing speed to arrival traffic rate, power saving can be expected depending on the network utilization. The same idea is already materialized just as Intel's SpeedStep technology. We first confirm this consideration by measuring the power consumption of some switches and show that a Gigabit Ethernet port consumes 2 or 3 W with an addition of a link. Based on this experiment, we propose a scheme to achieve the dynamic control of the transmission capacity between switches in order to reduce the power consumption based upon the change in traffic volume. To control the transmission speed dynamically and gradually, Link Aggregation is used in our scheme. From simulation studies, we show that the proposed architecture can achieve both high power saving capability and sufficient feasibility.","Text categorization,
Clustering algorithms,
Impurities,
Statistics,
Computer science,
Testing,
Data mining,
Conferences,
Availability,
Labeling"
3D reconstruction based on SIFT and Harris feature points,"This paper presents a new 3D reconstruction method using feature points extracted by the SIFT and Harris corner detector. Since the SIFT feature points can be detected stably and relatively accurately, the proposed algorithm first uses the SIFT matching points to calculate the fundamental matrix. On the other hand many of the feature points detected by the SIFT are not what we need for reconstruction, so by combining the SIFT feature points with the Harris corners it is possible to obtain more vivid and detailed 3D information. Experiments have been conducted to validate the proposed method.","Image reconstruction,
Cameras,
Computer vision,
Image sequences,
Feature extraction,
Calibration,
Iterative algorithms,
Detectors,
Robustness,
Object detection"
Pushing the performance of Biased Neighbor Selection through Biased Unchoking,"Locality promotion in P2P content distribution networks is currently a major research topic. One of the goals of all discussed approaches is to reduce the interdomain traffic that causes high costs for ISPs. However, the focus of the work in this field is generally on the type of locality information that is provided to the overlay and on the entities that exchange this information. An aspect that is mostly neglected is how this information is used by the peers. In this paper, we consider the predominant approach of Biased Neighbor Selection and compare it with Biased Unchoking, which is an alternative locality aware peer selection strategy that we propose in this paper. We show that both mechanisms complement each other for the BitTorrent file sharing application and achieve the best performance when combined.",
Iterative learning control for multi-agent formation,"This paper employs iterative learning control scheme to generate a sequence of control signals for multi-agent formation control. It is assumed that individual agent of a group of multi-agents is governed by nonlinear dynamics, which could be known in part; in such case, we would like to find control sequences of individual agents such that they form a desired formation with respect to other agents, from initial starting points to final stop points. That is, we would like to ensure that the multi-agents form relative desired states with respect to other agents along the desired trajectory. The algorithm established in this paper can be used to find a control sequence of multi-agent systems for keeping relative formation, in off-line tuning manner. The utility of the algorithm established in this paper can be therefore used for finding optimal control strategy of nonlinear dynamic systems with partially available system information.","Control systems,
Optimal control,
Iterative algorithms,
Signal generators,
Nonlinear dynamical systems,
Convergence,
Mechatronics,
Intelligent systems,
Multiagent systems,
Robots"
Active skeleton for non-rigid object detection,"We present a shape-based algorithm for detecting and recognizing non-rigid objects from natural images. The existing literature in this domain often cannot model the objects very well. In this paper, we use the skeleton (medial axis) information to capture the main structure of an object, which has the particular advantage in modeling articulation and non-rigid deformation. Given a set of training samples, a tree-union structure is learned on the extracted skeletons to model the variation in configuration. Each branch on the skeleton is associated with a few part-based templates, modeling the object boundary information. We then apply sum-and-max algorithm to perform rapid object detection by matching the skeleton-based active template to the edge map extracted from a test image. The algorithm reports the detection result by a composition of the local maximum responses. Compared with the alternatives on this topic, our algorithm requires less training samples. It is simple, yet efficient and effective. We show encouraging results on two widely used benchmark image sets: the Weizmann horse dataset [7] and the ETHZ dataset [16].","Skeleton,
Object detection,
Layout,
Lighting,
Least squares methods,
Light sources,
Least squares approximation,
Automation,
Educational institutions,
Information science"
Study on Association Rules Mining Based on Searching Frequent Free Item Sets Using Partition,"Mining of association rules is an important problem in data mining, given a large set of data, extracting frequent item sets in this set is a challenging job in data mining. Item sets matching is the chief problem in extracting frequent item sets. And item set matching is the bottleneck of the mining process. It also has been proved that extracting frequent free item sets is a useful method. Many efficient algorithms have been proposed in the literature. The idea presented in this paper is to divide the database into multiple partitions and then find frequent free item sets in each partition, then merge the several partitions to generate other frequent free item sets and count the support. The algorithm costs little memory to save additional support numbers of item sets in each partition but greatly reduces the time of item set matching which is the bottleneck of the mining process. The experiments on real datasets have showed its good performance.","Association rules,
Data mining,
Uncertainty,
Production systems,
Decision making,
Computer science,
Information processing,
Inference algorithms,
Computational efficiency,
Control systems"
Design and implementation of an ASIC-based sensor device for WSN applications,"The practical applications of wireless sensor networks require the sensor devices to be high in computation ability, low in power consumption, small in size, as well as competitive in cost. In addition, there are other particular attributes which require special consideration in sensor device design. These include, for example, the capabilities in protecting the embedded program from foreign intrusion and supporting efficient air-programming, which are very common requirements in many large-scale applications. In this paper, we describe the system architecture and design methodology of an ASCI-based sensor network device to meet those attributes for a class of applications. Compared with existing works, the unique features of the sensor device include: (1) a module-based extensible processor and co-processor cooperation architecture is designed with application-specific components to accelerate the signal collection, processing, and networking via hardware-software co-design to balance computation efficiency and design flexibility; (2) a program protection mechanism is developed based on un-resemble coder algorithm to protect the program data from being read out by system intruders so as to improve the security of the sensor device; (3) an air-programming component is proposed to enable an efficient remote programming which is a very common requirement in network deployment and maintenance. In this paper, we present the design, implementation, and evaluation of the prototype sensor device based on a general configurable FPGA platform for developing next-generation sensor devices. Initial results of the first version of prototype chip will also be introduced.","Wireless sensor networks,
Sensor phenomena and characterization,
Sensor systems and applications,
Computer networks,
Signal design,
Algorithm design and analysis,
Protection,
Prototypes,
Energy consumption,
Costs"
Active location reporting for emergency call in UMTS IP multimedia subsystem,"The IP multimedia core network subsystem (IMS) provides multimedia services for Universal Mobile Telecommunications System (UMTS). In IMS, an emergency call is established by an Emergency-Call Session Control Function (ECSCF). The E-CSCF dispatches the call to the nearest public safety answering point (PSAP) according to the location of the caller. After emergency call setup, the caller s location is tracked by the PSAP through Location Polling. This paper investigates the performance of location tracking. Then we propose the active location reporting scheme to improve the performance of location tracking. Our study indicates that the active location reporting scheme may significantly outperform the location polling scheme.","3G mobile communication,
Multimedia systems,
Radio access networks,
Ground penetrating radar,
Global Positioning System,
Safety,
Access protocols,
Computer science,
Telecommunication control,
Telecommunication standards"
Tracking a large number of objects from multiple views,"We propose a multi-object multi-camera framework for tracking large numbers of tightly-spaced objects that rapidly move in three dimensions. We formulate the problem of finding correspondences across multiple views as a multidimensional assignment problem and use a greedy randomized adaptive search procedure to solve this NP-hard problem efficiently. To account for occlusions, we relax the one-to-one constraint that one measurement corresponds to one object and iteratively solve the relaxed assignment problem. After correspondences are established, object trajectories are estimated by stereoscopic reconstruction using an epipolar-neighborhood search. We embedded our method into a tracker-to-tracker multi-view fusion system that not only obtains the three-dimensional trajectories of closely-moving objects but also accurately settles track uncertainties that could not be resolved from single views due to occlusion. We conducted experiments to validate our greedy assignment procedure and our technique to recover from occlusions. We successfully track hundreds of flying bats and provide an analysis of their group behavior based on 150 reconstructed 3D trajectories.","Trajectory,
Image reconstruction,
Cameras,
Biology,
Multidimensional systems,
Layout,
Computer vision,
State estimation,
Stress,
Surveillance"
Relative bearing estimation from commodity radios,"Relative bearing between robots is important in applications like pursuit-evasion [11] and SLAM [7]. This is also true in sensor networks, where the bearing of one sensor node relative to another has been used for localization [5], [18], [20] and topology control [14], [21], [6]. Most systems use dedicated sensors like an IR array or a camera to obtain relative bearing. We study the use of radio signal strength (RSS) in commodity radios for obtaining relative bearing. We show that by using the robot's mobility, commodity radios can be used to obtain coarse relative bearing. This measurement can be used for a suite of applications that do not require very precise bearing measurement. We analyze signal strength variations in simulation and experiment and also show an algorithm that uses this coarse bearing computation in a practical setting.","Direction of arrival estimation,
Sensor arrays,
Robot sensing systems,
Simultaneous localization and mapping,
Network topology,
Sensor systems,
Infrared sensors,
Robot vision systems,
Cameras,
Mobile robots"
A Software for S-box Performance Analysis and Test,"S-box (Substitution box) is one of the core components in the block cipher and plays an important role in the process of encrypting plaintext. In this paper, the performance indexes of S-box are summarized and analyzed. The corresponding numeric methods for calculating them are presented. Then, the software is developed, which can not only calculate the performance indexes of S-box but also find the ones that satisfy the performance requirement from lots of S-boxes. Based on the simulation test, the conclusion can be drawn that the software is very useful to aid the research of S-box and gives a good support to design block cipher with high security.","Software performance,
Performance analysis,
Software testing,
Cryptography,
Educational institutions,
Information security,
Electronic commerce,
Electronic equipment testing,
Conference management,
Computer science"
A research roadmap for model-driven design of embedded systems for automation components,"The aim of this paper is to describe a research roadmap for a multi-domain model-driven embedded systems design approach and the corresponding meta-model which is applicable to the domain of complex Industrial Automation and Control Systems (IACS). The special requirements of the industrial automation sector are taken into account by this novel approach, utilizing existing model-driven techniques. This approach is currently being developed in the Framework Seven (FP7) Embedded Systems Design project MEDEIA funded by the European Commission.","Embedded system,
Design automation,
Automatic control,
Electrical equipment industry,
Control systems,
Industrial control,
Robotics and automation,
Design engineering,
Control system synthesis,
Production"
From retrospect to prospect: Assessing modularity and stability from software architecture,"Architecture-level decisions, directly influenced by environmental factors, are crucial to preserve modularity and stability throughout software development life-cycle. Tradeoffs of modularization alternatives, such as aspect-oriented vs. object-oriented decompositions, thus need to be assessed from architecture models instead of source code. In this paper, we present a suite of architecture-level metrics, taking external factors that drive software changes into consideration and measuring how well an architecture produces independently substitutable modules. We formalize these metrics using logical models to automate quantitative stability and modularity assessment. We evaluate the metrics using eight aspect-oriented and object-oriented releases of a software product-line architecture, driven by a series of heterogeneous changes. By contrasting with an implementation-level analysis, we observe that these metrics can effectively reveal which modularization alternative generates more stable, modular design from high-level models.","Software architecture,
Computer architecture,
Object oriented modeling,
Computer science,
Environmental factors,
Programming,
Informatics,
Software measurement,
Stability analysis,
Current measurement"
"On the feasibility of the visible wavelength, at-a-distance and on-the-move iris recognition","The dramatic growth in practical applications for iris biometrics has been accompanied by relevant developments in the underlying algorithms and techniques. Among others, one active research area concerns about the development of iris recognition systems less constrained to users, either increasing the imaging distances, simplifying the acquisition protocols or the required lighting conditions. In this paper we address the possibility of perform reliable recognition using visible wavelength images captured under high heterogeneous lighting conditions, with subjects at-a-distance (between 4 and 8 meters) and on-the-move. The feasibility of this extremely ambitious type of recognition is analyzed, its major obstacles and challenges discussed and some directions for forthcoming work pointed.",
Learning mappings for face synthesis from near infrared to visual light images,"This paper deals with a new problem in face recognition research, in which the enrollment and query face samples are captured under different lighting conditions. In our case, the enrollment samples are visual light (VIS) images, whereas the query samples are taken under near infrared (NIR) condition. It is very difficult to directly match the face samples captured under these two lighting conditions due to their different visual appearances. In this paper, we propose a novel method for synthesizing VIS images from NIR images based on learning the mappings between images of different spectra (i.e., NIR and VIS). In our approach, we reduce the inter-spectral differences significantly, thus allowing effective matching between faces taken under different imaging conditions. Face recognition experiments clearly show the efficacy of the proposed approach.",
Multi-robot SLAM using ceiling vision,"In this paper we present a new vision-based SLAM approach for multi-robot formulation. For a cooperative map reconstruction, the robots have to know each other's relative poses, but estimating these at the start of operation puts a limit on real applications. In our study, the robots start the single SLAM with their own global coordinate, and merge their maps during the operation by detecting the overlapped region of their maps. The robots automatically recognize the occurrence of map overlapping by matching their current frame with the maps built by other robots. With the robust data association technique from the ceiling-vision based SLAM, the proposed algorithm robustly detects the overlapping regions and estimates the accurate transformations for map alignment. In our experiment, we have verified that our algorithm successfully enables the multi-robot SLAM without any initial correspondence or encounter of robots.","Simultaneous localization and mapping,
Robot kinematics,
Robot sensing systems,
Robotics and automation,
Layout,
Merging,
Robustness,
Mobile robots,
Position measurement,
Robot vision systems"
A Volunteer-Computing-Based Grid Environment for Connect6 Applications,"This paper presents a volunteer-computing-based grid environment or called a desktop grid environment for Connect6 applications. The Connect6 application described in this paper is to let professional Connect6 players to develop or solve openings, based on two programs, NCTU6 and Verifier. NCTU6 is to make Connect6 moves, written by the team led by Wu [19][21]. NCTU6 Verifier (abbr. Verifier), modified from NCTU6, is to verify whether one player wins in a given game position, or to generate the defensive moves if not winning in the position. Since both NCTU6 and Verifier consume huge amount of computation resources and requires on-demand responses, we design a desktop grid environment that provides players with on-demand computing through dynamic resource provisioning. The underlying desktop grid achieves high throughput computing by harvesting the idle CPU times on desktop computers connected to the Internet.","Grid computing,
Application software,
Throughput,
Internet,
Computer science,
Sun,
Information science,
Central Processing Unit,
Computer applications,
Parallel processing"
Vehicle Objects Detection of Video Images Based on Gray-Scale Characteristics,"An integrated approach to detect moving vehicles, by using of gray-scale images with complicated background, applied in intelligent traffic surveillance is proposed. In this paper, firstly, color images are converted to gray-scale images in the images pre-processing. Then the methods of frame differencing and selective background updating are utilized to generate initial background and update current background. Furthermore, every processed image is filtered by fast median filter to remove noise caused by vidicon movement or background jitter. When the current background is obtained, moving objects in the video can be detected effectively by background frame differencing complemented with inter-frame differencing. Finally, morphological filtering is used for decreasing accumulative errors. The experimental result shows this approach can detect moving vehicles effectively in real time, which is suitable for video traffic surveillance system.","Vehicle detection,
Object detection,
Gray-scale,
Surveillance,
Intelligent vehicles,
Color,
Image converters,
Filters,
Background noise,
Jitter"
Signal processing techniques for spectrum sensing in cognitive radio systems: Challenges and perspectives,"Cognitive radio (CR) is regarded as an emerging technology to utilize the scarce RF (radio frequency) spectrum in opportunistic manner to increase the spectrum efficiency. However, major challenge to successful operation and realization of CR system is the identification and detection of primary user signals in the wide band regime to identify spectrum opportunities reliably and optimally. Several signal detection techniques for spectrum sensing have been proposed in the literature in order to identify idle spectrum so that the CR user can use those idle spectrum bands opportunistically without causing harmful interference to the licensed/primary users. In this article, we provide a comprehensive study of signal detection techniques for spectrum sensing in CR system. We also outline current state of the research and future perspectives. With this article, readers can have a more thorough understanding of signal processing techniques for spectrum sensing in CR system and the research trends in this area.","Signal processing,
Cognitive radio,
Chromium,
Interference,
Radio frequency,
Signal detection,
Radiofrequency identification,
FCC,
Finance,
Government"
Using Generalized Query Tree to Cope with the Capture Effect in RFID Singulation,"The Query Tree Protocol (QT) in Law et al. (2000) is an efficient RFID tag singulation algorithm that is guaranteed to read all the tags in the broadcast range of a reader. However, QT ignores the capture effect. That is, after the reader broadcasts a bit string query prefix, it is assumed that it can distinguish one of three responses, namely {no response, one response, collision}. If the capture effect is modeled, QT would no longer be guaranteed to singulate all the tags in the reader's range, since ""capturing"" a tag ID in the midst of a collision would leave all the other tags in that collision unsingulated. In this paper, we introduce two modifications to QT that always singulate all the tags even when the capture effect is considered. We call these the Generalized Query Tree Protocols (GQT1, GQT2). We provide analytical bounds and simulation results of the singulation times of these new protocols in relation to QT.","Radiofrequency identification,
Broadcasting,
Intrusion detection,
Analytical models,
Backscatter,
Wireless application protocol,
Decoding,
Computer science,
RFID tags,
Passive RFID tags"
FPGA implementation of a configurable cache/scratchpad memory with virtualized user-level RDMA capability,"We report on the hardware implementation of a local memory system for individual processors inside future chip multiprocessors (CMP). It intends to support both implicit communication, via caches, and explicit communication, via directly accessible local (“scratchpad”) memories and remote DMA (RDMA). We provide run-time configurability of the SRAM blocks near each processor, so that part of them operates as 2nd level (local) cache, while the rest operates as scratchpad. We also strive to merge the communication subsystems required by the cache and scratchpad into one integrated Network Interface (NI) and Cache Controller (CC), in order to economize on circuits. The processor communicates with the NI in user-level, through virtualized command areas in scratchpad; through a similar mechanism, the NI also provides efficient support for synchronization, using two hardware primitives: counters, and queues. We describe the block diagram, the hardware cost, and the latencies of our FPGA-based prototype implementation, which integrates four MicroBlaze processors, each with 64 KBytes of local SRAM, a crossbar NoC, and a DRAM controller on a Xilinx-5 FPGA. One-way, end-to-end, user-level communication completes within about 30 clock cycles for short transfer sizes.","Field programmable gate arrays,
Hardware,
Random access memory,
Communication system control,
Runtime,
Network interfaces,
Synchronization,
Counting circuits,
Costs,
Delay"
An energy efficient and QoS aware multipath routing protocol for wireless sensor networks,"Enabling real time applications in Wireless Sensor Networks (WSNs) demands certain delay and bandwidth requirements which pose more challenges in the design of networking protocols. Therefore, enabling such applications in this type of networks requires energy and Quality of Service (QoS) awareness in different layers of the protocol stack. In many of these applications (such as multimedia applications, or real time and mission critical applications), the network traffic is mixed of delay sensitive and reliability demanding data. Hence, QoS routing becomes an important issue. In this paper, we propose an Energy Efficient and QoS aware multipath routing protocol (we name it shortly as EQSR) that maximizes the network lifetime through balancing energy consumption across multiple nodes, uses the concept of service differentiation to allow high important traffic (or delay sensitive traffic) to reach the sink node within an acceptable delay, reduces the end to end delay through spreading out the traffic across multiple paths, and increases the throughput through introducing data redundancy. EQSR uses the residual energy, node available buffer size, and Signal-to-Noise Ratio (SNR) to predict the best next hop through the paths construction phase. Based on the concept of service differentiation the EQSR protocol employs a queuing model to handle both real time and non-real time traffic. By means of computer simulations, we evaluated and studied the performance of our routing protocol and compared it with another protocol. Simulation results have shown that our protocol achieves lower average delay and higher packet delivery ratio than the other protocol.","Energy efficiency,
Routing protocols,
Wireless sensor networks,
Quality of service,
Traffic control,
Telecommunication traffic,
Delay effects,
Bandwidth,
Wireless application protocol,
Mission critical systems"
Efficient Dense Structure Mining Using MapReduce,"Structure mining plays an important part in the researches in biology, physics, Internet and telecommunications in recently emerging network science. As a main task in this area, the problem of structure mining on graph has attracted much interest and been studied in variant avenues in prior works. However, most of these works mainly rely on single chip computational capacity and have been constrained by local optimization. Thus it is an impossible mission for these methods to process massive graphs. In this paper, we propose an unified distributed method in solving some critical graph mining problems on top of a cluster system with the help of MapReduce. These problems include graph transformation, subgraph partition, maximal clique enumeration, connected component finding and community detection. All of these methods are implemented to fully utilize MapReduce execution mechanism, namely the “map-reduce” process. Moreover, considering how our algorithms can be applied in further “cloud” service, we employ several large scale datasets to demonstrate the efficiency and scalability of our solutions.","Cloud computing,
Data mining,
Decision trees,
Costs,
Machine learning algorithms,
Clustering algorithms,
Computer networks,
Data processing,
Training data,
Conferences"
Quality Assurance of Software Applications Using the In Vivo Testing Approach,"Software products released into the field typically have some number of residual defects that either were not detected or could not have been detected during testing. This may be the result of flaws in the test cases themselves, incorrect assumptions made during the creation of test cases, or the infeasibility of testing the sheer number of possible configurations for a complex system; these defects may also be due to application states that were not considered during lab testing, or corrupted states that could arise due to a security violation. One approach to this problem is to continue to test these applications even after deployment, in hopes of finding any remaining flaws. In this paper, we present a testing methodology we call in vivo testing, in which tests are continuously executed in the deployment environment. We also describe a type of test we call in vivo tests that are specifically designed for use with such an approach: these tests execute within the current state of the program (rather than by creating a clean slate) without affecting or altering that state from the perspective of the end-user. We discuss the approach and the prototype testing framework for Java applications called Invite. We also provide the results of case studies that demonstrate Invite's effectiveness and efficiency.","Quality assurance,
Software quality,
Application software,
In vivo,
Software testing,
System testing,
Security,
Production,
Concurrent computing,
Computer science"
The Research on Software Metrics and Software Complexity Metrics,"The process of software development, including documentation, design, program, test, and maintenance can be measured statistically. Therefore the quality of software can be monitored efficiently. Software metrics is very important in research of software engineering and it has developed gradually. In this paper, software metrics definition were given and the history of and the types of software metrics were overviewed. Software complexity measuring is the important constituent of software metrics and it is concerning the cost of software development and maintenance. In order to improve the software quality and the project controllability, it is necessary to control the software complexity by measuring the related aspects. This paper respectively expounds McCabe methods and C & K metric method for examples of complexity metrics","Software metrics,
Software measurement,
Software quality,
Programming,
Software maintenance,
Documentation,
Software testing,
Monitoring,
Software engineering,
History"
Integrated Mobility Model (IMM) for VANETs simulation and its impact,"Mobility models represent real world scenarios for vehicular ad hoc networks (VANETs) and play a vital role in the performance evaluation of routing protocols. More research focus is now on the development of realistic mobility models for vehicular ad hoc networks. A number of mobility models have been presented and their impact on the performance on the routing protocols has been tested. In this paper we have introduced a new mobility model, Integrated Mobility Model (IMM), for vehicular ad hoc networks (VANETs). Integrated Mobility Model (IMM) is an integration of Manhattan Mobility Model, Freeway Mobility Model, Stop Sign Model and Traffic Sign Model and some other characteristics. In addition, we evaluated routing protocols AODV, DSR and OLSR using our Integrated Mobility Model and also Manhattan mobility model and Freeway mobility model and compared the results.","Routing protocols,
Traffic control,
Ad hoc networks,
Roads,
Testing,
Computational modeling,
Analytical models,
Performance analysis,
Atmospheric modeling,
Computer simulation"
Estimation of Channelized Hotelling Observer Performance With Known Class Means or Known Difference of Class Means,"This paper concerns task-based image quality assessment for the task of discriminating between two classes of images. We address the problem of estimating two widely-used detection performance measures, SNR and AUC, from a finite number of images, assuming that the class discrimination is performed with a channelized Hotelling observer. In particular, we investigate the advantage that can be gained when either 1) the means of the signal-absent and signal-present classes are both known, or 2) when the difference of class means is known. For these two scenarios, we propose uniformly minimum variance unbiased estimators of SNR2, derive the corresponding sampling distributions and provide variance expressions. In addition, we demonstrate how the bias and variance for the related AUC estimators may be calculated numerically by using the sampling distributions for the SNR2 estimators. We find that for both SNR2 and AUC, the new estimators have significantly lower bias and mean-square error than the traditional estimator, which assumes that the class means, and their difference, are unknown.",
Multiple watermarking of medical images for content authentication and recovery,"Medical Image data require strict security, confidentiality and integrity when transmitted from one hospital to another hospital. This can be achieved by adopting the procedures which can guarantee the image quality as well as secrecy of patient data to unauthorized users. To achieve these requirements we have proposed a multiple watermarking method. The scheme embeds robust watermark in region of non interest (RONI) for achieving security and confidentiality. While integrity control is achieved by inserting fragile watermark in region of interest ROI. Since ROI in the medical image is important from diagnosis point of view so it must be preserved. In order to avoid the distortion caused in ROI due to watermark insertion process, original ROI data is first separated and embedded outside the ROI. This will help in recovery of original ROI at the receiving end in contrast to the techniques reported in the literature which do not guarantee the integrity of the ROI after watermarking process. The image visual quality as well as tamper localization has been evaluated. We have used weighted peak signal to noise ratio (WPSNR) for measuring image quality after watermarking.","Watermarking,
Biomedical imaging,
Authentication,
Medical diagnostic imaging,
Data security,
Hospitals,
Image quality,
Robustness,
PSNR,
Noise measurement"
VirtualFace: An Algorithm to Guarantee Packet Delivery of Virtual-Coordinate-Based Routing Protocols in Wireless Sensor Networks,"Because the global positioning system (GPS) consumes a large amount of power and does not work indoors, many virtual-coordinate-based routing protocols are proposed for wireless sensor networks in which geographic location information is unavailable. Each of them, however, cannot guarantee packet delivery or constructs a virtual coordinate system with a complex structure. In this paper, we propose a method capable of augmenting virtual-coordinate-based routing protocols to guarantee packet delivery. Firstly, we introduce the virtual face construction protocol and the virtual face naming protocol to construct and name virtual faces, respectively. Subsequently, the VirtualFace algorithm is presented to route a packet from a dead-end node to a progress node by traversing the boundaries of the virtual faces from face to face. Simulations show that virtual-coordinate-based routing protocols including GLIDER, Hop ID, GLDR, and VCap augmented with the VirtualFace algorithm guarantee packet delivery while ensuring moderate routing path length overhead costs.","Routing protocols,
Wireless sensor networks,
Global Positioning System,
Peer to peer computing,
Communications Society,
Computer science,
Electronic mail,
Costs,
Aggregates,
Base stations"
Using Templates to Predict Execution Time of Scientific Workflow Applications in the Grid,"Workflow execution time predictions for Grid infrastructures is of critical importance for optimized workflow executions, advance reservations of resources, and overhead analysis. Predicting workflow execution time is complex due to multeity of workflow structures, involvement of several Grid resources in workflow execution, complex dependencies of workflow activities and dynamic behavior of the Grid. In this paper we present an online workflow execution time prediction system exploiting similarity templates. The workflows are characterized considering the attributes describing their performance at different Grid infrastructural levels. A “supervised exhaustive search” is employed to find suitable templates. We also make a provision of including expert user knowledge about the workflow performance in the procession of our methods. Results for three real world applications are presented to show the effectiveness of our approach.","Processor scheduling,
Engines,
Grid computing,
Application software,
Computer science,
Runtime environment,
Dynamic scheduling,
Degradation,
Prediction methods,
Search methods"
Supporting carrier grade services over wireless mesh networks: The approach of the European FP-7 STREP CARMEN [Very Large Projects],"CARMEN is a three-year Specific Targeted Research Project (STREP) funded by the European Commission within the 7th Framework Program. The CARMEN access network will complement existing access technologies by exploiting low cost mesh networking techniques, thus minimizing deployment and maintenance costs. The CARMEN architecture introduces an abstraction layer that hides the specifics of the underlying access technology providing an abstract interface on top of which higher layers can be easily developed. This allows for the integration of current and future heterogeneous wireless technologies to provide scalable and efficient mobile ubiquitous Internet access, able to adapt to different environments and user requirements. Following these goals, CARMEN aims to define, study and implement link and technology abstractions, mobility support, and quality of service. The architecture also includes advanced monitoring features that allow for dynamic self-configuration, thereby reducing the installation and operational costs.","Wireless mesh networks,
Costs,
Quality of service,
Mesh networks,
Media Access Protocol,
Urban areas,
Time factors,
Spread spectrum communication,
Relays,
Network topology"
A Dynamic Approach toward QoS-Aware Service Workflow Composition,"Web service-based workflow management systems have garnered considerable attention for automating and scheduling dependent operations. Such systems often support user preferences, e.g., time of completion, but with the rebirth of distributed computing via the grid/cloud, new challenges are abound: multiple disparate data sources, networks, nodes, and the potential for moving very large datasets. In this paper, we present a framework for integrating QoS support in a service workflow composition system. The relationship between workflow execution time and accuracy is exploited through an automatic workflow composition scheme. The algorithm, equipped with a framework for defining cost models on service completion times and error propagation, composes service workflows which can adapt to user's QoS preferences.","Costs,
Web services,
Workflow management software,
Computer science,
USA Councils,
Processor scheduling,
Dynamic scheduling,
Distributed computing,
Clouds,
Availability"
Phishing detection using classifier ensembles,"This paper introduces an approach to classifying emails into Phishing / non-Phishing categories using the C5.0 algorithm which achieves very high precision and an ensemble of other classifiers that achieve high recall. The representation of instances used in this paper is very small consisting of only five features. Results of an evaluation of this system, using over 8,000 emails approximately half of which were phishing emails and the remainder legitimate, are presented. These results show the benefits of using this recall boosting technique over that of any individual classifier or collection of classifiers.","Clustering algorithms,
Credit cards,
Uniform resource locators,
Internet,
Computer security,
Information security,
Laboratories,
Informatics,
Information technology,
Reliability engineering"
Aspects of GPU for general purpose high performance computing,"We discuss hardware and software aspects of GPGPU, specifically focusing on NVIDIA cards and CUDA, from the viewpoints of parallel computing. The major weak points of GPU against newest supercomputers are identified to be and summarized as only four points: large SIMD vector length, small memory, absence of fast L2 cache, and high register spill penalty. As software concerns, we derive optimal scheduling algorithm for latency hiding of host-device data transfer, and discuss SPMD parallelism on GPUs.",
Congestion Control using Efficient Explicit Feedback,"This paper proposes a framework for congestion control, called binary marking congestion control (BMCC) for high bandwidth-delay product networks. The basic components of BMCC are i) a packet marking scheme for obtaining high resolution congestion estimates using the existing bits available in the IP header for explicit congestion notification (ECN) and ii) a set of load-dependent control laws that use these congestion estimates to achieve efficient and fair bandwidth allocations on high bandwidth-delay product networks, while maintaining a low persistent queue length and negligible packet loss rate. We present analytical models that predict and provide insights into the convergence properties of the protocol. Using extensive packet-level simulations, we assess the efficacy of BMCC and perform comparisons with several proposed schemes. BMCC outperforms VCP, MLCP, XCP, SACK+RED/ECN and in some cases RCP, in terms of average flow completion times for typical Internet flow sizes.","Feedback,
Convergence,
Protocols,
Internet,
Channel allocation,
Signal resolution,
Communications Society,
Communication system control,
Computer science,
USA Councils"
"Routing Metric Designs for Greedy, Face and Combined-Greedy-Face Routing","Different geographic routing protocols have different requirements on routing metric designs to ensure proper operation. Combining a wrong type of routing metric with a geographic routing protocol may produce unexpected results, such as geographic routing loops and unreachable nodes. In this paper, we propose a novel routing algebra system to investigate the compatibilities between routing metrics and three geographic routing protocols including greedy, face and combined-greedy- face routing. Four important algebraic properties, respectively named odd symmetry, transitivity, source independence and local minimum freeness, are defined in this algebra system. Based on these algebraic properties, the necessary and sufficient conditions for loop-free and delivery guaranteed routing are derived when greedy, face and combined-greedy-face routing serve as packet forwarding schemes or as path discovery algorithms respectively. Our work provides essential criterions for evaluating and designing geographic routing protocols.","Routing protocols,
Algebra,
Wireless networks,
Design engineering,
Global Positioning System,
Switches,
Communications Society,
Computer science,
USA Councils,
Peer to peer computing"
Fuzzy clustering using hybrid fuzzy c-means and fuzzy particle swarm optimization,"Fuzzy clustering is an important problem which is the subject of active research in several real world applications. Fuzzy c-means (FCM) algorithm is one of the most popular fuzzy clustering techniques because it is efficient, straightforward, and easy to implement. However FCM is sensitive to initialization and is easily trapped in local optima. Particle swarm optimization (PSO) is a stochastic global optimization tool which is used in many optimization problems. In this paper a hybrid fuzzy clustering method based on FCM and fuzzy PSO (FPSO) is proposed which make use of the merits of both algorithms. Experimental results show that our proposed method is efficient and can reveal encouraging results.","Particle swarm optimization,
Clustering algorithms,
Clustering methods,
Ant colony optimization,
Fuzzy sets,
Machine learning algorithms,
Partitioning algorithms,
Iterative algorithms,
Machine intelligence,
Stochastic processes"
High accuracy context recovery using clustering mechanisms,"This paper examines the recovery of user context in indoor environmnents with existing wireless infrastructures to enable assistive systems. We present a novel approach to the extraction of user context, casting the problem of context recovery as an unsupervised, clustering problem. A well known density-based clustering technique, DBSCAN, is adapted to recover user context that includes user motion state, and significant places the user visits from WiFi observations consisting of access point id and signal strength. Furthermore, user rhythms or sequences of places the user visits periodically are derived from the above low level contexts by employing a state-of-the-art probabilistic clustering technique, the Latent Dirichlet Allocation (LDA), to enable a variety of application services. Experimental results with real data are presented to validate the proposed unsupervised learning approach and demonstrate its applicability.","Data mining,
Rhythm,
Mobile computing,
Pervasive computing,
Bluetooth,
Global Positioning System,
Context,
Sensor phenomena and characterization,
Thermal sensors,
Delay"
Real-time quantification of resting tremor in the Parkinson's disease,"Resting tremor (RT) is one of the most frequent signs of the Parkinson's disease (PD), occurring with various severities in about 75% of the patients. Current diagnosis is based on subjective clinical assessment, which is not always easy to capture subtle, mild and intermittent tremors. The aim of the present study is to assess the suitability and clinical value of a computer based real-time system as an aid to diagnosis of PD, in particular the presence of RT. Five healthy subjects were asked to simulate several severities of RT in hands and feet in three static activities. The behaviour of the subjects is measured using tri-axial accelerometers, which are placed at four different positions on the body. Frequency-domain features, strongly correlated with the RT activity, are extracted from the accelerometer data. The classification of RT severity based on those features, provided accuracy 76%. The real-time system designed for efficient extraction of those features and the provision of a continuous RT severity measure is described.","Parkinson's disease,
Materials science and technology,
Biomedical engineering,
Intelligent systems,
Information systems,
Frequency,
Electromyography,
Real time systems,
Accelerometers,
Data mining"
A novel anti-phishing framework based on honeypots,"As a powerful anti-phishing tool, honeypots have been widely used by security service providers and financial institutes to collect phishing mails, so that new phishing sites can be earlier detected and quickly shut down. Another popular use of honeypots is to collect useful information about phishers' activities, which is used to make various kinds of statistics for the purposes of research and forensics. Recently, it has also been proposed to actively feed phishers with honeytokens. In the present paper, we discuss some problems of existing anti-phishing solutions based on honeypots. We propose to overcome these problems by transforming the real e-banking system itself into a honeypot equipped with honeytokens and supported by some other kinds of honeypots. A phishing detector is used to automatically detect suspicious phishers' attempts of stealing money from victims' accounts, and then ask for the potential victims' reconfirmation. This leads to a novel anti-phishing framework based on honeypots. As an indispensable part of the framework, we also propose to use phoneybots, i.e., active honeypots running in virtual machines and mimicking real users' behavior to access the real e-banking system automatically, in order to submit honeytokens to pharmers and phishing malware. The involvement of phoneybots is crucial to fight against advanced phishing attacks such as pharming and malware-based phishing attacks.","Clustering algorithms,
Credit cards,
Uniform resource locators,
Internet,
Computer security,
Information security,
Laboratories,
Informatics,
Information technology,
Reliability engineering"
Way Stealing: Cache-assisted automatic Instruction Set Extensions,"This paper introduces way stealing, a simple architectural modification to a cache-based processor to increase data bandwidth to and from application-specific instruction set extensions (ISEs). Way stealing provides more bandwidth to the ISE-logic than the register file alone and does not require expensive coherence protocols, as it does not add memory elements to the processor. When enhanced with way stealing, ISE identification flows detect more opportunities for acceleration than prior methods; consequently, way stealing can accelerate applications to up to 3.7times, whilst reducing the memory sub-system energy consumption by up to 67%, despite data-cache related restrictions.","Bandwidth,
Protocols,
Permission,
Acceleration,
Costs,
Computer aided instruction,
Data engineering,
Circuits and systems,
Registers,
Energy consumption"
Wide-Area monitoring and control algorithms for large power systems using synchrophasors,"Power system operation is constantly facing contingencies such as from line faults and generator outages. For operational reliability, the system must be able to withstand the contingencies, either by itself (for N-1 contingency) or with the help of Special Protection Schemes (SPS) or Remedial Action Schemes (for N-2 or worse contingencies). When the system is operating under unforeseen conditions or under unusually high stress, the system can face dynamic instability related to any of voltage stability, small-signal stability or transient stability phenomena. At Washington State University, we have been developing real-time monitoring and control algorithms for handling these three types of instability phenomena using wide-area synchrophasor measurements. The presentation will highlight the different phenomena and the tools that have been developed for fast detection and mitigation of the instability mechanisms.","Monitoring,
Control systems,
Power system control,
Power systems,
Power system protection,
Power system reliability,
Stability,
Power system faults,
Power generation,
Stress"
Neural prosthetic systems: Current problems and future directions,"By decoding neural activity into useful behavioral commands, neural prosthetic systems seek to improve the lives of severely disabled human patients. Motor decoding algorithms, which map neural spiking data to control parameters of a device such as a prosthetic arm, have received particular attention in the literature. Here, we highlight several outstanding problems that exist in most current approaches to decode algorithm design. These include two problems that we argue will unlikely result in further dramatic increases in performance, specifically spike sorting and spiking models. We also discuss three issues that have been less examined in the literature, and we argue that addressing these issues may result in dramatic future increases in performance. These include: non-stationarity of recorded waveforms, limitations of a linear mappings between neural activity and movement kinematics, and the low signal to noise ratio of the neural data. We demonstrate these problems with data from 39 experimental sessions with a non-human primate performing reaches and with recent literature. In all, this study suggests that research in cortically-controlled prosthetic systems may require reprioritization to achieve performance that is acceptable for a clinically viable human system.","Prosthetics,
Signal processing algorithms,
Decoding,
Neural prosthesis,
Humans,
Sorting,
Biomedical signal processing,
Voltage control,
Animals,
Signal mapping"
An empirical study on bug assignment automation using Chinese bug data,"Bug assignment is an important step in bug life-cycle management. In large projects, this task would consume a substantial amount of human effort. To compare with the previous studies on automatic bug assignment in FOSS (Free/Open Source Software) projects, we conduct a case study on a proprietary software project in China. Our study consists of two experiments of automatic bug assignment, using Chinese text and the other non-text information of bug data respectively. Based on text data of the bug repository, the first experiment uses SVM to predict bug assignments and achieve accuracy close to that by human triagers. The second one explores the usefulness of non-text data in making such prediction. The main results from our study includes that text data are most useful data in the bug tracking system to triage bugs, and automation based on text data could effectively reduce the manual effort.","Automation,
Open source software,
Computer bugs,
Software measurement,
Software engineering,
Humans,
Data mining,
Software debugging,
Engineering management,
Support vector machines"
BLOGS: Balanced local and global search for non-degenerate two view epipolar geometry,"This work considers the problem of estimating the epipolar geometry between two cameras without needing a prespecified set of correspondences. It is capable of resolving the epipolar geometry for cases when the views differ significantly in terms of baseline and rotation, resulting in a large number features in one image that have no correspondence in the other image. We do conditional characterization of the probability space of correspondences based on Joint Feature Distributions (JFD) [21]. We seek to maximize the probabilistic support of the putative correspondence set over a number of MCMC iterations, guided by proposal distributions based on similarity or JFD. Similarity based guidance provides large movements (global) through correspondence space and JFD based guidance provides small movements (local) around the best known epipolar geometry the algorithm has found so far. We also propose a simple and novel method to rule out, at each iteration, correspondences that lead to degenerate configurations, thus speeding up convergence. We compare our algorithm with LO-RANSAC [2], NAPSAC [7], MAPSAC [19] and BEEM [9], which are the current state of the art competing methods, on a dataset that has significantly more change in baseline, rotation, and scale than those used in the current literature. We quantitatively benchmark the performance using manually specified ground truth corresponding point pairs. We find that our approach can achieve results of similar quality as the current state of art in 10 times lesser number of iterations. We are also able to tolerate upto 90% outlier correspondences.","Blogs,
Motion estimation,
Iterative algorithms,
Cameras,
Layout,
Robustness,
Computational geometry,
Calibration,
Computer science,
Image resolution"
Towards the Robustness of Dynamic Loop Scheduling on Large-Scale Heterogeneous Distributed Systems,"Dynamic loop scheduling (DLS) algorithms provide application-level load balancing of loop iterates, with the goal of maximizing application performance on the underlying system. These methods use run-time information regarding the performance of the application's execution (for which irregularities change over time). Many DLS methods are based on probabilistic analyses, and therefore account for unpredictable variations of application and system related parameters. Scheduling scientific and engineering applications in large-scale distributed systems (possibly shared with other users) makes the problem of DLS even more challenging. Moreover, the chances of failure, such as processor or link failure, are high in such large-scale systems. In this paper, we employ the hierarchical approach for three DLS methods, and propose metrics for quantifying their robustness with respect to variations of two parameters (load and processor failures), for scheduling irregular applications in large-scale heterogeneous distributed systems.","Robustness,
Dynamic scheduling,
Large-scale systems,
Processor scheduling,
Load management,
Runtime,
Computational modeling,
Distributed computing,
Vehicle dynamics,
Computer science"
Asymmetric 3D/2D face recognition based on LBP facial representation and canonical correlation analysis,"In the recent years, 3D Face recognition has emerged as a major solution to deal with the unsolved issues for reliable 2D face recognition, i.e. lighting condition and viewpoint variations. However, 3D method is currently limited by its registration and computation cost. In this paper, we propose to investigate a solution named asymmetric face recognition scheme, enrolling people in 3D environment but performing identification in 2D. The goal is to limit the use of 3D data to where it really helps to improve recognition performances. In our approach, Local Binary Patterns (LBP) is used as an efficient facial representation for both 2D texture images and 3D range images. A weighted Chi square distance is used as matching score between the 2D LBP facial representations; Canonical Correlation Analysis (CCA) is applied to learn the mapping between LBP-based range face images (3D) and LBP facial texture images (2D). Both matching scores are further fused to obtain the final result. Compared with the traditional 2D/2D algorithms, the proposed asymmetric face recognition scheme achieves better accuracy; while avoiding the high cost of data acquisition and computation in 3D/3D approaches.","Face recognition,
Probes,
Face detection,
Image analysis,
Image texture analysis,
Pattern matching,
Lighting,
Computational efficiency,
Biometrics,
Laboratories"
A reconfigurable architecture for the Phylogenetic Likelihood Function,"As FPGA devices become larger, more coarse-grain modules coupled with large scale reconfigurable fabric become available, thus enabling new classes of applications to run efficiently, as compared to a general-purpose computer. This paper presents an architecture that benefits from the large number of DSP modules in Xilinx technology to implement massive floating point arithmetic. Our architecture computes the Phylogenetic Likelihood Function (PLF) which accounts for approximately 95% of total execution time in all state-of-the-art Maximum Likelihood (ML) based programs for reconstruction of evolutionary relationships. We validate and assess performance of our architecture against a highly optimized and parallelized software implementation of the PLF that is based on RAxML, which is considered to be one of the fastest and most accurate programs for phylogenetic inference. Both software and hardware implementations use double precision floating point arithmetic. The new architecture achieves speedups ranging from 1.6 up to 7.2 compared to a high-end 8-way dual-core general-purpose computer running the aforementioned highly optimized OpenMP-based multi-threaded version of the PLF.",
Multi-category bioinformatics dataset classification using extreme learning machine,"This paper presents recently introduced learning algorithm called Extreme Learning Machine (ELM) for Single-hidden Layer Feed-forward Neural-networks (SLFNs) which randomly chooses hidden nodes and analytically determines the output weights of SLFNs. The ELM avoids problems like local minima, improper learning rate and over fitting commonly faced by iterative learning methods and completes the training very fast. We have evaluated the multi-category classification performance of ELM on five different data sets related to bioinformatics namely, the Breast Cancer Wisconsin data set, the Pima Diabetes data set, the Heart-Statlog data set, the Hepatitis data set and the Hypothyroid data set. A detailed analysis of different activation functions with varying number of neurons is also carried out which concludes that Algebraic Sigmoid function outperforms all other activation functions on these data sets. The evaluation results indicate that ELM produces better classification accuracy with reduced training time and implementation complexity compared to earlier implemented models.","Bioinformatics,
Machine learning,
Iterative algorithms,
Feedforward systems,
Algorithm design and analysis,
Iterative methods,
Learning systems,
Breast cancer,
Diabetes,
Liver diseases"
Shot Boundary Detection and Keyframe Extraction Based on Scale Invariant Feature Transform,"In shot boundary detection, the key technology is to compute the visual content discontinuity values between consecutive video frames. In this paper, a unified framework is proposed to detect the shot boundaries and extract the keyframes of a shot. Firstly, the Scale invariant feature transform (SIFT) is adopted to compute the visual content discontinuity values. Then a new method, which is called the Local Double Threshold Shot Boundary Detection (LDT-SBD), is used to detect shot boundaries. Lastly, two mechanisms are proposed to extract keyframe. Experimental results show the framework is effective and has a good performance.","Gunshot detection systems,
Cameras,
Telecommunication computing,
Data mining,
Content based retrieval,
Noise robustness,
Information science,
Helium,
Feature extraction,
Indexing"
Higher-order gradient descent by fusion-move graph cut,"Markov Random Field is now ubiquitous in many formulations of various vision problems. Recently, optimization of higher-order potentials became practical using higher-order graph cuts: the combination of i) the fusion move algorithm, ii) the reduction of higher-order binary energy minimization to first-order, and iii) the QPBO algorithm. In the fusion move, it is crucial for the success and efficiency of the optimization to provide proposals that fits the energies being optimized. For higher-order energies, it is even more so because they have richer class of null potentials. In this paper, we focus on the efficiency of the higher-order graph cuts and present a simple technique for generating proposal labelings that makes the algorithm much more efficient, which we empirically show using examples in stereo and image denoising.","Layout,
Lighting,
Least squares methods,
Light sources,
Least squares approximation,
Automation,
Educational institutions,
Information science,
Geometry,
Jacobian matrices"
Node Reclamation and Replacement for Long-lived Sensor Networks,"When deployed for long-term tasks, the energy required to support sensor nodes' activities is far more than the energy that can be preloaded in their batteries. No matter how the battery energy is conserved, once the energy is used up, the network life terminates. Therefore, guaranteeing long- term energy supply has persisted as a big challenge. To address this problem, we propose a node replacement and reclamation (NRR) strategy, with which a mobile robot or human labor called mobile repairman (MR) periodically traverses the sensor network, reclaims nodes with low or no power supply, replaces them with fully-charged ones, and brings the reclaimed nodes back to an energy station for recharging. To effectively and efficiently realize the strategy, we develop an adaptive rendezvous- based two-tier scheduling (ARTS) scheme to schedule the replacement/reclamation activities of the MR and the duty cycles of nodes. Extensive simulations have been conducted to verify the effectiveness and efficiency of the ARTS scheme.","Peer to peer computing,
Wireless sensor networks,
Batteries,
Humans,
Subspace constraints,
Computer science,
Mobile robots,
Adaptive scheduling,
Condition monitoring,
Photovoltaic cells"
On the Design of a Suitable Hardware Platform for Protocol Stack Processing in LTE Terminals,"In this paper we present a design methodology for the identification and development of a suitable hardware platform (including dedicated hardware accelerators) for the data plane processing of the LTE protocol stack layer 2 (L2) in downlink direction. For this purpose, a hybrid design approach is adopted allowing first investigations of future mobile phone platforms on the system level (using virtual prototyping) combined with more accurate power-area explorations of hardware accelerators on the architectural level. Additionally, we show the employment of an LTE data generator peripheral, realizing L2 uplink processing and thus enabling platform analyses in a closed virtual environment. Furthermore, a modeling technique for a fast and efficient design of virtual hardware accelerator peripherals is demonstrated. A reasonable hardware/software partitioning can thereby be achieved early in the design phase. Once the system architecture is settled and thus the solution space is reduced, VHDL models of the accelerators are developed in order to find a suitable hardware implementation for LTE terminals based on timing constraints by system level simulations. As a case study, the LTE ciphering scheme, including the Advanced Encryption Standard (AES), is applied. We show results of our methodology by developing a deciphering hardware accelerator that enables the LTE protocol stack to process data rates of 100 Mbit/s and beyond.","Hardware,
Protocols,
Design methodology,
Downlink,
Mobile handsets,
Virtual prototyping,
Employment,
Virtual environment,
Power system modeling,
Computer architecture"
Learning a distance metric from multi-instance multi-label data,"Multi-instance multi-label learning (MIML) refers to the learning problems where each example is represented by a bag/collection of instances and is labeled by multiple labels. An example application of MIML is visual object recognition in which each image is represented by multiple key points (i.e., instances) and is assigned to multiple object categories. In this paper, we study the problem of learning a distance metric from multi-instance multi-label data. It is significantly more challenging than the conventional setup of distance metric learning because it is difficult to associate instances in a bag with its assigned class labels. We propose an iterative algorithm for MIML distance metric learning: it first estimates the association between instances in a bag and its assigned class labels, and learns a distance metric from the estimated association by a discriminative analysis; the learned metric will be used to update the association between instances and class labels, which is further used to improve the learning of distance metric. We evaluate the proposed algorithm by the task of automated image annotation, a well known MIML problem. Our empirical study shows an encouraging result when combining the proposed algorithm with citation-kNN, a state-of-the-art algorithm for multi-instance learning.","Iterative algorithms,
Algorithm design and analysis,
Supervised learning,
Kernel,
Nearest neighbor searches,
Support vector machines,
Support vector machine classification,
Computer science,
Data engineering,
Radiology"
Monitoring BPEL-Based Web Service Composition Using AOP,"With the loose coupling and dynamic attributes of Web Service, the implementation behavior may be different from original requirement, so it’s essential to implement the run-time monitoring. Monitoring analyzes the conformance of a Web Service to the requirements. This paper gives a novel monitoring prototype framework. It introduces WS-Policy to express the user’s monitoring requirement on the Services. Then, AOP (Aspect Oriented Programming) technology is used to extend the BPEL execution engine in order to capture run-time information of the service. Finally, monitoring information is analyzed against the EMSC (Extended Message Sequence Charts) and METG property specification of that service, reaching the predefined goal of monitoring. To evaluate our approach, we have executed a travel reservation service sample under the monitoring framework. Case study demonstrates the feasibility.","Web services,
Runtime,
Computerized monitoring,
Engines,
Logic,
Switches,
Information science,
Sun,
Computer science,
Prototypes"
Revisiting uncertainty analysis for optimum planes extracted from 3D range sensor point-clouds,"In this work, we utilize a recently studied more accurate range noise model for 3D sensors to derive from scratch the expressions for the optimum plane which best fits a point-cloud and for the combined covariance matrix of the plane's parameters. The parameters in question are the plane's normal and its distance to the origin. The range standard-deviation model used by us is a quadratic function of the true range and is a function of the incidence angle as well. We show that for this model, the maximum-likelihood plane is biased, whereas the least-squares plane is not. The plane-parameters' covariance matrix for the least-squares plane is shown to possess a number of desirable properties, e.g., the optimal solution forms its null-space and its components are functions of easily understood terms like the planar-patch's center and scatter.We verify our covariance expression with that obtained by the eigenvector perturbation method. We further compare our method to that of renormalization with respect to the theoretically best covariance matrix in simulation. The application of our approach to real-time range-image registration and plane fusion is shown by an example using a commercially available 3D range sensor. Results show that our method has good accuracy, is fast to compute, and is easy to interpret intuitively.","Uncertainty,
Covariance matrix,
Maximum likelihood estimation,
Perturbation methods,
Robotics and automation,
Computer science,
Jacobian matrices,
Scattering,
Computational modeling,
Sensor fusion"
Training of radial basis function classifiers with resilient propagation and variational Bayesian inference,"For classification tasks, the application of generative classifiers sometimes has advantages over the use of exclusively discriminative classifiers because loss functions can be considered or rejection criteria can be defined more easily, for instance. We show how a radial basis function (RBF) network with multivariate (elliptical) Gaussian basis functions can be trained in two different ways to obtain a classifier with either a more generative or a more discriminative behavior. Our generative classifier allows a probabilistic interpretation of the external outputs (posterior probability of class membership) and the hidden neurons' activations (posterior probability of a component of the model). For that purpose a variational Bayesian inference approach is applied, which also finds an appropriate number of hidden neurons (i.e., components) “on the fly”. A discriminative classifier is obtained using the resilient propagation training technique. We investigate the properties of the two training techniques in detail by introducing a measure for generative properties of the trained classifiers and by comparing these classifiers on various data sets.","Bayesian methods,
Radial basis function networks,
Neurons,
Training data,
Inference algorithms,
Neural networks,
USA Councils,
Propagation losses,
Waste materials,
Decision theory"
FreePDK v2.0: Transitioning VLSI education towards nanometer variation-aware designs,"This paper discusses an extension to an open source, variation aware Process Design Kit (PDK), based on Scalable CMOS design rules. This PDK is designed for 45nm feature sizes and is utilized for use in VLSI research, computer architecture, education and small businesses. This kit includes all the necessary layout design rules and extraction command decks to capture layout dependent systematic variation and perform statistical circuit analysis. The kit also includes a standard cell library, MIPS® processor and associated GNU-compliant compiler and the necessary support files to enable full chip place and route and verification for System on Chip designs. An analog and digital system test chip is also included with this PDK-extension allowing exploration of nanometer-based VLSI designs.","Very large scale integration,
Computer science education,
Process design,
CMOS process,
Computer architecture,
Circuit analysis,
Software libraries,
System-on-a-chip,
Digital systems,
Circuit testing"
Understanding the paradoxical effects of power control on the capacity of wireless networks,"Recent works show conflicting results: network capacity may increase or decrease with higher transmission power under different scenarios. In this work, we want to understand this paradox. Specifically, we address the following questions: (1)Theoretically, should we increase or decrease transmission power to maximize network capacity? (2) Theoretically, how much network capacity gain can we achieve by power control? (3) Under realistic situations, how do power control, link scheduling and routing interact with each other? Under which scenarios can we expect a large capacity gain by using higher transmission power? To answer these questions, firstly, we prove that the optimal network capacity is a non-decreasing function of transmission power. Secondly, we prove that the optimal network capacity can be increased unlimitedly by higher transmission power in some network configurations. However, when nodes are distributed uniformly, the gain of optimal network capacity by higher transmission power is upper-bounded by a positive constant. Thirdly, we discuss why network capacity may increase or decrease with higher transmission power under different scenarios using carrier sensing and the minimum hop-count routing. Extensive simulations verify our analysis.","Power control,
Wireless networks,
Routing,
Wireless mesh networks,
Power supplies,
Scheduling algorithm,
Interference,
Analytical models,
IP networks,
Throughput"
Host based intrusion detection using RBF neural networks,A novel approach of host based intrusion detection is suggested in this paper that uses Radial basis Functions Neural Networks as profile containers. The system works by using system calls made by privileged UNIX processes and trains the neural network on its basis. An algorithm is proposed that prioritize the speed and efficiency of the training phase and also limits the false alarm rate. In the detection phase the algorithm provides implementation of window size to detect intrusions that are temporally located. Also a threshold is implemented that is altered on basis of the process behavior. The system is tested with attacks that target different intrusion scenarios. The result shows that the radial Basis Functions Neural Networks provide better detection rate and very low training time as compared to other soft computing methods. The robustness of the training phase is evident by low false alarm rate and high detection capability depicted by the application,"Intrusion detection,
Neural networks,
Radial basis function networks,
Phase detection,
Military computing,
Application software,
Monitoring,
Computer science,
Educational institutions,
Computer networks"
Videogames in Computer Space: The Complex History of Pong,"The earliest digital games emerged out of laboratories and research centers in the 1960s and 1970s. The intertwined histories of Nolan Bushnell's Computer Space and Pong illustrate the transition from these ""university games"" to accessible entertainment and educational games as well as the complicated historical relationship among the arcade, computer, and videogames.",
A Dynamic Online Traffic Classification Methodology Based on Data Stream Mining,"Recently, traffic classification (TC) becomes more and more important for network management and measurement tasks. The new-coming machine learning based classification methods can achieve high classification accuracy and fast identification ability; however, all these related TC methods up to now always have the assumption of the stability of classification model constituted from network traffic. It is not true since seldom real-world traffic is static. In this paper, we make a first step towards classifying dynamic online traffic in a data stream perspective to handle the dynamic real-time network traffic. In this paper, we validate the dynamic feature of real-world traffic for the first time, using concept drift from two different levels: overall traffic level and application level. The conclusion convinces us that the user behavior reflected in traffic can vary dramatically due to different conditions and different periods. We then propose a novel integrated dynamic online traffic classification framework; called DSTC (Data Stream based Traffic Classification). This DSTC differs from previous work since it aims to deal with dynamic traffic with online identification ability. It is a more realistic framework in which training phase can go simultaneously with classification phase and more accurate training model can be constructed with the feedback from classification result. Experiment results have shown that DSTC can have a high stable classification accuracy of above 95% for network traffic with different periods and user conditions, while accuracy for the traditional classification methodology can vary from 81% to 97% when dealing with different traffic.","Data mining,
Telecommunication traffic,
Traffic control,
Communication system traffic control,
Streaming media,
Laboratories,
Machine learning,
Statistics,
Computer science,
Data engineering"
GSO: Designing a well-founded service ontology to support dynamic service discovery and composition,"A pragmatic and straightforward approach to semantic service discovery is to match inputs and outputs of user requests with the input and output requirements of registered service descriptions. This approach can be extended by using pre-conditions, effects and semantic annotations (meta-data) in an attempt to increase discovery accuracy. While on one hand these additions help improve discovery accuracy, on the other hand complexity is added as service users need to add more information elements to their service requests. In this paper we present an approach that aims at facilitating the representation of service requests by service users, without loss of accuracy. We introduce a goal-based service framework (GSF) that uses the concept of goal as an abstraction to represent service requests. This paper presents the core concepts and relations of the goal-based service ontology (GSO), which is a fundamental component of the GSF, and discusses how the framework supports semantic service discovery and composition. GSO provides a set of primitives and relations between goals, tasks and services. These primitives allow a user to represent its goals, and a supporting platform to discover or compose services that fulfil them.","Ontologies,
Computer science,
Pervasive computing,
Software engineering,
Information systems,
Impedance matching,
Computer industry,
Protocols,
Humans,
Computer interfaces"
A New Strategy for Pairwise Test Case Generation,"Pairwise testing has become an important approach to software testing because it often provides effective error detection at low cost, and a key problem of it is the test case generation method. As the part of an effort to develop an optimized strategy for pairwise testing, this paper proposes an efficient pairwise test case generation strategy, called VIPO(Variant of In-Parameter-order ), which is a variant of IPO strategy. We compare its effectiveness with some existing strategies including IPO, Tconfig, Pict and AllPairs. Experimental results demonstrate that VIPO outperformed them in terms of the number of generated test case within reasonable execution times, in most cases.","Software testing,
System testing,
Information technology,
Application software,
Computer science,
Computer errors,
Costs,
Performance evaluation,
Benchmark testing,
Software quality"
A Comprehensive Survey on Text Summarization Systems,,"Data mining,
Taxonomy,
Information resources,
IP networks,
Search engines,
Cellular phones,
Joining processes,
Internet,
Humans"
Comparison of standard image segmentation methods for segmentation of brain tumors from 2D MR images,"In the analysis of medical images for computer-aided diagnosis and therapy, segmentation is often required as a preliminary step. Medical image segmentation is a complex and challenging task due to the complex nature of the images. The brain has a particularly complicated structure and its precise segmentation is very important for detecting tumors, edema, and necrotic tissues in order to prescribe appropriate therapy. Magnetic Resonance Imaging is an important diagnostic imaging technique utilized for early detection of abnormal changes in tissues and organs. It possesses good contrast resolution for different tissues and is, thus, preferred over Computerized Tomography for brain study. Therefore, the majority of research in medical image segmentation concerns MR images. As the core juncture of this research a set of MR images have been segmented using standard image segmentation techniques to isolate a brain tumor from the other regions of the brain. Subsequently the resultant images from the different segmentation techniques were compared with each other and analyzed by professional radiologists to find the segmentation technique which is the most accurate. Experimental results show that the Otsu's thresholding method is the most suitable image segmentation method to segment a brain tumor from a Magnetic Resonance Image.","Image segmentation,
Neoplasms,
Biomedical imaging,
Medical diagnostic imaging,
Image analysis,
Medical treatment,
Magnetic resonance imaging,
Computer aided diagnosis,
Computed tomography,
Magnetic analysis"
Impact of Selective Dropping Attacks on Network Coding Performance in DTNs and a Potential Mitigation Scheme,"Some ad hoc network scenarios are characterized by frequent partitions and intermittent connectivity. A store-and-forward network architecture known as the disruption tolerant network (DTN) has been designed for such challenging network environments. To further improve the delivery performance, some researchers have proposed some network coding schemes for DTNs. However, not much papers discuss the security issues of network coding schemes in DTNs. In this paper, we first discuss some attacks that can be launched against network coding schemes in DTNs. Then, we focus on evaluating the impact of selective data dropping attacks on the delivery performance of a network coding scheme we design for DTN. Next, we describe a mitigation scheme that we design to overcome such attacks. Our mitigation scheme uses dynamic redundancy factor to generate more coded packets when a source notices performance degradation in the delivery performance. Via simulation studies, we show that our mitigation scheme is effective in restoring the performance degradation caused by the selective dropping attacks as long as alternate DTN paths exist for a source/destination pair.","Network coding,
Disruption tolerant networking,
Ad hoc networks,
Routing,
Data security,
Redundancy,
Degradation,
Computer science,
Computer architecture,
Unicast"
Performance and power consumption modeling for green COTS Software Router,"We consider a new generation of COTS Software Routers (SRs), able to effectively exploit multi-Core/CPU HW platforms. Our main objective is to analyze, to evaluate and to model the impact of power saving mechanisms, generally included in today's COTS processors, on the SR behavior and networking performance. To this purpose, we tried to understand and to separately characterize the roles of both HW and SW layers through a large set of internal and external experimental measures, obtained with a heterogeneous set of HW platforms and SR setups. Moreover, starting from this detailed measure analysis, we propose a simple model, able to represent the SR performance with a high accuracy level in terms of packet throughput and related power consumption. The proposed model can be effectively applied inside “green optimization” mechanisms to minimize power consumption, while maintaining a certain SR performance target.","Energy consumption,
Software performance,
Strontium,
Power system modeling,
Energy management,
Power measurement,
Open source software,
Telecommunications,
Communication system software,
Power generation"
zSlices based general type-2 FLC for the control of autonomous mobile robots in real world environments,"Fuzzy Logic Control is generally credited with being an adequate methodology for real world control applications which are subject to large amounts of uncertainties. Recent work has shown that interval type-2 Fuzzy Logic Controllers (FLCs) can outperform type-1 FLCs in applications which encompass large amounts of uncertainty. However, the application of general type-2 FLCs and investigations of their performance have been very limited. This paper employs the recently introduced concept of zSlices based general type-2 fuzzy sets to implement a zSlices based general type-2 FLC (zFLC). We will present an overview of the implementation and operations of the zFLC for a two-wheel mobile robot navigating in real world outdoor environments. Furthermore, we present a performance analysis of the zFLC which is compared to the type-1 and interval type-2 FLCs.","Mobile robots,
Uncertainty,
Fuzzy sets,
Fuzzy logic,
Performance analysis,
Navigation,
Robot control,
Testing,
Environmental factors,
Computational intelligence"
Latent Dirichlet learning for document summarization,"Automatic summarization is developed to extract the representative contents or sentences from a large corpus of documents. This paper presents a new hierarchical representation of words, sentences and documents in a corpus, and infers the Dirichlet distributions for latent topics and latent themes in word level and sentence level, respectively. The sentence-based latent Dirichlet allocation (SLDA) is accordingly established for document summarization. Different from the vector space summarization, SLDA is built to fit the fine structure of text documents, and is specifically designed for sentence selection. SLDA acts as a sentence mixture model with a mixture of Dirichlet themes, which are used to generate the latent topics in observed words. The theme model is inherent to distinguish sentences in a summarization system. In the experiments, the proposed SLDA outperforms other methods for document summarization in terms of precision, recall and F-measure.","Data mining,
Linear discriminant analysis,
Computer science,
Internet,
Web pages,
Compaction,
Functional analysis,
Sampling methods,
Convergence,
Speech"
Automatic memory partitioning and scheduling for throughput and power optimization,"Hardware acceleration is crucial in modern embedded system design to meet the explosive demands on performance and cost. Selected computation kernels for acceleration are usually captured by nest loops, which are optimized by state-of-the-art techniques like loop tiling and loop pipelining. However, memory bandwidth bottlenecks prevent designs to reach optimal throughput with respect to available parallelism. In this paper we present an automatic memory partitioning technique which can efficiently improve throughput and reduce energy consumption of pipelined loop kernels for given throughput constraints and platform requirement. Our partition scheme consists of two steps, the first step considers cycle accurate scheduling information to meet the hard constraints on memory bandwidth requirements specifically for synchronized hardware designs. Experimental results show an average 6X throughput improvement on a set of real world designs with moderate area increase (about 45% on average), given that less resource sharing opportunities exist with higher throughput in optimized designs. The second step further partitions the memory banks for reducing the dynamic power consumption of the final design. In contrast with previous approaches, our technique can statically compute memory access frequencies in polynomial time with little to none profiling. Experimental results show about 30% power reduction on the same set of benchmarks.","Throughput,
Hardware,
Acceleration,
Kernel,
Bandwidth,
Energy consumption,
Processor scheduling,
Embedded system,
Explosives,
Costs"
System level security modeling using attack trees,"Vulnerabilities in intrusion tolerant systems have dependence on various dynamic aspects such as redundant mechanisms, fault and error recovery mechanisms, and different operation modes. The conventional nodes of attack trees can not adequately capture the attacks towards those systems, thus constructing security models for the systems is very difficult. This paper introduces new nodes to model the security of those systems. The nodes include: PAND node, k/n node, SEQ node, CSUB node, and Housing node. We provide the syntax and graphical representation for each node. The nodes allow us to model attacks that require exploitation of vulnerabilities which have dependence on ordering events, sequence-dependant events, conditional failures and mechanisms which involve configuration changes with time. We use the nodes to construct attack trees for different security related systems.","Tree graphs,
Fault trees,
Error correction,
Nuclear and plasma sciences,
Computer security,
Fault detection,
Redundancy,
Visualization,
XML,
Automation"
"An Undergraduate Mechatronics Project Class at Philadelphia University, Jordan: Methodology and Experience","Mechatronics is a branch of engineering whose final product should involve mechanical movements controlled by smart electronics. The design and implementation of functional prototypes are an essential learning experience for the students in this field. In this paper, the guidelines for a successful mechatronics project class are presented, evaluated, and discussed. Furthermore, the paper introduces a general mechatronic system design methodology that should equip students to carry out a successful mechatronics project in their undergraduate training. Three student projects at Philadelphia University, Jordan, are examined in detail, with descriptions of their goals, design, and implementation.","Mechatronics,
Seminars,
Prototypes,
Writing,
Materials,
Design methodology,
Data mining"
Improved Imperialist Competitive Algorithm for Constrained Optimization,"This paper introduces an improved evolutionary algorithm based on the Imperialist Competitive Algorithm. The original approach in the Imperialist Competitive Algorithm has difficulty in implement practically with the increase of the dimension of the search spaces, as the ambiguous definition of the “random angle” in the process of optimization. Compare to the original algorithm, the proposed approach based on the concept of small probability perturbation has more simplicity to be implemented, especially in solving high-dimensional optimization problems. Furthermore, the present algorithm has been extended to constrained optimization problem, using a classical penalty technique to handle constraints. Several numerical optimization examples are tested by applying the proposed algorithm, and the results show its applicability and flexibility in dealing with different types of optimization problems.","Constraint optimization,
Evolutionary computation,
Costs,
Space technology,
Testing,
Adaptive arrays,
Chemical industry,
Computer applications,
Application software,
Automation"
A Practical Differentially Private Random Decision Tree Classifier,"In this paper, we study the problem of constructing private classifiers using decision trees, within the framework of differential privacy. We first construct privacy-preserving ID3 decision trees using differentially private sum queries. Our experiments show that for many data sets a reasonable privacy guarantee can only be obtained via this method at a steep cost of accuracy in predictions. We then present a differentially private decision tree ensemble algorithm using the random decision tree approach. We demonstrate experimentally that our approach yields good prediction accuracy even when the size of the datasets is small. We also present a differentially private algorithm for the situation in which new data is periodically appended to an existing database. Our experiments show that our differentially private random decision tree classifier handles data updates in a way that maintains the same level of privacy guarantee.","Decision trees,
Classification tree analysis,
Data privacy,
Databases,
Accuracy,
Computer science,
USA Councils,
Sliding mode control,
Data mining,
Conferences"
A-ADHOC: An adaptive real-time distributed MAC protocol for Vehicular Ad Hoc Networks,"Vehicular Ad Hoc Networks (VANET) is the foundation of Intelligent Traffic System (ITS), and recently many MAC protocols for VANET are proposed, among which a reliable MAC protocol called ADHOC has aroused much attention. By investigating the details of ADHOC protocol, we have discovered several unsolved problems that might lead to a network failure. In this paper, we provide a quantitative analysis of the success probability for contending nodes and prove that an adaptive frame length is quite necessary. We propose Adaptive-ADHOC (A-ADHOC) MAC protocol, which implements a robust mechanism supporting the adaptive frame length. Evaluation result shows that A-ADHOC can maintain a high contending success probability and obtain about 50% reduction of response time over original ADHOC protocol, while providing important enhancement on network robustness.","Media Access Protocol,
Ad hoc networks,
Time division multiple access,
Access protocols,
Road safety,
Telecommunication traffic,
Robustness,
Global Positioning System,
Vehicle dynamics,
Broadcasting"
An Assessment of the ECTS in Software Engineering: A Teaching Experience,"Spain is currently implementing the regulatory modifications promulgated by the Declaration of Bologna, which should result in the updating of the structure of university degrees, and the inclusion of the European Credit Transfer and Accumulation System (ECTS) methodology. In some Spanish universities, the experimental adoption of this methodology has been encouraged, with the aim of gaining experience which could be extrapolated to other qualifications and subjects when this system becomes compulsory, which is expected to be in 2008. This paper examines the teaching methodologies that follow the experimental introduction of the ECTS in the Technical Software Engineering degree course taught at the Higher Technical College of the University of Cordoba, Spain. Based on data gathered in three specific subjects during the first and third years, this paper defines the teaching strategies implemented in order to increase the active role of students during their learning. The utility of e-learning tools in this type of teaching is discussed and the academic results are compared with results obtained in previous years using more traditional teaching methodologies.","Education,
Mathematics,
Software engineering,
Laboratories,
Qualifications,
Materials,
Software"
Delaunay-triangulation based complete coverage in wireless sensor networks,"One of the major issues in a wireless sensor network (WSN) is how to cover an interested area. In this paper, we consider the area coverage problem for variable sensing radii WSN. With variable sensing range, the difficulties to cover a continuous space in the area coverage problem becomes exceptionally harder than covering discrete points in the target (or point) coverage problem. Very few papers have paid effort for the former problem. Wang and Medidi have solved the area coverage problem based on Delaunay triangulation structure [2]. However, due to the boundary effect their proposed algorithms cannot always provide complete surveillance for the whole network. In this work, we improve the work in [2] so that the monitored area can be completely covered. A theorem confirms that our improved algorithm provides complete coverage for all the cases. In addition, the simulation further shows that our energy-efficient algorithm has an obvious improvement on coverage status with very small compensation of network lifetime.","Wireless sensor networks,
Energy consumption,
Computer science,
Surveillance,
Monitoring,
Sensor phenomena and characterization,
Energy efficiency,
Intrusion detection,
Scheduling algorithm,
Approximation algorithms"
Adaptive Resource Allocation in Multiuser OFDM System Based on Genetic Algorithm,"Multiuser orthogonal frequency division multiplexing (MU-OFDM) is a promising technique for achieving high downlink capacities in future cellular and wireless local area network systems. The capacity of MU-OFDM can be maximized as long as subchannels and power are allocated adaptively, a new method based on genetic algorithm (GA) is proposed for adaptive power allocation in MU-OFDM system. Some proportional fairness constraints are also added to assure that each user can achieve a required data rate. Each subchannel to the user with the best channel-to-noise ratio has been assigned and an optimal power allocation algorithm based on GA has been proposed. The results demonstrate that the new method we proposed is optimal and efficient to resource allocation.","Resource management,
Genetic algorithms,
OFDM modulation,
Downlink,
Bit error rate,
Constraint optimization,
Mobile communication,
Mobile computing,
Computer networks,
Information science"
Energy-Optimal Grid-Based Clustering in Wireless Microsensor Networks,"Wireless microsensor networks usually consist of a large number of small sensor nodes with limited onboard energy supply and deployed densely in a given area for information harvesting purposes.  To reduce energy consumption and prolong network lifetime, clustering techniques are often used, among which the grid-based ones are very popular due to their simplicity and scalability.  In this paper, we analyze and evaluate the energy-optimal grid size for a grid-based clustering and routing scheme proposed specifically for wireless microsensor networks.  Through numerical and simulation results, we reveal the tradeoff generic to all grid-based clustering schemes.  In addition, we propose a randomized technique to further prolong the network lifetime and discuss other energy-saving opportunities.  This paper provides some insights into the intrinsic limits of grid-based clustering schemes for wireless micro sensor networks.","Microsensors,
Wireless sensor networks,
Energy consumption,
Routing,
Computer science,
Scalability,
Numerical simulation,
Energy conservation,
Propagation losses,
Energy efficiency"
A file-system-aware FTL design for flash-memory storage systems,"As flash memory became popular over various platforms, there is a strong demand on the performance degradation problem, due to the special characteristics of flash memory. This research proposes the design of a file-system-aware flash translation layer, in which a filter mechanism is designed to separate the access requests of file-system metadata and file contents for better performance. A recovery scheme is then proposed to maintain the integrity of a file system. The proposed flash translation layer is implemented as a Linux device driver and evaluated with respect to ext2 and ext3 file systems. The experimental results show significant performance improvement over ext2 and ext3 file systems with limited system overheads.","Flash memory,
File systems,
Computer science,
Filters,
Solid state circuits,
Switches,
Energy consumption,
Design engineering,
Degradation,
Linux"
TCP transmission rate control mechanism based on channel utilization and contention ratio in AD hoc networks,"In ad hoc networks, both contention and congestion can severely affect the performance of TCP. In our work, we first show that the over-injection of conventional TCP window mechanism results in severe contentions, and medium contentions cause network congestion. Furthermore, introducing two metrics, channel utilization (CU) and contention ratio (CR), we characterize the network status. Then, based on these two metrics, we propose a new TCP transmission rate control mechanism based on Channel utilization and Contention ratio (TCPCC). In this mechanism, each node collects the information about the network busy status and determines the CU and CR accordingly. The CU and CR values fed back through ACK are ultimately determined by the bottleneck node along the flow. The TCP sender controls its transmission rate based on the feedback information. Simulation results show that the proposed TCPCC mechanism significantly outperforms the conventional TCP mechanism and the TCP contention control mechanism in terms of throughput and end-to-end delay.","Ad hoc networks,
Chromium,
Throughput,
Delay,
Computer science,
Feedback,
Degradation,
Performance loss,
Interference,
Size control"
Wigner distributions and how they relate to the light field,"In wave optics, the Wigner distribution and its Fourier dual, the ambiguity function, are important tools in optical system simulation and analysis. The light field fulfills a similar role in the computer graphics community. In this paper, we establish that the light field as it is used in computer graphics is equivalent to a smoothed Wigner distribution and that these are equivalent to the raw Wigner distribution under a geometric optics approximation. Using this insight, we then explore two recent contributions: Fourier slice photography in computer graphics and wavefront coding in optics, and we examine the similarity between explanations of them using Wigner distributions and explanations of them using light fields. Understanding this long-suspected equivalence may lead to additional insights and the productive exchange of ideas between the two fields.","Logic gates,
Computational modeling"
How anthropomorphism affects empathy toward robots,A long-standing question within the robotics community is about the degree of human-likeness robots ought to have when interacting with humans. We explore an unexamined aspect of this problem: how people empathize with robots along the anthropomorphic spectrum. We conducted an experiment that measured how people empathized with robots shown to be experiencing mistreatment by humans. Our results indicate that people empathize more strongly with more human-looking robots and less with mechanical-looking robots.,"Robots,
Humans,
Abstracts,
Seismic measurements"
Cluster-Based Cross-Layer Design for Cooperative Caching in Mobile Ad Hoc Networks,"Several protocols have been proposed to improve data accessibility and reduce query delay in MANETs. Some of these proposals have adopted the cooperative caching scheme, allowing multiple mobile hosts within a neighborhood to cache and share data items in their local caches. Cross-layer optimization has not been fully exploited to further improve the performance of cooperative caching in these proposals. In this paper we propose a cluster-based cooperative caching scheme. A cross-layer design approach is employed to further improve the performance of cooperative caching and prefetching schemes. The cross-layer information is maintained in a separate data structure and is shared among network protocol layers. The experimental results in the NS-2 simulation environment demonstrate that the proposed approach improves caching performance in terms of data accessibility, query delay and query distance compared to the caching scheme that does not adopt the cooperative caching strategy.",
On-Chip Single Embryo Coculture With Microporous-Membrane-Supported Endometrial Cells,"In vitro culture (IVC) of the mammalian embryo is an essential technique in reproductive technology and other related life science disciplines. Although embryos are usually cultured in groups, a single embryo culture has been highly desired for IVC to investigate developmental processes. In this study, we proposed and developed the first single embryo coculture device, which allows making an array of a single embryo coculture with endometrial cells by controlling the culture environment in a microfluidic device. To realize this concept, we investigated three key issues: selection of a culture medium for the embryo coculture with endometrial cells using a mouse embryo and endometrial cells, evaluation of an on-microporous-membrane coculture of endometrial cells and an embryo to control the polarization of endometrial cells on the membrane, and evaluation of the coculture of endometrial cells and the embryo in the microfluidic device. We successfully obtained an array of a single coculture of embryo with endometrial cells in a microfluidic device. This concept will open and enhance the management of an individual embryo for assisted reproductive technology, livestock breeding, and fundamental stage research by further development.","Embryo,
Microfluidics,
In vitro,
Mice,
Biomembranes,
Agriculture,
Subspace constraints,
Humans,
Chemicals,
Polarization"
Neural network based secure media access control protocol for wireless sensor networks,"This paper discusses an application of a neural network in wireless sensor network security. It presents a multilayer perceptron (MLP) based media access control protocol (MAC) to secure a CSMA-based wireless sensor network against the denial-of-service attacks launched by adversaries. The MLP enhances the security of a WSN by constantly monitoring the parameters that exhibit unusual variations in case of an attack. The MLP shuts down the MAC layer and the physical layer of the sensor node when the suspicion factor, the output of the MLP, exceeds a preset threshold level. Backpropagation and particle swarm optimization algorithms are used for training the MLP. The MLP-guarded secure WSN is implemented using the Vanderbilt Prowler simulator. Simulation results show that the MLP helps in extending the lifetime of the WSN.","Media Access Protocol,
Neural networks,
Wireless application protocol,
Wireless sensor networks,
Multilayer perceptrons,
Computer crime,
Condition monitoring,
Physical layer,
Backpropagation,
Particle swarm optimization"
Improving Nastalique specific pre-recognition process for Urdu OCR,"Urdu language is written using Arabic script in Nastalique writing style. Nastalique script is highly cursive, context sensitive and is hard to process as only the last character in its ligature sits on the baseline. In addition, it exhibits character and ligature level spatial overlap. Due to these factors, the placement of dots and other diacritics is also highly contextual and variable. There is now increasing amount of work to process and recognize Nastalique script to develop Urdu OCR. This paper proposes improvements to these methods. The paper focuses on Nastalique specific pre-processing methods which can be employed before the text recognition process. The recognition and post recognition processes will be addressed separately.","Optical character recognition software,
Writing,
Text recognition,
Data mining,
Information retrieval,
Image segmentation,
Image recognition,
Optical distortion,
Optical sensors,
Character recognition"
A maximum-power-point tracking algorithm applied to a photovoltaic water-pumping system,This paper proposes a new maximum-power-point tracking algorithm (MPPT) applied to an induction-motor-pump fed by a photovoltaic generator. The induction motor is controlled by the field-oriented control method using two PI current regulators and a PI speed regulator. The maximum-power-point tracker generates the reference speed from the fitted function of reference speed versus solar illumination. The computer simulations and results are presented to prove that the proposed algorithm can be used as a robust maximum-power-point tracker for the photovoltaic water-pumping system.,"Photovoltaic systems,
Solar power generation"
Capturing custom link semantics among heterogeneous artifacts and tools,"Automated techniques aid in minimizing the overhead associated with the capture and maintenance of trace links. However, many challenges to automated traceability remain, such as linking heterogeneous artifacts and capturing custom link semantics. In this position paper, we propose a combination of techniques, including prospective link capture, open hypermedia, and rules, in order to address these challenges and complement current automated techniques. Our approach borrows ideas from e-Science, a domain in which tracing data plays a crucial role in the repeatability of experiments.","Programming,
Joining processes,
Software maintenance,
Software tools,
Ontologies,
Software engineering,
Performance evaluation,
Computer simulation,
Data analysis,
Testing"
Response Time Analysis for the Abort-and-Restart Event Handlers of the Priority-Based Functional Reactive Programming (P-FRP) Paradigm,"Programming microcontrollers is a different paradigm from microprocessor programming. The traditional way to program microcontrollers is to write the program in C or an assembly language, but modern embedded systems are more complex. The Priority-based Functional Reactive Programming (P-FRP) paradigm could make microcontroller programming better. P-FRP makes it possible to treat programs as functions (stateless) and amenable to proofs and type-safety. In this paper, we focus on the abort-and-restart event handler semantics of P-FRP, which is neither a concurrency control policy nor a true scheduling policy. Instead, it is a policy in which the most important task is scheduled first. This paper refines the response time analysis for the abort-and-restart model on single-core systems.","Delay,
Functional programming,
Real time systems,
Microcontrollers,
Embedded system,
Computer applications,
Embedded computing,
Application software,
Computer science,
Microprocessors"
Particle Swarm Optimization for Sorted Adapted Gaussian Mixture Models,"Recently, we introduced the sorted Gaussian mixture models (SGMMs) algorithm providing the means to tradeoff performance for operational speed and thus permitting the speed-up of GMM-based classification schemes. The performance of the SGMM algorithm depends on the proper choice of the sorting function, and the proper adjustment of its parameters. In the present work, we employ particle swarm optimization (PSO) and an appropriate fitness function to find the most advantageous parameters of the sorting function. We evaluate the practical significance of our approach on the text-independent speaker verification task utilizing the NIST 2002 speaker recognition evaluation (SRE) database while following the NIST SRE experimental protocol. The experimental results demonstrate a superior performance of the SGMM algorithm using PSO when compared to the original SGMM. For comprehensiveness we also compared these results with those from a baseline Gaussian mixture model-universal background model (GMM-UBM) system. The experimental results suggest that the performance loss due to speed-up is partially mitigated using PSO-derived weights in a sorted GMM-based scheme.","Particle swarm optimization,
Speaker recognition,
Testing,
Sorting,
NIST,
Gaussian processes,
Databases,
Protocols,
Performance loss,
Computer science education"
Performance analysis of a LINK-16/JTIDS compatible waveform transmitted over a channel with pulse-noise interference,"The Joint Tactical Information Distribution System (JTIDS) is a hybrid frequency-hoped, direct sequence spread spectrum system that utilizes a (31, 15) Reed-Solomon (RS) code and cyclical code-shift keying (CCSK) modulation for the data packets, where each encoded symbol consists of five bits. In this paper, an alternative waveform compatible with the existing JTIDS direct sequence spread spectrum channel waveform is analyzed. The system considered uses the same (31, 15) RS encoding as the original JTIDS but uses 32-ary orthogonal signaling with 32 chip baseband waveforms such as Walsh functions instead of CCSK. Currently, the JTIDS waveform is received noncohe-rently at the chip level, but in this paper the performance of the alternative, JTIDS-compatible waveform is evaluated for coherent as well as for noncoherent demodulation in order to ascertain the performance possible if coherent demodulation were practical. For coherent demodulation, each pair of five-bit symbols at the output of the RS encoder is assumed to undergo serial-to-parallel conversion to two five-bit symbols, which are then independently transmitted on the in-phase and quadrature component of the carrier, with the result that the data rate for coherent demodulation is twice that for noncoherent demodulation. The performance of the alternative waveform for the relatively benign case where additive white Gaussian noise is the only noise present as well as when pulse-noise interference is present is investigated for both coherent and noncoherent demodulation.","Performance analysis,
Interference,
Demodulation,
Spread spectrum communication,
Additive white noise,
Frequency,
Reed-Solomon codes,
Modulation coding,
Baseband,
Gaussian noise"
Long-term prediction of time series by combining direct and MIMO strategies,"Reliable and accurate prediction of time series over large future horizons has become the new frontier of the forecasting discipline. Current approaches to long-term time series forecasting rely either on iterated predictors, direct predictors or, more recently, on the multi-input multi-output (MIMO) predictors. The iterated approach suffers from the accumulation of errors, the Direct strategy makes a conditional independence assumption, which does not necessarily preserve the stochastic properties of the time series, while the MIMO technique is limited by the reduced flexibility of the predictor. The paper compares the direct and MIMO strategies and discusses their respective limitations to the problem of long-term time series prediction. It also proposes a new methodology that is a sort of intermediate way between the Direct and the MIMO technique. The paper presents the results obtained with the ESTSP 2007 competition dataset.","MIMO,
Predictive models,
Stochastic processes,
Computer science,
Neural networks,
Recurrent neural networks,
Computational intelligence,
Machine learning"
Dynamic programming formulation of Micro-Grid operation with heat and electricity constraints,"This paper addresses unit commitment for Micro-Grid optimization including renewable sources, working under deregulated power market. As Micro-Grid supplies both of heat and electricity consumer, operational optimization must be done in coordination. So far in this paper, renewable energy sources are considered to be negative load, and batteries, as well, are used as the load flattened device to raise possibly operational function. In state of solution, Dynamic Programming Method has been developed that solve out the maximum profit Micro-Grid that owner might achieve from energy trading in a day, either in isolated or connected mode.","Dynamic programming,
Resistance heating,
Costs,
Renewable energy resources,
Batteries,
Power generation,
Energy storage,
Energy consumption,
Wind energy,
Photovoltaic systems"
Trust Based Malicious Nodes Detection in MANET,"Node misbehavior due to selfish or malicious intention could significantly degrade the performance of MANET because most existing routing protocols in MANET are aiming at finding most efficiency path. To deal with misbehavior in MANET, an incentive mechanism should be integrated into routing decision-making. In this paper firstly we review existing techniques for secure routing, and then propose to use trust vector model based routing protocols. Each node would evaluate its own trust vector parameters about neighbors through monitoring neighbors' pattern of traffic in network. At the same time, trust dynamics is included in term of robustness. Then we evaluate the performance of the proposed mechanism by modifying Dynamic Source Routing (DSR) so that each node has a dynamic changing ""trust vector"" for its neighbors' behaviors. We also compare the simulation results of with and without the proposed mechanism. The simulation results demonstrate that the proposed mechanisms can effectively detect malicious nodes and mitigate black hole attacks.","Mobile ad hoc networks,
Routing protocols,
Ad hoc networks,
Information security,
Protection,
Public key,
Availability,
Robustness,
Communication system security,
Computer science"
OSAMI-D: An open service platform for healthcare monitoring applications,"In this paper conceptions and architectural considerations of the OSAMI project and their specializations towards the requirements of the e-health domain by the German subproject (OSAMI-D) are described. Along with the expected shift of healthcare service between stationary towards ambulatory care, a standardized way of integrating medical data acquired at home into the IT infrastructure of hospitals and the synchronization with medical workflows have to be implemented. Therefore, the OSAMI-D project will provide open source components that implement the required interfaces. Preliminary results of the requirements analysis and the implementation of first domain-specific services are presented. These services are used to realize two home care scenarios, which support ambulant cardiologic rehabilitation (indoor and outdoor). Special emphasis is placed on standards and formats for the communication and storage of patient data.","Medical services,
Biomedical monitoring,
Patient monitoring,
Wireless sensor networks,
Condition monitoring,
Remote monitoring,
Protocols,
Service oriented architecture,
Intelligent sensors,
Hospitals"
"Detection of P, QRS, and T Components of ECG using wavelet transformation","Electrocardiogram (ECG) signals are composed of five important waves: P, Q, R, S, and T. Sometimes, a sixth wave (U) may follow T. Q, R, and S are grouped together to form the QRS-complex. Detection of these waves is a vital step in ECG signal analysis to extract hidden patterns. Many prior studies have focused only on detection of the QRS-complex, because P and T waves are sparse and harder to isolate from the signal. In this paper, we develop an algorithm to detect all five waves - P, Q, R, S, and T in ECG signals using wavelet transformation. The accuracy for P wave detection is 99.5%, 99.8% for QRS complex, and 99.2% for T waves.","Electrocardiography,
Electric shock,
Computer science,
Signal analysis,
Heart rate detection,
Biomedical engineering,
Helium,
Cardiac disease,
Fibrillation,
Heart rate"
Year,,
Computing with nanoscale memory: Model and architecture,"Emerging nanoscale devices hold tremendous potential in terms of integration density, low power operation and switching speed. Unlike CMOS devices, however, majority of these devices are not suitable for implementing cascaded, irregular logic structure. On the other hand, dense and periodic structures of most emerging nanodevices as well as their bi-stable nature make them amenable to large high-density memory array design. Moreover, self-assembly of many nanostructures is efficient for a bottom-up system design flow. Hence, reconfigurable computing paradigms that use memory as underlying computing element, appear promising for these devices. In this paper, first we study nanoscale FPGA, which extends conventional spatial CMOS FPGA architecture using nanoscale memory and interconnect. Next, we focus on a time-multiplexed memory based computing paradigm that employs two-dimensional memory for improved performance, integration density and resource usage.","Computer architecture,
Memory architecture,
Field programmable gate arrays,
Semiconductor device modeling,
Nanoscale devices,
CMOS logic circuits,
Logic devices,
Reconfigurable logic,
Periodic structures,
Self-assembly"
Data Gathering by Mobile Mules in a Spatially Separated Wireless Sensor Network,"While wireless sensor networks (WSNs) are typically targeted at large-scale deployment, due to many practical or inevitable reasons, a WSN may not always remain connected. In this paper, we consider the possibility that a WSN may be spatially separated into multiple subnetworks. Data gathering, which is a fundamental mission of WSN, thus may rely on a mobile mule (“mule” for short) to conduct data gathering by visiting each subnetwork. This leads to the problem of minimizing the path length traversed by the mobile mule. We show that minimizing the path length, which may reflect the data gathering latency and the energy consumption of the mule is a generalization of the traveling salesman problem and is NP-complete. Some heuristics based on geometrical properties of node deployment are proposed. Our simulation results show that these heuristics perform very close to optimal solutions in most practical cases.","Wireless sensor networks,
Delay,
Mobile computing,
Traveling salesman problems,
Mobile communication,
Environmental factors,
Computer science,
Energy consumption,
Computer network management,
Conference management"
A contractivity approach for probabilistic bisimulations of diffusion processes,"This work is concerned with the problem of characterizing and computing probabilistic bisimulations of diffusion processes. A probabilistic bisimulation relation between two such processes is defined through a bisimulation function, which induces an approximation metric on the expectation of the (squared norm of the) distance between the two processes. We introduce sufficient conditions for the existence of a bisimulation function, based on the use of contractivity analysis for probabilistic systems. Furthermore, we show that the notion of stochastic contractivity is related to a probabilistic version of the concept of incremental stability. This relationship leads to a procedure that constructs a discrete approximation of a diffusion process. The procedure is based on the discretization of space and time. Given a diffusion process, we raise sufficient conditions for the existence of such an approximation, and show that it is probabilistically bisimilar to the original process, up to a certain approximation precision.","Diffusion processes,
Stochastic processes,
Stability,
Sufficient conditions,
Mathematical model,
Formal verification,
Distributed computing,
Systems engineering and theory,
Computer science,
Automata"
Intelligent middleware for high speed maritime mesh networks with satellite communications,"We provide an overview of a Maritime Intelligent Transport System (ITS) which enables increased use of business and social ship based applications by new communications means. The system is based on a high speed maritime mesh network and is fully integrated with legacy satellite communications using our middleware system component. The Intelligent middleware enables user preferences driven use of both the new and legacy communications and awareness of the changing mesh connectivity. The Maritime ITS system is an outcome of a collaboration project between I2R and NICTA. We describe details of the developed proof of concept (POC) platform, including the system architecture, hardware and software modules implemented. We further provide details of the validation testing of the POC system, using selected applications, and outline plans for future enhancements.","Intelligent networks,
Middleware,
Mesh networks,
Satellite communication,
Intelligent systems,
Business communication,
Marine vehicles,
Collaboration,
Computer architecture,
Hardware"
Distributed execution for resource-constrained mobile consumer devices,"Mobile consumer devices take increasingly important roles, more closely and personally interacting with users. As users get used to mobile devices, they often want the same level of computing experience as they can have from desktop PCs, but still in small and light form factors. Considering current technology, we find the limitations of the processor and the memory are still too big in current mobile devices to satisfy demanding mobile users. To alleviate resource limitations, many researchers explored techniques to share the resources of powerful surrogate servers nearby. In that line of research, we propose slim execution for an effective mobile computing paradigm. To experimentally verify our execution model, we develop a code transforming tool, distributed execution transformer (DiET). The DiET takes original Java bytecode and replaces the bodies of heavy methods with remote procedure calls to surrogate servers. Since the modified bytecode is still a legal Java bytecode, mobile devices can download and run the modified bytecode on standard JVMs, cooperating with surrogate servers. Our experiments with the SciMark 2.0 show our distributed execution scheme reduces the execution time by up to 71%.","Personal communication networks,
Mobile computing,
Personal digital assistants,
Distributed computing,
Mobile handsets,
Java,
Supercomputers,
Weather forecasting,
Computer science,
Application software"
Simulation in education and training,"Historically, the use of simulation has been an important aspect of training in some fields (such as aviation). As the cost of computing power decreases simulation is now finding its way into training for other fields. As simulation moves into these other fields, it is increasingly moving away from traditional large hardware systems (e.g., full-motion simulators) to rich virtual environments such as serious games. However, matching the most efficient type and category of simulation to train specific learning needs is a specialized skill and there is a shortage or gap in the training of simulation specialists who can effectively design and employ training simulation. The skills needed by these professionals are presented and a program that has been established to train professionals in developing these required skills is discussed.","Medical simulation,
Computational modeling,
Costs,
FAA,
Power generation economics,
Environmental economics,
Insurance,
Educational technology,
Hardware,
Virtual environment"
Fast group communications in multihop wireless networks subject to physical interference,"In this paper, we present short communication schedules for broadcast, data aggregation, data gathering, and gossiping in multihop wireless networks subject to physical interference. We assume that all communications proceed in synchronous time-slots, each node can transmit at most one packet of fixed size in each time-slot, and all nodes have fixed and equal transmission power. Under mild assumptions, all of our communication schedules for those four group communications have constant approximation bounds. These communication schedules are built upon a general technique which enables a unified graph-theoretic treatment of the communication scheduling subject to the physical interference constraint.","Spread spectrum communication,
Wireless networks,
Broadcasting,
Signal to noise ratio,
Interference constraints,
Computer science,
Processor scheduling,
Broadcast technology,
Protocols,
Tree graphs"
New algorithms for efficient scheduling in Grid Ad-Hoc networks,"The number of devices that can be connected wirelessly in an ad hoc network has increased greatly. Implementation of Grid computing in such environment would create a high processing power by aggregation of processing power of nodes. For implementation of Grid systems on ad-hock wireless networks we propose a new layer called Ad Hoc Grid Layer (AHGL). This layer contains all necessary services for Grid implementation. Considering a dynamic nature of the ad hoc network, it is very important for job execution to select and allocate appropriate processing node. In Grid computing this work is done by the service called scheduler. Using the wired Grid scheduling algorithms for assigning jobs in a Grid ad hoc network is not straightforward. Hence, in this paper we propose new scheduling algorithms which optimize time needed to transmit and execute jobs in Grid environment created on an ad hoc network. Simulation results show that the new proposed scheduling algorithms provide better performances in comparison with application of scheduling algorithms used in wired Grid.",
Python Tools for Reproducible Research on Hyperbolic Problems,Reproducible research in computational science is only possible if the computer codes used to generate published results are distributed and/or archived in a form that can later be used to regenerate the results and can be examined to determine details of the method used. The author discusses some difficulties in achieving this goal and surveys a set of Python tools for facilitating reproducible research on finite volume methods for hyperbolic conservation laws using the Clawpack software.,"Scholarships,
Distributed computing,
Reproducibility of results,
Standards publication,
Testing,
Publishing,
Books,
Writing,
Finite volume methods,
Scientific computing"
Variation-tolerant non-uniform 3D cache management in die stacked multicore processor,"Process variations in integrated circuits have significant impact on their performance, leakage and stability. This is particularly evident in large, regular and dense structures such as DRAMs. DRAMs are built using minimized transistors with presumably uniform speed in an organized array structure. Process variation can introduce latency disparity among different memory arrays. With the proliferation of 3D stacking technology, DRAMs become a favorable choice for stacking on top of a multicore processor as a last level cache for large capacity, high bandwidth, and low power. Hence, variations in bank speed creates a unique problem of non-uniform cache accesses in 3D space. In this paper, we investigate cache management techniques for tolerating process variation in a 3D DRAM stacked onto a multicore processor. We modeled the process variation in a 4-layer DRAM memory to characterize the latency variations among different banks. As a result, the notion of fast and slow banks from the core's standpoint is no longer associated with their physical distances with the banks. They are determined by the different bank latencies due to process variation. We develop cache migration schemes that utilizes fast banks while limiting the cost due to migration. Our experiments show that there is a great performance benefit in exploiting fast memory banks through migration. On average, a variation-aware management can improve the performance of a workload over the baseline (where one of the slowest bank speed is assumed for all banks) by 17.8%. We are also only 0.45% away in performance from an ideal memory where no process variation is present.","Multicore processing,
Delay,
Stacking,
Random access memory,
Circuits,
Space technology,
Process design,
Permission,
Nanoscale devices,
Wires"
Inductive pointing device for tongue control system for computers and assistive devices,"Experimental results for pointing tasks using a tongue control system are reported in this paper. Ten untrained subjects participated in the experiment. Both typing and pointing tasks were performed, in three short-term training sessions, in consecutive days, by each subject. The system provided a key pad (14 sensors) and a mouse pad (10 sensors with joystick functionality) whose placements were interchanged (front, back) in half of the subjects. The pointing tasks consisted of selecting and tracking a target circle (of 50, 75 and 100 pixels diameter) that occurred randomly in each of the 16 positions uniformly distributed along the perimeter of a layout circle of 250 pixels diameter. The throughput was of 0.808 bits per second and the time on target was of 0.164 of the total tracking time. The pads layout, the subjects, the sessions, the target diameters, and the angle of the tracking direction had a statistically significant effect on the two performance measures. Long term training is required to assess the improvement of the user capability.","Tongue,
Control systems,
Coils,
Sensor systems,
Target tracking,
Magnetic sensors,
Inductance,
Magnetic flux,
Humans,
Switches"
"Truly Efficient
2
-Round Perfectly Secure Message Transmission Scheme","In the model of perfectly secure message transmission (PSMT) schemes, there are n channels between a sender and a receiver. An infinitely powerful adversary A may corrupt (observe and forge) the messages sent through t out of n channels. The sender wishes to send a secret s to the receiver perfectly privately and perfectly reliably without sharing any key with the receiver. In this paper, we show the first 2-round PSMT for n = 2t + 1 such that not only the transmission rate is O(n) but also the computational costs of the sender and the receiver are both polynomial in n. This means that we solve the open problem raised by Agarwal, Cramer, and de Haan at CRYPTO 2006. The main novelty of our approach is to introduce a notion of pseudobasis to the coding theory. It will be an independent interest for coding theory, too.","Privacy,
Cryptography,
Broadcasting,
Computational efficiency,
Polynomials,
Information security,
Computer science,
Informatics,
Complexity theory,
Protocols"
Multi-level Speech Emotion Recognition Based on HMM and ANN,"This paper proposes a new approach for emotion recognition based on a hybrid of hidden Markov models (HMMs) and artificial neural network (ANN), using both utterance and segment level information from speech. To combine the advantage on capability to dynamic time warping of HMMs and pattern recognition of ANN, the utterance is viewed as a series of voiced segments, and feature vectors extracted from the segments are normalized into fixed coefficients using orthogonal polynomials methods, and then, distortions are calculated as an input of ANN. Meanwhile, the utterance as a whole is modeled by HMMs, and likelihood probabilities derived from the HMMs are normalized to be another input of ANN. Adopting Beihang University Database of Emotional Speech (BHUDES) and Berlin database of emotional speech, comparison between isolated HMMs and hybrid of HMMs/ANN proves that the approach introduced in this paper is more effective, and the average recognition rate of five emotion states has reached 81.7%.","Emotion recognition,
Hidden Markov models,
Artificial neural networks,
Feature extraction,
Data mining,
Speech recognition,
Acoustic distortion,
Pattern recognition,
Spatial databases,
Support vector machines"
An ontology and rule based intelligent information system to detect and predict myocardial diseases,"Elderly people have a high risk of myocardial diseases. Hence, here we propose an architecture for Ambient Assisted Living (AAL) and telemedicine that supports pre-hospital health emergencies, remote monitoring of patients with chronic conditions and medical collaboration through sharing health-related information resources. Furthermore, it is going to use medical data from vital signs for, on one hand, the detection of symptoms using a rule system based on Jess (tachycardia, arrhythmia …) and, on the other hand, the prediction of illness using chronobiology algorithms. This paper proposes a knowledge base to represent general human information, heart details and electrocardiogram parameters based on ontologies and a Jess' rule system to detect anomalies and patterns from electrocardiogram information. This solution has been tested by the research team's staff, using a wearable electrocardiogram.","Ontologies,
Intelligent systems,
Information systems,
Myocardium,
Diseases,
Biomedical monitoring,
Senior citizens,
Telemedicine,
Remote monitoring,
Collaboration"
Unified Bundling and Registration of Brain White Matter Fibers,"Magnetic resonance diffusion tensor imaging is being widely used to reconstruct brain white matter fiber tracts. To characterize structural properties of the tracts, reconstructed fibers are often grouped into bundles that correspond to coherent anatomic structures. For further group analysis of fiber bundles, it is desirable that corresponding bundles from different studies are coregistered. To address these needs simultaneously, a unified fiber bundling and registration (UFIBRE) framework is proposed in this work. The framework is based on maximizing a posteriori Bayesian probabilities using an expectation maximization algorithm. Given a set of segmented template bundles and a whole-brain target fiber set, the UFIBRE algorithm optimally bundles the target fibers and registers them with the template. The bundling component in the UFIBRE algorithm simplifies fiber-based registration into bundle-to-bundle registration, and the registration component in turn guides the bundling process to find bundles consistent with the template. Experiments with in vivo data demonstrate that the estimated bundles have an ~ 80% consistency with ground truth and the root mean square error between their bundle medial axes is less than one voxel. The proposed algorithm is highly efficient, offering potential routine use for group analysis of white matter fibers.","Diffusion tensor imaging,
Image reconstruction,
Biomedical imaging,
Resonance,
In vivo,
Tensile stress,
Biomedical engineering,
Clustering algorithms,
Bayesian methods,
Root mean square"
System-level energy and performance projections for nanomagnet-based logic,"This paper examines how realistic implementations of the drive circuitry needed to control circuit elements made from nano-scale magnets can affect system-level energy and performance. Expected non-uniform clock fields, clock field discontinuities and out-of-plane fields are all considered. We find that realistic fabrication mechanisms should not inhibit logical correctness and that this technology appears capable of outperforming low power CMOS equivalents with similar energy requirements - and paths to additional energy savings exist.","Logic,
Educational technology,
Automation,
Computer aided instruction,
Automatic control,
Military computing,
Computer science,
Computer science education,
Instruments,
Application software"
Understanding energy consumption of sensor enabled applications on mobile phones,"Recent research in ubiquitous and mobile computing uses mobile phones and wearable accelerometers to monitor individuals’ physical activities for personalized and proactive health care. The goal of this project is to measure and reduce the energy demand placed on mobile phones that monitor individuals’ physical activities for extended periods of time with limited access to battery recharging and mobile phone reception. Many issues must be addressed before mobile phones become a viable platform for remote health monitoring, including: security, reliability, privacy, and, most importantly, energy. Mobile phones are battery-operated, making energy a critical resource that must be carefully managed to ensure the longest running time before the battery is depleted. In a sense, all other issues are secondary, since the mobile phone will simply not function without energy. In this project, we therefore focus on understanding the energy consumption of a mobile phone that runs MIT wockets, physical activity monitoring applications, and consider ways to reduce its energy consumption.","Energy consumption,
Mobile handsets,
Remote monitoring,
Batteries,
Wearable sensors,
Mobile computing,
Pervasive computing,
Physics computing,
Wearable computers,
Biomedical monitoring"
Scalability for Virtual Worlds,"Networked virtual environments (net-VEs) are the next wave of digital entertainment, with Massively Multiplayer Online Games (MMOs) a very popular instance. Current MMO architectures are server-centric in that all game logic is executed at the servers of the company hosting the game. This architecture has lead to severe scalability problems, in particular since MMOs require realistic graphics and game physics – computationally expensive tasks that are currently computed centrally. We propose a distributed action based protocol for net-VEs that pushes most computation to the computers of the players and thereby achieves massive scalability. The key feature of our proposal is a novel distributed consistency model that allows us to explore the tradeoff between scalability, computational complexity at the server, and consistency. We investigate our model both theoretically and through a comprehensive experimental evaluation.",
Hardware aging-based software metering,"Reliable and verifiable hardware, software and content usage metering (HSCM) are of primary importance for wide segments of e-commerce including intellectual property and digital rights management. We have developed the first HSCM technique that employs intrinsic aging properties of components in modern and pending integrated circuits (ICs) to create the first self-enforceable HSCM approach. There are variety of hardware aging techniques that range from electro-migration in wires to slow-down of crystal-based clocks. We focus on transistor aging due to negative bias temperature instability (NBTI) effects where the delay of gates increases proportionally to usage times. We address the problem of how we can measure the amount of time a particular licensed software (LS) is used by designing an aging circuitry and exposing it to the unique inputs associated with each LS. If a particular LS is used longer than specified, it automatically disables itself. Our novel HSCM technique uses a multi-stage optimization problem of computing the delays of gates, their aging degradation factors, and finally LS usage using convex programming. The experimental results show not just viability of the technique but also surprisingly high accuracy in the presence of measurement noise and imperfect aging models. HSCM can be used for many other business and engineering applications such as power minimization, software evaluation, and processor design.",
Extending k-Coverage Lifetime of Wireless Sensor Networks Using Mobile Sensor Nodes,"One of the important issues in wireless sensor network (WSN) is to k-cover the target sensing field and to extend its lifetime. We propose a method to k-cover the field and maximize the WSN lifetime by moving mobile sensor nodes to appropriate positions for a WSN consisting of both static and mobile sensor nodes which periodically collect environmental information. Our target problem is NP-hard. So, we propose a genetic algorithm (GA) based scheme to find a near optimal solution in practical time. In order to speed up the calculation, we devised a method to check a sufficient condition of k-coverage of the field. For the problem that nodes near the sink node have to forward the data from farther nodes, we make a tree where the amount of communication traffic is balanced among all nodes, and add this tree to the initial candidate solutions of our GAbased algorithm. Through computer simulations, we confirmed that our method achieves much longer k-coverage lifetime than conventional methods for 100 to 300 node WSNs.","Wireless sensor networks,
Batteries,
Mobile computing,
Information science,
Energy consumption,
Genetic algorithms,
Sufficient conditions,
Computer simulation,
Temperature sensors,
Frequency"
WoLFram- A Word Level Framework for Formal Verification,"Due to high computational costs of formal verification on pure Boolean level, proof techniques on the word level, like Satisfiability Modulo Theories (SMT), were proposed. Verification methods originally based on Boolean satisfiability (SAT) can directly benefit from this progress. In this work we present the word level framework WoLFram that enables the development of applications for formal verification of systems independent of the underlying proof technique. The framework is partitioned into an application layer, a core engine and a back-end layer. A wide range of applications is implemented, e.g.~equivalence and property checking including algorithms for coverage/property analysis, debugging and robustness checking. The back-end supports Boolean as well as word level techniques, like SMT and Constraint Solving (CSP). This makes WoLFram a stable backbone for the development and quick evaluation of emerging verification techniques.","Formal verification,
Surface-mount technology,
Engines,
Hardware design languages,
Prototypes,
Algorithm design and analysis,
Debugging,
Robustness,
Circuits,
Computer science"
Earliest Deadline Scheduling for Continuous Queries over Data Streams,"Many stream-based applications have real-time performance requirements for continuous queries over time varying data streams. In order to address this challenge, a real-time continuous query model is presented to process multiple queries with timing constraints. In this model, the execution of one tuple passing through an operator path is modeled as a real-time task instance. A fine-grained scheduling strategy named OP-EDF is proposed for real-time scheduling, which schedules the operator path with the earliest deadline of the waiting tuples at any time slot. The performance of the OP-EDF is analyzed from three aspects: schedulability, response time and system overhead. Furthermore, two improved batch scheduling algorithms, termed OP-EDF-Batch and OP-EDF-Gate, are introduced to decrease system overhead of the OP-EDF. The experiment results show that the proposed continuous query model and improved scheduling algorithms are effective in real-time query processing for data streams with bursty arrival rates.","Scheduling algorithm,
Processor scheduling,
Real time systems,
Embedded software,
Computer science,
Data engineering,
Timing,
Query processing,
Monitoring,
Databases"
Error scrubbing codes for flash memories,"Flash memories are the most widely used type of non-volatile electronic memories. Every flash memory cell has q discrete levels for storing information. A prominent property of flash memories is that although it is easy to increase a cell level, to decrease any cell level, a whole block of cells have to be erased and reprogrammed. To minimize the number of expensive block erasure operations and to maintain the data integrity, the data needs to be stored with a strong error-correcting code that can correct enough errors between two erasure operations. In this paper, we introduce the concept of error scrubbing codes. With this new type of error-correcting codes, the cell levels are actively increased when errors appear, even if the errors increase cell levels as well. We show that error-scrubbing codes can outperform conventional error-correcting codes for multi-level flash memories. We present two families of codes based on the L1 metric and a modular construction.","Flash memory,
Error correction codes,
Modular construction,
Error correction,
Computer errors,
Computer science,
Nonvolatile memory,
Flash memory cells,
Noise level"
Integrating prosodic features in extractive meeting summarization,"Speech contains additional information than text that can be valuable for automatic speech summarization. In this paper, we evaluate how to effectively use acoustic/prosodic features for extractive meeting summarization, and how to integrate prosodic features with lexical and structural information for further improvement. To properly represent prosodic features, we propose different normalization methods based on speaker, topic, or local context information. Our experimental results show that using only the prosodic features we achieve better performance than using the non-prosodic information on both the human transcripts and recognition output. In addition, a decision-level combination of the prosodic and non-prosodic features yields further gain, outperforming the individual models.","Feature extraction,
Broadcasting,
Data mining,
Loudspeakers,
Humans,
Computer science,
Information resources,
Speech analysis,
Supervised learning,
System performance"
Real-time highway traffic information extraction based on airborne video,"Real-time image processing is a difficult work for traffic video monitoring. This paper proposed a method to detect and track vehicles on highway based on airship video and therefore calculate traffic parameters in real-time. A blocking road extraction was performed to determine the ROI, and automatically calculate the tilt of the road which contributes to vehicles detection. A lane marks registration was applied to help vehicles detection and calculate geometric size and displacement of vehicles. Vehicles were tracked by an amended mean-shift method whose parameters were RGB color difference instead of color components. Detection was implemented only in key frames and tracking was applied in other frames to reduce the cost of calculation, which made it possible for real time traffic information extraction.","Road transportation,
Data mining,
Vehicle detection,
Road vehicles,
Remote monitoring,
Image processing,
Layout,
Image motion analysis,
Intelligent transportation systems,
Automated highways"
An adaptive steganography scheme for voice over IP,"This paper presents an adaptive steganography scheme for Voice over IP (VoIP). Differing from existing steganography techniques for VoIP, this scheme enhances the embedding transparency by taking into account the similarity between Least Significant Bits (LSBs) and embedded messages. Moreover, we introduce the notion of Partial Similarity Value (PSV). By properly setting the threshold PSV, we can adaptively balance the embedding transparency and capacity. We evaluate the effectiveness of this scheme with G.729a as the codec of the cover speech in StegTalk, a covert communication system based on VoIP. The experimental results demonstrate that our technique provides better performance than the traditional method.","Steganography,
Internet telephony,
Real time systems,
Image storage,
Communication system security,
Paper technology,
Computer science,
Speech codecs,
Speech analysis,
Costs"
A Supervised Time Series Feature Extraction Technique Using DCT and DWT,"The increased availability of time series datasets prompts the development of new tools and methods that allow machine learning classifiers to better cope with time series data. Time series data are usually characterized by a high space dimensionality and a very strong correlation among features. This special nature makes the development of effective time series classifiers a challenging task. This work proposes and analyzes methods combining spectral decomposition and feature selection for time series classification problems and compares them against methods that work with original time series and time-dependent features. Briefly, our approach first applies discrete cosine transform (DCT) or discrete wavelet transform (DWT) on time series data. Then, it performs supervised feature selection/reduction by selecting only the most discriminative set of coefficients to represent the data. Experimental evaluations, carried out on multiple datasets, demonstrate the benefits of our approach in learning efficient and accurate time series classifiers.","Feature extraction,
Discrete cosine transforms,
Discrete wavelet transforms,
Machine learning,
Support vector machines,
Support vector machine classification,
Application software,
Computer science,
Spectral analysis,
Time series analysis"
RASPberry: A stable reader activation scheduling protocol in multi-reader RFID systems,"Recent technological advances have motivated large-scale deployment of RFID systems. RFID readers are often static and carefully deployed in a planned manner. However, the distribution and movements of tags are often dynamically changed and unpredictable. We study a challenging problem of scheduling the activation of the readers without collision such that the system can work in a stable way in the long term. Here a schedule is stable if at any time slot, the number of total unread tags is bounded from above with high probability under this scheduling. In this paper, we propose a stable reader activation scheduling protocol, RASPberry, in multi-reader RFID systems. We analytically prove that our scheduling protocol, RASPberry, is stable if the arrival rate of tags is less than the processing rate of all readers. In RASPberry, at any time slot, a reader can determine its status using only information of readers within a local neighborhood. To the best of our knowledge, this is the first work to address the stability problem of reader activation scheduling in RFID systems. Our extensive simulations show that our system performs very well.","Radiofrequency identification,
Stability,
Throughput,
Computer science,
Processor scheduling,
Dynamic scheduling,
Scheduling algorithm,
Access protocols,
Security"
Feature Selection Algorithm Based on Association Rules Mining Method,This paper presents a novel feature selection algorithm based on the technique of mining association rules. The main idea of the proposed algorithm is to find the features that are closely correlative with the class attribute by association rules mining method. Experimental results on several real and artificial data sets demonstrate that the proposed feature selection algorithm is able to obtain a smaller and satisfactory feature subset when compared with other existing feature selection algorithms. It is a new feature selection algorithm with vast of application prospect and research value.,"Association rules,
Data mining,
Machine learning algorithms,
Machine learning,
Filters,
Training data,
Statistics,
Information retrieval,
Data processing,
Information science"
Operator Placement for Snapshot Multi-predicate Queries in Wireless Sensor Networks,"This work aims at minimize the cost of answering snapshot multi-predicate queries in high-communication-cost networks. High-communication-cost (HCC) networks is a family of networks where communicating data is very demanding in resources, for example in wireless sensor networks transmitting data drains the battery life of sensors involved. The important class of multi-predicate queries in horizontally or vertically distributed databases is addressed. We show that minimizing the communication cost for multi-predicate queries is NP-hard and we propose a dynamic programming algorithm to compute the optimal solution for small problem instances. We also propose a low complexity, approximate, heuristic algorithm for solving larger problem instances efficiently and running it on nodes with low computational power (e.g. sensors). Finally, we present a variant of the Fermat point problem where distances between points are minimal paths in a weighted graph, and propose a solution. An extensive experimental evaluation compares the proposed algorithms to the best known technique used to evaluate queries in wireless sensor networks and shows improvement of 10\% up to 95\%. The low complexity heuristic algorithm is also shown to be scalable and robust to different query characteristics and network size.","Wireless sensor networks,
Heuristic algorithms,
Dynamic programming,
Computer science,
Cost function,
Robustness,
Mobile computing,
Computer network management,
Conference management,
Middleware"
Exploring beneath the PIG Ice Shelf with the Autosub3 AUV,"On 31st January 2009, two numbers: “range and bearing” flashing up on a laptop screen, indicated that Autosub3 had returned from its last mission beneath the Pine Island Glacier (PIG) Ice Shelf in the Western Antarctic. The Autosub technical team from NOCS, Southampton, onboard the US ice breaker Nathanial B Palmer breathed a collective sigh of relief. Any significant technical failure would have resulted in total loss of the multi million Euro Autonomous Underwater Vehicle with no hope of recovery from 60 km into the ice shelf cavity. This was the last of six successful missions to investigate the shape the ice shelf, the sea bed bathymetry, the currents and the physical oceanography within the ice cavity. Each are vital to understanding the interaction between the sea water and the ice shelf, and quantifying whether the melting rate is changing. During the cruise, Autosub3 had run beneath the ice for almost 4 days and for 510 km.","Ice shelf,
Antarctica,
Portable computers,
Network-on-a-chip,
Underwater vehicles,
Oceanographic techniques,
Jacobian matrices,
Earth,
Observatories,
Navigation"
MatSeek: An Ontology-Based Federated Search Interface for Materials Scientists,"By providing an integrated Web interface to the critical materials science databases and analytical tools, MatSeek represents a significant advance toward a full-fledged materials-informatics workbench.","Ontologies,
Metasearch,
XML,
Materials science and technology,
Vocabulary,
Intelligent systems,
Deductive databases,
Data warehouses,
Semantic Web,
Mechanical factors"
Stress-driven MEMS assembly + electrostatic forces = 1mm diameter robot,"As the size of the modules in a self-reconfiguring modular robotic system shrinks and the number of modules increases, the flexibility of the system as a whole increases. In this paper, we describe the manufacturing methods and mechanisms for a 1 millimeter diameter module which can be manufactured en masse. The module is the first step towards realizing the basic unit of claytronics, a modular robotic system designed to scale to millions of units.","Robotic assembly,
Micromechanical devices,
Electrostatics,
Intelligent robots,
Pulp manufacturing,
Circuits,
Lithography,
Shape,
Energy exchange,
USA Councils"
Symmetric Cryptography in Javascript,"We take a systematic approach to developing a symmetric cryptography library in Javascript. We study various strategies for optimizing the code for the Javascript interpreter, and observe that traditional crypto optimization techniques do not apply when implemented in Javascript. We propose a number of optimizations that reduce both running time and code size. Our optimized library is about four times faster and 12% smaller than the fastest and smallest existing symmetric Javascript encryption libraries. On Internet Explorer 8, our library is about 11 times faster than the fastest previously existing code. In addition, we show that certain symmetric systems that are faster than AES when implemented in native x86 code, are in fact much slower than AES when implemented in Javascript. As a result, the choice of ciphers for a Javascript crypto library may be substantially different from the choice of ciphers when implementing crypto natively. Finally, we study the problem of generating strong randomness in Javascript and give extensive measurements validating our techniques.","Cryptography,
Java,
Clouds,
Software libraries,
Application software,
Computer security,
Computer science,
Internet,
Web server,
Medical services"
A portable device for real time drowsiness detection using novel active dry electrode system,"Electroencephalogram (EEG) signals give important information about the vigilance states of a subject. Therefore, this study constructs a real-time EEG-based system for detecting a drowsy driver. The proposed system uses a novel six channels active dry electrode system to acquire EEG non-invasively. In addition, it uses a TMS320VC5510 DSP chip as the algorithm processor, and a MSP430F149 chip as a controller to achieve a real-time portable system. This study implements stationary wavelet transform to extract two features of EEG signal: integral of EEG and zero crossings as the input to a back propagation neural network for vigilance states classification. This system can discriminate alertness and drowsiness in real-time. The accuracy of the system is 79.1% for alertness and 90.91% for drowsiness states. When the system detects drowsiness, it will warn drivers by using a vibrator and a beeper.","Real time systems,
Electrodes,
Electroencephalography,
Vehicles,
Feature extraction,
Computer crashes,
Sleep,
Artificial neural networks,
Wavelet transforms,
Frequency"
SmartLDWS: A robust and scalable lane departure warning system for the smartphones,"Lane Departure Warning Systems (LDWS) have recently become an integral part of many advance vision-based drive assistance systems. However, high cost and the requirement of professional installation have limited such systems to mostly commercial or luxury vehicles. To help bring the technology to the mainstream market, we have leveraged the popularity of smartphones and built SmartLDWS, the first LDWS that runs on these devices. SmartLDWS employs a novel lane detection algorithm that is both robust and scalable to overcome poor camera quality and limited processing power faced by most smartphones. Experimental results show that the system performs reliably with extremely low false-positive under different weather and lighting conditions, detecting various types of lane markings at over 30fps.","Robustness,
Alarm systems,
Smart phones,
Intelligent transportation systems,
Detection algorithms,
Vehicles,
Costs,
Smart cameras,
Face detection,
Computer vision"
Partial discharge modelling in a spherical cavity within a dielectric insulation material as a function of frequency,"The measurement of partial discharge (PD) is used in the performance assessment of an insulation system in high voltage components. Through modeling the discharge process a better understanding of the phenomena may be attained. This paper is an extension from previous works by the same authors which have considered the modeling of PD activity from a spherical cavity by using Finite Element Analysis (FEA) method. However, this paper describes the development of an improved model for a spherical cavity within a homogeneous dielectric material. The model developed has been used to study the influence of applied frequency on PD activity. The model has also been used to simulate the PD measurement results. Therefore, parameters in the model that are affecting PD frequency dependent behavior can be identified through comparison between experimental measurement and simulation results.","Partial discharges,
Dielectrics and electrical insulation,
Dielectric materials,
Voltage,
Electrons,
Equations,
Dielectric measurements,
Partial discharge measurement,
Frequency dependence,
Surface discharges"
A comparison of Type-1 and Type-2 fuzzy controllers in a micro-robot context,"In this paper we compare the differences between type-1 and interval type-2 fuzzy logic controllers, with seven, five and two three term membership functions. The controllers were used to control a DC motor model in a closed loop simulation. The performance of each controller to a step change and a change in motor inertia with and without added noise was recorded. The results showed that there was no statistical difference between the type-1 and type-2 controllers. It was also found that a type-1 three term controller was as good as a type-1 or type-2 seven term controller, in controlling the micro robot DC motor model.","Fuzzy control,
Fuzzy sets,
Fuzzy logic,
Three-term control,
DC motors,
Control systems,
Testing,
Robot control,
Fuzzy systems,
Measurement uncertainty"
A Chain Reaction DoS Attack on 3G Networks: Analysis and Defenses,"The IP multimedia subsystem (IMS) is being deployed in the third generation (3G) networks since it supports many kinds of multimedia services. However, the security of IMS networks has not been fully examined. This paper presents a novel DoS attack against IMS. By congesting the presence service, a core service of IMS, a malicious attack can cause chained automatic reaction of the system, thus blocking all the services of IMS. Because of the low-volume nature of this attack, an attacker only needs to control several clients to paralyze an IMS network supporting one million users. To address this DoS attack, we propose an online early defense mechanism, which aims to first detect the attack, then identify the malicious clients, and finally block them. We formulate this problem as a change-point detection problem, and solve it based on the non-parametric GRSh test. Through trace-driven experiments, we demonstrate that our defense mechanism can throttle this DoS attack within a short defense time window while generating few false alarms.","Computer crime,
Resonance light scattering,
Telecommunication traffic,
Subscriptions,
Protocols,
Communications Society,
Information analysis,
Computer science,
Maintenance engineering,
Signal generators"
An Approach for Eliciting Software Requirements and its Prioritization Using Analytic Hierarchy Process,"Most software engineering methods presume that requirements are explicitly and completely stated; however, experience shows that requirements are rarely complete and usually contain implicit requirements. The failure or success of a software system depends on the quality of the requirements. The quality of the requirements is influenced by the techniques employed during requirements elicitation. Requirements elicitation is most critical part of the software development because errors at this beginning stage propagate through the development process and the hardest to repair later. In this paper we have proposed an algorithmic approach to elicit the software requirements and its prioritization of the requirements using analytic hierarchy process (AHP).","Security,
Ontologies,
Computer science,
Software engineering,
Software systems,
Programming,
Software algorithms,
Communications technology,
Information management,
Technology management"
A Fragile Digital Watermark Used to Verify the Integrity of Vector Map,"The methods of integrity authentication for vector maps were described at fist, then digital watermarking for vector data was introduced. Finally, a fragile watermark algorithm for vector maps was proposed, which was based on that the vector map was divided into a series of blocks, and watermark information was embedded in these blocks. The extraction procedure not only verifies the integrity of the watermarked vector map, but also locates modification. The watermarked vector map can be used for its data is modified slightly, and the original vector map can be restored by extraction procedure for special application.","Watermarking,
Cryptography,
Data mining,
Message authentication,
Data security,
Digital signatures,
Robustness,
Protection,
Computer science,
Computer graphics"
Streaming electrification current density distribution inside pipes assuming overcharged boundary layer,"In this paper the distribution of electrification current flowing across a pipe, for alternating charge near a surface, is presented. A distribution of charge density was arbitrarily assumed compatible to the Stern theory. Assuming that the velocity distribution is parabolic (in laminar regime) or logarithmic (in turbulent regime), the convective current density can be obtained. The distribution of current density shows changes of sign at different distances from the boundary. This may be the reason for the change of total electrification current sign that is often observed during experimental measurements.","Current density,
Current measurement,
Electrostatic measurements,
Thermal conductivity,
Electrodes,
Automatic control,
Computer science,
Home appliances,
Power transformers,
Hazards"
Using Open Web APIs in Teaching Web Mining,"With the advent of the World Wide Web, many business applications that utilize data mining and text mining techniques to extract useful business information on the Web have evolved from Web searching to Web mining. It is important for students to acquire knowledge and hands-on experience in Web mining during their education in information systems curricula. This paper reports on an experience using open Web application programming interfaces (APIs) that have been made available by major Internet companies (e.g., Google, Amazon, and eBay) in a class project to teach Web mining applications. The instructor's observations of the students' performance and a survey of the students' opinions show that the class project achieved its objectives and students acquired valuable experience in leveraging the APIs to build interesting Web mining applications.","Web mining,
Web services,
Computer applications,
Information systems,
Data mining,
Knowledge engineering,
Visualization"
Auto-Correlation Property of Speech and its Application in Voice Activity Detection,"The paper analyzes short term auto-correlation property of speech signal and confirms it through detailed comparing experiment with other kinds of signals. By applying the auto-correlation property of current speech frame and frames nearby, a new feature for voice activity detecting called weighted short-term summation of auto-correlation (WSAC) is formed. It is testified that the new VAD feature can robustly used in environment degraded by noise which has poor correlation, and its performance has little connection with various SNRs, change of noise power etc., in contrast with traditional features commonly used in VAD. Properties of the new feature and principle of robust VAD algorithm based on it are explained in this paper, experiment results and correlative analysis are also given.","Autocorrelation,
Speech analysis,
Working environment noise,
Speech enhancement,
Signal processing algorithms,
Speech processing,
Signal to noise ratio,
Noise robustness,
Noise reduction,
Signal analysis"
Social network classification incorporating link type values,"Classification of nodes in a social network and its applications to security informatics have been extensively studied in the past. However, previous work generally does not consider the types of links (e.g., whether a person is friend or a close friend) that connect social networks members for classification purposes. Here, we propose modified Naive Bayes Classification schemes to make use of the link type information in classification tasks. Basically, we suggest two new Bayesian classification methods that extend a traditional relational Naive Bayes Classifier, namely, the Link Type relational Bayes Classifier and the Weighted Link Type Bayes Classifier. We then show the efficacy of our proposed techniques by conducting experiments on data obtained from the Internet Movie Database.","Social network services,
Computer science,
Facebook,
Heat engines,
Application software,
Computer security,
Informatics,
Bayesian methods,
Internet,
Motion pictures"
The Social Behaviors of Experts in Massive Multiplayer Online Role-Playing Games,"We examine the social behaviors of game experts in Everquest II, a popular massive multiplayer online role-playing game (MMO). We rely on exponential random graph models (ERGM) to examine the anonymous privacy-protected social networks of 1,457 players over a five-day period. We find that those who achieve the most in the game send and receive more communication, while those who perform the most efficiently at the game show no difference in communication behavior from other players. Both achievement and performance experts tend to communicate with those at similar expertise levels, and higher-level experts are more likely to receive communication from other players.","Social network services,
Large-scale systems,
Collaboration,
Computer science,
Communication systems,
Social factors,
Industrial engineering,
Engineering management,
Game theory,
Virtual environment"
A Review of Mobile Terminal-Based Applications for Self-Management of Patients with Diabetes,"Peer-reviewed studies of mobile terminal-based applications for self-management of patients with diabetes were reviewed. Databases of medical and engineering studies were searched for relevant publications, and 39 publications describing 28 studies were identified as examining feasibility, acceptability or effectiveness using empirical methods, preferably involving prospective users. Research methods, application design, and findings were summarized. The current status is that the key features of successful applications for diabetes management have not yet been clarified because of the difficulties in statistical comparison between studies and the limitations of each study. The importance of clarifying the key feature for enhancing motivation in long-term self-management of diabetes taking human-factor-engineering methods into account is discussed.","Diabetes,
Mobile computing,
Personal digital assistants,
Software libraries,
Telemedicine,
Application software,
Intersymbol interference,
Cellular phones,
Hospitals,
Computer science"
Mean-Field Analysis for the Evaluation of Gossip Protocols,"Gossip protocols are designed to operate in very large, decentralised networks. A node in such a network bases its decision to interact (gossip) with another node on its partial view of the global system. Because of the size of these networks, analysis of gossip protocols is mostly done using simulations, that tend to be expensive in computation time and memory consumption. We employ mean-¿eld approximation for an analytical evaluation of gossip protocols. Nodes in the network are represented by small identical stochastic models. Joining all nodes would result in an enormous stochastic process. If the number of nodes goes to in¿nity, however, mean-¿eld analysis allows us to replace this intractably large stochastic process by a small deterministic process. This process approximates the behaviour of very large gossip networks, and can be evaluated using simple matrix-vector multiplications.","Protocols,
Peer to peer computing,
Stochastic processes,
Large-scale systems,
Yarn,
Analytical models,
H infinity control,
Information analysis,
Computer science,
Telematics"
LTE/SAE Model and its Implementation in NS 2,"Expectation and requirements for future wireless communication systems continue to grow and evolve. Thus, 3GPP has considered LTE/SAE to ensure its competitiveness in the future. In LTE/SAE, one of the recurring problems is dimensioning and testing. Modeling is an effective way to solve the problem because model is easy to generate test scenarios and inexpensive in changing test configurations and running test cases. This paper is to introduce how to build an accurate enough LTE/SAE model in NS 2 so that other optimization features can be tested. This open source model includes traffic model and network model. The network model concentrates on the air interface and S1 interface. In the end of the paper, one example is given to demonstrate how to use the model.","Testing,
Educational institutions,
Computer science,
Telecommunication traffic,
Traffic control,
Media Access Protocol,
OFDM,
Performance analysis,
Mobile computing,
Sensor systems"
A clustering-based multi-channel Vehicle-to-Vehicle (V2V) communication system,"Traffic safety in any type of transportation systems is a global concerned issue. In this paper, we propose a clustering-based multi-channel Vehicle-to-Vehicle communication system to provide traffic accident avoidance mechanism. In the proposed system, vehicles are self-organized into different clusters, and the traditional single common medium in ad hoc networks has been divided into multiple control channels and a single data channel. An infrastructure-based TDMA/CDMA technique is designed for the intra- and inter-cluster communications via data channel. The IEEE 802.11 CSMA/CA protocol is utilized by control channels. Our system is evaluated by extensive simulations. The simulation results show the proposed system can support traffic safety efficiently.",
Robust face recognition using posterior union model based neural networks,"Face recognition with unknown, partial distortion and occlusion is a practical problem, and has a wide range of applications, including security and multimedia information retrieval. The authors present a new approach to face recognition subject to unknown, partial distortion and occlusion. The new approach is based on a probabilistic decision-based neural network, enhanced by a statistical method called the posterior union model (PUM). PUM is an approach for ignoring severely mismatched local features and focusing the recognition mainly on the reliable local features. It thereby improves the robustness while assuming no prior information about the corruption. We call the new approach the posterior union decision-based neural network (PUDBNN). The new PUDBNN model has been evaluated on three face image databases (XM2VTS, AT&T and AR) using testing images subjected to various types of simulated and realistic partial distortion and occlusion. The new system has been compared to other approaches and has demonstrated improved performance.","statistical analysis,
face recognition,
neural nets,
probability"
Variable structure control method to the output tracking control of cascade non-linear switched systems - brief paper,This study is concerned with the output tracking control problem for a class of cascade non-linear switched systems with external disturbances under some average dwell-time based switching laws. The problem is solved by the variable structure control technique. The variable structure controllers and the average dwell time are designed under which the output of the closed-loop switched system can follow the desired output exactly after a finite time interval and all the states remain globally bounded. The effectiveness of the proposed design approach is illustrated with simulation results.,"variable structure systems,
cascade systems,
closed loop systems,
control system synthesis,
nonlinear control systems,
time-varying systems"
Context-Aware Recommendation by Aggregating User Context,"Traditional recommendation approaches do not consider the changes of user preferences according to context. As a result, these approaches consider the user’s overall preferences, although the user preferences on items varies according to his/her context. However, in our context-aware approach, we take into account not only user preferences, but also context information. Our approach can be easily adopted for content-based and collaborative filtering based recommendations. To exploit raw context information in recommendation, we abstract the raw context information to a concept level. Moreover, by aggregating the context information, we can improve the quality of recommendation. The results of several experiments show that our method is more precise than the traditional recommendation approaches.","Collaboration,
Filtering,
Context-aware services,
Computer science,
Business,
Information analysis,
Joining processes,
Fuzzy set theory,
Large-scale systems,
Extrapolation"
HMM based hand gesture recognition: A review on techniques and approaches,"Gesture is one of the most natural and expressive ways of communications between human and computer in a virtual reality system. We naturally use various gestures to express our own intentions in everyday life. Hand gesture is one of the important methods of non-verbal communication for human beings for its freer in movements and much more expressive than any other body parts. Hand gesture recognition has a number of potential applications in human-computer interaction, machine vision, virtual reality, machine control in industry, and so on. As a gesture is a continuous motion on a sequential time series, the HMMs (Hidden Markov Models) must be a prominent recognition tool. The most important thing in hand gesture recognition is what the input features are that best represent the characteristics of the moving hand gesture.This paper presents part of literature review on ongoing research and findings on different technique and approaches in gesture recognition using HMMs for vision-based approach.","Hidden Markov models,
Handicapped aids,
Humans,
Image recognition,
Virtual reality,
Cameras,
Character recognition,
Natural languages,
Vocabulary,
Australia"
Efficient shared cache management through sharing-aware replacement and streaming-aware insertion policy,"Multi-core processors with shared caches are now commonplace. However, prior works on shared cache management primarily focused on multi-programmed workloads. These schemes consider how to partition the cache space given that simultaneously-running applications may have different cache behaviors. In this paper, we examine policies for managing shared caches for running single multi-threaded applications. First, we show that the shared-cache miss rate can be significantly reduced by reserving a certain amount of space for shared data. Therefore, we modify the replacement policy to dynamically partition each set between shared and private data. Second, we modify the insertion policy to prevent streaming data (data not reused before eviction) from promoting to the MRU position. Finally, we use a low-overhead sampling mechanism to dynamically select the optimal policy. Compared to LRU policy, our scheme reduces the miss rate on average by 8.7% on 8MB caches and 20.1% on 16MB caches respectively.","Space technology,
Technology management,
Computer science,
Microprocessors,
Multicore processing,
Sampling methods,
Research and development,
Multithreading,
Resource management,
Knowledge management"
Performance and Efficiency of Memetic Pittsburgh Learning Classifier Systems,"In this paper we empirically evaluate several local search (LS) mechanisms that heuristically edit classification rules and rule sets to improve their performance. Two kinds of operators are studied, (1) rule-wise operators, which edit individual rules, and (2) a rule set-wise operator, which takes the rules from N parents (N ≥ 2) to generate a new offspring, selecting the minimum subset of candidate rules that obtains maximum training accuracy. Moreover, various ways of integrating these operators within the evolutionary cycle of learning classifier systems are studied. The combinations of LS operators and policies are integrated in a Pittsburgh approach framework that we call MPLCS for memetic Pittsburgh learning classifier system. MPLCS is systematically evaluated using various metrics. Several datasets were employed with the objective of identifying which combination of operators and policies scale well, are robust to noise, generate compact solutions, and use the least amount of computational resources to solve the problems.","smart recombination,
Learning classifier systems,
Pittsburgh approach,
memetic algorithms,
local search operators"
Emotion recognition with consideration of facial expression and physiological signals,"An emotion recognition system with consideration of facial expression and physiological signals is proposed in this paper. A specific designed mood induction experiment is performed to collect facial expressing images and physiological signals of subjects. We detected 14 feature points and extracted 12 facial features from facial expression images. Meanwhile, we measure the skin conductivity, finger temperature and heart rate from the subject. Both facial and physiological features are adopted to train the classifiers. Two learning vector quantization (LVQ) neural networks were applied to classify four emotions: love, joy, surprise and fear. Experimental results show the proposed recognition system is able to identify four emotions by facial expressions, physiological signals, and both of them.","Emotion recognition,
Signal design,
Mood,
Face detection,
Feature extraction,
Computer vision,
Facial features,
Conductivity measurement,
Skin,
Fingers"
Lower cost quantum gate realizations of multiple-control Toffoli gates,"A systematic method is presented for realizing multiple-control Toffoli gates using elementary quantum gates. Results are presented showing that for the NOT, controlled-NOT and the two square root of NOT gates, the method produces circuits as good and in certain cases better than those known to date. A major feature of the work reported here is that in addition to the previously studied cases of one and the maximum required ancillary lines, the presented method covers all cases in between. The approach is general and can be directly applied to other sets of elementary gates.","Costs,
Quantum computing,
DNA computing,
Circuit synthesis,
Computer science,
Libraries,
Councils,
Boolean functions,
Feedback circuits"
Priority-based spectrum allocation for cognitive radio networks employing NC-OFDM transmission,"In this paper, we present three novel priority-based spectrum allocation techniques for enabling dynamic spectrum access (DSA) networking for non-contiguous orthogonal frequency division multiplexing (NC-OFDM) transmission. With each communication link in the network possessing a specified pair of bit error rate (BER) and throughput requirements for supporting a specific application, the proposed technique assigns one or more blocks of wireless spectrum to these applications in an attempt to simultaneously satisfy these requirements. Specifically, the proposed techniques assigns blocks of spectrum possessing aggregate bandwidth that is sufficient for supporting the intended wireless data service over the communication link. Moreover, since several portions of the wireless spectrum may be heavily attenuated due to frequency-selective fading resulting from multipath propagation, communication links requiring high error robustness are assigned frequency bands located further away from these attenuated regions of spectrum. Thus, the proposed spectrum allocation techniques aims at accommodating communication links supporting several wireless services possessing different performance requirements.","Cognitive radio,
Base stations,
Bit error rate,
OFDM,
Throughput,
Bandwidth,
Frequency,
Fading,
Computer networks,
Mobile communication"
TagRec: Leveraging Tagging Wisdom for Recommendation,"Due to the exponential growth of information on the Web, Recommender Systems have been developed to generate suggestions to help users overcome information overload and sift through huge amounts of information efficiently. Many existing approaches to recommender systems can neither handle very large datasets nor easily deal with users who have made very few ratings. Moreover, traditional recommender systems consider only the rating information, resulting in the loss of flexibility. Tagging has recently emerged as a popular way for users to annotate, organize and share resources on the Web. Several research tasks have shown that tags can represent users’ judgments about Web contents quite accurately. In the light of the facts that both the rating activity and tagging activity can reflect users’ opinions, this paper proposes a factor analysis approach called TagRec based on a unified probabilistic matrix factorization by utilizing both users’ tagging information and rating information. The complexity analysis indicates that our approach can be applied to very large datasets. Furthermore, experimental results on MovieLens data set show that our method performs better than the state-of-the-art approaches.","Tagging,
Recommender systems,
Sparse matrices,
Collaboration,
Filtering,
Information analysis,
Chaos,
Computer science,
Navigation,
Humans"
I-Bug: An intensity-based bug algorithm,"This paper introduces a sensor-based planning algorithm that uses less sensing information than any others within the family of bug algorithms. The robot is unable to access precise information regarding position coordinates, angular coordinates, time, or odometry, but is nevertheless able to navigate itself to a goal among unknown piecewise-analytic obstacles in the plane. The only sensor providing real values is an intensity sensor, which measures the signal strength emanating from the goal. The signal intensity function may or may not be symmetric; the main requirement is that the level sets are concentric images of simple closed curves, i.e. topological circles. Convergence analysis and distance bounds are established for the presented approach.","Robot sensing systems,
Robot kinematics,
Poles and towers,
Robotics and automation,
Level set,
Radio navigation,
Computer science,
Convergence,
Underwater vehicles,
Wireless sensor networks"
Usability challenges in surgical simulator training,"Surgical virtual reality simulators have been taken into use in order to improve surgical skills training. Emergence of simulators increases the need for research and knowledge related to usability of medical simulators. In this study usability of laparoscopic surgical simulator was researched experimentally through combined analysis. Data was gathered with heuristic evaluation, questionnaires, and interviews as well as recorded simulator parameters. Results suggest that the surgical simulator could be more efficient learning and training tool if usability issues such as support and error prevention were reconsidered in more detail. There also seem to be grounds for connecting user support into structured simulator training program.","Usability,
Surgery,
Medical simulation,
Analytical models,
Computational modeling,
Hospitals,
Sections,
Virtual reality,
Computer simulation,
Computer science"
Nondestructive electromagnetic material characterization using a dual waveguide probe: A full wave solution,"A nondestructive technique for determining the complex permittivity and permeability of a perfect electric conductor backed magnetic shielding material using a dual waveguide probe is presented. The dual waveguide probe allows for the simultaneous collection of reflection and transmission coefficients which distinguishes it from single probe methods common in the literature. Theoretical development of these coefficients, which is accomplished through a coupled magnetic field integral equations formulation using Love's equivalence principle and solved via the method of moments (MOM), is discussed. Evaluation of the resulting MOM impedance matrix elements is performed using complex plane integration leading to enhanced computational efficiency and physical insight. Comparison of the theoretical and measured reflection and transmission coefficients using a root finding algorithm leads to the desired permittivity and permeability. Measurement results of a magnetic shielding material are presented and compared to traditional methods for the purpose of validating the new technique. The probe's sensitivity to aperture alignment, sample thickness, and flange thickness is also investigated.","Probes,
Manganese,
Reflection,
Magnetic fields,
Electromagnetic waveguides,
Permeability,
Method of moments"
Finding compound structures in images using image segmentation and graph-based knowledge discovery,"We present an unsupervised method for discovering compound image structures that are comprised of simpler primitive objects. An initial segmentation step produces image regions with homogeneous spectral content. Then, the segmentation is translated into a relational graph structure whose nodes correspond to the regions and the edges represent the relationships between these regions. We assume that the region objects that appear together frequently can be considered as strongly related. This relation is modeled using the transition frequencies between neighboring regions, and the significant relations are found as the modes of a probability distribution estimated using the features of these transitions. Experiments using an Ikonos image show that subgraphs found within the graph representing the whole image correspond to parts of different high-level compound structures.",
Capabilities of Low-Power Wireless Jammers,"In this paper, motivated by the goal of modeling the fine-grain capabilities of jammers for the context of security in low-power wireless networks, we experimentally characterize jamming in networks of CC2420 radio motes and CC1000 radio motes. Our findings include that it is easy to locate J (relative to S and R) and choose its power level so that J can corrupt S's messages with high probability as well as corrupt individual S's bits with nontrivial probability. Internal jammers are however limited in at least two ways: One, it is hard for them to prevent R from detecting that it has received an uncorrupted message from S. And two, the outcome of their corruptions are not only not deterministic, even the probabilities of corrupted outcomes are time-varying. We therefore conclude that it is hard to predict the value resulting from colliding S's messages (bits) with J's messages (bits) and, conversely, to deduce the value sent by S's or J's from the corrupted value received by R.","Jamming,
Wireless networks,
Communication system security,
Interference,
Protocols,
Communications Society,
Computer science,
Context modeling,
Computer security,
Power system security"
Decentralised final value theorem for discrete-time LTI systems with application to minimal-time distributed consensus,"In this study, we consider an unknown discrete-time, linear time-invariant, autonomous system and characterise, the minimal number of discrete-time steps necessary to compute the asymptotic final value of a state. The results presented in this paper have a direct link with the celebrated final value theorem. We apply these results to the design of an algorithm for minimal-time distributed consensus and illustrate the results on an example.","Algorithm design and analysis,
Linear systems,
Polynomials,
Aerospace engineering,
Computer networks,
Application software,
Intelligent agent,
Scholarships"
Ultra compact filters for ultra-wideband (UWB) applications using multilayer ring resonators,"Novel compact ultra-wideband (UWB) bandpass filters are proposed. It is based on multilayer ring resonators (MRR). The ring is applied to any loop shape. The filter has a simple structure for fabrication and the design technique can be adapted in order to realize both narrowband and ultra-wideband filters. With the MRR, filters of bandwidths in excess of 10 GHz while enjoying a wide stop-band can be designed. The fractional bandwidth of more than 143% and a group delay of less than 0.2ns are achieved with this class of filters. Also, in this class of filters, it is easy to place the attenuation poles in order to fit a requirement. Due to its small circuit size (around 2 mm in each dimension), this filter can be used in hybrid integration with small UWB, WLAN and radar devices. Since the filter is located within two ground planes, no radiation affects its characteristics. Two types of the proposed UWB filters were fabricated and the measurement verified the theory and simulation.","Resonator filters,
Ultra wideband technology,
Nonhomogeneous media,
Optical ring resonators,
Band pass filters,
Bandwidth,
Shape,
Fabrication,
Narrowband,
Delay"
On the achievable rate of the fading dirty paper channel with imperfect CSIT,"The problem of dirty paper coding (DPC) over the (multi-antenna) fading dirty paper channel (FDPC) Y = H(X + S) + Z is considered when there is imperfect knowledge of the channel state information H at the transmitter (CSIT). The case of FDPC with positive definite (p.d.) input covariance matrix was studied by the authors in a recent paper, and here the more general case of positive semi-definite (p.s.d.) input covariance is dealt with. Towards this end, the choice of auxiliary random variable is modified. The algorithms for determination of inflation factor proposed in the p.d. case are then generalized to the case of p.s.d. input covariance. Subsequently, the largest DPC-achievable high-SNR (signal-to-noise ratio) scaling factor over the no-CSIT FDPC with p.s.d. input covariance matrix is derived. This scaling factor is seen to be a non-trivial generalization of the one achieved for the p.d. case. Next, in the limit of low SNR, it is proved that the choice of all-zero inflation factor (thus treating interference as noise) is optimal in the ‘ratio’ sense, regardless of the covariance matrix used. Further, in the p.d. covariance case, the inflation factor optimal at high SNR is obtained when the number of transmit antennas is greater than the number of receive antennas, with the other case having been already considered in the earlier paper. Finally, the problem of joint optimization of the input covariance matrix and the inflation factor is dealt with, and an iterative numerical algorithm is developed.",
Qualitative Determinacy and Decidability of Stochastic Games with Signals,"We consider the standard model of finite two-person zero-sum stochastic games with signals. We are interested in the existence of almost-surely winning or positively winning strategies, under reachability, safety, Buchi or co-Buchi winning objectives. We prove two qualitative determinacy results. First, in a reachability game either player 1
can achieve almost-surely the reachability objective, or player 2 can ensure surely the complementary safety objective, or both players have positively winning strategies. Second, in a Buchi game if player 1 cannot achieve almost-surely the Buchi objective, then player 2 can ensure positively the complementary co-Buchi objective. We prove that players only need strategies with finite-memory, whose sizes range from no memory at all to doubly-exponential number of states, with matching lower bounds. Together with the qualitative determinacy results, we also provide fix-point algorithms for deciding which player has an almost-surely winning or a positively winning strategy and for computing the finite memory strategy. Complexity ranges from EXPTIME to 2-EXPTIME with matching lower bounds, and better complexity can be achieved for some special cases where one of the players is better informed than her opponent.","Stochastic processes,
Stochastic systems,
Control system synthesis,
Safety,
Control systems,
Monitoring,
Game theory,
Open systems,
Logic,
Computer science"
Learning Difficulties in Programming Courses: Undergraduates' Perspective and Perception,"Researchers have been searching for alternatives in teaching programming subjects. A reason to this is due to the fact that the compulsory subject in the field of Information Technology has been a challenge and they are tough subjects to learn. On top of that, lacking the understanding in concepts has reduced undergraduates’ interests to pursue further exploration and self-experimentation. In this research work, a study was conducted to investigate the factors that lead to undergraduates’ learning difficulty in programming courses and also their perception on which teaching methodology could be implemented to create richer and interesting learning process. The study involved 182 undergraduates from Multimedia University, Malaysia, who have taken the fundamental programming subject named Computer Programming I. The findings affirmed that undergraduates prefer to learn programming by referring to examples and using drill-practice method whereas learning via lecturing would only decrease their interest level. The challenge has provided an evidence to call for a better solution, game-based learning as an alternative to teach and learn computer programming subjects. Therefore, the authors proposed a game-based learning framework which consists of components that leverage the pedagogical aspects in designing game-based learning environment for programming subjects.","Education,
Programming,
Information technology"
Improving Performance of Matrix Multiplication and FFT on GPU,"In this paper we discuss about our experiences in improving the performance of two key algorithms: the single-precision matrix-matrix multiplication subprogram (SGEMM of BLAS) and single-precision FFT using CUDA. The former is computation-intensive, while the latter is memory bandwidth or communication-intensive. A peak performance of 393 Gflops is achieved on NVIDIA GeForce GTX280 for the former, about 5% faster than the CUBLAS 2.0 library. Better FFT performance results are obtained for a range of dimensions. Some common principles are discussed for the design and implementation of many-core algorithms.","Yarn,
Libraries,
Testing,
Bandwidth,
Programming profession,
Hardware,
Laboratories,
Software performance,
Educational technology,
Computer science education"
Use your mobile computing devices to learn - Contextual mobile learning system design and case studies,"In recent years, with the rapid development of mobile computing technologies, a new learning style-mobile learning has exploded everywhere in our society, which is considered as an essential learning style in the future. This paper presents a mobile learning system taking into account learning context, which is also called contextual mobile learning. With the help of this system, we can learn just in time in our daily lives whenever we need to learn, using mobile computing devices like Tablet PCs and PDAs. The main principles and the development process such as production of learning units and contextualization process are presented. Two case studies are introduced to apply the system in concrete scenarios.",
"Efficient Sketches for Earth-Mover Distance, with Applications","We provide the first sub-linear sketching algorithm for estimating the planar Earth-Mover Distance with a constant approximation. For sets living in the two-dimensional grid [\Delta]^2
, we achieve space\Delta^{\eps}
for approximation O(1/\eps)
, for any desired $0","Nearest neighbor searches,
Kernel,
Approximation algorithms,
Image recognition,
Computer science,
Application software,
Cost function,
Design optimization,
Algorithm design and analysis,
Streaming media"
A collaborative personalized affective video retrieval system,"In this demonstration, a collaborative personalized affective video retrieval is introduced. A dataset of 155 video clips extracted from Hollywood movies were annotated by the emotion felt by participants. More than 1300 annotations from 40 participants were gathered in a database to be used for affective retrieval system. The retrieval system is able to retrieve videos based on emotional keyword query as well as arousal and valence query. The user's personal profile (gender, age, cultural background) was employed to improve the collaborative filtering in retrieval.","Collaboration,
Motion pictures,
Information retrieval,
Filtering,
Cultural differences,
Internet,
Multimedia databases,
Content based retrieval,
Computer vision,
Multimedia systems"
On a small gain theorem for networks of iISS systems,"This paper considers networks consisting of integral input-to-state stable (iISS) subsystems and addresses the problem of verifying iISS property of a given network. First, we focus on construction of continuously differentiable Lyapunov functions, and derive a condition ensuring the iISS of the network comprising n subsystems. Although this approach referred to as the sum-type construction has not yet been reduced to an easily computable condition for general n, the n = 2 case recovers the iISS small-gain condition for two subsystems developed recently. Next, in the case of n subsystems, using Lipschitz continuous Lyapunov functions, this paper derives a small-gain condition. It is shown that this second approach referred to as the max-type construction fails to offer a Lyapunov function if there exist subsystems which are not input-to-state stable (ISS). The relation between the two formulations is discussed in the case of two ISS subsystems.","Lyapunov method,
Stability,
Indium tin oxide,
Nonlinear systems,
Mathematics,
Sufficient conditions,
Interconnected systems,
Integral equations,
Power system interconnection,
Logistics"
Robust Event Boundary Detection in Sensor Networks - A Mixture Model Based Approach,"Detecting event frontline or boundary sensors in a complex sensor network environment is one of the critical problems for sensor network applications. In this paper, we propose a novel algorithm for event frontline sensor detection based on statistical mixture methods with model selection (Akaike, 1973). A boundary sensor is considered as being associated with a multimodal local neighborhood of (univariate or multivariate) sensing readings, and each non-boundary sensor is treated as being with a unimodal sensor reading neighborhood. Furthermore, the set of sensor readings within each sensor's spatial neighborhood is formulated using Gaussian mixture model (McLachlan and Peel, 2000). Two classes of boundary and non-boundary sensors can be effectively classified using the model selection techniques for finite mixture models. Our extensive experimental results demonstrate that our algorithm effectively detects the event boundary with a high accuracy under moderate noise levels.","Event detection,
Multimodal sensors,
Noise robustness,
Computer science,
USA Councils,
Noise level,
Information processing,
Algorithm design and analysis,
Data mining,
Temperature sensors"
Vehicle detection and tracking in relatively crowded conditions,"Aiming at vehicle detection and tracking problems in video monitoring and controlling system, this paper mainly studies vehicle detection and tracking problems in conditions of high traffic density in daytime. This paper is distinguished by two key contributions. First, we develop an improvement — SEAP(Simple but Efficient After Process) which checks the detection results in an accurate way and is an after process of Adaboost [1] detector which used to detect car in every frame. Second, we propose a tracking algorithm named 4-states tracking algorithm based on Kalman[5] linear filter. Tracking results turn unsteady as traffic density grows higher because of much more false positives and false negatives appear. However, 4-states tracking algorithm can solve this problem in an easy way by introducing FSM (Finite State Machine) into tracking algorithm. Finally, we implement a real-time vehicle detection and tracking system with the upper methods. Experiments give good results in relative crowded Conditions.",
Visual exploration of algorithm parameter space,"In this article we apply information visualization techniques to the domain of swarm intelligence. We describe an intuitive approach that enables researchers and designers of stochastic optimization algorithms to efficiently determine trends and identify optimal regions in an algorithm's parameter search space. The parameter space is evenly sampled using low-discrepancy sequences, and visualized using parallel coordinates. Various techniques are applied to iteratively highlight areas that influence the optimization algorithm's performance on a particular problem. By analyzing experimental data with this technique, we were able to gain important insight into the complexity of the target problem domain. For example, we were able to confirm some underlying theoretical assumptions of an important class of population-based stochastic algorithms. Most importantly, the technique improves the efficiency of finding good parameter settings by orders of magnitude.","Particle swarm optimization,
Stochastic processes,
Algorithm design and analysis,
Iterative algorithms,
Data visualization,
Design optimization,
Heuristic algorithms,
Data analysis,
Neural networks,
Mathematical model"
Predicting the execution time of grid workflow applications through local learning,"Workflow execution time prediction is widely seen as a key service to understand the performance behavior and support the optimization of Grid workflow applications. In this paper, we present a novel approach for estimating the execution time of workflows based on Local Learning. The workflows are characterized in terms of different attributes describing structural and runtime information about workflow activities, control and data flow dependencies, number of Grid sites, problem size, etc. Our local learning framework is complemented by a dynamic weighing scheme that assigns weights to workflow attributes reflecting their impact on the workflow execution time. Predictions are given through intervals bounded by the minimum and maximum predicted values, which are associated with a confidence value indicating the degree of confidence about the prediction accuracy. Evaluation results for three real world workflows on a real Grid are presented to demonstrate the prediction accuracy and overheads of the proposed method.","workflow management software,
grid computing,
learning (artificial intelligence),
optimisation"
Forward and backward guarding in early output logic,"Quasi Delay Insensitive asynchronous logic is a very robust system allowing safe implementations while requiring minimal timing assumptions. Unfortunately the design methodologies using this system have always yielded very slow designs. Early output logic is a method which aims to improve the performance of QDI circuits without decreasing their robustness. In order to force QDI restrictions on early output circuits a form of guarding is necessary. This paper presents a new form of guarding which allows partial stage completion allowing desynchronisation of inputs. This is shown to be highly advantageous in cases where the previous style performed poorly. Because the two styles can be mixed, the designs no longer suffer from very poor performance of some QDI constructions.","Timing,
Delay,
Robustness,
Logic circuits,
Asynchronous circuits,
Wire,
Encoding,
Protocols,
Computer science,
Design methodology"
Semantic keyword extraction via adaptive text binarization of unstructured unsourced video,"We propose a fully automatic method for summarizing and indexing unstructured presentation videos based on text extracted from the projected slides. We use changes of text in the slides as a means to segment the video into semantic shots. Unlike precedent approaches, our method does not depend on availability of the electronic source of the slides, but rather extracts and recognizes the text directly from the video. Once text regions are detected within keyframes, a novel binarization algorithm, Local Adaptive Otsu (LOA), is employed to deal with the low quality of video scene text, before feeding the regions to the open source Tesseract1 OCR engine for recognition. We tested our system on a corpus of 8 presentation videos for a total of 1 hour and 45 minutes, achieving 0.5343 Precision and 0.7446 Recall Character recognition rates, and 0.4947 Precision and 0.6651 Recall Word recognition rates. Besides being used for multimedia documents, topic indexing, and cross referencing, our system can be integrated into summarization and presentation tools such as the VAST MultiMedia Browser [1].",
Using ZigBee and Room-Based Location Technology to Constructing an Indoor Location-Based Service Platform,"In recent years, under the advancement of mobile communication technology and popularization of information technology, diversified value added services have emerged. Location-Based Services (LBS) is an application of information and Internet integrated technology that has been developed rapidly in recent years. This study utilized a room-based indoor location mode to construct a wireless sensor network location-oriented service platform. This platform could meet the indoor location search by combining the ZigBee wireless location technology. Based on the concept of LBS, the indoor location-oriented service could be realized by combining the ZigBee location technology and room-based location mode, in order to achieve a ubiquitous campus environment. Users only need to utilize ZigBee Tag with PDA, and a wireless network to log in this information platform, so as to receive the services of ubiquitous campus. The preliminary study has completed this information platform, and implemented it in this campus environment with satisfactory results.","ZigBee,
Radiofrequency identification,
Wireless sensor networks,
Information systems,
Bluetooth,
Service oriented architecture,
Signal processing,
Computer science,
Tellurium,
Mobile communication"
Finger-Vein Image Enhancement Based on Combination of Gray-Level Grouping and Circular Gabor Filter,"As a newly emergent biometric technology, finger-vein recognition has attracted more attentions in personal identification. Generally, finger-vein images have low contrast and uneven illumination due to finger-vein imaging manner and finger-shape variation. So, finger-vein enhancement is indispensable for reliable finger-vein network extraction. This paper proposes a new method based on combination of gray-level grouping (GLG) and circular Gabor filter (CGF) for finger-vein image enhancement. First, GLG is used to reduce illumination fluctuation and improve the contrast of finger-vein images. Then a circular Gabor filter is used to further strengthen vein ridges in images. The experimental results show that this proposed method is capable of enhancing finger-vein image effectively.","Image enhancement,
Gabor filters,
Veins,
Biometrics,
Fingerprint recognition,
Lighting,
Iris,
Fingers,
Histograms,
Information security"
Eye Tracking for Avatar Eye Gaze Control During Object-Focused Multiparty Interaction in Immersive Collaborative Virtual Environments,"In face-to-face collaboration, eye gaze is used both as a bidirectional signal to monitor and indicate focus of attention and action, as well as a resource to manage the interaction. In remote interaction supported by Immersive Collaborative Virtual Environments (ICVEs), embodied avatars representing and controlled by each participant share a virtual space. We report on a study designed to evaluate methods of avatar eye gaze control during an object-focused puzzle scenario performed between three networked CAVETM-like systems. We compare tracked gaze, in which avatars' eyes are controlled by head-mounted mobile eye trackers worn by participants, to a gaze model informed by head orientation for saccade generation, and static gaze featuring non-moving eyes. We analyse task performance, subjective user experience, and interactional behaviour. While not providing statistically significant benefit over static gaze, tracked gaze is observed as the highest performing condition. However, the gaze model resulted in significantly lower task performance and increased error rate.","Avatars,
Collaboration,
Virtual environment,
Eyes,
Remote monitoring,
Resource management,
Design methodology,
Control systems,
Performance evaluation,
Performance analysis"
Security Analysis of Enterprise Network Based on Stochastic Game Nets Model,"In this paper, we propose a novel modeling method, Stochastic Game Nets (SGN), and use it to model and analyze the security issues in enterprise networks. Firstly, the definition and modeling algorithm of Stochastic Game Nets are given. And then we apply the Stochastic Game Nets method to describe the attack and defense course in the enterprise networks successfully, and find a Nash equilibrium. Finally we analyze the confidentiality and integrity of the enterprise network quantificationally based on the model. The method can also be applied to other areas with respect to a game.","Stochastic processes,
Computer security,
Nash equilibrium,
Computer networks,
Game theory,
Computer crime,
Protocols,
Communications Society,
Computer science,
LAN interconnection"
Variable rate Steganography in gray scale digital images using neighborhood pixel information,"In order to improve the security by providing the stego image with imperceptible quality, three different steganographic methods for gray level images are presented in this paper. Four Neighbors, Diagonal Neighbors and Eight Neighbors methods are employed in our scheme. These methods utilize a pixel's dependency on its neighborhood and psycho visual redundancy to ascertain the smooth areas and edged areas in the image. In smooth areas we embed three bits of secret information. In the edged areas, variable rate bits are embedded. From the experimental results it is seen that the proposed methods achieve a much higher visual quality as indicated by the high Peak Signal-to-Noise Ratio (PSNR) in spite of hiding a larger number of secret bits in the image. In addition, to embed this large amount of secret information, at most only half of the total number of pixels in an image is used. Moreover, extraction of the secret information is independent of original cover image.","Steganography,
Digital images,
Pixel,
Data security,
Psychology,
PSNR,
Internet,
Redundancy,
Information technology,
Artificial intelligence"
Monitoring of Watermelon Ripeness Based on Fuzzy Logic,"The aim of this study is to monitor watermelon ripeness based on image processing technique and fuzzy logic as classifier. The RGB color technique is utilized as the extracted features for the watermelon’s rind. Further, the extracted feature is classified using fuzzy logic system to determine the ripeness level of the watermelon. In addition, the same set of watermelons is graded by both human expert and fuzzy logic system for comparison purpose. Results attained demonstrate the ability of the proposed method in grading and classifying ripeness of watermelons.","Fuzzy logic,
Electrical equipment industry,
Feature extraction,
Humans,
Fuzzy set theory,
Computer science,
Computerized monitoring,
Image processing,
Job shop scheduling,
Agriculture"
An Efficient Approach to Interpreting Rigorous Tolerance Semantics for Complicated Tolerance Specification,"The significance of tolerance is obvious and undoubted for product design and manufacturing. However, it is not intuitionistic for the computer to understand the tolerance semantics, which heavily baffles the integration of CAD and CAM. In this paper, a uniform approach to interpreting the rigorous tolerance semantics for complicated tolerance specification (CTS) is proposed with the help of reclassification of tolerance based on tolerance zone (TZ). First, the tolerance is reclassified into three categories: Immovable-TZ (ITZ) tolerance, Translational-TZ (TTZ) tolerance and Floatable-TZ (FTZ) tolerance. Second, a uniform method of interpreting the rigorous tolerance semantics is presented based on the variation along degrees of freedom direction (VDOF) for CTSs. To improve the computational efficiency of implementation of interpreting rigorous tolerance semantics, a novel method is proposed to determine the TZ boundary without performing Boolean intersection operations in 3-D CAD systems. Finally, the algorithm is implemented and some test results are given.","Computer aided manufacturing,
CADCAM,
Computational efficiency,
Testing,
Data models,
ISO standards,
Product design,
Design automation,
Impedance,
Geometry"
Enhanced Local Repair AODV (ELRAODV),"AODV is one of the most common reactive protocols used in MANET [1]. This paper proposes enhancement in popular on demand routing protocol AODV [2] by improving its local repair mechanism. Enhanced Local Repair AODV (ELRAODV) makes mobile nodes more aware of the local connectivity by extending original HELLO to NHELLO message in AODV as proposed in [3]. That extra information of the local neighbors allows ELRAODV to repair a route by sending a unicast request instead to broadcast in original AODV. The results show that ELRAODV performs better in terms of routing overhead, end to end delay than classic AODV. The simulation is done through network Simulator-2 (ns2).","Mobile ad hoc networks,
Routing protocols,
Broadcasting,
Network topology,
Computer science,
Computational modeling,
Wireless LAN,
Spread spectrum communication,
Telecommunication computing,
Telecommunication control"
Nonlinear model predictive control of an inverted pendulum,"In this paper, nonlinear model predictive control is applied to an inverted pendulum apparatus. The sample interval for control calculations is 25 milli-seconds and the associated non-convex constrained optimisation problem involves 61-variables with 241-constraints. Despite this being a challenging problem, it was solved online using a standard sequential quadratic programming approach on a modest hardware platform. The efficacy of the control algorithm is validated via experimental results.","Predictive models,
Predictive control,
Vehicle dynamics,
Constraint optimization,
Quadratic programming,
Hardware,
Circuit simulation,
Displays,
Milling machines,
Computational modeling"
Autonomous behavior system combing motivation with consciousness using dopamine,"To enhance the affinity between humans and robots, we have attempted to give a robot ""consciousness"" and “emotion” such as that identified in humans and animals. A hierarchical structure model has been developed to connect the robot's consciousness with the robot's behavior. However, it is difficult to autonomously control the timing that changes the consciousness and behavior of the robot. Therefore, in order to induce and autonomously change consciousness and behavior, a motivation model has been developed, and was combined with the hierarchical structure model. Then, the action of dopamine in neurotransmitters was incorporated in the motivation model to add activity to the robot in conjunction with the incentive to perform an behavior. In this paper the expression of emotion by a Conscious Behavior Robot (Conbe-I) that incorporated this motivation model, and the autonomous behaviors performed to take an object from human's hand were studied.","Medical robotics,
Service robots,
Robot sensing systems,
Humanoid robots,
Animal structures,
Neurotransmitters,
Robot control,
Human robot interaction,
Timing,
Control systems"
Resolving the Simultaneous Resettability Conjecture and a New Non-Black-Box Simulation Strategy,"Canetti, Goldreich, Goldwasser, and Micali (STOC 2000) introduced the notion of resettable zero-knowledge proofs, where the protocol must be zero-knowledge even if a cheating verifier can reset the prover and have several interactions in which the prover uses the same random tape. Soon afterwards, Barak, Goldreich, Goldwasser, and Lindell (FOCS 2001) studied the closely related notion of resettable soundness, where the soundness condition of the protocol must hold even if the cheating prover can reset the verifier to have multiple interactions with the same verifier's random tape. The main problem left open by this work was whether it is possible to have a single protocol that is simultaneously resettable zero knowledge and resettably sound. We resolve this question by constructing such a protocol. At the heart of our construction is a new non-black-box simulation strategy, which we believe to be of independent interest. This new strategy allows for simulators which ""marry'' recursive rewinding techniques (common in the context of concurrent simulation) with non-black-box simulation. Previous non-black-box strategies led to exponential blowups in computational complexity in such circumstances, which our new strategy is able to avoid.","Cryptographic protocols,
Computational modeling,
Context modeling,
Cryptography,
Computer science,
Computer simulation,
Content addressable storage,
Educational institutions,
Heart,
Context-aware services"
Tracking-reconstruction or reconstruction-tracking? Comparison of two multiple hypothesis tracking approaches to interpret 3D object motion from several camera views,"We developed two methods for tracking multiple objects using several camera views. The methods use the Multiple Hypothesis Tracking (MHT) framework to solve both the across-view data association problem (i.e., finding object correspondences across several views) and the across-time data association problem (i.e., the assignment of current object measurements to previously established object tracks). The “tracking-reconstruction method” establishes two-dimensional (2D) objects tracks for each view and then reconstructs their three-dimensional (3D) motion trajectories. The “reconstruction-tracking method” assembles 2D object measurements from all views, reconstructs 3D object positions, and then matches these 3D positions to previously established 3D object tracks to compute 3D motion trajectories. For both methods, we propose techniques for pruning the number of association hypotheses and for gathering track fragments. We tested and compared the performance of our methods on thermal infrared video of bats using several performance measures. Our analysis of video sequences with different levels of densities of flying bats reveals that the reconstruction-tracking method produces fewer track fragments than the tracking-reconstruction method but creates more false positive 3D tracks.","Tracking,
Cameras,
Image reconstruction,
Position measurement,
Time measurement,
Layout,
Computer vision,
Current measurement,
Trajectory,
Testing"
Design of energy-efficient channel buffers with router bypassing for network-on-chips (NoCs),"Network-on-chip (NoC) architectures are fast becoming an attractive solution to address the interconnect delay problems in chip multiprocessors (CMPs). However, increased power dissipation and limited performance improvements have hindered the wide-deployment of NoCs. In this paper, we combine two techniques of adaptive channel buffers and router pipeline bypassing to simultaneously reduce power consumption and improve performance. Power consumption can be decreased by reducing the size of the router buffers. However, as reducing router buffers alone will significantly degrade performance, we compensate by utilizing the newly proposed dual-function channel buffers that allow flits to be stored on wires when required. Network bypassing technique, on the other hand, allows flits to bypass the router pipeline and thereby avoid the router buffers altogether. We combine the two techniques and attempt to keep the flits on the wires from source to destination. Our simulation results of the proposed methodology combining the two techniques, yield a overall power reduction of 62% over the baseline and improve performance (throughput and latency) by more than 10%.","Network-on-a-chip,
Energy efficiency,
Buffer storage,
Pipelines,
Energy consumption,
Wires,
Added delay,
Power dissipation,
Degradation,
Throughput"
Model of Collective Acceleration of Ions in Spark Stage of Vacuum Discharge,"A new mechanism of the collective acceleration of anomalous ions at the spark stage of a vacuum discharge is proposed. A 2-D electromagnetic particle-in-cell code was developed to study the mechanism. Computer simulation showed that ions are accelerated in the presence of a plasma cloud in the interelectrode gap, where the development of strong electron instability during the propagation of a cathode electron beam leads to potential buildup to a level exceeding the applied voltage. The appearance of the accelerated ions of the interelectrode plasma is accompanied by a jump in the diode current.","Acceleration,
Sparks,
Plasma accelerators,
Electron beams,
Computer simulation,
Plasma simulation,
Clouds,
Electromagnetic propagation,
Cathodes,
Voltage"
Faster Generation of Random Spanning Trees,"In this paper, we set forth a new algorithm for generating approximately uniformly random spanning trees in undirected graphs. We show how to sample from a distribution that is within a multiplicative
(1+δ)
of uniform in expected time
Undefined control sequence \TO
. This improves the sparse graph case of the best previously known worst-case bound of
O(min{mn,
n
2.376
})
, which has stood for twenty years. To achieve this goal, we exploit the connection between random walks on graphs and electrical networks, and we use this to introduce a new approach to the problem that integrates discrete random walk-based techniques with continuous linear algebraic methods. We believe that our use of electrical networks and sparse linear system solvers in conjunction with random walks and combinatorial partitioning techniques is a useful paradigm that will find further applications in algorithmic graph theory.","Tree graphs,
Partitioning algorithms,
Graph theory,
Computer science,
Linear systems,
Sparse matrices,
Random processes"
A Differentially Private Graph Estimator,"We consider the problem of making graph databases such as social network structures available to researchers for knowledge discovery while providing privacy to the participating entities. We show that for a specific parametric graph model, the Kronecker graph model, one can construct an estimator of the true parameter in a way that both satisfies the rigorous requirements of differential privacy and is asymptotically efficient in the statistical sense. The estimator, which may then be published, defines a probability distribution on graphs. Sampling such a distribution yields a synthetic graph that mimics important properties of the original sensitive graph and, consequently, could be useful for knowledge discovery.","Databases,
Social network services,
Computer science,
Data privacy,
Probability distribution,
Random variables,
Data mining,
Conferences,
Sampling methods,
Diseases"
Testing requirements for mobile applications,"Wireless networks have contributed to the technological advance that popularized the use of mobile devices, and fostered the development of applications targeted to these devices. However, issues such as mobility and communication intermittency, as well as processing, storage and battery constraints demand changes in the traditional software testing process. This paper aims to present testing requirements that are specific to applications developed for mobile devices. We also show that these requirements can improve productivity and efficacy in the testing process of an application.",
Boundary ownership by lifting to 2.1D,"This paper addresses the “boundary ownership” problem, also known as the figure/ground assignment problem. Estimating boundary ownerships is a key step in perceptual organization: it allows higher-level processing to be applied on non-accidental shapes corresponding to figural regions. Existing methods for estimating the boundary ownerships for a given set of boundary curves model the probability distribution function (PDF) of the binary figure/ground random variables associated with the curves. Instead of modeling this PDF directly, the proposed method uses the 2.1D model: it models the PDF of the ordinal depths of the image segments enclosed by the curves. After this PDF is maximized, the boundary ownership of a curve is determined according to the ordinal depths of the two image segments it abuts. This method has two advantages: first, boundary ownership configurations inconsistent with every depth ordering (and thus very likely to be incorrect) are eliminated from consideration; second, it allows for the integration of cues related to image segments (not necessarily adjacent) in addition to those related to the curves. The proposed method models the PDF as a conditional random field (CRF) conditioned on cues related to the curves, T-junctions, and image segments. The CRF is formulated using learnt non-parametric distributions of the cues. The method significantly improves the currently achieved figure/ ground assignment accuracy, with 20.7% fewer errors in the Berkeley Segmentation Dataset.","Image segmentation,
Humans,
Shape,
Level set,
Computer science,
Probability distribution,
Random variables,
Layout,
Parallel processing,
Performance evaluation"
Test Input Generation Using UML Sequence and State Machines Models,We propose a novel testing approach that combines information from UML sequence models and state machine models. Current approaches that rely solely on sequence models do not consider the effects of the message path under test on the states of the participating objects. Dinh-Trong et al. proposed an approach to test input generation using information from class and sequence models.We extend their Variable Assignment Graph (VAG) based approach to include information from state machine models. The extended VAG (EVAG) produces multiple execution paths representing the effects of the messages on the states of their target objects.We performed mutation analysis on the implementation of a video store system to demonstrate that our test inputs are more effective than those that cover only sequence diagram paths.,"Unified modeling language,
System testing,
Software testing,
Performance evaluation,
Genetic mutations,
Performance analysis,
Computer science,
Knowledge engineering,
Collaborative work,
Optimized production technology"
A biologically inspired robot for lunar In-Situ Resource Utilization,"Successful long-term settlements on the Moon will need a supply of resources such as oxygen and water, yet the process of regularly transporting these resources from Earth would be prohibitively costly and dangerous. One alternative would be an approach using heterogeneous, autonomous robotic teams, which could collect and extract these resources from the surrounding environment (In-Situ Resource Utilization). The Whegs™ robotic platform, with its demonstrated capability to negotiate obstacles and traverse irregular terrain, is a good candidate for a lunar rover concept. In this research, Lunar Whegs™ is constructed as a proof-of-concept rover that would be able to navigate the surface of the moon, collect a quantity of regolith, and transport it back to a central processing station. The robot incorporates an actuated scoop, specialized feet for locomotion on loose substrates, Light Detection and Ranging (LIDAR) obstacle sensing and avoidance, and sealing and durability features for operation in an abrasive environment.",
Service Composition in Service-Oriented Wireless Sensor Networks with Persistent Queries,"Service-oriented wireless sensor network (WSN) has been recently proposed as an architecture to rapidly develop applications in WSNs. In WSNs, a query task may require a set of services and may be carried out repetitively with a given frequency during its lifetime. A service composition solution shall be provided for each execution of such a persistent query task. Due to the energy saving strategy, some sensors may be scheduled to be in sleep mode periodically. Thus, a service composition solution may not always be valid during the lifetime of a persistent query. When a query task needs to be conducted over a new service composition solution, a routing update procedure is involved which consumes energy. In this paper, we study service composition design which minimizes the number of service composition solutions during the lifetime of a persistent query. We also aim to minimize the total service composition cost when the minimum number of required service composition solutions is derived. A greedy algorithm and a dynamic programming algorithm are proposed to complete these two objectives respectively. The optimality of both algorithms provides the service composition solutions for a persistent query with minimum energy consumption.",
Generating formal specifications for security-critical applications - A model-driven approach,"The SecureMDD approach aims to generate both, a formal specification for verification and executable code, from UML diagrams. The UML models define the static as well as dynamic components of the system under development. This model-driven approach is focused on security-critical applications that are based on cryptographic protocols, esp. Java Card applications. In this paper we describe the generation of the formal specification from the UML model which is then used as input for our interactive verification system KIV. The formal specification is based on abstract state machines and algebraic specifications. It allows to formulate and to prove application-specific security properties.","Formal specifications,
Object oriented modeling,
Unified modeling language,
Cryptographic protocols,
Java,
Security,
Application software,
Software engineering,
Computer languages,
Computer science"
Investigations on features for log-linear acoustic models in continuous speech recognition,"Hidden Markov Models with Gaussian Mixture Models as emission probabilities (GHMMs) are the underlying structure of all state-of-the-art speech recognition systems. Using Gaussian mixture distributions follows the generative approach where the class-conditional probability is modeled, although for classification only the posterior probability is needed. Though being very successful in related tasks like Natural Language Processing (NLP), in speech recognition direct modeling of posterior probabilities with log-linear models has rarely been used and has not been applied successfully to continuous speech recognition. In this paper we report competitive results for a speech recognizer with a log-linear acoustic model on the Wall Street Journal corpus, a Large Vocabulary Continuous Speech Recognition (LVCSR) task. We trained this model from scratch, i.e. without relying on an existing GHMM system. Previously the use of data dependent sparse features for log-linear models has been proposed. We compare them with polynomial features and show that the combination of polynomial and data dependent sparse features leads to better results.","Speech recognition,
Hidden Markov models,
Probability,
Acoustic emission,
Acoustic distortion,
Computer science,
Polynomials,
Natural language processing,
Vocabulary,
Maximum likelihood estimation"
Automatic learning and extraction of multi-local features,"In this paper we introduce a new kind of feature - the multi-local feature, so named as each one is a collection of local features, such as oriented edgels, in a very specific spatial arrangement. A multi-local feature has the ability to capture underlying constant shape properties of exemplars from an object class. Thus it is particularly suited to representing and detecting visual classes that lack distinctive local structures and are mainly defined by their global shape. We present algorithms to automatically learn an ensemble of these features to represent an object class from weakly labelled training images of that class, as well as procedures to detect these features efficiently in novel images. The power of multi-local features is demonstrated by using the ensemble in a simple voting scheme to perform object category detection on a standard database. Despite its simplicity, this scheme yields detection rates matching state-of-the-art object detection systems.",
Sigma Set: A small second order statistical region descriptor,"Given an image region of pixels, second order statistics can be used to construct a descriptor for object representation. One example is the covariance matrix descriptor, which shows high discriminative power and good robustness in many computer vision applications. However, operations for the covariance matrix on Riemannian manifolds are usually computationally demanding. This paper proposes a novel second order statistics based region descriptor, named “Sigma Set”, in the form of a small set of vectors, which can be uniquely constructed through Cholesky decomposition on the covariance matrix. Sigma Set is of low dimension, powerful and robust. Moreover, compared with the covariance matrix, Sigma Set is not only more efficient in distance evaluation and average calculation, but also easier to be enriched with first order statistics. Experimental results in texture classification and object tracking verify the effectiveness and efficiency of this novel object descriptor.","Robustness,
Histograms,
Fractals,
Lighting,
Statistics,
Solids,
Computer science,
Power engineering and energy,
Mathematics,
Power engineering computing"
Real-Time Hand Detection and Tracking against Complex Background,Most hand detection and tracking algorithms can be only applied in the fairly simple and similar background. We propose to combine a modified object detection method proposed by Viola and Jones [4] with the skin-color detection method to perform hand detection and tracking against complex background. Out experimental results show that the proposed method is effective in near real-time speed (15 frames per second.),"Object detection,
Face detection,
Skin,
Computer science,
Phase detection,
Frequency estimation,
Training data,
Signal processing,
Information management,
Signal processing algorithms"
Marching Cubes without Skinny Triangles,"Most computational codes that use irregular grids depend on the single worst triangle's quality: skinny triangles can lead to bad performance and numerical instabilities. Marching cubes (MC) is the standard isosurface grid generation algorithm, and, whereas most triangles it generates are good, it almost always generates some bad triangles. Here, we show how simple changes to MC can lead to a drastically reduced number of degenerate triangles, making it a more practical choice for isosurface grid generation.",
RF energy scavenging system utilising switched capacitor DC-DC converter,"An RF energy scavenging circuit implementing a power matched Villard voltage doubler followed by a switched capacitor DC-DC converter for scavenging ultra-low RF power levels (20 dBm) is presented. Measurement results for the circuit, fabricated in a 130 nm CMOS process, show that 1 V can be generated across a 5 M load from as little as 25.5 Bm of input RF energy at 2.2 GHz. This represents a 9.5 dB improvement, over the measured sensitivity of our RF energy scavenging circuit without the use of a switched capacitor DC-DC converter stage.","voltage multipliers,
CMOS integrated circuits,
DC-DC power convertors,
energy harvesting,
switched capacitor networks,
UHF integrated circuits"
A food image recognition system with Multiple Kernel Learning,"Since health care on foods is drawing people's attention recently, a system that can record everyday meals easily is being awaited. In this paper, we propose an automatic food image recognition system for recording people's eating habits. In the proposed system, we use the Multiple Kernel Learning (MKL) method to integrate several kinds of image features such as color, texture and SIFT adaptively. MKL enables to estimate optimal weights to combine image features for each category. In addition, we implemented a prototype system to recognize food images taken by cellular-phone cameras. In the experiment, we have achieved the 61.34% classification rate for 50 kinds of foods. To the best of our knowledge, this is the first report of a food image classification system which can be applied for practical use.","Image recognition,
Kernel,
Support vector machines,
Cameras,
Image classification,
Medical services,
Prototypes,
Object recognition,
Support vector machine classification,
Computer science"
Patch-based within-object classification,"Advances in object detection have made it possible to collect large databases of certain objects. In this paper we exploit these datasets for within-object classification. For example, we classify gender in face images, pose in pedestrian images and phenotype in cell images. Previous work has mainly targeted the above tasks individually using object specific representations. Here, we propose a general Bayesian framework for within-object classification. Images are represented as a regular grid of non-overlapping patches. In training, these patches are approximated by a predefined library. In inference, the choice of approximating patch determines the classification decision. We propose a Bayesian framework in which we marginalize over the patch frequency parameters to provide a posterior probability for the class. We test our algorithm on several challenging “real world” databases.","Object detection,
Image databases,
Face detection,
Testing,
Libraries,
Bayesian methods,
Detectors,
Educational institutions,
Computer vision,
Neural networks"
Behaviour and protection of doubly-fed induction generators during network faults,"The paper considers the behaviour of the rotor current of a DFIG during a fault on the network. It is seen that a large disturbance of the stator voltage will cause high transient rotor currents which trigger the crowbar protection for the rotor side converter (RSC). Once the crowbar is applied it cannot be removed until its current reaches zero. During the period the crowbar is applied, the machine behaves as a conventional FSIG, losing control of the active and reactive power. Therefore the time taken for the crowbar current to reach zero is crucial as it determines when the RSC can regain power control and how soon the AC voltage can recover. It is seen that the crowbar current can take a long time to decrease to zero and that this has a significant impact on the voltage recovery after fault. The larger the crowbar resistor and the further the generator is from synchronous speed, the shorter the crowbar current transient is. Power and energy dissipation through the crowbar resistor are also considered, and it is observed that the power varies in accordance with the maximum power transfer theorem. The detailed impact of the timings of crowbar removal and RSC reactivation on system voltage recovery is also investigated. It is seen that reactive power control during fault clearance can assist AC voltage recovery.","Protection,
Induction generators,
Rotors,
Voltage,
Reactive power control,
Resistors,
Stators,
Power control,
Synchronous generators,
Energy dissipation"
An Evaluation of the NSGA-II and MOCell Genetic Algorithms for Self-Management Planning in a Pervasive Service Middleware,"Planning (for example choosing most suitable servicesfor self-configuration) is one important task in selfmanagement for pervasive service computing, and can bereduced to the problem of multi-objective services selectionwith constraints. Genetic algorithms (GAs) are effectivein solving such multi-objective optimization problems, andare one of the most successful computational intelligenceapproaches currently available. GAs are beginning to beused in planning for self-management, but there is a lack ofcomprehensive work that evaluates GAs performance andsolution quality, and guides the setting of GAs’ parameters.This situation makes the application of GAs difficultin the pervasive service computing domain in which performance may be critical and the settings of parameters may have big consequences for performance. In this paper, wewill present our evaluations of two GAs, namely NSGA-IIand MOCell, in the GA framework JMetal2.1, for achievingmulti-objective selection of available services. From theseevaluations, suggestions on how and when to use NSGA-IIand MOCell are given in the planning for self-management.Our experiences show that to get a true Pareto front for aproblem, combining solutions set from different GAs is abetter way than using a single GA.",
Novel strategies for hardware acceleration of frequent itemset mining with the apriori algorithm,"Apriori is a prominent data mining algorithm concerned with the problem of frequent itemset mining (FIM) which generally exhibits poor performance on general-purpose systems. This paper presents a novel hardware accelerator for Apriori, improving upon previous hardware acceleration efforts.",
A novel two-tier Bayesian based method for hair segmentation,"In this paper, a novel two-tier Bayesian based method is proposed for hair segmentation. In the first tier, we construct a Bayesian model by integrating hair occurrence prior probabilities (HOPP) with a generic hair color model (GHCM) to obtain some reliable hair seed pixels. These initial seeds are further propagated to their neighborhood pixels by utilizing segmentation results of Mean Shift, to obtain more seeds. In the second tier, all of these selected seeds are used to train a hair-specific Gaussian model, which are combined with HOPP to build the second Bayesian model for pixel classification. Mean Shift results are further utilized to remove holes and spread hair regions. The experimental results illustrate the effectiveness of our approach.","Bayesian methods,
Hair,
Image segmentation,
Content addressable storage,
Face detection,
Information processing,
Computers,
National electric code,
Diversity reception,
Pixel"
"Empirical Investigations of Model Size, Complexity and Effort in a Large Scale, Distributed Model Driven Development Process","Model driven development (MDD) is a software engineering practice that is gaining in popularity. We aim to investigate to what extend it is effective. There is a lack of empirical data to verify the pay-offs of employing MDD tools and techniques. In order to increase the knowledge we have of the impact of MDD in large scale industrial projects, we investigate the project characteristics of a large software development project in which MDD is used in a pure form. This study focuses on analyzing model size and complexity and metrics related to model quality and effort. Furthermore, project team members were asked to elaborate on their views on the impact of using MDD. Our findings include that larger models are more complex, contain more diagrams, are changed more often and worked on longer but do not necessarily contain more defects. However, models that are changed often do contain more defects. Benefits mentioned by team members were an increase in productivity, benefits from a consistent implementation and their perception of improvement of overall quality. Also, a reduction in complexity was attributed to the use of MDD techniques. We could confirm the perceived increase in the quality of the product in that the average amount of defects found is significantly lower than in similar size projects in which MDD was not employed.","Large-scale systems,
Programming,
Productivity,
Software engineering,
Computer science,
Application software,
Computer industry,
Software maintenance,
Software quality,
Government"
Analysis on cyber threats to SCADA systems,"SCADA is the acronym of Supervisory Control and Data Acquisition, which is a communication technology scheme for collecting data from distant facilities and also controlling them on control systems. By SCADA technology it is not necessary to assign operators to remote locations for operating the facilities there. In the beginning SCADA system was locally introduced, and it has been applied to larger and wide-area systems as the information technology evolves. As SCADA system expands to wide area, it has been connected to common communication infrastructure while it was a locally independent control system network initially. Since the Internet came up, all different kinds of communication networks have been being integrated into the Internet for both technological and economic efficiencies. But it also causes the SCADA system to be revealed to the cyber security risks common on the Internet at the same time. Today many major utility services are provided on SCADA networks, so it is very critical to protect the system from those risks. The damage would be very serious if one of the SCADA systems are attacked and thereby stop normal operation, therefore it should be considered to take countermeasures against the threats. Cyber security problem requires multidirectional approach considering many different aspects of vulnerabilities in the system. This paper analyzes the SCADA network vulnerabilities on the aspects of cyber security.","SCADA systems,
Communication system control,
Control systems,
Computer security,
IP networks,
Communications technology,
Information technology,
Communication networks,
Internet,
Protection"
Layered steered space-time codes using multi-dimensional sphere packing modulation,"We present a novel multi-functional multiple-input multiple-output (MIMO) scheme, that combines the benefits of space-time codes (STC), of vertical Bell Labs layered space-time (V-BLAST) scheme as well as of beamforming. To further enhance the attainable system performance and to maximise the coding advantage of the proposed transmission scheme, the system is also combined with multi-dimensional sphere packing (SP) modulation. Additionally, we quantify the capacity of the proposed multi-functional MIMO aided multi-dimensional SP arrangement and propose a novel technique of computing an upper limit on the achievable bandwidth efficiency of the system based on extrinsic information transfer (EXIT) charts. Further system performance improvements can be attained by serially concatenating our proposed scheme with an outer code together with a unity-rate code (URC), where three different receiver structures are created by varying the iterative detection configuration of the constituent decoders/demappers. Moreover, the convergence behaviour of the proposed schemes is evaluated with the aid of EXIT charts. Explicitly, the three proposed systems are capable of operating within 0.9 dB, 0.6 dB and 0.4 dB of the maximum achievable rate limit. Additionally, the three stage assisted SP aided scheme is capable of outperforming its counterpart employing QPSK by 1 dB at a BER of 10-6.","Space time codes,
Modulation coding,
MIMO,
System performance,
Array signal processing,
Bandwidth,
Iterative decoding,
Convergence,
Quadrature phase shift keying,
Bit error rate"
Ontology based application level intrusion detection system by using Bayesian filter,"Web application security is the major security concern for e-business and information sharing communities. Research showed that more than 75% attacks are being deployed at application layer and almost 90% applications are vulnerable to these attacks. Various security mechanisms in the form of signature base models, anomaly detection, scanner, firewall and intrusion detection has been proposed but ineffective to provide complete security solution at application level. These provide partial solutions are ineffective to provide defense against zero day attacks with low false positive rate. We have introduced a novel approach for effective defenses against the application level attacks. Our system use the Bayesian filter to mitigate the context base attacks which are easily eludes packet level inspection. Our intelligent system is ontology base which analyze the input semantically and capable to detect zero day attacks with negligible false positive rates. The ontology base system can be refined and extended over time. Ontology base system also help in focusing on specific portion of network packet where attack is possible, thus reduce the research space and avoid sequential search.","Ontologies,
Intrusion detection,
Bayesian methods,
Information security,
Computer hacking,
Data security,
Filters,
National security,
Protection,
Web server"
Analysis and design of punctured rate-1/2 turbo codes exhibiting low error floors,"The objective of this paper is two-fold. Initially, we present an analytic technique to rapidly evaluate an approximation to the union bound on the bit error probability of turbo codes. This technique exploits the most significant terms of the union bound, which can be calculated straightforwardly by considering the properties of the constituent convolutional encoders. Subsequently, we use the bound approximation to demonstrate that specific punctured rate-1/2 turbo codes can achieve a lower error floor than that of their rate-1/3 parent codes. In particular, we propose pseudo-random puncturing as a means of improving the bandwidth efficiency of a turbo code and simultaneously lowering its error floor.","Turbo codes,
Convolutional codes,
Transfer functions,
Error probability,
Bandwidth,
Telecommunication computing,
Computer errors,
Satellites,
Wireless communication,
Performance analysis"
Master's Degrees in Software Engineering: An Analysis of 28 University Programs,"The software engineering institute published the last reference curriculum for a master's in software engineering in 1991. In 2007, a coalition from academia, industry, and government began creating a new reference curriculum. An early step was to establish a baseline of graduate education by surveying 28 master's programs in software engineering. The survey was largely limited to US schools. Key findings showed that the universities viewed software engineering largely as a specialization of computer science, that faculty size is generally small with few dedicated professors, and that new master's programs continue to start despite the decrease in computer science majors over the past few years. We used the IEEE Computer Society's Software Engineering Body of Knowledge (SWEBOK) to structure our analysis of the 28 curricula, focusing primarily on courses and topics required or semirequired of all students. (A course is semirequired if there is at least a 50 percent chance a student must take it.) Major findings show wide variation in the depth and breadth of SWEBOK coverage in required and semirequired courses, less than 40 percent of all programs requiring an introductory course on software engineering, and many universities having required and semirequired courses that are peripheral to SWEBOK.","Software engineering,
Taxonomy,
Computer science,
Aerospace engineering,
Automotive engineering,
Systems engineering and theory,
Educational institutions,
Educational programs,
Radio access networks,
Collaboration"
Social Tagging in Query Expansion: A New Way for Personalized Web Search,"Social networks and collaborative tagging systems are rapidly gaining popularity as primary means for sorting and sharing data: users tag their bookmarks in order to simplify information dissemination and later lookup. Social Bookmarking services are useful in two important respects: first, they can allow an individual to remember the visited URLs, and second, tags can be made by the community to guide users towards valuable content. In this paper we focus on the latter use: we present a novel approach for personalized web search using query expansion. We further extend the family of well-known co-occurence matrix technique models by using a new way of exploring social tagging services. Our approach shows its strength particularly in the case of disambiguation of word contexts. We show how to design and implement such a system in practice and conduct several experiments on a real web-dataset collected from Regione Lazio Portal1. To the best of our knowledge this is the first study centered on using social bookmarking and tagging techniques for personalization","Tagging,
Web search,
Information retrieval,
Computer science,
Automation,
Artificial intelligence,
Laboratories,
Collaboration,
Sorting,
Uniform resource locators"
How people talk when teaching a robot,"We examine affective vocalizations provided by human teachers to robotic learners. In unscripted one-on-one interactions, participants provided vocal input to a robotic dinosaur as the robot selected toy buildings to knock down. We find that (1) people vary their vocal input depending on the learner's performance history, (2) people do not wait until a robotic learner completes an action before they provide input and (3) people naïvely and spontaneously use intensely affective vocalizations. Our findings suggest modifications may be needed to traditional machine learning models to better fit observed human tendencies. Our observations of human behavior contradict the popular assumptions made by machine learning algorithms (in particular, reinforcement learning) that the reward function is stationary and path-independent for social learning interactions. We also propose an interaction taxonomy that describes three phases of a human-teacher's vocalizations: direction, spoken before an action is taken; guidance, spoken as the learner communicates an intended action; and feedback, spoken in response to a completed action.","Robots,
Buildings,
Dinosaurs,
Training,
Humans,
Learning,
Machine learning"
"Research in Geant4 electromagnetic physics design, and its effects on computational performance and quality assurance","The Geant4 toolkit offers a rich variety of electromagnetic physics models; so far the evaluation of this Geant4 domain has been mostly focused on its physics functionality, while the features of its design and their impact on simulation accuracy, computational performance and facilities for verification and validation have not been the object of comparable attention yet, despite the critical role they play in many experimental applications. A new project is in progress to study the application of new design concepts and software techniques in Geant4 electromagnetic physics, and to evaluate how they can improve on the current simulation capabilities. The application of a policy-based class design is investigated as a means to achieve the objective of granular decomposition of processes; this design technique offers various advantages in terms of flexibility of configuration and computational performance. The current Geant4 physics models have been re-implemented according to the new design as a pilot project. The main features of the new design and first results of performance improvement and testing simplification are presented; they are relevant to many Geant4 applications, where computational speed and the containment of resources invested in simulation production and quality assurance play a critical role.","Physics computing,
Quality assurance,
Computational modeling,
Application software,
Electromagnetic modeling,
Software design,
Process design,
Testing,
Computer applications,
Production"
Offer-based scheduling of deadline-constrained Bag-of-Tasks applications for utility computing systems,"Metaschedulers can distribute parts of a Bag-of-Tasks (BoT) application among various resource providers in order to speed up its execution. When providers cannot disclose private information such as their load and computing power, which are usually heterogeneous, the metascheduler needs to make blind scheduling decisions. We propose three policies for composing resource offers to schedule deadline-constrained BoT applications. Offers act as a mechanism in which resource providers expose their interest in executing an entire BoT or only part of it without revealing their load and total computing power. We also evaluate the amount of information resource providers need to expose to the metascheduler and its impact on the scheduling. Our main findings are: (i) offer-based scheduling produces less delay for jobs that cannot meet deadlines in comparison to scheduling based on load availability (i.e. free time slots); thus it is possible to keep providers' load private when scheduling multi-site BoTs; and (ii) if providers publish their total computing power they can have more local jobs meeting deadlines.","Processor scheduling,
Computer applications,
Job shop scheduling,
Grid computing,
Cloud computing,
Application software,
Information resources,
Delay,
Biological system modeling,
Message passing"
Gait-Based Recognition of Human Using an Embedded Hidden Markov Models,"An embedded hidden Markov models (e-HMM) gait recognition scheme based on gait energy image (GEI) is proposed. First, the mean GEI is calculated from gait periodic, then we analyze the mean GEI regions, making use of the two dimensional discrete cosine transform (2D-DCT) to transfer the regions into observation vector, and complete the e-HMM training and humans recognition. We compare the proposed algorithm with other gait recognition approaches on USF HumanID Database and CASIA Gait Database. Experimental results show that the proposed approach is valid and has encouraging recognition performance.","Humans,
Hidden Markov models,
Legged locomotion,
Image recognition,
Image databases,
Speech recognition,
Face recognition,
Power engineering and energy,
Discrete cosine transforms,
Spatial databases"
A Fuzzy Logic-Based Trust Model in Grid,"Trust is a fundamental concern in Grid environment. In this paper, we focus on the behavior trust that varies with time. Because of the fuzzy nature of trust, it is more appropriate to adopt fuzzy logic to express and compute trust than adopt probabilities approach. A new behavior trust model based on fuzzy-logic in Grid environment is proposed. By variable weighted fuzzy comprehensive evaluation, Direct Trust can be gotten; by derivation and combination of trust, Reputation can be obtained. Expert’s experience is used to set and simplify fuzzy rules. Malicious recommendation in trust transmission process are also be removed and punished in this model. Simulation results show that entities in Grid can use resources or deploy services more securely in the support of this fuzzy trust model.","Fuzzy logic,
Fuzzy sets,
Grid computing,
Bayesian methods,
Computer security,
Wireless communication,
Computer networks,
Computer science,
Electronic mail,
Probability distribution"
A programming architecture for smart autonomous underwater vehicles,"Autonomous underwater vehicles (AUVs) are an indispensable tool for marine scientists to study the world's oceans. The Slocum glider is a buoyancy driven AUV designed for missions that can last weeks or even months. Although successful, its hardware and layered control architecture is rather limited and difficult to program. Due to limits in its hardware and software infrastructure, the Slocum glider is not able to change its behavior based on sensor readings while underwater. In this paper, we discuss a new programming architecture for AUVs like the Slocum. We present a new model that allows marine scientists to express AUV missions at a higher level of abstraction, leaving low-level software and hardware details to the compiler and runtime system. The Slocum glider is used as an illustration of how our programming architecture can be implemented within an existing system. The Slocum's new framework consists of an event driven, finite state machine model, a corresponding compiler and runtime system, and a hardware platform that interacts with the glider's existing hardware infrastructure. The new programming architecture is able to implement changes in glider behavior in response to sensor readings while submerged. This crucial capability will enable advanced glider behaviors such as underwater communication and swarming. Experimental results based on simulation and actual glider deployments off the coast of New Jersey show the expressiveness and effectiveness of our prototype implementation.","Underwater vehicles,
Hardware,
Oceans,
Computer architecture,
Costs,
Gravity,
Control systems,
Intelligent robots,
USA Councils,
Robot programming"
Settling the Complexity of Arrow-Debreu Equilibria in Markets with Additively Separable Utilities,"We prove that the problem of computing an Arrow-Debreu market equilibrium is PPAD-complete even when all traders use additively separable, piecewise-linear and concave utility functions. In fact, our proof shows that this market-equilibrium problem does not have a fully polynomial-time approximation scheme, unless every problem in PPAD is solvable in polynomial time.","Polynomials,
Piecewise linear approximation,
Programmable control,
USA Councils,
Piecewise linear techniques,
Computer science,
Computational complexity,
Nash equilibrium,
Pricing"
Integrated voltage reference and comparator circuits for GaN smart power chip technology,"GaN smart power chip technology has been realized using GaN-on-Si HEMT platform, featuring monolithically integrated high-voltage power devices and low-voltage peripheral devices for mixed-signal functional blocks. Two imperative functional blocks for smart power applications with wide-temperature-range stability are demonstrated. The first one is a voltage reference generator, and the second one is a temperature-compensated comparator. These circuits are capable of proper functions within a wide temperature range from room temperature up to 250 °C, illustrating the unique advantage of the wide-bandgap GaN. The voltage reference generator was designed with an AlGaN/GaN HEMT and Schottky diodes, and the devices were operated in the subthreshold regime to obtain low power consumption. The voltage reference generator achieved an average drift of less than 70 ppm/°C and can be used as a reference voltage in various biasing and sensing circuits. The temperature-dependent performance of a conventional comparator is characterized and a new temperature-compensated comparator circuit is proposed. The positive limiting level of the temperature-compensated comparator is less than 450 ppm/°C drift compared to 1350 ppm/°C in the conventional comparator.",
Capacitor voltage estimation for predictive control algorithm of flying capacitor converters,"Multilevel Converters have emerged as a promising alternative to traditional two level converters, especially flying capacitor converters because of the fact that this topology requires only a main dc-link voltage and presents a easy way to increase the output voltage levels by increasing the number of cells. Unfortunately, a balancing of capacitor voltage is required. Recently, predictive control algorithms have been presented in order to control not only the output current but also to achieve good performance in the balancing of the capacitor voltages. For this purpose, it is necessary to know the state of these voltages generally taking a measurement of them, therefore the number of sensors required will be increased regarding the output voltage levels desired. This paper presents an estimation of the capacitor voltages using a Discrete Kalman Filter. This algorithm is employed to determinate correctly the system state and thus provides this information to the Predictive Controller in order to determinate the best switching combination to be applied in the next sample period.","Capacitors,
Predictive control,
Prediction algorithms,
Voltage control,
Switches,
Voltage measurement,
Switching converters,
Electric variables control,
Capacitive sensors,
Current measurement"
On the effectiveness of fuzzy clustering as a data discretization technique for large-scale classification of solar images,"This paper presents experimental results on the utilization of fuzzy clustering as a discretization technique for purpose of solar images recognition. By extracting texture features from our solar images, and consequently applying fuzzy clustering techniques on these features, we were able to determine what clustering algorithm and what algorithm's initialization parameters produced the best data discretization. Based on these results we discretized some of our texture features and ran them on two different classifiers comparing how well the classifiers performed on our original data versus the discretized data. Our experimental results demonstrate that discretization of our data via fuzzy clustering carries significant potential since on our classifiers produced similar results on the original and the discretized data, and the reduction of storage space achieved through cluster-based discretization has been very significant.","Large-scale systems,
Sun,
Pixel,
Image recognition,
Data mining,
Feature extraction,
Clustering algorithms,
Observatories,
Information retrieval,
Image retrieval"
Generalization of signatures for SSH encrypted traffic identification,"The objective of this work is to discover generalized signatures for identifying encrypted traffic where SSH is taken as an example application. What we mean by generalized signatures is that the signatures learned by training on one network are still valid when they are applied to traffic coming from a totally different network. We identified 13 signatures and 14 flow attributes for SSH traffic classification where IP addresses, source/destination ports and payload information are not employed. The signatures are able to identify encrypted traffic with high detection rate and low false positive rate. We can achieve up to 97% DR and 0.8% FPR for identifying SSH traffic.",Cryptography
A new signature similarity measure,"The paper presents a new signature similarity measure and new efficient method of recognizing handwritten signatures. Each signature is represented as a set of features such as coordinates of signature points, pen pressure, and speed of writing. Proposed approach consists in dividing signature into windows and calculating similarity values between individual windows. The influence of the size of windows and their location in a signature has been analysed. Additionally, the influence of individual features on the signature similarity value has been examined.",
Diversity exploration and negative correlation learning on imbalanced data sets,"Class imbalance learning is an important research area in machine learning, where instances in some classes heavily outnumber the instances in other classes. This unbalanced class distribution causes performance degradation. Some ensemble solutions have been proposed for the class imbalance problem. Diversity has been proved to be an influential aspect in ensemble learning, which describes the degree of different decisions made by classifiers. However, none of those proposed solutions explore the impact of diversity on imbalanced data sets. In addition, most of them are based on re-sampling techniques to rebalance class distribution, and over-sampling usually causes overfitting (high generalisation error). This paper investigates if diversity can relieve this problem by using negative correlation learning (NCL) model, which encourages diversity explicitly by adding a penalty term in the error function of neural networks. A variation model of NCL is also proposed - NCLCost. Our study shows that diversity has a direct impact on the measure of recall. It is also a factor that causes the reduction of F-measure. In addition, although NCL-based models with extreme settings do not produce better recall values of minority class than SMOTEBoost [1], they have slightly better performance of F-measure and G-mean than both independent ANNs and SMOTEBoost and better recall than independent ANNs.",
"Cylindrical Silicon-on-Insulator Microdosimeter: Design, Fabrication and TCAD Modeling","A novel silicon-on-insulator (SOI) microdosimeter has been designed and fabricated using planar processing techniques to realise a device with a micron-scale well-defined sensitive volume. Cylindrical structures were employed to allow for an improved definition of the average chord length of the sensitive volume over that of previous elongated parallelepiped solid-state detector designs. The structures were manufactured on individual silicon mesas situated on top of a buried oxide insulating layer. The mesa design eliminated lateral charge diffusion. Two kinds of test structures were designed with sensitive region widths of 2 mum and 10 mum. In addition, an array of 900 cylindrical diodes was fabricated to increase the charge collection statistics. TCAD (Technology Computer Aided Design) modeling of the electrostatic potential and electric field profile of the cylindrical microdosimeter was carried out to obtain 3D potential and electric field profiles. The modeling revealed a radial electric field within the cylindrical-shaped sensitive volume with a 1/r dependence. While the electric field at the core of the cylindrical microdosimeter was not sufficiently high to induce avalanche signal multiplication, the higher electric field at the core should still assist in the measurement of low linear-energy transfer (LET) events.","Silicon on insulator technology,
Fabrication,
Electric potential,
Solid state circuits,
Detectors,
Manufacturing,
Insulation,
Testing,
Diodes,
Statistics"
A 1-GS/s 6-bit 6.7-mW ADC in 65-nm CMOS,"An asynchronous 6bit 1GS/s ADC is achieved by time interleaving two ADCs based on binary successive approximation algorithm (SA) using a capacitive ladder. The semi-close loop asynchronous technique eliminates the high internal clocks and significantly speeds up the SA algorithm. One bit redundancy is implemented to compensate the process variation of parasitic and the MOM capacitance. Fabricated in 65nm CMOS with an active area of 0.11mm2, it achieves a peak SNDR of 31.5dB at 1 GS/s sampling rate and has a power consumption of 6.7mW for the analog and digital processing.","Interleaved codes,
Approximation algorithms,
Clocks,
Message-oriented middleware,
Parasitic capacitance,
CMOS process,
Sampling methods,
Energy consumption"
Visualizing the Intellectual Structure with Paper-Reference Matrices,"Visualizing the intellectual structure of scientific domains using co-cited units such as references or authors has become a routine for domain analysis. In previous studies, paper-reference matrices are usually transformed into reference-reference matrices to obtain co-citation relationships, which are then visualized in different representations, typically as node-link networks, to represent the intellectual structures of scientific domains. Such network visualizations sometimes contain tightly knit components, which make visual analysis of the intellectual structure a challenging task. In this study, we propose a new approach to reveal co-citation relationships. Instead of using a reference-reference matrix, we directly use the original paper-reference matrix as the information source, and transform the paper-reference matrix into an FP-tree and visualize it in a Java-based prototype system. We demonstrate the usefulness of our approach through visual analyses of the intellectual structure of two domains: information visualization and Sloan Digital Sky Survey (SDSS). The results show that our visualization not only retains the major information of co-citation relationships, but also reveals more detailed sub-structures of tightly knit clusters than a conventional node-link network visualization.","Data visualization,
Information analysis,
Prototypes,
Java,
Tree data structures,
Chaos,
Joining processes,
Testing,
Libraries,
Information science"
An approximation algorithm for minimum-delay peer-to-peer streaming,"Peer-to-peer (P2P) technology provides a scalable solution in multimedia streaming. Many streaming applications, such as IPTV and video conferencing, have rigorous constraints on end-to-end delays. Obtaining assurances on meeting those delay constraints in dynamic and heterogenous network environments is a challenge. In this paper, we devise a streaming scheme which minimizes the maximum end-to-end streaming delay for a mesh-based overlay network paradigm. We first formulate the minimum-delay P2P streaming problem, called the MDPS problem, and prove its NP-completeness. We then present a polynomial-time approximation algorithm to this problem, and show that the performance of our algorithm is bounded by a ratio of O. Our simulation study reveals the effectiveness of our algorithm, and shows a reasonable message overhead.","Approximation algorithms,
Peer to peer computing,
Streaming media,
IPTV,
Videoconference,
Scheduling algorithm,
Robustness,
Delay estimation,
Computer science,
Bioinformatics"
Efficient data aggregation in multi-hop wireless sensor networks under physical interference model,"Efficient aggregation of data collected by sensors is crucial for a successful application of wireless sensor networks (WSNs). Both minimizing the energy cost and reducing the time duration (or called latency) of data aggregation have been extensively studied for WSNs. Algorithms with theoretical performance guarantees are only known under the protocol interference model, or graph-based interference models generally. In this paper, we study the problem of designing time efficient aggregation algorithm under the physical interference model. To the best of our knowledge, no algorithms with theoretical performance guarantees are known for this problem in the literature. We propose an efficient algorithm that produces a data aggregation tree and a collision-free aggregation schedule. We theoretically prove that the latency of our aggregation schedule is bounded by O(R + Delta) time-slots. Here R is the network radius and Delta is the maximum node degree in the communication graph of the original network. In addition, we derive the lower-bound of latency for any aggregation scheduling algorithm under the physical interference model. We show that the latency achieved by our algorithm asymptotically matches the lower-bound for random wireless networks. Our extensive simulation results corroborate our theoretical analysis.",
Malware Detection Based on Suspicious Behavior Identification,"Along with the popularization of computers, especially the wide use of Internet, malicious code in recent years has presented a serious threat to our world. In this paper, through the analysis against the suspicious behaviors of vicious program by function calls, we present an approach of malware detection which is based on analysis and distilling of representative characteristic and systemic description of the suspicious behaviors indicated by the sequences of APIs called under Windows. Based on function calls and control flow analysis, according to the identification of suspicious behavior, the technique implements a strategy of detection from malicious binary executables.","Prototypes,
Educational technology,
Object detection,
Magnetic heads,
Computer science education,
Systems engineering education,
Computer science,
Switching systems,
Systems engineering and theory,
Internet"
MReC4.5: C4.5 Ensemble Classification with MapReduce,"Classification is a significant technique in data mining research and applications. C4.5 is a widely used classification method, and ensemble learning adopts a parallel and distributed computing model for classification. Based on analyses of the MapReduce computing paradigm and the process of ensemble learning, we find that the parallel and distributed computing model in MapReduce is appropriate for implementing ensemble learning. This paper takes the advantages of C4.5, ensemble learning and the MapReduce computing model, and proposes a new method MReC4.5 for parallel and distributed ensemble classification. Our experimental results show that increasing the number of nodes would benefit the effectiveness of classification modeling, and serialization operations at the model level make the MReC4.5 classifier “construct once, use anywhere”.","Distributed computing,
Data mining,
Computer science,
Concurrent computing,
Training data,
Classification algorithms,
Testing,
Decision trees,
Parallel programming,
Cloud computing"
Undulatory and pedundulatory robotic locomotion via direct and retrograde body waves,"The present paper explores the effect of the mechanism-substrate frictional interface on the locomotion characteristics of robotic mechanisms employing traveling waves for propulsion. For these investigations, an extended class of undulatory robotic locomotors is considered, termed pedundulatory, which augment lateral body undulations by coordinated dorso-ventral oscillations of multiple pairs of lateral paddle-shaped appendages (parapodia). We examine how, the same robotic prototype, allows the implementation of four distinct bio-inspired undulatory and pedundulatory modes of locomotion, by modifying the motion control strategy depending on the mechanism-substrate frictional interface. These modes employ retrograde or direct body waves, either standalone (giving rise to eel-like and ochromonas-like undulatory locomotion modes, respectively), or combined with appropriately coordinated substrate contact by the parapodial appendages (giving rise to centipede-like and polychaete-like pedundulatory modes, respectively). These four modes are investigated and comparatively assessed, both in simulation and via extensive experiments on granular substrates with the Nereisbot prototype. Our results validate the identified locomotion principles and also highlight the enhanced performance and gait repertoire of pedundulatory systems, compared to purely undulatory ones.","Robot kinematics,
Motion control,
Prototypes,
Robotics and automation,
Computational modeling,
Head,
Medical robotics,
Propulsion,
Virtual prototyping,
Tail"
Using a Task Modeling Formalism in the Design of Serious Games for Emergency Medical Procedures,"Lack of standard methodologies to guide and organize game design can result in longer and less predictable game production processes. Moreover, the need for interaction among domain experts (providing the instructional content) and game developers is a peculiar aspect of serious games that makes their development more difficult. This paper focuses on the design of games for procedural training, and proposes to adopt a task modeling technique (ConcurTaskTrees, CTT [15]) in the modeling of training scenarios. In particular, we show how CTT can be used to (i) analyze and structure pedagogical content about the procedures, (ii) support and monitor procedure execution in the game. We also describe how we employed CTT in the design of a serious game for training nurses in emergency medical procedures on disabled patients.","Production,
Application software,
Predictive models,
Human computer interaction,
Computer science,
Patient monitoring,
Biomedical monitoring,
Stress,
Research and development,
Process design"
A New Data-Mining Based Approach for Network Intrusion Detection,"Nowadays, as information systems are more open to the Internet, the importance of secure networks is tremendously increased. New intelligent Intrusion Detection Systems (IDSs) which are based on sophisticated algorithms rather than current signature-base detections are in demand. In this paper, we propose a new data-mining based technique for intrusion detection using an ensemble of binary classifiers with feature selection and multiboosting simultaneously. Our model employs feature selection so that the binary classifier for each type of attack can be more accurate, which improves the detection of attacks that occur less frequently in the training data. Based on the accurate binary classifiers, our model applies a new ensemble approach which aggregates each binary classifier’s decisions for the same input and decides which class is most suitable for a given input. During this process, the potential bias of certain binary classifier could be alleviated by other binary classifiers’ decision. Our model also makes use of multiboosting for reducing both variance and bias. The experimental results show that our approach provides better performance in terms of accuracy and cost than the winner entry of the ‘Knowledge Development and Data mining’ (KDD) ’99 cup challenge. Future works will extend our analysis to a new ‘Protected Repository for the Defense of Infrastructure against Cyber Threats’ (PREDICT) dataset as well as real network data.",
Generation of function block based designs using Semantic Web technologies,"Embedded systems are used in many domains today. However, the design process of these systems is complex and time-consuming. In automation domains the complexity can be decreased by using function block based designs to specify the functionality of the system to be created. But this has to be done still manually. This paper introduces an approach to generate such function block based designs automatically from requirements. Therefore a generative approach is used, where design patterns were assembled to complete designs. The pattern description and the generation process are realized by Semantic Web technologies. The evaluation of the approach is exemplified by functional schematics of room automation designs.",
Why are software projects moving from centralized to decentralized version control systems?,"Version control systems are essential for co-ordinating work on a software project. A number of open- and closed-source projects are proposing to move, or have already moved, their source code repositories from a centralized version control system (CVCS) to a decentralized version control system (DVCS). In this paper we summarize the differences between a CVCS and a DVCS, and describe some of the rationales and perceived benefits offered by projects to justify the transition.","Centralized control,
Control systems,
Open source software,
Computer science,
Quality management,
Switches,
Software development management,
Programming,
Software quality,
Project management"
A Distributed Algorithm to Enumerate All Maximal Cliques in MapReduce,"Structure mining plays an important part in the researches in biology, physics, internet or telecommunications in recently emerging network science. As a main task in this area, the problem of maximal clique enumeration has attracted much interest and been studied in variant avenues in prior works. However, most of these works mainly rely on single chip computational capacity and have been constrained by local optimization. Thus it is an impossible mission for these methods to process terabytes datasets. In this paper, to extract maximal cliques from graphs, we propose a general enumeration process in a distributed manner on cluster system with the help of MapReduce. Graph is firstly split into small subgraphs automatically. Then a novel key-based clique enumeration algorithm is proposed based on subgraphs. We demonstrate that our algorithm has a high parallelism and a prominent performance on extremely huge graphs. Our method is implemented to fully utilize MapReduce execution mechanism and the experiments are soundly discussed as using such a powerful distributed platform. However we not only show the scalability and efficiency of the algorithm but also share some critical experience in using MapReduce computing model.",
Shape guided contour grouping with particle filters,"We propose a novel framework for contour based object detection and recognition, which we formulate as a joint contour fragment grouping and labeling problem. For a given set of contours of model shapes, we simultaneously perform selection of relevant contour fragments in edge images, grouping of the selected contour fragments, and their matching to the model contours. The inference in all these steps is performed using particle filters (PF) but with static observations. Our approach needs one example shape per class as training data. The PF framework combined with decomposition of model contour fragments to part bundles allows us to implement an intuitive search strategy for the target contour in a clutter of edge fragments. First a rough sketch of the model shape is identified, followed by fine tuning of shape details. We show that this framework yields not only accurate object detections but also localizations in real cluttered images.","Shape,
Particle filters,
Image edge detection,
Object detection,
Image recognition,
Humans,
Computer vision,
Labeling,
Training data,
Visual perception"
"Feature Extraction and Classification of EEG Signals Using Wavelet Transform, SVM and Artificial Neural Networks for Brain Computer Interfaces","Brain Computer Interface one of hopeful interface technologies between humans and machines. Electroencephalogram-based Brain Computer Interfaces have become a hot spot in the research of neural engineering, rehabilitation, and brain science. The artifacts are disturbance that can occur during the signal acquisition and that can alter the analysis of the signals themselves. Detecting artifacts produced in electroencephalography data by muscle activity, eye blinks and electrical noise is a common and important problem in electroencephalography research. In this research, we used five different methods for detecting trials containing artifacts. Finally we used two different neural networks, and support vector machine to classify features that are extracted by wavelet transform.","Feature extraction,
Electroencephalography,
Wavelet transforms,
Support vector machines,
Support vector machine classification,
Artificial neural networks,
Brain computer interfaces,
Humans,
Neural engineering,
Signal analysis"
From Reconfigurable Architectures to Self-Adaptive Autonomic Systems,"Systems on a Chip (SoC) can draw various benefits such as adaptability and efficient acceleration of compute-intensive tasks from the inclusion of reconfigurable hardware as a system component. Dynamic reconfiguration capabilities of current reconfigurable devices create an additional dimension in the temporal domain. During the design space exploration phase, overheads associated with reconfiguration and hardware/software interfacing need to be evaluated carefully in order to harvest the full potential of dynamic reconfiguration. In order to overcome the limits deriving by the increasing complexity and the associated workload to maintain such complex infrastructure, one possibility is to adopt self-adaptive and autonomic computing systems [1]. A self-adaptive and autonomic computing system is a system able to configure, heal, optimize and protect itself without the need for human intervention.","Reconfigurable architectures,
Hardware,
Acceleration,
Software performance,
Costs,
Computer architecture,
Computer science,
Artificial intelligence,
Laboratories,
Space exploration"
Minimizing Communication Cost in Distributed Multi-query Processing,"Increasing prevalence of large-scale distributed monitoring and computing environments such as sensor networks, scientific federations, Grids etc., has led to a renewed interest in the area of distributed query processing and optimization. In this paper we address a general, distributed multi-query processing problem motivated by the need to minimize the communication cost in these environments. Specifically we address the problem of optimally sharing data movement across the communication edges in a distributed communication network given a set of overlapping queries and query plans for them (specifying the operations to be executed). Most of the problem variations of our general problem can be shown to be NP-Hard by a reduction from the Steiner tree problem. However, we show that the problem can be solved optimally if the communication network is a tree, and present a novel algorithm for finding an optimal data movement plan. For general communication networks, we present efficient approximation algorithms for several variations of the problem. Finally, we present an experimental study over synthetic datasets showing both the need for exploiting the sharing of data movement and the effectiveness of our algorithms at finding such plans.","Query processing,
Communication networks,
Large-scale systems,
Approximation algorithms,
Cost function,
Publish-subscribe,
Polynomials,
Tree graphs,
Data engineering,
Computer science"
A distributed pool architecture for genetic algorithms,"The genetic algorithm (GA) paradigm is a well-known heuristic for solving many problems in science and engineering. As problem sizes increase, a natural question is how to exploit advances in distributed and parallel computing to speed up the execution of GAs. This paper proposes a new distributed architecture for GAs, based on distributed storage of the individuals in a persistent pool. Processors extract individuals from the pool in order to perform the computations and then insert the resulting individuals back into the pool. Unlike previously proposed approaches, the new approach is tailored for distributed systems in which processors are loosely coupled, failure-prone and can run at different speeds. Proof-of-concept simulation results are presented indicating that the approach can deliver improved performance due to the distribution and tolerates a large fraction of crash failures.","Genetic algorithms,
Master-slave,
Computer crashes,
Parallel processing,
Computer architecture,
Distributed computing,
Product design,
Genetic engineering,
Computational modeling,
Optimization methods"
CORP: Cooperative rateless code protocol for vehicular content dissemination,"Data dissemination in vehicular networks has been a challenge due to unpredictable network dynamics and channel unreliability. In fact, conventional approaches that rely on TCP or UDP do not perform well and cannot consistently guarantee reliable communications. A potential solution to these problems involves using erasure-correcting codes to make UDP transmissions reliable. However, it is also important to note that, due to vehicular mobility and road obstacles, connections among nodes can be as short as a few seconds. Therefore, in a communication between two nodes, the delay between node discovery and data exchange must be minimized. In this paper we present CORP, a Cooperative Rateless Protocol that exploits the reliability of the rateless coding approach while performing fast and efficient dissemination through cooperating nodes within the network. The use of rateless codes and unicast connections places minimal constraints on the delay between node discovery and data transmission, thus simplifying and rendering feasible the content reliable download even in a highly dynamic scenario. Results from simulations reveal that performance improves as more and more nodes cooperate. In sum, CORP yields important improvements in terms of speed of dissemination when compared to traditional approaches such as TCP.",Protocols
Least-Squares Contour Alignment,"The contour alignment problem, considered in this letter, is to compute the minimal distance in a least-squares sense, between two explicitly represented contours, specified by corresponding points, after arbitrary rotation, scaling, and translation of one of the contours. This is a constrained nonlinear optimization problem with respect to the translation, rotation, and scaling parameters; however, it is transformed into an equivalent linear least-squares problem by a nonlinear change of variables. Therefore, a global solution of the contour alignment problem can be computed efficiently. It is shown that a normalized minimum value of the cost function is invariant to ordering and affine transformation of the contours and can be used as a measure for the distance between the contours. A solution is proposed to the problem of finding a point correspondence between the contours.",
Predicting NDUM Student's Academic Performance Using Data Mining Techniques,"The ability to predict the students’ academic performance is very important in institution educational system. Recently some researchers have been proposed data mining techniques for higher education. In this paper, we compare two data mining techniques which are: Artificial Neural Network (ANN) and the combination of clustering and decision tree classification techniques for predicting and classifying students’ academic performance. The data set used in this research is the student data of Computer Science Department, Faculty of Science and Defence Technology, National Defence University of Malaysia (NDUM).","Data mining,
Computer science,
Artificial neural networks,
Educational technology,
Decision trees,
Classification tree analysis,
Statistics,
Instruction sets,
Computer science education,
Application software"
Visual analysis of graphs with multiple connected components,"In this paper, we present a system for the interactive visualization and exploration of graphs with many weakly connected components. The visualization of large graphs has recently received much research attention. However, specific systems for visual analysis of graph data sets consisting of many components are rare. In our approach, we rely on graph clustering using an extensive set of topology descriptors. Specifically, we use the self-organizing-map algorithm in conjunction with a user-adaptable combination of graph features for clustering of graphs. It offers insight into the overall structure of the data set. The clustering output is presented in a grid containing clusters of the connected components of the input graph. Interactive feature selection and task-tailored data views allow the exploration of the whole graph space. The system provides also tools for assessment and display of cluster quality. We demonstrate the usefulness of our system by application to a shareholder network analysis problem based on a large real-world data set. While so far our approach is applied to weighted directed graphs only, it can be used for various graph types.",
Weighted nonnegative matrix factorization,"Nonnegative matrix factorization (NMF) is a widely-used method for low-rank approximation (LRA) of a nonnegative matrix (matrix with only nonnegative entries), where nonnegativity constraints are imposed on factor matrices in the decomposition. A large body of past work on NMF has focused on the case where the data matrix is complete. In practice, however, we often encounter with an incomplete data matrix where some entries are missing (e.g., a user-rating matrix). Weighted low-rank approximation (WLRA) has been studied to handle incomplete data matrix. However, there is only few work on weighted nonnegative matrix factorization (WNMF) that is WLRA with nonnegativity constraints. Existing WNMF methods are limited to a direct extension of NMF multiplicative updates, which suffer from slow convergence while the implementation is easy. In this paper we develop relatively fast and scalable algorithms for WNMF, borrowed from well-studied optimization techniques: (1) alternating nonnegative least squares; (2) generalized expectation maximization. Numerical experiments on MovieLens and Netflix prize datasets confirm the useful behavior of our methods, in a task of collaborative prediction.","Matrix decomposition,
Least squares approximation,
Collaboration,
Convergence,
Least squares methods,
Data analysis,
Computer science,
Singular value decomposition,
Feature extraction,
Spectrogram"
Spring-Clutch: A safe torque limiter based on a spring and CAM mechanism with the ability to reinitialize its position,"Service robots are anticipated to be used in unstructured areas such as homes, hospitals, and public areas in the near future. However, safety issues need to be addressed before this can occur. In particular, robot manipulators that handle objects by physical contact run the risk of colliding with people or objects. Thus, it is important to prevent collisions that could injure people and damage robot manipulators. In this study, a safe joint mechanism is developed to ensure the safe use of a manipulator. This mechanism, termed `Spring-Clutch,' is a simple passive mechanism that consists of a coil spring and a CAM mechanism. When a torque is applied that is less than a threshold value, Spring-Clutch functions as a rigid joint between the input and the output. However, when an applied torque exceeds the threshold, angular displacement occurs between the input and output to reduce the collision force. If the applied torque is removed, Spring-Clutch immediately returns to its nominal position without the need for additional operations. This paper describes the design principles and performance of Spring-Clutch, and discusses the possibility of its practical use as a joint mechanism for safe manipulation.","Torque,
Springs,
Computer aided manufacturing,
CADCAM,
Manipulators,
Control systems,
Force control,
Intelligent robots,
Safety,
Coils"
Dynamic linking and loading in networked embedded systems,"We present a holistic dynamic linking and loading mechanism in networked embedded systems. Our design and implementation are guided by four requirements, which are to provide (i) minimal code size (ii) efficient execution and loading speed (iii) portable design (iv) isolated kernel/application development. First, we develop a tool to minimize the standard ELF format via many techniques in order to reduce the code dissemination cost. Second, we employ the techniques of pre-relocating and pre-linking (to kernel functions) to reduce the run-time linking overhead, thus improving the loading speed. Third, based on relocatable ELF and the modular design of the dynamic linker and loader, our approach can be easily ported to different platforms. Fourth, by maintaining a kernel jump table, we provide a clean isolation between kernel and application development. We have implemented the dynamic linking and loading mechanism on SenSpire OS, a micro sensor node operating system. The evaluation results show that our design and implementation meet our design goals: the code size of our SELF format is only 15%–30% of that of standard ELF, 38%–83% of that of CELF, a compact ELF format for the Contiki operating system; the loading speed improvement varies from 40%–50% compared to the standard mechanism; our design is portable to both MicaZ and TelosB motes, and we allow updating both application modules and kernel services in isolation without prior knowledge about the whole system information.","Joining processes,
Embedded system,
Kernel,
Ground penetrating radar,
Geophysical measurement techniques,
Code standards,
Operating systems,
Standards development,
Costs,
Runtime"
Automated Versioning in OSGi: A Mechanism for Component Software Consistency Guarantee,"Consistency of component software is a crucial condition required for correct program execution. The existing consistency controls of OSGi at build time or in runtime cannot prevent type mismatch failures caused by independent client and server bundle development. This paper describes our solution to this problem using automated versioning of components. Version identifiers are generated from results of subtype-based comparison of component representations, thus achieving a consistent and formally backed interpretation of the version numbering scheme. The implementation of the approach allows its integration into standard OSGi bundle development and build cycle.",
A Bayesian Clustering Method for Tracking Neural Signals Over Successive Intervals,"This paper introduces a new, unsupervised method for sorting and tracking the action potentials of individual neurons in multiunit extracellular recordings. Presuming the data are divided into short, sequential recording intervals, the core of our strategy relies upon an extension of a traditional mixture model approach that incorporates clustering results from the preceding interval in a Bayesian manner, while still allowing for signal nonstationarity and changing numbers of recorded neurons. As a natural byproduct of the sorting method, current and prior signal clusters can be matched over time in order to track persisting neurons. We also develop techniques to use prior data to appropriately seed the clustering algorithm and select the model class. We present results in a principal components space; however, the algorithm may be applied in any feature space where the distribution of a neuron's spikes may be modeled as Gaussian. Applications of this signal classification method to recordings from macaque parietal cortex show that it provides significantly more consistent clustering and tracking results than traditional methods based on expectation-maximization optimization of mixture models. This consistent tracking ability is crucial for intended applications of the method.","Bayesian methods,
Clustering methods,
Neurons,
Sorting,
Extracellular,
Clustering algorithms,
Electrodes,
Brain modeling,
Signal generators,
Principal component analysis"
Fuzzy approach for the evaluation of trust and reputation of services,"A service-oriented environment has special characteristics that distinguishes it from other computing environments: (i) the environment is dynamic; (ii) the number of service providers is unbounded; (iii) services are owned by various stakeholders with different aims and objectives; (iv) there is no central authority that can control all the service providers and consumers; (v) service providers and consumers are self-interested. Given these special characteristics, the evaluation of trust and reputation is very important in such an open, dynamic and distributed environment. Therefore, a fuzzy-based trust and reputation approach using three trust sources was developed. Simulating the real world in which deception happens, an evaluation is performed showing the usefulness and robustness of the fuzzy approach by a comparison with a weighted approach.","Quality of service,
Instruments,
Centralized control,
Security,
Protection,
Law,
Legal factors,
Fuzzy control,
Computer science,
Performance evaluation"
Exploiting memory customization in FPGA for 3D stencil computations,"3D stencil computations are compute-intensive kernels often appearing in high-performance scientific and engineering applications. The key to efficiency in these memory-bound kernels is full exploitation of data reuse. This paper explores the design aspects for 3D-Stencil implementations that maximize the reuse of all input data on a FPGA architecture. The work focuses on the architectural design of 3D stencils with the form n × (n + 1) × n, where n = {2, 4, 6, 8, ...}. The performance of the architecture is evaluated using two design approaches, ¿Multi-Volume¿ and ¿Single-Volume¿. When n = 8, the designs achieve a sustained throughput of 55.5 GFLOPS in the ¿Single-Volume¿ approach and 103 GFLOPS in the ¿Multi-Volume¿ design approach in a 100-200 MHz multi-rate implementation on a Virtex-4 LX200 FPGA. This corresponds to a stencil data delivery of 1500 bytes/cycle and 2800 bytes/cycle respectively. The implementation is analyzed and compared to two CPU cache approaches and to the statically scheduled local stores on the IBM PowerXCell 8i. The FPGA approaches designed here achieve much higher bandwidth despite the FPGA device being the least recent of the chips considered. These numbers show how a custom memory organization can provide large data throughput when implementing 3D stencil kernels.","Field programmable gate arrays,
Nearest neighbor searches,
Kernel,
Bandwidth,
Finite difference methods,
Finite impulse response filter,
Computer applications,
Throughput,
Time domain analysis,
Hardware"
An empirical study on the comprehension of stereotyped UML class diagram layouts,"An empirical study is presented that investigates how stereotype based layouts impact the comprehension of UML class diagrams. This work replicates a previous study using eye-tracking equipment but uses online questionnaires instead. Subjects were given two types of tasks: one addressing UML syntax and the other addressing software design. Three different layout strategies are compared. Along with general aesthetics, the layouts are primarily organized by class stereotypes of control, boundary, and entity. A confidence value for each question was collected from the subjects to help validate the categorization of subjects. Results of the study are compared and contrasted to the eye-tracking study done with the same tasks and layouts. Results show a significant improvement in performance in both types of tasks with the multi-cluster stereotyped layouts.","Unified modeling language,
Software design,
Computer science,
Best practices,
Software maintenance,
Control system synthesis,
Color"
Accurate 3D ground plane estimation from a single image,"Accurate localization of landmarks in the vicinity of a robot is a first step towards solving the SLAM problem. In this work, we propose algorithms to accurately estimate the 3D location of the landmarks from the robot only from a single image taken from its on board camera. Our approach differs from previous efforts in this domain in that it first reconstructs accurately the 3D environment from a single image, then it defines a coordinate system over the environment, and later it performs the desired localization with respect to this coordinate system using the environment's features. The ground plane from the given image is accurately estimated and this precedes segmentation of the image into ground and vertical regions. A Markov Random Field (MRF) based 3D reconstruction is performed to build an approximate depth map of the given image. This map is robust against texture variations due to shadows, terrain differences, etc. A texture segmentation algorithm is also applied to determine the ground plane accurately. Once the ground plane is estimated, we use the respective camera's intrinsic and extrinsic calibration information to calculate accurate 3D information about the features in the scene.","Robot kinematics,
Cameras,
Image reconstruction,
Image segmentation,
Simultaneous localization and mapping,
Robot vision systems,
Markov random fields,
Robustness,
Calibration,
Layout"
Adding SVC Spatial Scalability to Existing H.264/AVC Video,"This paper describes an innovative algorithm for adding SVC Spatial Scalability to all existing non-scalable H.264/AVC video streams. The algorithmic system builds on a full-decode-full-encode method of conversion and employs reuse of available data by an efficient downscaling of video information for different layers, thus reducing the complexity of the algorithm by manifold whilst maintaining a high coding efficiency. The complexity is further reduced by bypassing of various time consuming processes of the encoding process by the use of sharing and reuse of video information coming from original bitstream. Extensive analysis has been done to evaluate the gains in the coding efficiency and complexity of the algorithm using the full-decode-full-encode approach as a benchmark. The ultimate reduction in complexity is at least 60% from full-decode-full-encode while maintaining the output video quality. The modular nature of the algorithm is designed to be easily adaptable to other scalable video standards as well.","Static VAr compensators,
Scalability,
Automatic voltage control,
Streaming media,
High definition video,
Bandwidth,
Decoding,
Video sharing,
HDTV,
Video coding"
C2A: Controlled conservative advancement for continuous collision detection of polygonal models,"We present a simple and fast algorithm to perform continuous collision detection between polygonal models undergoing rigid motion for interactive applications. Our approach can handle all triangulated models and makes no assumption about the underlying geometry and topology. The algorithm uses the notion of conservative advancement (CA), originally developed for convex polytopes [1], [2]. We extend this formulation to general models using swept sphere volume hierarchy and present a compact formulation to compute the motion bounds along with a novel controlling scheme. We have implemented the algorithm and highlight its performance on various benchmarks. In practice, our algorithm can perform continuous collision queries in few milli-seconds on models composed of tens of thousands of triangles.","Charge coupled devices,
Motion planning,
Motion detection,
Solid modeling,
Detection algorithms,
Motion control,
Geometry,
Topology,
Robot motion,
Layout"
Second order difference based detection and directional weighted median filter for removal of random valued impulsive noise,"The proposed approach of removal of random valued impulsive noise from images works in two phases. The first phase detects contaminated pixels and the second phase filters only those pixels keeping others intact. The detection scheme utilizes second order difference of pixels in a test window and the filtering scheme is a variation median filter based on the edge information. The proposed scheme is simulated extensively on standard images and comparison with existing schemes reveal that our scheme outperforms them in terms of Peak Signal to Noise Ratio (PSNR), number of false detection and miss detection. The proposed scheme is also good at preserving finer details. Further, the computational complexity and number of iterations needed by the proposed scheme is less than the existing counterparts.","Nonlinear filters,
Pixel,
Information filtering,
Information filters,
Adaptive filters,
Image edge detection,
PSNR,
Detectors,
Phase detection,
Phase noise"
Advanced Flooding Attack on a SIP Server,"Voice over IP is gaining more popularity in today's communications. The Session Initiation Protocol (SIP) is the most popular VoIP signalling protocol is vulnerable to many kinds of attacks. Among all these attack, flood-based denial of service attacks have been identified as the biggest threat to SIP. Even though a great deal of research has been conducted into mitigating denial of service attacks, only a small proportion have been specific to SIP. This paper examines how denial of service attacks affect the performance of a SIP-based system, and proposes an Improved Security-Enhanced SIP System (ISESS) to mitigate such attacks. Experimental results are provided to demonstrate the effectiveness of ISESS. The experimental results show that with ISESS, during a flood-based denial of service attack, the performance of the system can be improved substantially.","Floods,
Protocols,
Computer crime,
Authentication,
Internet telephony,
IP networks,
Computer science,
Software engineering,
Security,
Registers"
Decentralized Detection of Group Formations from Wearable Acceleration Sensors,"We propose an approach for detecting collectivebehavior patterns using body-worn sensors in a decentralized,online manner. To reduce complexity, we introduce a set ofcollective behavior primitives as building blocks, adequate fordetection by sensing and communications means. With this, wepresent a generic distributed signal processing procedure suitablefor a decentralized detection of these behavior blocks. In anexperiment, we verify relevant elements of this procedure byevaluating towards which extend body-worn acceleration sensorscan be used to infer whether a set of people is walking togetheras a coherent group. Early results discussed demonstrate thefeasibility of our approach, leading to an outline of next stepsfor reliable detection of collective behavior patterns in real-life.","Acceleration,
Wearable sensors,
Mobile communication,
Wearable computers,
Context,
Mobile handsets,
Laboratories,
Telephony,
Signal processing,
Legged locomotion"
Combining I/O operations for multiple array variables in parallel netCDF,"Parallel netCDF (PnetCDF) is a popular library used in many scientific applications to store scientific datasets. It provides high-performance parallel I/O while maintaining file-format compatibility with Unidata's netCDF. Array variables comprise the bulk of the data in a netCDF dataset, and for accesses to large regions of single array variables, PnetCDF attains very high performance. However, the current PnetCDF interface only allows access to one array variable per call. If an application instead accesses a large number of small-sized array variables, this interface limitation can cause significant performance degradation, because high end network and storage systems deliver much higher performance with larger request sizes. Moreover, the record variables data is stored interleaved by record, and the contiguity information is lost, so the existing MPI-IO collective I/O optimization can not help. This paper presents a new mechanism for PnetCDF to combine multiple I/O operations for better I/O performance. This mechanism can be used in a new function that takes arguments for reading/writing multiple array variables, allowing application programmers to explicitly access multiple array variables in a single call. It can also be used in the implementation of asynchronous I/O functions, so that the combination is carried out implicitly, without changes to the application. Our performance results demonstrate significant improvement using well-known application benchmarks.",
Double turbo equalization of continuous phase modulation with frequency domain processing,"In this paper, a doubly-iterative linear receiver, equipped with a soft-information aided frequency domain minimum mean-squared error (MMSE) equalizer, is proposed for the combined equalization and decoding of coded continuous phase modulation (CPM) signals over long multipath fading channels. In the proposed receiver architecture, the front-end frequency domain equalizer (FDE) is followed by the soft-input, soft-output (SISO) CPM demodulator and channel decoder modules. The receiver employs double turbo processing by performing back-end demodulation/decoding iterations per each equalization iteration to improve the a priori information for the front-end FDE. As presented by the computational complexity analysis and simulations, this process provides not only a significant reduction in the overall computational complexity, but also a performance improvement over the previously proposed iterative and noniterative MMSE receivers.","Continuous phase modulation,
Frequency domain analysis,
Iterative decoding,
Equalizers,
Demodulation,
Computational complexity,
Fading,
Performance analysis,
Computational modeling,
Analytical models"
A novel approach using transformation techniques and decision tree algorithm on images for performing Digital Watermarking,"Digital Watermarking is an emerging copyright protection technology. The paper presents a new robust watermarking technique based on combining the power of transform domain technique, the Discrete Cosine Transform (DCT) and the data mining technique such as Decision Tree Induction (ID3). The paper focuses on a technique through which the notion of decision tree can be applied on transformed vectors to build the decision tree. We train the image blocks for deriving the classification tree. The resulting decision tree provides decision making rules to identify good quality image blocks for insertion of watermark. The implementation results have shown that the algorithm has an acceptable robustness against the JPEG compression and addition of noise.",
High improvement of speaker identification and verification by combining MFCC and phase information,"In conventional speaker recognition methods based on MFCC, phase information has been ignored. We proposed a method that integrated the phase information with MFCC on a speaker identification method, and a preliminary experiment was performed. In this paper, we propose a new modified feature parameter (that is, coordidates on an unit circle) obtained from the original phase information, and evaluated it by using speech database consisting of normal, fast and slow speaking modes. The speaker identification experiments were performed using NTT database which consists of sentences uttered by 35 Japanese speakers (22 males and 13 females) on five sessions over ten months. Each speaker uttered only 5 training utterances at a normal speaking mode (about 20 seconds in total). The proposed new phase information was more robust than the original phase information for all speaking modes. By integrating the new phase information with the MFCC, the speaker identification error rate was remarkably reduced for normal, fast and slow speaking rates in comparison with a standard MFCC-based method. In this paper, speaker verification experiments were also evaluated using the phase information. The experiments show that the phase information is also very useful for the speaker verification.","Mel frequency cepstral coefficient,
Hidden Markov models,
Speaker recognition,
Spatial databases,
Speech analysis,
Systems engineering and theory,
Robustness,
Error analysis,
Feature extraction,
Data mining"
Camera traps as sensor networks for monitoring animal communities,"Studying animal movement and distribution is of critical importance to addressing environmental challenges including invasive species, infectious diseases, climate and land-use change. Motion sensitive camera traps offer a visual sensor to record the presence of a species at a location, recording their movement in the Eulerian sense. Modern digital camera traps that record video present new analytical opportunities, but also new data management challenges. This paper describes our experience with a year-long terrestrial animal monitoring system at Barro Colorado Island, Panama. The data gathered from our camera network shows the spatio-temporal dynamics of terrestrial bird and mammal activity at the site - data relevant to immediate science questions, and long-term conservation issues. We believe that the experience gained and lessons learned during our year long deployment and testing of the camera traps are applicable to broader sensor network applications and are valuable for the advancement of the sensor network research. We suggest that the continued development of these hardware, software, and analytical tools, in concert, offer an exciting sensor-network solution to monitoring of animal populations which could realistically scale over larger areas and time spans.","Monitoring,
Animals,
Diseases,
Video recording,
Digital cameras,
Birds,
Testing,
Application software,
Hardware,
Software tools"
Interactive Multimedia for Adaptive Online Education,"Interactive multimedia content can help improve learning performance by enhancing user satisfaction and engagement. Multimedia content also can help improve concept representation, which is not possible in conventional multiple-choice and fill-in-the-blank formats. In this article, we present a broader view of multimedia education, focusing on future applications. Our vision is to provide publicly accessible education anywhere, at anytime, and to anyone. To realize this vision, we propose the Computer Reinforced Online Multimedia Education (Crome) framework, which integrates the main components of education, including learning, teaching, and testing, as well as adaptive testing and student modeling.","Computer science education,
Educational institutions,
System testing,
Electronic learning,
Cultural differences,
Research and development,
Automatic testing,
North America,
Multimedia systems,
Pervasive computing"
C and M: A New Network Coding Scheme for Wireless Networks,"This paper proposes C&M, a novel network coding scheme to deal with both inter-flow and intra-flow traffic in wireless networks. Prior work on wireless network coding design belongs to either inter-flow network coding or intra-flow network coding. C&M attempts to combine advantages of both network coding approaches to develop a more efficient network coding scheme. Based on COPE, C&M allows each node to make use of intra-flow network coding such as MORE to improve the transmission reliability in a lossy environment, consequently obtaining higher throughput. Moreover, we propose the multiple-path transmitting scheme to increase the throughput of wireless networks with low link delivery probability further. Finally, we provide our view on how to integrate network coding into network stack and discuss some potential benefits of C&M.","Network coding,
Wireless networks,
Throughput,
Telecommunication traffic,
Information security,
Decoding,
Computer security,
Laboratories,
Computer networks,
Computer science education"
Competitive optimization of cognitive radio MIMO systems via game theory,"The concept of cognitive radio (CR) has recently received great attention from the researchers' community as a promising paradigm to achieve efficient use of the frequency resource by allowing the coexistence of licensed (primary) and unlicensed (secondary) users in the same bandwidth. In this paper, we propose a distributed approach based on game theory to design cognitive MIMO transceiver in hierarchical CR networks, where primary users establish null and/or soft shaping constraints on the transmit covariance matrix of secondary users, so that the interference generated by secondary users be confined within the interference-temperature limits. We formulate the resource allocation problem among secondary users as a strategic noncooperative game, where each transmit/receive pair competes against the others to maximize the information rate over his own MIMO channel, under transmit power and/or null/soft shaping constraints. We provide a unified set of conditions that guarantee the uniqueness and global asymptotic stability of the Nash equilibrium of all the proposed games through totally distributed and asynchronous algorithms. Interestingly, the proposed algorithms overcome the main drawback of classical waterfilling based algorithms-the violation of the interference-temperature limits-and they have many of the desired features required for cognitive radio applications, such as low-complexity, distributed nature, robustness against missing or outdated updates of the users, and fast convergence behavior.","Cognitive radio,
MIMO,
Game theory,
Chromium,
Interference constraints,
Frequency,
Bandwidth,
Transceivers,
Constraint theory,
Covariance matrix"
Approximating Minimum Cost Connectivity Problems via Uncrossable Bifamilies and Spider-Cover Decompositions,"We give approximation algorithms for the {\sf \footnotesize Gene\-ralized Steiner Network} ({\sf \small GSN}) problem. The input consists of a graph
G=(V,E)
with edge/node costs, a node subset
S⊆V
, and connectivity requirements
{r(s,t):s,t∈T⊆V}
.The goal is to find a minimum cost subgraph
H
that for all
s,t∈T
contains
r(s,t)
pairwise edge-disjoint
st
-paths so that no two of them have a node in
S−{s,t}
in common. Three extensively studied particular cases are: {\sf \footnotesize Edge-GSN} (
S=∅
), {\sf \footnotesize Node-GSN} (
S=V
), and {\sf \footnotesize Element-GSN} (
r(s,t)=0
whenever
s∈S
or
t∈S
).Let
k=
max
s,t∈T
r(s,t)
.In {\sf \footnotesize Rooted GSN} there is
s∈T
so that
r(u,t)=0
for all
u≠s
, and in the {\sf \footnotesize Subset {\normalsize
k
}-Connected Subgraph} problem
r(s,t)=k
for all
s,t∈T
. For edge costs, our ratios are
O(
k
2
)
for {\sf \footnotesize Rooted GSN} and
O(
k
2
logk)
for {\sf \footnotesize Subset {\normalsize
k
}-Connected Subgraph}.This improves the previous ratio
O(
k
2
logn)
and settles the approximability of these problems to a constant for bounded
k
. For node-cost, our ratios are:
Unknown environment 'itemize'",
Hybrid attitude estimation for laparoscopic surgical tools: A preliminary study,"Laparoscopic surgery poses a challenging problem for a real-time navigation system: how to keep tracking the surgical tools inside the human body intraoperatively. This paper proposes a sensor fusion method for a hybrid tracking system that incorporates a miniature inertial measurement unit and an electromagnetic navigation system, in order to obtain continuous orientation information, even in the presence of metal objects. The sensor fusion algorithm employs an extended Kalman filter to integrate the data from the two sensor streams, based on a quaternion formulation of the system dynamics. The preliminary experimental results show that the integration of low-cost inertial measurement is able to compensate the distortion of EM tracking.","Minimally invasive surgery,
Optical distortion,
Optical sensors,
Biomedical optical imaging,
Sensor fusion,
Distortion measurement,
Optical filters,
Navigation,
Humans,
Coils"
Mining high average-utility itemsets,"The average utility measure is adopted in this paper to reveal a better utility effect of combining several items than the original utility measure. A mining algorithm is then proposed to efficiently find the high average-utility itemsets. It uses the summation of the maximal utility among the items in each transaction including the target itemset as the upper bounds to overestimate the actual average utilities of the itemset and processes it in two phases. As expected, the mined high average-utility itemsets in the proposed way will be fewer than the high utility itemset under the same threshold. Experiments results also show the performance of the proposed algorithm.","Data mining,
Itemsets,
Upper bound,
Electric variables measurement,
Length measurement,
Cybernetics,
USA Councils,
Computer science,
Information management,
Association rules"
Episodic sampling: Towards energy-efficient patient monitoring with wearable sensors,"Energy efficiency presents a critical design challenge in wireless, wearable sensor technology, mainly because of the associated diagnostic objectives required in each monitoring application. In order to maximize the operating lifetime during real-life monitoring and maintain sufficient classification accuracy, the wearable sensors require hardware support that allows dynamic power control on the sensors and wireless interfaces as well as monitoring algorithms to control these components intelligently. This paper introduces a context-aware sensing technique known as episodic sampling – a method of performing context classification only at specific time instances. Based on Additive-Increase/Multiplicative-Decrease (AIMD), episodic sampling demonstrates an energy reduction of 85 percent with a loss of only 5 percent in classification accuracy in our experiment.","Sampling methods,
Energy efficiency,
Patient monitoring,
Wearable sensors,
Wireless sensor networks,
Intelligent sensors,
Hardware,
Power control,
Intelligent control,
Competitive intelligence"
Extracting Multi-facet Community Structure from Bipartite Networks,"Bipartite networks can represent various kinds of structures, dynamics, and interaction patterns found in social activities. M. E. J. Newman proposed a measure by which you can quantitatively evaluate the quality of network division, but his work is only applicable to uniform networks. This article extends his work and proposes a new modularity measure that can be applied to bipartite networks as well. Unlike the biparitite modularity measures previously proposed, the new measure acknowledges the fact that each individual in the society has more than just one aspect, and can thus be used to extract multi-faceted community structures from bipartite networks. The mathematical properties of the proposal is examined and compared with previous work. Empirical evaluation is conducted by using a data set synthesized from an artificial model and a real-life data set found in the field of ethnography.","Social network services,
Data mining,
Computer networks,
Uniform resource locators,
Clustering algorithms,
Subscriptions,
Proposals,
Network synthesis,
History,
Sociology"
Semantic-Rich Markov Models for Web Prefetching,"Domain knowledge for web applications is currently being made available as domain ontology with the advent of the semantic web, in which semantics govern relationships among objects of interest (e. g., commercial items to be purchased in an e-Commerce web site). Our earlier work proposed to integrate semantic information into all phases of the web usage mining process, for an intelligent semantics-aware web usage mining framework. There are ways to integrate semantic information into Markov models used in the third phase for next page request prediction. Semantic information is combined with the transition probability matrix of a Markov model. This way, it provides a low order Markov model with intelligent accurate predictions and less complexity than higher order models, also solving the problem of contradicting prediction. This paper proposes to use semantic information to prune states in Selective Markov models SMM, semantic information can lead to context-aware higher order Markov models with about 16% less space complexity.","Prefetching,
Web pages,
Predictive models,
Ontologies,
Data mining,
Context modeling,
Web sites,
Markov processes,
Conferences,
Computer science"
Computer-Assisted Preoperative Planning for Reduction of Proximal Femoral Fracture Using 3-D-CT Data,"This paper describes procedures for repositioning calculations of fractured bone fragments using 3-D-computed tomography (CT), aimed at preoperative planning for computer-guided fracture reduction of the proximal femur. Fracture boundaries of the bone fragments, as ldquofracture lines (FLs),rdquo and the mirror-transformed contralateral femur shape extracted from 3-D-CT were used for repositioning of the fragments. We first describe a method for extracting FLs based on 3-D curvature analysis and then formulate repositioning methods based on registration of bone fragments using the following three constraints: 1) contralateral (CL) femur shape; 2) FLs; and 3) both CL femur shape and fracture lines, as ldquoboth constraintsrdquo. We performed experiments using CT datasets from five simulated and four real patients with proximal femoral fracture. We evaluated the rotation error in reposition calculations and the contact ratio between repositioned fragment boundaries, which are crucial for the recovery of proper functional axes and bone adhesion of fragments, respectively. Experimental results showed that good accuracy and stability were attainable when registration using both constraints was performed after registration using the fracture-line constraint. On average, 6.0deg plusmn0.8deg in rotation error and 89% plusmn 3% in contact ratio were obtained without providing precise initial values.","Bones,
Shape,
Orthopedic surgery,
Computed tomography,
Information science,
Biomedical imaging,
Robot sensing systems,
Robotics and automation,
Adhesives,
Stability"
Concept Analysis for Class Cohesion,"A concept lattice based approach for analysis of class cohesion is presented. The approach facilitates rapid identification of less cohesive classes. It also helps identify less cohesive methods, attributes and classes in one go. Further, the approach guides refactorings such as extract class, move method, localize attributes and remove unused attributes.The effectiveness of the technique is demonstrated through examples.",
Hybrid Routing Protocol for Wireless Mesh Network,IEEE 802.11s defines a new mesh data frame format and an extensibility framework for routing. The default routing protocol Hybrid Wireless Mesh Protocol (HWMP) is described. HWMP is based on Ad hoc On-demand Distance Vector Routing (AODV) and has a configurable extension for proactive routing. It uses MAC address with layer 2 routing and uses Radio-Aware as routing metric. We implemented HWMP on OPNET and evaluated its performance. The experimental results showed that HWMP has the lower average latency and the higher data transmission throughput compared with AODV.,
Detecting motion from noisy scenes using Genetic Programming,"A machine learning approach is presented in this study to automatically construct motion detection programs. These programs are generated by Genetic Programming (GP), an evolutionary algorithm. They detect motion of interest from noisy data when there is no prior knowledge of the noise. Programs can also be trained with noisy data to handle noise of higher levels. Furthermore, these auto-generated programs can handle unseen variations in the scene such as different weather conditions and even camera movements.","Motion detection,
Layout,
Genetic programming,
Detectors,
Vehicle detection,
Radar detection,
Phase detection,
Computer science,
Information technology,
Machine learning"
Directed Test Suite Augmentation,"As software evolves, engineers use regression testing to evaluate its fitness for release. Such testing typically begins with existing test cases, and many techniques have been proposed for reusing these cost-effectively. After reusing test cases, however, it is also important to consider code or behavior that has not been exercised by existing test cases and generate new test cases to validate these. This process is known as test suite augmentation. In this paper we present a directed test suite augmentation technique, that utilizes results from reuse of existing test cases together with an incremental concolic testing algorithm to augment test suites so that they are coverage-adequate for a modified program. We present results of an empirical study examining the effectiveness of our approach.","Software testing,
System testing,
Flow graphs,
Software engineering,
Computer science,
Lead,
Computer vision,
Fault detection,
Costs"
A Semantic Imitation Model of Social Tag Choices,We describe a semantic imitation model of social tagging that integrates formal representations of semantics and a stochastic tag choice process to explain and predict emergent behavioral patterns. The model adopts a probabilistic topic model to separately represent external word-topic and internal word-concept relations. These representations are coupled with a tag-based topic inference process that predicts how existing tags may influence the semantic interpretation of a document. The inferred topics influence the choice of tags assigned to a document through a random utility model of tag choices. We show that the model is successful in explaining the stability in tag proportions across time and power-law frequency-rank distributions of tag co-occurrences for semantically general and narrow tags. The model also generates novel predictions on how emergent behavioral patterns may change when users with different domain expertise interact with a social tagging system. The model demonstrates the weaknesses of single-level analyses and highlights the importance of adopting a multi-level modeling approach to explain online social behavior.,"Tagging,
Predictive models,
Power system modeling,
Stochastic processes,
Stability,
Computational modeling,
Cognitive science,
Human factors,
Frequency,
Dictionaries"
Design and FPGA implementation of radix-10 algorithm for square root with limited precision primitives,"We present a radix-10 fixed-point digit-recurrence algorithm for square root using limited-precision multipliers, adders, and table-lookups. The algorithm, except in the initialization steps, uses the digit-recurrence algorithm for division with limited-precision primitives. We discuss the proposed square root algorithm, a design, and its FPGA implementation on a Xilinx Virtex-5 FPGA. We present the cost and delay characteristics for precisions of 7 (single-precision), 8, 14 (double-precision), 16, 24, and 32 decimal digits. The costs range from 720 to 2263 LUTs with maximum clock frequencies around 53MHz, and latencies ranging from 133 to 597 ns (with unoptimized routing delays). The proposed scheme uses short (2-3 digit-wide) operators which leads to compact modules, and may have an advantage at the layout level as well as in power optimization. The proposed approach is general and can be adapted to other higher radix square root implementations. Moreover, a combined scheme for division and square root can be efficiently implemented.","Algorithm design and analysis,
Field programmable gate arrays,
Delay,
Costs,
Table lookup,
Computer science,
Error correction,
Convolution,
Clocks,
Frequency"
Compressed sensing with probabilistic measurements: A group testing solution,"Detection of defective members of large populations has been widely studied in the statistics community under the name “group testing”, a problem which dates back to World War II when it was suggested for syphilis screening. There, the main interest is to identify a small number of infected people among a large population using collective samples. In viral epidemics, one way to acquire collective samples is by sending agents inside the population. While in classical group testing, it is assumed that the sampling procedure is fully known to the reconstruction algorithm, in this work we assume that the decoder possesses only partial knowledge about the sampling process. This assumption is justified by observing the fact that in a viral sickness, there is a chance that an agent remains healthy despite having contact with an infected person. Therefore, the reconstruction method has to cope with two different types of uncertainty; namely, identification of the infected populapteioopnle and the partially unknown sampling procedure. In this work, by using a natural probabilistic model for “viral infections”, we design non-adaptive sampling procedures that allow successful identification of the infected population with overwhelming probability 1 − o(1). We propose both probabilistic and explicit design procedures that require a “small” number of agents to single out the infected individuals. More precisely, for a contamination probability p, the number of agents required by the probabilistic and explicit designs for identification of up to k infected members is bounded by m = O(k2(log n)/p2) and m = O(k2 (log2 n)/p2), respectively. In both cases, a simple decoder is able to successfully identify the infected population in time O(mn).","Compressed sensing,
Testing,
Sampling methods,
Decoding,
Diseases,
Reconstruction algorithms,
Cellular phones,
Military computing,
Statistical analysis,
Uncertainty"
Bit Encryption Is Complete,"Under CPA and CCA1 attacks, a secure bit encryption scheme can be applied bit-by-bit to construct a secure many-bit encryption scheme. The same construction fails, however, under a CCA2 attack. In fact, since the notion of CCA2 security was introduced by Rackoff and Simon~\cite{RackoffSi92}, it has been an open question to determine whether single bit CCA2 secure encryption implies the existence of many-bit CCA2 security. We positively resolve this long-standing question and establish that bit encryption is complete for CPA, CCA1, and CCA2 notions. Our construction is black-box, and thus requires novel techniques to avoid known impossibility results concerning trapdoor predicates~\cite{GMR}. To the best of our knowledge, our work is also the first example of a non-shielding reduction (introduced in~\cite{GMM07}) in the standard (i.e., not random-oracle) model.","Security,
Computer science,
Informatics,
Public key,
Public key cryptography,
Inspection,
Decoding"
Event Recognition in Sensor Networks by Means of Grammatical Inference,"Modern military and civilian surveillance applications should provide end users with the high level representation of events observed by sensors rather than with the raw data measurements. Hence, there is a need for a system that can infer higher level meaning from collected sensor data. We demonstrate that probabilistic context free grammars (PCFGs) can be used as a basis for such a system. To recognize events from raw sensor network measurements, we use a PCFG inference method based on Stolcke (1994) and Chen(1996). We present a fast algorithm for deriving a concise probabilistic context free grammar from the given observational data. The algorithm uses an evaluation metric based on Bayesian formula for maximizing grammar a posteriori probability given the training data. We also present a real-world scenario of monitoring a parking lot and the simulation based on this scenario. We described the use of PCFGs to recognize events in the results of such a simulation. We finally demonstrate the deployment details of such an event recognition system.","Wireless sensor networks,
Inference algorithms,
Sensor systems,
Training data,
Monitoring,
Production,
Communications Society,
Computer science,
Pervasive computing,
Military computing"
Convex hull generation methods for polytopic representations of LPV models,"The paper focuses on the control design of LPV and qLPV models via TS fuzzy model representation and Linear Matrix Inequality (LMI) based design under the Parallel Distributed Compensation control design framework. The related literature reports considerable research on how to manipulate with LMI's to achieve better control performance. This paper shows that this effort is not enough, the optimization of the control performance must include the convex hull manipulation on the TS fuzzy model beside manipulating with the LMIs. The LMI's guaranty the optimal solution for a given convex hull, but the convex hull representation is not invariant. Furthermore, the solutions by LMI's are very sensitive for the convex hull. The paper proposes a systematic concept for various convex hull manipulation method.","Fuzzy control,
Control design,
Tensile stress,
Linear matrix inequalities,
Power system modeling,
Fuzzy systems,
Concurrent computing,
Distributed computing,
Design automation,
Electronic mail"
Dynamic Spatial Approximation Trees for Massive Data,"Metric space searching is an emerging technique to address the problem of efficient similarity searching in many applications, including multimedia databases and other repositories handling complex objects. Although promising, the metric space approach is still immature in several aspects that are well established in traditional databases. In particular, most indexing schemes are not dynamic, that is, few of them tolerate insertion of elements at reasonable cost over an existing index and only a few work efficiently in secondary memory. In this paper we introduce a secondary-memory variant of the Dynamic Spatial Approximation Tree, which has shown to be competitive in main memory. The resulting index handles well the secondary memory scenario and is competitive with the state of the art, becoming a useful alternative in a wide range of database applications. Moreover, our ideas are applicable to other secondary-memory trees where there is little control over the tree shape.","Spatial databases,
Extraterrestrial measurements,
Application software,
Multimedia databases,
Indexing,
Costs,
Indexes,
Shape control,
Degradation,
Computer science"
Dynamic probabilistic counter-based broadcasting in mobile ad hoc networks,"In Mobile Ad hoc Network (MANETs), flooding is the simplest broadcasting mechanism where each node retransmits every uniquely received packet exactly once. Despite its simplicity it could potentially leads to high redundant retransmissions causing high channel contention and thus excessive packet collisions in the network. This phenomenon referred to as broadcast storm problem has been shown to greatly increase the network communication overhead and end-to-end delay. Numerous probabilistic approaches have been proposed to mitigate the impact of this inherent phenomenon. However, most of these techniques are inadequate in reducing the number of redundant retransmissions while still guaranteeing that all nodes receive the packet. Further, in most cases they use a predetermined forwarding probability value for all nodes in the network which is quite unlikely to be optimal in other network set up. In this paper, we propose a new dynamic probabilistic counter-based broadcast scheme that can dynamically compute the forwarding probability at a node based on its neighbourhood information. Simulation results show that the new broadcast scheme achieves superior performance in terms of retransmitting nodes, collision rate, and end-to-end delay without sacrificing reachability compared to the existing schemes.",
Intellectual Property Rights Requirements for Heterogeneously-Licensed Systems,"Heterogeneously-licensed systems pose new challenges to analysts and system architects. Appropriate intellectual property rights must be available for the installed system, but without unnecessarily restricting other requirements, the system architecture, and the choice of components both initially and as it evolves. Such systems are increasingly common and important in e-business, game development, and other domains. Our semantic parameterization analysis of open-source licenses confirms that while most licenses present few roadblocks, reciprocal licenses such as the GNU General Public License produce knotty constraints that cannot be effectively managed without analysis of the system's license architecture. Our automated tool supports intellectual property requirements management and license architecture evolution. We validate our approach on an existing heterogeneously-licensed system.","Intellectual property,
Licenses,
Open source software,
Computer architecture,
USA Councils,
Runtime,
Law,
Legal factors,
Computer science,
Trademarks"
A high throughput CABAC encoder for ultra high resolution video,"In this paper, a high throughput fully hardwired CABAC encoder is proposed for real-time encoding video of ultra high resolution, e.g., QFHD (3840×2160). We analyze the distribution of bins in various syntax elements and accordingly propose a new architecture which includes an optimized context memory access scheme, a multi-bin Binary Arithmetic Encoder (BAE), and an SE-specific cycle-reduction context modeler for increasing BAE utilization. Our architecture can process up to 8 bins per cycle. Running at 222 MHz, it is capable of real-time encoding QFHD video in the worst case of main profile Level 5.1. We have successfully integrated the proposed CABAC encoder into an H.264/AVC encoder system using a multi-media SoC platform.","Throughput,
Encoding,
Context modeling,
Computer architecture,
Arithmetic,
Data analysis,
Video compression,
Computer science,
Automatic voltage control,
Multimedia systems"
Inkjet printing PEDOT:PSS using desktop inkjet printer,"PEDOT:PSS has been used recently into many organic-based devices in order to help charge transfer and improve efficiency of the devices. PEDOT:PSS exhibit various interesting properties. It posses relatively good electrochemical, ambient, and thermal stability of its electrical properties as compared with the other polythiophenes. One aim of manufacturing organic-based device is to lowering the fabrication cost. Due to PEDOT:PSS's stability , it is possible to pattern PEDOT:PSS using inkjet printing. We found that using the CANON IP4500 desktop inkjet printer, the structure of 150 micron could be patterned on PET substrate. By modifying the surface properties of the substrate , the structure of 20 micron could be achieved. The conductivity of inkjet printed PEDOT:PSS could be further enhanced by annealing at 80 C. The conductivity could be 3 times improved. The morphology of the annealed PEDOT:PSS was further investigated using atomic force microscopy(AFM) and the cause for conductivity enhancement could be explained via localization length extension in variable range hopping theory.","Printing,
Printers,
Conductivity,
Annealing,
Atomic force microscopy,
Charge transfer,
Thermal stability,
Manufacturing,
Fabrication,
Costs"
