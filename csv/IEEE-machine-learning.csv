Title,Abstract,Keywords
Bonferroni mean with weighted interaction,"Bonferroni mean aggregates the interaction between all pairs of inputs from some n-dimensional input vector. Therefore it is able to capture the dependency structure between the inputs. Weighted version of the Bonferroni mean then assumes that each input has a possibly different weight. Such an approach poses several constraints that decrease the modelling flexibility of the dependency structure on the input space. In this paper we present an overview of the different approaches to weighted Bonferroni mean and introduce the Bonferroni mean with weighted interaction where interactions between inputs are weighted rather than the inputs themselves. We show several important properties of the Bonferroni mean with weighted interaction and its connection to concepts studied before. Finally, using the described approach we design a system for emergency management that is able to raise an alarm in the case where certain criteria are met.",
Converse Theorems for Safety and Barrier Certificates,"An important tool for proving safety of dynamical systems is the notion of a barrier certificate. In this technical note we prove that every robustly safe ordinary differential equation has a barrier certificate. Moreover, we show a construction of such a barrier certificate based on a set of states that is reachable in finite time.",
Scalable Reactive Molecular Dynamics Simulations for Computational Synthesis,"Reactive molecular-dynamics (MD) simulation is a powerful research tool for describing chemical reactions. We eliminate the speed-limiting charge iteration in MD with a novel extended-Lagrangian scheme. The extended-Lagrangian reactive MD (XRMD) code drastically improves energy conservation while substantially reducing time-to-solution. Furthermore, we introduce a new polarizable charge equilibration (PQEq) model to accurately predict atomic charges and polarization. The XRMD code based on hybrid message passing+multithreading achieves a weak-scaling parallel efficiency of 0.977 on 786,432 IBM Blue Gene/Q cores for a 67.6 billion-atom system. The performance is portable to the 2nd generation Intel Xeon Phi, Knights Landing. Blue Gene/Q simulations for the computational synthesis of materials via novel exfoliation mechanisms for synthesizing atomically thin transition metal dichalcogenide layers, which will dominate nanomaterials science in this century.",
Leveraging Content Sensitiveness and User Trustworthiness to Recommend Fine-Grained Privacy Settings for Social Image Sharing,"To configure successful privacy settings for social image sharing, two issues are inseparable: (1) content sensitiveness of the images being shared; and (2) trustworthiness of the users being granted to see the images. This paper aims to consider these two inseparable issues simultaneously to recommend finegrained privacy settings for social image sharing. For achieving more compact representation of image content sensitiveness (privacy), two approaches are developed: (a) a deep network is adapted to extract 1024-D discriminative deep features; and (b) a deep multiple instance learning algorithm is adopted to identify 280 privacy-sensitive object classes and events. Secondly, users on the social network are clustered into a set of representative social groups to generate a discriminative dictionary for user trustworthiness characterization. Finally, both the image content sensitiveness and the user trustworthiness are integrated to train tree classifier to recommend fine-grained privacy settings for social image sharing. Our experimental studies have demonstrated both the efficiency and the effectiveness of our proposed algorithms.","Privacy,
Feature extraction,
Visualization,
Computer science,
Electronic mail,
Social network services"
Fast Algorithms for Computing Path-Difference Distances,"Tree comparison metrics are an important tool for the study of phylogenetic trees. Path-difference distances measure the dissimilarity between two phylogenetic trees (on the same set of taxa) by comparing their path-length vectors. Various norms can be applied to this distance. Three important examples are the l_{1}
-, l_{2}
, and l_{\infty}
-norms. The previous best algorithms for computing path-difference distances all have O(n^2)
running time. In this paper, we show how to compute the l_1-norm path-difference distance in O(n \text log^2 n)
time and how to compute the l_{2}
- and l_{\infty}
-norm path-difference distances in O(n\ \text log\ n)
time. By extending the presented algorithms, we also show that the l_{p}
-norm path-difference distance can be computed in O(pn\ \text log^2\ n)
time for any positive integer p
. In addition, when the integer p
is even, we show that the distance can be computed in O(p^2 n\ \text log\ n)
time as well.",
A Heuristic Algorithm for a Low Autocorrelation Binary Sequence Problem with Odd Length and High Merit Factor,"A Low Autocorrelation Binary Sequence (LABS) problem is a hard combinatorial problem and its solutions are important in many practical applications. Till now, the largest best-known skew-symmetric sequence with merit factor greater than 9 had a length of 189. In this paper, a new heuristic algorithm is presented for the LABS problem. The proposed algorithm stores promising solutions and this mechanism enables the algorithm to perform local searches on these solutions in a systematic way. Our algorithm was tested on skew-symmetric sequences and the obtained results are compared with results of the state-of-the-art algorithms. The proposed algorithm was able to find some new best-known skew-symmetric solutions with merit factor greater than 9 in sequence lengths over 200. The obtained results improve the suggestion from 1985 (Beenker et al.) and 1987 (Bernasconi) greatly, where the merit factor is approximately equal to 6 for long skew-symmetric sequences with length up to 199. Now, the largest best-known skew-symmetric sequence with merit factor greater than 9 has the length 225. Additionally, now all merit factors are greater than 8.5 on the interval from 159 up to 225 for odd lengths.","Correlation,
Heuristic algorithms,
Search problems,
Upper bound,
Memetics,
Computer science,
Systematics"
A Tensor-Based Multiple Clustering Approach With Its Applications in Automation Systems,"Multiple clustering analysis has the clear advantages to discover latent data pattern in big data from different views, so it has tremendous practical values in automation industries. However, most of current algorithms are difficult to group heterogeneous data to multiple clusterings according to the requirements of different applications. This paper presents a flexible multiple clustering analytic and service framework, and a novel tensor-based multiple clusterings (TMC) approach. Heterogeneous data objects in cyber-physical-social systems are first represented as low-order tensors and a weight tensor construction approach is proposed to measure the importance of attributes combinations in heterogeneous feature spaces. Then, a selective weighted tensor distance is explored to cluster tensorized data objects for different applications. This paper, through a real-world smart bike maintenance system, illustrates TMC and evaluates its clustering performance. Experimental results reveal TMC can obtain higher quality clustering results but with lower redundancies to meet different requirements of applications in automation systems.",
A Systematic Study of Online Class Imbalance Learning With Concept Drift,"As an emerging research topic, online class imbalance learning often combines the challenges of both class imbalance and concept drift. It deals with data streams having very skewed class distributions, where concept drift may occur. It has recently received increased research attention; however, very little work addresses the combined problem where both class imbalance and concept drift coexist. As the first systematic study of handling concept drift in class-imbalanced data streams, this paper first provides a comprehensive review of current research progress in this field, including current research focuses and open challenges. Then, an in-depth experimental study is performed, with the goal of understanding how to best overcome concept drift in online learning with class imbalance.",
An Analytical method of Network Service scalability,"The research of network service scalability is essentially the research of the capability of providing service, which is based on services and the relationship among the services. However, most of the research ignore the systematic research on analytical methods of service scalability in network systems, owing to the complexity and the diversity of the network system. In this paper, we propose an analytical method of network service scalability which is composed of a network description model and a service evaluation model. The method uses the network description model to describe services and the relationship among them. After presenting the network description, the method uses the service evaluation model to analyze the network service scalability. For analyzing and simulating purposes, we select three P2P network models which are different with each other in network topologies as an example. Our simulation results which are in accord with the results of the example analysis verify the correctness and applicability of the analytical method.",
On the Design of Minimal-Cost Pipeline Systems Satisfying Hard/Soft Real-Time Constraints,"Pipeline systems provide high throughput for applications by overlapping the executions of tasks. In the architectures with heterogeneity, two basic issues in the design of application-specific pipelines need to be studied: what type of functional unit to execute each task, and where to place buffers. Due to the increasing complexity of applications, pipeline designs face a bundle of problems. One of the most challenging problems is the uncertainty on the execution times, which makes the deterministic techniques inapplicable. In this paper, the execution times are modeled as random variables. Given an application, our objective is to construct the optimal pipeline, such that the total cost of the resultant pipeline can be minimized while satisfying the required timing constraints with the given guaranteed probability. We first prove the NP-hardness of the problem. Then, we present Mixed Integer Linear Programming (MILP) formulations to obtain the optimal solution. Due to the high time complexity of MILP, we devise an efficient
(1+ϵ)
-approximation algorithm, where the value of
ϵ
is less than 5% in practice. Experimental results show that our algorithms can achieve significant reductions in cost over the existing techniques, reaching up to 31.93% on average",
Multicore Mixed-Criticality Systems: Partitioned Scheduling and Utilization Bound,"In mixed-criticality (MC) systems, multiple activities with various certification requirements (thus with different criticality levels) can co-exist on shared hardware platforms, where multicore processors have emerged as the de facto computing engines. In this paper, by using the partitioned earliest-deadline-first with virtual deadlines (EDF-VDs) scheduler for a set of periodic MC tasks running on multicore systems, we derive a criticality-aware utilization bound for efficient feasibility tests and then identify its characteristics. Our analysis shows that the bound increases with increasing number of cores and decreasing system criticality level. We show that, since the utilizations of MC tasks at different criticality levels can vary considerably, the utilization contribution of a task on different cores may have large variations and thus can significantly affect the system schedulability under the EDF-VD scheduler. Based on these observations, we propose a novel and efficient criticality-aware task partitioning algorithm (CA-TPA) to compensate for the inherent pessimism of the utilization bound. In order to improve the system schedulability, the task priorities are determined according to their utilization contributions to the system in CA-TPA. Moreover, by analyzing the utilization variations of tasks at different levels, we develop several heuristics to minimize the utilization increment and balance the workload on cores. The simulation results show that the CA-TPA scheme is very effective in achieving higher schedulability ratio and yielding balanced workloads. The actual implementation in Linux operating system further demonstrates the applicability of CA-TPA with lower run-time overhead, compared to the existing partitioning schemes.","Multicore processing,
Program processors,
Partitioning algorithms,
Computer science,
Scheduling algorithms,
Engines"
Analyzing E-Commerce Business Process Nets via Incidence Matrix and Reduction,"E-commerce business process nets (EBPNs) are a novel formal model for describing and validating e-commerce systems including interactive parties such as shopper, merchant, and third-party payment platform. Data errors and nondeterminacy of the data states during the trading process can be depicted with the help of EBPNs. However, the problem about how to analyze EBPNs remains largely open. To analyze their data-liveness, data-boundedness, and reachability, this paper presents two analysis methods. For EBPNs, reachability analysis is proposed based on a 3-D incidence matrix method. Additionally, reduction methods are proposed for a special EBPN. Finally, the validity and reliability of the proposed methods are illustrated via the examples of e-commerce systems.",
Real-Time Process Adaptation: A Context-Aware Replanning Approach,"Complexity and dynamism of day-to-day activities are inextricably linked, thus the need for constant adaptation to address emerging demands has grown. Process adaptation is the action of customizing a process instance to make it applicable to a particular situation. However, unplanned conditions may occur at any time during process execution. So, the design of a complete process model has given place to a flexible design based on reuse and adaptation. This paper addresses the problem of dynamic adaptation within a process-aware information system (IS). On top of a theory for context-aware ISs, we argue that an unexpected situation can be characterized by a number of known contextual elements and could be used to automate the decision of replanning the process flow in a specific instance, in order to preserve the process strategy. The solution was evaluated in real setting observational studies in the domains of oil and gas and air traffic control.",
L_{0} -Regularized Image Downscaling,"In this paper, we propose a novel L_{0} -regularized optimization framework for image downscaling. The optimization is driven by two L_{0} -regularized priors. The first prior, gradient-ratio prior, is based on the observation that the number of edges in the downscaled image is approximately inverse square proportional to the downscaling factor. By introducing L_{0} norm sparsity to the gradient ratio, the downscaled image is able to preserve the most salient edges as well as the visual perception of the original image. The second prior, downsampling prior, is to constrain the downsampling matrix so that pixels of the downscaled image are estimated according to those optimal neighboring pixels. Extensive experiments on the Urban100 and BSDS500 data sets show that the proposed algorithm achieves superior performance over the state-of-the-arts, in terms of both quality and robustness.",
An Efficient Prediction-Based User Recruitment for Mobile Crowdsensing,"Mobile crowdsensing is a new paradigm in which a group of mobile users exploit their smart devices to cooperatively perform a large-scale sensing job. One of the users’ main concerns is the cost of data uploading, which affects their willingness to participate in a crowdsensing task. In this paper, we propose an efficient Prediction-based User Recruitment for mobile crowdsEnsing (PURE), which separates the users into two groups corresponding to different price plans: Pay as you go (PAYG) and Pay monthly (PAYM). By regarding the PAYM users as destinations, the minimizing cost problem goes to recruiting the users that have the largest contact probability with a destination. We first propose a semi-Markov model to determine the probability distribution of user arrival time at points of interest (PoIs) and then get the inter-user contact probability. Next, an efficient prediction-based user-recruitment strategy for mobile crowdsensing is proposed to minimize the data uploading cost. We then propose PURE-DF by extending PURE to a case in which we address the tradeoff between the delivery ratio of sensing data and the recruiter number according to Delegation Forwarding. We conduct extensive simulations based on three widely-used real-world traces: roma/taxi, epfl, and geolife. The results show that, compared with other recruitment strategies, PURE achieves a lower recruitment payment and PURE-DF achieves the highest delivery efficiency.",
The 2016 Two-Player GVGAI Competition,"This paper showcases the setting and results of the first Two-Player General Video Game AI competition, which ran in 2016 at the IEEE World Congress on Computational Intelligence and the IEEE Conference on Computational Intelligence and Games. The challenges for the general game AI agents are expanded in this track from the single-player version, looking at direct player interaction in both competitive and cooperative environments of various types and degrees of difficulty. The focus is on the agents not only handling multiple problems, but also having to account for another intelligent entity in the game, who is expected to work towards their own goals (winning the game). This other player will possibly interact with first agent in a more engaging way than the environment or any non-playing character may do. The top competition entries are analyzed in detail and the performance of all agents is compared across the four sets of games. The results validate the competition system in assessing generality, as well as showing Monte Carlo Tree Search continuing to dominate by winning the overall Championship. However, this approach is closely followed by Rolling Horizon Evolutionary Algorithms, employed by the winner of the second leg of the contest.",
Understanding the Relationship Between Interactive Optimisation and Visual Analytics in the Context of Prostate Brachytherapy,"The fields of operations research and computer science have long sought to find automatic solver techniques that can find high-quality solutions to difficult real-world optimisation problems. The traditional workflow is to exactly model the problem and then enter this model into a general-purpose “black-box” solver. In practice, however, many problems cannot be solved completely automatically, but require a “human-in-the-loop” to iteratively refine the model and give hints to the solver. In this paper, we explore the parallels between this interactive optimisation workflow and the visual analytics sense-making loop. We assert that interactive optimisation is essentially a visual analytics task and propose a problem-solving loop analogous to the sense-making loop. We explore these ideas through an in-depth analysis of a use-case in prostate brachytherapy, an application where interactive optimisation may be able to provide significant assistance to practitioners in creating prostate cancer treatment plans customised to each patient's tumour characteristics. However, current brachytherapy treatment planning is usually a careful, mostly manual process involving multiple professionals. We developed a prototype interactive optimisation tool for brachytherapy that goes beyond current practice in supporting focal therapy - targeting tumour cells directly rather than simply seeking coverage of the whole prostate gland. We conducted semi-structured interviews, in two stages, with seven radiation oncology professionals in order to establish whether they would prefer to use interactive optimisation for treatment planning and whether such a tool could improve their trust in the novel focal therapy approach and in machine generated solutions to the problem.",
Web Media and Stock Markets : A Survey and Future Directions from a Big Data Perspective,"Stock market volatility is influenced by information release, dissemination, and public acceptance. With the increasing volume and speed of social media, the effects of Web information on stock markets are becoming increasingly salient. However, studies of the effects of Web media on stock markets lack both depth and breadth due to the challenges in automatically acquiring and analyzing massive amounts of relevant information. In this study, we systematically reviewed 229 research articles on quantifying the interplay between Web media and stock markets from the fields of Finance, Management Information Systems, and Computer Science. In particular, we first categorized the representative works in terms of media type and then summarized the core techniques for converting textual information into machine-friendly forms. Finally, we compared the analysis models used to capture the hidden relationships between Web media and stock movements. Our goal is to clarify current cutting-edge research and its possible future directions to fully understand the mechanisms of Web information percolation and its impact on stock markets from the perspectives of investors cognitive behaviors, corporate governance, and stock market regulation.",
Artificial Intelligence in the Rising Wave of Deep Learning: The Historical Path and Future Outlook [Perspectives],"Artificial intelligence (AI) is a branch of computer science and a technology aimed at developing the theories, methods, algorithms, and applications for simulating and extending human intelligence. Modern AI enables going from an old world-where people give computers rules to solve problems-to a new world-where people give computers problems directly and the machines learn how to solve them on their own using a set of algorithms. An algorithm is a self-contained sequence of instructions and actions to be performed by a computational machine. Starting from an initial state and initial input, the instructions describe computational steps, which, when executed, proceed through a finite number of well-defined successive states, eventually producing an output and terminating at a final ending state. AI algorithms are a rich set of algorithms used to perform AI tasks, notably those pertaining to perception and cognition that involve learning from data and experiences simulating human intelligence.",
An Energy Conserving Routing Scheme for Wireless Body Sensor Nanonetwork Communication,"Current developments in nanotechnology make electromagnetic communication possible at the nanoscale for applications involving Body Sensor Networks (BSNs). This specialized branch of Wireless Sensor Networks, drawing attention from diverse fields such as engineering, medicine, biology, physics and computer science, has emerged as an important research area contributing to medical treatment, social welfare, and sports. The concept is based on the interaction of integrated nanoscale machines by means of wireless communications. One key hurdle for advancing nanocommunications is the lack of an apposite networking protocol to address the upcoming needs of the nanonetworks. Recently, some key challenges have been identified, such as nanonodes with extreme energy constraints, limited computational capabilities, Terahertz frequency bands with limited transmission range, etc., in designing protocols for wireless nanosensor networks. This work proposes an improved performance scheme of nanocommunication over Terahertz bands for wireless BSNs making it suitable for smart e-health applications. The scheme contains – a new energy-efficient forwarding routine for electromagnetic communication in wireless nanonetworks consisting of hybrid clusters with centralized scheduling; a model designed for channel behavior taking into account the aggregated impact of molecular absorption, spreading loss, and shadowing; and an energy model for energy harvesting and consumption. The outage probability is derived for both single and multilinks and extended to determine the outage capacity. The outage probability for a multilink is derived using a cooperative fusion technique at a predefined fusion node. Simulated using a Nano-Sim simulator, performance of the proposed model has been evaluated for energy efficiency, outage capacity, and outage probability. The results demonstrate the efficiency of the proposed scheme through maximized energy utilization in both single and multihop communication; multisensor fusion at the fusion node enhances the link quality of the transmission.",
WoCE: A framework for Clustering Ensemble by Exploiting the Wisdom of Crowds Theory,"The wisdom of crowds (WOCs), as a theory in the social science, gets a new paradigm in computer science. The WOC theory explains that the aggregate decision made by a group is often better than those of its individual members if specific conditions are satisfied. This paper presents a novel framework for unsupervised and semisupervised cluster ensemble by exploiting the WOC theory. We employ four conditions in the WOC theory, i.e., diversity, independency, decentralization, and aggregation, to guide both constructing of individual clustering results and final combination for clustering ensemble. First, independency criterion, as a novel mapping system on the raw data set, removes the correlation between features on our proposed method. Then, decentralization as a novel mechanism generates high quality individual clustering results. Next, uniformity as a new diversity metric evaluates the generated clustering results. Further, weighted evidence accumulation clustering method is proposed for the final aggregation without using thresholding procedure. Experimental study on varied data sets demonstrates that the proposed approach achieves superior performance to state-of-the-art methods.",
A Survey on Quantum Channel Capacities,"Quantum information processing exploits the quantum nature of information. It offers fundamentally new solutions in the field of computer science and extends the possibilities to a level that cannot be imagined in classical communication systems. For quantum communication channels, many new capacity definitions were developed in comparison to classical counterparts. A quantum channel can be used to realize classical information transmission or to deliver quantum information, such as quantum entanglement. Here we review the properties of the quantum communication channel, the various capacity measures and the fundamental differences between the classical and quantum channels.",
Efficient Analog Circuits for Boolean Satisfiability,"Efficient solutions to nonpolynomial (NP)-complete problems would significantly benefit both science and industry. However, such problems are intractable on digital computers based on the von Neumann architecture, thus creating the need for alternative solutions to tackle such problems. Recently, a deterministic, continuous-time dynamical system (CTDS) was proposed [1] to solve a representative NP-complete problem, Boolean Satisfiability (SAT). This solver shows polynomial analog time-complexity on even the hardest benchmark
k
-SAT (
k≥3
) formulas, but at an energy cost through exponentially driven auxiliary variables. This paper presents a novel analog hardware SAT solver, AC-SAT, implementing the CTDS via incorporating novel, analog circuit design ideas. AC-SAT is intended to be used as a coprocessor and is programmable for handling different problem specifications. It is especially effective for solving hard
k
-SAT problem instances that are challenging for algorithms running on digital machines. Furthermore, with its modular design, AC-SAT can readily be extended to solve larger size problems, while the size of the circuit grows linearly with the product of the number of variables and the number of clauses. The circuit is designed and simulated based on a 32-nm CMOS technology. Simulation Program with Integrated Circuit Emphasis (SPICE) simulation results show speedup factors of ~104 on even the hardest 3-SAT problems, when compared with a state-of-the-art SAT solver on digital computers. As an example, for hard problems with
N=50
variables and
M=212
clauses, solutions are found within from a few nanoseconds to a few hundred nanoseconds.",
Safety and Security in Cyber-Physical Systems and Internet-of-Things Systems,"Safety and security have traditionally been distinct problems in engineering and computer science. The introduction of computing elements to create cyber-physical systems (CPSs) has opened up a vast new range of potential problems that do not always show up on the radar of traditional engineers. Security, in contrast, is traditionally viewed as a data or communications security problem to be handled by computer scientists and/or computer engineers. Advances in CPSs and the Internet-of-Things (IoT) requires us to take a unified view of safety and security. This paper defines a safety/security threat model for CPSs and IoT systems and surveys emerging techniques which improve the safety and security of CPSs and IoT systems.",
Genetic Improvement of Software: a Comprehensive Survey,"Genetic improvement uses automated search to find improved versions of existing software. We present a comprehensive survey of this nascent field of research with a focus on the core papers in the area published between 1995 and 2015. We identified core publications including empirical studies, 96% of which use evolutionary algorithms (genetic programming in particular). Although we can trace the foundations of genetic improvement back to the origins of computer science itself, our analysis reveals a significant upsurge in activity since 2012. Genetic improvement has resulted in dramatic performance improvements for a diverse set of properties such as execution time, energy and memory consumption, as well as results for fixing and extending existing system functionality. Moreover, we present examples of research work that lies on the boundary between genetic improvement and other areas, such as program transformation, approximate computing, and software repair, with the intention of encouraging further exchange of ideas between researchers in these fields.","Genetic programming,
Software,
Software testing,
History,
Software engineering"
"Software Engineering for Computational Science: Past, Present, Future","While the importance of in silico experiments for the scientific discovery process increases, state-of-the-art software engineering practices are rarely adopted in computational science. To understand the underlying causes for this situation and to identify ways for improving the current situation, we conduct a literature survey on software engineering practices in computational science. As a result of our survey, we identified 13 recurring key characteristics of scientific software development that can be divided into three groups: characteristics that results (1) from the nature of scientific challenges, (2) from limitations of computers, and (3) from the cultural environment of scientific software development. Our findings allow us to point out shortcomings of existing approaches for bridging the gap between software engineering and computational science and to provide an outlook on promising research directions that could contribute to improving the current situation.",
Data Visualization Saliency Model: A Tool for Evaluating Abstract Data Visualizations,"Evaluating the effectiveness of data visualizations is a challenging undertaking and often relies on one-off studies that test a visualization in the context of one specific task. Researchers across the fields of data science, visualization, and human-computer interaction are calling for foundational tools and principles that could be applied to assessing the effectiveness of data visualizations in a more rapid and generalizable manner. One possibility for such a tool is a model of visual saliency for data visualizations. Visual saliency models are typically based on the properties of the human visual cortex and predict which areas of a scene have visual features (e.g. color, luminance, edges) that are likely to draw a viewer's attention. While these models can accurately predict where viewers will look in a natural scene, they typically do not perform well for abstract data visualizations. In this paper, we discuss the reasons for the poor performance of existing saliency models when applied to data visualizations. We introduce the Data Visualization Saliency (DVS) model, a saliency model tailored to address some of these weaknesses, and we test the performance of the DVS model and existing saliency models by comparing the saliency maps produced by the models to eye tracking data obtained from human viewers. Finally, we describe how modified saliency models could be used as general tools for assessing the effectiveness of visualizations, including the strengths and weaknesses of this approach.","Data visualization,
Visualization,
Measurement,
Data models,
Brain modeling,
Predictive models,
Tools"
Learning Pose-Aware Models for Pose-Invariant Face Recognition in the Wild,"We propose a method designed to push the frontiers of unconstrained face recognition in the wild with an emphasis on extreme out-of-plane pose variations. Existing methods either expect a single model to learn pose invariance by training on massive amounts of data or else normalize images by aligning faces to a single frontal pose. Contrary to these, our method is designed to explicitly tackle pose variations. Our proposed Pose-Aware Models (PAM) process a face image using several pose-specific, deep convolutional neural networks (CNN). 3D rendering is used to synthesize multiple face poses from input images to both train these models and to provide additional robustness to pose variations at test time. Our paper presents an extensive analysis on the IJB-A benchmark, evaluating the effects that landmark detection accuracy, CNN layer selection, and pose model selection all have on the performance of the recognition pipeline. It further provides comparative evaluations on the IARPA Janus Benchmarks A (IJB-A) and the PIPA dataset. These tests show that our approach outperforms existing methods, even surprisingly matching the accuracy of methods that were specifically fine-tuned to the target dataset. Parts of this work previously appeared in [1] and [2].",
Model-Driven Feedforward Prediction for Manipulation of Deformable Objects,"Robotic manipulation of deformable objects is a difficult problem especially because of the complexity of the many different ways an object can deform. Searching such a high-dimensional state space makes it difficult to recognize, track, and manipulate deformable objects. In this paper, we introduce a predictive, model-driven approach to address this challenge, using a precomputed, simulated database of deformable object models. Mesh models of common deformable garments are simulated with the garments picked up in multiple different poses under gravity, and stored in a database for fast and efficient retrieval. To validate this approach, we developed a comprehensive pipeline for manipulating clothing as in a typical laundry task. First, the database is used for category and the pose estimation is used for a garment in an arbitrary position. A fully featured 3-D model of the garment is constructed in real time, and volumetric features are then used to obtain the most similar model in the database to predict the object category and pose. Second, the database can significantly benefit the manipulation of deformable objects via nonrigid registration, providing accurate correspondences between the reconstructed object model and the database models. Third, the accurate model simulation can also be used to optimize the trajectories for the manipulation of deformable objects, such as the folding of garments. Extensive experimental results are shown for the above tasks using a variety of different clothings.",
"Principal Component Analysis Based Filtering for Scalable, High Precision k-NN Search","Approximate k
Nearest Neighbours (A k
NN) search is widely used in domains such as computer vision and machine learning. However, Ak
NN search in high-dimensional datasets does not scale well on multicore platforms, due to its large memory footprint. Parallel A k
NN search using space subdivision for filtering helps reduce the memory footprint, but its loss of precision is unstable. In this paper, we propose a new data filtering method—PCAF—for parallel Ak
NN search based on principal component analysis. PCAF improves on previous methods, demonstrating sustained, high scalability for a wide range of high-dimensional datasets on both Intel and AMD multicore platforms. Moreover, PCAF maintains highly precise Ak
NN search results.",
When Dijkstra meets vanishing point: a stereo vision approach for road detection,"In this paper, we propose a vanishing-point constrained Dijkstra road model for road detection in a stereovision paradigm. First, the stereo-camera is used to generate the u- and v-disparity maps of road image, from which the horizon can be extracted. With the horizon and ground region constraints, we can robustly locate the vanishing point of road region. Second, a weighted graph is constructed using all pixels of the image, and the detected vanishing point is treated as the source node of the graph. By computing a vanishing-point constrained Dijkstra minimum-cost map, where both disparity and gradient of gray image are used to calculate cost between two neighbor pixels, the problem of detecting road borders in image is transformed into that of finding two shortest paths that originate from the vanishing point to two pixels in the last row of image. The proposed approach has been implemented and tested over 2600 grayscale images of different road scenes in the KITTI dataset. The experimental results demonstrate that this training-free approach can detect horizon, vanishing point and road regions very accurately and robustly. It can achieve promising performance.",
D^{3} : A Dynamic Dual-Phase Deduplication Framework for Distributed Primary Storage,"Deploying deduplication for distributed primary storage is a sophisticated and challenging task, considering that the demands of low read/write latency, stable read/write performance, and efficient space saving are all of paramount importance. Unfortunately, existing schemes cannot present a satisfactory solution for the aforementioned requirements simultaneously. In this article, we propose D^{3} , a dynamic dual-phase deduplication framework for distributed primary storage. Several major innovations are established in D ^{3} . First, we formulate a deduplication-oriented taxonomy called Dedup-Type , to group data with similar deduplication-related characteristics into larger categories. It serves as coarse-grained filter and one of the prioritizing references in D^{3} . Second, D^{3} is a dual-phase framework—inline-phase and offline-phase deduplication processes work in concert with each other. Third, D ^{3} operates in a dynamic manner. We design two critical mechanisms: context-aware threshold adjustment (CTA) for local inline-phase deduplication, and deferred priority-based enforcement (DPE) for global offline-phase deduplication. The CTA mechanism enables selective deduplication under a periodically updated threshold. Data skipped during the inline phase is regarded as a candidate for offline phase, and is handled in a prioritized order under the governance of DPE mechanism. Evaluation results demonstrate that, compared with conventional inline and offline deduplication schemes, D^{3} achieves more efficient and stabler read/write performance with competitive space saving.",
ClusterFetch: A Lightweight Prefetcher for Intensive Disk Reads,"By overlapping disk accesses with computation-intensive operations, prefetching can reduce delays in launching an application and in loading significant amounts of data while the application is running. The key to effective prefetching is making the tradeoff between the mining accuracy of selecting relevant blocks, and the time to decide those blocks. To address this problem, we propose a new prefetcher called ClusterFetch. In its learning mode, ClusterFetch detects periods of intensive disk accesses by monitoring the speed at which read requests are queued; it re-organizes these reads and locates the file opened by the application just before each such period. During subsequent runs of the same application, ClusterFetch prefetches the data associated with the opening of a “trigger” file. Our experimental results show that ClusterFetch implemented in Linux can reduce the application launch time by up to 41.3 percent and the loading time by up to 38.2 percent, while taking up less than 200 KB of main memory.",
Bubble Budgeting: Throughput Optimization for Dynamic Workloads by Exploiting Dark Cores in Many Core Systems,"All the cores of a many-core chip cannot be active at the same time, due to reasons like low CPU utilization in server systems and limited power budget in dark silicon era. These free cores (referred to as bubbles) can be placed near active cores for heat dissipation so that the active cores can run at a higher frequency level, boosting the performance of applications that run on active cores. Budgeting inactive cores (bubbles) to applications to boost performance has the following three challenges. First, the number of bubbles varies due to open workloads. Second, communication distance increases when a bubble is inserted between two communicating tasks (a task is a thread or process of a parallel application), leading to performance degradation. Third, budgeting too many bubbles as coolers to running applications leads to insufficient cores for future applications. In order to address these challenges, in this paper, a bubble budgeting scheme is proposed to budget free cores to each application so as to optimize the throughput of the whole system. Throughput of the system depends on the execution time of each application and the waiting time incurred for newly arrived applications. Essentially, the proposed algorithm determines the number and locations of bubbles to optimize the performance and waiting time of each application, followed by tasks of each application being mapped to a core region. A Rollout algorithm is used to budget power to the cores as the last step. Experiments show that our approach achieves 50 percent higher throughput when compared to state-of-the-art thermal-aware runtime task mapping approaches. The runtime overhead of the proposed algorithm is in the order of 1M cycles, making it an efficient runtime task management method for large-scale many-core systems.",
SAMADroid: A Novel 3-Level Hybrid Malware Detection Model for Android Operating System,"For the last few years, Android is known to be the most widely used operating system and this rapidly increasing popularity has attracted the malware developer’s attention. Android allows downloading and installation of apps from other unofficial market places. This gives malware developers an opportunity to put repackaged malicious applications in third-party app-stores and attack the Android devices. A large number of malware analysis and detection systems have been developed which uses static analysis, dynamic analysis or hybrid analysis to keep Android devices secure from malware. However, the existing research clearly lags in detecting malware efficiently and accurately. For accurate malware detection, multilayer analysis is required which consumes large amount of hardware resources of resource constrained mobile devices. This research proposes an efficient and accurate solution to this problem, named SAMADroid, which is a novel 3-level hybrid malware detection model for Android operating systems. The research contribution includes multiple folds. Firstly, many of the existing Android malware detection techniques are thoroughly investigated and categorized on the basis of their detection methods. Also, their benefits along with limitations are deduced. A novel 3-level hybrid malware detection model for Android operating systems is developed, that can provide high detection accuracy by combining the benefits of the three different levels: i) Static and Dynamic Analysis; ii) Local and Remote Host; iii) Machine Learning Intelligence. Experimental results show that SAMADroid achieves high malware detection accuracy by ensuring the efficiency in terms of power and storage consumption.","Malware,
Androids,
Humanoid robots,
Operating systems,
Feature extraction,
Runtime"
Making a Small World Smaller: Path Optimization in Networks,"Reduction of end-to-end network delay is an optimization task with applications in multiple domains. Low delays enable improved information flow in social networks, quick spread of ideas in collaboration networks, low travel times for vehicles on road networks and increased rate of packets in the case of communication networks. Delay reduction can be achieved by both improving the propagation capabilities of individual nodes and adding additional edges in the network. One of the main challenges in such network design problems is that the effects of local changes are not independent, and as a consequence, there is a combinatorial search-space of possible improvements. Thus, minimizing the cumulative propagation delay requires novel scalable and data-driven approaches. We consider the problem of network delay minimization via node upgrades. We show that the problem is NP-hard and prove strong inapproximability results about it (i.e. APX-hard) even for equal vertex delays. However, probabilistic approximations for a restricted version of the problem can be obtained. We propose a greedy heuristic which has good quality in practice, but does not scale to very large instances. To enable scalability, we develop approximations for Greedy with probabilistic guarantees. Our methods scale almost linearly with the graph size and consistently outperform competitors in quality. We evaluate our approaches on several real-world graphs from different genres. We achieve up to 2 orders of magnitude speed-up compared to alternatives from the literature on moderate size networks, and obtain high-quality results in minutes on large datasets while competitors from the literature require more than 4 hours.",
Use of Deep Learning for Characterization of Microfluidic Soft Sensors,"Soft sensors made of highly deformable materials are one of the enabling technologies to various soft robotic systems, such as soft mobile robots, soft wearable robots, and soft grippers. However, major drawbacks of soft sensors compared with traditional sensors are their nonlinearity and hysteresis in response, which are common especially in microfluidic soft sensors. In this research, we propose to address the above issues of soft sensors by taking advantage of deep learning. We implemented a hierarchical recurrent sensing network, a type of recurrent neural network model, to the calibration of soft sensors for estimating the magnitude and the location of a contact pressure simultaneously. The proposed approach in this paper were not only able to model the nonlinear characteristic with hysteresis of the pressure response, but also find the location of the pressure.",
Microservices Scheduling Model over Heterogeneous Cloud-Edge Environments as Support for IoT Applications,"Motivated by the high-interest in increasing the utilization of non-general purpose devices in reaching computational objectives with a reduced cost, we propose a new model for scheduling microservices over heterogeneous Cloud-Edge environments. Our model uses a particular mathematical formulation for describing an architecture that includes heterogeneous machines that can handle different microservices. Since any new model asks for an early risk-analysis of the solution, we improved the CloudSim simulation framework to be suitable for an experiment that includes that kind of systems. In this paper, we discuss two examples of real-life utilizations of our proposed scheduling architecture. For an objective appreciation of the first example, we also include some experimental results based on the developed simulation tool. As a result of our interpretation of the experimental results we find out that some very simple scheduling algorithms may outperform some others in given situations that are frequently present in Cloud-Edge environments when we are using a microservice-oriented approach.",
Monolithic Piezoelectric Insect with Resonance Walking,"This article describes the design, manufacture and performance of an untethered hexapod robot titled MinRAR V2. This robot utilizes a monolithic piezoelectric element, machined to allow for individual activation of bending actuators. The legs were designed so that the first two resonance modes overlap and therefore produce a walking motion at resonance. The monolithic construction significantly improves the matching of resonance modes between legs when compared to previous designs. Miniature control and high voltage driving electronics were designed to drive 24 separate piezoelectric elements powered from a single 3.7V lithium polymer battery. The robot was driven both tethered and untethered and was able to achieve a maximum forward velocity of 98mm/s when driven at 190Hz and 6mm/s at 5Hz untethered. The robot is capable of a wide range of movements including banking, on the spot turning and reverse motion.",
Bi-Objective Optimization of Data-Parallel Applications on Homogeneous Multicore Clusters for Performance and Energy,"Performance and energy are now the most dominant objectives for optimization on modern parallel platforms composed of multicore CPU nodes. The existing intra-node and inter-node optimization methods employ a large set of decision variables but do not consider problem size as a decision variable and assume a linear relationship between performance and problem size and between energy consumption and problem size. We demonstrate using experiments of real-life data-parallel applications on modern multicore CPUs that these relationships have complex (non-linear and even non-convex) properties and, therefore, that the problem size has become an important decision variable that can no longer be ignored. This key finding motivates our work in this paper. In this paper, we first formulate the bi-objective optimization problem for performance and energy (BOPPE) for data-parallel applications on homogeneous clusters of modern multicore CPUs. It contains only one but heretofore unconsidered decision variable, the problem size. We then present an efficient and exact global optimization algorithm called ALEPH that solves the BOPPE. It takes as inputs, discrete functions of performance and dynamic energy consumption against problem size and outputs the globally Pareto-optimal set of solutions. The solutions are the workload distributions, which achieve inter-node optimization of data-parallel applications for performance and energy. While existing solvers for BOPPE give only one solution when the problem size and number of processors are fixed, our algorithm gives a diverse set of globally Pareto-optimal solutions. The algorithm has time complexity of
O(
m
2
×
p
2
)
where
m
is the number of points in the discrete speed/energy function and
p
is the number of available processors. We experimentally study the efficiency and scalability of our algorithm for two data parallel applications, matrix multiplication and fast Fourier transform, on a modern multicore CPU and homogeneous clusters of such CPUs. Based on our experiments, we show that the average and maximum sizes of the globally Pareto-optimal sets determined by our algorithm are 15 and 34 and 7 and 20 for the two applications respectively. Comparing with load-balanced workload distribution solution, the average and maximum percentage improvements in performance and energy respectively demonstrated for the first application are (13%,97%) and (18%,71%). For the second application, these improvements are (40%,95%) and (22%,127%). Assuming 5 percent performance degradation from the optimal is acceptable, the average and maximum improvements in energy consumption demonstrated for the two applications respectively are 9 and 44 and 8 and 20 percent. Using the algorithm and its building blocks, we also present a study of interplay between performance and energy. We demonstrate how ALEPH can be combined with DVFS-based Multi-Objective Optimization (MOP) methods to give a better set of (globally Pareto-optimal) solutions.",
An Efficient Nondominated Sorting Algorithm for Large Number of Fronts,"Nondominated sorting is a key operation used in multiobjective evolutionary algorithms (MOEA). Worst case time complexity of this algorithm is O(MN²), where N is the number of solutions and M is the number of objectives. For stochastic algorithms like MOEAs, it is important to devise an algorithm which has better average case performance. In this paper, we propose a new algorithm that makes use of faster scalar sorting algorithm to perform nondominated sorting. It finds partial orders of each solution from all objectives and use these orders to skip unnecessary solution comparisons. We also propose a specific order of objectives that reduces objective comparisons. The proposed method introduces a weighted binary search over the fronts when the rank of a solution is determined. It further reduces total computational effort by a large factor when there is large number of fronts. We prove that the worst case complexity can be reduced to ϴ(MNCmaxlog2(F+1)), where the number of fronts is F and the maximum number of solutions per front is Cmax; however, in general cases, our worst case complexity is still O(MN²). Our best case time complexity is O(MNlogN). We also achieve the best case complexity O(MNlogN+N²), when all solutions are in a single front. This method is compared with other state-of-the-art algorithms--efficient nondomination level update, deductive sort, corner sort, efficient nondominated sort and divide-and-conquer sort--in four different datasets. Experimental results show that our method, namely, bounded best order sort, is computationally more efficient than all other competing algorithms.",
A deep learning model for estimating story points,"Although there has been substantial research in software analytics for effort estimation in traditional software projects, little work has been done for estimation in agile projects, especially estimating the effort required for completing user stories or issues. Story points are the most common unit of measure used for estimating the effort involved in completing a user story or resolving an issue. In this paper, we propose a prediction model for estimating story points based on a novel combination of two powerful deep learning architectures: long short-term memory and recurrent highway network. Our prediction system is end-to-end trainable from raw input data to prediction outcomes without any manual feature engineering. We offer a comprehensive dataset for story points-based estimation that contains 23,313 issues from 16 open source projects. An empirical evaluation demonstrates that our approach consistently outperforms three common baselines (Random Guessing, Mean, and Median methods) and six alternatives (e.g. using Doc2Vec and Random Forests) in Mean Absolute Error, Median Absolute Error, and the Standardized Accuracy.",
RILoD: Reduction of Information Loss in a WSN System in the Presence of Dumb Nodes,"A sensor node behaves as “dumb,” when it continues to sense its vicinity, but fail to communicate with its neighbors due to the shrinkage in the communication range attributed to adverse environmental conditions such as rainfall, fog, and high temperature. However, the node behaves normally with the resumption of favorable environmental conditions. The nodes get isolated from the network only temporarily because of the temporal nature of adverse environmental conditions. Therefore, the dumb behavior of sensor nodes is temporary in nature. During the period of isolation, a sensor node continues its sensing, but it cannot send the sensed information to the sink, which increases the possibility of information loss in the network. Connectivity reestablishment of an isolated node with the network by activating the intermediate sleep nodes or adjusting the communication range gives the opportunity to send data to the sink. However, all the isolated nodes cannot reestablish connectivity due to the unavailability of intermediate nodes in the reduced or adjusted communication range. In this context, we propose a scheme, named RILoD, for a self-adaptive wireless sensor network, in which we use mobile mules to collect data from these isolated nodes and deliver them to the parent node, which can, in turn, forward the data to the sink. Using this technique, every isolated node can send their sensed information to the sink. Simulation results show that the proposed scheme reduces information loss by up to 32%. Furthermore, the proposed scheme has improved success probability of data delivery over the existing state of the art.",
LTE/LTE-A Network Security Data Collection and Analysis for Security Measurement: A Survey,"The Long Term Evolution (LTE)/LTE-Advanced (LTE-A) network provides advanced services for billions of users with its higher bandwidths, better spectrum efficiency, and lower latency than legacy cellular networks. But it still suffers from new security threats due to its all IP-based heterogeneous architecture. Therefore, there is a critical need to perform a rapid and accurate network security measurement in LTE/LTE-A network. To achieve LTE/LTE-A network security measurement, security-relevant data (in short security data) collection and data analysis for attack detection are required as prerequisites. However, most of the existing work only focuses on data collection and analysis for a certain type of LTE/LTE-A attacks. Little work has been done to comprehensively perform data collection and analysis for detecting various attacks on LTE/LTE-A network. Different from previous work, in this paper, we review the security data collection and data analysis methods in terms of various attacks in order to provide the basis of security measurement in LTE/LTE-A network. We first present a comprehensive taxonomy of attacks according to the LTE/LTE-A network structure. Then, we propose a number of criteria for evaluating the performance of data collection and analysis methods. And we lay our emphasis on the survey of data collection and analysis methods for significant active attack detection in LTE/LTE-A network. All the reviewed methods are analyzed and discussed based on the proposed evaluation criteria. Furthermore, current open issues and future research challenges are presented with a view to stimulating future research. Finally, an adaptive data collection and data analysis model for security measurement in LTE/LTE-A network is proposed.","Security,
Data collection,
Data analysis,
Communication networks,
3GPP,
Long Term Evolution,
IP networks"
Competitive Interaction Design of Cooperative Systems Against Attacks,"This technical note proposes a resilient cooperative control design for networked cooperative systems when subjected to external attacks. The systems considered in this paper can have any information topology described by a leader-follower digraph. A potential attack on such systems consists of unknown bounded signals generated from any linear or nonlinear finite-L2-gain exogenous dynamical system and injected distributively into nodes of the system's network. The purpose of the attack is to destabilize the consensus dynamics by intercepting the system's communication network and corrupting its local state feedback. The proposed resilient control design consists of introducing a virtual system with hidden network such that the overall system consisting of the original consensus system, the virtual system, and the attack dynamics is stable without requiring any information about the locations or nature of the attack. This is accomplished by utilizing the concept of competitive interaction to provide explicit design criteria for the hidden network of the virtual system to interact with the original system. A graph theoretical approach and a Lyapunov direct method are used to analyze",
Kinematic-free Orientation Control for a Deformable Manipulator based on the Geodesic in Rotation Group SO(3),"The robot orientation/attitude/rotation control problem is an important research topic with many applications including robotic manipulation, spacecraft control, quadrotor control, etc. The orientation control is particularly challenging when the system model is unknown due to the fact that the rotation group is a nonlinear manifold. In this paper, we propose a kinematic-free orientation control method for a deformable manipulator whose kinematic parameters are unknown. We estimate the Jacobian matrix by independently and incrementally actuating each joint and observing their effects on the orientation of the affected end-effector. Considering the nonlinear property, we generate a temporary and anticipant target by cutting a small distance on the geodesic in each control period, instead of using the target orientation directly. The geodesic is from the current orientation to the target orientation in SO (3). Our method guarantees global convergence and can automatically adapt to drastic changes in kinematic parameters, which is very difficult if using model-based methods. Our method is implemented to control a 4-DOF deformable manipulator in simulations and experiments. The simulation and experimental results demonstrate the effectiveness of our method.",
Mixture of Attractors: A novel Movement Primitive Representation for Learning Motor Skills from Demonstrations,"In this paper, we introduce Mixture of Attractors, a novel movement primitive representation which allows for learning complex object-relative movements. The movement primitive representation inherently supports multiple coordinate frames, enabling the system to generalize a skill to unseen object positions and orientations. In contrast to most other approaches, a skill is learned by solving a convex optimization problem Therefore, the quality of the skill does not depend on a good initial estimate of parameters. The resulting movements are automatically smooth and can be of arbitrary shape. The approach is evaluated and compared to other movement primitive representations on data from the Omniglot handwriting data set and on real demonstrations of a handwriting task. The evaluations show that the presented approach outperforms other state-of-the-art concepts in terms of generalization capabilities and accuracy.",
Scalable Privacy-Preserving Participant Selection for Mobile Crowdsensing Systems: Participant Grouping and Secure Group Bidding,"Mobile crowdsensing (MCS) has been emerging as a new sensing paradigm where vast numbers of mobile devices are used for sensing and collecting data in various applications. Auction based participant selection has been widely used for current MCS systems to achieve user incentive and task assignment optimization. However, participant selection problems solved with auction-based approaches usually involve participants' privacy concerns because a participant's bids may contain her private information (such as location visiting patterns), and disclosure of participants' bids may disclose their private information as well. In this paper, we study how to protect such bid privacy in a temporally and spatially dynamic MCS system. We assume that both sensing tasks and mobile participants have dynamic characteristics over spatial and temporal domains. Following the classical VCG auction, we carefully design a scalable grouping based privacy-preserving participant selection scheme, where participants are grouped into multiple participant groups and then auctions are organized within groups via secure group bidding. By leveraging Lagrange polynomial interpolation to perturb participants' bids within groups, participants' bid privacy is preserved. In addition, the proposed solution does not affect the operation of current MCS platform since the groups act as regular users to the platform. Both theoretical analysis and real-life tracing data simulations verify the efficiency and security of the proposed solution.",
Thermal Augmented Expression Recognition,"Visible facial images provide geometric and appearance patterns of facial expressions and are sensitive to illumination changes. Thermal facial images record facial temperature distribution and are robust to light conditions. Therefore, expression recognition is enhanced by visible and thermal image fusion. In most cases, only visible images are available due to the widespread popularity of visible cameras and the high cost of thermal cameras. Thus, we propose a novel visible expression recognition method by using thermal infrared (IR) data as privileged information, which is only available during training. Specifically, we first learn a deep model for visible images and thermal images. Then we use the learned feature representations to train support vector machine (SVM) classifiers for expression classification. We jointly refine the deep models as well as the SVM classifiers for both thermal images and visible images by imposing the constraint that the outputs of the SVM classifiers from two views are similar. Thermal IR images during training are then exploited to construct better facial representations and expression classifiers from visible images. We extend the proposed thermal augmented expression recognition method for partially unpaired data, acknowledging that visible images and thermal images maybe not be recorded synchronously. Experimental resulton the MAHNOB laughter database demonstrate that the proposed thermal augmented expression recognition method can effectively exploit thermal IR images' supplementary role for visible facial expression recognition during training to obtain better facial representations and a better visible expression classifier. The proposed thermal augmented expression recognition method achieves state-of-the-art expression recognition performance for both paired and unpaired facial images.",
NetCycle+: A Framework for Collective Evolution Inference in Dynamic Heterogeneous Networks,"Previous works on collective inference mainly focus on exploiting the autocorrelation among instances in a static network during the inference process. There are also approaches on time series prediction, which mainly exploit the autocorrelation within an instance at different time points. However, the response variables of related instances can co-evolve over time and their evolutions are not following a static correlation across time, but are following an internal life cycle. In this paper, we study the collective evolution inference problem, where the goal is to predict the response variables values for a group of related instances at the end of their life cycles. This problem is highly challenging because different instances can co-evolve over time and they can be at different stages of their life cycles and thus have different evolving patterns. Moreover, instances usually connected through heterogeneous networks, which involve complex relationships among instances. We propose an approach, called NetCycle+, by incorporating information from related instances and their life cycles. Furthermore, for studying the deep dependencies between nodes, we extend the graph convolution into NetCycle+. Experiments demonstrate that our approach can improve the inference performance by considering the autocorrelation through networks and the life cycles of the instances.",
TARCO: Two-Stage Auction for D2D Relay Aided Computation Resource Allocation in HetNet,"In heterogeneous cellular network, task scheduling for computation offloading is one of the biggest challenges. Most works focus on alleviating heavy burden of macro base stations by moving the computation tasks on macro cell user equipment (MUE) to remote cloud or small cell base stations. But the selfishness of network users is seldom considered. Motivated by the cloud edge computing, this paper provides incentive for task transfer from macro cell users to small cell base stations. The proposed incentive scheme utilizes small cell user equipment to provide relay service. The problem of computation offloading is modeled as a two-stage auction, in which the remote MUEs with common social character can form a group and then buy the computation resource of small-cell base stations with the relay of small cell user equipment. A two-stage auction scheme named TARCO is contributed to maximize utilities for both sellers and buyers in the network. The truthful, individual rationality and budget balance of the TARCO are also proved in this paper. In addition, two algorithms are proposed to further refine TARCO on the social welfare of the network. One can achieve higher utility of MUEs and the other can obtain higher total social welfare. Extensive simulation results demonstrate that, TARCO is better than random algorithm by 104.90% in terms of average utility of MUEs, while the performance of TARCO is further improved up to 28.75% and 17.06% by the proposed two algorithms, respectively.","Computer architecture,
Microprocessors,
Base stations,
Device-to-device communication,
Relays,
Mobile communication"
Optimization Algorithms for Multi-Access Green Communications in Internet of Things,"The exponential increase of the intelligent connected devices and the dramatic growth of the wireless data traffic have motivated the development of the green wireless networks as well as the Internet of Things. In this paper, we study the minimization problem of the total power to satisfy the required rate constraints in Internet of Things, where the users simultaneously communicate through multiple independent channels. This problem is complicated due to the non-linear data rate function based on the Shannon capacity formula. To this end, we first transfer the initial problem in power domain to an equivalent problem in rate domain instead of direct approximation for the high data rate. Then, we approximate it to a convex problem with the spectral radius constraints by the use of the Neumann expansion and nonlinear Perron-Frobenius theorem. By doing so, we achieve the close upper bound for this total power minimization problem. Moreover, we obtain the lower bound by making use of the convex relaxation technique, and finally get the global optimal solution by leveraging the branch-and-bound method. Simulation results verify that our proposed algorithms have a good approximation to the global optimal value for the power and rate allocations.",
A Novel High-Resolution Optical Instrument for Imaging Oceanic Bubbles,"The formation of bubbles from breaking waves has a significant effect on air–sea gas transfer and aerosol production. Detailed data in situ about the bubble populations are required to understand these processes. However, these data are difficult to acquire because bubble populations are complex, spatially inhomogeneous, and short lived. This paper describes the design and development of a novel high-resolution underwater optical instrument for imaging oceanic bubbles at the sea. The instrument was successfully deployed in 2013 as part of the HiWINGS campaign in the North Atlantic Ocean. It contains a high-resolution machine vision camera, strobe flash unit to create a light sheet, and single board computer to control system operation. The instrument is shown to successfully detect bubbles of radii in the range 20–10 000 μm.",
Mitigating Power Fluctuations in Electric Ship Propulsion With Hybrid Energy Storage System: Design and Analysis,"Shipboard electric propulsion systems experience large power and torque fluctuations on their drive shaft due to propeller rotational motion and waves. This paper explores new solutions to address these fluctuations by integrating a hybrid energy storage system (HESS) and exploring energy management (EM) strategies. The HESS combines battery packs with ultracapacitor banks. Two strategies for real-time EM of HESS are considered: one splits the power demand such that high- and low-frequency power fluctuations are compensated by ultracapacitors and batteries, respectively; another considers the HESS as a single entity and designs an EM strategy to coordinate the operations of the ultracapacitors and batteries. For both strategies, model predictive control is used to address power tracking and energy saving under various operating constraints. To quantitatively analyze the performance of HESS and its associated controls, a propeller and ship dynamic model, which captures the underlying physical behavior, is established to support the control development and system optimization. Power fluctuation mitigation and HESS loss minimization, the main objectives, are evaluated in different sea conditions. Simulation results show that the coordination within HESS provides substantial benefits in terms of reducing fluctuations and losses.","Propellers,
Marine vehicles,
Torque,
Batteries,
Resistance"
Motivated Optimal Developmental Learning for Sequential Tasks Without Using Rigid Time-Discounts,"Many methods for reinforcement learning use symbolic representations--nonemergent--such as Q-learning. We use emergent representations here, without human handcrafted symbolic states (i.e., each state corresponds to a different location). This paper models reinforcement learning for hidden neurons in emergent networks for sequential tasks. In this paper, their influences on sequential tasks (e.g., robot navigation in different scenarios) are investigated where the learned value and results of a behavior rely on not only the current experience just like in a pattern recognition (episodic) but also the prediction of future experiences (e.g., delayed rewards) and environments (e.g., previously learned navigational trajectories). We show that this new model of motivated learning amounts to the computation of the maximum-likelihood estimate through ``life'' where punishment and reward have increased weights. This new formulation avoids the greediness of time-discount in Q-learning. Its complex nonlinear sequential optimization has been solved in a closed-form procedure under the condition of the limited computational resources and limited learning experience so far, because we convert it into a simpler problem of incremental and linear estimation. The experimental results showed that the serotonin and dopamine systems speed up learning for sequential tasks, because not all events are equally important. As far as we know, this is the first work that studies the influences of reinforcers (via serotonin and dopamine) on hidden neurons (Y neurons) for sequential tasks in dynamic scenarios using emergent representations.",
Comparing Fusion Models for DNN-Based Audiovisual Continuous Speech Recognition,"Audiovisual fusion is one of the most challenging tasks that continues to attract substantial research interest in the field of audiovisual automatic speech recognition (AV-ASR). In the last few decades, many approaches for integrating the audio and video modalities were proposed to enhance the performance of automatic speech recognition in both clean and noisy conditions. However, very few studies can be found in the literature that compare different fusion models for AV-ASR. Even less research work compares audiovisual fusion models for large vocabulary continuous speech recognition (LVCSR) models using deep neural networks (DNNs). This paper reviews and compares the performance of five audiovisual fusion models: the feature fusion model, the decision fusion model, the multistream hidden Markov model (HMM), the coupled HMM, and the turbo decoders. A complete evaluation of these fusion models is conducted using a standard speaker-independent DNN-based LVCSR Kaldi recipe in three experimental setups: a clean-train-clean-test, a clean-train-noisy-test, and a matched-training setup. All experiments have been applied to the recently released NTCD-TIMIT audiovisual corpus. The task of NTCD-TIMIT is phone recognition in continuous speech. Using NTCD-TIMIT with its freely available visual features and 37 clean and noisy acoustic signals allows for this study to be a common benchmark, to which novel LVCSR AV-ASR models and approaches can be compared.",
Antenna Mode Switching for Full-Duplex Destination-Based Jamming Secure Transmission,"We investigate the secrecy rate optimization problem in a wiretap channel with a single-antenna source, a singleantenna eavesdropper, and a multiple-antenna full-duplex (FD) destination. To fully utilize the spatial degrees of freedom of multiple antennas, the function of antennas at the destination is not predefined, i.e., each antenna can operate in a transmit or receive mode. We propose a low-complexity near-optimal joint optimization scheme by jointly applying the dynamic antenna mode switching (AMS) and optimal power allocation (OPA) techniques, to maximize the secrecy rate of FD destinationbased jamming (DBJ) system. The proposed joint optimization scheme is valid for two different eavesdropping channel state information (ECSI) availability cases, i.e., instantaneous ECSIs and statistical ECSIs. Specifically, closed-form expressions of OPA factors are first derived, and then the optimal transmit and receive antennas sets at the destination are determined by combining the OPA factor and applying a greedy-searchbased AMS approach for both ECSIs availabilities respectively. Moreover, through complexity analysis, the search complexity of the proposed scheme is proven to be significantly reduced compared with the exhaustive searching method. Simulation results verify the secrecy performance superiority of the proposed scheme over the traditional FD-DBJ method.",
An Empirical study on Predicting Blood Pressure using Classification and Regression Trees,"Blood pressure diseases have become one of the major threats to human health. Continuous measurement of blood pressure has proven to be a prerequisite for effective incident prevention. In contrast with the traditional prediction models with low measurement accuracy or long training time, non-invasive blood pressure measurement is a promising use for continuous measurement. Thus in this paper, classification and regression trees (CART) are proposed and applied to tackle the problem. Firstly, according to the characteristics of different information, different CART models are constructed. Secondly, in order to avoid the over-fitting problem of these models, the cross-validation method is used for selecting the optimum parameters so as to achieve the best generalization of these models. Based on the biological data collected from CM400 monitor, this approach has achieved better performance than the common existing models such as linear regression, ridge regression, the support vector machine and neural network in terms of accuracy rate, root mean square error, deviation rate, Theil IC, and the required training time is also comparatively less. With increasing data, the accuracy rate of predicting systolic blood pressure and diastolic blood pressure by CART exceeds 90%, and the training time is less than 0.5s.",
A Location-Query-Browse Graph for Contextual Recommendation,"Traditionally, recommender systems modelled the physical and cyber contextual influence on people’s moving, querying, and browsing behaviors in isolation. Yet, searching, querying, and moving behaviors are intricately linked, especially indoors. Here, we introduce a tripartite location-query-browse graph (LQB) for nuanced contextual recommendations. The LQB graph consists of three kinds of nodes: locations, queries, and Web domains. Directed connections only between heterogeneous nodes represent the contextual influences, while connections of homogeneous nodes are inferred from the contextual influences of the other nodes. This tripartite LQB graph is more reliable than any monopartite or bipartite graph in contextual location, query, and Web content recommendations. We validate this LQB graph in an indoor retail scenario with extensive dataset of three logs collected from over 120,000 anonymized, opt-in users over a 1-year period in a large inner-city mall in Sydney, Australia. We characterize the contextual influences that correspond to the arcs in the LQB graph, and evaluate the usefulness of the LQB graph for location, query, and Web content recommendations. The experimental results show that the LQB graph successfully captures the contextual influence and significantly outperforms the state of the art in these applications.",
Cooperative Data Aggregation and Dynamic Resource Allocation for Massive Machine Type Communication,"The accommodation of massive machine type communication (mMTC) in cellular networks brings up serious technical challenges due to concurrent massive access of MTC devices. These challenges may further be aggravated by the presence of delay tolerant and intolerant services in an MTC network. This article proposes a cooperative data aggregation (CDA) scheme by employing fixed data aggregator (FDA) and multiple mobile data aggregators (MDAs) to cater MTC devices having variable quality of service (QoS) requirements. In this vein, a distributedMDA selection algorithm is also proposed to designate an appropriate user equipment as aggregator. The proposed CDA scheme effectively caters the massive access and provides ubiquitous availability of the aggregating devices in the MTC network. In addition, the limited channel resources impel an FDA to schedule resources besides data aggregation. Therefore, a resource allocation scheme is also proposed to dynamically allocate channels to the MTC devices subject to their QoS requirements. The proposed resource scheduling scheme ensures that transmission requests from delay intolerant MTC devices are contented on priority basis. The proposed CDA and dynamic resource scheduling schemes are analyzed and compared with the existing data aggregation and resource scheduling schemes, respectively. The numerical results corroborate that our proposed CDA scheme in conjunction with dynamic resource allocation improves the outage probability, energy efficiency and system capacity by 30%, 25% and 44%, respectively as compared to the single FDA scheme.",
Reverse Approximate Nearest Neighbor Queries,"Given a set of facilities and a set of users, a reverse nearest neighbors (RNN) query retrieves every user
u
for which the query facility
q
is its closest facility. Since
q
is the closest facility to
u
, the user
u
is said to be influenced by
q
. In this paper, we propose a relaxed definition of influence where a user
u
is said to be influenced by not only its closest facility but also every other facility that is almost as close to
u
as its closest facility is. Based on this definition of influence, we propose reverse approximate nearest neighbors (RANN) queries. Formally, given a value
x>1
, an RANN query
q
returns every user
u
for which
dist(u,q)≤x×NNDist(u)
where
NNDist(u)
denotes the distance between a user
u
and its nearest facility, i.e.,
q
is an approximate nearest neighbor of
u
. In this paper, we study both snapshot and continuous versions of RANN queries. In a snapshot RANN query, the underlying data sets do not change and the results of a query are to be computed only once. In the continuous version, the users continuously change their locations and the results of RANN queries are to be continuously monitored. Based on effective pruning techniques and several non-trivial observations, we propose efficient RANN query processing algorithms for both the snapshot and continuous RANN queries. We conduct extensive experiments on both real and synthetic data sets and demonstrate that our algorithm for both snapshot and continuous queries are significantly better than the competitors.",
Efficient Recommendation of Aggregate Data Visualizations,"Data visualization is a common and effective technique for data exploration. However, for complex data, it is infeasible for an analyst to manually generate and browse all possible visualizations for insights. This observation motivated the need for automated solutions that can effectively recommend such visualizations. The main idea underlying those solutions is to evaluate the utility of all possible visualizations and then recommend the top-k visualizations. This process incurs high data processing cost, that is further aggravated by the presence of numerical dimensional attributes. To address that challenge, we propose novel view recommendation schemes, which incorporate a hybrid multi-objective utility function that captures the impact of numerical dimension attributes. Our first scheme, Multi-Objective View Recommendation for Data Exploration (MuVE), adopts an incremental evaluation of our multi-objective utility function, which allows pruning of a large number of low-utility views and avoids unnecessary objective evaluations. Our second scheme, upper MuVE (uMuVE), further improves the pruning power by setting the upper bounds on the utility of views and allowing interleaved processing of views, at the expense of increased memory usage. Finally, our third scheme, Memory-aware uMuVE (MuMuVE), provides pruning power close to that of uMuVE, while keeping memory usage within a pre-specified limit.",
Anatomical Landmark based Deep Feature Representation for MR Images in Brain Disease Diagnosis,"Most automated techniques for brain disease diagnosis utilize hand-crafted (e.g., voxel-based or region-based) biomarkers from structural magnetic resonance (MR) images as feature representations. However, these hand-crafted features are usually high-dimensional or require regions-of-interest (ROIs) defined by experts. In addition, because of the possibly heterogeneous property between the hand-crafted features and the subsequent model, existing methods may lead to sub-optimal performances in brain disease diagnosis. In this paper, we propose a landmark based deep feature learning (LDFL) framework to automatically extract patch-based representation from MRI for automatic diagnosis of Alzheimer's disease (AD). We first identify discriminative anatomical landmarks from MR images in a data-driven manner, and then propose a convolutional neural network (CNN) for patch-based deep feature learning. We have evaluated the proposed method on subjects from three public datasets, including the Alzheimer's Disease Neuroimaging Initiative (ADNI-1), ADNI-2, and the Minimal Interval Resonance Imaging in Alzheimer's Disease (MIRIAD) dataset. Experimental results of both tasks of brain disease classification and MR image retrieval demonstrate that the proposed LDFL method improves the performance of brain disease classification and MR image retrieval.",
Context-Aware Answer Sentence Selection With Hierarchical Gated Recurrent Neural Networks,"In this paper, we study the task of reading comprehension style answer sentence selection that aims to select the best sentence from a given passage to answer a question. Unlike most previous works that match the question and each candidate sentence separately, we observe that the context information among sentences in the same passage plays a vital role in this task. We propose modeling context information with hierarchical gated recurrent neural networks. Specifically, we first apply a word level recurrent neural network to model the context independent matching between the question and each candidate sentence. We then employ a sentence level recurrent neural network to incorporate the context information among all candidate sentences. Moreover, we introduce the gate mechanism to select matching information before feeding into recurrent neural networks at both word and sentence level. Experiments on the WikiQA and SQuAD datasets show that our model outperforms state-of-the-art methods.",
Inadequate Software Testing Can Be Disastrous [Essay],"In October 2007, Activision published the Guitar Hero III: Legends of Rock video game for the Nintendo Wii gaming console. The extremely popular game sold 1.4 million copies during the first six days of its release. Guitar Hero III: Legends of Rock was a game that allowed players to play songs with a guitar-like controller. Music was the primary output of this highly entertaining game.",
Remote Sensing Image Fusion Based on Adaptively Weighted Joint Detail Injection,"Remote sensing image fusion based on the detail injection scheme consists of two steps: spatial details extraction and injection. The quality of the extracted spatial details plays an important role in the success of a detail injection scheme. In this paper, a remote sensing image fusion method based on adaptively weighted joint detail injection is presented. In the proposed method, the spatial details are first extracted from the multispectral (MS) and panchromatic (PAN) images through à trous wavelet transform and multiscale guided filter. Different from the traditional detail injection scheme, the extracted details are then sparsely represented to produce the primary joint details by dictionary learning from the sub-images themselves. To obtain the refined joint details information, we subsequently design an adaptive weight factor considering the correlation and difference between the previous joint details and PAN image details. Finally, the refined joint details are injected into the MS image using modulation coefficient to achieve the fused image. The proposed method has been tested on QuickBird, IKONOS and WorldView-2 datasets and compared to several state-of-the-art fusion methods in both subjective and objective evaluations. The experimental results indicate that the proposed method is effective and robust to images from various satellites sensors.",
Increasing Radiation Efficiency Using Antenna Shape Optimization Approach,"From the characteristic mode point of view of electrically small antennas, radiation efficiency normally decreases significantly if multiple modes are well excited. Amongst all these well-excited modes, the one with the lowest modal radiation efficiency determines how much actual radiation efficiency is guaranteed. Based on this idea, this letter proposes a method for improving antenna efficiency using shape optimization approach. The optimized shape can give rise to a high efficiency and an implied high gain after being excited using a single port whose location is easily selected by observing characteristic mode current distributions.",
On the Time Scales of Energy Arrival and Channel Fading in Energy Harvesting Communications,"In wireless communication systems powered by harvested energy, besides the channel fading, there exists another dimension of dynamics, i.e., energy arrival variation. In this paper, we propose a framework for analyzing the energy harvesting powered wireless transmissions where the energy arrival variations and the channel fading are of different timescales. The energy arrival rate changes every N(N ≥ 1) time slots, and the channel state changes every M(M ≥ 1) slots. We consider a power allocation problem among the time slots, which can be formulated as a Markov decision process (MDP) and solved by dynamic programming (DP) algorithm. For the special case that M = 1, a low-complexity two-stage DP algorithm is proposed, which decouples the original problem into inner and outer sub-problems. The inner problem deals with the power allocation in channel fading timescale in every N slots where the energy arrival rate keeps constant, and the outer problem deals with the energy management when the energy arrival rate changes. Numerical simulations show that the average data rate decreases as N or M increases, and the two-stage DP algorithm can perform close to the DP optimal algorithm.",
Model Predictive Power Control for Cooperative Vehicle Safety Systems,"In vehicular networking, the heavy traffic can cause channel congestion and hence degrade the tracking accuracy of cooperative vehicle safety systems. To overcome this problem, a dynamic packet reception model that integrates the packets reception rate and the vehicle density is proposed. Then, a trafficflow- based vehicle density estimation method is designed. This estimation method is capable of predicting the vehicle density in the scenario where there exist strong interactions among vehicles. Based on the vehicle density method, a dynamical transmission power control strategy is developed. This transmission power control strategy employs Model Predictive Control to make the optimal control decisions based on the estimated vehicle density. Experimental analyses demonstrate that the dynamical power control strategy can greatly enhance the vehicle tracking performance of cooperative vehicle safety systems under dynamical traffic situation.","Power control,
Estimation,
Real-time systems,
Vehicle dynamics,
Standards,
Loss measurement,
Optimization"
Covert Verb Reading Contributes to Signal Classification of Motor Imagery in BCI,"Motor imagery is widely used in the brain–computer interface (BCI) systems that can help people actively control devices to directly communicate with the external world, but its training and performance effect is usually poor for normal people. To improve operators’ BCI performances, here we proposed a novel paradigm, which combined the covert verb reading in the traditional motor imagery paradigm. In our proposed paradigm, participants were asked to covertly read the presented verbs during imagining right hand or foot movements referred by those verbs. EEG signals were recorded with both our proposed paradigm and the traditional paradigm. By the common spatial pattern method, we, respectively, decomposed these signals into spatial patterns and extracted their features used in the following classification of support vector machine. Compared with the traditional paradigm, our proposed paradigm could generate clearer spatial patterns following a somatotopic distribution, which led to more distinguishable features and higher classification accuracies than those in the traditional paradigm. These results suggested that semantic processing of verbs can influence the brain activity of motor imagery and enhance the mu event-related desynchronisation. The combination of semantic processing with motor imagery is therefore a promising method for the improvement of operators’ BCI performances.",
Speeding Up VM Startup by Cooperative VM Image Caching,"Virtual machine (VM) management is at the core of virtualized cloud data centers. Among others, how to reduce the startup delay of VMs is a key issue for improving user experience and resource utility. In this paper, we study this issue by jointly considering VM placement and VM image caching. We formulate the joint placement problem and design several joint algorithms, including both online and offline algorithms, to speed up VM startup. In our design, we adopt the cooperative caching approach, where image cache copies are shared among physical machines (PMs) so as to reduce image retrieval time. The key point of our algorithms lies in how to appropriately place VM image cache among PMs so as to speed up VM startup as much as possible. The proposed algorithms are evaluated by extensive simulations via SimGrid. The results show that our algorithms can achieve shorter startup delay in most cases, compared with existing ones.",
ConPredictor: Concurrency Defect Prediction in Real-World Applications,"Concurrent programs are difficult to test due to their inherent non-determinism. To address this problem, testing often requires the exploration of thread schedules of a program; this can be time-consuming when applied to real-world programs. Software defect prediction has been used to help developers find faults and prioritize their testing efforts. Prior studies have used machine learning to build such predicting models based on designed features that encode the characteristics of programs. However, research has focused on sequential programs; to date, no work has considered defect prediction for concurrent programs, with program characteristics distinguished from sequential programs. In this paper, we present ConPredictor, an approach to predict defects specific to concurrent programs by combining both static and dynamic program metrics. Specifically, we propose a set of novel static code metrics based on the unique properties of concurrent programs. We also leverage additional guidance from dynamic metrics constructed based on mutation analysis. Our evaluation on four large open source projects shows that ConPredictor improved both within-project defect prediction and cross-project defect prediction compared to traditional features.",
G-CRS: GPU Accelerated Cauchy Reed-Solomon Coding,"Recently, erasure coding has been extensively deployed in large-scale storage systems to replace data replication. With the increase in disk I/O throughput and network bandwidth, the performance of erasure coding becomes a major bottleneck of erasure-coded storage systems. In this paper, we propose a graphics processing unit (GPU)-based implementation of erasure coding named G-CRS, which employs the Cauchy Reed-Solomon (CRS) code, to overcome the aforementioned bottleneck. To maximize the coding performance of G-CRS, we designed and implemented a set of optimization strategies, such as a compact structure to store the bitmatrix in GPU constant memory, efficient data access through shared memory, and decoding parallelism, to fully utilize the GPU resources. In addition, we derived a simple yet accurate performance model to demonstrate the maximum coding performance of G-CRS on GPU. We evaluated the performance of G-CRS through extensive experiments on modern GPU architectures such as Maxwell and Pascal, and compared with other state-of-the-art coding libraries. The evaluation results revealed that the throughput of G-CRS was 10 times faster than most of the other coding libraries. Moreover, G-CRS outperformed PErasure (a recently developed, well optimized CRS coding library on the GPU) by up to 3 times in the same architecture.",
Building Novel VHF-based Wireless Sensor Networks for the Internet of Marine Things,"Traditional marine monitoring systems such as oceanographic and hydrographic research vessels use either wireless sensor networks with a limited coverage, or expensive satellite communication that is not suitable for small and midsized vessels. This paper proposes a novel Internet of Marine Things (IoMaT) data acquisition and cartography system in the marine environment using Very High Frequency (VHF) available on the majority of ships. The proposed system is equipped with many sensors such as sea depth, temperature, wind speed and direction, and the collected data is sent to 5G edge cloudlets connected to sink/base station nodes on shore. The sensory data is ultimately aggregated at a central cloud on the internet to produce up to date cartography systems. Several observations and obstacles unique to the marine environment have been discussed and feed into the solutions presented. The impact of marine sparsity on the network is examined and a novel hybrid Mobile Ad-hoc/Delay Tolerant routing protocol (MADNET) is proposed to switch automatically between Mobile Ad-hoc Network (MANET) and Delay Tolerant Network (DTN) routing according to the network connectivity. The low rate data transmission offered by VHF radio has been investigated in terms of the network bottlenecks and the data collection rate achievable near the sinks. A data synchronization and transmission approach has also been proposed at the 5G network core using Information Centric Networks (ICN).",
AARF: Any-Angle Routing for Flow-Based Microfluidic Biochips,"Flow-based microfluidic biochips are promising with significant applications for automating and miniaturizing laboratory procedures in biochemistry. Automated design methods for flow-based microfluidic biochips are becoming increasingly important due to the advancement in both integration scale and design complexity for complicated biochemical applications. Though the multilayer soft lithography fabrication provides flexibility to route both flow and control channels in any angle, existing routing algorithms still adopt Manhattan routing metrics, which design channel in either vertical or horizontal direction only. Moreover, based on the computational fluid dynamics (CFD) analysis, rectilinear channels with 90° bends have the following issues: (1) reduced the fluidic flow rate, which degrades the performance of the biochip and may even result in the erroneous outcome of the whole procedure, and (2) increased pressure at the right-angle bend, which negatively affects the reliability of the biochip. To fully utilize the routing flexibility, this paper proposes the first any-angle routing algorithm for flow-based microfluidic biochip, called AARF. Computational simulation results show that compared with traditional Manhattan routing method, the proposed AARF significantly improves the total wirelength and total effective wirelength (considering the turning angles) by 17.11% and 35.91%, respectively, which prove the effectiveness of the AARF routing flow.",
Online Aggregation of the Forwarding Information Base: Accounting for Locality and Churn,"This paper studies the problem of compressing the forwarding information base (FIB), but taking a wider perspective. Indeed, FIB compression goes beyond sheer compression, as the gain in memory use obtained from the compression has consequences on the updates that will have to be applied to the compressed FIB. We are interested in the situation where forwarding rules can change over time, e.g., due to border gateway protocol (BGP) route updates. Accordingly, we frame FIB compression as an online problem and design competitive online algorithms to solve it. In contrast to prior work which mostly focused on static optimizations, we study an online variant of the problem where routes can change over time and where the number of updates to the FIB is taken into account explicitly. The reason to consider this version of the problem is that leveraging temporal locality while accounting for the number of FIB updates helps to keep routers CPU load low and reduces the number of FIB updates to be transferred, e.g., from the network-attached software-defined network controller to a remote switch. This paper introduces a formal model which is an interesting generalization of several classic online aggregation problems. Our main contribution is an O(w)-competitive algorithm, where w is the length of an IP address. We also derive a lower bound which shows that our result is asymptotically optimal within a natural class of algorithms, based on so-called sticks.",
A Decision Support System for Wind Power Production,"Renewable energy production is constantly growing worldwide, and some countries produce a relevant percentage of their daily electricity consumption through wind energy. Therefore, decision support systems that can make accurate predictions of wind-based power production are of paramount importance for the traders operating in the energy market and for the managers in charge of planning the nonrenewable energy production. In this paper, we present a decision support system that can predict electric power production, estimate a variability index for the prediction, and analyze the wind farm (WF) production characteristics. The main contribution of this paper is a novel system for long-term electric power prediction based solely on the weather forecasts; thus, it is suitable for the WFs that cannot collect or manage the real-time data acquired by the sensors. Our system is based on neural networks and on novel techniques for calibrating and thresholding the weather forecasts based on the distinctive characteristics of the WF orography. We tuned and evaluated the proposed system using the data collected from two WFs over a two-year period and achieved satisfactory results. We studied different feature sets, training strategies, and system configurations before implementing this system for a player in the energy market. This company evaluated the power production prediction performance and the impact of our system at ten different WFs under real-world conditions and achieved a significant improvement with respect to their previous approach.",
Classification and Mapping of Adaptive Security for Mobile Computing,"Context: Mobile computing has emerged as a disruptive technology that has empowered its users with portable, connected and context-aware computation. However, issues such as resource poverty, energy efficiency and specifically data security and privacy represent critical challenges for mobile computing. Objective: The objective of this work is to systematically identify, taxonomically classify and map the state-of-research on adaptive security (a.k.a. self-protection) for mobile computing. Methodology: We followed evidence based software engineering method to conduct a systematic mapping study of 43 qualitatively selected studies - published from 2003 to 2017 - on adaptive security for mobile computing. Results and Conclusions: Classification and mapping of the research highlights three prominent themes that support adaptive security for (i) Mobile Device Data and Resources, (ii) Mobile to Mobile Communication, and (iii) Mobile to Server Communication. Mapping analysis suggests that security of mobile device data and resources is the most researched theme. The mapping study highlights that active and futuristic research are primarily focused on security as a service, whereas; the frequent research challenges relate to self-protecting mobile devices, user-driven privacy decisions and context-aware security. The results of the mapping study facilitate knowledge transfer that can benefit researchers and practitioners to understand the role of adaptive and context-aware security in mobile computing environments.","Security,
Mobile computing,
Mobile handsets,
Mobile communication,
Servers,
Data privacy,
Systematics"
Film mood and its quantitative determinants in different types of scenes,"Films elicit emotions in viewers by infusing the story they tell with an affective character or tone — in a word, a mood. Considerable effort has been made recently to develop computational methods to estimate affective content in film. However, these efforts have focused almost exclusively on style-based features while neglecting to consider different scene types separately. In this study, we investigated the quantitative determinants of film mood across scenes classified by their setting and use of sounds. We examined whether viewers could assess film mood directly in terms of hedonic tone, energetic arousal, and tense arousal; whether their mood ratings differed by scene type; and how various narrative and stylistic film attributes as well as low- and high-level computational features related to the ratings. We found that the viewers were adept at assessing film mood, that sound-based scene classification brought out differences in the mood ratings, and that the low- and high-level features related to different mood dimensions. The study showed that computational film mood estimation can benefit from scene type classification and the use of both low- and high-level features. We have made our clip assessment and annotation data as well as the extracted computational features publicly available.","Mood,
Estimation,
Speech,
Visualization,
Feature extraction,
Cameras"
Modeling skull-face anatomical/morphological correspondence for craniofacial superimposition-based identification,"Craniofacial superimposition (CFS) is a forensic identification technique which studies the anatomical and morphological correspondence between a skull and a face. It involves the process of overlaying a variable number of facial images with the skull. This technique has great potential since nowadays the wide majority of the people have photographs where their faces are clearly visible. In addition, the skull is a bone that hardly degrades under the effect of fire, humidity, temperature changes, etc. Three consecutive stages for the CFS process have been distinguished: the acquisition and processing of the materials; the skull-face overlay; and the decision making. This final stage consists of determining the degree of support for a match based on the previous overlays. The final decision is guided by different criteria depending on the anatomical relations between the skull and the face. In previous approaches, we proposed a framework for automating this stage at different levels taking into consideration all the information and uncertainty sources involved. In this study, we model new anatomical skull-face regions and we tackle the last level of the hierarchical decision support system. For the first time, we present a complete system which provides a final degree of craniofacial correspondence. Furthermore, we validate our system as an automatic identification tool analyzing its capabilities in closed (known information or a potential list of those involved) and open lists (little or no idea at first who may be involved) and comparing its performance with the manual results achieved by experts, obtaining a remarkable performance. The proposed system has been demonstrated to be valid for sortlisting a given data set of initial candidates (in 62,5% of the cases the positive one is ranked in the first position) and to serve as an exclusion method (97,4% and 96% of true negatives in training and test, respectively).","Face,
Decision making,
Decision support systems,
Computational modeling,
Forensics,
Bones,
Uncertainty"
Virtual Network Function Scheduling: A Matching Game Approach,"Network function virtualization is a promising technique for telecom providers to efficiently manage network services at low cost. However, existing works mainly focus on resource allocation and thus leave behind an important issue: the virtual network function (VNF) scheduling. Current approaches, e.g., round-robin scheduling or heuristic algorithms, still expose some unsolved issues, such as high computational cost and inability to perform online scheduling. In this letter, we propose a matching-based algorithm to solve the NP-hard VNF scheduling problem. This approach can guarantee a stable scheduling, in which all network services are satisfied with the assignment. Finally, the effectiveness of our method is verified through numerical evaluation, showing that our approach can increase the number of completed VNFs by 36.8% compared with the current round-robin method.",
Heterogeneous Metric Learning of Categorical Data with Hierarchical Couplings,"Learning appropriate metric is critical for effectively capturing complex data characteristics. The metric learning of categorical data with hierarchical coupling relationships and local heterogeneous distributions is very challenging yet rarely explored. This paper proposes a Heterogeneous mEtric Learning with hIerarchical Couplings, HELIC, for this type of categorical data. HELIC captures both low-level value-to-attribute and high-level attribute-to-class hierarchical couplings, and reveals the intrinsic heterogeneities embedded in each level of couplings. Theoretical analyses of the effectiveness and generalization error bound verify that HELIC effectively represents the above complexities. Extensive experiments on 30 data sets with diverse characteristics demonstrate that HELIC-enabled classification significantly enhances the accuracy (up to 40.93%), compared with five state-of-the-art baselines.","Couplings,
Frequency measurement,
Kernel,
Complexity theory,
Indexes"
A Reconfigurable Cache for Efficient Use of Tag RAM as Scratch-Pad Memory,"The cache memory has been a predominant component in modern chips, easily taking up more than 50% of the silicon area. It is then desirable to make the cache memory flexible for different needs. Therefore, many modern processor chips allow users to configure a part of the cache memory as the scratch-pad memory (SPM), a high-speed internal memory for rapid data access. However, such approach uses only the data RAM of the cache memory while leaving the tag RAM unused and thus wasting its capacity. This paper presents a cache organization, called Tag-SPM architecture, which allows the tag RAM to be used as the SPM and thus increases its capacity. It is accomplished with small Tag/Data-SPM controllers and four additional multiplexers in the cache organization. The proposed Tag-SPM architecture has been implemented with an academic ARM-based microprocessor with 4-/4-kB four-way set-associative instruction/data caches at the register transfer level level. Experiments show that the proposed architecture boosts the SPM capacity by 12.5% and requires only 0.08% area (434 gates) overhead without impairing the cache's circuit speed in TSMC's 90-nm standard cell implementation. Furthermore, the power overhead is negligible. When the Tag-SPM architecture is applied to typical cache systems, such as in ARM's Cortex-A5 and Cortex-A53 processors, additional 12.5% SPM space per way can also be gained in both cases. The analyses show that our Tag-SPM architecture is a highly cost-effective way to boost the SPM space.","Random access memory,
Registers,
Organizations,
Cache memory,
Memory management,
Indexes"
Towards Memory-Efficient Allocation of CNNs on Processing-in-Memory Architecture,"In contrast to conventional computing-centric applications, CNNs are known to be both computationally and memory intensive. The emerging Processing-in-Memory (PIM) alleviates the memory bottleneck by integrating both processing elements and memory into a 3D-stacked architecture. Although this architecture can offer near-data processing to reduce data movement, memory is still a limiting factor of the entire system. We observe that an unsolved key challenge is how to efficiently allocate convolutions to 3D-stacked PIM to combine the advantages of both neural and computational processing.",
Providing Task Allocation and Secure Deduplication for Mobile Crowdsensing via Fog Computing,"Mobile crowdsensing enables a crowd of individuals to cooperatively collect data for special interest customers using their mobile devices. The success of mobile crowdsensing largely depends on the participating mobile users. The broader participation, the more sensing data are collected; nevertheless, the more replicate data may be generated, thereby bringing unnecessary heavy communication overhead. Hence it is critical to eliminate duplicate data to improve communication efficiency, a.k.a., data deduplication. Unfortunately, sensing data is usually protected, making its deduplication challenging. In this paper, we propose a fog-assisted mobile crowdsensing framework, enabling fog nodes to allocate tasks based on user mobility for improving the accuracy of task assignment. Further, a fog-assisted secure data deduplication scheme (Fo-SDD) is introduced to improve communication efficiency while guaranteeing data confidentiality. Specifically, a BLS-oblivious pseudo-random function is designed to enable fog nodes to detect and remove replicate data in sensing reports without exposing the content of reports. To protect the privacy of mobile users, we further extend the Fo-SDD to hide users' identities during data collection. In doing so, Chameleon hash function is leveraged to achieve contribution claim and reward retrieval for anonymous mobile users. Finally, we demonstrate that both schemes achieve secure, efficient data deduplication.",
Temporal Change Detection in SAR Images Using Log Cumulants and Stacked Autoencoder,"This letter proposes a change detection algorithm for damage assessment caused by fires in Ireland using Sentinel 1 data. The novelty, in this letter, is a feature extraction within tunable
Q
discrete wavelet transform (TQWT) using higher order log cumulants of fractional Fourier transform (FrFT), which were fed into a stacked autoencoder (SAE) to distinguish changed and unchanged areas. The extracted features were used to train the SAE layerwise using an unsupervised learning algorithm. After training the decoding layer was replaced by a logistic regression layer to perform supervised fine-tuning and classification. The proposed algorithm was compared with the algorithm that used log cumulants of FrFT within the oriented dual-tree wavelet transform using support vector machine (SVM) classifier. The experimental results showed that the proposed combination of algorithms decreased the overall error (OE) for real synthetic aperture radar images by 6%, when TQWT was used instead of oriented dual-tree wavelet transform and OE was decreased by another 5% when SAE was used instead of the SVM classifier.",
Large-Scale Polarization-Insensitive Silicon Photonic MEMS Switches,"We report on 50x50 silicon photonic MEMS switches with reduced polarization dependence. The switches make use of two-level waveguide crossbars and MEMS-actuated adiabatic couplers. Simulations indicate that by eliminating all polarization-dependent elements (e.g., waveguide crossings), low polarization-dependent losses (<1 dB) can be realized. An experimental prototype switch was fabricated by bonding two silicon-on-insulator (SOI) wafers. A polarization-dependent loss of 8.5 dB and a polarization-dependent delay of 44 ps were measured. The extinction ratios are greater than 40 dB for both polarizations. With improved fabrication, total on-chip loss < 2 dB can potentially be achieved for both polarizations.","Optical switches,
Optical waveguides,
Couplers,
Silicon,
Optical device fabrication,
Photonics"
Optimistic DRX for Machine-type Communications in LTE-A Network,"In long term evolution-advanced (LTE-A) network, the third generation partnership project (3GPP) has proposed machine-type communications (MTC) as a new paradigm where devices transfer data among themselves with limited human interaction. For devices in MTC, power saving has become an important issue because many devices are powered by battery. In LTE-A network, the standard power saving mechanism, called discontinuous reception (DRX), is designed for normal mobile users, not for MTC devices. In this paper, we propose optimistic DRX (ODRX) mechanism to be suitable for MTC devices. ODRX considers the radio resource control (RRC) connection release and re-establishment to save more power. We also introduce the optimistic flag to allow more longer sleep periods. Analytical and simulation models are proposed to investigate performance of ODRX, and ODRX is then compared with the standard DRX and dynamic DRX (DDRX) through simulation experiments. The results show that ODRX outperforms standard DRX and DDRX by gaining significant extra power saving with little extra wake up latency. We also propose guidelines to configure ODRX parameters.",
Enhanced Hidden Moving Target Defense in Smart Grids,"Recent research has proposed a moving target defense (MTD) approach that actively changes transmission line susceptance to preclude stealthy false data injection (FDI) attacks against the state estimation of a smart grid. However, existing studies were often conducted under a weak adversarial setting, in that they ignore the possibility that alert attackers can also try to detect the activation of MTD before they launch the FDI attacks. We call this new threat as Parameter Confirming-First (PCF) FDI. To improve the stealthiness of MTD, we propose a hidden MTD approach that cannot be detected by the attackers and prove its equivalence to an MTD that maintains the power flows of the whole grid. Moreover, we analyze the completeness of MTD and show that any hidden MTD is incomplete in that FDI attacks may bypass the hidden MTD opportunistically. This result suggests that the stealthiness and completeness are two conflicting goals in MTD design. Finally, we propose an approach to enhancing the hidden MTD against a class of highly structured FDI attacks. We also discuss the MTD’s operational costs under the dc and ac models. We conduct simulations to show the effectiveness of the hidden MTD against PCF-FDI attacks under realistic settings.",
A Tensor-train Deep Computation Model for Industry Informatics Big Data Feature Learning,"The deep computation model has been proved to be effective for big data hierarchical feature and representation learning in the tensor space. However, it requires expensively computational resources including high-performance computing units and large memory to train a deep computation model with a large number of parameters, limiting its effectiveness and efficiency for industry informatics big data feature learning. In this paper, a tensor-train deep computation model is presented for industry informatics big data feature learning. Specially, the tensor-train network is used to compress the parameters significantly by converting the dense weight tensors into the tensor-train format. Furthermore, an learning algorithm is implemented based on gradient descent and back-propagation to train the parameters of the presented tensor-train deep computation model. Extensive experiments are carried on STL-10, CUAVE and SNAE2 to evaluate the presented model in terms of the approximation error, classification accuracy drop, parameters reduction and speedup. Results demonstrate that the presented model can improve the training efficiency and save the memory space greatly for the deep computation model with small accuracy drops, proving its potential for industry informatics big data feature learning.","Tensile stress,
Computational modeling,
Data models,
Big Data,
Informatics,
Industries,
Training"
An Adaptive Droupout Deep Computation Model for Industrial IoT Big Data Learning with Crowdsourcing to Cloud Computing,"Deep computation, as an advanced machine learning model, has achieved the sate-of-the-art performance for feature learning on big data in industrial Internet of Things. However, the current deep computation model usually suffers from over-fitting due to the lack of public available labeled training samples, limiting its performance for big data feature learning. Motivated by the idea of active learning, an adaptive dropout deep computation model with crowdsourcing to cloud is proposed for industrial IoT big data feature learning in this paper. First, a distribution function is designed to set the dropout rate for each hidden layer to prevent over-fitting for the deep computation model. Furthermore, the outsourcing selection algorithm based on maximum entropy is employed to choose appropriate samples from the training set to crowdsource on the cloud platform. Finally, an improved SLME scheme is presented to aggregate answers given by human workers and to update the parameters of the adaptive dropout deep computation model simultaneously. Extensive experiments are conducted to evaluate the performance of the presented model by comparing with the dropout deep computation model and other state-of-the-art crowdsourcing algorithms. The results demonstrate that the proposed model can prevent over-fitting effectively and aggregate the labeled samples to train the parameters of the deep computation model with crowdsouring for industrial IoT big data feature learning.","Computational modeling,
Adaptation models,
Tensile stress,
Data models,
Big Data,
Training,
Cloud computing"
PRTS: A Passive RFID Real-Time Tracking System under the Conditions of Sparse Measurements,"In many logistics and manufacturing applications, tracking of moving objects with Radio frequency identification (RFID) tags on a conveyor belt is a premise for many other processes, e.g., sorting procedures, stamping IDs in a surveillance video. However, in complex industrial environments, a sharp decline in Tag Read Record (TRR) often results in severe spatial ambiguity. In this type of scenarios, existing systems cannot work effectively owing to the prevalent existence of noise. This paper proposes a Passive RFID Real-time Tracking System (PRTS) with tolerance of a small TRR, which is designed for tracking RFIDtagged mobile objects. We use detailed deduction to convert the tracking problem into a sparse signal reconstruction one. To solve this problem, we devise a novel Normal Sparse Signal Reconstruction method based on Greedy Pursuit by making the best of the available prior knowledge and further ameliorate it via calibration of the phase deviation from frequency and angleof- arrival responses. Furthermore, we leverage the simplified Particle Filters to facilitate the real-time tracking of mobile objects on conveyor belts. We implement the prototype PRTS with commercial-off-the-shelf (COTS) RFID devices and evaluate it in various scenarios. Experimental results demonstrate that PRTS achieves a mean relative error of 7 cm under the conditions of extremely sparse measurements.","Belts,
Radiofrequency identification,
Antennas,
Phase measurement,
Radar tracking,
Real-time systems"
CryptCloud+: Secure and Expressive Data Access Control for Cloud Storage,"Secure cloud storage, an emerging cloud service, guarantees the confidentiality of outsourced data while providing flexible data access control for cloud users whose data are out of their physical control. Ciphertext-Policy Attribute-Based Encryption (CP-ABE) is one of the promising secure mechanisms to support fine-grained access control on encrypted data in cloud settings. However, due to its inherent ""all-or-nothing"" decryption control characteristic, there is a risk for the misuse of access credentials. In this paper, we consider the two main types of access credential misuse, namely: semi-trusted authority's illegal access credential (re-)distribution, and cloud user's illegal access credential leakage. To mitigate these two types of access credential misuse, we propose the first accountable authority revokable CP-ABE based cloud storage system with white-box traceability and auditing, referred to as CryptCloud+. We also prove the security of our system and present the experimental results to demonstrate the utility of our system.",
SAT-Based Fault Equivalence Checking in Functional Safety Verification,"Detecting equivalence classes of injected faults for functional verification of electronic systems is an important task because it helps reducing the number of faults to qualify a verification environment, and hence improves the performance of qualification process and the validation time required for large-scale electronic systems. This paper describes an efficient way of detecting equivalent injected faults in a mapped netlist in order to speedup the qualification process of a verification environment for functional safety. The presented fault models include general faults resulting in arbitrary functional changes, in addition to conventional stuck-at faults. The solution is based on structural pruning and functional analysis performed by a synergistic combination of iterative Boolean satisfiability and guided simulation. It should be noted that traditional brute-force-like methods would take many hours or even days to identify thousands of equivalent functional faults injected to a small circuit with only hundred of cells. The proposed approach can achieve excellent fault reduction ratios within few seconds for such small circuits. The implementation also scales well for the largest ISCAS’89 and OpenCores benchmarks containing more than 35K gates and 490K general functional faults.","Circuit faults,
Safety,
ISO Standards,
Automotive engineering,
Solid modeling,
Boolean functions,
Logic gates"
A Cloud-based Decision Support System for Self-Healing in Distributed Automation Systems using Fault Tree Analysis,"Downtime is a key performance index for industrial automation systems. An industrial automation system achieves maximum productivity when its downtime is reduced to the minimum. One approach to minimize downtime is to predict system faults and recover from them automatically. A cloud-based decision support system is proposed for rapid problem identifications and to assist the self-management processes. By running multiple parallel simulations of control software with real-time inputs ahead of system time, faults could be detected and corrected automatically using autonomous industrial software agents. Fault trees, as well as control algorithms, are modeled using IEC 61499 function blocks that can be directly executed on both physical controllers and cloud services. A case study of water heating process is used to demonstrate the self-healing process supported by the cloud-based decision support system.",
Minimizing Controller Response Time Through Flow Redirecting in SDNs,"Software defined networking (SDN) is becoming increasingly prevalent for its programmability that enables centralized network configuration and management. With the growth of SDNs, a cluster of controllers cooperatively manages more and more switches/flows in a network to avoid the single-controller congestion/failure and improve the control-plane robustness. Under the architecture with multiple controllers, it is expected to minimize the maximum response time on these controllers to provide better QoS for users. To achieve this target, two previous methods are mainly used, the static scheme and the dynamic scheme. However, these methods may lead to an increase of the control-plane communication overhead/delay. In this paper, we propose to minimize the maximum response time on controllers through flow redirecting, which is implemented by installing wildcard rules on switches. We formulate the minimum controller response time problem, which takes the flow-table size and link capacity constraints into account, as an integer linear program, and prove its NP-Hardness. Two algorithms with bounded approximation factors are designed to solve this problem. We implement the proposed methods on our SDN testbed. The testing results and extensive simulation results show that our proposed algorithm can reduce the maximum controller response time by about 50%-80% compared with the static/dynamic methods under the same controller cost, or reduce the number of controllers by 30% compared with the dynamic method while preserving almost the same controller response time.","Control systems,
Time factors,
Algorithm design and analysis,
IEEE transactions,
Approximation algorithms,
Simulation,
Heuristic algorithms"
Robust Image Fingerprinting Based on Feature Point Relationship Mining,"Local feature points have been widely employed in robust image fingerprinting. One of their intrinsic advantages is their invariance under geometric transforms. However, their robustness against certain attacks that modify the positions of points, such as additive noising and blurring, is limited. In addition, local-feature-point-based approaches ignore the distribution of the feature points. In this paper, we harness feature point relationships, including local structures and global relevance, to overcome these limitations. In the relationship mining strategy, Delaunay triangulation is first applied to the feature points to capture their geometric structures. Subsequently, local structures are represented by searching for an independent set in the mapping graph constructed via Delaunay triangulation, whereas the global relevance is represented by the Laplacian of the graph. Finally, the local structures and global relevance are used as input to the quantization process of the image fingerprinting system. In the process of quantization, we propose an unsupervised quantization strategy called between-cluster distance-based quantization (BCDQ) to preserve the neighborhood structure between the binary fingerprint space and the original feature space. Experimental results show that the proposed method achieves effective performance under common modifications.",
Joint Trajectory and Power Optimization for UAV Relay Networks,"In this letter, we consider an unmanned aerial vehicle (UAV) relay network, where the UAV works as an amplify-and-forward relay. We optimize the trajectory of UAV, the transmit power of UAV, and the mobile device by minimizing the outage probability of this relay network. The analytical expression of outage probability is derived first. A closed-form low-complexity solution with joint trajectory design and power control is proposed to solve this non-convex problem. Simulation results show that the outage probability of the proposed solution is significantly lower than that of the fixed power relay and circle trajectory for the UAV relay.","Trajectory,
Power system reliability,
Probability,
Power control,
Unmanned aerial vehicles,
Relay networks (telecommunications)"
"4.32-pJ/b, Overlap-Free, Feedforward Edge-Combiner-Based Ultra-Wideband Transmitter for High-Channel-Count Neural Recording","We present an ultralow-power, ultra-wideband (UWB) transmitter (TX) in standard 65-nm CMOS processes. The TX consists of feedforward edge combiners and interpolators for ultralow-power operation and reliable pulse generation that is essential in UWB TXs. The implemented circuit avoids pulse overlapping without complicated calibrations and has achieved an energy efficiency of 4.32 pJ/b at 200-Mbps data rate. The TX is suitable for energy-constraint, high-data-rate applications such as wireless telemetry in implantable high-density neural recording interfaces.","Delays,
Feedforward systems,
Logic gates,
Baseband,
Wireless communication,
Transmitters,
FCC"
SnapMig: Accelerating VM Live Storage Migration by Leveraging the Existing VM Snapshots in the Cloud,"VM live storage migration is becoming increasingly important and indispensable in the current cloud data centers. Nevertheless, conventional VM migration approaches induce significant extra storage and network traffic to the source server that is already heavily loaded or scheduled for upgrade or repair. As a result, both the VM performance perceived by the application/user and the migration performance are degraded significantly. In this paper, we address this problem by proposing a novel scheme, called SnapMig, to improve the VM live storage migration efficiency and eliminate its performance impact on user applications at the source server by effectively leveraging the existing VM snapshots in backup servers. By outsourcing the task of transferring VM base image and snapshots to the destination server to backup servers, the source server only needs to migrate the latest state changes to the destination server, leading to simultaneous improvement on VM performance, migration time and multiple-VM migration efficiency. Our lightweight prototype implementation of the SnapMig scheme demonstrates that, compared with the state-of-the-art approaches, SnagMig can significantly reduce the migration time and improve the source-server VM performance at the same time. Moreover, the performance improvement provided by SnapMig becomes much more pronounced with multiple concurrent VM migrations.","Servers,
Cloud computing,
Performance evaluation,
Virtual machine monitors,
Maintenance engineering,
Electronic mail"
Graph Thumbnails: Identifying and Comparing Multiple Graphs at a Glance,"We propose Graph Thumbnails, small icon-like visualisations of the high-level structure of network data. Graph Thumbnails are designed to be legible in small multiples to support rapid browsing within large graph corpora. Compared to existing graph-visualisation techniques our representation has several advantages: (1) the visualisation can be computed in linear time; (2) it is canonical in the sense that isomorphic graphs will always have identical thumbnails; and (3) it provides precise information about the graph structure. We report the results of two user studies. The first study compares Graph Thumbnails to node-link and matrix views for identifying similar graphs. The second study investigates the comprehensibility of the different representations. We demonstrate the usefulness of this representation for summarising the evolution of protein-protein interaction networks across a range of species.","Visualization,
Layout,
Data visualization,
Proteins,
Measurement"
"EXIT Chart Aided Convergence Analysis of Recursive Soft
m
-Sequence Estimation in Nakagami-m Fading Channels","A delay of less than one millisecond is required by low-latency 5G wireless communication systems for supporting the ‘tactile’ Internet. Hence, conventional initial synchronisation cannot be readily employed because of its potentially excessive delay. In this paper, an EXtrinsic Information Transfer (EXIT) Chart assisted approach is used for the convergence analysis of
m
-sequences using Recursive Soft Sequence Estimation (RSSE) in the context of Nakagami-m fading channels. Explicitly, the novelty of our work is based on employing a new type of EXIT Charts operating without using interleavers. This is a challenge, because the original EXIT charts rely on the employment of long, high-delay interleavers for ensuring that the inputs to the decoders become uncorrelated. We then evaluate the performance of various classes of
m
-sequences with the aid of the proposed EXIT charts and demonstrate that the
m
-sequences generated by the lower-order polynomials maximise the mutual information more promptly with the aid of our RSSE scheme than those, which belong to a higher-order polynomial.",
Throughput Optimization in WLAN/Cellular Integrated Network Using Partially Overlapped Channels,"The wireless network that integrates a heterogeneous cellular network and a WLAN is referred to as a WLAN/cellular integrated network (WCIN). In a fourth generation WCIN, in order to achieve collision-avoidance between the LTE-A and IEEE 802.11 family, some interference-free mechanisms, such as carrier sense adaptive transmission and listen-before-talk, are proposed. However, the interference-free medium access mechanisms leave some unexploited partially overlapped 802.11ac channels, which result in a waste of spectrum. In this paper, we propose an interference-tolerant medium access method to optimize the WCIN throughput by utilizing those partially overlapped channels (POCs). First, we show the feasibility of enhancing WLAN throughput by utilizing 802.11ac POCs in a WCIN. Second, we mathematically model the partial overlap when an 802.11ac channel is partially overlapping with an LTE-A component carrier. Third, we propose an interference-tolerant medium access mechanism to optimize the WCIN throughput. The interference-tolerant one ensures that the interference to LTE-A users is tolerable in a given WCIN. Finally, we construct a hardware-in-the-loop testbed to evaluate and compare our proposed mechanism with three other state-of-the-art mechanisms. The experimental results show that our approach achieves at most 39% more throughput than one of the state-of-the-art collision-avoidance mechanisms regarding the entire WCIN.",
Hierarchical Matching Game for Service Selection and Resource Purchasing in Wireless Network Virtualization,"Wireless network virtualization is identified as one of the key enabling technologies to bring fifth-generation networks into fruition. In this letter, we model the service selection and resource purchasing problem as a two-stage combinatorial optimization problem. To solve this problem, we propose a hierarchical matching game-based scheme, which satisfies the efficient resource allocation and strict isolation requirements. Simulation results show that our proposal outperforms the fixed sharing approach by 32% and achieves up to 97% of performance obtained by the optimal approach (general sharing scheme) in terms of average sum rate.","Indium phosphide,
III-V semiconductor materials,
Resource management,
Games,
Manganese,
Contracts,
Virtualization"
"""Sampling"" as a Baseline Optimizer for Search-based Software Engineering","Increasingly, Software Engineering (SE) researchers use search-based optimization techniques to solve SE problems with multiple conflicting objectives. These techniques often apply CPU-intensive evolutionary algorithms to explore generations of mutations to a population of candidate solutions. An alternative approach, proposed in this paper, is to start with a very large population and sample down to just the better solutions. We call this method ""SWAY"", short for ""the sampling way"". This paper compares SWAY versus state-of-the-art search-based SE tools using seven models: five software product line models; and two other software process control models (concerned with project management, effort estimation, and selection of requirements) during incremental agile development. For these models, the experiments of this paper show that SWAY is competitive with corresponding state-of-the-art evolutionary algorithms while requiring orders of magnitude fewer evaluations. Considering the simplicity and effectiveness of SWAY, we, therefore, propose this approach as a baseline method for search-based software engineering models, especially for models that are very slow to execute.",
Exploiting Feature and Class Relationships in Video Categorization with Regularized Deep Neural Networks,"In this paper, we study the challenging problem of categorizing videos according to high-level semantics such as the existence of a particular human action or a complex event. Although extensive efforts have been devoted in recent years, most existing works combined multiple video features using simple fusion strategies and neglected the utilization of inter-class semantic relationships. This paper proposes a novel unified framework that jointly exploits the feature relationships and the class relationships for improved categorization performance. Specifically, these two types of relationships are estimated and utilized by imposing regularizations in the learning process of a deep neural network (DNN). Through arming the DNN with better capability of harnessing both the feature and the class relationships, the proposed regularized DNN (rDNN) is more suitable for modeling video semantics. We show that rDNN produces better performance over several state-of-the-art approaches. Competitive results are reported on the well-known Hollywood2 and Columbia Consumer Video benchmarks. In addition, to stimulate future research on large scale video categorization, we collect and release a new benchmark dataset, called FCVID, which contains 91,223 Internet videos and 239 manually annotated categories.",
Person Re-Identification by Camera Correlation Aware Feature Augmentation,"The challenge of person re-identification (re-id) is to match individual images of the same person captured by different non-overlapping camera views against significant and unknown cross-view feature distortion. While a large number of distance metric/subspace learning models have been developed for re-id, the cross-view transformations they learned are view-generic and thus potentially less effective in quantifying the feature distortion inherent to each camera view. Learning view-specific feature transformations for re-id (i.e., view-specific re-id), an under-studied approach, becomes an alternative resort for this problem. In this work, we formulate a novel view-specific person re-identification framework from the feature augmentation point of view, called Camera coR relation Aware Feature augmenTation (CRAFT). Specifically, CRAFT performs cross-view adaptation by automatically measuring camera correlation from cross-view visual data distribution and adaptively conducting feature augmentation to transform the original features into a new adaptive space. Through our augmentation framework, view-generic learning algorithms can be readily generalized to learn and optimize view-specific sub-models whilst simultaneously modelling view-generic discrimination information. Therefore, our framework not only inherits the strength of view-generic model learning but also provides an effective way to take into account view specific characteristics. Our CRAFT framework can be extended to jointly learn view-specific feature transformations for person re-id across a large network with more than two cameras, a largely under-investigated but realistic re-id setting. Additionally, we present a domain-generic deep person appearance representation which is designed particularly to be towards view invariant for facilitating cross-view adaptation by CRAFT. We conducted extensively comparative experiments to validate the superiority and advantages of our proposed framework over state-of-the-art competitors on contemporary challenging person re-id datasets.",
Watch-n-Patch: Unsupervised Learning of Actions and Relations,"There is a large variation in the activities that humans perform in their everyday lives. We consider modeling these composite human activities which comprises multiple basic level actions in a completely unsupervised setting. Our model learns high-level co-occurrence and temporal relations between the actions. We consider the video as a sequence of short-term action clips, which contains human-words and object-words. An activity is about a set of action-topics and object-topics indicating which actions are present and which objects are interacting with. We then propose a new probabilistic model relating the words and the topics. It allows us to model long-range action relations that commonly exist in the composite activities, which is challenging in previous works. We apply our model to the unsupervised action segmentation and clustering, and to a novel application that detects forgotten actions, which we call action patching. For evaluation, we contribute a new challenging RGB-D activity video dataset recorded by the new Kinect v2, which contains several human daily activities as compositions of multiple actions interacting with different objects. Moreover, we develop a robotic system that watches and reminds people using our action patching algorithm. Our robotic setup can be easily deployed on any assistive robots.",
Trouble Ticket Routing Models and Their Applications,"A trouble ticket is an important information carrier in system maintenance, which records problem symptoms, the resolving process, and resolutions. A critical challenge for the ticket management system is how to quickly deal with trouble tickets and fix problems. Thousands of tickets bouncing among multiple expert groups before being fixed will consume limited system maintenance resources and may also violate the service level agreement (SLA). Thus, trouble tickets should be routed to the right expert group as quickly as possible in order to reduce the processing delay. In this paper, to address the challenge in ticket routing, we exploit three different routing models by mining the combination of problem descriptions and resolution sequences from the historical resolved tickets, and develop the corresponding routing recommendation algorithms to determine the next expert group to solve the problem. To evaluate the performance of routing recommendation algorithms, we conduct extensive experiments on a real ticket data set. The experimental results show that the proposed models and algorithm can effectively shorten the mean number of steps to resolve (MSTR) with a high ratio of the number of successfully resolved tickets to the total number of tickets (RSR), especially for the long routing sequences generated from manual assignments. These models and algorithms have the potential of being used in a ticket routing recommendation engine to greatly reduce human intervention in the routing process.","Routing,
Maintenance engineering,
Data mining,
Problem-solving,
Analytical models,
Servers,
Manuals"
Data Quality Guided Incentive Mechanism Design for Crowdsensing,"In crowdsensing, appropriate rewards are always expected to compensate the participants for their consumptions of physical resources and involvements of manual efforts. While continuous low quality sensing data could do harm to the availability and preciseness of crowdsensing based services, few existing incentive mechanisms have ever addressed the issue of data quality. The design of quality based incentive mechanism is motivated by its potential to avoid inefficient sensing and unnecessary rewards. In this paper, we incorporate the consideration of data quality into the design of incentive mechanism for crowdsensing, and propose to pay the participants as how well they do, to motivate the rational participants to efficiently perform crowdsensing tasks. This mechanism estimates the quality of sensing data, and offers each participant a reward based on her effective contribution. We also implement the mechanism and evaluate its improvement in terms of quality of service and profit of service provider. The evaluation results show that our mechanism achieves superior performance when compared to general data collection model and uniform pricing scheme.",
Hardware Design of an Energy-Efficient High-Throughput Median Filter,"This paper presents a hardware design for an energy-efficient, high-speed, one-dimensional median filter. Existing architectures focus on operating speeds, thus resulting in redundant power dissipation. This paper presents an algorithm and mathematical model for controlling the clock signals attached to circuit by analyzing the behavior of the filter, which immobilizes the data in registers and reduces not only signal transitions but also switching activities, thereby reducing the total dynamic power consumption. Furthermore, the proposed architecture provides high-speed computation. A median result can be produced in each clock cycle, and the maximum operating frequency performance is nearly independent of the filter size. The proposed architecture uses 90-nm process technology and experimental results show that the proposed method is more energy efficient than existing designs. The power consumption is reduced by 25% on average.",
Eliminating Path Redundancy via Postconditioned Symbolic Execution,"Symbolic execution is emerging as a powerful technique for generating test inputs systematically to achieve exhaustive path coverage of a bounded depth. However, its practical use is often limited by path explosion because the number of paths of a program can be exponential in the number of branch conditions encountered during the execution. To mitigate the path explosion problem, we propose a new redundancy removal method called postconditioned symbolic execution. At each branching location, in addition to determine whether a particular branch is feasible as in traditional symbolic execution, our approach checks whether the branch is subsumed by previous explorations. This is enabled by summarizing previously explored paths by weakest precondition computations. Postconditioned symbolic execution can identify path suffixes shared by multiple runs and eliminate them during test generation when they are redundant. Pruning away such redundant paths can lead to a potentially exponential reduction in the number of explored paths. Since the new approach is computationally expensive, we also propose several heuristics to reduce its cost. We have implemented our method in the symbolic execution engine KLEE [1] and conducted experiments on a large set of programs from the GNU Coreutils suite. Our results confirm that redundancy due to common path suffix is both abundant and widespread in real-world applications.",
IGD Indicator-based Evolutionary Algorithm for Many-objective Optimization Problems,"Inverted Generational Distance (IGD) has been widely considered as a reliable performance indicator to concurrently quantify the convergence and diversity of multi-and many-objective evolutionary algorithms. In this paper, an IGD indicator-based evolutionary algorithm for solving many-objective optimization problems (MaOPs) has been proposed. Specifically, the IGD indicator is employed in each generation to select the solutions with favorable convergence and diversity. In addition, a computationally efficient dominance comparison method is designed to assign the rank values of solutions along with three newly proposed proximity distance assignments. Based on these two designs, the solutions are selected from a global view by linear assignment mechanism to concern the convergence and diversity simultaneously. In order to facilitate the accuracy of the sampled reference points for the calculation of IGD indicator, we also propose an efficient decomposition-based nadir point estimation method for constructing the Utopian Pareto front which is regarded as the best approximate Pareto front for real-world MaOPs at the early stage of the evolution. To evaluate the performance, a series of experiments is performed on the proposed algorithm against a group of selected state-of-the-art many-objective optimization algorithms over optimization problems with 8-, 15-, and 20-objective. Experimental results measured by the chosen performance metrics indicate that the proposed algorithm is very competitive in addressing MaOPs.",
Parasitic Effect Analysis in Memristor-Array-Based Neuromorphic Systems,"Neuromorphic systems using memristors as artificial synapses have attracted broad interest for energy-efficient computing applications. However, networks based on these purely passive devices can be affected by parasitic effects such as series resistance and sneak path problems. Here, we analyze the effects of parasitic factors on the performance of memristor-based neuromorphic systems. During vector-array multiplication, the line resistance can cause significant distortion of the output current and the activity of the corresponding neurons. An approach to compensate the line resistance effects based on an approximate model consisting of only few known parameters is proposed and shows excellent ability to capture the complex network behavior. During training and feature detection, the series resistance can cause significant degradation of the learned dictionary, with only a few dominant neurons being trained. Using a scaling factor based on the proposed simple model, these effects can be successfully mitigated, and the correct network operations can be restored. These results provide insight and practical measures on the parasitic effects for implementation of the neuromorphic system using memristor arrays.","Memristors,
Resistance,
Neuromorphics,
Neurons,
Distortion,
Integrated circuit modeling,
Switches"
An Efficient Channel Scanning Scheme With Dual-Interfaces for Seamless Handoff in IEEE 802.11 WLANs,"In this letter, we propose an efficient channel scanning scheme for IEEE 802.11 WLANs to reduce the handoff delay. The proposed scheme is fundamentally different from existing channel scanning schemes in that access points (APs), not mobile stations, switch channels. Specifically, each AP is equipped with dual wireless network interfaces, one of which is used for normal AP operations and the other is dedicated to the channel scanning assistance. In this circumstance, mobile stations do not perform active scans and stay on their operating channel, while APs switch channels and broadcast beacon frames by using the additional interface. Thus, mobile stations can maintain up-to-date information on neighboring APs without scanning other channels. Consequently, the service disruption during channel scanning is eliminated and the quality of ongoing data communication is not degraded. Our performance evaluation results show that the proposed scheme outperforms existing channel scanning schemes.","Delays,
Mobile communication,
Switches,
IEEE 802.11 Standard,
Authentication,
Data communication"
Reconfigurable Bandwidth Bandpass Filter With Enhanced Out-of-Band Rejection Using \pi -Section-Loaded Ring Resonator,"A novel ring resonator bandpass filter with reconfigurable bandwidth with central frequency at 2.4 GHz is demonstrated. Theoretical analysis for computing the resonant frequencies is shown, and the design approach of implementing \pi
-section stubs connected to the ring to obtain the bandwidth reconfigurability is explained. The use of reconfigurable \pi
-section allows the alteration of the stub’s effective width and therefore the filter’s bandwidth reconfigurability. p-i-n diodes are used as switching elements to achieve the narrowband/wideband response. Coupled line sections are used for suppressing the higher modes by generating out-of-band transmission zeros, resulting in a significantly enhanced out-of-band rejection. Measurements indicate that the 3-dB fractional bandwidth can be switched from 58.5% to 75% at a fixed center frequency of 2.4 GHz with an insertion loss better than 1.1 dB. Moreover, the −20-dB stopband performance is extended to 2.7f_{0}
.","Bandwidth,
Resonant frequency,
Impedance,
Optical ring resonators,
P-i-n diodes,
Passband,
Frequency control"
On the Construction of LDPC Codes Free of Small Trapping Sets by Controlling Cycles,"Low-density parity-check (LDPC) codes exhibit excellent error correcting capability. However, small trapping sets in the Tanner graph are harmful to the iterative decoding algorithm. In this letter, we present a method of constructing (3, n)
girth-eight quasi-cyclic LDPC codes with low error floor by removing the small trapping sets from the Tanner graph. To address this issue, we analyze the relationship between eight-cycles and small trapping sets of Tanner graphs based on fully connected base graphs without parallel edges. We find that if some eight-cycles are not found in the Tanner graphs, any elementary trapping set in the range of a \leq 8
and b \leq 3
is removed naturally. We also derive a lower bound on the permutation size for the construction of such codes. The experimental simulation shows a favorable error rate performance with lower error floor over additive white Gaussian noise channels.","Iterative decoding,
Error analysis,
Indexes,
Signal to noise ratio,
Electronic mail"
Template Matching via Densities on the Roto-Translation Group,"We propose a template matching method for the detection of 2D image objects that are characterized by orientation patterns. Our method is based on data representations via orientation scores, which are functions on the space of positions and orientations, and which are obtained via a wavelet-type transform. This new representation allows us to detect orientation patterns in an intuitive and direct way, namely via cross-correlations. Additionally, we propose a generalized linear regression framework for the construction of suitable templates using smoothing splines. Here, it is important to recognize a curved geometry on the position-orientation domain, which we identify with the Lie group SE(2): the roto-translation group. Templates are then optimized in a B-spline basis, and smoothness is defined with respect to the curved geometry. We achieve state-of-the-art results on three different applications: detection of the optic nerve head in the retina (99.83 percent success rate on 1,737 images), of the fovea in the retina (99.32 percent success rate on 1,616 images), and of the pupil in regular camera images (95.86 percent on 1,521 images). The high performance is due to inclusion of both intensity and orientation features with effective geometric priors in the template matching. Moreover, our method is fast due to a cross-correlation based matching approach.","Smoothing methods,
Retina,
Linear regression,
Pattern matching,
Splines (mathematics),
Wavelet transforms"
Design and Characterization of a Low-Cost FPGA-Based TDC,"We present an FPGA implementation of a Timeto- Digital Converter based on a low cost, low area Spartan 6 device. The converter is based on a tapped delay line model. Several implementation details are discussed with particular focus on critical blocks such as the input stage and thermometerto- binary decoding techniques. We implemented a tap filtering technique to improve the differential non-linearity (DNL) of the single delay line while keeping a good LSB value of 25.57 ps with a single-shot precision (SSP) between 0:69 ÷ 1:46 LSB. Measured DNL and INL lie in the range between −0:90 ÷ 1:23 and −0:43 ÷ 2:96 LSB, respectively. We then implemented an interpolating TDC to overcome the limitations of a single delay line in terms of linearity and measurement range. The interpolating TDC uses the sliding scale technique, where the time interval to be measured is asynchronous with respect to the FPGA clock, achieving DNL and INL in the range −0:072 ÷ 0:070 and −0:755 ÷ 0:872 LSB. SSP is in the 1:096 ÷ 2:815 range. Moreover, we present a novel comparison between the DNLs obtained with two different methods: statistical code density test and using a finely-controlled delay source. Finally, we present the results of a Montecarlo simulation used to investigate the effects of non-linear propagation of the signal through the delay line.",
A Fast Linear Algorithm for Magnetic Dipole Localization Using Total Magnetic Field Gradient,"Localization of magnetic target can be estimated through a magnetic anomaly. Generally, we need to solve the high-order nonlinear equations for estimating the position and magnetic moment of the target. Therefore, optimization algorithms are applied to calculate the solution of the nonlinear equations. In this paper, we present a fast linear algorithm for locating the target based on the total magnetic field gradient. In the algorithm, we give the closed-form formula of the static magnetic target localization. According to the properties of the vector, we can obtain a cubic equation of any 1-D position of the target. Thus, we can easily solve the cubic equation and obtain the closed-form formula of the target localization. Compared with the optimization algorithms, the proposed method provides good performance using short time and can be used to locate the target in real time. The proposed method is validated by the numerical simulation and real experimental data. The position and magnetic moment of the target are calculated rapidly. And the results show that the estimated parameters of the static target using the proposed method are very close to the true values.",
Slide: Towards Fast and Accurate Mobile Fingerprinting for Wi-Fi Indoor Positioning Systems,"The deployment of Wi-Fi fingerprint-based indoor positioning systems is severely hindered by the lack of an efficient and low-cost way to establish a signal fingerprint database. In this paper, we present a novel fingerprinting method, slide, that can collect fingerprints in a fast and accurate way. Slide uses a commodity flashlight and a smartphone to achieve linear positioning. This allows automatic mapping from the received signal strength to the position on a line, serving as a building block for fingerprinting in general environments. Slide also features a channel-based scanning method, which acquires fingerprint location after each Wi-Fi channel scanning, to mitigate the fingerprint misalignment problem found in the general mobile fingerprinting. Quantitative analysis and experimental results show that slide is faster than the manual fingerprinting method by up to an order of magnitude with comparable positioning accuracy, and is also more efficient than state-of-the-art mobile fingerprinting methods.",
Frequency Mixer Based on Doppler Effect,"In this letter, we have proposed and demonstrated a novel mixer that requires no local oscillator (LO), but mixes the radio frequency (RF) with its Doppler shifted frequencies on a nonlinear reconfigurable composite right/left-handed transmission line, on which a moving reflective surface is controlled by an external digital circuit. An incident RF encounters a frequency shift when reflected from this moving surface and a difference frequency between the incident frequency and Doppler shifted frequency is generated on this nonlinear transmission line. This kind of mixer has the advantage in high frequency and cognitive radio applications since an LO is not required, meanwhile the intermediate frequency is also tunable electronically.","Mixers,
Frequency measurement,
Doppler effect,
Transmission line measurements,
Radio frequency,
Receivers"
A Unified Model for Joint Normalization and Differential Gene Expression Detection in RNA-Seq data,"The RNA-sequencing (RNA-seq) is becoming increasingly popular for quantifying gene expression levels. Since the RNA-seq measurements are relative in nature, between-sample normalization of counts is an essential step in differential expression (DE) analysis. The normalization of existing DE detection algorithms is ad hoc and performed once for all prior to DE detection, which may be suboptimal since ideally normalization should be based on non-DE genes only and thus coupled with DE detection. We propose a unified statistical model for joint normalization and DE detection of log-transformed RNA-seq data. Sample-specific normalization factors are modeled as unknown parameters in the gene-wise linear models and jointly estimated with the regression coefficients. By imposing sparsity-inducing L1 penalty (or mixed L1/L2 penalty for multiple treatment conditions) on the regression coefficients, we formulate the problem as a penalized least-squares regression problem and apply the augmented lagrangian method to solve it. Simulation studies show that the proposed model and algorithms perform better than or comparably to existing methods in terms of detection power and false-positive rate. The performance gain increases with increasingly larger sample size or higher signal to noise ratio, and is more significant when a large proportion of genes are differentially expressed in an asymmetric manner.","Gene expression,
Sequential analysis,
Tuning,
Numerical models,
Data models,
Indexes,
Electronic mail"
A Differentiated Caching Mechanism to Enable Primary Storage Deduplication in Clouds,"Existing primary deduplication techniques either use inline caching to exploit locality in primary workloads or use post- processing deduplication to avoid the negative impact on I/O performance. However, neither of them works well in the cloud servers running multiple services for the following two reasons: Firstly, the temporal locality of duplicate data writes may not exist in some primary storage workloads thus inline caching often fails to achieve good deduplication ratio. Secondly, the post-processing deduplication does not provide the benefit of I/O deduplication. A hybrid deduplication mechanism is promising to deal with the problems. Fingerprint caching plays an important role for achieving efficient hybrid deduplication. However, existing fingerprint caching could not deal with the locality interference between data streams from different VMs. In this paper, we present a detailed analysis of the limitations of using existing caching algorithms in primary deduplication in the cloud. We reveal that existing caching algorithms targeting on poor locality incur significant memory overhead in fingerprint cache management. To address this, we propose a novel fingerprint caching mechanism which estimates the temporal locality of duplicates in different data streams and prioritizes the cache allocation based on the estimation. We integrate the caching mechanism to build a hybrid deduplication system. The experimental results show that the proposed mechanism provides significant improvement for both deduplication ratio and overhead reduction.",
Mobile Lattice-Coded Physical-Layer Network Coding With Practical Channel Alignment,"Physical-layer network coding (PNC) is a communications paradigm that exploits overlapped transmissions to boost the throughput of wireless relay networks. A high point of PNC research was a theoretical proof that PNC that makes use of nested lattice codes could approach the information-theoretic capacity of a two-way relay network (TWRN), where two end nodes communicate via a relay node. The capacity cannot be achieved by conventional methods of time-division or straightforward network coding. Many practical challenges, however, remain to be addressed before the full potential of lattice-coded PNC can be realized. Two major challenges are: (1) for good performance in lattice-coded PNC, channels of simultaneously transmitting nodes must be aligned; (2) for lattice-coded PNC to be practical, the complexity of lattice encoding at the transmitters and lattice decoding at the receiver must be reduced. We address these challenges and implement a first lattice-coded PNC system on a software-defined radio (SDR) platform. Specifically, we design and implement a low-overhead channel precoding system that accurately aligns the channels of distributed nodes. In our implementation, the nodes only use low-cost temperature-compensated oscillators (TCXO)—a consequent challenge is that the channel alignment must be done more frequently and more accurately compared with the use of expensive oscillators. The low overhead and accurate channel alignment are achieved by (1) a channel precoding system implemented over FPGA to realize fast feedback of channel state information; (2) a highly-accurate carrier frequency offset (CFO) estimation method; and (3) a partial-feedback channel estimation method that significantly reduces the amount of feedback information from the receiver to the transmitters for channel precoding at the transmitters. To reduce lattice encoding and decoding complexities, we adapt the low-density lattice code (LDLC) for use in PNC systems. Experiments show that our implemented lattice-coded PNC achieves better bit error rate performance compared with time-division and straightforward network coding systems. It also has good throughput performance in mobile non-LoS scenarios.",
A Low-Complexity Maximum-Likelihood Decoder for Tail-Biting Convolutional Codes,"Due to the growing interest in applying tail-biting convolutional coding techniques in real-time communication systems, fast decoding of tail-biting convolutional codes has become an important research direction. In this work, a new maximum-likelihood (ML) decoder for tail-biting convolutional codes is proposed. It is named Bidirectional Priority-First Search Algorithm (BiPFSA) because Priority-First Search Algorithm has been used both in forward and backward directions during decoding. Simulations involving the antipodal transmission of (2,1,6) and (2,1,12) tail-biting convolutional codes over additive white Gaussian noise channels show that BiPFSA not only has the least average decoding complexity among state-of-the-art decoding algorithms for tail-biting convolutional codes but can also provide a highly stable decoding complexity with respect to growing information length and code constraint length. More strikingly, at high SNR, its average decoding complexity can even approach the ideal benchmark complexity, obtained under a perfect noise-free scenario by any sequential-type decoding. This demonstrates the superiority of BiPFSA in terms of decoding efficiency.","Maximum likelihood decoding,
Convolutional codes,
Complexity theory,
Measurement,
Encoding,
Shift registers"
Unsupervised Deep Hashing with Similarity-Adaptive and Discrete Optimization,"Recent vision and learning studies show that learning compact hash codes can facilitate massive data processing with significantly reduced storage and computation. Particularly, learning deep hash functions has greatly improved the retrieval performance, typically under the semantic supervision. In contrast, current unsupervised deep hashing algorithms can hardly achieve satisfactory performance due to either the relaxed optimization or absence of similarity-sensitive objective. In this work, we propose a simple yet effective unsupervised hashing framework, named Similarity-Adaptive Deep Hashing (SADH), which alternatingly proceeds over three training modules: deep hash model training, similarity graph updating and binary code optimization. The key difference from the widely-used two-step hashing method is that the output representations of the learned deep model help update the similarity graph matrix, which is then used to improve the subsequent code optimization. In addition, for producing high-quality binary codes, we devise an effective discrete optimization algorithm which can directly handle the binary constraints with a general hashing loss. Extensive experiments validate the efficacy of SADH, which consistently outperforms the state-of-the-arts by large gaps.",
Peer-Assisted Video Streaming With RTMFP Flash Player: A Measurement Study on PPTV,"Real-time media flow protocol (RTMFP) is a protocol developed by Adobe for multimedia delivery under both client–server and peer-to-peer (P2P) paradigms. Currently, major Internet video service providers, such as PPTV and iQIYI, have already built their Web-based video streaming systems with RTMFP. In such a system, a user only needs to install a Flash Player plug-in on his Web browser, and can stream videos in a peer-assisted way. Despite its wide usage, RTMFP has received little attention from the measurement community. In this paper, we select PPTV as an example and study the RTMFP video streaming technology with a measurement approach. We reveal the architecture of PPTV’s RTMFP streaming system and show that, compared with proprietary P2P networks, the RTMFP network has a different content distribution policy, and exhibits different features on peers’ streaming behaviors, potential system bottleneck, and network dynamics. We also study RTMFP’s video transmission and find that the protocol’s selective retransmission scheme can effectively overcome packet losses and improve the video playback quality; however, the TCP-like congestion control mechanism of RTMFP does not lead to fairness between RTMFP and Transmission Control Protocol (TCP) traffics, due to the mismatch between the inherited pull-based video segment distribution model of the P2P streaming application and the protocol’s built-in congestion control mechanism. This paper provides insights into the RTMFP-based video streaming technology and is helpful for people to construct better peer-assisted video systems with RTMFP.",
Scheduling Analysis of Imprecise Mixed-Criticality Real-Time Tasks,"In this paper, we study the scheduling problem of the im- precise mixed-criticality model (IMC) under earliest deadline first with virtual deadline (EDF-VD) scheduling upon uniprocessor systems. Two schedulability tests are presented. The first test is a concise utilization- based test which can be applied to the implicit deadline IMC task set. The suboptimality of the proposed utilization-based test is evaluated via a widely-used scheduling metric, speedup factors. The second test is a more effective test but with higher complexity which is based on the concept of demand bound function (DBF). The proposed DBF-based test is more generic and can apply to constrained deadline IMC task set. Moreover, in order to address the high time cost of the existing deadline tuning algorithm, we propose a novel algorithm which significantly improve the efficiency of the deadline tuning procedure. Experimental results show the effectiveness of our proposed schedulability tests, confirm the theoretical suboptimality results with respect to speedup factor, and demonstrate the efficiency of our proposed algorithm over the existing deadline tunning algorithm. In addition, issues related to the implementation of the IMC model under EDF-VD are discussed.",
Multimodal Deep Embedding via Hierarchical Grounded Compositional Semantics,"For a number of important problems, isolated semantic representations of individual syntactic words or visual objects do not suffice, but instead a compositional semantic representation is required; for example, a literal phrase or a set of spatially concurrent objects. In this paper, we aim to harness the existing image–sentence databases to exploit the compositional nature of image–sentence data for multimodal deep embedding. In particular, we propose an approach called hierarchical-alike (bottom–up two layers) multimodal grounded compositional semantics (hiMoCS) learning. The proposed hiMoCS systemically captures the compositional semantic connotation of multimodal data in the setting of hierarchical-alike deep learning by modeling the inherent correlations between two modalities of collaboratively grounded semantics, such as the textual entity (with its describing attribute) and visual object, the phrase (e.g., subject-verb–object triplet), and spatially concurrent objects. We argue that hiMoCS is more appropriate to reflect the multimodal compositional semantics of the image and its narrative textual sentence, which are strongly coupled. We evaluate hiMoCS on the several benchmark data sets and show that the utilization of the hiMoCS (textual entities and visual objects, textual phrase, and spatially concurrent objects) achieves a much better performance than only using the flat grounded compositional semantics.","Semantics,
Visualization,
Bicycles,
Correlation,
Machine learning,
Buildings,
Feature extraction"
A Subspace Learning Approach to Multishot Person Reidentification,"This paper addresses the challenging problem of multishot person reidentification (Re-ID) in real world uncontrolled surveillance systems. A key issue is how to effectively represent and process the multiple data with various appearance information due to the variations of pose, occlusions, and viewpoints. To this end, this paper develops a novel subspace learning approach, which pursues regularized low-rank and sparse representation for multishot person Re-ID. For the images of a person crossing a certain camera, we assume that the appearances of those subset images with similar viewpoints against a camera draw from the same low-rank subspace, and all the images of a person under a camera lie on a union of low-rank subspaces. Based on this assumption, we propose to learn a nonnegative low-rank and sparse graph to represent the person images. Moreover, the recurring pattern prior is integrated into our model to refine the affinities among images. Extensive experiments on four public benchmark datasets yield impressive performance by improving 22.9% on imagery library for intelligent detection systems video re identification (iLIDS-VID), 42.4% on person RE-ID (PRID) dataset 2011, 39.7% and 30.6% on speech, audio, image, and video technology-SoftBio camera 3/8 and camera 5/8, respectively, and 1.6% on motion analysis and re identification set compared to the state-of-the-art methods.","Cameras,
Sparse matrices,
Image color analysis,
Robustness,
Image sequences,
Surveillance"
Detecting and Removing Visual Distractors for Video Aesthetic Enhancement,"Personal videos often contain visual distractors, i.e. objects that are accidentally captured that can distract viewers from focusing on the main subjects. We propose a method to automatically detect and localize these distractors, by learning from a manually-labeled dataset. To achieve spatially- and temporally-coherent detection, we propose to extract features at the Temporal-Superpixel (TSP) level in a traditional SVM-based learning framework. We have also experimented with end-to-end learning with Convolutional Neural Networks (CNNs), which achieve slightly higher performance. The classification result is further refined in a post-processing step based on graph-cut optimization. Experimental results show that our method achieves an accuracy of 81% and a recall of 86%. We demonstrate several ways to remove the detected distractors to improve the video quality, including video hole filling; video frame replacement; and camera path re-planning.",
EduCTX: A blockchain-based higher education credit platform,"Blockchain technology enables the creation of a decentralized environment where transactions and data are not under the control of any third party organization. Any transaction ever completed is recorded in a public ledger in a verifiable and permanent way. Based on blockchain technology, we propose a global higher education credit platform, named EduCTX. This platform is based on the concept of the European Credit Transfer and Accumulation System (ECTS). It constitutes a globally trusted, decentralized higher education credit and grading system that can offer a globally unified viewpoint for students and higher education institutions (HEIs), as well as for other potential stakeholders such as companies, institutions and organizations. As a proof of concept, we present a prototype implementation of the environment, based on the open-source Ark Blockchain Platform. Based on a globally distributed peer-topeer network, EduCTX will process, manage and control ECTX tokens, which represent credits that students gain for completed courses such as ECTS. HEIs are the peers of the blockchain network. The platform is a first step towards a more transparent and technologically advanced form of higher education systems. The EduCTX platform represents the basis of the EduCTX initiative which anticipates that various HEIs would join forces in order to create a globally efficient, simplified and ubiquitous environment in order to avoid language and administrative barriers. Therefore we invite and encourage HEIs to join the EduCTX initiative and the EduCTX blockchain network.","Education,
Prototypes,
Europe,
Bitcoin,
Open source software,
Organizations"
An Algorithm of an X-ray Hit Allocation to a Single Pixel in a Cluster and Its Test-Circuit Implementation,"An on-chip implementable algorithm for allocation of an X-ray photon imprint, called a hit, to a single pixel in the presence of charge sharing in a highly segmented pixel detector is described. Its proof-of-principle implementation is also given supported by the results of tests using a highly collimated X-ray photon beam from a synchrotron source. The algorithm handles asynchronous arrivals of X-ray photons. Activation of groups of pixels, comparisons of peak amplitudes of pulses within an active neighborhood and finally latching of the results of these comparisons constitute the three procedural steps of the algorithm. A grouping of pixels to one virtual pixel, that recovers composite signals and event driven strobes, to control comparisons of fractional signals between neighboring pixels are the actuators of the algorithm. The circuitry necessary to implement the algorithm requires an extensive inter-pixel connection grid of analog and digital signals, that are exchanged between pixels. A test-circuit implementation of the algorithm was achieved with a small array of
32×32
pixels and the device was exposed to an 8 keV highly collimated to a diameter of 3-
μm
X-ray beam. The results of these tests are given in this paper assessing physical implementation of the algorithm.","Photonics,
Electrodes,
Detectors,
Resource management,
Laboratories,
Electrical engineering,
Cloud computing"
DPPDL: A Dynamic Partial-Parallel Data Layout for Green Video Surveillance Storage,"Video surveillance requires storing massive amounts of video data, which results in the rapid increasing of storage energy consumption. With the popularization of video surveillance, green storage for video surveillance is very attractive. The existing energy-saving methods for massive storage mostly concentrate on the data centers, mainly with random access, whereas the storage of video surveillance has inherent workload characteristics and access pattern, which can be fully exploited to save more energy. A dynamic partial-parallel data layout (DPPDL) is proposed for green video surveillance storage. It adopts a dynamic partial-parallel strategy, which dynamically allocates the storage space with an appropriate degree of partial parallelism according to performance requirement. Partial parallelism benefits energy conservation by scheduling only partial disks to work; a dynamic degree of parallelism can provide appropriate performances for various intensity workloads. DPPDL is evaluated by a simulated video surveillance consisting of 60–300 cameras with
1920×1080
pixels. The experiment shows that DPPDL is most energy efficient, while tolerating single disk failure and providing more than 20% performance margin. On average, it saves 7%, 19%, 31%, 36%, 56%, and 59% more energy than a CacheRAID, Semi-RAID, Hibernator, MAID, eRAID5, and PARAID, respectively.","Video surveillance,
Energy efficiency,
Parallel processing,
Cameras,
Energy storage,
Streaming media,
Layout"
Lidar-Based Gait Analysis and Activity Recognition in a 4D Surveillance System,"This paper presents new approaches for gait and activity analysis based on data streams of a rotating multibeam (RMB) Lidar sensor. The proposed algorithms are embedded into an integrated 4D vision and visualization system, which is able to analyze and interactively display real scenarios in natural outdoor environments with walking pedestrians. The main focus of the investigations is gait-based person reidentification during tracking and recognition of specific activity patterns, such as bending, waving, making phone calls, and checking the time looking at wristwatches. The descriptors for training and recognition are observed and extracted from realistic outdoor surveillance scenarios, where multiple pedestrians are walking in the field of interest following possibly intersecting trajectories; thus, the observations might often be affected by occlusions or background noise. Since there is no public database available for such scenarios, we created and published a new Lidar-based outdoor gait and activity data set on our website that contains point cloud sequences of 28 different persons extracted and aggregated from 35-min-long measurements. The presented results confirm that both efficient gait-based identification and activity recognition are achievable in the sparse point clouds of a single RMB Lidar sensor. After extracting the people trajectories, we synthesized a free-viewpoint video, in which moving avatar models follow the trajectories of the observed pedestrians in real time, ensuring that the leg movements of the animated avatars are synchronized with the real gait cycles observed in the Lidar stream.",
SEGB: Security Enhanced Group Based AKA Protocol for M2M Communication in an IoT Enabled LTE/LTE-A Network,"Nowadays machine to machine (M2M) communication and its applications are growing tremendously around the globe as millions of devices are communicating with each other in an IoT enabled LTE/LTE-A network. These applications are effective and secure only after the successful verification of machine type communication devices (MTCDs). Hence, various group based authentication and key agreement (AKA) protocols were proposed in the literature to achieve the authentication. These protocols fulfill all the security requirements such as privacy preservation, mutual authentication, integrity, and confidentiality. But, none of them have the credential to overcome the single key problem in the communication network. In addition, they do not have the efficacy to maintain the group key unlink-ability and are susceptible to identified attacks. In some of the protocols, each MTCD needs to authenticate independently to simultaneously access the communication network that generates network congestion overhead. In view of these problems, we propose the security enhanced group based (SEGB) AKA protocol for M2M communication in an IoT enabled LTE/LTE-A network. The SEGB-AKA protocol solves the problem of the single key during the authentication process and achieves the key forward/backward secrecy (KFS/KBS). The protocol overcomes the problem of signaling congestion and high bandwidth consumption. The formal security analysis of the protocol is carried out by the Automated Internet Security Protocols and Applications (AVISPA) tool. The security analysis shows that the protocol achieves the security goals and is free from various known attacks. Moreover, the performance of the proposed SEGB-AKA protocol is analyzed with existing group based AKA protocols. The analysis shows that the protocol has better results in terms of network overheads and fulfills all the security requirements of M2M communication.","Protocols,
Machine-to-machine communications,
Authentication,
Servers,
Privacy,
3GPP"
A Neuro-Fuzzy Visual Servoing Controller for an Articulated Manipulator,"The challenges of selecting appropriate image features, optimizing complex nonlinear computations and minimizing the approximation errors always exist in visual servoing. A fuzzy neural network controller is developed for a six-degrees-of-freedom robot manipulator to perform visual servoing is proposed to tackle these problems. To increase the accuracy of the image preprocesses, a synthetic image process performs feature extraction for the controller. The method combines a support vector machine contour recognition algorithm and a color-based feature recognition algorithm. For visual servoing, a control method based on the fuzzy cerebellar model articulation controller with the Takagi–Sugeno framework (FCMAC-T-S) is proposed to directly map an image feature error vector to a desired robot end-effector velocity. This approach achieves visual servoing control without the need of computing the inverse interaction matrix. The control variables are learned and updated by the T-S fuzzy inference. This simplifies the implementation of visual servoing in real-time applications. The proposed control method is used to control an articulated manipulator with an eye-in-hand configuration. The results of simulations and experiments demonstrate that the proposed visual servoing controller has good performance, in terms of stability and convergence.","Visual servoing,
Feature extraction,
Manipulators,
Jacobian matrices,
Robot kinematics,
Cameras"
GapReduce: a gap filling algorithm based on partitioned read sets,"With the advances in technologies of sequencing and assembly, draft sequences of more and more genomes are available. However, there commonly exist gaps in these draft sequences which influence various downstream analysis of biological studies. Gap filling methods can shorten the length of gaps and improve the completion of these draft sequences of genomes. Although some gap filling tools have been developed, their effectiveness and accuracy need to be improved. In this study, we develop a novel tool, called GapReduce, which can fill the gaps using the paired reads. For a gap, GapReduce selects the reads whose mate reads are aligned on the left or the right flanking region, and partitions the reads to two sets. Then GapReduce adopts different
k
values and
k−mer
frequency thresholds to iteratively construct De Bruijn graphs, which are used for finding the correct path to fill the gap. For overcoming the branching problems caused by repetitive regions and sequencing errors in the procedure of path selection, GapReduce designs a novel approach that simultaneously considers
k−mer
frequency and distribution of paired reads based on the partitioned read sets. We compare the performance of GapReduce with current popular gap filling tools. The experimental results demonstrate that GapReduce can produce satisfactory gap filling results, especially for long insert size datasets. GapReduce is publicly available for downloading at https://github.com/bioinfomaticsCSU/GapReduce.",
Simplified Small-Signal Model for Output Voltage Control of Asymmetric Cascaded H-Bridge Multilevel Inverter,"This paper proposes a simplified small-signal model for output voltage control of a single-phase asymmetrical cascaded H-bridge multilevel inverter (ACHMI). The ACHMI is an n-series connected H-bridge converter, each one with a unique value at the dc link and usually scaled at {1:2:6:…} or {1:3:9:…}. By assuming that the small-signal variation component is equal in all n converter terminal ports, a simplified small-signal model is obtained. This assumption is carefully described and justified. To verify the veracity of the proposed model, two distinct control strategies are applied. One is a single-loop control scheme based on a modified proportional-integral (PI) controller. The other one is a double-loop control scheme based on a PI controller with feedforward action of the load current. Both controllers are tuned based on the dynamic behavior of the proposed model. Since the designed controllers based on the simplified model make the ACHMI output voltage to follow the reference without steady-state error, the proposed simplified model truly represents the inverter. Experimental results show the efficacy of the simplified model of the ACHMI through the two mentioned control strategies as well as the ACHMI installed in a microgrid.",
"Reliability in Super- and Near-Threshold Computing: A Unified Model of RTN, BTI, and PV","Near-threshold computing (NTC) poses stringent constraints on designing reliable circuits, as degradations have a magnified impact at lower supply voltages (
V
dd
) compared with super-threshold supply voltages. While phenomena, such as bias temperature instability (BTI) scale down with
V
dd
, mitigate their magnified impact with reduced degradations and, thus, have little impact on NTC reliability. Process variation (PV) and random telegraph noise (RTN) do not scale with
V
dd
and, therefore, become key reliability challenges in NTC. On the other hand, in super-threshold computing (STC), PV and BTI are the dominant phenomena, as BTI induces considerable degradations at nominal
V
dd
and PV imposes large enough shifts to matter at any supply voltage. Therefore, to allow
V
dd
-scaling from super-to near-threshold, we need to consider all of BTI, RTN, and PV. Ergo, we present a unified RTN and BTI model that models their shared physical origin and is validated against experimental data across a wide voltage range. Our unified model and PV model capture the joint impact of RTN, BTI, and PV within a probabilistic reliability estimation for NTC and STC circuits. We employed our proposed model to analyze the reliability of SRAM cells showing how taking error correction codes into account is able to mitigate the deleterious effects of BTI, RTN, and PV by 36% compared with unprotected circuits.",
L1-Norm Distance Linear Discriminant Analysis Based on an Effective Iterative Algorithm,"Recent works have proposed two L1-norm distance measure-based linear discriminant analysis (LDA) methods, L1-LD and LDA-L1, which aim to promote the robustness of the conventional LDA against outliers. In LDA-L1, a gradient ascending iterative algorithm is applied, which, however, suffers from the choice of stepwise. In L1-LDA, an alternating optimization strategy is proposed to overcome this problem. In this paper, however, we show that due to the use of this strategy, L1-LDA is accompanied with some serious problems that hinder the derivation of the optimal discrimination for data. Then, we propose an effective iterative framework to solve a general L1-norm minimization–maximization (minmax) problem. Based on the framework, we further develop a effective L1-norm distance-based LDA (called L1-ELDA) method. Theoretical insights into the convergence and effectiveness of our algorithm are provided and further verified by extensive experimental results on image databases.","Principal component analysis,
Robustness,
Linear discriminant analysis,
Optimization,
Convergence,
Forestry,
Iterative methods"
A Framework for the Automatic Vectorization of Parallel Sort on x86-based Processors,"The continued growth in the width of vector registers and the evolving library of intrinsics on the modern x86 processors make manual optimizations for data-level parallelism tedious and error-prone. In this paper, we focus on parallel sorting, a building block for many higher-level applications, and propose a framework for the Automatic SIMDization of Parallel Sorting (ASPaS) on x86-based multi- and many-core processors. That is, ASPaS takes any sorting network and a given instruction set architecture (ISA) as inputs and automatically generates vector code for that sorting network. After formalizing the sort function as a sequence of comparators and the transpose and merge functions as sequences of vector-matrix multiplications, ASPaS can map these functions to operations from a selected ""pattern pool"" that is based on the characteristics of parallel sorting, and then generate the vector code with the real ISA intrinsics. The performance evaluation on the Intel Ivy Bridge and Haswell CPUs, and Knights Corner MIC illustrates that automatically generated sorting codes from ASPaS can outperform the widely used sorting tools, achieving up to
5.2×
speedup over the single-threaded implementations from STL and Boost and up to
6.7×
speedup over the multi-threaded parallel sort from Intel TBB.","Sorting,
Microwave integrated circuits,
Registers,
Merging,
Distributed databases,
Instruction sets"
Modeling of Agent Cognition in Extensive Games via Artificial Neural Networks,"The decision-making process, which is regarded as cognitive and ubiquitous, has been exploited in diverse fields, such as psychology, economics, and artificial intelligence. This paper considers the problem of modeling agent cognition in a class of game-theoretic decision-making scenarios called extensive games. We present a novel framework in which artificial neural networks are incorporated to simulate agent cognition regarding the structure of the underlying game and the goodness of the game situations therein. An algorithmic procedure is investigated to describe the process for solving games with cognition, and then, a new equilibrium concept is proposed as a refinement of the classical one--subgame perfect equilibrium--by involving players' cognitive reasoning. Moreover, a series of results concerning the computational complexity, soundness, and completeness of the algorithm, as well as the existence of an equilibrium solution, is obtained. This framework, which is shown to be general enough to model the way in which AlphaGo plays Go, may offer a means for bridging the gap between theoretical models and practical problem-solving.",
Modeling and Analysis of Passive Switching Crossbar Arrays,"Emerging technologies have enabled efficient, high-speed realizations of ultra-dense crossbar arrays, driving the need for better insight in the transient operation of such systems. Previous work focused mostly on the effect of line resistance and its impact on steady-state response. In this paper, we develop a compact RC
framework that includes the effects of parasitics. We use memristors as an exemplar device where interconnect parasitics (resistance, inductance, capacitance, and conductance) are extracted using ANSYS Q3D extractor for 5- and 50- nm
feature sizes. A model for the crossbar is presented, considering the stray and coupling capacitive parasitics of the crossbar. The derived model is based on state-space representation and provides more insight into the behavior of crossbar arrays containing either linear or nonlinear switching devices. The framework provides a closed-form solution to evaluate Elmore delay, as well as the steady-state response of the system. Signal delay is evaluated and compared for both grounded and floating interconnect inputs and verified against HSPICE, showing a perfect match.","Wires,
Capacitance,
Integrated circuit interconnections,
Switches,
Couplings,
Resistance,
Memristors"
A Joint Multi-Criteria Utility-Based Network Selection Approach for Vehicle-to-Infrastructure Networking,"The emerging technologies for connected vehicles have become hot topics. In addition, connected vehicle applications are generally found in heterogeneous wireless networks. In such a context, user terminals face the challenge of access network selection. The method of selecting the appropriate access network is quite important for connected vehicle applications. This paper jointly considers multiple decision factors to facilitate vehicle-to-infrastructure networking, where the energy efficiency of the networks is adopted as an important factor in the network selection process. To effectively characterize users' preference and network performance, we exploit energy efficiency, signal intensity, network cost, delay, and bandwidth to establish utility functions. Then, these utility functions and multi-criteria utility theory are used to construct an energy-efficient network selection approach. We propose design strategies to establish a joint multi-criteria utility function for network selection. Then, we model network selection in connected vehicle applications as a multi-constraint optimization problem. Finally, a multi-criteria access selection algorithm is presented to solve the built model. Simulation results show that the proposed access network selection approach is feasible and effective.",
Computational Artifacts of the In Situ Electric Field in Anatomical Models Exposed to Low-Frequency Magnetic Field,"An in situ (internal) electric field is used as a dosimetric quantity for human protection from low-frequency electromagnetic fields (lower than 5 MHz) under international safety standard/;guidelines. The IEEE standard uses a homogenous elliptical cross section to derive external field strength corresponding to an in situ field strength, while the International Committee on Non-Ionizing Radiation Protection (ICNIRP) guidelines use anatomical models to relate them. In the latter, “the 99th percentile value of the in situ electric field averaged over the cube of its side length of 2 mm” is used to represent the maximum in situ electric field. This metric was introduced to suppress computational artifacts that are inherent when using voxelized anatomical models, in which curved boundaries are discretized with a stair-casing approximation. To suppress the error, a few schemes have been proposed for treating the computational artifacts. In this study, the various schemes to suppress the artifacts are reviewed. Subsequently, a postprocessing method for determining the appropriate maximum in situ field strength is proposed. The performance of the proposed scheme is first verified by comparison with an analytical solution in a multilayered sphere. The method is then applied for different exposure scenarios in anatomically realistic human models where the volume under computation is also considered.","Computational modeling,
Magnetic multilayers,
Conductivity,
Mathematical model,
Nonhomogeneous media,
Guidelines,
Electric fields"
Reduced-Reference Quality Assessment of Screen Content Images,"The screen content images (SCIs) quality influences the user experience and the interactive performance of remote computing systems. With numerous approaches proposed to evaluate the quality of natural images, much less work has been dedicated to reduced-reference image quality assessment (RR-IQA) of SCIs. Here, we propose an RR-IQA method from the perspective of SCI visual perception. In particular, the quality of the distorted SCI is evaluated by comparing a set of extracted statistical features that consider both primary visual information and unpredictable uncertainty. A unique property that differentiates the proposed method from previous RR-IQA methods for natural images is the consideration of behaviors when human subjects view the screen content, which motivates us to establish the perceptual model according to the distinct properties of SCIs. Validations based on the screen content IQA database show that the proposed algorithm provides accurate predictions across a wide range of SCI distortions with negligible transmission overhead.","Feature extraction,
Visualization,
Uncertainty,
Prediction algorithms,
Image edge detection,
Quality assessment,
Image quality"
Theory of Double Ladder Lumped Circuits With Degenerate Band Edge,"A conventional periodic LC ladder circuit forms a transmission line that has a regular band edge between a passband and a stopband. Here for the first time, we develop the theory of simple yet unconventional double ladder circuit that exhibits a special degeneracy condition referred to as a degenerate band edge (DBE). The degeneracy occurs when four independent eigenstates coalesce into a single eigenstate at the DBE frequency. In addition to possible practical applications, this circuit may provide insight into DBE behavior that is not clear in more complex systems. We show that double ladder resonators exhibit unusual behavior of the loaded quality factor near the DBE, leading to a stable resonance frequency against load variations. These two properties in the proposed circuit are superior to the analogous properties in single ladder circuits. Our proposed analysis leads to analytic expressions for all circuit quantities thus providing insight into the very complex behavior near degeneracy points in periodic circuits. Interestingly, here we show for the first time That a DBE is obtained with unit cells that are symmetric along the propagation direction. The proposed theory of double ladders presented here has potential applications in filters, couplers, oscillators, and pulse shaping networks.","Periodic structures,
Oscillators,
RLC circuits,
Q-factor,
Steady-state,
Loading,
Dispersion"
"Detection of Good and Bad Sensor-Nodes in Presence of Malicious Attacks, and Its Application to Data Aggregation","Most sensor nodes have multiple inexpensive and unreliable sensors embedded in them. For many applications readings from multiple sensors are aggregated. However, presence of malicious attacks adds challenge to sensor data aggregation. Detection of those compromised and unreliable sensors, and sensor-nodes are important for robust data aggregation as well as their management and maintenance. In this work we develop, 1) a method for identification of good and bad sensor-nodes, and 2) apply it for secure data aggregation algorithms. We consider altered/unreliable readings as outliers and identify them using an augmented and modified version of a local outlier factor computation method. We use outlier detection algorithm for 1) reliable and unreliable sensor detection, and 2) use the results from this algorithm for unreliable sensor-node identification algorithm. We show its usefulness for secure data aggregation algorithms. Extensive evaluations of the proposed algorithm show that it identifies good and bad nodes, and estimates true sensor value efficiently.",
Aging-aware Workload Management on Embedded GPU Under Process Variation,"Graphics Processing Units (GPUs) have been employed in embedded systems to handle increased amounts of computation and to satisfy the timing requirement. Due to the small feature size, chip aging and within-die parameter variations have been considered to be among the challenging problems for state-of-the-art processors, including GPUs. In order to deal with the process variation, several processors use chip-level guardbanding, which uses the lowest operating frequency that results in a significant chip-level performance drop. Other processors improve their performance efficiency through core-level guardbanding that may use a different operating frequency for each core. Existing aging management techniques are based on the chip-level guardbanding, which assigns the same number of instructions to the cores that have the same aging status. However, in the presence of the process variation, existing aging management techniques have a limitation in minimizing the aging effect because each core has a different amount of stress for the same number of instructions. In order to tackle this problem, we propose a low-overhead aging and process variation aware workload management technique for embedded GPUs. The proposed technique considers the process variation and the current aging status together, and assigns a different number of instructions to clusters to minimize the aging effect in the presence of process variation. Results show that our technique improves the GPU aging in over 95% of cases whereas the state-of-the-art compiler-based technique improves the GPU aging in 72.25% of cases. Moreover, compared to the compiler-based technique, our technique reduces the performance overhead by 40% while achieving almost the same GPU aging improvement.",
Automated Detection and Measurement of Corneal Haze and Demarcation Line in Spectral-Domain Optical Coherence Tomography Images,"Keratoconus is a progressive eye disease that may lead to significant loss of visual acuity. Corneal cross-linking (CXL) is a surgical procedure that halts the progression of Keratoconus. One commonly used clinical indicator of CXL success, albeit being an indirect one, is the presence and depth of stromal demarcation line. In addition, corneal haze beyond the demarcation line can be an ominous sign of loss of corneal transparency, which is a much-dreaded side-effect of CXL. To date, ophthalmologists evaluate the presence and depth of the demarcation line, and grade corneal haze using slit lamp biomicroscopy and/or optical coherence tomography (OCT). Interpreting the output of the former is very biased at best, while analyzing the information presented by the latter is time consuming, potentially error-prone, and observer-dependent. In this paper, we propose the first method that employs image analysis and machine learning to automatically detect and measure corneal haze and demarcation line presence and depth in OCT images. The automated method provides the user with haze statistics as well as visual annotation reflecting the shape and location of the haze and demarcation line in the cornea. Our experimental results demonstrate the efficacy and effectiveness of the proposed techniques vis-a-vis manual measurements in a much faster, repeatable and reproducible manner.",
One-Dimensional Nonlinear Model for Producing Chaos,"Motivated by the concept of circuit design in digital circuit, this paper proposes a one-dimensional (1D) nonlinear model (1D-NLM) for producing 1D discrete-time chaotic maps. Our previous works have designed four nonlinear operations of generating new chaotic maps. However, they focus only on discussing individual nonlinear operations and their properties, but fail to consider their relationship among these operations. The proposed 1D-NLM includes these existing nonlinear operations, develops two new nonlinear operations, discusses their relationship among different nonlinear operations, and investigates the properties of different combinations of these operations. To show the effectiveness of 1D-NLM in generating new chaotic maps, as examples, we provide four new chaotic maps and study their dynamics properties from following three aspects: equilibrium point, stability, and bifurcation diagram. Performance evaluations are provided using the Lyapunov exponent, Shannon entropy, correlation dimension, and initial state sensitivity. The evaluation results show that these new chaotic maps have more complex chaotic behaviors than existing ones. To demonstrate the performance of 1D-NLM in practical applications, we use a pseudo-random number generator (PRNG) to compare new and existing chaotic maps. The randomness test results indicate that new chaotic map generated by 1D-NLM shows better performance than existing ones in designing PRNG.","Chaotic communication,
Bifurcation,
Logistics,
Switches,
Modulation,
Mathematical model"
Motion Capture Data Completion via Truncated Nuclear Norm Regularization,"The objective of motion capture (mocap) data completion is to recover missing measurement of the body markers from mocap. It becomes increasingly challenging as the missing ratio and duration of mocap data grow. Traditional approaches usually recast this problem as a low-rank matrix approximation problem based on the nuclear norm. However, the nuclear norm defined as the sum of all the singular values of a matrix is not a good approximation to the rank of mocap data. This paper proposes a novel approach to solve mocap data completion problem by adopting a new matrix norm, called truncated nuclear norm. An efficient iterative algorithm is designed to solve this problem based on the augmented Lagrange multiplier. The convergence of the proposed method is proved mathematically under mild conditions. To demonstrate the effectiveness of the proposed method, various comparative experiments are performed on synthetic data and mocap data. Compared to other methods, the proposed method is more efficient and accurate.","Light rail systems,
Data models,
Convergence,
Iterative methods,
Algorithm design and analysis,
Discrete cosine transforms,
Minimization"
Variance-Constrained State Estimation for Nonlinearly Coupled Complex Networks,"This paper studies the state estimation problem for nonlinearly coupled complex networks. A variance-constrained state estimator is developed by using the structure of the extended Kalman filter, where the gain matrix is determined by optimizing an upper bound matrix for the estimation error covariance despite the linearization errors and coupling terms. Compared with the existing estimators for linearly coupled complex networks, a distinct feature of the proposed estimator is that the gain matrix can be derived separately for each node by solving two Riccati-like difference equations. By using the stochastic analysis techniques, sufficient conditions are established which guarantees the state estimation error is bounded in mean square. A numerical example is provided to show the effectiveness and applicability of the proposed estimator.","Covariance matrices,
Estimation error,
Upper bound,
Complex networks,
Symmetric matrices,
Linear matrix inequalities,
Couplings"
Composite Intelligent Learning Control of Strict-Feedback Systems With Disturbance,"This paper addresses the dynamic surface control of uncertain nonlinear systems on the basis of composite intelligent learning and disturbance observer in presence of unknown system nonlinearity and time-varying disturbance. The serial-parallel estimation model with intelligent approximation and disturbance estimation is built to obtain the prediction error and in this way the composite law for weights updating is constructed. The nonlinear disturbance observer is developed using intelligent approximation information while the disturbance estimation is guaranteed to converge to a bounded compact set. The highlight is that different from previous work directly toward asymptotic stability, the transparency of the intelligent approximation and disturbance estimation is included in the control scheme. The uniformly ultimate boundedness stability is analyzed via Lyapunov method. Through simulation verification, the composite intelligent learning with disturbance observer can efficiently estimate the effect caused by system nonlinearity and disturbance while the proposed approach obtains better performance with higher accuracy.","Fuzzy logic,
Observers,
Control systems,
Time-varying systems,
Sun,
Stability analysis"
Evaluation of Hierarchical Watersheds,"This paper aims to understand the practical features of hierarchies of morphological segmentations, namely the quasi-flat zones hierarchy and watershed hierarchies, and to evaluate their potential in the context of natural image analysis. We propose a novel evaluation framework for the hierarchies of partitions designed to capture various aspects of those representations: precision of their regions and contours, possibility to extract high quality horizontal cuts and optimal non-horizontal cuts for image segmentation, and the ease of finding a set of regions representing a semantic object. This framework is used to assess and to optimize hierarchies with respect to the possible pre- and post-processing steps. We show that, used in conjunction with a state-of-the-art contour detector, watershed hierarchies are competitive with the complex state-of-the-art methods for hierarchy construction. In particular, the proposed framework allows us to identify a watershed hierarchy based on a novel extinction value, the number of parent nodes that outperforms the other hierarchies of morphological segmentations. This coupled with the fact that watershed hierarchies satisfy clear global optimality properties and can be efficiently computed on large data, make them valuable candidates for various computer vision tasks.",
Analysis With Histogram of Connectivity: For Automated Evaluation of Piping Layout,"An autonomous framework to evaluate layout of a piping design in the form of piping and instrumentation diagram (P&ID) according to a set of standards of marine and offshore industry is proposed. The method starts with transforming a P&ID into a vector
x
in
R
d
. Transformation is done based on a concept introduced for piping known as Histogram of Connectivity. The proposed descriptor captures two essential properties of P&ID: attributes of each component and connectivity among the components. Next, linear support vector machine (SVM) is used to learn a classifier from existing compliant and noncompliant designs. Subsequently, the linear classifier can be used to check if an unseen design complies with the standards. In addition, to enable follow up on noncompliant design including correction or modification, a method to analyze the reason of noncompliance prediction by the learned SVM model is introduced. The method has demonstrated encouraging performance in two challenging data sets of designs created with advice from experienced engineers in the industry, based on International Convention for the Prevention of Pollution from Ships (MARPOL) and Rules for Classification of Ships of Lloyd’s Register. Note to Practitioners—This paper is motivated by need of marine and offshore industry for automated solution for design appraisal. This paper aims to address this issue by using a machine learning-based approach. Some compliant and noncompliant designs are provided to a developed algorithm for a machine (or computer) to learn. After learning is completed, the machine is able to classify unseen designs as compliant or noncompliant. As highlighted in this paper, the developed method has demonstrated encouraging performance in two case studies, including specific parts in MARPOL and Rules of Lloyd’s Register. For adoption by industry, necessary steps include collecting some designs (compliant and noncompliant) available in an organization and feeding these into the developed method for learning by machine before it can predict. With ability of highlighting possible connections that cause noncompliance, follow up and correction on a noncompliant design is made possible.",
On the Equivalence of HLLE and LTSA,"Among the representative algorithms of manifold learning, Hessian locally linear embedding (HLLE) and local tangent space alignment (LTSA) algorithms haven been regarded as two different algorithms. However, in practice, the effects of these two algorithms are very similar and LTSA performs better than HLLE in some applications. This paper tries to account for this phenomenon from a mathematical point of view. There are only two differences between HLLE and LTSA. First, LTSA includes a data point into its neighborhood, while HLLE does not. Second, HLLE and LTSA use different methods to align the local coordinates of manifold. In this paper, we show that, the first difference between HLLE and LTSA is not essential. However, from the viewpoint of data utilization, LTSA does better than HLLE in the neighborhood construction. This may account for why LTSA can perform better than HLLE in some applications. As for the second difference between HLLE and LTSA, we first prove that, the alignment equations used by HLLE and LTSA are exactly the same. Second, we prove that, although HLLE and LTSA uses different methods to solve the alignment equation, their solutions are exactly the same, provided that HLLE adopts the same method as LTSA to construct the neighborhoods. Based on these arguments, we claim that HLLE and LTSA are equivalent to each other. This conclusion can also be verified experimentally by using manifold learning MATLAB demo (MANI), a widely-used experimental platform of manifold learning. When testing HLLE on MANI, if HLLE adopts the same method as LTSA to construct the neighborhoods, the experimental results presented by MANI will be the same as those of LTSA.","Manifolds,
Matrix decomposition,
Linear programming,
Principal component analysis,
Cybernetics,
Information technology,
Sun"
Achievable Rate of the Multi-User Two-Way Full-Duplex Relay System,"In contrast to the traditional assumption of the independent and identically distributed fading channels, this paper studies the achievable rate of the multi-user two-way full-duplex (FD) relay system under the independent and non-identically distributed (i.ni.d.) fading channels, which leads to the different selection probabilities and different weighted rates of different user pairs contributing to the overall system achievable rate as well. To be specific, we first derive the user pair selection probabilities of the Max-Min scheduling scheme for the i.ni.d. channels. Then, based on the selection probabilities, we solve the hard problem of the weighted rate derivation of each pair. Finally, according to the total probability theorem, the general expressions of the system achievable rate are obtained. In addition, theoretical evaluations and numerical simulations are conducted. The results verify the accuracy of the theoretical analysis and reveal the superiority of the FD mode especially within the low transmitting power regime.","Relays,
Fading channels,
Protocols,
Electronic mail,
Wireless communication,
Interference,
Closed-form solutions"
New Stability Criteria of Delayed Load Frequency Control Systems via Infinite-Series-Based Inequality,"A new approach is proposed for the stability problem of delayed load frequency control (LFC) scheme with fixed and time-varying delay cases included in the current paper. New stability criteria with delay dependence in terms of linear matrix inequalities for LFC systems are derived by a novel augmented Lyapunov–Krasovski (L–K) functional. Our proof deployment for system stability of power grids employs the further improved integral inequality in the form of infinite series, which turns out to be less conservative than Wirtinger's inequality that encompasses Jensen inequality. Simulation case studies are carried out to show the effectiveness and superiority of the presented delay-dependent PI-type LFC design scheme.",
Using Degradation Messages to Predict Hydraulic System Failures in a Commercial Aircraft,"This paper presents a failure prognostics methodology based on degradation messages using a particle filter framework. In the proposed method, the degradation messages are interpreted as quantized measurements. The use of degradation messages reduces the need for data communication bandwidth, since they only need to be transmitted or stored when some predefined threshold is crossed. In contrast, the direct monitoring of degradation-related measurements requires frequent updates, usually at a fixed sampling rate. This characteristic is fundamental on failure prognostics applications in real aircraft due to the high infrastructure costs associated with data transmission. A case study using field data recorded from commercial aircraft is presented to illustrate the proposed methodology. The problem under consideration consists of estimating the time remaining until the fluid level reaches unacceptably low values in the hydraulic system. Two quantization steps are considered in the evaluation. Predictions employing direct measurements of fluid level are also performed to establish a comparative performance baseline. The results show that it is possible to choose a quantization step that allows a reduction in the transmission costs without significant loss of prognostics performance.
Note to Practitioners—Failure prediction is a topic of great interest in various industry sectors. Predicting these events is challenging not only because of the advanced techniques involved in signal processing but also the availability of these signals with proper quality. Specifically in the aeronautical industry, the availability of signals for failure prediction may rely on in-flight data transmission systems. The usage of such systems can lead to prohibitive costs when transmitting large quantities of data. As a consequence, most commercial aircraft monitoring systems can only send a small set of failure messages indicating that the systems are failed or, in some cases, that they achieved an intermediate degradation level. This paper proposes an adaptation of a state-of-the-art failure prognostics algorithm (particle filter) that uses intermediate degradation messages to predict failure instants. The method is tested with real commercial aircraft data and the impact of the number of intermediate degradation levels is analyzed.",
No Reference Quality Assessment for Screen Content Images With Both Local and Global Feature Representation,"In this paper, we propose a novel no reference quality assessment method by incorporating statistical luminance and texture features (NRLT) for screen content images (SCIs) with both local and global feature representation. The proposed method is designed inspired by the perceptual property of the human visual system (HVS) that the HVS is sensitive to luminance change and texture information for image perception. In the proposed method, we first calculate the luminance map through the local normalization, which is further used to extract the statistical luminance features in global scope. Second, inspired by existing studies from neuroscience that high-order derivatives can capture image texture, we adopt four filters with different directions to compute gradient maps from the luminance map. These gradient maps are then used to extract the second-order derivatives by local binary pattern. We further extract the texture feature by the histogram of high-order derivatives in global scope. Finally, support vector regression is applied to train the mapping function from quality-aware features to subjective ratings. Experimental results on the public large-scale SCI database show that the proposed NRLT can achieve better performance in predicting the visual quality of SCIs than relevant existing methods, even including some full reference visual quality assessment methods.",
Multi-Objective Pareto Optimization of Electromagnetic Devices Exploiting Kriging With Lipschitzian Optimized Expected Improvement,This paper focuses on resolving the storage issue of correlation matrices generated by kriging surrogate models in the context of electromagnetic optimization problems with many design variables and multiple objectives. The suggested-improved kriging approach incorporating a direct algorithm is able to maintain memory requirements at a nearly constant level while offering high efficiency of searching for a global optimum. The feasibility and efficiency of this proposed methodology are demonstrated using an example of a classic two-variable analytic function and a new proposed benchmark TEAM multi-objective Pareto optimization problem.,"Correlation,
Pareto optimization,
Linear programming,
Memory management,
Benchmark testing,
Predictive models"
Discovering the Relationship Between Generalization and Uncertainty by Incorporating Complexity of Classification,"The generalization ability of a classifier learned from a training set is usually dependent on the classifier’s uncertainty, which is often described by the fuzziness of the classifier’s outputs on the training set. Since the exact dependency relation between generalization and uncertainty of a classifier is quite complicated, it is difficult to clearly or explicitly express this relation in general. This paper shows a specific study on this relation from the viewpoint of complexity of classification by choosing extreme learning machines as the classification algorithms. It concludes that the generalization ability of a classifier is statistically becoming better with the increase of uncertainty when the complexity of the classification problem is relatively high, and the generalization ability is statistically becoming worse with the increase of uncertainty when the complexity is relatively low. This paper tries to provide some useful guidelines for improving the generalization ability of classifiers by adjusting uncertainty based on the problem complexity.","Training,
Uncertainty,
Mathematical model,
Complexity theory,
Cognition,
Algorithm design and analysis,
Indexes"
"A 173 GHz Amplifier With a 18.5 dB Power Gain in a 130 nm SiGe Process: A Systematic Design of High-Gain Amplifiers Above
f
max
/2","A novel theory of stability for two-port networks is developed. Using this theory, a new method of designing amplifiers with a high-power gain working close to the maximum frequency of oscillation (
f
max
) is proposed. Contrary to the existing amplifier design methodologies, in this method, the transistor capability of power amplification is fully utilized. This becomes more important at frequencies close to the
f
max
where having a high-power gain is challenging due to the degraded activity of the employed device. The proposed method considers the modeling errors and process–voltage–temperature variations of the employed components in the design stage to ensure that the fabricated amplifier will be stable with a decent power gain even if the worst case variations and modeling errors happen. To show the feasibility of the proposed approach, a three-stage amplifier at 173 GHz, using bipolar junction transistors from a 130 nm SiGe process, is designed. The fabricated amplifier has a maximum measured power gain of 18.5 dB at 173 GHz. A similar three-stage amplifier using the same transistors with the same bias would give a maximum gain of 6.8 dB in simulation, assuming perfect lossless conjugate matching at input, output, and between stages. So it is clear that the fabricated amplifier achieves a significant improvement over the power gain.","Gain,
Transducers,
Stability analysis,
Transistors,
Silicon germanium,
Ports (Computers),
Power measurement"
Multiround Private Information Retrieval: Capacity and Storage Overhead,"Private information retrieval (PIR) is the problem of retrieving one message out of K messages from N noncommunicating replicated databases, where each database stores all K messages, in such a way that each database learns no information about which message is being retrieved. The capacity of PIR is the maximum number of bits of desired information per bit of downloaded information among all PIR schemes. The capacity has recently been characterized for PIR as well as several of its variants. In every case it is assumed that all the queries are generated by the user simultaneously. Here we consider multiround PIR, where the queries in each round are allowed to depend on the answers received in previous rounds. We show that the capacity of multiround PIR is the same as the capacity of single-round PIR. The result is generalized to also include T-privacy constraints. Combined with previous results, this shows that there is no capacity advantage from multiround over single-round schemes, non-linear over linear schemes or from ϵ-error over zero-error schemes. However, we show through an example that there is an advantage in terms of storage overhead. We provide an example of a multiround, non-linear, ϵ-error PIR scheme that requires a strictly smaller storage overhead than the best possible with single-round, linear, zero-error PIR schemes.",
A K-Band Backscatter Fiducial for Continuous Calibration in Coherent Millimeter-Wave Imaging,"We present a modulated ultrawideband backscatter calibration target (fiducial) intended for group delay calibration in large-aperture multitransceiver millimeter-wave imagers. The fiducial is designed to resemble a modulated point scatterer across the K-band (17.5–26.5 GHz). Multiple such fiducials may be used to mitigate thermal and mechanical drift across multiple transceivers comprising the imager. This approach allows tracking and removing both time-varying amplitude and phase drift in the RF hardware and associated cables. Backscatter modulation of the fiducial allows the system to separate the fiducial from the imaged scene and clutter in the environment. We show that the −10 dB beamwidth of the proposed fiducial is approximately 84° along the azimuth plane and 60° along the elevation plane. A proof of concept group delay calibration experiment is presented for a K-band laboratory setup, where a single fiducial and a metal plate target are placed in a scene together. After the backscatter-based calibration, the measured range error of the metal plate at a two-way slant distance of 70.54 cm is reduced to only 1.06 mm (0.15% position error).",
Collision Detection and Signal Recovery for UHF RFID Systems,"In this paper, we present a novel high-throughput anti-collision algorithm for passive ultrahigh-frequency (UHF) radio-frequency identification (RFID) systems. Our algorithm utilizes signal recovery techniques of collided tag signals to both recover tag communications and obtain an accurate count of all tags in the field. Passive UHF RFID systems are used for long-range passive communications for applications including supply chain management and electronic tolling. Anti-collision algorithms are used to ensure successful RFID tag communications due to the likelihood of multiple tags being in the field and attempting to communicate simultaneously. The limited on-tag functionality necessitates the use of simple anti-collision algorithms such as a dynamic frame slotted Aloha (DFSA) algorithm. With our novel collision detection and signal recovery anti-collision algorithm, the RFID reader can retrieve multiple valid communications from each collided slot in a DFSA-based anti-collision protocol, while our algorithm allocates an optimal number of slots resulting in more collided but recoverable slots and fewer empty slots. Our algorithm achieves a nearly 100% throughput improvement with an expected throughput of 0.85 compared with an expected throughput of 0.426 for a standard DFSA algorithm. The reader receiver with the proposed algorithm is implemented in a field-programmable gate array and the whole reader system is verified using the communication tests with commercial tags. According to the synthesized results in an SMIC 0.13-
μm
CMOS technology, the collision detection and signal recovery module consumes about 135k GE.
Note to Practitioners—Signal recovery methods are capable of extracting information from collided signals. In this paper, we introduce and analyze a signal recovery method based on the voltage histogram of the received signal. Both the original signals and the number of collided signals are recovered. The information of tags is used by our novel algorithm to increase the system throughput in DFSA-based anti-collision algorithms. The recovered signals allow our algorithm to directly obtain the tag identifier. By determining the number of collided signals, our algorithm is able to calculate an accurate estimate of the total number of tags in the reader’s field. With signal recovery and an accurate tag count, our algorithm is able to reduce the total number of slots needed to identify all tags, thereby increasing the throughput. We present a novel frame length optimization method that increases throughout by shrinking the number of slots, thereby intentionally causing collisions and reducing the total number of empty slots. In addition, we present a hardware implementation of our novel method that is suitable for integration into RFID readers. When the signal strengths of the tags in collision in the in-phase or quadrature path are not close to each other, the performance of our method is much better than that of the traditional method in which the collision signal is treated as ineffective and ignored.","Signal processing algorithms,
Throughput,
Radiofrequency identification,
Heuristic algorithms,
Collision avoidance,
Hardware,
Algorithm design and analysis"
The Modeling and Analysis of the Extensible Network Service Model,"With the increasing number of new application requirements in network, the capability of providing services in traditional network is being challenged constantly. The network service model which is the core of the network architecture directly determines the capability of providing network services. However, there lacks the explicit service model about the research of the next generation network architectures. We propose the extensible network service model. In this paper, the basic principle of the extensible network service model is summarized; and then, the mathematical modeling of the extensible network service model is provided; based on the mathematical modeling, we analyze the functional description and the performance calculation for composite services when a service composition instance is implemented; finally, through the experimental modeling and the simulation, the performances of the extensible network service model are compared with those of the traditional network when providing the same service. The experimental results show that services can be customized or composed flexibly without sacrificing the performances in the extensible network service model. And, it can be seen that the model can implement service extension better and the model idea is correct.",
Vulnerability Analysis for the Authentication Protocols in Trusted Computing Platforms and a Proposed Enhancement of the OffPAD Protocol,"Trusted computing architecture ensures the behavior of software that runs on a user machine by protecting software-level attacks. Due to the potential of exposing a user’s private information while accessing a system, many studies have focused on analyzing existing protocols to develop new methods based on biometrics or additional devices to add new layers of security to the authentication process. For a few years, the idea of utilizing the combination of something you know with something you have and a personal authentication device (PAD) has become common in verification protocols. Very recently, a more secure PAD, namely the Offline Personal Authentication Device (OffPAD), was invented to improve the authentication process. This single device can be used to manage the identities of both users and service providers as well as support the authentication process, while being offline most of the time. In this paper, a rigorous vulnerability analysis for OffPAD-based authentication techniques is conducted using an attack tree analysis. Finally, to overcome the vulnerabilities, mitigation techniques are proposed.","Authentication,
Protocols,
Servers,
Hardware,
Biometrics (access control)"
MemepiC: Towards a Unified In-Memory Big Data Management System,"In-memory data management systems have recently gained a lot of traction due to cheaper and faster DRAM and other hardware advancement. However, these systems are either pure storage systems with online data query service, or just offline batch processing systems with data analytics functionality. Heavy data movement (e.g., data loading) occurs in order to analyze the data. In this paper, we propose an innovative in-memory data management system -- MemepiC, which unifies both online data query and data analytics functionality, allowing low-latency storage service and efficient in-situ data analytics. We also explore the recent emerging RDMA technique in the context of in-memory data management systems, by designing an RDMA-based communication protocol for message delivery inside MemepiC, and proposing to overlap execution and RDMA communication. Extensive experiments are conducted to show the superior performance of MemepiC in terms of both the storage and the data analytics services, compared against other in-memory systems.","Data analysis,
Big Data,
Memory management,
Random access memory,
Engines,
Kernel,
Protocols"
Error Correcting Input and Output Hashing,"Most learning-based hashing algorithms leverage sample-to-sample similarities, such as neighborhood structure, to generate binary codes, which achieve promising results for image retrieval. This type of methods are referred to as instance-level encoding. However, it is nontrivial to define a scalar to represent sample-to-sample similarity encoding the semantic labels and the data structure. To address this issue, in this paper, we seek to use a class-level encoding method, which encodes the class-to-class relationship, to take the semantic information of classes into consideration. Based on these two encodings, we propose a novel framework, error correcting input and output (EC-IO) coding, which does class-level and instance-level encoding under a unified mapping space. Our proposed model contains two major components, which are distribution preservation and error correction. With these two components, our model maps the input feature of samples and the output code of classes into a unified space to encode the intrinsic structure of data and semantic information of classes simultaneously. Under this framework, we present our hashing model, EC-IO hashing (EC-IOH), by approximating the mapping space with the Hamming space. Extensive experiments are conducted to evaluate the retrieval performance, and EC-IOH exhibits superior and competitive performances comparing with popular supervised and unsupervised hashing methods.","Encoding,
Binary codes,
Semantics,
Data structures,
Linear programming,
Manifolds,
Cybernetics"
Cost Effective Network Flow Measurement for Software Defined Networks: A Distributed Controller Scenario,"Software Defined Networking (SDN) has emerged as an evolutionary paradigm in Datacenter Networks (DCNs) by separating data from control plane and centralizing network decision making. Traffic flow measurement in SDN is relatively light-weight in comparison to the traditional methods. It enables flow measurement system to overcome the issues of traditional measurement systems such as cost and accuracy by employing a centralized controller. Nevertheless, a full physically centralized controller introduces negative impacts on the network as well as the measurement system (i.e., introducing extra overhead or accuracy issues). However, few efforts have been devoted to measurement techniques in SDN distributed controller architecture where every controller pulls its corresponding flow statistics and these statistics are required to expose by only one single expression as if they are collected by one controller. Moreover, the imposed costs of flow measurement in distributed controller architecture is still an issue that remains unsolved. In this paper, we attempt to fill in this gap and present a novel and a practical solution for cost-effective measurement system in SDN distributed controller deployment. We also propose a synchronization mechanism for aggregating traffic statistics in the multiple controller model. We evaluate our method through extensive emulations in a datacenter topology and present our findings to demonstrate the impact of multiple controller on overhead and accuracy.","Monitoring,
Software defined networking,
Telecommunication traffic,
Tools,
Software measurement,
Synchronization,
Emulation"
Body Parts Synthesis for Cross-Quality Pose Estimation,"Although encouraging results have been obtained in human pose estimation in recent years, the performance may degrade dramatically when the image quality differs between training and testing datasets. This paper addresses problems in cross-image-quality human pose estimation. To achieve this, we follow unsupervised domain adaptation approach in which labels in the target domain are unavailable. Unlike existing unsupervised domain adaptation methods that find label information from unlabeled data, the target pose information (label) is instead generated by synthesizing body parts with similar image-quality of the target domain. A translative dictionary is learned to associate the source and target domains, and a crossquality adaptation model is developed to refine the source pose estimator using the synthesized target body parts. We perform cross-quality experiments on three datasets with different image quality by using two state-of-the-art pose estimators and compare the proposed method with five unsupervised domain adaptation methods. Our experimental results show that the proposed method outperforms not only the source pose estimators, but also other unsupervised domain adaptation methods.","Pose estimation,
Image quality,
Training,
Testing,
Dictionaries,
Adaptation models,
Skeleton"
Deep Regression Segmentation for Cardiac Bi-ventricle MR Images,"Cardiac bi-ventricle segmentation can help physicians to obtain clinical indices such as mass and volume of left ventricle (LV) and right ventricle (RV). In this paper, we propose a regression segmentation framework to delineate boundaries of bi-ventricle from cardiac MR images by building a regression model automatically and accurately. First, we extract DAISY feature from images. Then, a point based representation method is employed to depict the boundaries. Finally, we use DAISY as input and boundary points as labels to train the regression model based on Deep Belief Network. Regression combined deep learning and DAISY feature can capture high level image information and accurately segment bi-ventricle with fewer assumptions and lower computational cost. In our experiment, the performance of the proposed framework is compared to manual segmentation on 145 clinical subjects (2900 images in total), which are collected from 3 hospitals affiliated with two health care centers (London Healthcare Center and St. Josephs HealthCare). The results of our method and manually segmented method are highly consistent. High Pearson’s correlation coefficient (PCC) between automated boundaries and manual annotation is up to 0.995 (endocardium of LV), 0.997 (epicardium of LV), 0.985 (RV). Average Dice metric (DM) is up to 0.916 (endocardium of LV), 0.941 (epicardium of LV), 0.844 (RV). Altogether, experimental results are capable of demonstrating the efficacy of our regression segmentation framework for cardiac MR images.",
Cloud-Based Approximate Constrained Shortest Distance Queries Over Encrypted Graphs With Privacy Protection,"Constrained shortest distance (CSD) querying is one of the fundamental graph query primitives, which finds the shortest distance from an origin to a destination in a graph with a constraint that the total cost does not exceed a given threshold. CSD querying has a wide range of applications, such as routing in telecommunications and transportation. With an increasing prevalence of cloud computing paradigm, graph owners desire to outsource their graphs to cloud servers. In order to protect sensitive information, these graphs are usually encrypted before being outsourced to the cloud. This, however, imposes a great challenge to CSD querying over encrypted graphs. Since performing constraint filtering is an intractable task, existing work mainly focuses on unconstrained shortest distance queries. CSD querying over encrypted graphs remains an open research problem. In this paper, we propose Connor, a novel graph encryption scheme that enables approximate CSD querying. Connor is built based on an efficient, tree-based ciphertext comparison protocol, and makes use of symmetric-key primitives and the somewhat homomorphic encryption, making it computationally efficient. Using Connor, a graph owner can first encrypt privacy-sensitive graphs and then outsource them to the cloud server, achieving the necessary privacy without losing the ability of querying. Extensive experiments with real-world data sets demonstrate the effectiveness and efficiency of the proposed graph encryption scheme.","Cloud computing,
Encryption,
Privacy,
Protocols,
Indexes"
BioPad: Leveraging off-the-Shelf Video Games for Stress Self-Regulation,"This paper presents an approach to use commercial videogames for biofeedback training. It consists of intercepting signals from the game controller and adapting them in real-time based on physiological measurements from the player. We present three sample implementations and a case study for teaching stress self-regulation via an immersive car racing game. We use a crossover gaming device to manipulate controller signals, and a respiratory sensor to monitor the players’ breathing rate. We then alter the speed of the car to encourage slow deep breathing, in this way, allowing players to reduce their arousal while playing the game. We evaluate the approach against an alternative form of biofeedback that uses a graphic overlay to convey physiological information, and a control condition (playing the game without biofeedback). Experimental results show that our approach can promote deep breathing during gameplay, and also during a subsequent task, once biofeedback is removed. Our results also indicate that delivering biofeedback through subtle changes in gameplay can be as effective as delivering them directly through a visual display. These results open the possibility to develop low-cost and engaging biofeedback interventions using a variety of commercial videogames to promote adherence.",
Anonymous and Traceable Group Data Sharing in Cloud Computing,"Group data sharing in cloud environments has become a hot topic in recent decades. With the popularity of cloud computing, how to achieve secure and efficient data sharing in cloud environments is an urgent problem to be solved. In addition, how to achieve both anonymity and traceability is also a challenge in the cloud for data sharing. This paper focuses on enabling data sharing and storage for the same group in the cloud with high security and efficiency in an anonymous manner. By leveraging the key agreement and the group signature, a novel traceable group data sharing scheme is proposed to support anonymous multiple users in public clouds. On the one hand, group members can communicate anonymously with respect to the group signature, and the real identities of members can be traced if necessary. On the other hand, a common conference key is derived based on the key agreement to enable group members to share and store their data securely. Note that a symmetric balanced incomplete block design is utilized for key generation, which substantially reduces the burden on members to derive a common conference key. Both theoretical and experimental analyses demonstrate that the proposed scheme is secure and efficient for group data sharing in cloud computing.","Cloud computing,
Information management,
Encryption,
Access control,
Maintenance engineering"
Speech2Health: A Mobile Framework for Monitoring Dietary Composition From Spoken Data,"Diet and physical activity are known as important lifestyle factors in self-management and prevention of many chronic diseases. Mobile sensors such as accelerometers have been used to measure physical activity or detect eating time. In many intervention studies, however, stringent monitoring of overall dietary composition and energy intake is needed. Currently, such a monitoring relies on self-reported data by either entering text or taking an image that represents food intake. These approaches suffer from limitations such as low adherence in technology adoption and time sensitivity to the diet intake context. In order to address these limitations, we introduce development and validation of Speech2Health, a voice-based mobile nutrition monitoring system that devises speech processing, natural language processing (NLP), and text mining techniques in a unified platform to facilitate nutrition monitoring. After converting the spoken data to text, nutrition-specific data are identified within the text using an NLP-based approach that combines standard NLP with our introduced pattern mapping technique. We then develop a tiered matching algorithm to search the food name in our nutrition database and accurately compute calorie intake values. We evaluate Speech2Health using real data collected with 30 participants. Our experimental results show that Speech2Health achieves an accuracy of 92.2% in computing calorie intake. Furthermore, our user study demonstrates that Speech2Health achieves significantly higher scores on technology adoption metrics compared to text-based and image-based nutrition monitoring. Our research demonstrates that new sensor modalities such as voice can be used either standalone or as a complementary source of information to existing modalities to improve the accuracy and acceptability of mobile health technologies for dietary composition monitoring.",
A Low Complexity Robust Beamforming Using Diagonal Unloading for Acoustic Source Localization,"In acoustic array processing, beamforming is a class of algorithms commonly used to estimate the position of a radiating sound source. This paper presents a diagonal unloading (DU) transformation method for the conventional response power beamforming to achieve robust localization with low computational complexity. The transformation is obtained by subtracting an opportune diagonal matrix from the covariance matrix of the array output vector. Specifically, the DU beamformer aims at subtracting the signal subspace from the noisy signal space. It is hence a data-dependent covariance matrix conditioning method. We show how to calculate precisely the unloading parameters, and we present a comparison of the proposed DU beamforming, the robust minimum variance distortionless response (MVDR) filter and the multiple signal classification (MUSIC) method, in terms of their respective eigenanalyses. Theoretical analysis and experiments conducted on both simulated and real acoustic data demonstrate that the DU beamformer localization performance is comparable to that of robust MVDR and MUSIC. Since its computational cost is equivalent to that of a conventional beamformer, the proposed DU beamformer method can thus be very attractive due to its effectiveness and computational efficiency.",
Face Demorphing,"The morphing attack proved to be a serious threat for modern automated border control systems where face recognition is used to link the identity of a passenger to his/her e-document. In this paper, we show that by exploiting the live face image acquired at the gate, the morphed face image stored in the document can be reverted (or demorphed) enough to reveal the identity of the legitimate document owner, thus allowing the system to issue a warning. A number of practical experiments on two data sets proves the efficacy of our approach.",
Fast Image Super-Resolution via Local Adaptive Gradient Field Sharpening Transform,"This paper proposes a single-image super-resolution scheme by introducing a gradient field sharpening transform that converts the blurry gradient field of upsampled low-resolution (LR) image to a much sharper gradient field of original highresolution (HR) image. Different from the existing methods that need to figure out the whole gradient profile structure and locate the edge points, we derive a new approach that sharpens the gradient field adaptively only based on the pixels in a small neighborhood. To maintain image contrast, image gradient is adaptively scaled to keep the integral of gradient field stable. Finally the HR image is reconstructed by fusing the LR image with the sharpened HR gradient field. Experimental results demonstrate that the proposed algorithm can generate more accurate gradient field and produce super-resolved images with better objective and visual qualities. Another advantage is that the proposed gradient sharpening transform is very fast and suitable for low-complexity applications.","Image edge detection,
Transforms,
Image resolution,
Image reconstruction,
Visualization,
Adaptation models,
Interpolation"
Stain Formation on Deforming Inelastic Cloth,"We propose a novel approach to simulating the formation and evolution of stains on cloths in motion. We accurately capture the diffusion of a pigmented solution over a complex knitted or woven fabric through homogenization of its inhomogeneous and/or anisotropic properties into bulk anisotropic diffusion tensors. Secondary effects such as absorption, adsorption and evaporation are also accounted for through physically-based modeling. Finally, the influence of the cloth motion on the shape and evolution of the stain is captured by evaluating the inertial (e.g., centrifugal and Coriolis) forces experienced by the solution. The governing equations of motion are integrated in time directly on a deforming triangle mesh discretizing the inelastic cloth for efficiency and robustness. The deformation of the cloth can be precomputed or integrated through simplified two-way coupling, by using off-the-shell cloth simulations. Finally, numerical experiments demonstrate the plausibility of our results in practical applications by reproducing the usual shape and behavior of stains on various fabrics.","Mathematical model,
Weaving,
Fabrics,
Nonhomogeneous media,
Computational modeling,
Surface treatment,
Anisotropic magnetoresistance"
Multiscale Rotation-Invariant Convolutional Neural Networks for Lung Texture Classification,"We propose a new multiscale rotation-invariant convolutional neural network (MRCNN) model for classifying various lung tissue types on high-resolution computed tomography. MRCNN employs Gabor-local binary pattern that introduces a good property in image analysis—invariance to image scales and rotations. In addition, we offer an approach to deal with the problems caused by imbalanced number of samples between different classes in most of the existing works, accomplished by changing the overlapping size between the adjacent patches. Experimental results on a public interstitial lung disease database show a superior performance of the proposed method to state of the art.","Lungs,
Feature extraction,
Informatics,
Support vector machines,
Neural networks,
Biomedical imaging,
Computed tomography"
Predicting the Risk of Heart Failure with EHR Sequential Data Modeling,"Electronic health records (EHRs) contain patient diagnostic records, physician records, and records of hospital departments. For heart failure, we can obtain mass unstructured data from EHR time series. By analyzing and mining these time-based EHRs, we can identify the links between diagnostic events and ultimately predict when a patient will be diagnosed. However, it is difficult to use the existing EHR data directly because they are sparse and non-standardized. Thus, this paper proposes an effective and robust architecture for heart failure prediction. The main contribution of this paper is to predict heart failure using a neural network (i.e., to predict the possibility of cardiac illness based on a patient’s electronic medical data). Specifically, we employed one-hot encoding and word vectors to model the diagnosis events and predicted heart failure events using the basic principles of a long short-term memory network (LSTM) model. Evaluations based on a real-world dataset demonstrate the promising utility and efficacy of the proposed architecture in the prediction of the risk of heart failure.","Heart,
Predictive models,
Logic gates,
Time series analysis,
Data models,
Neural networks,
Numerical models"
Nucleus Segmentation Using Gaussian Mixture based Shape Models,"We identify cells in microscopy images with stained nuclei, using the following process: Candidate seeds for nuclei are identified as extrema in a Laplacian-of-Gaussian space, and weak candidates are eliminated from clusters obtained by ellipse fitting; a region of interest for each nucleus is then defined by combining local and global thresholding; and these regions are repeatedly merged and split by modeling the shape of a nucleus and measuring the roughness of the shared boundaries connected nuclei. This method showed superior abilities to detect the nucleus regions and to split the boundaries of connected nuclei. Our experiments show higher scores in comparison with five other techniques in terms of eight evaluation metrics.",
Eliminating Leakage in Reverse Fuzzy Extractors,"In recent years, physically unclonable functions (PUFs) have been proposed as a promising building block for key storage and device authentication. PUFs are physical systems, and as such, their responses are inherently noisy, precluding a straightforward derivation of cryptographic key material from raw PUF measurements. To overcome this drawback, fuzzy extractors are used to eliminate the noise and guarantee robust outputs. A special type is reverse fuzzy extractors, shifting the computational load of error correction toward a computationally powerful verifier. However, the reverse fuzzy extractor reveals error patterns to any eavesdropper, which may cause privacy issues (due to a systematic drift of the PUF responses, the error pattern is linkable to the identity) and even security problems (if the noise is data-dependent). In this paper, we quantify the issue of leakage due to asymmetry of noise, leveraging the binary asymmetric channel (BAC) model. We further propose to concatenate two BACs to form a symmetric channel, as a solution that is able to eliminate such noise. Finally, we propose a modified reverse fuzzy extractor that does not leak via the error patterns even in the case of systematic drift of the PUF responses.","Noise measurement,
Protocols,
Iron,
Decoding,
Measurement uncertainty,
Authentication,
Error correction"
PrivBioMTAuth: Privacy Preserving Biometrics-Based and User Centric Protocol for User Authentication From Mobile Phones,"We introduce a privacy preserving biometrics-based authentication solution by which users can authenticate to different service providers from mobile phones without involving identity providers in the transactions. Authentication is performed via zero-knowledge proof of knowledge, based on a cryptographic identity token that encodes the biometric identifier of the user and a secret provided by the user, making it three-factor authentication. Our approach for generating a unique, repeatable, and revocable biometric identifier from the user’s biometric image is based on a machine learning-based classification technique, which involves the features extracted from the user’s biometric image. We have implemented a prototype of the proposed authentication solution and evaluated our solution with respect to its performance, security, and privacy. The evaluation has been performed on a public data set of face images.",
User-Centric Networks Selection With Adaptive Data Compression for Smart Health,"The increasing demand for intelligent and sustainable healthcare services has prompted the development of smart health systems. Rapid advances in wireless access technologies and in-network data reduction techniques can significantly assist in implementing such smart systems through providing seamless integration of heterogeneous wireless networks, medical devices, and ubiquitous access to data. Utilization of the spectrum across diverse radio technologies is expected to significantly enhance network capacity and quality of service (QoS) for emerging applications such as remote monitoring over mobile-health (m-health) systems. However, this imposes an essential need to develop innovative networks selection mechanisms that account for energy efficiency while meeting application quality requirements. In this context, this paper proposes an efficient networks selection mechanism with adaptive compression for improving medical data delivery over heterogeneous m-health systems. We consider different performance aspects, as well as networks characteristics and application requirements, so as to obtain an efficient solution that grasps the conflicting nature of the various users’ objectives and addresses their inherent tradeoffs. The proposed methodology advocates a user-centric approach towards leveraging heterogeneous wireless networks to enhance the performance of m-health systems. Simulation results show that our solution significantly outperforms state-of-the-art techniques.","Handheld computers,
Smart phones,
Sensors,
Monitoring,
Wireless communication,
Cloud computing,
Medical services"
Adapting Stochastic Block Models to Power-Law Degree Distributions,"Stochastic block models (SBMs) have been playing an important role in modeling clusters or community structures of network data. But, it is incapable of handling several complex features ubiquitously exhibited in real-world networks, one of which is the power-law degree characteristic. To this end, we propose a new variant of SBM, termed power-law degree SBM (PLD-SBM), by introducing degree decay variables to explicitly encode the varying degree distribution over all nodes. With an exponential prior, it is proved that PLD-SBM approximately preserves the scale-free feature in real networks. In addition, from the inference of variational E-Step, PLD-SBM is indeed to correct the bias inherited in SBM with the introduced degree decay factors. Furthermore, experiments conducted on both synthetic networks and two real-world datasets including Adolescent Health Data and the political blogs network verify the effectiveness of the proposed model in terms of cluster prediction accuracies.",
Energy-Efficient Device-to-Device Communications for Green Smart Cities,"Huge demands for ubiquitous high data rate wireless communications have caused a sharp increase in energy consumption and greenhouse gas emission. In order to realize a sustainable development, it is critical to incorporate green communication technique into smart city developments. D2D communication has been recognized as one of the key technologies to improve data rate and reduce power consumption. In this paper, with the target of achieving green communications through D2D, we investigate the joint optimization of uplink subcarrier assignment and power allocation in D2D underlying cellular networks. Specifically, the problem formulation is to minimize the energy cost of all users in the system while guaranteeing the required data rate of both the D2D user equipments and cellular user equipments. Such an optimization problem is in general a mixed-integer nonlinear programming problem (MINLP) that is generally NP-hard. To make this problem tractable, we decompose it into the subcarrier assignment and power allocation subproblems. In particular, we design a heuristic algorithm to assign subcarrier by assuming that the transmit power is evenly allocated over all subcarriers. After that, we solve the power allocation subproblem by exploiting the difference between concave function (D.C.) structure of the constraints and transform it into a convex optimization problem. Simulation results demonstrate the remarkable improvement in terms of power consumption by using our algorithms.","Device-to-device communication,
Resource management,
Interference,
Smart cities,
Power demand,
Energy consumption,
Cellular networks"
A Near-Optimal Algorithm for Constraint Test Ordering in Automated Stowage Planning,"The container stowage planning problem is known to be NP-hard and heuristic algorithms have been proposed. Conventionally, the efficiency of the stowage planning algorithms are improved by pruning or reducing the search space. We observe that constraint evaluation is the core of most algorithms. In addition, the order at which the constraints are evaluated can have significant impact on the efficiency of the constraint evaluation engine. We propose random sample model (RSM) and sequential sample model (SSM) for analysis of the problem. We present and evaluate seven strategies in optimizing the constraint evaluation engine. We show how to achieve the optimal constraint ordering with respect to RSM and SSM, respectively. However, the optimal ordering for SSM requires perfect information about the states of the constraint tests, which is impractical. We present an alternative strategy and show empirically that its efficiency is close to the optimal. Experiments show that, compared to a naïve ordering, an average of 2.74 times speed up in the evaluation engine can be achieved.",
Deep-PRWIS: Periocular Recognition Without the Iris and Sclera Using Deep Learning Frameworks,"This paper is based on a disruptive hypothesis for periocular biometrics—in visible-light data, the recognition performance is optimized when the components inside the ocular globe (the iris and the sclera) are simply discarded, and the recognizer’s response is exclusively based on the information from the surroundings of the eye. As a major novelty, we describe a processing chain based on convolution neural networks (CNNs) that defines the regions-of-interest in the input data that should be privileged in an implicit way, i.e., without masking out any areas in the learning/test samples. By using an ocular segmentation algorithm exclusively in the learning data, we separate the ocular from the periocular parts. Then, we produce a large set of “multi-class” artificial samples, by interchanging the periocular and ocular parts from different subjects. These samples are used for data augmentation purposes and feed the learning phase of the CNN, always considering as label the ID of the periocular part. This way, for every periocular region, the CNN receives multiple samples of different ocular classes, forcing it to conclude that such regions should not be considered in its response. During the test phase, samples are provided without any segmentation mask and the network naturally disregards the ocular components, which contributes for improvements in performance. Our experiments were carried out in full versions of two widely known data sets (UBIRIS.v2 and FRGC) and show that the proposed method consistently advances the state-of-the-art performance in the closed-world setting, reducing the EERs in about 82% (UBIRIS.v2) and 85% (FRGC) and improving the Rank-1 over 41% (UBIRIS.v2) and 12% (FRGC).","Iris recognition,
Convolution,
Eyelids,
Feature extraction,
Machine learning"
Robust 2D Engineering CAD Graphics Hashing for Joint Topology and Geometry Authentication via Covariance-Based Descriptors,"This paper investigates the joint authentication of topology and geometry information of 2D engineering computer-aided design graphics, which focus more on topological modeling than geometric modeling of objects. A robust hashing scheme is proposed for joint topology and geometry authentication. The covariance matrices of descriptors are explored to fuse and encode both topology and geometry features of different types into a compact representation. First, a normalized binary shape texture is rendered for each geometric object through the render-to-texture technique. Then, for each geometric object, geometry features are computed based on statistical features that are extracted from image rings. Additionally, topology features are generated according to the topological relations among joint objects. To generate hash codes of the graphic, all geometric objects are first grouped according to their geometry features. Then, for each group, the covariance matrices of descriptors are applied to fuse both the topology and geometry features of all objects, and the intermediate hash codes of each group are computed based on the covariance matrices. The final hash sequence is formed by concatenating the intermediate hash codes that correspond to each group. Secret keys are introduced into both feature extraction and hash construction. The hashes are robust against topology-preserving graphic manipulations and sensitive to malicious attacks. By decomposing the hashes, the locations of tampered objects can be determined. Experimental results are presented to evaluate the performance and show the effectiveness of the method.",
EHDC: An Energy Harvesting Modeling and Profiling Platform for Body Sensor Networks,"Energy harvesting is a promising solution to the limited battery lifetimes of body sensor nodes. Self-powered sensor systems capable of quasi-perpetual operation enable the possibility of truly continuous monitoring of patients beyond the clinic. However, the discontinuous and dynamic characteristics of harvesting in real-world scenarios—and their implications for the design and operation of self-powered systems—are not yet well understood. This paper presents a mobile energy harvesting and data collection (EHDC) platform designed to provide a deeper understanding of energy harvesting dynamics. The EHDC platform monitors and records the instantaneous usable power generated by body-worn harvesters, while also collecting human activity and environmental data to provide a comprehensive real-world evaluation of two energy harvesting modalities common to body sensor networks: solar and thermoelectric. The platform was initially validated with benchtop tests and later with real-world deployments on two subjects. 7-h-long multimodal energy harvesting profiles were generated, and the environmental and behavioral data were used to expand upon previously developed Kalman filter based mathematical models for energy harvesting prediction. Results confirm the validity of the EHDC platform and harvesting models, establishing the potential for longer term monitoring of energy harvesting characteristics; thus, informing the design and operation of self-powered body sensor networks.",
Assessment of Gait Characteristics in Total Knee Arthroplasty Patients Using a Hierarchical Partial Least Squares Method,"Quantitative gait analysis is an important tool in objective assessment and management of total knee arthroplasty (TKA) patients. Studies evaluating gait patterns in TKA patients have tended to focus on discrete data such as spatiotemporal information, joint range of motion and peak values of kinematics and kinetics, or consider selected principal components of gait waveforms for analysis. These strategies may not have the capacity to capture small variations in gait patterns associated with each joint across an entire gait cycle, and may ultimately limit the accuracy of gait classification. The aim of this study was to develop an automatic feature extraction method to analyse patterns from high-dimensional autocorrelated gait waveforms. A general linear feature extraction framework was proposed and a hierarchical partial least squares method derived for discriminant analysis of multiple gait waveforms. The effectiveness of this strategy was verified using a dataset of joint angle and ground reaction force waveforms from 43 patients after TKA surgery and 31 healthy control subjects. Compared with principal component analysis and partial least squares methods, the hierarchical partial least squares method achieved generally better classification performance on all possible combinations of waveforms, with the highest classification accuracy
85.14%
. The novel hierarchical partial least squares method proposed is capable of capturing virtually all significant differences between TKA patients and the controls, and provides new insights into data visualization. The proposed framework presents a foundation for more rigorous classification of gait, and may ultimately be used to evaluate the effects of interventions such as surgery and rehabilitation.","Feature extraction,
Load modeling,
Principal component analysis,
Loading,
Knee,
Surgery,
Support vector machines"
Prediction of Patient-Controlled Analgesic Consumption: A Multimodel Regression Tree Approach,"Several factors contribute to individual variability in postoperative pain, therefore, individuals consume postoperative analgesics at different rates. Although many statistical studies have analyzed postoperative pain and analgesic consumption, most have identified only the correlation and have not subjected the statistical model to further tests in order to evaluate its predictive accuracy. In this study involving 3052 patients, a multistrategy computational approach was developed for analgesic consumption prediction. This approach uses data on patient-controlled analgesia demand behavior over time and combines clustering, classification, and regression to mitigate the limitations of current statistical models. Cross-validation results indicated that the proposed approach significantly outperforms various existing regression methods. Moreover, a comparison between the predictions by anesthesiologists and medical specialists and those of the computational approach for an independent test data set of 60 patients further evidenced the superiority of the computational approach in predicting analgesic consumption because it produced markedly lower root mean squared errors.",
Multimodal Neuroimaging Feature Learning With Multimodal Stacked Deep Polynomial Networks for Diagnosis of Alzheimer's Disease,"The accurate diagnosis of Alzheimer's disease (AD) and its early stage, i.e., mild cognitive impairment, is essential for timely treatment and possible delay of AD. Fusion of multimodal neuroimaging data, such as magnetic resonance imaging (MRI) and positron emission tomography (PET), has shown its effectiveness for AD diagnosis. The deep polynomial networks (DPN) is a recently proposed deep learning algorithm, which performs well on both large-scale and small-size datasets. In this study, a multimodal stacked DPN (MM-SDPN) algorithm, which MM-SDPN consists of two-stage SDPNs, is proposed to fuse and learn feature representation from multimodal neuroimaging data for AD diagnosis. Specifically speaking, two SDPNs are first used to learn high-level features of MRI and PET, respectively, which are then fed to another SDPN to fuse multimodal neuroimaging information. The proposed MM-SDPN algorithm is applied to the ADNI dataset to conduct both binary classification and multiclass classification tasks. Experimental results indicate that MM-SDPN is superior over the state-of-the-art multimodal feature-learning-based algorithms for AD diagnosis.",
Automatic Fetal Head Circumference Measurement in Ultrasound Using Random Forest and Fast Ellipse Fitting,"Head circumference (HC) is one of the most important biometrics in assessing fetal growth during prenatal ultrasound examinations. However, the manual measurement of this biometric by doctors often requires substantial experience. We developed a learning-based framework that used prior knowledge and employed a fast ellipse fitting method (ElliFit) to measure HC automatically. We first integrated the prior knowledge about the gestational age and ultrasound scanning depth into a random forest classifier to localize the fetal head. We further used phase symmetry to detect the center line of the fetal skull and employed ElliFit to fit the HC ellipse for measurement. The experimental results from 145 HC images showed that our method had an average measurement error of 1.7 mm and outperformed traditional methods. The experimental results demonstrated that our method shows great promise for applications in clinical practice.",
Optic Disk Detection in Fundus Image Based on Structured Learning,"Automated optic disk (OD) detection plays an important role in developing a computer aided system for eye diseases. In this paper, we propose an algorithm for the OD detection based on structured learning. A classifier model is trained based on structured learning. Then, we use the model to achieve the edge map of OD. Thresholding is performed on the edge map, thus a binary image of the OD is obtained. Finally, circle Hough transform is carried out to approximate the boundary of OD by a circle. The proposed algorithm has been evaluated on three public datasets and obtained promising results. The results (an area overlap and Dices coefficients of 0.8605 and 0.9181, respectively, an accuracy of 0.9777, and a true positive and false positive fraction of 0.9183 and 0.0102) show that the proposed method is very competitive with the state-of-the-art methods and is a reliable tool for the segmentation of OD.","Image edge detection,
Image segmentation,
Feature extraction,
Detectors,
Informatics,
Diseases,
Transforms"
Low-PAPR Layered/Enhanced ACO-SCFDM for Optical-Wireless Communications,"In this letter, we propose layered/enhanced asymmetrically clipped optical single-carrier frequency-division multiplexing (L/E-ACO-SCFDM) for optical-wireless communications. L/E-ACO-SCFDM has a lower computational complexity and peak-to-average power ratio (PAPR) than L/E-ACO orthogonal frequency-division multiplexing (L/E-ACO-OFDM). The computational complexity of the simplified transmitter in L/E-ACO-SCFDM with
R
layers is
(2−2/
2
R
)O(N)
, which is lower than the computational complexity of
(2−2/
2
R
)O(N
log
2
N)
in L/E-ACO-OFDM. At a complementary cumulative distribution function of
10
−3
, the PAPR of L/E-ACO-SCFDM is approximately 4.2, 3.4, and 2.7 dB lower than that of L/E-ACO-OFDM for 2, 3, and 4 layers, respectively. The simulation results indicate that L/E-ACO-SCFDM has better performance than L/E-ACO-OFDM under the transmitter nonlinearity and multipath fading.",
Specification and Verification of Separation of Duty Constraints in Attribute-Based Access Control,"Constraints form an important aspect of any access control system and are often regarded as one of the principle motivations behind developing different access control models. The two primary concerns related to a constraint are its specification and enforcement. Among the various types of constraints, enforcement of the Separation of Duty (SoD) constraint is considered to be the most important in commercial applications. In this paper, we introduce the problem of SoD specification, verification, and enforcement in attribute-based access control (ABAC) systems. We then demonstrate the effect of modifications in the different components of ABAC on enforcement. We also analyze the complexity of the enforcement problem and provide a methodology for solving it. Experiments on a wide range of data sets show encouraging results.","Authorization,
Organizations,
Complexity theory,
Standards organizations,
Indexes"
QoS-Aware Cloudlet Load Balancing in Wireless Metropolitan Area Networks,"With advances in wireless communication technology, more and more people depend heavily on portable mobile devices for businesses, entertainments and social interactions. This poses a great challenge of building a seamless application experience across different computing platforms. A key issue is the resource limitations of mobile devices due to their portable size, however this can be overcome by remotely executing computation-intensive tasks on clusters of near by computers called cloudlets. As increasing numbers of people access the Internet via mobile devices, it is reasonable to envision in the near future that cloudlet services will be available for the public through easily accessible public wireless metropolitan area networks. In this paper we investigate how to balance the workload among multiple cloudlets in such a network to optimize mobile application performance. We introduce a novel system model to capture the response time delays of offloaded tasks and formulate an optimization problem with the aim to minimize the maximum response time among offloaded tasks. We then propose two algorithms for the problem. We finally evaluate the performance of the proposed algorithms in realistic simulation environments. The experimental results demonstrate the significant potential of the proposed algorithms in reducing response time, maximizing user experience.",
X-ray and Proton Radiation Effects on 40 nm CMOS Physically Unclonable Function Devices,"Total ionizing dose effects are investigated on a physically unclonable function (PUF) based on CMOS breakdown. Devices irradiated to 2.0 Mrad(SiO2) show less than 11% change in current ratio at 1.2 V. The read-out window of programmed PUFs decreases significantly at high dose proton irradiation, and then recovers back to the original value after annealing. The proton test results for the pFET selector, the unbroken nFET, and the broken nFET indicate that the threshold voltage shift of the pFET selector contributes mainly to the degradation of the PUF.",
Deep construction of an affective latent space via multimodal enactment,"We draw on a simulationist approach to the analysis of facially displayed emotions - e.g., in the course of a face-to-face interaction between an expresser and an observer. At the heart of such perspective lies the enactment of the perceived emotion in the observer. We propose a novel probabilistic framework based on a deep latent representation of a continuous affect space, which can be exploited for both the estimation and the enactment of affective states in a multimodal space (visible facial expressions and physiological signals). The rationale behind the approach lies in the large body of evidence from affective neuroscience showing that when we observe emotional facial expressions, we react with congruent facial mimicry. Further, in more complex situations, affect understanding is likely to rely on a comprehensive representation grounding the reconstruction of the state of the body associated with the displayed emotion. We show that our approach can address such problems in a unified and principled perspective, thus avoiding ad hoc heuristics while minimising learning efforts.","Observers,
Computational modeling,
Face,
Visualization,
Manganese,
Mirrors,
Psychology"
Palmprint Identification Using an Ensemble of Sparse Representations,"Among various palmprint identification methods proposed in the literature, Sparse Representation for Classification (SRC) is very attractive offering high accuracy. Although SRC has good discriminative ability, its performance strongly depends on the quality of the training data. In particular, SRC suffers from two major problems: lack of training samples per class and large intra-class variations. In fact, palmprint images not only contain identity information but they also have other information such as illumination and geometrical distortions due to the unconstrained conditions and the movement of the hand. In this case, the sparse representation assumption may not hold well in the original space since samples from different classes may be considered from the same class. This work aims to enhance palmprint identification performance through SRC by proposing a simple yet efficient method based on an ensemble of sparse representations through an ensemble of discriminative dictionaries satisfying SRC assumption. The ensemble learning has the advantage to reduce the sensitivity due to the limited size of the training data and is performed based on random subspace sampling over 2D-PCA space while keeping the image inherent structure and information. In order to obtain discriminative dictionaries satisfying SRC assumption, a new space is learned by minimizing and maximizing the intra-class and inter-class variations using 2DLDA. Extensive experiments are conducted on two publicly available palmprint datasets: multispectral and PolyU. Obtained results showed very promising results compared to both state-of-the-art holistic and coding methods. Besides these findings, we provide an empirical analysis of the parameters involved in the proposed technique to guide the neophyte.","Training,
Dictionaries,
Training data,
Encoding,
Feature extraction,
Iris recognition"
TripImputor: Real-Time Imputing Taxi Trip Purpose Leveraging Multi-Sourced Urban Data,"Travel behavior understanding is a long-standing and critically important topic in the area of smart cities. Big volumes of various GPS-based travel data can be easily collected, among which the taxi GPS trajectory data is a typical example. However, in GPS trajectory data, there is usually little information on travelers' activities, thereby they can only support limited applications. Quite a few studies have been focused on enriching the semantic meaning for raw data, such as travel mode/purpose inferring. Unfortunately, trip purpose imputation receives relatively less attention and requires no real-time response. To narrow the gap, we propose a probabilistic two-phase framework named TripImputor, for making the real-time taxi trip purpose imputation and recommending services to passengers at their dropoff points. Specifically, in the first phase, we propose a two-stage clustering algorithm to identify candidate activity areas (CAAs) in the urban space. Then, we extract fine-granularity spatial and temporal patterns of human behaviors inside the CAAs from foursquare check-in data to approximate the priori probability for each activity, and compute the posterior probabilities (i.e., infer the trip purposes) using Bayes' theorem. In the second phase, we take a sophisticated procedure that clusters historical dropoff points and matches the dropoff clusters and CAAs to immerse the real-time response. Finally, we evaluate the effectiveness and efficiency of the proposed two-phase framework using real-world data sets, which consist of road network, check-in data generated by over 38,000 users in one year, and the large-scale taxi trip data generated by over 19,000 taxis in a month in Manhattan, the New York City, USA. Experimental results demonstrate that the system is able to infer the trip purpose accurately, and can provide recommendation results to passengers within 1.6 s in Manhattan on average, just using a single normal PC.","Public transportation,
Real-time systems,
Trajectory,
Global Positioning System,
Semantics,
Data mining,
Urban areas"
Entity-based language model smoothing approach for smart search,"Smart search plays an important role in all walks of life, for example, according to business needs, accurate search of required knowledge from massive resources is an important way to enhance industrial intelligence. Smoothing of the language model is essential for obtaining high-quality search results because it helps to reduce mismatching and overfitting problems caused by data sparseness. Traditional smoothing methods lexically focus on the global corpus and locally cluster documents information without semantic analysis, which leads to deficiency of the semantic correlations between query statements and documents. In this paper, we propose an entity-based language model smoothing approach for smart search that uses semantic correlation and takes entities as bridges to build the entity semantic language model using a knowledge base. In this approach, entities in the documents are linked to an external knowledge base, such as Wikipedia. Then, the entity semantic language model is generated by using soft-fused and hard-fused methods. A two-level merging strategy is also presented to smooth the language model according to whether a given word is semantically relevant to the document or not, which integrates the Dir-smoothing and JM-smoothing methods. Experimental results show that the smoothed language model more closely approximates the word probability distribution under the document semantic theme and more accurately estimates the relevance between query and document.",
A Topic Modeling Approach for Traditional Chinese Medicine Prescriptions,"In traditional Chinese medicine (TCM), prescriptions are the daughters of doctors' clinical experiences, which is the main way to cure diseases in China for several thousand years. In the long Chinese history, a large number of prescriptions have been invented based on TCM theories. Regularities in the prescriptions are important for both clinical practice and novel prescription development. Previous works used many methods to discover regularities in prescriptions, but rarely described how a prescription is generated using TCM theories. In this work, we propose a topic model which characterizes the generative process of prescriptions in TCM theories and further incorporate domain knowledge into the topic model. Using 33,765 prescriptions in TCM prescription books, the model can reflect the prescribing patterns in TCM. Our method can outperform several previous topic models and group recommendation methods on generalization performance, herbs recommendation, symptoms suggestion and prescribing patterns discovery.","Data models,
Diseases,
Adaptation models,
Data mining,
Analytical models,
Probabilistic logic"
Long-Term Multi-Resource Fairness for Pay-as-you Use Computing Systems,"Many current computing systems such as clouds and supercomputers charge users for their resource usages. A user's demand is often changing over time, indicating that it is difficult to keep the high resource utilization all the time for cost efficiency. Resource sharing is a classical and effective approach for high resource utilization. In view of the heterogeneous resource demands of users' workloads, multi-resource allocation fairness is a must for resource sharing in such pay-as-you-use computing systems. However, we find that, existing multi-resource fair policies such as Dominant Resource Fairness (DRF), implemented in currently popular resource management systems such as YARN [4] and Mesos [24], are not suitable for the pay-as-you-use computing systems. We show that this is because of their memoryless characteristic that can cause the following problems in the pay-as-you-use computing systems: 1). users can get resource benefits by cheating; 2). users might not be able to get the total amount of resources that they are entitled to in terms of their resource contributions. In this paper, we propose a new policy called H-MRF, which generalizes DRF and Asset Fairness with the long-term notion. We show that it can address these problems and is suitable for pay-as-you-use computing systems. We have implemented it into YARN by developing a prototype called MRYARN. Finally, we evaluate H-MRF using both testbed and simulated experiments. The experimental results show that there are about
1.1∼1.5
sharing benefit degrees and
1.2X∼1.8X
performance improvement for users with H-MRF, better than existing fair schedulers.",
Multiple 3-D Feature Fusion Framework for Hyperspectral Image Classification,"Due to the 3-D nature of hyperspectral images, as well as the spatial properties (such as regularity and continuity) of land covers, many 3-D feature extraction operators have been designed to fully exploit the joint spatial-spectral information. However, the large amount of obtained features can suffer from the ''curse of dimensionality'' problem, especially for the small training sample set. Moreover, various spatial-spectral features can represent the characteristics of the hyperspectral image from different aspects. In this paper, a multiple 3-D feature fusion framework (M3DF
3
) has been proposed for hyperspectral image classification. First, we extend the 2-D Gabor surface feature into 3-D (3DSF) domains to comply with the spatial-spectral structure of the hyperspectral image, which is directly applied on the original hyperspectral image instead of the Gabor features. Second, three 3-D feature extraction methods, including the 3-D morphological profile, the 3-D local binary pattern, and the proposed 3DSF, that, respectively, characterize the hyperspectral image from three different angles, i.e., morphology, local dependence, and shape smoothness, are fused under a multitask sparse representation framework to take full advantage of the multiple 3-D features together. The proposed M3DF
3
approach was fully tested on three real-world hyperspectral image data, i.e., the widely used Indian Pines, Pavia University, and Houston University. The results show that our method can achieve as high as 68.22%, 79.44%, and 72.84% accuracies, respectively, even when only few samples, i.e., three samples per class, are used for training.",
An Efficient Representation-Based Method for Boundary Point and Outlier Detection,"Detecting boundary points (including outliers) is often more interesting than detecting normal observations, since they represent valid, interesting, and potentially valuable patterns. Since data representation can uncover the intrinsic data structure, we present an efficient representation-based method for detecting such points, which are generally located around the margin of densely distributed data, such as a cluster. For each point, the negative components in its representation generally correspond to the boundary points among its affine combination of points. In the presented method, the reverse unreachability of a point is proposed to evaluate to what degree this observation is a boundary point. The reverse unreachability can be calculated by counting the number of zero and negative components in the representation. The reverse unreachability explicitly takes into account the global data structure and reveals the disconnectivity between a data point and other points. This paper reveals that the reverse unreachability of points with lower density has a higher score. Note that the score of reverse unreachability of an outlier is greater than that of a boundary point. The top-
m
ranked points can thus be identified as outliers. The greater the value of the reverse unreachability, the more likely the point is a boundary point. Compared with related methods, our method better reflects the characteristics of the data, and simultaneously detects outliers and boundary points regardless of their distribution and the dimensionality of the space. Experimental results obtained for a number of synthetic and real-world data sets demonstrate the effectiveness and efficiency of our method.","Data structures,
Computational modeling,
Liver,
Distributed databases,
Learning systems,
Data mining,
Time complexity"
A Survey of Workload Assessment Algorithms,"Supervisory control environments, such as the NASA control room can induce high workload levels in situations where a single error is capable of costing millions of dollars. An intelligent system can improve human supervisor performance by monitoring the human’s workload levels and intelligently adapting the system capabilities, such as adapting the interaction medium or reallocating roles and responsibilities between the human and the system. Systems capable of responding promptly and accurately to the human’s changes in workload require a workload assessment algorithm that can detect changes to all components of workload in real time. A review of 24 workload assessment algorithms across six task domains is provided. Each algorithm is reviewed based on four criteria: sensitivity, diagnosticity, suitability, and generalizability. The majority of the reviewed algorithms were developed for a specific task domain and are unable to generalize different tasks. Further, the majority of the algorithms do not account for individual differences, only assess one or two workload components, and do not classify underload.","Speech,
Visualization,
Sensitivity,
Electroencephalography,
NASA"
Integration of Anonymous Credential Systems in IoT constrained environments,"The pervasive nature of Internet of Things entails additional threats that compromise the security and privacy of IoT devices and, eventually, the users. This issue is aggravated in constrained IoT devices equipped with minimal hardware resources. Current security and privacy implementations need to be re-designed and implemented maintaining its Level of Assurance (LoA), aiming for this family of devices. To cope with this issue, this paper proposes the first novel attempt to leverage Anonymous Credential Systems (ACS) to preserve the privacy of autonomous IoT constrained devices. Concretely, we have designed a solution to integrate IBM’s Identity Mixer into constrained IoT ecosystems, endowing the IoT with ACS’s privacy-preserving capabilities. The solution has been designed, implemented and evaluated proving its feasibility.","Smart cards,
Privacy,
Java,
Mixers,
Cryptography,
Performance evaluation"
Upper and Lower Tight Error Bounds for Feature Omission with an Extension to Context Reduction,"In this work, fundamental analytic results in the form of error bounds are presented that quantify the effect of feature omission and selection for pattern classification in general, as well as the effect of context reduction in string classification, like automatic speech recognition, printed/handwritten character recognition, or statistical machine translation. A general simulation framework is introduced that supports discovery and proof of error bounds, which lead to the error bounds presented here. Initially derived tight lower and upper bounds for feature omission are generalized to feature selection, followed by another extension to context reduction of string class priors (aka language models) in string classification. For string classification, the quantitative effect of string class prior context reduction on symbol-level Bayes error is presented. The tightness of the original feature omission bounds seems lost in this case, as further simulations indicate. However, combining both feature omission and context reduction, the tightness of the bounds is retained. A central result of this work is the proof of the existence, and the amount of a statistical threshold w.r.t. the introduction of additional features in general pattern classification, or the increase of context in string classification beyond which a decrease in Bayes error is guaranteed.","Context modeling,
Analytical models,
Upper bound,
Feature extraction,
Measurement uncertainty,
Automatic speech recognition"
A Secure Device-to-Device Link Establishment Scheme for LoRaWAN,"In the Internet of Things (IoT) era, the importance of communication technology to support communication between various types of devices has become greater than ever before. LoRa and LoRaWAN are newly designed communication technologies for low-power long-distance communication in the IoT environment. Recently, LoRa and LoRaWAN have received much attention in academia and industry because low-power longdistance communication has not been supported by traditional communication technologies. Current LoRaWAN supports only communication between the end node and the central server. A recently introduced study enables device-to-device (D2D) communication in the LoRaWAN environment, which is not supported by the current LoRaWAN standard. This study shows that direct communication between devices can reduce battery consumption. However, security is not considered in this study and the security features of the LoRaWAN standard are also not applicable, which means that LoRaWAN D2D communication is exposed to a security threat. Therefore, we propose a secure link establishment scheme to protect the LoRaWAN D2D communication. With our scheme, both D2D nodes can securely share cryptographic keys for protecting the D2D communication. Through the security analysis, we prove that our scheme guarantees mutual authentication, confidentiality, and integrity. Performance evaluation shows that our scheme increases power consumption by approximately 5% compared with the basic scheme without security, but has little impact on overall battery lifetime. Consequently, the proposed scheme guarantees fundamental security requirements with sufficient feasibility.","Device-to-device communication,
Security,
Network servers,
Servers,
Standards,
Energy consumption,
Sensors"
Connections Between Nuclear-Norm and Frobenius-Norm-Based Representations,"A lot of works have shown that frobenius-norm-based representation (FNR) is competitive to sparse representation and nuclear-norm-based representation (NNR) in numerous tasks such as subspace clustering. Despite the success of FNR in experimental studies, less theoretical analysis is provided to understand its working mechanism. In this brief, we fill this gap by building the theoretical connections between FNR and NNR. More specially, we prove that: 1) when the dictionary can provide enough representative capacity, FNR is exactly NNR even though the data set contains the Gaussian noise, Laplacian noise, or sample-specified corruption and 2) otherwise, FNR and NNR are two solutions on the column space of the dictionary.","Dictionaries,
Artificial neural networks,
Gaussian noise,
Laplace equations,
Linear programming,
Learning systems,
Minimization"
O(N)-Space Spatiotemporal Filter for Reducing Noise in Neuromorphic Vision Sensors,"Neuromorphic vision sensors are an emerging technology inspired by how retina processing images. A neuromorphic vision sensor only reports when a pixel value changes rather than continuously outputting the value every frame as is done in an ""ordinary"" Active Pixel Sensor (ASP). This move from a continuously sampled system to an asynchronous event driven one effectively allows for much faster sampling rates; it also fundamentally changes the sensor interface. In particular, these sensors are highly sensitive to noise, as any additional event reduces the bandwidth, and thus effectively lowers the sampling rate. In this work we introduce a novel spatiotemporal filter with O(N) memory complexity for reducing background activity noise in neuromorphic vision sensors. Our design consumes
10×
less memory and has
100×
reduction in error compared to previous designs. Our filter is also capable of recovering real events and can pass up to 180% more real events.","Spatiotemporal phenomena,
Memory management,
Correlation,
Hardware,
Neuromorphics,
Complexity theory"
Large-scale Image Geo-Localization Using Dominant Sets,"This paper presents a new approach for the challenging problem of geo-locating an image using image matching in a structured database of city-wide reference images with known GPS coordinates. We cast the geo-localization as a clustering problem on local image features. Akin to existing approaches on the problem, our framework builds on low-level features which allow partial matching between images. For each local feature in the query image, we find its approximate nearest neighbors in the reference set. Next, we cluster the features from reference images using Dominant Set clustering, which affords several advantages over existing approaches. First, it permits variable number of nodes in the cluster which we use to dynamically select the number of nearest neighbors (typically coming from multiple reference images) for each query feature based on its discrimination value. Second, as we also quantify in our experiments, this approach is several orders of magnitude faster than existing approaches. Thus, we obtain multiple clusters (different local maximizers) and obtain a robust final solution to the problem using multiple weak solutions through constrained Dominant Set clustering on global image features, where we enforce the constraint that the query image must be included in the cluster. This second level of clustering also bypasses heuristic approaches to voting and selecting the reference image that matches to the query. We evaluate the proposed framework on an existing dataset of 102k street view images as well as a new larger dataset of 300k images, and show that it outperforms the state-of-the-art by 20% and 7%, respectively, on the two datasets.",
Velocity Estimation for Ultralightweight Tendon-Driven Series Elastic Robots,"Accurate velocity estimation is an important basis for robot control, but especially challenging for highly elastically driven robots. These robots show large swing or oscillation effects if they are not damped appropriately during the performed motion. In this letter, we consider an ultralightweight tendon-driven series elastic robot arm equipped with low-resolution joint position encoders. We propose an adaptive Kalman filter for velocity estimation that is suitable for these kinds of robots with a large range of possible velocities and oscillation frequencies. Based on an analysis of the parameter characteristics of the measurement noise variance, an update rule based on the filter position error is developed that is easy to adjust for use with different sensors. Evaluation of the filter both in simulation and in robot experiments shows a smooth and accurate performance, well suited for control purposes.","Estimation,
Kalman filters,
Robot sensing systems,
Noise measurement,
Optimization"
An Actively Detuned Wireless Power Receiver With Public Key Cryptographic Authentication and Dynamic Power Allocation,"This paper presents a CMOS resonant wireless charging receiver with an active detuning mechanism for controlling the received power, without using any passive components being switched in and out. This detuning mechanism is first combined with an on-chip elliptic curve accelerator that achieves
0.77−μJ
/elliptic curve scalar multiplication and in-band telemetry for authenticating a wireless charger using elliptic curve cryptography, with up to
16×
rejection at the output of the receiver. Second, equitable power distribution between two receivers coupled to the same charger is demonstrated by controlled detuning of the closer receiver. The system can overcome up to a 4:1 asymmetry in distance to the charger between two receivers. Implemented in 0.18-
μm
CMOS, the receiver IC delivers 520-mW peak output power and 74% peak end-to-end efficiency in the tuned mode.","Receivers,
Resonant frequency,
Wireless communication,
Communication system security,
Authentication,
Couplings,
Capacitors"
On the Performance of Spatially Correlated Large Antenna Arrays for Millimeter-Wave Frequencies,"A spatially correlated large antenna array operating at millimeter-wave (mmWave) frequencies is considered. Based on a Saleh–Valenzuela channel model, closed-form expressions of the 3-D spatial correlation (SC) for wide, narrow, and Von Mises power elevation spectra (PESs) are analytically derived. The effects of the PES on the convergence to massive multiple-input-multiple-output properties are then illustrated by defining and deriving a diagonal dominance metric. Numerically, the effects of antenna element mutual coupling (MC) are shown on the effective SC, eigenvalue structure, and mmWave user rate for different antenna topologies. It is concluded that although MC can significantly reduce SC for side-by-side dipole antenna elements, the change in antenna effective gain (and, therefore, signal-to-noise ratio) caused by MC becomes a dominating effect and ultimately determines the antenna array performance. The user rate of an mmWave system with hybrid beamforming, using an orthogonal matching pursuit (OMP) algorithm, is then shown for different antenna topologies with dipole and cross-polarized (x-pol) antenna elements. It is seen that even for small numbers of radio frequency chains, the OMP algorithm works well relative to the fully digital case for channels with high SC, such as the x-pol antenna array.","Antenna arrays,
Dipole antennas,
Channel models,
Topology,
MIMO,
Three-dimensional displays"
A Framework of Rapid Regional Tsunami Damage Recognition From Post-event TerraSAR-X Imagery Using Deep Neural Networks,"Near real-time building damage mapping is an indispensable prerequisite for governments to make decisions for disaster relief. With high-resolution synthetic aperture radar (SAR) systems, such as TerraSAR-X, the provision of such products in a fast and effective way becomes possible. In this letter, a deep learning-based framework for rapid regional tsunami damage recognition using post-event SAR imagery is proposed. To perform such a rapid damage mapping, a series of tile-based image split analysis is employed to generate the data set. Next, a selection algorithm with the SqueezeNet network is developed to swiftly distinguish between built-up (BU) and nonbuilt-up regions. Finally, a recognition algorithm with a modified wide residual network is developed to classify the BU regions into wash away, collapsed, and slightly damaged regions. Experiments performed on the TerraSAR-X data from the 2011 Tohoku earthquake and tsunami in Japan show a BU region extraction accuracy of 80.4% and a damage-level recognition accuracy of 74.8%, respectively. Our framework takes around 2 h to train on a new region, and only several minutes for prediction.",
Frequency Locator Polynomial for Wideband Sparse Spectrum Sensing With Multichannel Subsampling,"BigBand is a technique for wideband spectrum sensing (WSS) that can capture gigahertz of sparse spectrum in real time by using shifted multiple channels without sampling the signal at GS/s. In this paper, we focus on enforcing BigBand with the proposed improved algorithm, namely, AD-BigBand, for efficient and robust computation. AD-BigBand converts the nonlinear process (locating frequency and computing the spectrum value) of sparse WSS into two steps of solving linear equations. In particular, we define an r -degree frequency locator polynomial to rapidly locate nonzero frequencies. The computing complexity of the proposed method decreases from O(K{\log ^2}K ({{{n\log K} \over K}})^{\log K}) of shifted sampling based BigBand to O(K{\log ^2}K), where the K-sparse signals of bandwitdth n are subsampled with arithmetic time-shifted sensing channels. We implement AD-BigBand on a commercial four-channel acquisition card. Experimental results show that AD-BigBand improves the WSS capabilities and exhibits higher processing speed and lower error spectrum construction rate than BigBand, especially in the case of relative low signal-to-noise ratio.","Sensors,
Signal processing algorithms,
Wideband,
Real-time systems,
Robustness,
Hardware,
Approximation algorithms"
Mode-Target Games: Reactive Synthesis for Control Applications,"In this paper, we introduce a class of linear temporal logic (LTL) specifications for which the problem of synthesizing controllers can be solved in polynomial time. The new class of specifications is an LTL fragment that we term Mode Target (MT) and is inspired by numerous control applications where there are modes and corresponding (possibly multiple) targets for each mode. We formulate the problem of synthesizing a controller enforcing an MT specification as a game and provide an algorithm that requires
O(
∑
i
t
i
n
2
)
symbolic steps, where
n
is the number of states in the game graph, and
t
i
is the number of targets corresponding to mode
i
.","Games,
Automobiles,
Complexity theory,
ISO Standards,
Tin,
Indexes,
Electronic mail"
Multi-user Multi-Keyword Rank Search over Encrypted Data in Arbitrary Language,"Multi-keyword rank searchable encryption (MRSE) returns the top-
k
results in response to a data user's request of multi-keyword search over encrypted data, and hence provides an efficient way for preserving data privacy in cloud storage systems while without loss of data usability. Many existing MRSE systems are constructed based on an algorithm which we term as
k
-nearest neighbor for searchable encryption (KNN-SE). Unfortunately, KNN-SE has a number of shortcomings which limit its practical applications. In this paper, we propose a new MRSE system which overcomes almost all the defects of the KNN-SE based MRSE systems. Specifically, our new system does not require a predefined keyword set and supports keywords in arbitrary languages, is a multi-user system which supports flexible search authorization and time-controlled revocation, and it achieves better data privacy protection since even the cloud server is not able to tell which documents are the top-
k
results returned to a data user. We also conduct extensive experiments to demonstrate the efficiency of the new system.",
Fully Decomposable Compressive Sampling With Joint Optimization for Multidimensional Sparse Representation,"Conventional compressive sampling methods cannot efficiently exploit structured sparsity for sampling multidimensional signals like video sequences. In this paper, we propose a fully decomposable compressive sampling model that adopts the Kronecker product framework to exploit the structured sparsity spanning multidimensional signals. It enables efficient sampling in a progressive fashion by retaining the block-diagonal feature of Kronecker products. A synthetic sensing matrix is developed for joint optimization over sampling signals with multiple dimensions. Instead of adjusting global Gram matrix, separable minimization of mutual coherence in multiple dimensions is jointly formulated for a stable recovery with enhanced convergence rate. Sampling rate allocation is considered to improve recovery performance based on the decomposable compressive sampling. The proposed model is employed in video acquisition for temporal sparsity along motion trajectory. Experiment results show that the proposed model can improve the recovery performance with a reduced number of necessary samples in comparison to the state-of-the-art methods.","Sensors,
Matrix decomposition,
Optimization,
Coherence,
Wavelet transforms,
Compressed sensing,
Tensile stress"
Lattice-based Turn Model for Adaptive Routing,"This paper presents a model for designing partially adaptive logic-based distributed routing algorithms. Unlike the previous methods, the Lattice-based Turn Model adopts a turn prohibition scheme based on integer lattices, which identifies turn prohibitions with the points of different full-rank integer lattices. Due to the generality of the proposed model, existing approaches can be considered a subset of the solution space identified by this work. Morover, we propose three theorems that are the key for the design of lattice-based routing algorithms. In particular, the second theorem gives a necessary and sufficient condition to prove that a lattice-based routing algorithm is deadlock-free as long as the lattice basis meets certain requirements. Based on the proposed model, a novel routing algorithm, called lattice-based routing algorithm (LBRA), is presented. Simulation results exhibit encouraging performance improvements over state-of-the-art approaches considering real and synthetic benchmarks. For instance, average 71% and 18% latency reductions have been observed under transpose1 traffic compared to respectively Odd-Even and Repetitive Turn Model routing algorithms. Furthermore, LBRA achieves up to 38% and 17% performance improvement under real traffic as compared to Odd-Even and Repetitive Turn Model routing algorithms.",
Point-to-Set Distance Metric Learning on Deep Representations for Visual Tracking,"For autonomous driving application, a car shall be able to track objects in the scene in order to estimate where and how they will move such that the tracker embedded in the car can efficiently alert the car for effective collision-avoidance. Traditional discriminative object tracking methods usually train a binary classifier via a support vector machine (SVM) scheme to distinguish the target from its background. Despite demonstrated success, the performance of the SVM-based trackers is limited because the classification is carried out only depending on support vectors (SVs) but the target’s dynamic appearance may look similar to the training samples that have not been selected as SVs, especially when the training samples are not linearly classifiable. In such cases, the tracker may drift to the background and fail to track the target eventually. To address this problem, in this paper, we propose to integrate the point-to-set/ image-to-imageSet distance metric learning (DML) into visual tracking tasks and take full advantage of all the training samples when determining the best target candidate. The point-to-set DML is conducted on convolutional neural network features of the training data extracted from the starting frames. When a new frame comes, target candidates are first projected to the common subspace using the learned mapping functions, and then the candidate having the minimal distance to the target template sets is selected as the tracking result. Extensive experimental results show that even without model update the proposed method is able to achieve favorable performance on challenging image sequences compared with several state-of-the-art trackers.",
Is There a Robust Technique for Selecting Aspect Ratios in Line Charts?,"The aspect ratio of a line chart heavily influences the perception of the underlying data. Different methods explore different criteria in choosing aspect ratios, but so far, it was still unclear how to select aspect ratios appropriately for any given data. This paper provides a guideline for the user to choose aspect ratios for any input 1D curves by conducting an in-depth analysis of aspect ratio selection methods both theoretically and experimentally. By formulating several existing methods as line integrals, we explain their parameterization invariance. Moreover, we derive a new and improved aspect ratio selection method, namely the
L
1
-LOR (local orientation resolution), with a certain degree of parameterization invariance. Furthermore, we connect different methods, including AL (arc length based method), the banking to 45° principle, RV (resultant vector) and AS (average absolute slope), as well as
L
1
-LOR and AO (average absolute orientation). We verify these connections by a comparative evaluation involving various data sets, and show that the selections by RV and
L
1
-LOR are complementary to each other for most data. Accordingly, we propose the dual-scale banking technique that combines the strengths of RV and
L
1
-LOR, and demonstrate its practicability using multiple real-world data sets.","Banking,
Market research,
Robustness,
Guidelines,
Visual perception,
Indexes,
Data visualization"
Exploiting Target Data to Learn Deep Convolutional Networks for Scene-Adapted Human Detection,"The difference between sample distributions of public data sets and specific scenes can be very significant. As a result, the deployment of generic human detectors in real-world scenes most often leads to sub-optimal detection performance. To avoid the labor-intensive task of manual annotations, we propose a semi-supervised approach for training deep convolutional networks on partially labeled data. To exploit a large amount of unlabeled target data, the knowledge learnt from public data sets is transferred to new model training by adapting an auxiliary detector to the target scene. We hypothesize that the components of the auxiliary detector capture essential human characteristics useful for constructing a scene-adapted detector. A selective ensemble algorithm is proposed to select a subset of the components relevant to the target scene for recombination. The resulting model is applied for collecting high-confidence samples from unlabeled target data. Furthermore, a deep convolutional network is trained by progressively labeling and selecting new training samples in a self-paced way. The detailed experimental evaluation verifies the effectiveness and superiority of the proposed approach in scene-specific human detection.","Detectors,
Training,
Feature extraction,
Adaptation models,
Data models,
Labeling"
Semantic Neighbor Graph Hashing for Multimodal Retrieval,"Hashing methods have been widely used for approximate nearest neighbor search in recent years due to its computational and storage effectiveness. Most existing multimodal hashing methods try to preserve the similarity relationship based on either metric distances or semantic labels in a procrustean way, while ignoring the intra-class and inter-class variations inherent in the metric space. In this paper, we propose a novel multimodal hashing method, termed as semantic neighbor graph hashing (SNGH), which aims to preserve the fine-grained similarity metric based on the semantic graph that is constructed by jointly pursuing the semantic supervision and the local neighborhood structure. Specifically, the semantic graph is constructed to capture the local similarity structure for the image modality and the text modality, respectively. Furthermore, we define a function based on the local similarity in particular to adaptively calculate multi-level similarities by encoding the intra-class and inter-class variations. After obtaining the unified hash codes, the logistic regression with kernel trick is employed to learn view-specific hash functions independently for each modality. Extensive experiments are conducted on four widely used multimodal data sets. The experimental results demonstrate the superiority of the proposed SNGH method compared with the state-of-the-art multimodal hashing methods.",
A Fractional-Order Variational Framework for Retinex: Fractional-Order Partial Differential Equation-Based Formulation for Multi-Scale Nonlocal Contrast Enhancement with Texture Preserving,"This paper discusses a novel conceptual formulation of the fractional-order variational framework for retinex, which is a fractional-order partial differential equation (FPDE) formulation of retinex for the multi-scale nonlocal contrast enhancement with texture preserving. The well-known shortcomings of traditional integer-order computation-based contrast-enhancement algorithms, such as ringing artefacts and staircase effects, are still in great need of special research attention. Fractional calculus has potentially received prominence in applications in the domain of signal processing and image processing mainly because of its strengths like long-term memory, nonlocality, and weak singularity, and because of the ability of a fractional differential to enhance the complex textural details of an image in a nonlinear manner. Therefore, in an attempt to address the aforementioned problems associated with traditional integer-order computation-based contrast-enhancement algorithms, we have studied here, as an interesting theoretical problem, whether it will be possible to hybridize the capabilities of preserving the edges and the textural details of fractional calculus with texture image multi-scale nonlocal contrast enhancement. Motivated by this need, in this paper, we introduce a novel conceptual formulation of the fractional-order variational framework for retinex. First, we implement the FPDE by means of the fractional-order steepest descent method. Second, we discuss the implementation of the restrictive fractional-order optimization algorithm and the fractional-order Courant–Friedrichs–Lewy condition. Third, we perform experiments to analyze the capability of the FPDE to preserve edges and textural details, while enhancing the contrast. The capability of the FPDE to preserve edges and textural details is a fundamental important advantage, which makes our proposed algorithm superior to the traditional integer-order computation-based contrast enhancement algorithms, especially for images rich in textural details.","Fractional calculus,
Signal processing algorithms,
Histograms,
Image edge detection,
Medical services,
Image color analysis,
Partial differential equations"
Cost-Sensitive Feature Selection by Optimizing F-Measures,"Feature selection is beneficial for improving the performance of general machine learning tasks by extracting an informative subset from the high-dimensional features. Conventional feature selection methods usually ignore the class imbalance problem, thus the selected features will be biased towards the majority class. Considering that F-measure is a more reasonable performance measure than accuracy for imbalanced data, this paper presents an effective feature selection algorithm that explores the class imbalance issue by optimizing F-measures. Since F-measure optimization can be decomposed into a series of cost-sensitive classification problems, we investigate the cost-sensitive feature selection by generating and assigning different costs to each class with rigorous theory guidance. After solving a series of cost-sensitive feature selection problems, features corresponding to the best F-measure will be selected. In this way, the selected features will fully represent the properties of all classes. Experimental results on popular benchmarks and challenging real-world data sets demonstrate the significance of cost-sensitive feature selection for the imbalanced data setting and validate the effectiveness of the proposed method.",
Fast Online Tracking With Detection Refinement,"Most of the existing multiple object tracking (MOT) methods employ the tracking-by-detection framework. Among them, the min-cost network flow optimization techniques become the most popular and standard ones. In these methods, the graph structure models the MOT problem and finds the optimal flow in a connected graph of detections to encode the accurate track trajectories. However, the existing network flow is not suitable for directly online tracking, where the tracking results depend too much on the initial detections. To solve these problems, we present a fast online MOT algorithm by introducing the minimum output sum of squared error filter. The proposed method can adaptively refine the tracking targets according to the proposed rules of correcting the detection mistakes. Furthermore, we introduce an alternative targets hypotheses to reduce the dependence on detections and adaptively refine the object detection boxes. The experimental results on the MOT 2015 benchmark demonstrate that our method achieves comparable or even better results than previous approaches.","Target tracking,
Correlation,
Robustness,
Trajectory,
Visualization,
Object tracking"
Probabilistic Decision Based Block Partitioning for Future Video Coding,"In the latest Joint Video Exploration Team development, the quadtree plus binary tree (QTBT) block partitioning structure has been proposed for future video coding. Compared to the traditional quadtree structure of High Efficiency Video coding (HEVC) standard, QTBT provides more flexible patterns for splitting the blocks, which results in dramatically increased combinations of block partitions and high computational complexity. In view of this, a confidence interval based early termination (CIET) scheme is proposed for QTBT to identify the unnecessary partition modes in the sense of rate-distortion (RD) optimization. In particular, a RD model is established to predict the RD cost of each partition pattern without the full encoding process. Subsequently, the mode decision problem is casted into a probabilistic framework to select the final partition based on the confidence interval decision strategy. Experimental results show that the proposed CIET algorithm can speed up QTBT block partitioning structure by reducing 54.7% encoding time with only 1.12% increase in terms of bit rate. Moreover, the proposed scheme performs consistently well for the high resolution sequences, of which the video coding efficiency is crucial in real applications.","Binary trees,
Encoding,
Video coding,
Image coding,
Complexity theory,
Transforms,
Probabilistic logic"
Object-Part Attention Model for Fine-Grained Image Classification,"Fine-grained image classification is to recognize hundreds of subcategories belonging to the same basic-level category, such as 200 subcategories belonging to the bird, which is highly challenging due to large variance in the same subcategory and small variance among different subcategories. Existing methods generally first locate the objects or parts and then discriminate which subcategory the image belongs to. However, they mainly have two limitations: 1) relying on object or part annotations which are heavily labor consuming; and 2) ignoring the spatial relationships between the object and its parts as well as among these parts, both of which are significantly helpful for finding discriminative parts. Therefore, this paper proposes the object-part attention model (OPAM) for weakly supervised fine-grained image classification and the main novelties are: 1) object-part attention model integrates two level attentions: object-level attention localizes objects of images, and part-level attention selects discriminative parts of object. Both are jointly employed to learn multi-view and multi-scale features to enhance their mutual promotion; and 2) Object-part spatial constraint model combines two spatial constraints: object spatial constraint ensures selected parts highly representative and part spatial constraint eliminates redundancy and enhances discrimination of selected parts. Both are jointly employed to exploit the subtle and local differences for distinguishing the subcategories. Importantly, neither object nor part annotations are used in our proposed approach, which avoids the heavy labor consumption of labeling. Compared with more than ten state-of-the-art methods on four widely-used datasets, our OPAM approach achieves the best performance.","Feature extraction,
Birds,
Automobiles,
Noise measurement,
Visualization,
Redundancy,
Image classification"
Embedding Structured Contour and Location Prior in Siamesed Fully Convolutional Networks for Road Detection,"Road detection from the perspective of moving vehicles is a challenging issue in autonomous driving. Recently, many deep learning methods spring up for this task, because they can extract high-level local features to find road regions from raw RGB data, such as convolutional neural networks and fully convolutional networks (FCNs). However, how to detect the boundary of road accurately is still an intractable problem. In this paper, we propose siamesed FCNs (named “s-FCN-loc”), which is able to consider RGB-channel images, semantic contours, and location priors simultaneously to segment the road region elaborately. To be specific, the s-FCN-loc has two streams to process the original RGB images and contour maps, respectively. At the same time, the location prior is directly appended to the siamesed FCN to promote the final detection performance. Our contributions are threefold: 1) An s-FCN-loc is proposed that learns more discriminative features of road boundaries than the original FCN to detect more accurate road regions. 2) Location prior is viewed as a type of feature map and directly appended to the final feature map in s-FCN-loc to promote the detection performance effectively, which is easier than other traditional methods, namely, different priors for different inputs (image patches). 3) The convergent speed of training s-FCN-loc model is 30% faster than the original FCN because of the guidance of highly structured contours. The proposed approach is evaluated on the KITTI road detection benchmark and one-class road detection data set, and achieves a competitive result with the state of the arts.","Roads,
Feature extraction,
Semantics,
Image edge detection,
Image segmentation"
Auto-Weighted Multi-View Learning for Image Clustering and Semi-Supervised Classification,"Due to the efficiency of learning relationships and complex structures hidden in data, graph-oriented methods have been widely investigated and achieve promising performance. Generally, in the field of multi-view learning, these algorithms construct informative graph for each view, on which the following clustering or classification procedure are based. However, in many real-world data sets, original data always contain noises and outlying entries that result in unreliable and inaccurate graphs, which cannot be ameliorated in the previous methods. In this paper, we propose a novel multi-view learning model which performs clustering/semi-supervised classification and local structure learning simultaneously. The obtained optimal graph can be partitioned into specific clusters directly. Moreover, our model can allocate ideal weight for each view automatically without explicit weight definition and penalty parameters. An efficient algorithm is proposed to optimize this model. Extensive experimental results on different real-world data sets show that the proposed model outperforms other state-of-the-art multi-view algorithms.","Tensile stress,
Clustering algorithms,
Kernel,
Manifolds,
Correlation,
Laplace equations,
Clustering methods"
Differential Evolutionary Superpixel Segmentation,"Superpixel segmentation has been of increasing importance in many computer vision applications recently. To handle the problem, most state-of-the-art algorithms either adopt a local color variance model or a local optimization algorithm. This paper develops a new approach, named differential evolutionary superpixels, which is able to optimize the global properties of segmentation by means of a global optimizer. We design a comprehensive objective function aggregating within-superpixel error, boundary gradient, and a regularization term. Minimizing the within-superpixel error enforces the homogeneity of superpixels. In addition, the introduction of boundary gradient drives the superpixel boundaries to capture the natural image boundaries, so as to make each superpixel overlaps with a single object. The regularizer further encourages producing similarly sized superpixels that are friendly to human vision. The optimization is then accomplished by a powerful global optimizer—differential evolution. The algorithm constantly evolves the superpixels by mimicking the process of natural evolution, while using a linear complexity to the image size. Experimental results and comparisons with eleven state-of-the-art peer algorithms verify the promising performance of our algorithm.","Image segmentation,
Clustering algorithms,
Algorithm design and analysis,
Optimization,
Computational complexity,
Partitioning algorithms"
Exploiting Spatial-Temporal Locality of Tracking via Structured Dictionary Learning,"In this paper, a novel spatial-temporal locality is proposed and unified via a discriminative dictionary learning framework for visual tracking. By exploring the strong local correlations between temporally obtained target and their spatially distributed nearby background neighbors, a spatial-temporal locality is obtained. The locality is formulated as a subspace model and exploited under a unified structure of discriminative dictionary learning with a subspace structure. Using the learned dictionary, the target and its background can be described and distinguished effectively through their sparse codes. As a result, the target is localized by integrating both the descriptive and the discriminative qualities. Extensive experiments on various challenging video sequences demonstrate the superior performance of proposed algorithm over the other state-of-the-art approaches.","Target tracking,
Correlation,
Dictionaries,
Machine learning,
Visualization,
Video sequences,
Computer vision"
A Background Modeling and Foreground Detection Algorithm Using Scaling Coefficients Defined With a Color Model Called Lightness-Red-Green-Blue,"This paper presents an algorithm for background modeling and foreground detection that uses scaling coefficients, which are defined with a new color model called lightness-red-green-blue (LRGB). They are employed to compare two images by finding pixels with scaled lightness. Three backgrounds are used: 1) verified background with pixels that are considered as background; 2) testing background with pixels that are tested several times to check if they belong to the background; and 3) final background that is a combination of the testing and verified background (the testing background is used in places, where the verified background is not defined). If a testing background pixel matches pixels from previous frames (the match is tested using scaling coefficients), it is copied to the verified background, otherwise the pixel is set as the weighted average of the corresponding pixels of the last input images. After the background is computed, foreground objects are detected by using the scaling coefficients and additional criteria. The algorithm was evaluated using the SABS data set, Wallflower data set and a subset of the CDnet 2014 data set. The average F measure and sensitivity with the SABS Data set were 0.7109 and 0.8725, respectively. In the Wallflower data set, the total number of errors was 5280 and the total F-measure was 0.9089. In the CDnet 2014 data set, the F-measure for the baseline test case was 0.8887 and for the shadow test case was 0.8300.","Computational modeling,
Image color analysis,
Matrix decomposition,
Sparse matrices,
Principal component analysis,
Kernel,
Robustness"
Network Localization and Synchronization Using Full-Duplex Radios,"Both localization and synchronization of mobile nodes are important for wireless networks. In this paper, we propose new methods for network localization and synchronization (NLS) using full-duplex radios through only two frames of transmission. Specifically, all nodes simultaneously transmit their signature signals in the first frame, while receiving others’ signals via full-duplex radios. In the second frame, nodes transmit either scrambled versions of their received signals in the first frame or a digital packet of the channel parameter estimates of the received signals. We develop distributed algorithms to estimate the arrival times of different components in the received signals. These arrival times are then used to determine the local network geometry and clock offsets. The Cramér-Rao lower bounds for internode distances and clock offsets are derived, and the former can be translated into error bounds of the node positions. Compared with conventional frequency division duplex or time-division duplex, we demonstrate the high efficiency of NLS using full-duplex radios, revealing its potential beyond data communications in future wireless networks.","Synchronization,
Clocks,
Wireless networks,
Transmission line matrix methods,
Silicon"
A New Approach to Track Multiple Vehicles With the Combination of Robust Detection and Two Classifiers,"It plays an important role to accurately track multiple vehicles in intelligent transportation, especially in intelligent vehicles. Due to complicated traffic environments it is difficult to track multiple vehicles accurately and robustly, especially when there are occlusions among vehicles. To alleviate these problems, a new approach is proposed to track multiple vehicles with the combination of robust detection and two classifiers. An improved ViBe algorithm is proposed for robust and accurate detection of multiple vehicles. It uses the gray-scale spatial information to build dictionary of pixel life length to make ghost shadows and object’s residual shadows quickly blended into the samples of the background. The improved algorithm takes good post-processing method to restrain dynamic noise. In this paper, we also design a method using two classifiers to further attack the problem of failure to track vehicles with occlusions and interference. It classifies tracking rectangles with confidence values between two thresholds through combining local binary pattern with support vector machine (SVM) classifier and then using a convolutional neural network (CNN) classifier for the second time to remove the interference areas between vehicles and other moving objects. The two classifiers method has both time efficiency advantage of SVM and high accuracy advantage of CNN. Comparing with several existing methods, the qualitative and quantitative analysis of our experiment results showed that the proposed method not only effectively removed the ghost shadows, and improved the detection accuracy and real-time performance, but also was robust to deal with the occlusion of multiple vehicles in various traffic scenes.","Support vector machines,
Robustness,
Interference,
Object detection,
Optical imaging,
Feature extraction,
Adaptive optics"
Multiple Pedestrian Tracking From Monocular Videos in an Interacting Multiple Model Framework,"We present a multiple pedestrian tracking method for monocular videos captured by a fixed camera in an interacting multiple model (IMM) framework. Our tracking method involves multiple IMM trackers running in parallel, which are tied together by a robust data association component. We investigate two data association strategies which take into account both the target appearance and motion errors. We use a 4D color histogram as the appearance model for each pedestrian returned by a people detector that is based on the histogram of oriented gradients features. Short-term occlusion problems and false negative errors from the detector are dealt with using a sliding window of video frames, where tracking persists in the absence of observations. Our method has been evaluated, and compared both qualitatively and quantitatively with four state-of-the-art visual tracking methods using benchmark video databases. The experiments demonstrate that, on average, our tracking method outperforms these four methods.",
Robustness of Interdependent Power Grids and Communication Networks: A Complex Network Perspective,"In this brief, by considering realistic network operational settings, we propose a novel model to investigate the cascading failures in interdependent power grids and communication networks. We perform a critical node analysis on single networks to identify the vital nodes from the perspective of network robustness. Moreover, we assess the robustness of an interdependent system composed of an Internet AS-level network and the IEEE 118 Bus. Our simulation results show that assortative coupling of node destructiveness is more robust for densely coupled networks, whereas disassortative coupling of node robustness and node destructiveness performs better for sparsely coupled cases.","Robustness,
Power grids,
Communication networks,
Power system faults,
Power system protection,
Couplings,
Generators"
Decimal Full Adders Specially Designed for Quantum-Dot Cellular Automata,"New emerging technologies, with low area/ power/latency properties, are gaining momentum as replacements for CMOS. In particular, for quantum-dot cellular automata (QCA) realization, many arithmetic circuits have been redesigned. The basic QCA elements are a three-way majority gate and an inverter. The trivial mapping of logical circuits to their QCA equivalents via direct replacement of AND and OR gates with partially utilized majority (PUM) gates (i.e., with a “0” and “1” input, respectively) leads to exploitation of QCA basic components. Regarding the revitalized decimal arithmetic units in digital processors, researchers have begun to design decimal arithmetic circuits on QCA. For example, several QCA decimal full adders are trivially designed via the aforementioned direct mapping. However, only one proposition in the literature tries to make better use of majority gates, but yet replaces many AND and OR gates by PUMs. In this brief, we propose a QCA decimal full adder that is mostly composed of fully utilized majority gates (i.e., with no constant inputs), and rarely includes PUMs. The proposed circuit has been designed and tested by QCADesigner, and compared with relevant previous works, where the cell count, area, and delay, show 39%, 78%, 12% improvement, respectively.",
Compressive Sensing-Based Detection With Multimodal Dependent Data,"Detection with high-dimensional multimodal data is a challenging problem when there are complex inter- and intra- modal dependencies. While several approaches have been proposed for dependent data fusion (e.g., based on copula theory), their advantages come at a high price in terms of computational complexity. In this paper, we treat the detection problem with compressive sensing (CS) where compression at each sensor is achieved via low-dimensional random projections. CS has recently been exploited to solve detection problems under various assumptions on the signals of interest; however, its potential for dependent data fusion has not been explored adequately. We exploit the capability of CS to capture statistical properties of uncompressed data in order to compute decision statistics for detection in the compressed domain. First, a Gaussian approximation is employed to perform likelihood ratio (LR) based detection with compressed data. In this approach, intermodal dependence is captured via a compressed version of the covariance matrix of the concatenated (temporally and spatially) uncompressed data vector. We show that, under certain conditions, this approach with a small number of compressed measurements per node leads to enhanced performance compared to detection with uncompressed data using widely considered suboptimal approaches. Second, we develop a nonparametric approach where a decision statistic based on the second order statistics of uncompressed data is computed in the compressed domain. The second approach is promising over the first approach and the other related nonparametric approaches when multimodal data is highly correlated at the expense of slightly increased computational complexity.",
A Differential Quantizer-Based Error Feedback Modulator for Analog-to-Digital Converters,"A differential quantizer-based error feedback modulator intended for digitizing analog signals and its comparison to the traditional interpolative sigma delta analog-to-digital conversion is presented in this brief. The differential quantizer-based error feedback modulator also falls under the class of noise-shaping data converters. This newly introduced technique replaces the integrator with a differential quantizer to achieve noise-shaping characteristics. Thus, integrator associated non-idealities, loop-stability issues, and optimization of the integrator scaling coefficients is no more a concern. Differential quantizer-based error feedback modulator technique can perform well in high-precision and low-power applications. Behavioral-level simulation results demonstrate the mathematical equivalence of the differential quantizer based error feedback modulator technique with interpolative sigma delta modulator technique and confirms its novelty, theoretical stability, and scalability to higher order. The circuit level feasibility and effectiveness of the proposed architecture is verified in a 45-nm CMOS process using a 1-V supply with a power consumption of 0.22 and 0.5 mW for the first and second order modulators, respectively.","Modulation,
Signal to noise ratio,
Quantization (signal),
Sigma-delta modulation,
Noise shaping,
Gain,
Switches"
Composition of Feature Extraction Methods Shows Interesting Performances in Discriminating Wakefulness and NREM Sleep,"Intracranial electroencephalography (iEEG) is an invasive technique used to explore the cortical activity of the brain. In this letter, we focused on features of iEEG signals recorded during wakefulness and non-rapid eye movement (NREM) sleep in order to find differences between the two states, respectively. We preliminary screened the data using standard deviation analysis (STD). Then, we compared and combined STD values with coefficients from wavelet decomposition (Daubechies mother wavelet of order 4). Resulting parameters were classified using an artificial neural network. STD analysis underlined two brain areas [superior temporal sulcus (STS) and intraparietal-sulcus and parietal transverse (IPS)] with different electrical activity in the two states. STD values of STS and IPS channels were highly correlated in time; therefore, only STS was then used further in the features extraction analysis. Approximation and detail coefficients from Daubechies decomposition were used alone or in combination with the STD value. The overall accuracy of the pattern recognition was higher (98.57%), when features from different methods were used in combination. Our test was able to automatically recognize wake or NREM sleep status with very good discrimination performances using one single iEEG electrode.","Artificial neural networks (ANN),
biomedical signal processing,
brain modeling,
consciousness,
wavelet coefficients,
wavelet transforms (WT)"
Sampling and Reconstruction Using Bloom Filters,"In this paper, we address the problem of sampling from a set and reconstructing a set stored as a Bloom filter. To the best of our knowledge our work is the first to address this question. We introduce a novel hierarchical data structure called BloomSampleTree that helps us design efficient algorithms to extract an almost uniform sample from the set stored in a Bloom filter and also allows us to reconstruct the set efficiently. In the case where the hash functions used in the Bloom filter implementation are partially invertible, in the sense that it is easy to calculate the set of elements that map to a particular hash value, we propose a second, more space-efficient method called HashInvert for the reconstruction. We study the properties of these two methods both analytically as well as experimentally. We provide bounds on run times for both methods and sample quality for the BloomSampleTree based algorithm, and show through an extensive experimental evaluation that our methods are efficient and effective.","Arrays,
Indexes,
Twitter,
Electronic mail,
Dictionaries"
ADP-MAC: An Adaptive and Dynamic Polling-Based MAC Protocol for Wireless Sensor Networks,"Channel polling activity in MAC protocols of wireless sensor network (WSN) significantly governs energy, delay, and lifetime of the network, and therefore, it is required to adjust the polling intervals in accordance with the incoming traffic patterns. In this paper, an asynchronous duty-cycle-based MAC protocol: adaptive and dynamic polling-MAC (ADP-MAC) has been developed. This paper took a novel approach of switching the polling interval distribution of the receiver nodes by monitoring the co-efficient of variation of the incoming traffic. To represent different applications of WSN, constant-bit rate, Poisson, and Bursty arrivals have been used, whereas three types of polling distributions: deterministic, exponential, and dynamic have been studied. The performance parameters, such as energy, delay, and packet loss, are used to evaluate ADP-MAC against an established protocol synchronized channel polling-MAC (SCP-MAC). The major finding of this paper is that when the traffic arrival and polling interval distribution of ADP-MAC are in conformance, the performance in terms of both delay and energy turns out to be the best. Furthermore, ADP-MAC has been found to outperform SCP-MAC for each type of arrivals.","Wireless sensor networks,
Media Access Protocol,
Receivers,
Traffic control,
Energy consumption,
Synchronization"
A Clarke Transformation-Based DFT Phasor and Frequency Algorithm for Wide Frequency Range,"Despite its wide applications in power grid monitoring, the classic discrete Fourier transform (DFT)-based synchrophasor estimation algorithms suffer from significant errors when the power system operates under off-nominal frequency conditions. This phenomenon is caused by spectral leakage of DFT and becomes even more severe for single-phase synchrophasor estimation. To address this issue, a theory to eliminate the spectral leakage-caused errors is proposed and a Clarke transformation-based DFT synchrophasor estimation algorithm is proposed to implement the theory in this paper. The Clarke transformation constructs a second signal that has exactly 90° phase angle difference from the original single-phase input signal and helps eliminate the estimation errors for a wide frequency range. The proposed algorithm is tested under the conditions required in the phasor measurement unit standard C37.118.1-2011 and C37.118.1a-2014, as well as the harmonic and noise conditions not required in the standard to verify its performance. More importantly, the idea of using Clarke transformation can be used for other DFT-based synchrophasor algorithms in order to achieve higher synchrophasor measurement accuracy under dynamic conditions. An example is presented at last to demonstrate the expandability of the proposed idea.","Discrete Fourier transforms,
Heuristic algorithms,
Frequency estimation,
Phasor measurement units,
Finite impulse response filters,
Power grids"
On the Achievable Throughput of Energy-Harvesting Nanonetworks in the Terahertz Band,"In this paper, the maximum achievable throughput of electromagnetic nanonetworks in the terahertz (THz) band (0.1–10 THz) is comprehensively investigated. On the one hand, the peculiarities of the THz-band channel are taken into account by capturing the impact of the molecular absorption loss on the signal propagation. On the other hand, a two-state medium access control protocol is utilized to reflect the behavior of energy-harvesting nano-devices with constrained harvesting rate and maximum transmission power
P
0
. An ad-hoc nanonetwork is considered with
n
identical randomly located nano-devices, and each is capable of utilizing
W
Hz of bandwidth. When the node density of nanonetworks is low, the achievable throughput is
O(W
P
0
((nlogn
)
(
α
spr
−1/2)
/exp((
α
abs
/(nlogn
)
1/2
)))
)
(1/2)
, where
α
spr
and
α
abs
refer to the spreading loss coefficient and the molecular absorption loss coefficient. When the node density of nanonetworks is very high, the interference among nano-devices governs the network behavior and the achievable throughput becomes
O((
W
2
P
0
/I(n))((nlogn
)
(
α
spr
−1/2)
/ exp((
α
abs
/(nlogn
)
1/2
)))
)
(1/2)
. For both the cases, the upper boundaries of the achievable throughput are analytically derived, and the numerical results are provided. Numerical results illustrate that the molecular absorption loss plays the main role when the nanonetwork is sparse, and the interference dominates when the nanonetwork node density is very high.","Absorption,
Throughput,
Propagation losses,
Nanoscale devices,
Bandwidth,
Energy harvesting,
Interference"
Novel Linearized Power Flow and Linearized OPF Models for Active Distribution Networks With Application in Distribution LMP,"The locational marginal price (LMP) methodology has been discussed for distribution networks/systems under the smart grid initiative. In this paper, a new distribution LMP (DLMP) formulation is presented which includes reactive power prices and voltage constraints. To solve DLMP, three modeling tools, namely, linearized power flow for distribution (LPF-D), loss factors for distribution (LF-D), and linear optimal power flow for distribution (LOPF-D) are proposed. LPF-D solves not only voltage angles but also magnitudes through linear expression between bus injections and bus voltages, specifically for distribution systems. LF-D is solved recursively based on the radial topology of typical distribution systems. With the integration of LPF-D and LF-D, conventional optimal power flow (OPF) can be reformulated as LOPF-D which is essentially a linear programming model. Test results on various systems show that: 1) LPF-D efficiently yields very close results if compared with AC power flow; 2) LOPF-D provides very close dispatch results in both real and reactive power if compared with ACOPF; and 3) the proposed DLMPs calculated with LF-D and LOPF-D give accurate price information if compared with the prices from ACOPF. Further, these three tools are not limited to DLMP but can be potentially applied to other distribution analyses.","Load flow,
Reactive power,
Pricing,
Linear programming,
Mathematical model,
Niobium,
Generators"
DAGAN: Deep De-Aliasing Generative Adversarial Networks for Fast Compressed Sensing MRI Reconstruction,"Compressed Sensing Magnetic Resonance Imaging (CS-MRI) enables fast acquisition, which is highly desirable for numerous clinical applications. This can not only reduce the scanning cost and ease patient burden, but also potentially reduce motion artefacts and the effect of contrast washout, thus yielding better image quality. Different from parallel imaging based fast MRI, which utilises multiple coils to simultaneously receive MR signals, CS-MRI breaks the Nyquist-Shannon sampling barrier to reconstruct MRI images with much less required raw data. This paper provides a deep learning based strategy for reconstruction of CS-MRI, and bridges a substantial gap between conventional non-learning methods working only on data from a single image, and prior knowledge from large training datasets. In particular, a novel conditional Generative Adversarial Networks-based model (DAGAN) is proposed to reconstruct CS-MRI. In our DAGAN architecture, we have designed a refinement learning method to stabilise our U-Net based generator, which provides an endto-end network to reduce aliasing artefacts. To better preserve texture and edges in the reconstruction, we have coupled the adversarial loss with an innovative content loss. In addition, we incorporate frequency domain information to enforce similarity in both the image and frequency domains. We have performed comprehensive comparison studies with both conventional CSMRI reconstruction methods and newly investigated deep learning approaches. Compared to these methods, our DAGAN method provides superior reconstruction with preserved perceptual image details. Furthermore, each image is reconstructed in about 5 ms, which is suitable for real-time processing.",
Online Data Integrity Attacks Against Real-Time Electrical Market in Smart Grid,"The real-time electrical market operations in smart grid require reliable and accurate data from state estimation. However, state estimation is vulnerable to data integrity attacks, in which strategically manipulated meter measurements can bypass the conventional bad data detection and introduce errors. As a result, it becomes more likely for the attackers to control real-time electrical market through manipulations of meter measurements. In this paper, we first reveal the intrinsic relations between data integrity attacks and real-time electrical market operations, and explicitly characterize their complex interactions as a process simulator. Then a simulation-based global optimization problem is formulated from which attackers could maximize financial incentives through constructed data integrity attacks. More importantly, a novel systematic online attack construction strategy is proposed, such that attackers can launch the desired attacks only by the real-time data streams of meter measurements and no power network topology or parameter information is needed. A corresponding online defense strategy is also presented to detect and identify the malicious measurements without extra meter hardware investments. Finally, we evaluate the performance of the proposed attacking strategies and countermeasure through numerical simulations in IEEE test systems with both synthetic and real data from the New York Independent System Operator.","Real-time systems,
State estimation,
Power system stability,
Power measurement,
Network topology,
Power generation,
Smart grids"
Microgrids for Service Restoration to Critical Load in a Resilient Distribution System,"Microgrids can act as emergency sources to serve critical loads when utility power is unavailable. This paper proposes a resiliency-based methodology that uses microgrids to restore critical loads on distribution feeders after a major disaster. Due to limited capacity of distributed generators (DGs) within microgrids, dynamic performance of the DGs during the restoration process becomes essential. In this paper, the stability of microgrids, limits on frequency deviation, and limits on transient voltage and current of DGs are incorporated as constraints of the critical load restoration problem. The limits on the amount of generation resources within microgrids are also considered. By introducing the concepts of restoration tree and load group, restoration of critical loads is transformed into a maximum coverage problem, which is a linear integer program (LIP). The restoration paths and actions are determined for critical loads by solving the LIP. A 4-feeder, 1069-bus unbalanced test system with four microgrids is utilized to demonstrate the effectiveness of the proposed method. The method is applied to the distribution system in Pullman, WA, resulting in a strategy that uses generators on the Washington State University campus to restore service to the Hospital and City Hall in Pullman.","Microgrids,
Generators,
Switches,
Transient analysis,
Power system stability,
Electronic mail,
Reactive power"
"Interfacing Power System and ICT Simulators: Challenges, State-of-the-Art, and Case Studies","With the transition toward a smart grid, the power system has become strongly intertwined with the information and communication technology (ICT) infrastructure. The interdependency of both domains requires a combined analysis of physical and ICT processes, but simulating these together is a major challenge due to the fundamentally different modeling and simulation concepts. After outlining these challenges, such as time synchronization and event handling, this paper presents an overview of state-of-the-art solutions to interface power system and ICT simulators. Due to their prominence in recent research, a special focus is set on co-simulation approaches and their challenges and potentials. Further, two case studies analyzing the impact of ICT on applications in power system operation illustrate the necessity of a holistic approach and show the capabilities of state-of-the-art co-simulation platforms.","Mathematical model,
Computational modeling,
Communication networks,
Numerical models,
Power system dynamics,
Software"
New Verification Strategy for Finger-Vein Recognition System,"This paper proposes a new finger-vein recognition system that uses a binary robust invariant elementary feature from accelerated segment test feature points and an adaptive thresholding strategy. Subsequently, the proposed a multi-image quality assessments (MQA) are applied to conduct a second stage verification. As oppose to other studies, the region of interest is directly identified using a range of normalized feature point area, which reduces the complexity of pre-processing. This recognition structure allows an efficient feature points matching using a robust feature and rigorous verification using the MQA process. As a result, this method not only reduces the system computation time, comparisons against former relevant studies demonstrate the superiority of the proposed method.","Feature extraction,
Veins,
Biometrics (access control),
Robustness,
Sensors,
Complexity theory,
Fingers"
A Reliability Assessment Approach for Integrated Transportation and Electrical Power Systems Incorporating Electric Vehicles,"With the increasing utilization of electric vehicles (EVs), transportation systems and electrical power systems are becoming increasingly coupled. However, the interaction between these two kinds of systems are not well captured, especially from the perspective of transportation systems. This paper studies the reliability of integrated transportation and electrical power system (ITES). A bidirectional EV charging control strategy is first demonstrated to model the interaction between the two systems. Thereafter, a simplified transportation system model is developed, whose high efficiency makes the reliability assessment of the ITES realizable with an acceptable accuracy. Novel transportation system reliability indices are then defined from the view point of EV’s driver. Based on the charging control model and the transportation simulation method, a daily periodic quasi sequential reliability assessment method is proposed for the ITES system. Case studies based on RBTS system demonstrate that bidirectional charging controls of EVs will benefit the reliability of power systems, while decreasing the reliability of EVs travelling. Also, the optimal control strategy can be obtained based on the proposed method. Finally, case studies are performed based on a large scale test system to verify the practicability of the proposed method.",
EAPC: Energy-Aware Path Construction for Data Collection Using Mobile Sink in Wireless Sensor Networks,"Data collection is one of the paramount concerns in wireless sensor networks. Many data collection algorithms have been proposed for collecting data in particular monitoring regions. However, the efficiency of the paths for such data collection can be improved. This paper proposes an energy-aware path construction (EAPC) algorithm, which selects an appropriate set of data collection points, constructs a data collection path, and collects data from the points burdened with data. EAPC is intended to prolong the network lifetime, it accounts for the path cost from its current data collection point to the next point and the forwarding load of each sensor node. Performance evaluation reveals that the proposed EAPC has more efficient performance than existing data collection mechanisms in terms of network lifetime, energy consumption, fairness index, and efficiency index.",
Broadband Photonic Microwave Signal Processor With Frequency Up/Down Conversion and Phase Shifting Capability,"This paper presents a new structure that can realize both frequency up/down conversion and phase shifting operation. It solves the problems presented in the previously reported structures that are unable to realize frequency up conversion, require critical control on the laser source wavelength, have low isolation between the input and output port, and are limited to sub octave operation. The new structure also capable of realizing multiple phase shifts for use in a phased array antenna system.",
Direct Estimation of Density Functionals Using a Polynomial Basis,"A number of fundamental quantities in statistical signal processing and information theory can be expressed as integral functions of two probability density functions. Such quantities are called density functionals as they map density functions onto the real line. For example, information divergence functions measure the dissimilarity between two probability density functions and are useful in a number of applications. Typically, estimating these quantities requires complete knowledge of the underlying distribution followed by multidimensional integration. Existing methods make parametric assumptions about the data distribution or use nonparametric density estimation followed by high-dimensional integration. In this paper, we propose a new alternative. We introduce the concept of “data-driven basis functions”—functions of distributions whose value we can estimate given only samples from the underlying distributions without requiring distribution fitting or direct integration. We derive a new data-driven complete basis that is similar to the deterministic Bernstein polynomial basis and develop two methods for performing basis expansions of functionals of two distributions. We also show that the new basis set allows us to approximate functions of distributions as closely as desired. Finally, we evaluate the methodology by developing data-driven estimators for the Kullback–Leibler divergences and the Hellinger distance and by constructing empirical estimates of tight bounds on the Bayes error rate.","Estimation,
Error analysis,
Convergence,
Signal processing,
Histograms,
Information theory,
Density functional theory"
A Computationally Inexpensive Energy Model for Horizontal Electric Water Heaters With Scheduling,"Electric water heaters (EWHs) remain one of the main contributors to energy consumption in countries where they are used. EWH models serve as a step toward achieving optimized control, and can also be used to inform users of expected savings due to changes if the model is energy-based. Various models have been proposed, but none of them include more than half of the six key features that the model presented in this paper supports horizontal orientation, schedule control, low computational complexity, validation of the model, multinodal stratification, and multinodal standing losses. The presented model is validated against six datasets, four comprising 900 h with multiple water usage events\ and two with only standing losses. The results show that the model estimates energy consumption over ten days including usage with an error of less than 2% and 5% for schedule control and thermostat control, respectively. The simulation model is simple enough to execute ten days of simulation in less than 100 ms on a standard desktop machine, 150 times faster than a prominent model from literature, making it also suitable for large scale simulations or for use on mobile devices.","Computational modeling,
Water heating,
Temperature measurement,
Mathematical model,
Analytical models,
Load modeling,
Schedules"
qSwitch: Dynamical Off-Chip Bandwidth Allocation Between Local and Remote Accesses,"Multisocket computer systems are popular in workstations and servers. However, they suffer from the relatively low bandwidth of intersocket communication especially for massive parallel workloads that generate many intersocket requests for synchronizations and remote memory accesses. Intersocket traffic puts pressure on the underlying network connecting all processors with a limited bandwidth confined by pin resources. Given this constraint, we propose to dynamically increase the intersocket bandwidth by sacrificing off-chip memory bandwidth when systems have heavy intersocket communication but few off-chip memory accesses. Our design increases the physical bandwidth for intersocket communication via switching the function of pins from off-chip memory accesses to intersocket communication and can achieve an average performance speedup of 1.28 in geocentric mean for selected parallel multithreaded benchmarks.",
Exploiting Parallelism for Access Conflict Minimization in Flash-Based Solid State Drives,"Solid state drives (SSDs) have been widely deployed in personal computers, data centers, and cloud storages. In order to improve performance, SSDs are usually constructed with a number of channels with each channel connecting to a number of nand flash chips, each flash chip consisting of multiple dies and each die containing multiple planes. Based on this parallel architecture, I/O requests are potentially able to access parallel units simultaneously. Despite the rich parallelism offered by the parallel architecture, recent studies show that the utilization of flash parallel units is seriously low. This paper shows that the low parallel unit utilization is highly caused by the access conflict among I/O requests. In this paper, we propose parallel issue queueing (PIQ), a novel I/O scheduler at the host systems. PIQ groups I/O requests without conflicts into the same batch and I/O requests with conflicts into different batches. Hence, the multiple I/O requests in one batch can be fulfilled simultaneously by exploiting the rich parallelism of SSDs. Extensive experimental results show that PIQ delivers significant performance improvement especially for the applications which have heavy access conflicts.",
Non-Invasive Identification of Inertia Distribution Change in High Renewable Systems Using Distribution Level PMU,"This letter proposed an approach to identify the change of inertia distribution in high renewable power systems. Using the footprints of electromechanical wave propagation at the distribution level, this approach provides a new and non-invasive way to aware the system inertia distribution for primary frequency response. Actual measurements and high renewable dynamic models validated effectiveness of the approach.","Phasor measurement units,
Propagation,
Geophysical measurements,
Time-frequency analysis,
Interpolation,
Power systems"
Automatic Subretinal Fluid Segmentation of Retinal SD-OCT Images With Neurosensory Retinal Detachment Guided by Enface Fundus Imaging,"Objective: Accurate segmentation of neurosensory retinal detachment (NRD) associated subretinal fluid in spectral domain optical coherence tomography (SD-OCT) is vital for the assessment of central serous chorioretinopathy (CSC). A novel two-stage segmentation algorithm was proposed, guided by Enface fundus imaging. Methods: In the first stage, Enface fundus image was segmented using thickness map prior to detecting the fluid-associated abnormalities with diffuse boundaries. In the second stage, the locations of the abnormalities were used to restrict the spatial extent of the fluid region, and a fuzzy level set method with a spatial smoothness constraint was applied to subretinal fluid segmentation in the SD-OCT scans. Results: Experimental results from 31 retinal SD-OCT volumes with CSC demonstrate that our method can achieve a true positive volume fraction (TPVF), false positive volume fraction (FPVF), and positive predicative value (PPV) of 94.3%, 0.97%, and 93.6%, respectively, for NRD regions. Our approach can also discriminate NRD-associated subretinal fluid from subretinal pigment epithelium fluid associated with pigment epithelial detachment with a TPVF, FPVF, and PPV of 93.8%, 0.40%, and 90.5%, respectively. Conclusion: We report a fully automatic method for the segmentation of subretinal fluid. Significance: Our method shows the potential to improve clinical therapy for CSC.",
3-D Floating-Gate Synapse Array With Spike-Time-Dependent Plasticity,"This paper proposes a 3-D floating-gate (FG) synapse array for neuromorphic applications. The designed device has certain advantages over previous planar FG synapse devices: a smaller cell size due to the stacked structure and smaller operation voltage by the gate-all-around geometry. In addition, the operation method to implement spike time-dependent plasticity is proposed and demonstrated. The proposed array based on commercialized flash memory technology is expected be one of the most promising candidate architecture for neuromorphic applications.",
Enabling Security-Enhanced Attestation With Intel SGX for Remote Terminal and IoT,"Along with the advent and popularity of cloud computing, Internet of Things, and bring your own device, the trust requirement for terminal devices has increased significantly. An untrusted terminal, a terminal that runs in an untrustworthy execution environment, may cause serious security issues for enterprise networks. With the release of Software Guard Extension, Intel has provided a promising way to construct trusted terminals and services. Utilizing this technology, we propose a security-enhanced attestation for remote terminals, which can achieve shielded execution for measurements and attestation programs. Furthermore, we present a policy-based measurement mechanism where sensitive data, including secret keys and policy details are concealed using the enclave-specific keys. We implement our attestation prototype on real platform with Intel Skylake processor. Evaluation results show that our attestation system can provide much stronger security guarantees, yet incurs small performance overhead.",
Collision-free scheduling of multi-bridge machining systems: a colored traveling salesman problem-based approach,"Multi-bridge machining systems U+0028 MBMS U+0029 have gained wide applications in industry due to their high production capacity and efficiency. They contain multiple bridge machines working in parallel within their partially overlapping workspaces. Their scheduling problems can be abstracted into a serial-colored travelling salesman problem in which each salesman has some exclusive cities and some cities shared with its neighbor U+0028 s U+0029. To solve it, we develop a greedy algorithm that selects a neighboring city satisfying proximity. The algorithm allows a salesman to select randomly its shared cities and runs accordingly many times. It can thus be used to solve job scheduling problems for MBMS. Subsequently, a collision-free scheduling method is proposed to address both job scheduling and collision resolution issues of MBMS. It is an extension of the greedy algorithm by introducing time window constraints and a collision resolution mechanism. Thus, the augmented greedy algorithm can try its best to select stepwise a job for an individual machine such that no time overlaps exist between it and the job sequence of the neighboring machine dealt in the corresponding overlapping workspace; and remove such a time overlap only when it is inevitable. Finally, we conduct a case study of a large triplebridge waterjet cutting system by applying the proposed method.","Urban areas,
Job shop scheduling,
Greedy algorithms,
Machining,
Traveling salesman problems,
Collision avoidance"
Parallel dispatch: a new paradigm of electrical power system dispatch,"Modern power systems are evolving into sociotechnical systems with massive complexity, whose real-time operation and dispatch go beyond human capability. Thus, the need for developing and applying new intelligent power system dispatch tools are of great practical significance. In this paper, we introduce the overall business model of power system dispatch, the top level design approach of an intelligent dispatch system, and the parallel intelligent technology with its dispatch applications. We expect that a new dispatch paradigm, namely the parallel dispatch, can be established by incorporating various intelligent technologies, especially the parallel intelligent technology, to enable secure operation of complex power grids, extend system operators U+02BC capabilities, suggest optimal dispatch strategies, and to provide decision-making recommendations according to power system operational goals.",
EEG-Based Affect and Workload Recognition in a Virtual Driving Environment for ASD Intervention,"Objective: To build group-level classification models capable of recognizing affective states and mental workload of individuals with autism spectrum disorder (ASD) during driving skill training. Methods: Twenty adolescents with ASD participated in a six-session virtual reality driving simulator-based experiment, during which their electroencephalogram (EEG) data were recorded alongside driving events and a therapist's rating of their affective states and mental workload. Five feature generation approaches including statistical features, fractal dimension features, higher order crossings (HOC)-based features, power features from frequency bands, and power features from bins (
Δf=2Hz
) were applied to extract relevant features. Individual differences were removed with a two-step feature calibration method. Finally, binary classification results based on the k-nearest neighbors algorithm and univariate feature selection method were evaluated by leave-one-subject-out nested cross-validation to compare feature types and identify discriminative features. Results: The best classification results were achieved using power features from bins for engagement (0.95) and boredom (0.78), and HOC-based features for enjoyment (0.90), frustration (0.88), and workload (0.86). Conclusion: Offline EEG-based group-level classification models are feasible for recognizing binary low and high intensity of affect and workload of individuals with ASD in the context of driving. However, while promising the applicability of the models in an online adaptive driving task requires further development. Significance: The developed models provide a basis for an EEG-based passive brain computer interface system that has the potential to benefit individuals with ASD with an affect- and workload-based individualized driving skill training intervention.",
A Learning Scheme for Microgrid Reconnection,"This paper introduces a potential learning scheme that can dynamically predict the stability of the reconnection of subnetworks to a main grid. As the future electrical power systems tend toward smarter and greener technology, the deployment of self sufficient networks, or microgrids, becomes more likely. Microgrids may operate on their own or synchronized with the main grid, thus control methods need to take into account islanding and reconnecting of said networks. The ability to optimally and safely reconnect a portion of the grid is not well understood and, as of now, limited to raw synchronization between interconnection points. A support vector machine (SVM) leveraging real-time data from phasor measurement units is proposed to predict in real time whether the reconnection of a subnetwork to the main grid would lead to stability or instability. A dynamics simulator fed with preacquired system parameters is used to create training data for the SVM in various operating states. The classifier was tested on a variety of cases and operating points to ensure diversity. Accuracies of approximately 85% were observed throughout most conditions when making dynamic predictions of a given network.",
"On Inertia Distribution, Inter-Area Oscillations and Location of Electronically-Interfaced Resources","This paper explores the relationships between inertia distribution, inter-area oscillations, and location of electronically-interfaced resources that are enabled with either damping or inertia emulation controllers (EIRs). A two-machine system with an EIR is used for analytical derivations. Explicit analytical expressions are found for: (a) the location of the center of inertia (COI), which depends on the H-inertia constant and voltage set-points of the machines, and (b) the residue of the system transfer function, which is convex in terms of the EIR location. These expressions are validated using full-order models for machines, exciters, and governors; the results support the idea of placing EIRs further away from the COI, or equivalently, in areas with low inertia to attain the highest possible oscillation damping. In the case of large-scale systems, an inertia distribution index is proposed which allows estimating the distance from any bus to the COI location. The efficacy of the proposed index is tested in a real system. The system areas with less inertia are proved to be the best places to deploy EIRs. The proposed index does not require computationally expensive calculations as those from modal analysis and it seems promising to better reinforce power system dynamics through EIRs.",
On Random Dynamic Voltage Scaling for Internet-of-Things: A Game-Theoretic Approach,"Security is one of the top considerations in hardware designs for Internet-of-Things (IoT), where embedded cryptosystems are extensively used. Traditionally, random dynamic voltage scaling technology has been shown to be very effective in improving the resistance of cryptosystems against side-channel attacks. However, in this paper we demonstrate that the resistance can be undermined by providing lower off-chip power supply voltage. In order to address this issue, we then further propose to monitor the off-chip power supply voltage, and trigger an alarm to protect valued information once the power supply voltage is lower than the expected voltage (threshold voltage). However, considering both maintenance cost of IoT devices and the environment noise on power supply voltage, we first formulated this problem as a nonzero sum game model, and the attacker and the circuit supplier (defender) are the players of this game. The analysis of the Nash equilibria in this game show interesting guideline to the defender about the choice of threshold voltage, which is based on parameters of cryptosystem including the value of information, denial-of-service cost in IoT, etc.",
"A New Heuristic for
N
-Dimensional Nearest Neighbor Realization of a Quantum Circuit","One of the main challenges in quantum computing is to ensure error-free operation of the basic quantum gates. There are various implementation technologies of quantum gates for which the distance between interacting qubits must be kept within a limit for reliable operation. This leads to the so-called requirement of neighborhood arrangements of the interacting qubits, often referred to as nearest neighbor (NN) constraint. This is typically achieved by inserting SWAP gates in the quantum circuits, where a SWAP gate between two qubits exchanges their states. Minimizing the number of SWAP gates to provide NN compliance is an important problem to solve. A number of approaches have been proposed in this regard, based on local and global ordering techniques. In this paper, a generalized approach for combined local and global ordering of qubits have been proposed that is based on an improved heuristic for cost estimation and is also scalable. The approach can be extended to
N
-dimensional arrangement of qubits, for any arbitrary values of
N
. Practical constraints, however, restrict the maximum value of
N
to 3. Extensive experiments on benchmark functions have been carried out to evaluate the performance in terms of SWAP gate requirements. 3-D organization of qubits shows average reductions of 6.7% and 37.4%, respectively, in the number of SWAP gates over 2-D and 1-D organizations. Also compared to the best 2-D and 1-D results reported in the literature, on the average 8.7% and 8.4% reductions, respectively, are observed.","Logic gates,
Quantum computing,
Measurement,
Artificial neural networks,
Quantum entanglement,
Libraries,
Benchmark testing"
Electronic Conduction Mechanisms in Insulators,"The current density–electric field
(J−ξ)
characteristics of four insulators of dramatically different electrical qualities are assessed in terms of their operative electronic conduction mechanisms. Conduction in the two high-quality insulators is dominated by Ohmic conduction and Fowler–Nordheim tunneling, whereas conduction in the two low-quality insulators involves Ohmic conduction and space-charge limited current (SCLC). Ohmic conduction and SCLC are somewhat puzzling mechanisms for contributing to insulator leakage current since they require the existence of an Ohmic contact at the cathode. Our conventional understanding of an Ohmic contact makes it difficult to ascertain how an Ohmic contact could be formed to a wide bandgap insulator. This Ohmic contact dilemma is resolved by formulating an equivalent circuit appropriate for assessing the
J−ξ
characteristics of an insulator and then recognizing that an insulator Ohmic contact is obtained when the injection-limited current density from the cathode electrode is greater than that of the operative bulk-limited current density, i.e., Ohmic or SCLC for the four insulators under consideration.","Insulators,
Mathematical model,
Cathodes,
Ohmic contacts,
Leakage currents,
Thermionic emission,
Current density"
SVM-DT-based adaptive and collaborative intrusion detection,"As a primary defense technique, intrusion detection becomes more and more significant since the security of the networks is one of the most critical issues in the world. We present an adaptive collaboration intrusion detection method to improve the safety of a network. A self-adaptive and collaborative intrusion detection model is built by applying the Environmentsclasses, agents, roles, groups, and objects U+0028 E-CARGO U+0029 model. The objects, roles, agents, and groups are designed by using decision trees U+0028 DTs U+0029 and support vector machines U+0028 SVMs U+0029, and adaptive scheduling mechanisms are set up. The KDD CUP 1999 data set is used to verify the effectiveness of the method. The experimental results demonstrate the feasibility and efficiency of the proposed collaborative and adaptive intrusion detection method. Also, the proposed method is shown to be more predominant than the methods that use a set of single type support vector machine U+0028 SVM U+0029 in terms of detection precision rate and recall rate.","Intrusion detection,
Support vector machines,
Collaboration,
Adaptation models,
Detectors,
Algorithm design and analysis,
Feature extraction"
Data gathering in wireless sensor networks via regular low density parity check matrix,"A great challenge faced by wireless sensor networks U+0028 WSNs U+0029 is to reduce energy consumption of sensor nodes. Fortunately, the data gathering via random sensing can save energy of sensor nodes. Nevertheless, its randomness and density usually result in difficult implementations, high computation complexity and large storage spaces in practical settings. So the deterministic sparse sensing matrices are desired in some situations. However, it is difficult to guarantee the performance of deterministic sensing matrix by the acknowledged metrics. In this paper, we construct a class of deterministic sparse sensing matrices with statistical versions of restricted isometry property U+0028 StRIP U+0029 via regular low density parity check U+0028 RLDPC U+0029 matrices. The key idea of our construction is to achieve small mutual coherence of the matrices by confining the column weights of RLDPC matrices such that StRIP is satisfied. Besides, we prove that the constructed sensing matrices have the same scale of measurement numbers as the dense measurements. We also propose a data gathering method based on RLDPC matrix. Experimental results verify that the constructed sensing matrices have better reconstruction performance, compared to the Gaussian, Bernoulli, and CSLDPC matrices. And we also verify that the data gathering via RLDPC matrix can reduce energy consumption of WSNs.",
Wind and Solar Power Integration in Electricity Markets and Distribution Networks Through Service-Centric Virtual Power Plants,"A virtual power plant (VPP) is formulated and developed as a service-centric aggregator that enables the market integration of distributed energy resources and simultaneously supports cooperation with the distribution system operator in addressing the issue of network usage. A suitable schedule of interactions and communications between aggregators, market operators, system operators, generators, and consumers, regarding electricity market participation and network operation is proposed and presented in a sequence diagram. The cooperation on congestion management in the distribution network is highlighted as a solution to relieve network constraints via the optimal adjustment of active and reactive power of VPP resources while maximizing renewable energy integration across the pool under management. The VPP reduces uncertainty affiliated with input data by employing the latest forecasts through a rolling horizon approach in the planning stage. Thanks to the flexibility of the VPP to perform rescheduling in accordance with agreements it negotiated with its resources, it becomes possible to refrain from undesirable curtailments. Both the market-integrative and the service-centric roles of the VPP are verified through modeling and simulation with a benchmark European distribution network. The results confirm the added value of the proposed VPP in enhancing the integration of wind and solar power.",
High-Fidelity Model Order Reduction for Microgrids Stability Assessment,"Proper modeling of inverter-based microgrids is crucial for accurate assessment of stability boundaries. It has been recently realized that the stability conditions for such microgrids are significantly different from those known for large-scale power systems. In particular, the network dynamics, despite its fast nature, appears to have major influence on stability of slower modes. While detailed models are available, they are both computationally expensive and not transparent enough to provide an insight into the instability mechanisms and factors. In this paper, a computationally efficient and accurate reduced-order model is proposed for modeling inverter-based microgrids. The developed model has a structure similar to quasi-stationary model and at the same time properly accounts for the effects of network dynamics. The main factors affecting microgrid stability are analyzed using the developed reduced-order model and shown to be unique for microgrids, having no direct analogy in large-scale power systems. Particularly, it has been discovered that the stability limits for the conventional droop-based system are determined by the ratio of inverter rating to network capacity, leading to a smaller stability region for microgrids with shorter lines. Finally, the results are verified with different models based on both frequency and time domain analyses.","Power system stability,
Microgrids,
Stability analysis,
Power system dynamics,
Reduced order systems,
Computational modeling,
Inverters"
"Encoding-Decoding-Based control and filtering of networked systems: insights, developments and opportunities","In order to make the information transmission more efficient and reliable in a digital communication channel with limited capacity, various encoding-decoding techniques have been proposed and widely applied in many branches of the signal processing including digital communications, data compression, information encryption, etc. Recently, due to its promising application potentials in the networked systems U+0028 NSs U+0029, the analysis and synthesis issues of the NSs under various encoding-decoding schemes have stirred some research attention. However, because of the network-enhanced complexity caused by the limited network resources, it poses new challenges to the design of suitable encoding-decoding procedures to meet certain control or filtering performance for the NSs. In this survey paper, our aim is to present a comprehensive review of the encoding-decodingbased control and filtering problems for different types of NSs. First, some basic introduction with respect to the coding-decoding mechanism is presented in terms of its engineering insights, specific properties and theoretical formulations. Then, the recent representative research progress in the design of the encodingdecoding protocols for various control and filtering problems is discussed. Some possible further research topics are finally outlined for the encoding-decoding-based NSs.","Decoding,
Bandwidth,
System performance,
Data communication,
Encoding,
Distortion"
Toward Smart Building Design Automation: Extensible CAD Framework for Indoor Localization Systems Deployment,"Over the last years, many smart buildings applications, such as indoor localization or safety systems, have been subject of intense research. Smart environments usually rely on several hardware nodes equipped with sensors, actuators, and communication functionalities. The high level of heterogeneity and the lack of standardization across technologies make design of such environments a very challenging task, as each installation has to be designed manually and performed ad-hoc for the specific building. On the other hand, many different systems show common characteristics, like the strict dependency with the building floor plan, also sharing similar requirements such as a nodes allocation that provides sensing coverage and nodes connectivity. This paper provides a computer-aided design application for the design of smart building systems based on the installation of hardware nodes across the indoor space. The tool provides a site-specific algorithm for cost-effective deployment of wireless localization systems, with the aim to maximize the localization accuracy. Experimental results from real-world environment show that the proposed site-specific model can improve the positioning accuracy of general models from the state-of-the-art. The tool, available open-source, is modular and extensible through plug-ins allowing to model building systems with different requirements.",
A methodology for reliability of WSN based on software defined network in adaptive industrial environment,"As communication technology and smart manufacturing have developed, the industrial internet of things U+0028 IIoT U+0029 has gained considerable attention from academia and industry. Wireless sensor networks U+0028 WSNs U+0029 have many advantages with broad applications in many areas including environmental monitoring, which makes it a very important part of IIoT. However, energy depletion and hardware malfunctions can lead to node failures in WSNs. The industrial environment can also impact the wireless channel transmission, leading to network reliability problems, even with tightly coupled control and data planes in traditional networks, which obviously also enhances network management cost and complexity. In this paper, we introduce a new software defined network U+0028 SDN U+0029, and modify this network to propose a framework called the improved software defined wireless sensor network U+0028 improved SD-WSN U+0029. This proposed framework can address the following issues. 1 U+0029 For a large scale heterogeneous network, it solves the problem of network management and smooth merging of a WSN into IIoT. 2 U+0029 The network coverage problem is solved which improves the network reliability. 3 U+0029 The framework addresses node failure due to various problems, particularly related to energy consumption. Therefore, it is necessary to improve the reliability of wireless sensor networks, by developing certain schemes to reduce energy consumption and the delay time of network nodes under IIoT conditions. Experiments have shown that the improved approach significantly reduces the energy consumption of nodes and the delay time, thus improving the reliability of WSN.","Wireless sensor networks,
Software reliability,
Switches,
Energy consumption,
Software"
A Novel Fully Synthesizable All-Digital RF Transmitter for IoT Applications,"In this paper, a fully synthesizable all-digital transmitter (ADTX) is first proposed. This transmitter (TX) uses Cartesian architecture and supports wide-band quadratic-amplitude modulation with wide carrier frequency range. Furthermore, the design methodology for ADTX and corresponding bandpass filter is discussed. This TX is synthesized with digital register transfer level-graphic database system flow, and can be easily implemented in any standard CMOS technology. An exemplary TX is synthesized by TSMC 28-nm standard cell library with extremely small area (0.0009 mm2) and supports carrier frequency as high as 6 GHz with excellent error vector magnitude (<−30 dB). To the best of the authors’ knowledge, this is the first work on a fully synthesizable design of RF transistors, allowing easy technology migration and portability.","CMOS,
design methodology,
RF,
synthesis"
Information Geometry Approach to Verification of Dynamic Models in Power Systems,"This paper describes a new class of system identification procedures that are tailored to electric power systems, in particular to synchronous generators (SGs) and other dynamic components. Our procedure builds on computational advances in differential geometry, and offers a new, global characterization of challenges frequently encountered in system identification of electric power systems. The approach also benefits from increasing availability of high-quality measurements. While the proposed procedure is illustrated on SG example in a multimachine benchmark (IEEE 14-bus and real-world 441-bus power systems), it is equally applicable to identification of other system components, such as loads.","Data models,
Mathematical model,
Manifolds,
Power system stability,
Information geometry,
Sensitivity,
Computational modeling"
A Sequential Approach to Market State Modeling and Analysis in Online P2P Lending,"Online peer-to-peer (P2P) lending is an emerging wealth-management service for individuals, which allows lenders to directly bid and invest on the listings created by borrowers without going through any traditional financial intermediaries. As a nonbank financial platform, online P2P lending tends to have both high volatility and liquidity. Therefore, it is of significant importance to discern the hidden market states of the listings (e.g., hot and cold), which open venues for enhancing business analytics and investment decision making. However, the problem of market state modeling remains pretty open due to many technical and domain challenges, such as the dynamic and sequential characteristics of listings. To that end, in this paper, we present a focused study on market state modeling and analysis for online P2P lending. Specifically, we first propose two enhanced sequential models by extending the Bayesian hidden Markov model (BHMM), namely listing-BHMM (L-BHMM) and listing and marketing-BHMM (LM-BHMM), for learning the latent semantics between listings’ market states and lenders’ bidding behaviors. Particularly, L-BHMM is a straightforward model that only considers the local observations of a listing itself, while LM-BHMM considers not only the listing information but also the global information of current market (e.g., the competitive and complementary relations among listings). Furthermore, we demonstrate several motivating applications enabled by our models, such as bidding prediction and herding detection. Finally, we construct extensive experiments on two real-world data sets and make some deep analysis on bidding behaviors, which clearly validate the effectiveness of our models in terms of different applications and also reveal some interesting business findings.","Hidden Markov models,
Peer-to-peer computing,
Investment,
Analytical models,
Predictive models,
Cybernetics"
A Novel Intelligence Algorithm Based on the Social Group Optimization Behaviors,"The collective intelligent behaviors of insects or animal groups in nature have maintained the survival of the species for thousands of years. In this paper, a novel swarm intelligence algorithm called the social group entropy optimization (SGEO) algorithm is proposed for solving optimization tasks. The proposed algorithm is based on the social group model, the status optimization model, and the entropy model, which are the main contributions of this paper. First, the social group model and the feedback mechanism between Leaders and Followers are developed to reduce the probability of local optimum. Second, the status optimization model is described to reveal the changing rule about the population behavior states, to support the conversion between different social behaviors during evolution, to promote the algorithm to optimize quickly, and to avoid local optimization. Third, the entropy model is introduced to analyze the entropy of social groups, the change rule of difference entropy, and to set the information entropy as behavior's criterion of state optimization. In addition, the mathematical model of the SGEO is deduced from the group theory, matter dynamics, and the information entropy theory. The convergence and parallelism of it have been analyzed and verified theoretically. Moreover, to test the effectiveness of the SGEO, it is used to solve benchmark functions' problems that are commonly considered within the literature of evolutionary algorithms. Experimental results are compared with those of three other state-of-the-art algorithms. The superior performance of the SGEO validates its effectiveness and efficiency for the optimization problems, especially for the high-dimension problems.",
A Bi-Level Optimization Model for Grouping Constrained Storage Location Assignment Problems,"In this paper, a novel bi-level grouping optimization (BIGO) model is proposed for solving the storage location assignment problem with grouping constraint (SLAP-GC). A major challenge in this problem is the grouping constraint which restricts the number of groups each product can have and the locations of items in the same group. In SLAP-GC, the problem consists of two subproblems, one is how to group the items, and the other one is how to assign the groups to locations. It is an arduous task to solve the two subproblems simultaneously. To overcome this difficulty, we propose a BIGO. BIGO optimizes item grouping in the upper level, and uses the lower-level optimization to evaluate each item grouping. Sophisticated fitness evaluation and search operators are designed for both upper and lower level optimization so that the feasibility of solutions can be guaranteed, and the search can focus on promising areas in the search space. Based on the BIGO model, a multistart random search method and a tabu search algorithm are proposed. The experimental results on the real-world dataset validate the efficacy of the BIGO model and the advantage of the tabu search method over the random search method.","Optimization,
Search problems,
Mathematical model,
Correlation,
Clothing,
Cybernetics"
Joint-Feature Guided Depth Map Super-Resolution With Face Priors,"In this paper, we present a novel method to super-resolve and recover the facial depth map nicely. The key idea is to exploit the exemplar-based method to obtain the reliable face priors from high-quality facial depth map to improve the depth image. Specifically, a new neighbor embedding (NE) framework is designed for face prior learning and depth map reconstruction. First, face components are decomposed to form specialized dictionaries and then reconstructed, respectively. Joint features, i.e., low-level depth, intensity cues and high-level position cues, are put forward for robust patch similarity measurement. The NE results are used to obtain the face priors of facial structures and smooth maps, which are then combined in an uniform optimization framework to recover high-quality facial depth maps. Finally, an edge enhancement process is implemented to estimate the final high resolution depth map. Experimental results demonstrate the superiority of our method compared to state-of-the-art depth map super-resolution techniques on both synthetic data and real-world data from Kinect.",
Speedup Techniques for Multiobjective Integer Programs in Designing Optimal and Structurally Simple Supervisors of AMS,"This paper investigates several speedup techniques for a multiobjective integer linear program (ILP) used to obtain an optimal Petri net supervisor with a compressed structure for automated manufacturing systems (AMSs). An optimal supervisor can be obtained by forbidding all first-met bad markings and no legal markings of a plant net via place invariants. An iterative method to perform lexicographic multiobjective ILP is proposed to design such supervisor with a simple structure in terms of the numbers of control places and added arcs. Instead of a single ILP, several much smaller ILPs are formulated in the iterative method, and they can be solved much faster. To further reduce the ILP solution time, an efficient redundancy identification method is used. Finally, some AMS examples are provided to demonstrate the proposed speedup techniques and approaches.","System recovery,
Law,
Iterative methods,
Redundancy,
Petri nets,
Computational modeling"
Semisupervised Prior Free Rare Category Detection With Mixed Criteria,"Rare category detection aims to find interesting and statistically significant anomalies and incorporates ideas from active learning and semisupervised learning. The challenge of rare category detection is to find the rare classes of the anomalies in a data set where the data distribution is skewed. Most existing rare category detection methods suppose that the user knows the specific number of all classes in advance, which cannot be satisfied in most real scenarios. In this paper, we propose a new rare category detection framework composed of active learning and semisupervised hierarchical density-based clustering. The advantage of our method is that it is prior free and can benefit the rare category detecting process with the labeled data. In addition, the proposed framework can handle tasks with nonlinear mappings, which increases the ability to find rare classes when the class boundary is sophisticated. Compared to existing methods, better results are achieved by our method on both real and synthetic data sets in the experiment.","Measurement,
Kernel,
Clustering methods,
Semisupervised learning,
Data models,
Estimation"
Nonlinear Modeling and Verification of a Heaving Point Absorber for Wave Energy Conversion,"Although the heaving point absorber (PA) concept is well known in wave energy conversion research, few studies focus on appropriate modeling of nonlinear fluid viscous and mechanical friction dynamics. Even though these concepts are known to have nonlinear effects on the hydrodynamic system, most research studies consider linearity as a starting point and in doing so have a weak approach toward modeling the true dynamic behavior, particularly close to resonance. The sole use of linear modeling leads to limited ability to develop control strategies capable of true power capture optimization and suitable device operation. Based on a 1/50 scale cylindrical heaving PA, this research focuses on a strategy for hydrodynamic model development and experimental verification. In this study, nonlinear dynamics are considered, including the lumped effect of the fluid viscous and mechanical friction forces. The excellent correspondence between the derived nonlinear model and wave tank tested PA behaviors provides a strong background for wave energy tuning and control system design.",
"Millimeter-Wave Communications: Physical Channel Models, Design Considerations, Antenna Constructions and Link-Budget","The millimeter wave (mmWave) frequency band spanning from 30 GHz to 300 GHz constitutes a substantial portion of the unused frequency spectrum, which is an important resource for future wireless communication systems in order to fulfill the escalating capacity demand. Given the improvements in integrated components and enhanced power efficiency at high frequencies, wireless systems can operate in the mmWave frequency band. In this paper, we present a survey of the mmWave propagation characteristics, channel modeling and design guidelines, such as system and antenna design considerations for mmWave, including the link budget of the network, which are essential for mmWave communication systems.We commence by introducing the main channel propagation characteristics of mmWaves followed by channel modeling and design guidelines. Then, we report on the main measurement and modeling campaigns conducted in order to understand the mmWave band’s properties and present the associated channel models. We survey the different channel models focusing on the channel models available for the 28 GHz, 38 GHz, 60 GHz and 73 GHz frequency bands. Finally, we present the mmWave channel model and its challenges in the context of mmWave communication systems design.","Streaming media,
Interference,
Femtocell networks,
Macrocell networks,
Resource management,
Dynamic scheduling,
Ultrafast electronics"
Edge-Preserving Depth Map Upsampling by Joint Trilateral Filter,"Compared to the color images, their associated depth images captured by the RGB-D sensors are typically with lower resolution. The task of depth map super-resolution (SR) aims at increasing the resolution of the range data by utilizing the high-resolution (HR) color image, while the details of the depth information are to be properly preserved. In this paper, we present a joint trilateral filtering (JTF) algorithm for depth image SR. The proposed JTF first observes context information from the HR color image. In addition to the extracted spatial and range information of local pixels, our JTF further integrates local gradient information of the depth image, which allows the prediction and refinement of HR depth image outputs without artifacts like textural copies or edge discontinuities. Quantitative and qualitative experimental results demonstrate the effectiveness and robustness of our approach over prior depth map upsampling works.","Image color analysis,
Image edge detection,
Color,
Image resolution,
Sensors,
Kernel,
Image sensors"
Adaptive Intra Candidate Selection With Early Depth Decision for Fast Intra Prediction in HEVC,"To better exploit spatial correlations in a video frame, the High Efficiency Video Coding (HEVC) standard has adopted a great many more intra prediction modes than H.264/AVC. As a result, the complexity of rate-distortion-optimized (RDO) HEVC intra mode selection is very high. Many techniques have been proposed to expedite the intra mode selection process to achieve a better overall tradeoff between complexity and RD performance. In this paper, two novel techniques for adaptive intra mode candidate selection and bidirectional depth search algorithms are utilized to accelerate intra prediction. The proposed techniques show an average of 63% (up to 67%) time saving with only 1% BD-Rate increase, outperforming most of existing intra prediction algorithms.","Signal processing algorithms,
Encoding,
Prediction algorithms,
Indexes,
Search methods,
Complexity theory,
Correlation"
General Knowledge Embedded Image Representation Learning,"Image representation learning is a fundamental problem in understanding semantics of images. However, traditional classification-based representation learning methods face the noisy and incomplete problem of the supervisory labels. In this paper, we propose a general knowledge base embedded image representation learning approach, which uses general knowledge graph, which is a multitype relational knowledge graph consisting of human commonsense beyond image space, as external semantic resource to capture the relations of concepts in image representation learning. A relational regularized regression CNN (R
3
CNN) model is designed to jointly optimize the image representation learning problem and knowledge graph embedding problem. In this manner, the learnt representation can capture not only labeled tags but also related concepts of images, which involves more precise and complete semantics. Comprehensive experiments are conducted to investigate the effectiveness and transferability of our approach in tag prediction task, zero-shot tag inference task, and content-based image retrieval task. The experimental results demonstrate that the proposed approach performs significantly better than the existing representation learning methods. Finally, observation of the learnt relations show that our approach can somehow refine the knowledge base to describe images and label the images with structured tags.",
CTU-Level Complexity Control for High Efficiency Video Coding,"Among the existing video-related applications, a large proportion have requirements for the scalability of the video coding complexity, such as live video chatting and video coding on power-limited mobile devices. Hence, the complexity control algorithms, which aim to make an effective and flexible tradeoff between coding complexity and rate-distortion (RD) performance, have a great practical value. In this paper, a novel complexity control scheme for high efficiency video coding (HEVC) is proposed by dynamically adjusting the depth range for each coding tree unit (CTU). To control the complexity accurately, a statistical model is proposed to estimate the coding complexity of each CTU. Then the complexity budget is allocated to each CTU proportionally to its estimated complexity. At last, the depth range is optimized for each CTU based on the allocated complexity and the probability that contains the actual maximum depth. Our method works well even if the ratio of target complexity to full complexity drops to 40%. The experimental results show that our proposed method outperforms other four state-of-the-art methods in terms of the RD performance, and has superior complexity control accuracy and complexity control stability compared with other one-pass complexity control strategies.",
Lifelogging Data Validation Model for Internet of Things Enabled Personalized Healthcare,"Internet of Things (IoT) technology offers opportunities to monitor lifelogging data by a variety of assets, like wearable sensors, mobile apps, etc. But due to heterogeneity of connected devices and diverse human life patterns in an IoT environment, lifelogging personal data contains huge uncertainty and are hardly used for healthcare studies. Effective validation of lifelogging personal data for longitudinal health assessment is demanded. In this paper, lifelogging physical activity (LPA) is taken as a target to explore how to improve the validity of lifelogging data in an IoT enabled healthcare system. A rule-based adaptive LPA validation (LPAV) model, LPAV-IoT, is proposed for eliminating irregular uncertainties (IUs) and estimating data reliability in IoT healthcare environments. A methodology specifying four layers and three modules in LPAV-IoT is presented for analyzing key factors impacting validity of LPA. A series of validation rules are designed with uncertainty threshold parameters and reliability indicators and evaluated through experimental investigations. Following LPAV-IoT, a case study on a personalized healthcare platform myhealthavatar connecting three state-of-the-art wearable devices and mobile apps are carried out. The results reflect that the rules provided by LPAV-IoT enable efficiently filtering at least 75% of IU and adaptively indicating the reliability of LPA data on certain condition of IoT environments.","Internet of things,
Medical services,
Uncertainty,
Biomedical monitoring,
Adaptation models,
Reliability,
Data models"
Solving Nonlinear Optimization Problems of Real Functions in Complex Variables by Complex-Valued Iterative Methods,"Much research has been devoted to complex-variable optimization problems due to their engineering applications. However, the complex-valued optimization method for solving complex-variable optimization problems is still an active research area. This paper proposes two efficient complex-valued optimization methods for solving constrained nonlinear optimization problems of real functions in complex variables, respectively. One solves the complex-valued nonlinear programming problem with linear equality constraints. Another solves the complex-valued nonlinear programming problem with both linear equality constraints and an
ℓ
1
-norm constraint. Theoretically, we prove the global convergence of the proposed two complex-valued optimization algorithms under mild conditions. The proposed two algorithms can solve the complex-valued optimization problem completely in the complex domain and significantly extend existing complex-valued optimization algorithms. Numerical results further show that the proposed two algorithms have a faster speed than several conventional real-valued optimization algorithms.",
A First-Order Logic Framework of Major Choosing Decision Making With an Uncertain Reasoning Function,"This paper proposes an auto-decision making system, knowledge-based major choosing decision (KMCD), which helps remote students choose their majors. It is built within a first-order reasoning system with an uncertain reasoning function. The knowledge of both students who want to choose their majors and who have finished their studies are used in the decision making process of the system. KMCD provides personalized suggestions to those students who want to choose majors. Both the learning abilities of students and the career prospects of majors are carefully analyzed on the basis of the knowledge in database. As these analyzing results are well utilized in the decision making process of KMCD, it can be expected that the students who take the suggestions will have a better chance to get graduated smoothly and start their careers successfully after graduation. A detailed description of KMCD is given, which shows that the system is actually a reasonable decision making framework with a capability of expansion of other knowledge which is supposed to be useful for major choosing. An experimental study on data of real college students is devised to justify the effectiveness of KMCD.",
Intellectual Property-Based Lossless Image Compression for Camera Systems [Hardware Matters],"This article presents a novel framework for intellectual property (IP)- based lossless image compression used for camera systems. The proposed approach presents two subframeworks: a) two-dimensional (2-D) Haar wavelet transformation (HWT)- based forward pixel calculator IP for image compression and b) 2-D HWT-based inverse pixel calculator IP for image decompression. Each framework is capable of fully compressing and decompressing images through one-stage computation during forward and inverse transformations. The proposed approach has been tested on images of critical data sets of medical applications [e.g., computed tomography (CT) images and satellite applications, including NASA images]. Results of compressed and decompressed images along with compression efficiency through the proposed framework for IP-based lossless image compression are also reported in this article.",
Convolutional Neural Network for Intermediate View Enhancement in Multiview Streaming,"Multiview video streaming continues to gain popularity due to the great viewing experience it offers, as well as its availability that has been enabled by increased network throughput and other recent technical developments. User demand for interactive multiview video streaming that provides seamless view switching upon request is also increasing. However, it is a highly challenging task to stream stable and high quality videos that allow real-time scene navigation within the bandwidth constraint. In this paper, a convolutional neural network (ConvNet)-assisted seamless multiview video streaming system is proposed to tackle the challenge. The proposed method solves the problem from two perspectives. First, a ConvNet-assisted multiview representation method is proposed, which provides flexible interactivity without compromising on multiview video compression efficiency. Second, a bit allocation mechanism guided by a navigation model is developed to provide seamless navigation and adapt to network bandwidth fluctuations at the same time. These two blocks work closely to provide an optimized viewing experience to users. They can be integrated into any existing multiview video streaming framework to enhance overall performance. Experimental results demonstrate the effectiveness of the proposed method for seamless multiview streaming.",
An Information Bottleneck Approach to Optimize the Dictionary of Visual Data,"In this paper, we propose a novel information theoretic approach to obtain compact and discriminative dictionary of visual data. This approach squeezes discriminative information from the dictionary for efficient representation using information bottleneck. The dictionary is optimized from the initial sparse dictionary, which is learned from action data. In this, a constraint information optimization problem is formulated in which mutual information between the initial and optimized dictionary is minimized while maximizing mutual information between optimized dictionary and class labels. We use an effective similarity measure, Jensen–Shannon divergence with adaptive weightages, for class distributions of each dictionary atom. These adaptive weightages are obtained based on the usage of the dictionary atom among different classes. The resultant dictionary becomes discriminative and compact, while retaining maximum information with fewer atoms. Using simple reconstruction error, we test computational efficiency of the proposed method without compromising classification accuracy on popular benchmark datasets. It is further demonstrated how efficiently discriminative information is retained by comparing the classification performance of the dictionary before and after the removal of redundant dictionary atoms.","Dictionaries,
Optimization,
Atomic measurements,
Mutual information,
Visualization,
Encoding,
Matching pursuit algorithms"
Large-Scale Sparse Learning From Noisy Tags for Semantic Segmentation,"In this paper, we present a large-scale sparse learning (LSSL) approach to solve the challenging task of semantic segmentation of images with noisy tags. Different from the traditional strongly supervised methods that exploit pixel-level labels for semantic segmentation, we make use of much weaker supervision (i.e., noisy tags of images) and then formulate the task of semantic segmentation as a weakly supervised learning (WSL) problem from the view point of noise reduction of superpixel labels. By learning the data manifolds, we transform the WSL problem into an LSSL problem. Based on nonlinear approximation and dimension reduction techniques, a linear-time-complexity algorithm is developed to solve the LSSL problem efficiently. We further extend the LSSL approach to visual feature refinement for semantic segmentation. The experiments demonstrate that the proposed LSSL approach can achieve promising results in semantic segmentation of images with noisy tags.",
Finite-Time and Fixed-Time Cluster Synchronization With or Without Pinning Control,"In this paper, the finite-time and fixed-time cluster synchronization problem for complex networks with or without pinning control are discussed. Finite-time (or fixed-time) synchronization has been a hot topic in recent years, which means that the network can achieve synchronization in finite-time, and the settling time depends on the initial values for finite-time synchronization (or the settling time is bounded by a constant for any initial values for fixed-time synchronization). To realize the finite-time and fixed-time cluster synchronization, some simple distributed protocols with or without pinning control are designed and the effectiveness is rigorously proved. Several sufficient criteria are also obtained to clarify the effects of coupling terms for finite-time and fixed-time cluster synchronization. Especially, when the cluster number is one, the cluster synchronization becomes the complete synchronization problem; when the network has only one node, the coupling term between nodes will disappear, and the synchronization problem becomes the simplest master–slave case, which also includes the stability problem for nonlinear systems like neural networks. All these cases are also discussed. Finally, numerical simulations are presented to demonstrate the correctness of obtained theoretical results.","Synchronization,
Protocols,
Stability analysis,
Couplings,
Multi-agent systems,
Switches"
Querying Beneficial Constraints Before Clustering Using Facility Location Analysis,"This paper examines the problem of querying beneficial constraints before clustering. Existing methods in this area choose constraints heuristically based on some prior assumptions on the usefulness of constraints. However, the usefulness and propagation of constraints are two important issues in the constraints selection that are not investigated simultaneously in most existing works. This paper addresses the problem of querying beneficial constraints using facility location analysis that is one of the most well-studied areas of the operations research. To this end, the source problem of querying k beneficial constraints is transformed into an instance of target uncapacitated k-facility location problem (k-UFL) and then is benefited from existing algorithms in the target space to find a solution to the k-UFL problem. The solution to the k-UFL problem is then transformed into a solution of the source problem of querying k beneficial constraints. Both usefulness and propagation of constraints are achieved in this paper by respectively mapping them into the corresponding opening and service costs in target problem space and then minimizing the total cost in target space. The proposed method is based on an optimization framework and is entirely different from existing methods in the constraints selection that are limited to greedy approaches. A range of experiments is presented to compare the proposed method to alternatives and explore its behavior in the selection of clustering constraints.","Clustering algorithms,
Optimization,
Transforms,
Cybernetics,
Approximation algorithms,
Skeleton,
Operations research"
Harmonious Genetic Clustering,"To automatically determine the number of clusters and generate more quality clusters while clustering data samples, we propose a harmonious genetic clustering algorithm, named HGCA, which is based on harmonious mating in eugenic theory. Different from extant genetic clustering methods that only use fitness, HGCA aims to select the most suitable mate for each chromosome and takes into account chromosomes gender, age, and fitness when computing mating attractiveness. To avoid illegal mating, we design three mating prohibition schemes, i.e., no mating prohibition, mating prohibition based on lineal relativeness, and mating prohibition based on collateral relativeness, and three mating strategies, i.e., greedy eugenics-based mating strategy, eugenics-based mating strategy based on weighted bipartite matching, and eugenics-based mating strategy based on unweighted bipartite matching, for harmonious mating. In particular, a novel single-point crossover operator called variable-length-and-gender-balance crossover is devised to probabilistically guarantee the balance between population gender ratio and dynamics of chromosome lengths. We evaluate the proposed approach on real-life and artificial datasets, and the results show that our algorithm outperforms existing genetic clustering methods in terms of robustness, efficiency, and effectiveness.",
Learning From Cross-Domain Media Streams for Event-of-Interest Discovery,"Every day, vast amounts of data are uploaded to various social-sharing websites. Each social-sharing website has its own media dataset. Recently, mining media datasets has shown great potential for our daily lives, e.g., earthquake detection. Generally, different datasets have different characteristics. Combining different datasets is capable of achieving better performance than using any dataset independently, particularly if the datasets can compensate for each other. The resulting performance, however, depends on the fusion method. Effectively combining different datasets is challenging. As a solution to this challenge, this paper presents a generic two-stage framework for events of interest. Specifically, the first stage normalizes the contents of different datasets to make them comparable; then, the second stage combines the normalized contents for a ranked event list using graph-based algorithms. Practically, this paper unifies a flow-based media dataset and a check-in-based media dataset. Based on the precision for the top n events, the experimental results demonstrate that the proposed framework can achieve better performance in finding events associated with sports, local festivals, concerts, and exhibitions compared with a state-of-the-art approach that uses one dataset alone.","Media,
Public transportation,
Twitter,
Urban areas,
Data mining,
Real-time systems,
Earthquakes"
Deep Nonlinear Metric Learning for 3-D Shape Retrieval,"Effective 3-D shape retrieval is an important problem in 3-D shape analysis. Recently, feature learning-based shape retrieval methods have been widely studied, where the distance metrics between 3-D shape descriptors are usually hand-crafted. In this paper, motivated by the fact that deep neural network has the good ability to model nonlinearity, we propose to learn an effective nonlinear distance metric between 3-D shape descriptors for retrieval. First, the locality-constrained linear coding method is employed to encode each vertex on the shape and the encoding coefficient histogram is formed as the global 3-D shape descriptor to represent the shape. Then, a novel deep metric network is proposed to learn a nonlinear transformation to map the 3-D shape descriptors to a nonlinear feature space. The proposed deep metric network minimizes a discriminative loss function that can enforce the similarity between a pair of samples from the same class to be small and the similarity between a pair of samples from different classes to be large. Finally, the distance between the outputs of the metric network is used as the similarity for shape retrieval. The proposed method is evaluated on the McGill, SHREC’10 ShapeGoogle, and SHREC’14 Human shape datasets. Experimental results on the three datasets validate the effectiveness of the proposed method.","Shape,
Measurement,
Feature extraction,
Heating,
Encoding,
Kernel,
Solid modeling"
Cooperative Bargaining Game-Based Multiuser Bandwidth Allocation for Dynamic Adaptive Streaming Over HTTP,"Dynamic adaptive streaming over HTTP (DASH) has emerged as an efficient technology for video streaming. For a DASH system, a most common case is that a limited server bandwidth is competed by multiusers. In order to improve user quality of experience (QoE) and guarantee fairness, we propose to use the game theory in a proxy server to allocate the bandwidth collaboratively for multiusers. By taking user buffer length, received video bit rates, video qualities, etc., into account, the bandwidth allocation problem is formulated as a cooperative bargaining problem and the Nash bargaining solution (NBS) is obtained by convex optimization. The requested bit rate of users will be rewritten as the proxy calculated bit rate (i.e., NBS) when the user requested bit rate is larger. Experimental results demonstrate that user QoE and fairness can be improved significantly, i.e., the delay frequency and duration are smaller, and the received video qualities are higher and more stable, when comparing the proposed method with existing methods.",
A Dynamic Neighborhood Learning-Based Gravitational Search Algorithm,"Balancing exploration and exploitation according to evolutionary states is crucial to meta-heuristic search (M-HS) algorithms. Owing to its simplicity in theory and effectiveness in global optimization, gravitational search algorithm (GSA) has attracted increasing attention in recent years. However, the tradeoff between exploration and exploitation in GSA is achieved mainly by adjusting the size of an archive, named Kbest, which stores those superior agents after fitness sorting in each iteration. Since the global property of Kbest remains unchanged in the whole evolutionary process, GSA emphasizes exploitation over exploration and suffers from rapid loss of diversity and premature convergence. To address these problems, in this paper, we propose a dynamic neighborhood learning (DNL) strategy to replace the Kbest model and thereby present a DNL-based GSA (DNLGSA). The method incorporates the local and global neighborhood topologies for enhancing the exploration and obtaining adaptive balance between exploration and exploitation. The local neighborhoods are dynamically formed based on evolutionary states. To delineate the evolutionary states, two convergence criteria named limit value and population diversity, are introduced. Moreover, a mutation operator is designed for escaping from the local optima on the basis of evolutionary states. The proposed algorithm was evaluated on 27 benchmark problems with different characteristic and various difficulties. The results reveal that DNLGSA exhibits competitive performances when compared with a variety of state-of-the-art M-HS algorithms. Moreover, the incorporation of local neighborhood topology reduces the numbers of calculations of gravitational force and thus alleviates the high computational cost of GSA.","Convergence,
Topology,
Heuristic algorithms,
Sociology,
Statistics,
Gravity,
Optimization"
Everything You Wanted to Know about Smart Health Care: Evaluating the Different Technologies and Components of the Internet of Things for Better Health,"The Internet-of-Things (IoT) has taken over the business spectrum, and its applications vary widely from agriculture and health care to transportation. A hospital environment can be very stressful, especially for senior citizens and children. With the ever-increasing world population, the conventional patient-doctor appointment has lost its effectiveness. Hence, smart health care becomes very important. Smart health care can be implemented at all levels, starting from temperature monitoring for babies to tracking vital signs in the elderly. The complexity and cost of implementation varies based on the required precision of the individual device, functionalities, and sophistication of the application for which they are used. Smart health care also falls under vertical areas such as very-large-scale integration, embedded system, big data, machine learning, cloud computing, and artificial intelligence. This article discusses the importance, requirements, application of smart health care along with the current industry trends and products. It gives a deeper insight about the different platform across which more research can be pursued in this dynamic domain.",
Smart Home Environment for Mild Cognitive Impairment Population: Solutions to Improve Care and Quality of Life,"The transition stage from the natural cognitive decline of normal aging to the more serious decline of dementia is referred to as mild cognitive impairment (MCI). The cognitive changes caused in MCI are noticeable by the individuals experiencing them and by others, but the changes are not severe enough to interfere with daily life or with independent activities. Because there is a thin line between normal aging and MCI, it is difficult for individuals to discern between the two conditions. Moreover, if the symptoms of MCI are not diagnosed in time, it may lead to more serious and permanent conditions. However, if these symptoms are detected in time and proper care and precaution are taken, it is possible to prevent the condition from worsening. A smart-home environment that unobtrusively keeps track of the individual's daily living activities is a possible solution to improve care and quality of life.","Aging,
Intelligent sensors,
Biomedical monitoring,
Wearable sensors,
Medical devices,
Dementia"
GraphD: Distributed Vertex-Centric Graph Processing Beyond the Memory Limit,"We propose GraphD, an out-of-core Pregel-like system targeting efficient big graph processing with a small cluster of commodity PCs connected by Gigabit Ethernet, an environment affordable to most users. This is in contrast to some recent efforts for out-of-core graph computation with specialized hardware. In our setting, a vertex-centric program is often data-intensive, since the CPU cost of calculating a message value is negligible compared with the network cost of transmitting that message. As a result, network bandwidth is usually the bottleneck, and out-of-core execution would not sacrifice performance if disk IO overhead can be hidden by message transmission, which is achieved by GraphD through the parallelism of computation and communication. GraphD streams edge and message data on local disks, and thus consumes negligible memory space. For a broad class of Pregel algorithms where message combiner is applicable, GraphD completely eliminates the need of any expensive external-memory join or group-by, which is required by existing systems such as Pregelix and Chaos. Extensive experiments show that GraphD beats existing out-of-core systems by orders of magnitude, and achieves comparable performance to in-memory systems running with adequate memory.",
Leveraging Hardware-Assisted Virtualization for Deterministic Replay on Commodity Multi-Core Processors,"Deterministic replay, which provides the ability to travel backward in time and reconstruct the past execution flow of a multiprocessor system, has many prominent applications. Prior research in this area can be classified into two categories: hardware-only schemes and software-only schemes. While hardware-only schemes deliver high performance, they require significant modifications to the existing hardware. In contrast, software-only schemes work on commodity hardware, but suffer from excessive performance overhead and huge logs. In this article, we present the design and implementation of a novel system, Samsara, which uses the hardware-assisted virtualization (HAV) extensions to achieve efficient deterministic replay without requiring any hardware modification. Unlike prior software schemes which trace every single memory access to record interleaving, Samsara leverages HAV on commodity processors to track the read-set and write-set for implementing a chunk-based recording scheme in software. By doing so, we avoid all memory access detections, which is a major source of overhead in prior works. Evaluation results show that compared with prior software-only schemes, Samsara significantly reduces the log file size to 1/70th on average, and further reduces the recording overhead from about
10×
, reported by state-of-the-art works, to
2.1×
on average.",
"Random Regular Graph and Generalized De Bruijn Graph with
k
-Shortest Path Routing","The Random regular graph (RRG) has recently been proposed as an interconnect topology for future large scale data centers and HPC clusters. An RRG is a special case of directed regular graph (DRG) where each link is unidirectional and all nodes have the same number of incoming and outgoing links. In this work, we establish bounds for DRGs on diameter, average
k
-shortest path length, and a load balancing property with
k
-shortest path routing, and use these bounds to evaluate RRGs. The results indicate that an RRG with
k
-shortest path routing is not ideal in terms of diameter and load balancing. We further consider the Generalized De Bruijn Graph (GDBG), a deterministic DRG, and prove that for most network configurations, a GDBG is near optimal in terms of diameter, average
k
-shortest path length, and load balancing with a
k
-shortest path routing scheme. Finally, we use modeling and simulation to exploit the strengths and weaknesses of RRGs for different traffic conditions by comparing RRGs with GDBGs.",
Type Information Elimination from Objects on Architectures with Tagged Pointers Support,"Implementations of object-oriented programming languages associate type information with each object to perform various runtime tasks such as dynamic dispatch, type introspection, and reflection. A common means of storing such relation is by inserting a pointer to the associated type information into every object. Such an approach, however, introduces memory and performance overheads when compared with non-object-oriented languages. Recent 64-bit computer architectures have added support for tagged pointers by ignoring a number of bits - tag - of memory addresses during memory access operations and utilize them for other purposes; mainly security. This paper presents the first investigation into how this hardware support can be exploited by a Java Virtual Machine to remove type information from objects. Moreover, we propose novel hardware extensions to the address generation and load-store units to achieve low-overhead type information retrieval and tagged object pointers compression-decompression. The evaluation has been conducted after integrating the Maxine VM and the ZSim microarchitectural simulator. The results, across all the DaCapo benchmark suite, pseudo-SPECjbb2005, SLAMBench and GraphChi-PR executed to completion, show up to 26 and 10 percent geometric mean heap space savings, up to 50 and 12 percent geometric mean dynamic DRAM energy reduction, and up to 49 and 3 percent geometric mean execution time reduction with no significant performance regressions.","Java,
Memory management,
Layout,
Hardware,
Object oriented modeling,
Metadata"
Energy Efficiency Aware Task Assignment with DVFS in Heterogeneous Hadoop Clusters,"While Hadoop ecosystems become increasingly important for practitioners of large-scale data analysis, they also incur tremendous energy cost. This trend is driving up the need for designing energy-efficient Hadoop clusters in order to reduce the operational costs and the carbon emission associated with its energy consumption. However, despite extensive studies of the problem, existing approaches for energy efficiency have not fully considered the heterogeneity of both workload and machine hardware found in production environments. In this paper, we find that heterogeneity-oblivious task assignment approaches are detrimental to both performance and energy efficiency of Hadoop clusters. Our observation shows that even heterogeneity-aware techniques that aim to reduce the job completion time do not guarantee a reduction in energy consumption of heterogeneous machines. We propose a heterogeneity-aware task assignment approach, E-Ant, that aims to improve the overall energy consumption in a heterogeneous Hadoop cluster without sacrificing job performance. It adaptively schedules heterogeneous workloads on energy-efficient machines, without a priori knowledge of the workload properties. E-Ant employs an ant colony optimization approach that generates task assignment solutions based on the feedback of each task’s energy consumption reported by Hadoop TaskTrackers in an agile way. Furthermore, we integrate DVFS technique with E-Ant to further improve the energy efficiency of heterogeneous Hadoop clusters. It relies on a DVFS controller to dynamically scale the CPU frequency of each slave machine in response to time-varying resource demands. Experimental results on a heterogeneous cluster with varying hardware capabilities show that E-Ant with DVFS improves the overall energy savings for a synthetic workload from Microsoft by 23 and 17 percent compared to Fair Scheduler and Tarazu, respectively.",
CoCloud: Enabling Efficient Cross-Cloud File Collaboration Based on Inefficient Web APIs,"Cloud storage services such as Dropbox have been widely used for file collaboration among multiple users. However, this desirable functionality is yet restricted to the “walled-garden” of each service. At present, the only feasible approach to cross-cloud file collaboration seems to be using web APIs, whose performance is known to be highly unstable and unpredictable. Now that using inefficient web APIs is inevitable, in this paper we attempt to achieve sound user-perceived performance for cross-cloud file collaboration. This attempt is enabled by two key observations from real-world measurements. First, for each cloud, we are always able to deploy one or several nearby (client) proxies which can efficiently access the web APIs. Second, during file collaboration, significant similarity exists among different versions of a file. This can be exploited to substantially reduce inter-proxy traffic and thus shorten the data sync time. Guided by the observations, we design and implement an open-source prototype system called CoCloud. Currently, it supports file collaboration among four popular cloud storage services in the US and China. Its performance is well acceptable to users under representative workloads, even approaching or exceeding that of intra-cloud collaboration in many cases.",
VOLAP: A Scalable Distributed Real-Time OLAP System for High-Velocity Data,"This paper presents VelocityOLAP (VOLAP), a distributed real-time OLAP system for high-velocity data. VOLAP makes use of dimension hierarchies, is highly scalable, exploits both multi-core and multi-processor parallelism, and can guarantee serializable execution of insert and query operations. In contrast to other high performance OLAP systems such as SAP HANA or IBM Netezza that rely on vertical scaling or special purpose hardware, VOLAP supports cost-efficient horizontal scaling on commodity hardware or modest cloud instances. Experiments on 20 Amazon EC2 nodes with TPC-DS data show that VOLAP is capable of bulk ingesting data at over 600 thousand items per second, and processing streams of interspersed insertions and aggregate queries at a rate of approximately 50 thousand insertions and 20 thousand aggregate queries per second with a database of 1 billion items. VOLAP is designed to support applications that perform large aggregate queries, and provides similar high performance for aggregations ranging from a few items to nearly the entire database.",
Rapid Calculation of Max-Min Fair Rates for Multi-Commodity Flows in Fat-Tree Networks,"Max-min fairness is often used in the performance modeling of interconnection networks. Existing methods to compute max-min fair rates for multi-commodity flows have high complexity and are computationally infeasible for large networks. In this work, we show that by considering topological features, this problem can be solved efficiently for the fat-tree topology that is widely used in data centers and high performance compute clusters. Several efficient new algorithms are developed for this problem, including a parallel algorithm that can take advantage of multi-core and shared-memory architectures. Using these algorithms, we demonstrate that it is possible to find the max-min fair rate allocation for multi-commodity flows in fat-tree networks that support tens of thousands of nodes. We evaluate the run-time performance of the proposed algorithms and show improvement in orders of magnitude over the previously best known method. We further demonstrate a new application of max-min fair rate allocation that is only computationally feasible using our new algorithms.",
A Neural Approach to Source Dependence Based Context Model for Statistical Machine Translation,"In statistical machine translation, translation prediction considers not only the aligned source word itself but also its source contextual information. Learning context representation is a promising method for improving translation results, particularly through neural networks. Most of the existing methods process context words sequentially and neglect source long-distance dependencies. In this paper, we propose a novel neural approach to source dependence-based context representation for translation prediction. The proposed model is capable of not only encoding source long-distance dependencies but also capturing functional similarities to better predict translations (i.e., word form translations and ambiguous word translations). To verify our method, the proposed mode is incorporated into phrase-based and hierarchical phrase-based translation models, respectively. Experiments on large-scale Chinese-to-English and English-to-German translation tasks show that the proposed approach achieves significant improvement over the baseline systems and outperforms several existing context-enhanced methods.","Artificial neural networks,
Context modeling,
Decoding,
Speech,
Encoding,
Semantics,
Speech processing"
STABLE: Stress-Aware Boolean Matching to Mitigate BTI-Induced SNM Reduction in SRAM-Based FPGAs,"Biased-Temperature-Instability (BTI) aging mechanism reduces Static-Noise-Margin (SNM) of SRAM cells. This leads to a higher Soft-Error-Rate (SER), lower reliability, and lower SRAMs’ stability in FPGAs. SNM partially improves by leveraging the recovery phase of BTI through flipping SRAM content. We propose STABLE, a three-step post-synthesis stress-aware technique, in order to reduce the impact of BTI-induced SNM reduction in FPGA Look-up-Tables (LUTs) using the SAT-based Boolean Matching (BM) algorithm. STABLE partitions Data-Flow-Graph (DFG) of the implemented design into different cones. First, the SAT-based BM algorithm finds a new configuration for each cone while their functionalities are preserved and all SRAMs are flipped. Second, cones that did not pass the first step can benefit from unused SRAMs in their partially-used LUTs for storing the flipped configurations of such LUTs. Finally, flipped configurations of fully-used LUTs are stored in the closest unused LUTs. The main configuration of the implemented FPGA design is swapped by the new flipped configuration, periodically. Our extensive experimental analysis demonstrates 69 and 70 percent on average improvements in the SNM reduction () and the SER increase (
ΔSER
), respectively. Since the proposed methodology is deployed after the FPGA placement and routing of the application, the overhead is negligible.","Stress,
Transistors,
Table lookup,
Field programmable gate arrays,
SRAM cells,
Aging"
Multiple Sound Source Location Estimation in Wireless Acoustic Sensor Networks Using DOA Estimates: The Data-Association Problem,"In this paper, we consider the data-association problem for the localization of multiple sound sources in a wireless acoustic sensor network, where each node is a microphone array, using direction of arrival (DOA) estimates. The data-association problem arises because the central node that receives the multiple DOA estimates from the nodes cannot know to which source they belong. Hence, the DOAs from the different nodes that correspond to the same source must be found in order to perform accurate localization. We present a method to identify the correct association of DOAs to the sources and thus accurately estimate their locations. Our method results in high association and localization accuracy in realistic scenarios with missed detections, reverberation, noise, and moving sources and outperforms other recently proposed methods. It also incorporates a bitrate reduction scheme in order to keep the amount of information that needs to be transmitted in the network at low levels without affecting performance.",
Quadcopter Tracks Quadcopter via Real-Time Shape Fitting,"We suggest a novel algorithm that tracks given shapes in real time from a low-quality video stream. The algorithm is based on a careful selection of a small subset of pixels that suffices to obtain an approximation of the observed shape. The shape can then be extracted quickly from the small subset. We implemented the algorithm in a system for mutual localization of a group of low-cost toy-quadcopters. Each quadcopter carries only a single 8-g RGB camera, and stabilizes itself via real-time tracking of the other quadcopters in ~30 frames/s. Existing algorithms for real-time shape fitting are based on more expensive hardware, external cameras, or have significantly worse performance. We provide full open source to our algorithm, experimental results, benchmarks, and video that demonstrates our system. We then discuss generalizations to other shapes and extensions for more robotics applications.","Shape,
Real-time systems,
Approximation algorithms,
Streaming media,
Robots,
Cameras,
Robustness"
Optimization of Error-Bounded Lossy Compression for Hard-to-Compress HPC Data,"Since today's scientific applications are producing vast amounts of data, compressing them before storage/transmission is critical. Results of existing compressors show two types of HPC data sets: highly compressible and hard to compress. In this work, we carefully design and optimize the error-bounded lossy compression for hard-to-compress scientific data. We propose an optimized algorithm that can adaptively partition the HPC data into best-fit consecutive segments each having mutually close data values, such that the compression condition can be optimized. Another significant contribution is the optimization of shifting offset such that the XOR-leading-zero length between two consecutive unpredictable data points can be maximized. We finally devise an adaptive method to select the best-fit compressor at runtime for maximizing the compression factor. We evaluate our solution using 13 benchmarks based on real-world scientific problems, and we compare it with 9 other state-of-the-art compressors. Experiments show that our compressor can always guarantee the compression errors within the user-specified error bounds. Most importantly, our optimization can improve the compression factor effectively, by up to 49 percent for hard-to-compress data sets with similar compression/decompression time cost.","Data models,
Compressors,
Benchmark testing,
Solid modeling,
Analytical models,
Optimization,
Distributed databases"
A Design Space Exploration Methodology for Parameter Optimization in Multicore Processors,"The need for application-specific design of multicore/manycore processing platforms is evident with computing systems finding use in diverse application domains. In order to tailor multicore/manycore processors for application specific requirements, a multitude of processor design parameters have to be tuned accordingly which involves rigorous and extensive design space exploration over large search spaces. In this paper, we propose an efficient methodology for design space exploration. We evaluate our methodology over two search spaces small and large, using a cycle-accurate simulator (ESESC) and a standard set of PARSEC and SPLASH-2 benchmarks. For the smaller design space, we compare results obtained from our design space exploration methodology with results obtained from fully exhaustive search. The results show that solution quality obtained from our methodology are within 1.35 - 3.69 percent of the results obtained from fully exhaustive search while only exploring 2.74 - 3 percent of the design space. For larger design space, we compare solution quality of different results obtained by varying the number of tunable processor design parameters included in the exhaustive search phase of our methodology. The results show that including more number of tunable parameters in the exhaustive search phase of our methodology greatly improves solution quality.",
A Parallel-Elastic Actuator for a Torque-Controlled Back-Support Exoskeleton,"A torque-controlled back-support exoskeleton to assist manual handling is presented. Its objective is to provide a significant portion of the forces necessary to carry out the physical task, thereby reducing the compressive loads on the lumbar spine and the associated risk of injury. The design rationale for a parallel-elastic actuator (PEA) is proposed to match the asymmetrical torque requirements associated with the target task. The parallel spring relaxes the maximum motor torque requirements, with substantial effects on the resulting torque-control performance. A formal analysis and experimental evaluation is presented with the goal of documenting the improvement in performance. To this end, the proposed PEA is compared with a more traditional configuration without the parallel spring. The formal analysis and experimental results highlight the importance of the motor inertia reflected through the gearbox and illustrate the improvements in the proposed measures of torque-control performance.",
A Generic Construction of Quantum-Oblivious-Key-Transfer-Based Private Query with Ideal Database Security and Zero Failure,"Higher security and lower failure probability have always been people’s pursuits in quantum-oblivious-key-transfer-based private query (QOKT-PQ) protocols since Jacobi et al. [Phys. Rev. A 83, 022301 (2011)] proposed the first protocol of this kind. However, higher database security generally has to be obtained at the cost of a higher failure probability, and vice versa. Recently, based on a round-robin differential-phase-shift quantum key distribution protocol, Liu et al. [Sci. China-Phys. Mech. Astron. , 58, 100301 (2015)] presented a private query protocol (RRDPS-PQ protocol) utilizing ideal single-photon signal which realizes both ideal database security and zero failure probability. However, ideal single-photon source is not available today, and for large database the required pulse train is too long to implement. Here, we reexamine the security of RRDPS-PQ protocol under imperfect source and present an improved protocol using a special “low-shift and addition” (LSA) technique, which not only can be used to query from large database but also retains the features of “ideal database security” and “zero-failure” even under weak coherent source. Finally, we generalize the LSA technique and establish a generic QOKT-PQ model in which both “ideal database security” and “zero failure” are achieved via acceptable communications.","Protocols,
Databases,
Photonics,
Optical wavelength conversion,
Cryptography,
Quantum computing"
The CoDyCo Project Achievements and Beyond: Toward Human Aware Whole-Body Controllers for Physical Human Robot Interaction,"The success of robots in real-world environments is largely dependent on their ability to interact with both humans and said environment. The FP7 EU project CoDyCo focused on the latter of these two challenges by exploiting both rigid and compliant contacts dynamics in the robot control problem. Regarding the former, to properly manage interaction dynamics on the robot control side, an estimation of the human behaviors and intentions is necessary. In this letter, we present the building blocks of such a human-in-the-loop controller, and validate them in both simulation and on the iCub humanoid robot using a human-robot interaction scenario. In this scenario, a human assists the robot in standing up from being seated on a bench.","Robot sensing systems,
Service robots,
Robot kinematics,
Human-robot interaction,
Humanoid robots"
Performability Analysis of Large-Scale Multi-State Computing Systems,"Modern computing systems typically use a large number of independent, non-identical computing nodes to perform a set of coordinated computations in parallel. The computing system and its constituent computing nodes often exhibit more than two performance levels or states corresponding to different computing powers. This paper models and evaluates performability of large-scale multi-state computing systems, which is the probability that a computing system performs at a particular performance level. The heterogeneity in the constituent components of different nodes (due to factors such as different model generations, model suppliers, and operating environments) makes performability analysis difficult and challenging. In this paper a specification method for system performance level (SPL) is first introduced. A multi-valued decision diagram (MDD) based approach is then proposed for performability analysis of multi-state computing systems consisting of nodes with different state occupation probabilities, which encompasses novel and efficient MDD model generation procedures. Example and benchmark studies are performed to show that the proposed approach can offer efficient performability analysis of large-scale computing systems.",
Efficient Detection for Malicious and Random Errors in Additive Encrypted Computation,"Although data confidentiality is the primary security objective in additive encrypted computation applications, such as the aggregation of encrypted votes in electronic elections, ensuring the trustworthiness of data is equally important. And yet, integrity protections are generally orthogonal to additive homomorphic encryption, which enables efficient encrypted computation, due to the inherent malleability of homomorphic ciphertexts. Since additive homomorphic schemes are founded on modular arithmetic, our framework extends residue numbering to support fast modular reductions and homomorphic syndromes for detecting random errors inside homomorphic ALUs and data memories. In addition, our methodology detects malicious modifications of memory data, using keyed syndromes and block cipher-based integrity trees, which allow preserving the homomorphism of ALU operations, while enforcing non-malleability of memory data. Compared to traditional memory integrity protections, our tree-based syndrome generation and updating is parallelizable for increased efficiency, while requiring a small Trusted Computing Base for secret key storage and block cipher operations. Our evaluation shows more than 99.999 percent detection rate for random ALUs errors, as well as 100 percent detection rate of single bit-flips and clustered multiple bit upsets, for a runtime overhead between 1.2 and 5.5 percent, and a small area penalty.","Encryption,
Additives,
Computational modeling,
Fault detection,
Hardware"
Value the Recent Past: Approximate Causal Consistency for Partially Replicated Systems,"In wide-area distributed systems, data replication provides fault tolerance and low latency. Causal consistency in such systems is an interesting consistency model. Most existing works assume the data is fully replicated because this greatly simplifies the design of the algorithms to implement causal consistency. Recently, we proposed causal consistency under partial replication because it reduces the number of messages used under a wide range of workloads. One drawback of partial replication is that its meta-data tends to be relatively large when the message size is small. In this paper, we propose an algorithm Approx-Opt-Track which provides approximate causal consistency whereby we can reduce the meta-data at the cost of some violations of causal consistency. The amount of violations can be made arbitrarily small by controlling a tunable parameter, that we call credits. We present the analytic data to show the performance of Approx-Opt-Track. We then give simulation results to show the potential benefit of Approx-Opt-Track, viz., its ability to provide almost the same guarantees as causal consistency, at a smaller cost.","Protocols,
Approximation algorithms,
Social network services,
Real-time systems,
Message passing,
History,
Distributed databases"
"Collision Cones for Quadric Surfaces in
n
-Dimensions","In this letter, we present analytical expressions for collision cones associated with a class of hyper quadric surfaces moving in n-dimensional configuration space. Using a relative velocity paradigm, a geometric analysis of the distance, time, and point of closest approach between moving objects in n-dimensional space, is carried out to obtain a characterization of the collision cone between a point and a hyper spheroid as well as a constrained hyperboloid, which represent an interesting and useful class of objects in configuration space. It is shown that these n-dimensional collision cones can be integrated with sampling-based motion planners, avoiding the need to evaluate way points that lie inside the collision cone. The cones can also consist of the heading angles toward desirable regions in the configuration space, in which case planners may evaluate more way points inside the cone. Finally, analytical expressions of the collision cones are used, in conjunction with the concept of level sets, and incorporated into a Lyapunov-based design approach, to determine analytical expressions of nonlinear guidance laws that can manipulate the velocity vector of an object in n-dimensional space.","Shape,
Differential equations,
Aerospace engineering,
Navigation,
Collision avoidance,
Level set,
Indexes"
Adaptive Resource Allocation and Provisioning in Multi-Service Cloud Environments,"In the current cloud business environment, the cloud provider (CP) can provide a means for offering the required quality of service (QoS) for multiple classes of clients. We consider the cloud market where various resources such as CPUs, memory, and storage in the form of Virtual Machine (VM) instances can be provisioned and then leased to clients with QoS guarantees. Unlike existing works, we propose a novel Service Level Agreement (SLA) framework for cloud computing, in which a price control parameter is used to meet QoS demands for all classes in the market. The framework uses reinforcement learning (RL) to derive a VM hiring policy that can adapt to changes in the system to guarantee the QoS for all client classes. These changes include: service cost, system capacity, and the demand for service. In exhibiting solutions, when the CP leases more VMs to a class of clients, the QoS is degraded for other classes due to an inadequate number of VMs. However, our approach integrates computing resources adaptation with service admission control based on the RL model. To the best of our knowledge, this study is the first attempt that facilitates this integration to enhance the CP's profit and avoid SLA violation. Numerical analysis stresses the ability of our approach to avoid SLA violation while maximizing the CP's profit under varying cloud environment conditions.",
Comments on Truncation Errors for Polynomial Chaos Expansions,"Methods based on polynomial chaos expansion allow to approximate the behavior of systems with uncertain parameters by deterministic dynamics. These methods are used in a wide range of applications, spanning from simulation of uncertain systems to estimation and control. For practical purposes the exploited spectral series expansion is typically truncated to allow for efficient computation, which leads to approximation errors. Despite the Hilbert space nature of polynomial chaos, there are only a few results in the literature that explicitly discuss and quantify these approximation errors. This letter derives error bounds for polynomial chaos approximations of polynomial and non-polynomial mappings. Sufficient conditions are established, which allow investigating the question whether zero truncation errors can be achieved and which series order is required to achieve this. Furthermore, convex quadratic programs, whose argmin operator is a special case of a piecewise polynomial mapping, are studied due to their relevance in predictive control. Several simulation examples illustrate our findings.","Finite wordlength effects,
Random variables,
Uncertainty,
Chaos,
Stochastic processes,
Hilbert space,
Predictive control"
On Weak Signal Detection With Compressive Measurements,"The problem of weak signal detection in Gaussian noise is addressed in the Neyman-Pearson framework with compressive measurements. A locally optimum detector is first devised assuming that the signal is nonsparse by approximating the test statistic around zero using a Taylor series, which is a good estimate only in a small radius around zero. When the signal is sparse, it is shown that the performance of this test degrades. To improve its performance, a new test is devised by deriving the Padé approximation of the test statistic around zero. Padé approximants estimate functions as the rational quotient of two lower degree polynomials and consistently have a wider radius of convergence than the Taylor series. The performance of the Padé-approximated test is better than its Taylor series counterpart and is comparable to the conventional locally optimum test with uncompressed measurements. Simulation results are presented to support the analytical findings of the work.","Taylor series,
Manganese,
Detectors,
Signal detection,
Simulation,
Noise measurement,
Signal to noise ratio"
Micro-Lens-Based Matching for Scene Recovery in Lenslet Cameras,"Since a light-field camera is able to capture more information than a traditional camera, a lot of methods, such as depth estimation, image super-resolution, and view synthesis, are explored for recovering scene information. In this paper, we propose a novel framework for scene recovery based on lenslet-based light-field camera images. Instead of using traditional matching terms, we design a new micro-lens-based matching term to calculate structure information and recover several kinds of scene information simultaneously. On the one hand, inherent information in micro-lens images is selected to complement details in sub-aperture images. On the other hand, sub-aperture images are used to expand micro-lens images and synthesize new view images. A new micro-lens-based consistency metric is introduced for the matching term to handle occlusions in depth estimation and image reconstruction. The newly appeared and newly occluded areas in synthesized views are analyzed and recovered based on information from surrounding points. Experimental results show that the proposed depth estimation method outperforms state-of-the-art methods on both synthetic and lenslet-based light-field images, especially in low-texture and occlusion regions. Furthermore, the super-resolution and view synthesis methods are able to acquire view images with more details and less aliasing artifacts.",
Progressive Joint Modeling in Unsupervised Single-Channel Overlapped Speech Recognition,"Unsupervised single-channel overlapped speech recognition is one of the hardest problems in automatic speech recognition (ASR). Permutation invariant training (PIT) is a state of the art model-based approach, which applies a single neural network to solve this single-input, multiple-output modeling problem. We propose to advance the current state of the art by imposing a modular structure on the neural network, applying a progressive pretraining regimen, and improving the objective function with transfer learning and a discriminative training criterion. The modular structure splits the problem into three subtasks: frame-wise interpreting, utterance-level speaker tracing, and speech recognition. The pretraining regimen uses these modules to solve progressively harder tasks. Transfer learning leverages parallel clean speech to improve the training targets for the network. Our discriminative training formulation is a modification of standard formulations that also penalizes competing outputs of the system. Experiments are conducted on the artificial overlapped switchboard and hub5e-swb dataset. The proposed framework achieves over 30% relative improvement of word error rate over both a strong jointly trained system, PIT for ASR, and a separately optimized system, PIT for speech separation with clean speech ASR model. The improvement comes from better model generalization, training efficiency, and the sequence level linguistic knowledge integration.","Speech recognition,
Speech,
Training,
Acoustics,
Speech processing,
Neural networks"
Tensor Factorization for Low-Rank Tensor Completion,"Recently, a tensor nuclear norm (TNN) based method was proposed to solve the tensor completion problem, which has achieved state-of-the-art performance on image and video inpainting tasks. However, it requires computing tensor singular value decomposition (t-SVD), which costs much computation and thus cannot efficiently handle tensor data, due to its natural large scale. Motivated by TNN, we propose a novel low-rank tensor factorization method for efficiently solving the 3-way tensor completion problem. Our method preserves the low-rank structure of a tensor by factorizing it into the product of two tensors of smaller sizes. In the optimization process, our method only needs to update two smaller tensors, which can be more efficiently conducted than computing t-SVD. Furthermore, we prove that the proposed alternating minimization algorithm can converge to a Karush–Kuhn–Tucker point. Experimental results on the synthetic data recovery, image and video inpainting tasks clearly demonstrate the superior performance and efficiency of our developed method over state-of-the-arts including the TNN and matricization methods.","Tensile stress,
Matrix decomposition,
Discrete Fourier transforms,
Singular value decomposition,
Minimization,
Complexity theory"
An Information Distillation Framework for Extractive Summarization,"In the context of natural language processing, representation learning has emerged as a newly active research subject because of its excellent performance in many applications. Learning representations of words is a pioneering study in this school of research. However, paragraph (or sentence and document) embedding learning is more suitable/reasonable for some realistic tasks such as document summarization. Nevertheless, classic paragraph embedding methods infer the representation of a given paragraph by considering all of the words occurring in the paragraph. Consequently, those stop or function words that occur frequently may mislead the embedding learning process to produce a misty paragraph representation. Motivated by these observations, our major contributions in this paper are threefold. First, we propose a novel unsupervised paragraph embedding method, named the essence vector (EV) model, which aims at not only distilling the most representative information from a paragraph but also excluding the general background information to produce a more informative low-dimensional vector representation for the paragraph of interest. Second, in view of the increasing importance of spoken content processing, an extension of the EV model, named the denoising essence vector (D-EV) model, is proposed. The D-EV model not only inherits the advantages of the EV model but also can infer a more robust representation for a given spoken paragraph against imperfect speech recognition. Third, a new summarization framework, which can take both relevance and redundancy information into account simultaneously, is also introduced. We evaluate the proposed embedding methods (i.e., EV and D-EV) and the summarization framework on two benchmark summarization corpora. The experimental results demonstrate the effectiveness and applicability of the proposed framework in relation to several well-practiced and state-of-the-art summarization methods.","Context modeling,
Predictive models,
Speech,
Training,
Speech processing,
Neural networks"
Memory-Reduced Look-Up Tables for Efficient ADMM Decoding of LDPC Codes,"The Euclidean projection involved in the decoding of low-density parity-check (LDPC) codes with the alternating direction method of multipliers (ADMM) can be simplified by jointly using uniform quantization and look-up tables (LUTs). However, the memory requirement for the original LUT-based ADMM decoding is comparatively large. In this letter, a nonuniform quantization method is proposed to save the memory cost by minimizing the mean square error of the outputs of Euclidean projections during quantization. Simulation results over two exemplified LDPC codes show that the proposed method can achieve similar error-rate performances when compared with the original LUT-based ADMM decoding by using significantly less memory units.",
Body Structure Aware Deep Crowd Counting,"Crowd counting is a challenging task, mainly due to the severe occlusions among dense crowds. This paper aims to take a broader view to address crowd counting from the perspective of semantic modeling. In essence, crowd counting is a task of pedestrian semantic analysis involving three key factors: pedestrians, heads, and their context structure. The information of different body parts is an important cue to help us judge whether there exists a person at a certain position. Existing methods usually perform crowd counting from the perspective of directly modeling the visual properties of either the whole body or the heads only, without explicitly capturing the composite body-part semantic structure information that is crucial for crowd counting. In our approach, we first formulate the key factors of crowd counting as semantic scene models. Then, we convert the crowd counting problem into a multi-task learning problem, such that the semantic scene models are turned into different sub-tasks. Finally, the deep convolutional neural networks are used to learn the sub-tasks in a unified scheme. Our approach encodes the semantic nature of crowd counting and provides a novel solution in terms of pedestrian semantic analysis. In experiments, our approach outperforms the state-of-the-art methods on four benchmark crowd counting data sets. The semantic structure information is demonstrated to be an effective cue in scene of crowd counting.","Semantics,
Head,
Visualization,
Neural networks,
Detectors,
Estimation,
Information science"
Uncertainty Visualization Using Copula-Based Analysis in Mixed Distribution Models,"Distributions are often used to model uncertainty in many scientific datasets. To preserve the correlation among the spatially sampled grid locations in the dataset, various standard multivariate distribution models have been proposed in visualization literature. These models treat each grid location as a univariate random variable which models the uncertainty at that location. Standard multivariate distributions (both parametric and nonparametric) assume that all the univariate marginals are of the same type/family of distribution. But in reality, different grid locations show different statistical behavior which may not be modeled best by the same type of distribution. In this paper, we propose a new multivariate uncertainty modeling strategy to address the needs of uncertainty modeling in scientific datasets. Our proposed method is based on a statistically sound multivariate technique called Copula, which makes it possible to separate the process of estimating the univariate marginals and the process of modeling dependency, unlike the standard multivariate distributions. The modeling flexibility offered by our proposed method makes it possible to design distribution fields which can have different types of distribution (Gaussian, Histogram, KDE etc.) at the grid locations, while maintaining the correlation structure at the same time. Depending on the results of various standard statistical tests, we can choose an optimal distribution representation at each location, resulting in a more cost efficient modeling without significantly sacrificing on the analysis quality. To demonstrate the efficacy of our proposed modeling strategy, we extract and visualize uncertain features like isocontours and vortices in various real world datasets. We also study various modeling criterion to help users in the task of univariate model selection.","Computational modeling,
Uncertainty,
Analytical models,
Feature extraction,
Correlation,
Data models,
Standards"
Electric Power Network State Tracking From Multirate Measurements,"This paper proposes a novel tracking state estimator to process both fast-rate synchronized phasor and slow-rate supervisory control and data acquisition (SCADA) measurements. The former are assumed to be in limited number. The latter are exploited as and when they arrive to the control center. In order to restore observability, after each execution of the tracking state estimator, forecast SCADA measurements are used as pseudo-measurements in the next estimation. An event detection analysis allows assessing if the system is in quasi-steady-state. If so, an innovation analysis is performed to identify and eliminate erroneous SCADA measurements. The system state is computed by Hachtel's augmented matrix method. The option of exploiting time-tagged SCADA measurements is also considered. The method is illustrated through detailed dynamic simulations of a test system evolving toward voltage collapse, with and without emergency control.",
A Stochastic Optimal Control Approach for Exploring Tradeoffs between Cost Savings and Battery Aging in Datacenter Demand Response,"This brief paper optimizes power management for datacenters employing lithium-ion battery storage, with the specific goal of addressing the tradeoff between: 1) the cost saving achievable through the peak demand shaving and 2) the corresponding battery aging. To the best of the authors’ knowledge, this tradeoff has never been addressed using physics-based models of battery performance and degradation combined with stochastic models of datacenter demand. We build: 1) a Markov chain model of datacenter power demand; 2) a second-order model of battery diffusion/reaction dynamics; and 3) a physics-based model of battery aging via solid electrolyte interphase growth. Together, these models enable the solution of the battery health-conscious demand response problem via stochastic dynamic programming (SDP). A penalty function is used for enforcing a datacenter “power cap” within this SDP problem. By varying this power cap, we traverse the Pareto tradeoff between the cost savings due to demand response and battery health degradation.","Batteries,
Load management,
Aging,
Electrodes,
Stochastic processes,
Current density,
Degradation"
End-to-End Blind Image Quality Assessment Using Deep Neural Networks,"We propose a multi-task end-to-end optimized deep neural network (MEON) for blind image quality assessment (BIQA). MEON consists of two sub-networks—a distortion identification network and a quality prediction network—sharing the early layers. Unlike traditional methods used for training multi-task networks, our training process is performed in two steps. In the first step, we train a distortion type identification sub-network, for which large-scale training samples are readily available. In the second step, starting from the pre-trained early layers and the outputs of the first sub-network, we train a quality prediction sub-network using a variant of the stochastic gradient descent method. Different from most deep neural networks, we choose biologically inspired generalized divisive normalization (GDN) instead of rectified linear unit as the activation function. We empirically demonstrate that GDN is effective at reducing model parameters/layers while achieving similar quality prediction performance. With modest model complexity, the proposed MEON index achieves state-of-the-art performance on four publicly available benchmarks. Moreover, we demonstrate the strong competitiveness of MEON against state-of-the-art BIQA models using the group maximum differentiation competition methodology.","Training,
Image quality,
Image coding,
Nonlinear distortion,
Neural networks"
No-Reference Image Quality Assessment by Wide-Perceptual-Domain Scorer Ensemble Method,"A no-reference (NR) learning-based approach to assess image quality is presented in this paper. The devised features are extracted from wide perceptual domains, including brightness, contrast, color, distortion, and texture. These features are used to train a model (scorer) which can predict scores. The scorer selection algorithms are utilized to help simplify the proposed system. In the final stage, the ensemble method is used to combine the prediction results from selected scorers. Two multiple-scale versions of the proposed approach are also presented along with the single-scale one. They turn out to have better performances than the original single-scale method. Because of having features from five different domains at multiple image scales and using the outputs (scores) from selected score prediction models as features for multi-scale or cross-scale fusion (i.e., ensemble), the proposed NR image quality assessment models are robust with respect to more than 24 image distortion types. They also can be used on the evaluation of images with authentic distortions. The extensive experiments on three well-known and representative databases confirm the performance robustness of our proposed model.","Distortion,
Feature extraction,
Image quality,
Databases,
Predictive models,
Computational modeling,
Image color analysis"
A Novel State Transformation Approach to Tracking of Piecewise Linear Trajectories,"In this paper, we propose a novel approach for tracking of piecewise linear trajectories, such as triangular and staircase waveforms. We derive state and input transformations, which result in closed-loop error dynamics driven by a series of impulses. The proposed control structure takes the form of an output-feedback-feedforward system that is straightforward to implement. In contrast to the recently proposed tracking control methods for such trajectories, the closed-loop stability is not affected by the frequency of the desired triangular reference. The method is implemented on a nanopositioner serving as the scanning stage of an atomic force microscope.","Stability criteria,
Closed loop systems,
Bandwidth,
Trajectory,
Noise measurement,
Nanopositioning,
Microscopy"
Structure Models for Image-Assisted Geometry Measurement in Plenoptic Sampling,"We present a signal-processing framework for image-assisted geometry measurement in the image-based rendering (IBR). We study the utilized geometry information and estimating minimum sampling rate of the IBR. Our method combines decomposing a complex scene geometry into a collection of simpler structures on a block-by-block basis. The automatic simpler structure selection can be interactively refined by detected single salient points. In this manner, we reduce the spectral analysis problem of an irregular object to that of a simpler structure. Predictions on the frequency content can then be used to control the sampling rate. This extends previous work in which the IBR sampling is analyzed and estimated for nonuniform sampling. Extensive experimental evaluation demonstrates that our geometry simplification method significantly outperforms competing algorithms. Additionally, the minimum sampling rate of the IBR necessary for alias-free rendering will be reduced as the number of simpler structures increases.","Geometry,
Image reconstruction,
Rendering (computer graphics),
Shape,
Optical fibers,
Rough surfaces,
Surface roughness"
Frog: Asynchronous Graph Processing on GPU with Hybrid Coloring Model,"GPUs have been increasingly used to accelerate graph processing for complicated computational problems regarding graph theory. Many parallel graph algorithms adopt the asynchronous computing model to accelerate the iterative convergence. Unfortunately, the consistent asynchronous computing requires locking or atomic operations, leading to significant penalties/overheads when implemented on GPUs. As such, the coloring algorithm is adopted to separate the vertices with potential updating conflicts, guaranteeing the consistency/correctness of the parallel processing. Common coloring algorithms, however, may suffer from low parallelism because of a large number of colors generally required for processing a large-scale graph with billions of vertices. We propose a light-weight asynchronous processing framework called Frog with a preprocessing/hybrid coloring model. The fundamental idea is based on the Pareto principle (or 80-20 rule) about coloring algorithms as we observed through masses of real-world graph coloring cases. We find that a majority of vertices (about 80 percent) are colored with only a few colors, such that they can be read and updated in a very high degree of parallelism without violating the sequential consistency. Accordingly, our solution separates the processing of the vertices based on the distribution of colors. In this work, we mainly answer three questions: (1) how to partition the vertices in a sparse graph with maximized parallelism, (2) how to process large-scale graphs that cannot fit into GPU memory, and (3) how to reduce the overhead of data transfers on PCIe while processing each partition. We conduct experiments on real-world data (Amazon, DBLP, YouTube, RoadNet-CA, WikiTalk, and Twitter) to evaluate our approach and make comparisons with well-known non-preprocessed (such as Totem, Medusa, MapGraph, and Gunrock) and preprocessed (Cusha) approaches, by testing four classical algorithms (BFS, PageRank, SSSP, and CC). On all the tested applications and datasets, Frog is able to significantly outperform existing GPU-based graph processing systems except Gunrock and MapGraph. MapGraph gets better performance than Frog when running BFS on RoadNet-CA. The comparison between Gunrock and Frog is inconclusive. Frog can outperform Gunrock more than 1.04X when running PageRank and SSSP, while the advantage of Frog is not obvious when running BFS and CC on some datasets especially for RoadNet-CA.",
Linking Fine-Grained Locations in User Comments,"Many domain-specific websites host a profile page for each entity (e.g., locations on Foursquare, movies on IMDb, and products on Amazon) for users to post comments on. When commenting on an entity, users often mention other entities for reference or comparison. Compared with web pages and tweets, the problem of disambiguating the mentioned entities in user comments has not received much attention. This paper investigates linking fine-grained locations in Foursquare comments. We demonstrate that the focal location, i.e., the location that a comment is posted on, provides rich contexts for the linking task. To exploit such information, we represent the Foursquare data in a graph, which includes locations, comments, and their relations. A probabilistic model named FocalLink is proposed to estimate the probability that a user mentions a location when commenting on a focal location, by following different kinds of relations. Experimental results show that FocalLink is consistently superior under different collective linking settings.",
Weakly-Supervised Deep Embedding for Product Review Sentiment Analysis,"Product reviews are valuable for upcoming buyers in helping them make decisions. To this end, different opinion mining techniques have been proposed, where judging a review sentence's orientation (e.g., positive or negative) is one of their key challenges. Recently, deep learning has emerged as an effective means for solving sentiment classification problems. A neural network intrinsically learns a useful representation automatically without human efforts. However, the success of deep learning highly relies on the availability of large-scale training data. We propose a novel deep learning framework for product review sentiment classification which employs prevalently available ratings as weak supervision signals. The framework consists of two steps: (1) learning a high level representation (an embedding space) which captures the general sentiment distribution of sentences through rating information; and (2) adding a classification layer on top of the embedding layer and use labeled sentences for supervised fine-tuning. We explore two kinds of low level network structure for modeling review sentences, namely, convolutional feature extractors and long short-term memory. To evaluate the proposed framework, we construct a dataset containing 1.1M weakly labeled review sentences and 11,754 labeled review sentences from Amazon. Experimental results show the efficacy of the proposed framework and its superiority over baselines.","Machine learning,
Neural networks,
Feature extraction,
Sentiment analysis,
Syntactics,
Training"
Redundancy Reduction for Prevalent Co-Location Patterns,"Spatial co-location pattern mining is an interesting and important task in spatial data mining which discovers the subsets of spatial features frequently observed together in nearby geographic space. However, the traditional framework of mining prevalent co-location patterns produces numerous redundant co-location patterns, which makes it hard for users to understand or apply. To address this issue, in this paper, we study the problem of reducing redundancy in a collection of prevalent co-location patterns by utilizing the spatial distribution information of co-location instances. We first introduce the concept of semantic distance between a co-location pattern and its super-patterns, and then define redundant co-locations by introducing the concept of δ-covered, where
δ(0≤δ≤1)
is a coverage measure. We develop two algorithms RRclosed and RRnull to perform the redundancy reduction for prevalent co-location patterns. The former adopts the post-mining framework that is commonly used by existing redundancy reduction techniques, while the latter employs the mine-and-reduce framework that pushes redundancy reduction into the co-location mining process. Our performance studies on the synthetic and real-world data sets demonstrate that our method effectively reduces the size of the original collection of closed co-location patterns by about 50 percent. Furthermore, the RRnull method runs much faster than the related closed co-location pattern mining algorithm.","Redundancy,
Data mining,
Semantics,
Measurement,
Spatial databases,
Indexes"
MV-FTL: An FTL That Provides Page-Level Multi-Version Management,"In this paper, we propose MV-FTL, a multi-version flash transition layer (FTL) that provides page-level multi-version management. By extending a unique characteristic of solid-state drives (SSDs), the out-of-place (OoP) update to multi-version management, MV-FTL can both guarantee atomic page updates from each transaction and provide concurrency without requiring redundant log data writes as well. For evaluation, we first modified SQLite, a lightweight database management system (DBMS), to cooperate with MV-FTL. Owing to the architectural simplicity of SQLite, we clearly show that MV-FTL improves both the performance and the concurrency aspects of the system. In addition, to prove the effectiveness in a full-fledged enterprise-level DBMS, we modified MyRocks, a MySQL variant by Facebook, to use our new Patch Compaction algorithm, which deeply relies on MV-FTL. The TPC-C and LinkBench benchmark tests demonstrated that MV-FTL reduces the overall amount of writes, implying that MV-FTL can be effective in such DBMSs.","Concurrent computing,
Benchmark testing,
Compaction,
Atomic layer deposition,
Facebook,
Concurrency control,
Media"
Automated Construction and Maintenance of Wi-Fi Radio Maps for Crowdsourcing-based Indoor Positioning Systems,"If the construction and maintenance of Wi-Fi radio maps were fully automated, the implementation of a global-scale Wi-Fi indoor positioning system would be possible. This paper proposes a Wi-Fi radio map calibration system that automates the initial construction and maintenance of radio maps using crowdsourced fingerprints collected from numerous smartphones without location information. The system incorporates an unsupervised learning algorithm into an incremental and adaptive calibration process. The unsupervised learning algorithm constructs an initial radio map, using fingerprints collected from unknown locations, by finding a hidden structure among them. Once a positioning service is available based on the initial radio map, the radio map continues to adapt to signal changes in the environment through the incremental and adaptive calibration process using the fingerprints that are continuously collected from the service users. Experiments carried out in an office building have shown that the proposed system could successfully construct and maintain a precise radio map without requiring any location information. A longterm experiment that lasted for five months, the proposed system was able to not just maintain but also improve the quality of the radio map. These results indicate that Wi-Fi indoor positioning systems can be automatically constructed and maintained in continuously changing Wi-Fi environments without manual calibration efforts.","Fingerprint recognition,
Calibration,
Wireless fidelity,
Sensors,
Optimization,
Maintenance engineering,
Unsupervised learning"
MOSS-5: A Fast Method of Approximating Counts of 5-Node Graphlets in Large Graphs,"Counting 3-, 4-, and 5-node graphlets in graphs is important for graph mining applications such as discovering abnormal/evolution patterns in social and biology networks. In addition, it is recently widely used for computing similarities between graphs and graph classification applications such as protein function prediction and malware detection. However, it is challenging to compute these graphlet counts for a large graph or a large set of graphs due to the combinatorial nature of the problem. Despite recent efforts in counting 3-node and 4-node graphlets, little attention has been paid to characterizing 5-node graphlets. In this paper, we develop a computationally efficient sampling method to estimate 5-node graphlet counts. We not only provide a fast sampling method and unbiased estimators of graphlet counts, but also derive simple yet exact formulas for the variances of the estimators which are of great value in practice—the variances can be used to bound the estimates’ errors and determine the smallest necessary sampling budget for a desired accuracy. We conduct experiments on a variety of real-world datasets, and the results show that our method is several orders of magnitude faster than the state-of-the-art methods with the same accuracy.","Proteins,
Malware,
Sampling methods,
Electronic mail,
Kernel"
