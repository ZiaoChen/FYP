Title,Abstract,Keywords
Nonrigid registration using free-form deformations: application to breast MR images,"In this paper the authors present a new approach for the nonrigid registration of contrast-enhanced breast MRI. A hierarchical transformation model of the motion of the breast has been developed. The global motion of the breast is modeled by an affine transformation while the local breast motion is described by a free-form deformation (FFD) based on B-splines. Normalized mutual information is used as a voxel-based similarity measure which is insensitive to intensity changes as a result of the contrast enhancement. Registration is achieved by minimizing a cost function, which represents a combination of the cost associated with the smoothness of the transformation and the cost associated with the image similarity. The algorithm has been applied to the fully automated registration of three-dimensional (3-D) breast MRI in volunteers and patients. In particular, the authors have compared the results of the proposed nonrigid registration algorithm to those obtained using rigid and affine registration techniques. The results clearly indicate that the nonrigid registration algorithm is much better able to recover the motion and deformation of the breast than rigid or affine registration algorithms.","Magnetic resonance imaging,
Diseases,
Breast cancer,
Cancer detection,
Mammography,
Cost function,
Breast tissue,
Cyclic redundancy check,
Biomedical imaging,
Medical diagnostic imaging"
Evolutionary programming made faster,"Evolutionary programming (EP) has been applied with success to many numerical and combinatorial optimization problems in recent years. EP has rather slow convergence rates, however, on some function optimization problems. In the paper, a ""fast EP"" (FEP) is proposed which uses a Cauchy instead of Gaussian mutation as the primary search operator. The relationship between FEP and classical EP (CEP) is similar to that between fast simulated annealing and the classical version. Both analytical and empirical studies have been carried out to evaluate the performance of FEP and CEP for different function optimization problems. The paper shows that FEP is very good at search in a large neighborhood while CEP is better at search in a small local neighborhood. For a suite of 23 benchmark problems, FEP performs much better than CEP for multimodal functions with many local minima while being comparable to CEP in performance for unimodal and multimodal functions with only a few local minima. The paper also shows the relationship between the search step size and the probability of finding a global optimum and thus explains why FEP performs better than CEP on some functions but not on others. In addition, the importance of the neighborhood size and its relationship to the probability of finding a near-optimum is investigated. Based on these analyses, an improved FEP (IFEP) is proposed and tested empirically. This technique mixes different search operators (mutations). The experimental results show that IFEP performs better than or as well as the better of FEP and CEP for most benchmark problems tested.","Genetic programming,
Genetic mutations,
Testing,
Australia,
Computer science,
Simulated annealing,
Computational intelligence,
Convergence,
Performance analysis,
Performance evaluation"
This site canâ€™t be reached,,
Texture synthesis by non-parametric sampling,"A non-parametric method for texture synthesis is proposed. The texture synthesis process grows a new image outward from an initial seed, one pixel at a time. A Markov random field model is assumed, and the conditional distribution of a pixel given all its neighbors synthesized so far is estimated by querying the sample image and finding all similar neighborhoods. The degree of randomness is controlled by a single perceptually intuitive parameter. The method aims at preserving as much local structure as possible and produces good results for a wide variety of synthetic and real-world textures.","Sampling methods,
Image sampling,
Pixel,
Filters,
Histograms,
Computer science,
Integrated circuit synthesis,
Computer vision,
Image texture analysis,
Application software"
Parameter control in evolutionary algorithms,"The issue of controlling values of various parameters of an evolutionary algorithm is one of the most important and promising areas of research in evolutionary computation: it has a potential of adjusting the algorithm to the problem while solving the problem. In the paper we: 1) revise the terminology, which is unclear and confusing, thereby providing a classification of such control mechanisms, and 2) survey various forms of control which have been studied by the evolutionary computation community in recent years. Our classification covers the major forms of parameter control in evolutionary computation and suggests some directions for further research.","Evolutionary computation,
Computer science,
Terminology,
Genetic mutations,
Bridges,
Problem-solving,
Information technology,
Real time systems,
Power control"
Survey: interpolation methods in medical image processing,"Image interpolation techniques often are required in medical imaging for image generation (e.g., discrete back projection for inverse Radon transform) and processing such as compression or resampling. Since the ideal interpolation function spatially is unlimited, several interpolation kernels of finite size have been introduced. This paper compares 1) truncated and windowed sine; 2) nearest neighbor; 3) linear; 4) quadratic; 5) cubic B-spline; 6) cubic; g) Lagrange; and 7) Gaussian interpolation and approximation techniques with kernel sizes from 1/spl times/1 up to 8/spl times/8. The comparison is done by: 1) spatial and Fourier analyses; 2) computational complexity as well as runtime evaluations; and 3) qualitative and quantitative interpolation error determinations for particular interpolation tasks which were taken from common situations in medical image processing. For local and Fourier analyses, a standardized notation is introduced and fundamental properties of interpolators are derived. Successful methods should be direct current (DC)-constant and interpolators rather than DC-inconstant or approximators. Each method's parameters are tuned with respect to those properties. This results in three novel kernels, which are introduced in this paper and proven to be within the best choices for medical image interpolation: the 6/spl times/6 Blackman-Harris windowed sinc interpolator, and the C2-continuous cubic kernels with N=6 and N=8 supporting points. For quantitative error evaluations, a set of 50 direct digital X-rays was used. They have been selected arbitrarily from clinical routine. In general, large kernel sizes were found to be superior to small interpolation masks. Except for truncated sine interpolators, all kernels with N=6 or larger sizes perform significantly better than N=2 or N=3 point methods (p/spl Lt/0.005). However, the differences within the group of large-sized kernels were not significant. Summarizing the results, the cubic 6/spl times/6 interpolator with continuous second derivatives, as defined in (24), can be recommended for most common interpolation tasks. It appears to be the fastest six-point kernel to implement computationally. It provides eminent local and Fourier properties, is easy to implement, and has only small errors. The same characteristics apply to B-spline interpolation, but the 6/spl times/6 cubic avoids the intrinsic border effects produced by the B-spline technique. However, the goal of this study was not to determine an overall best method, but to present a comprehensive catalogue of methods in a uniform terminology, to define general properties and requirements of local techniques, and to enable the reader to select that method which is optimal for his specific application in medical imaging.",
"Global, voxel, and cluster tests, by theory and permutation, for a difference between two groups of structural MR images of the brain","The authors describe almost entirely automated procedures for estimation of global, voxel, and cluster-level statistics to test the null hypothesis of zero neuroanatomical difference between two groups of structural magnetic resonance imaging (MRI) data. Theoretical distributions under the null hypothesis are available for (1) global tissue class volumes; (2) standardized linear model [analysis of variance (ANOVA and ANCOVA)] coefficients estimated at each voxel; and (3) an area of spatially connected clusters generated by applying an arbitrary threshold to a two-dimensional (2-D) map of normal statistics at voxel level. The authors describe novel methods for economically ascertaining probability distributions under the null hypothesis, with fewer assumptions, by permutation of the observed data. Nominal Type I error control by permutation testing is generally excellent; whereas theoretical distributions may be over conservative. Permutation has the additional advantage that it can be used to test any statistic of interest, such as the sum of suprathreshold voxel statistics in a cluster (or cluster mass), regardless of its theoretical tractability under the null hypothesis. These issues are illustrated by application to MRI data acquired from 18 adolescents with hyperkinetic disorder and 16 control subjects matched for age and gender.","Statistical analysis,
Statistical distributions,
Magnetic resonance imaging,
Volume measurement,
Psychiatry,
Automatic testing,
Analysis of variance,
Probability distribution,
Hospitals,
Two dimensional displays"
Automated model-based tissue classification of MR images of the brain,"Describes a fully automated method for model-based tissue classification of magnetic resonance (MR) images of the brain. The method interleaves classification with estimation of the model parameters, improving the classification at each iteration. The algorithm is able to segment single- and multi-spectral MR images, corrects for MR signal inhomogeneities, and incorporates contextual information by means of Markov random Fields (MRF's). A digital brain atlas containing prior expectations about the spatial location of tissue classes is used to initialize the algorithm. This makes the method fully automated and therefore it provides objective and reproducible segmentations. The authors have validated the technique on simulated as well as on real MR images of the brain.",
Monte Carlo localization for mobile robots,"To navigate reliably in indoor environments, a mobile robot must know where it is. Thus, reliable position estimation is a key problem in mobile robotics. We believe that probabilistic approaches are among the most promising candidates to providing a comprehensive and real-time solution to the robot localization problem. However, current methods still face considerable hurdles. In particular the problems encountered are closely related to the type of representation used to represent probability densities over the robot's state space. Earlier work on Bayesian filtering with particle-based density representations opened up a new approach for mobile robot localization based on these principles. We introduce the Monte Carlo localization method, where we represent the probability density involved by maintaining a set of samples that are randomly drawn from it. By using a sampling-based representation we obtain a localization method that can represent arbitrary distributions. We show experimentally that the resulting method is able to efficiently localize a mobile robot without knowledge of its starting location. It is faster, more accurate and less memory-intensive than earlier grid-based methods,.","Monte Carlo methods,
Mobile robots,
Navigation,
Robot localization,
Uncertainty,
Computer science,
Indoor environments,
Orbital robotics,
State-space methods,
Bayesian methods"
Adaptive fuzzy segmentation of magnetic resonance images,"An algorithm is presented for the fuzzy segmentation of two-dimensional (2-D) and three-dimensional (3-D) multispectral magnetic resonance (MR) images that have been corrupted by intensity inhomogeneities, also known as shading artifacts. The algorithm is an extension of the 2-D adaptive fuzzy C-means algorithm (2-D AFCM) presented in previous work by the authors. This algorithm models the intensity inhomogeneities as a gain field that causes image intensities to smoothly and slowly vary through the image space. It iteratively adapts to the intensity inhomogeneities and is completely automated. In this paper, the authors fully generalize 2-D AFCM to three-dimensional (3-D) multispectral images. Because of the potential size of 3-D image data, they also describe a new faster multigrid-based algorithm for its implementation. They show, using simulated MR data, that 3-D AFCM yields lower error rates than both the standard fuzzy C-means (FCM) algorithm and two other competing methods, when segmenting corrupted images. Its efficacy is further demonstrated using real 3-D scalar and multispectral MR brain images.","Image segmentation,
Magnetic resonance,
Iterative algorithms,
Magnetic resonance imaging,
Magnetic noise,
Image analysis,
Nonuniform electric fields,
Filtering,
Surface fitting,
Two dimensional displays"
Improved decoding of Reed-Solomon and algebraic-geometry codes,"Given an error-correcting code over strings of length n and an arbitrary input string also of length n, the list decoding problem is that of finding all codewords within a specified Hamming distance from the input string. We present an improved list decoding algorithm for decoding Reed-Solomon codes. The list decoding problem for Reed-Solomon codes reduces to the following ""curve-fitting"" problem over a field F: given n points ((x/sub i//spl middot/y/sub i/))/sub i=1//sup n/, x/sub i/, y/sub i//spl isin/F, and a degree parameter k and error parameter e, find all univariate polynomials p of degree at most k such that y/sub i/=p(x/sub i/) for all but at most e values of i/spl isin/(1,...,n). We give an algorithm that solves this problem for e1/3, where the result yields the first asymptotic improvement in four decades. The algorithm generalizes to solve the list decoding problem for other algebraic codes, specifically alternant codes (a class of codes including BCH codes) and algebraic-geometry codes. In both cases, we obtain a list decoding algorithm that corrects up to n-/spl radic/(n(n-d')) errors, where n is the block length and d' is the designed distance of the code. The improvement for the case of algebraic-geometry codes extends the methods of Shokrollahi and Wasserman (see in Proc. 29th Annu. ACM Symp. Theory of Computing, p.241-48, 1998) and improves upon their bound for every choice of n and d'. We also present some other consequences of our algorithm including a solution to a weighted curve-fitting problem, which may be of use in soft-decision decoding algorithms for Reed-Solomon codes.",
Hidden digital watermarks in images,"An image authentication technique by embedding digital ""watermarks"" into images is proposed. Watermarking is a technique for labeling digital pictures by hiding secret information into the images. Sophisticated watermark embedding is a potential method to discourage unauthorized copying or attest the origin of the images. In our approach, we embed the watermarks with visually recognizable patterns into the images by selectively modifying the middle-frequency parts of the image. Several variations of the proposed method are addressed. The experimental results show that the proposed technique successfully survives image processing operations, image cropping, and the Joint Photographic Experts Group (JPEG) lossy compression.","Watermarking,
Pattern recognition,
Transform coding,
Image coding,
Seals,
Image recognition,
Multimedia communication,
Laboratories,
Computer science,
Electronic mail"
A critique of software defect prediction models,"Many organizations want to predict the number of defects (faults) in software systems, before they are deployed, to gauge the likely delivered quality and maintenance effort. To help in this numerous software metrics and statistical models have been developed, with a correspondingly large literature. We provide a critical review of this literature and the state-of-the-art. Most of the wide range of prediction models use size and complexity metrics to predict defects. Others are based on testing data, the ""quality"" of the development process, or take a multivariate approach. The authors of the models have often made heroic contributions to a subject otherwise bereft of empirical studies. However, there are a number of serious theoretical and practical problems in many studies. The models are weak because of their inability to cope with the, as yet, unknown relationship between defects and failures. There are fundamental statistical and data quality problems that undermine model validity. More significantly many prediction models tend to model only part of the underlying problem and seriously misspecify it. To illustrate these points the Goldilock's Conjecture, that there is an optimum module size, is used to show the considerable problems inherent in current defect prediction approaches. Careful and considered analysis of past and new results shows that the conjecture lacks support and that some models are misleading. We recommend holistic models for software defect prediction, using Bayesian belief networks, as alternative approaches to the single-issue models used at present. We also argue for research into a theory of ""software decomposition"" in order to test hypotheses about defect introduction and help construct a better science of software engineering.","Predictive models,
Bayesian methods,
Software quality,
Computer Society,
System testing,
Process design,
Software maintenance,
Software systems,
Software metrics,
Software testing"
Distributed clustering for ad hoc networks,"A Distributed Clustering Algorithm (DCA) and a Distributed Mobility-Adaptive Clustering (DMAC) algorithm are presented that partition the nodes of a fully mobile network: (ad hoc network) into clusters, this giving the network a hierarchical organization. Nodes are grouped by following a new weight-based criterion that allows the choice of the nodes that coordinate the clustering process based on node mobility-rebated parameters. The DCA is suitable for clustering ""quasistatic"" ad hoc networks. It is easy to implement and its time complexity is proven to be bounded by a network parameter that depends on the topology of the network rather than on its size, i.e., the invariant number of the network nodes. The DMAC algorithm adapts to the changes in the network topology due to the mobility of the nodes, and it is thus suitable for any mobile environment. Both algorithms are executed at each node with the sole knowledge of the identity of the one hop neighbors, and induce on the network the same clustering structure.","Ad hoc networks,
Clustering algorithms,
Partitioning algorithms,
Routing,
Computer science,
Electronic mail,
Mobile computing,
Telecommunication network topology,
Network topology,
Wireless networks"
Building knowledge through families of experiments,"Experimentation in software engineering is necessary but difficult. One reason is that there are a large number of context variables and, so, creating a cohesive understanding of experimental results requires a mechanism for motivating studies and integrating results. It requires a community of researchers that can replicate studies, vary context variables, and build models that represent the common observations about the discipline. The paper discusses the experience of the authors, based upon a collection of experiments, in terms of a framework for organizing sets of related studies. With such a framework, experiments can be viewed as part of common families of studies, rather than being isolated events. Common families of studies can contribute to important and relevant hypotheses that may not be suggested by individual experiments. A framework also facilitates building knowledge in an incremental manner through the replication of experiments within families of studies. To support the framework, the paper discusses the experiences of the authors in carrying out empirical studies, with specific emphasis on persistent problems encountered in experimental design, threats to validity, criteria for evaluation, and execution of experiments in the domain of software engineering.","Software engineering,
Buildings,
Design for experiments,
Testing,
Computer science,
Mathematical model,
Computer Society,
Context modeling,
Organizing,
Software measurement"
MRI simulation-based evaluation of image-processing and classification methods,"With the increased interest in computer-aided image analysis methods, there is a greater need for objective methods of algorithm evaluation. Validation of in vivo MRI studies is complicated by a lack of reference data and the difficulty of constructing anatomically realistic physical phantoms. The authors present here an extensible MRI simulator that efficiently generates realistic three-dimensional (3-D) brain images using a hybrid Bloch equation and tissue template simulation that accounts for image contrast, partial volume, and noise. This allows image analysis methods to be evaluated with controlled degradations of image data.",
The Pareto archived evolution strategy: a new baseline algorithm for Pareto multiobjective optimisation,"Most popular evolutionary algorithms for multiobjective optimisation maintain a population of solutions from which individuals are selected for reproduction. In this paper, we introduce a simpler evolution scheme for multiobjective problems, called the Pareto archived evolution strategy (PAES). We argue that PAES may represent the simplest possible non-trivial algorithm capable of generating diverse solutions in the Pareto optimal set. The algorithm is identified as being a (1+1) evolution strategy, using local search from a population of one but using a reference archive of previously found solutions in order to identify the approximate dominance ranking of the current and candidate solution vectors. PAES is intended as a good baseline approach, against which more involved methods may be compared, and may also serve well in some real-world applications when local search seems superior to or competitive with population-based methods. The performance of the new algorithm is compared with that of a MOEA based on the niched Pareto GA on a real world application from the telecommunications field. In addition, we include results from experiments carried out on a suite of four test functions, to demonstrate the algorithm's general capability.","Pareto optimization,
Search methods,
Genetic algorithms,
Computer science,
Evolutionary computation,
Testing,
Simulated annealing,
Optimization methods,
Telecommunication computing,
Routing"
Detecting intrusions using system calls: alternative data models,"Intrusion detection systems rely on a wide variety of observable data to distinguish between legitimate and illegitimate activities. We study one such observable-sequences of system calls into the kernel of an operating system. Using system-call data sets generated by several different programs, we compare the ability of different data modeling methods to represent normal behavior accurately and to recognize intrusions. We compare the following methods: simple enumeration of observed sequences; comparison of relative frequencies of different sequences; a rule induction technique; and hidden Markov models (HMMs). We discuss the factors affecting the performance of each method and conclude that for this particular problem, weaker methods than HMMs are likely sufficient.","Data models,
Intrusion detection,
Hidden Markov models,
Monitoring,
Proposals,
Computer science,
Reactive power,
Security,
Packaging,
Distributed computing"
Randomized kinodynamic planning,"The paper presents a state-space perspective on the kinodynamic planning problem, and introduces a randomized path planning technique that computes collision-free kinodynamic trajectories for high degree-of-freedom problems. By using a state space formulation, the kinodynamic planning problem is treated as a 2n-dimensional nonholonomic planning problem, derived from an n-dimensional configuration space. The state space serves the same role as the configuration space for basic path planning. The bases for the approach is the construction of a tree that attempts to rapidly and uniformly explore the state space, offering benefits that are similar to those obtained by successful randomized planning methods, but applies to a much broader class of problems. Some preliminary results are discussed for an implementation that determines the kinodynamic trajectories for hovercrafts and satellites in cluttered environments resulting in state spaces of up to twelve dimensions.",
Policy-enabled handoffs across heterogeneous wireless networks,"""Access is the killer app"" is the vision of the Daedalus project at UC Berkeley. Being able to be connected seamlessly anytime anywhere to the best network still remains an unfulfilled goal. Often, even determining the ""best"" network is a challenging task because of the widespread deployment of overlapping wireless networks. We describe a policy-enabled handoff system that allows users to express policies on what is the ""best"" wireless system at any moment, and make tradeoffs among network characteristics and dynamics such as cost, performance and power consumption. We designed a performance reporting scheme estimating current network conditions, which serves as input to the policy specification. A primary goal of this work is to make it possible to balance the bandwidth load across networks with comparable performance. To avoid the problem of handoff instability, i.e., many mobile hosts making the same handoff decision at essentially the same time, we designed randomization into our mechanism. Given the current ""best"" network, our system determines whether the handoff is worthwhile based on the handoff overhead and potential network usage duration.",
Dynamic matching and scheduling of a class of independent tasks onto heterogeneous computing systems,"Dynamic mapping (matching and scheduling) heuristics for a class of independent tasks using heterogeneous distributed computing systems are studied. Two types of mapping heuristics are considered: on-line and batch mode heuristics. Three new heuristics, one for batch and two for on-line, are introduced as part of this research. Simulation studies are performed to compare these heuristics with some existing ones. In total, five on-line heuristics and three batch heuristics are examined. The on-line heuristics consider; to varying degrees and in different ways, task affinity for different machines and machine ready times. The batch heuristics consider these factors, as well as aging of tasks waiting to execute. The simulation results reveal that the choice of mapping heuristic depends on parameters such as: (a) the structure of the heterogeneity among tasks and machines, (b) the optimization requirements, and (c) the arrival rate of the tasks.","Processor scheduling,
Dynamic scheduling,
Computer science,
Aging,
Resource management,
Indium tin oxide,
Subcontracting"
Elastic model-based segmentation of 3-D neuroradiological data sets,"This paper presents a new technique for the automatic model-based segmentation of three-dimensional (3-D) objects from volumetric image data. The development closely follows the seminal work of Taylor and Cootes on active shape models, but is based on a hierarchical parametric object description rather than a point distribution model. The segmentation system includes both the building of statistical models and the automatic segmentation of new image data sets via a restricted elastic deformation of shape models. Geometric models are derived from a sample set of image data which have been segmented by experts. The surfaces of these binary objects are converted into parametric surface representations, which are normalized to get an invariant object-centered coordinate system. Surface representations are expanded into series of spherical harmonics which provide parametric descriptions of object shapes. It is shown that invariant object surface parametrization provides a good approximation to automatically determine object homology in terms of sets of corresponding sets of surface points. Gray-level information near object boundaries is represented by 1-D intensity profiles normal to the surface. Considering automatic segmentation of brain structures as their driving application, the authors' choice of coordinates for object alignment was the well-accepted stereotactic coordinate system. Major variation of object shapes around the mean shape, also referred to as shape eigenmodes, are calculated in shape parameter space rather than the feature space of point coordinates. Segmentation makes use of the object shape statistics by restricting possible elastic deformations into the range of the training shapes. The mean shapes are initialized in a new data set by specifying the landmarks of the stereotactic coordinate system. The model elastically deforms, driven by the displacement forces across the object's surface, which are generated by matching local intensity profiles. Elastical deformations are limited by setting bounds for the maximum variations in eigenmode space. The technique has been applied to automatically segment left and right hippocampus, thalamus, putamen, and globus pallidus from volumetric magnetic resonance scans taken from schizophrenia studies. The results have been validated by comparison of automatic segmentation with the results obtained by interactive expert segmentation.","Image segmentation,
Deformable models,
Magnetic resonance imaging,
Active shape model,
Psychiatry,
Biomedical imaging,
Medical diagnostic imaging,
Image analysis,
Ultrasonic imaging,
Application software"
Patterns in property specifications for finite-state verification,"Model checkers and other finite-state verification tools allow developers to detect certain kinds of errors automatically. Nevertheless, the transition of this technology from research to practice has been slow. While there are a number of potential causes for reluctance to adopt such formal methods, we believe that a primary cause is that practitioners are unfamiliar with specification processes, notations, and strategies. In a recent paper, we proposed a pattern-based approach to the presentation, codification and reuse of property specifications for finite-state verification. Since then, we have carried out a survey of available specifications, collecting over 500 examples of property specifications. We found that most are instances of our proposed patterns. Furthermore, we have updated our pattern system to accommodate new patterns and variations of existing patterns encountered in this survey. This paper reports the results of the survey and the current status of our pattern system.","Formal specifications,
Permission,
Logic,
Mathematics,
Statistics,
Computer science,
Computer errors,
NASA,
System testing,
Automation"
Clustering with a genetically optimized approach,"Describes a genetically guided approach to optimizing the hard (J/sub 1/) and fuzzy (J/sub m/) c-means functionals used in cluster analysis. Our experiments show that a genetic algorithm (GA) can ameliorate the difficulty of choosing an initialization for the c-means clustering algorithms. Experiments use six data sets, including the Iris data, magnetic resonance, and color images. The genetic algorithm approach is generally able to find the lowest known J/sub m/ value or a J/sub m/ associated with a partition very similar to that associated with the lowest J/sub m/ value. On data sets with several local extrema, the GA approach always avoids the less desirable solutions. Degenerate partitions are always avoided by the GA approach, which provides an effective method for optimizing clustering models whose objective function can be represented in terms of cluster centers. A series random initializations of fuzzy/hard c-means, where the partition associated with the lowest J/sub m/ value is chosen, can produce an equivalent solution to the genetic guided clustering approach given the same amount of processor time in some domains.","Clustering algorithms,
Optimization methods,
Genetic algorithms,
Computer science,
Fuzzy logic,
Magnetic analysis,
Partitioning algorithms,
Iris,
Magnetic resonance,
Color"
Using the OMNeT++ discrete event simulation system in education,"The intent of this paper is to contribute to the teaching of computer networks, parallel and distributed systems and discrete event simulation by presenting a simulation system that is ideally suited for educational use. OMNeT++ is a C++-based discrete event simulator which uses the process-interaction approach. An OMNeT++ model consists of modules communicating by message passing. Modules can be arbitrarily nested. Model topology is specified by a topology description language which supports separation of interface and functionality and facilitates model reuse. One of the strengths of OMNeT++ is that one can execute the simulation under a powerful graphical user interface. The GUI makes the internals of a simulation model fully visible to the person running the simulation: it displays the network graphics, animates the message flow and lets the user peek into objects and variables within the model. The use of the tracing/debugging capabilities does not require extra code to be written by the simulation programmer. The combination of these features make OMNeT++ a good choice for use in the education. OMNeT++ is open-source and free for non-profit use. The CD-ROM contains the full source distribution, the manual in HTML format, and a Win95/NT executable with several sample simulation models and their sources.",Computer science education
Matching hierarchical structures using association graphs,"It is well-known that the problem of matching two relational structures can be posed as an equivalent problem of finding a maximal clique in a (derived) ""association graph."" However, it is not clear how to apply this approach to computer vision problems where the graphs are hierarchically organized, i.e., are trees, since maximal cliques are not constrained to preserve the partial order. We provide a solution to the problem of matching two trees by constructing the association graph using the graph-theoretic concept of connectivity. We prove that, in the new formulation, there is a one-to-one correspondence between maximal cliques and maximal subtree isomorphisms. This allows us to cast the tree matching problem as an indefinite quadratic program using the Motzkin-Straus theorem, and we use ""replicator"" dynamical systems developed in theoretical biology to solve it. Such continuous solutions to discrete problems are attractive because they can motivate analog and biological implementations. The framework is also extended to the matching of attributed trees by using weighted association graphs. We illustrate the power of the approach by matching articulated and deformed shapes described by shock trees.","Tree graphs,
Computer vision,
Computer science,
Shape,
Electric shock,
Mathematics,
Machine intelligence,
Systems biology,
Topology"
Measurement and modelling of the temporal dependence in packet loss,"Understanding and modelling packet loss in the Internet is especially relevant for the design and analysis of delay-sensitive multimedia applications. We present analysis of 128 hours of end-to-end unicast and multicast packet loss measurement. From these we selected 76 hours of stationary traces for further analysis. We consider the dependence as seen in the autocorrelation function of the original loss data as well as the dependence between good run lengths and loss run lengths. The correlation timescale is found to be 1000 ms or less. We evaluate the accuracy of three models of increasing complexity: the Bernoulli model, the 2-state Markov chain model and the k-th order Markov chain model. Out of the 38 trace segments considered, the Bernoulli model was found to be accurate for 7 segments, and the 2-state model was found to be accurate for 10 segments. A Markov chain model of order 2 or greater was found to be necessary to accurately model the rest of the segments. For the case of adaptive applications which track loss, we address two issues of on-line loss estimation: the required memory size and whether to use exponential smoothing or a sliding window average to estimate average loss rate. We find that a large memory size is necessary and that the sliding window average provides a more accurate estimate for the same effective memory size.","Loss measurement,
Internet,
Delay,
Unicast,
Sampling methods,
Moon,
Computer science,
Application software,
Autocorrelation,
Smoothing methods"
Monotonic algorithms for transmission tomography,"Presents a framework for designing fast and monotonic algorithms for transmission tomography penalized-likelihood image reconstruction. The new algorithms are based on paraboloidal surrogate functions for the log likelihood, Due to the form of the log-likelihood function it is possible to find low curvature surrogate functions that guarantee monotonicity. Unlike previous methods, the proposed surrogate functions lead to monotonic algorithms even for the nonconvex log likelihood that arises due to background events, such as scatter and random coincidences. The gradient and the curvature of the likelihood terms are evaluated only once per iteration. Since the problem is simplified at each iteration, the CPU time is less than that of current algorithms which directly minimize the objective, yet the convergence rate is comparable. The simplicity, monotonicity, and speed of the new algorithms are quite attractive. The convergence rates of the algorithms are demonstrated using real and simulated PET transmission scans.",
Data sieving and collective I/O in ROMIO,"The I/O access patterns of parallel programs often consist of accesses to a large number of small, noncontiguous pieces of data. If an application's I/O needs are met by making many small, distinct I/O requests, however, the I/O performance degrades drastically. To avoid this problem, MPI-IO allows users to access a noncontiguous data set with a single I/O function call. This feature provides MPI-IO implementations an opportunity to optimize data access. We describe how our MPI-IO implementation, ROMIO, delivers high performance in the presence of noncontiguous requests. We explain in detail the two key optimizations ROMIO performs: data sieving for noncontiguous requests from one process and collective I/O for noncontiguous requests from multiple processes. We describe how one can implement these optimizations portably on multiple machines and file systems, control their memory requirements, and also achieve high performance. We demonstrate the performance and portability with performance results for three applications-an astrophysics-application template (DIST3D) the NAS BTIO benchmark, and an unstructured code (UNSTRUC)-on five different parallel machines: HP Exemplar IBM SP, Intel Paragon, NEC SX-4, and SGI Origin2000.","Identity-based encryption,
File systems,
Read only memory,
Mathematics,
Computer science,
Laboratories,
National electric code,
Control systems,
Parallel machines"
Particle swarm optimiser with neighbourhood operator,"In recent years population based methods such as genetic algorithms, evolutionary programming, evolution strategies and genetic programming have been increasingly employed to solve a variety of optimisation problems. Recently, another novel population based optimisation algorithm - namely the particle swarm optimisation (PSO) algorithm, was introduced by R. Eberhart and J. Kennedy (1995). Although the PSO algorithm possesses some attractive properties, its solution quality has been somewhat inferior to other evolutionary optimisation algorithms (P. Angeline, 1998). We propose a number of techniques to improve the standard PSO algorithm. Similar techniques have been employed in the context of self organising maps and neural-gas networks (T. Kohonen, 1990; T.M. Martinez et al., 1994).",
A realistic spline-based dynamic heart phantom,"We develop a realistic computerized heart phantom for use in medical imaging research. This phantom is a hybrid of realistic patient-based phantoms and flexible geometry-based phantoms. The surfaces of heart structures are defined using non-uniform rational B-splines (NURBS), as used in 3D computer graphics. The NURBS primitives define continuous surfaces allowing the phantom to be defined at any resolution. Also, by fitting NURBS to patient data, the phantom is more realistic than those based on solid geometry. An important innovation is the extension of NURBS to the fourth dimension, time, to model heart motion. Points on the surfaces of heart structures were selected from a gated MRI study of a normal patient. Polygon surfaces were fit to the points for each time frame, and smoothed. 3D NURBS surfaces were fit to the smooth polygon surfaces and then a 4D NURBS surface was fit through these surfaces. Each of the principal 4D surfaces (atria, ventricles, inner and outer walls) contains approximately 200 control points. We conclude that 4D NURBS are an efficient and flexible way to describe the heart and other anatomical objects for a realistic phantom.","Spline,
Heart,
Imaging phantoms,
Surface fitting,
Surface topography,
Surface reconstruction,
Biomedical imaging,
Computer graphics,
Solids,
Geometry"
The Gaussian sampling strategy for probabilistic roadmap planners,"Probabilistic roadmap planners (PRMs) form a relatively new technique for motion planning that has shown great potential. A critical aspect of PRM is the probabilistic strategy used to sample the free configuration space. In this paper we present a new, simple sampling strategy, which we call the Gaussian sampler, that gives a much better coverage of the difficult parts of the free configuration space. The approach uses only elementary operations which makes it suitable for many different planning problems. Experiments indicate that the technique is very efficient indeed.","Sampling methods,
Orbital robotics,
Mobile robots,
Motion planning,
Computer science,
Neural networks,
Genetic algorithms,
Books,
Path planning,
Layout"
High-speed navigation using the global dynamic window approach,"Many applications in mobile robotics require the safe execution of a collision-free motion to a goal position. Planning approaches are well suited for achieving a goal position in known static environments, while real-time obstacle avoidance methods allow reactive motion behavior in dynamic and unknown environments. This paper proposes the global dynamic window approach as a generalization of the dynamic window approach. It combines methods from motion planning and real-time obstacle avoidance to result in a framework that allows robust execution of high-velocity, goal-directed reactive motion for a mobile robot in unknown and dynamic environments. The global dynamic window approach is applicable to nonholonomic and holonomic mobile robots.","Navigation,
Mobile robots,
Motion planning,
Robot sensing systems,
Orbital robotics,
Path planning,
Kinematics,
Laboratories,
Computer science,
Application software"
Simultaneous maximum a posteriori reconstruction of attenuation and activity distributions from emission sinograms,"In order to perform attenuation correction in emission tomography an attenuation map is required. The authors propose a new method to compute this map directly from the emission sinogram, eliminating the transmission scan from the acquisition protocol. The problem is formulated as an optimization task where the objective function is a combination of the likelihood and an a priori probability. The latter uses a Gibbs prior distribution to encourage local smoothness and a multimodal distribution for the attenuation coefficients. Since the attenuation process is different in positron emission tomography (PET) and single photon emission tomography (SPECT), a separate algorithm for each case is derived. The method has been tested on mathematical phantoms and on a few clinical studies. For PET, good agreement was found between the images obtained with transmission measurements and those produced by the new algorithm in an abdominal study. For SPECT, promising simulation results have been obtained for nonhomogeneous attenuation due to the presence of the lungs.","Electromagnetic interference,
Positron emission tomography,
Image reconstruction,
Single photon emission computed tomography,
Attenuation measurement,
Image segmentation,
Protocols,
Testing,
Imaging phantoms,
Abdomen"
"On plane-based camera calibration: A general algorithm, singularities, applications","We present a general algorithm for plane-based calibration that can deal with arbitrary numbers of views and calibration planes. The algorithm can simultaneously calibrate different views from a camera with variable intrinsic parameters and it is easy to incorporate known values of intrinsic parameters. For some minimal cases, we describe all singularities, naming the parameters that can not be estimated. Experimental results of our method are shown that exhibit the singularities while revealing good performance in non-singular conditions. Several applications of plane-based 3D geometry inference are discussed as well.","Cameras,
Calibration,
Transmission line matrix methods,
Application software,
Equations,
Symmetric matrices,
Computer vision,
Computer science,
Geometrical optics,
Printers"
R/spl times/W: a scheduling approach for large-scale on-demand data broadcast,"Broadcast is becoming an increasingly attractive data-dissemination method for large client populations. In order to effectively utilize a broadcast medium for such a service, it is necessary to have efficient on-line scheduling algorithms that can balance individual and overall performance and can scale in terms of data set sizes, client populations, and broadcast bandwidth. We propose an algorithm, called R/spl times/W, that provides good performance across all of these criteria and can be tuned to trade off average and worst-case waiting time. Unlike previous work on low overhead scheduling, the algorithm does not use estimates of the access probabilities of items, but rather, it makes scheduling decisions based on the current queue state, allowing it to easily adapt to changes in the intensity and distribution of the workload. We demonstrate the performance advantages of the algorithm under a range of scenarios using a simulation model and present analytical results that describe the intrinsic behavior of the algorithm.","Large-scale systems,
Satellite broadcasting,
Bandwidth,
Scheduling algorithm,
Application software,
Computer science,
Analytical models,
Algorithm design and analysis,
Web server,
Network servers"
Fast approximate energy minimization via graph cuts,"In this paper we address the problem of minimizing a large class of energy functions that occur in early vision. The major restriction is that the energy function's smoothness term must only involve pairs of pixels. We propose two algorithms that use graph cuts to compute a local minimum even when very large moves are allowed. The first move we consider is an /spl alpha/-/spl beta/-swap: for a pair of labels /spl alpha/,/spl beta/, this move exchanges the labels between an arbitrary set of pixels labeled a and another arbitrary set labeled /spl beta/. Our first algorithm generates a labeling such that there is no swap move that decreases the energy. The second move we consider is an /spl alpha/-expansion: for a label a, this move assigns an arbitrary set of pixels the label /spl alpha/. Our second algorithm, which requires the smoothness term to be a metric, generates a labeling such that there is no expansion move that decreases the energy. Moreover, this solution is within a known factor of the global minimum. We experimentally demonstrate the effectiveness of our approach on image restoration, stereo and motion.",
"Evolutionary Algorithms, Homomorphous Mappings, and Constrained Parameter Optimization","During the last five years, several methods have been proposed for handling nonlinear constraints using evolutionary algorithms (EAs) for numerical optimization problems. Recent survey papers classify these methods into four categories: preservation of feasibility, penalty functions, searching for feasibility, and other hybrids. In this paper we investigate a new approach for solving constrained numerical optimization problems which incorporates a homomorphous mapping between n-dimensional cube and a feasible search space. This approach constitutes an example of the fifth decoder-based category of constraint handling techniques. We demonstrate the power of this new approach on several test cases and discuss its further potential.","homomorphous mapping,
Evolutionary computation,
optimization technique,
nonlinear programming,
constrained optimization,
decoder"
A multimodal registration algorithm of eye fundus images using vessels detection and Hough transform,"Image registration is a real challenge because physicians handle many images. Temporal registration is necessary in order to follow the various steps of a disease, whereas multimodal registration allows us to improve the identification of some lesions or to compare pieces of information gathered from different sources. This paper presents an algorithm for temporal and/or multimodal registration of retinal images based on point correspondence. As an example, the algorithm has been applied to the registration of fluorescein images (obtained after a fluorescein dye injection) with green images (green filter of a color image). The vascular tree is first detected in each type of images and bifurcation points are labeled with surrounding vessel orientations. An angle-based invariant is then computed in order to give a probability for two points to match. Then a Bayesian Hough transform is used to sort the transformations with their respective likelihoods. A precise affine estimate is finally computed for most likely transformations. The best transformation is chosen for registration.","Lesions,
Retina,
Biomedical imaging,
Filters,
Diabetes,
Image registration,
Diseases,
Medical diagnostic imaging,
Retinopathy,
Algorithm design and analysis"
Cache-oblivious algorithms,"This paper presents asymptotically optimal algorithms for rectangular matrix transpose, FFT, and sorting on computers with multiple levels of caching. Unlike previous optimal algorithms, these algorithms are cache oblivious: no variables dependent on hardware parameters, such as cache size and cache-line length, need to be tuned to achieve optimality. Nevertheless, these algorithms use an optimal amount of work and move data optimally among multiple levels of cache. For a cache with size Z and cache-line length L where Z=/spl Omega/(L/sup 2/) the number of cache misses for an m/spl times/n matrix transpose is /spl Theta/(1+mn/L). The number of cache misses for either an n-point FFT or the sorting of n numbers is /spl Theta/(1+(n/L)(1+log/sub Z/n)). We also give an /spl Theta/(mnp)-work algorithm to multiply an m/spl times/n matrix by an n/spl times/p matrix that incurs /spl Theta/(1+(mn+np+mp)/L+mnp/L/spl radic/Z) cache faults. We introduce an ""ideal-cache"" model to analyze our algorithms. We prove that an optimal cache-oblivious algorithm designed for two levels of memory is also optimal for multiple levels and that the assumption of optimal replacement in the ideal-cache model. Can be simulated efficiently by LRU replacement. We also provide preliminary empirical results on the effectiveness of cache-oblivious algorithms in practice.",
Segmentation and measurement of the cortex from 3-D MR images using coupled-surfaces propagation,"The cortex is the outermost thin layer of gray matter in the brain; geometric measurement of the cortex helps in understanding brain anatomy and function. In the quantitative analysis of the cortex from MR images, extracting the structure and obtaining a representation for various measurements are key steps. While manual segmentation is tedious and labor intensive, automatic reliable efficient segmentation and measurement of the cortex remain challenging problems, due to its convoluted nature. Here, the authors' present a new approach of coupled-surfaces propagation, using level set methods to address such problems. Their method is motivated by the nearly constant thickness of the cortical mantle and takes this tight coupling as an important constraint. By evolving two embedded surfaces simultaneously, each driven by its own image-derived information while maintaining the coupling, a final representation of the cortical bounding surfaces and an automatic segmentation of the cortex are achieved. Characteristics of the cortex, such as cortical surface area, surface curvature, and cortical thickness, are then evaluated. The level set implementation of surface propagation offers the advantage of easy initialization, computational efficiency, and the ability to capture deep sulcal folds. Results and validation from various experiments on both simulated and real three dimensional (3-D) MR images are provided.","Image segmentation,
Level set,
Cerebral cortex,
Computed tomography,
Anatomy,
Image analysis,
Computational efficiency,
Brain modeling,
Computational modeling,
Magnetic resonance imaging"
A data mining framework for building intrusion detection models,"There is often the need to update an installed intrusion detection system (IDS) due to new attack methods or upgraded computing environments. Since many current IDSs are constructed by manual encoding of expert knowledge, changes to IDSs are expensive and slow. We describe a data mining framework for adaptively building Intrusion Detection (ID) models. The central idea is to utilize auditing programs to extract an extensive set of features that describe each network connection or host session, and apply data mining programs to learn rules that accurately capture the behavior of intrusions and normal activities. These rules can then be used for misuse detection and anomaly detection. New detection models are incorporated into an existing IDS through a meta-learning (or co-operative learning) process, which produces a meta detection model that combines evidence from multiple models. We discuss the strengths of our data mining programs, namely, classification, meta-learning, association rules, and frequent episodes. We report on the results of applying these programs to the extensively gathered network audit data for the 1998 DARPA Intrusion Detection Evaluation Program.","Data mining,
Intrusion detection,
Identity-based encryption,
Computer networks,
Protection,
Data security,
Computer science,
Encoding,
Buildings,
Gas detectors"
Simultaneous training of negatively correlated neural networks in an ensemble,"This paper presents a new cooperative ensemble learning system (CELS) for designing neural network ensembles. The idea behind CELS is to encourage different individual networks in an ensemble to learn different parts or aspects of a training data so that the ensemble can learn the whole training data better. In CELS, the individual networks are trained simultaneously rather than independently or sequentially. This provides an opportunity for the individual networks to interact with each other and to specialize. CELS can create negatively correlated neural networks using a correlation penalty term in the error function to encourage such specialization. This paper analyzes CELS in terms of bias-variance-covariance tradeoff. CELS has also been tested on the Mackey-Glass time series prediction problem and the Australian credit card assessment problem. The experimental results show that CELS can produce neural network ensembles with good generalization ability.","Neural networks,
Intelligent networks,
Training data,
Learning systems,
Laboratories,
Computer science,
Decorrelation,
Testing,
Australia,
Credit cards"
Semantic constraints for membership function optimization,"The optimization of fuzzy systems using bio-inspired strategies, such as neural network learning rules or evolutionary optimization techniques, is becoming more and more popular. In general, fuzzy systems optimized in such a way cannot provide a linguistic interpretation, preventing us from using one of their most interesting and useful features. This paper addresses this difficulty and points out a set of constraints that when used within an optimization scheme obviate the subjective task of interpreting membership functions. To achieve this a comprehensive set of semantic properties that membership functions should have is postulated and discussed. These properties are translated in terms of nonlinear constraints that are coded within a given optimization scheme, such as backpropagation. Implementation issues and one example illustrating the importance of the proposed constraints are included.","Constraint optimization,
Fuzzy systems,
Artificial neural networks,
Neural networks,
Fuzzy sets,
Fuzzy logic,
Genetic algorithms,
Adaptive systems,
Mathematics,
Computer science"
Symbolic model checking using SAT procedures instead of BDDs,"In this paper, we study the application of propositional decision procedures in hardware verification. In particular, we apply bounded model checking to equivalence and invariant checking. We present several optimizations that reduce the size of generated propositional formulas. In many instances, our SAT-based approach can significantly outperform BDD-based approaches. We observe that SAT-based techniques are particularly efficient in detecting errors in both combinational and sequential designs.","Data structures,
Boolean functions,
Automata,
Hardware,
Formal verification,
Permission,
Computer science,
Design automation,
Laboratories,
Logic design"
Resampling of data between arbitrary grids using convolution interpolation,"For certain medical applications resampling of data is required. In magnetic resonance tomography (MRT) or computer tomography (CT), e.g., data may be sampled on nonrectilinear grids in the Fourier domain. For the image reconstruction a convolution-interpolation algorithm, often called gridding, can be applied for resampling of the data onto a rectilinear grid. Resampling of data from a rectilinear onto a nonrectilinear grid are needed, e.g., if projections of a given rectilinear data set are to be obtained. In this paper the authors introduce the application of the convolution interpolation for resampling of data from one arbitrary grid onto another. The basic algorithm can be split into two steps. First, the data are resampled from the arbitrary input grid onto a rectilinear grid and second, the rectilinear data is resampled onto the arbitrary output grid. Furthermore, the authors like to introduce a new technique to derive the sampling density function needed for the first step of their algorithm. For fast, sampling-pattern-independent determination of the sampling density function the Voronoi diagram of the sample distribution is calculated. The volume of the Voronoi cell around each sample is used as a measure for the sampling density. It is shown that the introduced resampling technique allows fast resampling of data between arbitrary grids. Furthermore, it is shown that the suggested approach to derive the sampling density function is suitable even for arbitrary sampling patterns. Examples are given in which the proposed technique has been applied for the reconstruction of data acquired along spiral, radial, and arbitrary trajectories and for the fast calculation of projections of a given rectilinearly sampled image.","Convolution,
Interpolation,
Sampling methods,
Density functional theory,
Tomography,
Image reconstruction,
Image sampling,
Medical services,
Biomedical equipment,
Magnetic resonance"
Gradient-based iterative image reconstruction scheme for time-resolved optical tomography,"Currently available tomographic image reconstruction schemes for optical tomography (OT) are mostly based on the limiting assumptions of small perturbations and a priori knowledge of the optical properties of a reference medium. Furthermore, these algorithms usually require the inversion of large, full, ill-conditioned Jacobian matrixes. In this work a gradient-based iterative image reconstruction (GIIR) method is presented that promises to overcome current limitations. The code consists of three major parts: (1) A finite-difference, time-resolved, diffusion forward model is used to predict detector readings based on the spatial distribution of optical properties; (2) An objective function that describes the difference between predicted and measured data; (3) An updating method that uses the gradient of the objective function in a line minimization scheme to provide subsequent guesses of the spatial distribution of the optical properties for the forward model. The reconstruction of these properties is completed, once a minimum of this objective function is found. After a presentation of the mathematical background, two- and three-dimensional reconstruction of simple heterogeneous media as well as the clinically relevant example of ventricular bleeding in the brain are discussed. Numerical studies suggest that intraventricular hemorrhages can be detected using the GIIR technique, even in the presence of a heterogeneous background.","Image reconstruction,
Tomography,
Predictive models,
Hemorrhaging,
Iterative algorithms,
Jacobian matrices,
Iterative methods,
Finite difference methods,
Minimization methods,
Nonhomogeneous media"
On-demand multicast routing protocol,"This paper presents a novel multicast routing protocol for mobile ad hoc wireless networks. The protocol, termed ODMRP (on-demand multicast routing protocol), is a mesh-based, rather than a conventional tree-based multicast scheme and uses a forwarding group concept (only a subset of nodes forwards the multicast packets via scoped flooding). It applies on-demand procedures to dynamically build routes and maintain multicast group membership. ODMRP is well suited for ad hoc wireless networks with mobile hosts where bandwidth is limited, topology changes frequently, and power is constrained. We evaluate ODMRP's scalability and performance via simulation.","Multicast protocols,
Routing protocols,
Wireless mesh networks,
Bandwidth,
Network topology,
Scalability,
Broadcasting,
Laboratories,
Computer science,
Internet"
Geocasting in mobile ad hoc networks: location-based multicast algorithms,"The paper addresses the problem of geocasting in mobile ad hoc network (MANET) environments. Geocasting is a variant of the conventional multicasting problem. For multicasting, conventional protocols define a multicast group as a collection of hosts which register to a multicast group address. However for geocasting, the group consists of the set of all nodes within a specified geographical region. Hosts within the specified region at a given time form the geocast group at that time. We present two different algorithms for delivering packets to such a group, and present simulation results.","Intelligent networks,
Mobile ad hoc networks,
Multicast algorithms,
Electronic switching systems,
Costs,
Microwave integrated circuits,
Computer science,
Reactive power,
Protocols,
Liver"
Segmentation of digitized dermatoscopic images by two-dimensional color clustering,"A color-based segmentation scheme applied to dermatoscopic images is proposed. The RGB image is processed in the L*u*v* color space. A 2D histogram is computed with the two principal components and then smoothed with a Gaussian low-pass filter. The maxima location and a set of features are computed from the histogram contour lines. These features are the number of enclosed pixels, the surface of the base and the height of the maximum. They allow for the selection of valid clusters which determine the number of classes. The image is then segmented using a modified version of the fuzzy c-means (FCM) clustering technique that takes into account the cluster orientation. Finally, the segmented image is cleaned using mathematical morphology, the region borders are smoothed and small components are removed.","Image segmentation,
Lesions,
Color,
Pigmentation,
Skin cancer,
Histograms,
Malignant tumors,
Rendering (computer graphics),
Signal processing algorithms,
Data mining"
Estimation and removal of clock skew from network delay measurements,"Packet delay and loss traces are frequently used by network engineers, as well as network applications, to analyze network performance. The clocks on the end-systems used to measure the delays, however, are not always synchronized, and this lack of synchronization reduces the accuracy of these measurements. Therefore, estimating and removing relative skews and offsets from delay measurements between sender and receiver clocks are critical to the accurate assessment and analysis of network performance. We introduce a linear programming-based algorithm to estimate the clock skew in network delay measurements and compare it with three other algorithms. We show that our algorithm has a time complexity of O(N), leaves the delay after the skew removal positive, and is robust in the sense that the error margin of the skew estimate is independent of the magnitude of the skew. We use traces of real Internet delay measurements to assess the algorithm, and compare its performance to that of three other algorithms. Furthermore, we show through simulation that our algorithm is unbiased, and that the sample variance of the skew estimate is better (smaller) than existing algorithms.","Delay estimation,
Clocks,
Delay effects,
Internet,
Frequency synchronization,
Frequency measurement,
Loss measurement,
Computer science,
Application software,
Performance analysis"
Particle swarm optimization: surfing the waves,"A new optimization method has been proposed by J. Kennedy and R.C. Eberhart (1997; 1995), called Particle Swarm Optimization (PSO). This approach combines social psychology principles and evolutionary computation. It has been applied successfully to nonlinear function optimization and neural network training. Preliminary formal analyses showed that a particle in a simple one-dimensional PSO system follows a path defined by a sinusoidal wave, randomly deciding on both its amplitude and frequency (Y. Shi and R. Eberhart, 1998). The paper takes the next step, generalizing to obtain closed form equations for trajectories of particles in a multi-dimensional search space.","Particle swarm optimization,
Equations,
Frequency,
Genetic algorithms,
Computer science,
Optimization methods,
Psychology,
Organisms,
Marine animals,
Educational institutions"
Dynamic resource allocation schemes during handoff for mobile multimedia wireless networks,"User mobility management is one of the important components of mobile multimedia systems. In a cell-based network, a mobile should be able to seamlessly obtain transmission resources after handoff to a new base station. This is essential for both service continuity and quality of service assurance. In this paper, we present strategies for accommodating continuous service to mobile users through estimating resource requirements of potential handoff connections. A diverse mix of heterogeneous traffic with diverse resource requirements is considered. The investigate static and dynamic resource allocation schemes. The dynamic scheme probabilistically estimates the potential number of connections that will be handed off from neighboring cells, for each class of traffic. The performance of these strategies in terms of connection blocking probabilities for handoff and local new connection requests are evaluated. The performance is also compared to a scheme previously proposed by Yu and Leung (see IEEE J. Select. Areas Commun., vol.15, p.1208-25, 1997). The results indicate that using dynamic estimation and allocation, we can significantly reduce the dropping probability for handoff connections.","Resource management,
Wireless networks,
Base stations,
Quality of service,
Telecommunication traffic,
Multimedia systems,
Admission control,
Mobile radio mobility management,
Cellular networks,
Computer science"
A novel multiscale nonlinear thresholding method for ultrasonic speckle suppressing,"This paper presents a novel speckle suppression method for medical B-scan ultrasonic images. An original image is first separated into two parts with an adaptive filter. These two parts are then transformed into a multiscale wavelet domain and the wavelet coefficients are processed by a soft thresholding method, which is a variation of Donoho's (1995) soft thresholding method. The processed coefficients for each part are then transformed back into the space domain. Finally, the denoised image is obtained as the sum of the two processed parts. A computer-simulated image and an in vitro B-scan image of a pig heart have been used to test the performance of this new method. This technique effectively reduces the speckle noise, while preserving the resolvable details. It performs well in comparison to the multiscale thresholding technique without adaptive preprocessing and two other speckle-suppression methods.","Speckle,
Biomedical imaging,
Adaptive filters,
Wavelet domain,
Wavelet coefficients,
Back,
In vitro,
Heart,
Testing,
Noise reduction"
Accessing multiple mirror sites in parallel: using Tornado codes to speed up downloads,"Mirror sites enable client requests to be serviced by any of a number of servers, reducing load at individual servers and dispersing network load. Typically, a client requests service from a single mirror site. We consider enabling a client to access a file from multiple mirror sites in parallel to speed up the download. To eliminate complex client-server negotiations that a straightforward implementation of this approach would require, we develop a feedback-free protocol based on erasure codes. We demonstrate that a protocol using fast Tornado codes can deliver dramatic speedups at the expense of transmitting a moderate number of additional packets into the network. This scalable solution extends naturally to allow multiple clients to access data from multiple mirror sites simultaneously. The approach applies naturally to wireless networks and satellite networks as well.","Mirrors,
Tornadoes,
Network servers,
Computer science,
Access protocols,
Wireless networks,
Satellites,
File servers,
Web server,
Internet"
Reconstruction of the human cerebral cortex from magnetic resonance images,"Reconstructing the geometry of the human cerebral cortex from MR images is an important step in both brain mapping and surgical path planning applications. Difficulties with imaging noise, partial volume averaging, image intensity inhomogeneities, convoluted cortical structures, and the requirement to preserve anatomical topology make the development of accurate automated algorithms particularly challenging. Here the authors address each of these problems and describe a systematic method for obtaining a surface representation of the geometric central layer of the human cerebral cortex. Using fuzzy segmentation, an isosurface algorithm, and a deformable surface model, the method reconstructs the entire cortex with the correct topology, including deep convoluted sulci and gyri. The method is largely automated and its results are robust to imaging noise, partial volume averaging, and image intensity inhomogeneities. The performance of this method is demonstrated, both qualitatively and quantitatively, and the results of its application to six subjects and one simulated MR brain volume are presented.","Image reconstruction,
Humans,
Cerebral cortex,
Magnetic resonance,
Topology,
Surface reconstruction,
Brain modeling,
Geometry,
Brain mapping,
Surgery"
A Unifying Review of Linear Gaussian Models,"Factor analysis, principal component analysis, mixtures of gaussian clusters, vector quantization, Kalman filter models, and hidden Markov models can all be unified as variations of unsupervised learning under a single basic generative model. This is achieved by collecting together disparate observations and derivations made by many previous authors and introducing a new way of linking discrete and continuous state models using a simple nonlinearity. Through the use of other nonlinearities, we show how independent component analysis is also a variation of the same basic generative model. We show that factor analysis and mixtures of gaussians can be implemented in autoencoder neural networks and learned using squared error plus the same regularization term. We introduce a new model for static data, known as sensible principal component analysis, as well as a novel concept of spatially adaptive observation noise. We also review some of the literature involving global and local mixtures of the basic models and provide pseudocode for inference and learning for all the basic models.",
Modeling the global Internet,"A new scalable modeling framework and scalable parallel simulations make it possible to analyze the detailed behaviour of large, multidomain multiprotocol Internet models. The article focuses on simulation research. It describes the software designs that let us construct and run appropriately large models. After several years of research, we have developed a scalable network modeling framework, a scalable simulation framework (SSF), and scalable parallel discrete event simulators capable of modeling the Internet at unprecedented scales.","Internet,
Telecommunication traffic,
Traffic control,
Routing,
Computational modeling,
IP networks,
Network topology,
Peer to peer computing,
Intelligent networks,
Computer networks"
Emotionally expressive agents,"The ability to express emotions is important for creating believable interactive characters. To simulate emotional expressions in an interactive environment, an intelligent agent needs both an adaptive model for generating believable responses, and a visualization model for mapping emotions into facial expressions. Recent advances in intelligent agents and in facial modeling have produced effective algorithms for these tasks independently. We describe a method for integrating these algorithms to create an interactive simulation of an agent that produces appropriate facial expressions in a dynamic environment. Our approach to combining a model of emotions with a facial model represents a first step towards developing the technology of a truly believable interactive agent which has a wide range of applications from designing intelligent training systems to video games and animation tools.","Computational modeling,
Application software,
Computer interfaces,
Computer science,
Visualization,
Laboratories,
Identity-based encryption,
Animation,
Interactive systems,
Humans"
Bayesian multi-camera surveillance,"The task of multicamera surveillance is to reconstruct the paths taken by all moving objects that are temporally visible from multiple non-overlapping cameras. We present a Bayesian formalization of this task, where the optimal solution is the set of object paths with the highest posterior probability given the observed data. We show how to efficiently approximate the maximum a posteriori solution by linear programming and present initial experimental results.","Bayesian methods,
Surveillance,
Monitoring,
Cameras,
Computer science,
Road transportation,
Topology,
Streaming media,
Traffic control,
Statistics"
Measuring bandwidth,"Accurate network bandwidth measurement is important to a variety of network applications. Unfortunately, accurate bandwidth measurement is difficult. We describe some current bandwidth measurement techniques: using throughput, and packet pair. We explain some of the problems with these techniques, including poor accuracy, poor scalability, lack of statistical robustness, poor agility in adapting to bandwidth changes, lack of flexibility in deployment, and inaccuracy when used on a variety of traffic types. The authors solutions to these problems include the use of a packet window to adapt quickly to bandwidth changes, receiver only packet pair to combine accuracy and ease of deployment, and potential bandwidth filtering to increase the accuracy. These techniques are at least as accurate as previously used filtering algorithms, and in some situations, they are more than 37% more accurate.","Bandwidth,
Robustness,
Measurement techniques,
Filtering algorithms,
Scalability,
Web server,
Computer science,
Application software,
Throughput,
Information filtering"
A model-based method for the reconstruction of total knee replacement kinematics,"A better knowledge of the kinematics behavior of total knee replacement (TKR) during activity still remains a crucial issue to validate innovative prosthesis designs and different surgical strategies. Tools for more accurate measurement of in vivo kinematics of knee prosthesis components are therefore fundamental to improve the clinical outcome of knee replacement. In the present study, a novel model-based method for the estimation of the three-dimensional (3-D) position and orientation (pose) of both the femoral and tibial knee prosthesis components during activity is presented. The knowledge of the 3-D geometry of the components and a single plane projection view in a fluoroscopic image are sufficient to reconstruct the absolute and relative pose of the components in space. The technique is based on the best alignment of the component designs with the corresponding projection on the image plane. The image generation process is modeled and an iterative procedure localizes the spatial pose of the object by minimizing the Euclidean distance of the projection rays from the object surface. Computer simulation and static/dynamic in vitro tests using real knee prosthesis show that the accuracy with which relative orientation and position of the components can be estimated is better than 1.5/spl deg/ and 1.5 mm, respectively. In vivo tests demonstrate that the method is well suited for kinematics analysis on TKR patients and that good quality images can be obtained with a carefully positioning of the fluoroscope and an appropriate dosage. With respect to previously adopted template matching techniques, the present method overcomes the complete segmentation of the components on the projected image and also features the simultaneous evaluation of all the six degrees of freedom (DOF) of the object. The expected small difference between successive poses in in vivo sequences strongly reduces the frequency of false poses and both the operator and computation time.","Knee,
Kinematics,
Prosthetics,
In vivo,
Image reconstruction,
Testing,
Surgery,
Geometry,
Image generation,
Euclidean distance"
Way-predicting set-associative cache for high performance and low energy consumption,"This paper proposes a new approach using way prediction for achieving high performance and low energy consumption of set-associative caches. By accessing only a single cache way predicted, instead of accessing all the ways in a set, the energy consumption can be reduced. This paper shows that the way-predicting set-associative cache improves the ED (energy-delay) product by 60-70% compared to a conventional set-associative cache,.","Energy consumption,
Computer science,
Power engineering and energy,
Random access memory,
Proposals,
Filters,
Delay effects,
Degradation,
Hardware,
Permission"
Using abuse case models for security requirements analysis,"The relationships between the work products of a security engineering process can be hard to understand, even for persons with a strong technical background but little knowledge of security engineering. Market forces are driving software practitioners who are not security specialists to develop software that requires security features. When these practitioners develop software solutions without appropriate security-specific processes and models, they sometimes fail to produce effective solutions. We have adapted a proven object oriented modeling technique, use cases, to capture and analyze security requirements in a simple way. We call the adaptation an abuse case model. Its relationship to other security engineering work products is relatively simple, from a user perspective.","Computer aided software engineering,
Object oriented modeling,
Information security,
Knowledge engineering,
Mathematical model,
Computer security,
Displays,
Computer science,
Telephony,
File systems"
Registration of stereo and temporal images of the retina,"The registration of retinal images is required to facilitate the study of the optic nerve head and the retina. The method the authors propose combines the use of mutual information as the similarity measure and simulated annealing as the search technique. It is robust toward large transformations between the images and significant changes in light intensity. By using a pyramid sampling approach combined with simulated reannealing the authors find that registration can be achieved to predetermined precision, subject to choice of interpolation and the constraint of time. The algorithm was tested on 49 pairs of stereo images and 48 pairs of temporal images with success.",
Unconditionally secure key agreement and the intrinsic conditional information,"This paper is concerned with secret-key agreement by public discussion. Assume that two parties Alice and Bob and an adversary Eve have access to independent realizations of random variables X, Y, and Z, respectively, with joint distribution P/sub XYZ/. The secret-key rate S(X;Y/spl par/Z) has been defined as the maximal rate at which Alice and Bob can generate a secret key by communication over an insecure, but authenticated channel such that Eve's information about this key is arbitrarily small. We define a new conditional mutual information measure, the intrinsic conditional mutual information between S and Y when given Z, denoted by I(X;Y/spl darr/Z), which is an upper bound on S(X;Y/spl par/Z). The special scenarios are analyzed where X, Y, and Z are generated by sending a binary random variable R, for example a signal broadcast by a satellite, over independent channels, or two scenarios in which Z is generated by sending X and Y over erasure channels. In the first two scenarios it can be shown that the secret-key rate is strictly positive if and only if I(X;Y/spl darr/Z) is strictly positive. For the third scenario, a new protocol is presented which allows secret-key agreement even when all the previously known protocols fail.","Cryptography,
Communication channels,
Random variables,
Mutual information,
Signal generators,
Satellite broadcasting,
Cryptographic protocols,
Upper bound,
Signal analysis,
Computer science"
Rethinking media richness: towards a theory of media synchronicity,"The paper describes a new theory called a theory of media synchronicity which proposes that a set of five media capabilities are important to group work, and that all tasks are composed of two fundamental communication processes (conveyance and convergence). Communication effectiveness is influenced by matching the media capabilities to the needs of the fundamental communication processes, not aggregate collections of these processes (i.e., tasks) as proposed by media richness theory. The theory also proposes that the relationships between communication processes and media capabilities will vary between established and newly formed groups, and will change over time.","Convergence,
Identity-based encryption,
Aggregates,
Hip,
Uncertainty,
Testing,
Computer mediated communication,
Natural languages"
Bunch: a clustering tool for the recovery and maintenance of software system structures,"Software systems are typically modified in order to extend or change their functionality, improve their performance, port them to different platforms, and so on. For developers, it is crucial to understand the structure of a system before attempting to modify it. The structure of a system, however, may not be apparent to new developers, because the design documentation is non-existent or, worse, inconsistent with the implementation. This problem could be alleviated if developers were somehow able to produce high-level system decomposition descriptions from the low-level structures present in the source code. We have developed a clustering tool called Bunch that creates a system decomposition automatically by treating clustering as an optimization problem. The paper describes the extensions made to Bunch in response to feedback we received from users. The most important extension, in terms of the quality of results and execution efficiency, is a feature that enables the integration of designer knowledge about the system structure into an otherwise fully automatic clustering process. We use a case study to show how our new features simplified the task of extracting the subsystem structure of a medium size program, while exposing an interesting design flaw in the process.","Software maintenance,
Software systems,
Documentation,
Application software,
Cognitive science,
Mathematics,
Computer science,
Electronic mail,
Feedback,
Reverse engineering"
On the Laplace-Beltrami operator and brain surface flattening,"In this paper, using certain conformal mappings from uniformization theory, the authors give an explicit method for flattening the brain surface in a way which preserves angles. From a triangulated surface representation of the cortex, the authors indicate how the procedure may be implemented using finite elements. Further, they show how the geometry of the brain surface may be studied using this approach.","Brain,
Surface fitting,
Conformal mapping,
Magnetic resonance imaging,
Geometry,
Data visualization,
Topology,
Finite element methods,
Image segmentation,
Mathematics"
A theory of shape by space carving,"In this paper we consider the problem of computing the 3D shape of an unknown, arbitrarily-shaped scene from multiple photographs taken at known but arbitrarily-distributed viewpoints. By studying the equivalence class of all 3D shapes that reproduce the input photographs, we prove the existence of a special member of this class, the photo hull, that (1) can be computed directly from photographs of the scene, and (2) subsumes all other members of this class. We then give a provably-correct algorithm called Space Carving, for computing this shape and present experimental results on complex real-world scenes. The approach is designed to (1) build photorealistic shapes that accurately model scene appearance from a wide range of viewpoints, and (2) account for the complex interactions between occlusion, parallax, shading, and their effects on arbitrary views of a 3D scene.","Shape,
Layout,
Cameras,
Read only memory,
Electrical capacitance tomography,
Computer science,
Orbital robotics,
Computer vision,
Stereo vision,
Face detection"
Blind channel approximation: effective channel order determination,"A common assumption of blind channel identification methods is that the order of the true channel is known. This information is not available in practice, and we are obliged to estimate the channel order by applying a rank detection procedure to an ""overmodeled"" data covariance matrix. Information theoretic criteria have been widely suggested approaches for this task. We check the quality of their estimates in the context of order estimation of measured microwave radio channels and confirm that they are very sensitive to variations in the SNR and the number of data samples. This fact has prohibited their successful application for channel order estimation and hits created some confusion concerning the classification into under- and over-modeled cases. Recently, it has been shown that blind channel approximation methods should attempt to model only the significant part of the channel composed of the ""large"" impulse response terms because efforts toward modeling ""small"" leading and/or trailing terms lead to effective overmodeling, which is generically ill-conditioned and, thus, should be avoided. This can be achieved by applying blind identification methods with model order equal to the order of the significant part of the true channel called the effective channel order. Toward developing an efficient approach for the detection of the effective channel order, we use numerical analysis arguments. The derived criterion provides a ""maximally stable"" decomposition of the range space of an ""overmodeled"" data covariance matrix into signal and noise subspaces. It is shown to be robust to variations in the SNR and the number of data samples. Furthermore, it provides useful effective channel order estimates, leading to sufficiently good blind approximation/equalization of measured real-world microwave radio channels.",
Empirical evaluation of dissimilarity measures for color and texture,"This paper empirically compares nine image dissimilarity measures that are based on distributions of color and texture features summarizing over 1,000 CPU hours of computational experiments. Ground truth is collected via a novel random sampling scheme for color and via an image partitioning method for texture. Quantitative performance evaluations are given for classification, image retrieval, and segmentation tasks, and for a wide variety of dissimilarity measures. It is demonstrated how the selection of a measure, based on large scale evaluation, substantially improves the quality of classification, retrieval, and unsupervised segmentation of color and texture images.","Image segmentation,
Image retrieval,
Computer science,
Image sampling,
Reactive power,
Q measurement,
Computer vision,
Shape,
Electrical capacitance tomography,
Image recognition"
Automated extraction and variability analysis of sulcal neuroanatomy,"Systematic mapping of the variability in cortical sulcal anatomy is an area of increasing interest which presents numerous methodological challenges. To address these issues, the authors have implemented sulcal extraction and assisted labeling (SEAL) to automatically extract the two-dimensional (2-D) surface ribbons that represent the median axis of cerebral sulci and to neuroanatomically label these entities. To encode the extracted three-dimensional (3-D) cortical sulcal schematic topography (CSST) the authors define a relational graph structure composed of two main features: vertices (representing sulci) and arcs (representing the relationships between sulci). Vertices contain a parametric representation of the surface ribbon buried within the sulcus. Points on this surface are expressed in stereotaxic coordinates (i.e., with respect to a standardized brain coordinate system). For each of these vertices, the authors store length, depth, and orientation as well as anatomical attributes (e.g., hemisphere, lobe, sulcus type, etc.). Each are stores the 3-D location of the junction between sulci as well as a list of its connecting sulci. Sulcal labeling is performed semiautomatically by selecting a sulcal entity in the CSST and selecting from a menu of candidate sulcus names. In order to help the user in the labeling task, the menu is restricted to the most likely candidates by using priors for the expected sulcal spatial distribution. These priors, i.e., sulcal probabilistic maps, were created from the spatial distribution of 34 sulci traced manually on 36 different subjects. Given these spatial probability maps, the user is provided with the likelihood that the selected entity belongs to a particular sulcus. The cortical structure representation obtained by SEAL is suitable to extract statistical information about both the spatial and the structural composition of the cerebral cortical topography. This methodology allows for the iterative construction of a successively more complete statistical models of the cerebral topography containing spatial distributions of the most important structures, their morphometrics, and their structural components.","Surface topography,
Labeling,
Hospitals,
Seals,
Brain modeling,
Cerebral cortex,
Nervous system,
Neurosurgery,
Anatomy,
Two dimensional displays"
Inter-AS traffic patterns and their implications,"This paper reports on a study of traffic patterns among autonomous systems (ASes), based on traces taken at various points in the Internet. The traces display a highly nonuniform distribution of traffic on flows between pairs of hosts, networks, and ASes. Aggregation along coarser granularities, such as networks or ASes, accentuates this nonuniform distribution. In one typical trace, for example, the top 9% of flows between ASes accounts for 86.7% of the packets and 90.7% of the bytes transmitted. A highly nonuniform traffic pattern suggests that routers need to maintain only limited QoS flow state. The paper discusses the implications of this phenomenon on different proposed QoS mechanisms.","Telecommunication traffic,
Internet,
IP networks,
Spine,
Routing protocols,
Computer science,
Computer displays,
Buildings,
Large-scale systems,
Monitoring"
Multilevel k-way hypergraph partitioning,"In this paper, we present a new multilevel k-way hypergraph partitioning algorithm that substantially outperforms the existing state-of-the-art K-PM/LR algorithm for multi-way partitioning, both for optimizing local as well as global objectives. Experiments on the ISPD98 benchmark suite show that the partitionings produced by our scheme are on the average 15% to 23% better than those produced by the K-PM/LR algorithm, both in terms of the hyperedge cut as well as the (K-1) metric. Furthermore, our algorithm is significantly faster, requiring 4 to 5 times less time than that required by K-PM/LR.","Partitioning algorithms,
Iterative algorithms,
Contracts,
Permission,
Computer science,
Very large scale integration,
Algorithm design and analysis,
Databases,
Data mining,
Heuristic algorithms"
Multiway cut for stereo and motion with slanted surfaces,"Slanted surfaces pose a problem for correspondence algorithms utilizing search because of the greatly increased number of possibilities, when compared with fronto-parallel surfaces. In this paper we propose an algorithm to compute correspondence between stereo images or between frames of a motion sequence by minimizing an energy functional that accounts for slanted surfaces. The energy is minimized in a greedy strategy that alternates between segmenting the image into a number of non-overlapping regions (using the multiway-cut algorithm of Boykov, Veksler, and Zabih) and finding the affine parameters describing the displacement function of each region. A follow-up step enables the algorithm to escape local minima due to oversegmentation. Experiments on real images show the algorithm's ability to find an accurate segmentation and displacement map, as well as discontinuities and creases, from a wide variety of stereo and motion imagery.","Image segmentation,
Layout,
Cameras,
Geometry,
Computer science,
Lapping,
Read only memory,
Costs,
Shape"
A wireless hierarchical routing protocol with group mobility,"In this paper we present a hierarchical routing protocol in a large wireless, mobile network such as found in the automated battlefield or in extensive disaster recovery operations. Conventional routing does not scale well to network size. Likewise, conventional hierarchical routing cannot handle mobility efficiently. We propose a novel soft state wireless hierarchical routing protocol-Hierarchical State Routing (HSR). We distinguish between the ""physical"" routing hierarchy (dictated by geographical relationships between nodes) and ""logical"" hierarchy of subnets in which the members move as a group (e.g., company, brigade, battalion in the battlefield). HSR keeps track of logical subnet movements using home agent concepts akin to Mobile IP. A group mobility model is introduced and the performance of the HSR is evaluated through a detailed wireless simulation model.","Routing protocols,
Ad hoc networks,
Military computing,
Mobile computing,
Bandwidth,
Multimedia databases,
Delay,
Distributed databases,
Contracts,
Computer science"
Total variation regulated EM algorithm [SPECT reconstruction],"An iterative Bayesian reconstruction algorithm based on the total variation (TV) norm constraint is proposed. The motivation for using TV regularization is that it is extremely effective for recovering edges of images. This paper extends the TV norm minimization constraint to the field of SPECT image reconstruction with a Poisson noise model. The regularization norm is included in the OSL-EM (one step late expectation maximization) algorithm. Unlike many other edge-preserving regularization techniques, the TV based method depends one parameter. Reconstructions of computer simulations and patient data show that the proposed algorithm has the capacity to smooth noise and maintain sharp edges without introducing over/under shoots and ripples around the edges.","TV,
Bayesian methods,
Image reconstruction,
Probability distribution,
Iterative algorithms,
Student members,
Senior members,
Radiology,
Cities and towns,
Reconstruction algorithms"
Low-density MDS codes and factors of complete graphs,"We present a class of array code of size n/spl times/l, where l=2n or 2n+1, called B-Code. The distances of the B-Code and its dual are 3 and l-1, respectively. The B-Code and its dual are optimal in the sense that i) they are maximum-distance separable (MDS), ii) they have an optimal encoding property, i.e., the number of the parity bits that are affected by change of a single information bit is minimal, and iii) they have optimal length. Using a new graph description of the codes, we prove an equivalence relation between the construction of the B-Code (or its dual) and a combinatorial problem known as perfect one-factorization of complete graphs, thus obtaining constructions of two families of the B-Code and its dual, one of which is new. Efficient decoding algorithms are also given, both for erasure correcting and for error correcting. The existence of perfect one-factorizations for every complete graph with an even number of nodes is a 35 years long conjecture in graph theory. The construction of B-Codes of arbitrary odd length will provide an affirmative answer to the conjecture.",Linear codes
Learning mixtures of Gaussians,Mixtures of Gaussians are among the most fundamental and widely used statistical models. Current techniques for learning such mixtures from data are local search heuristics with weak performance guarantees. We present the first provably correct algorithm for learning a mixture of Gaussians. This algorithm is very simple and returns the true centers of the Gaussians to within the precision specified by the user with high probability. It runs in time only linear in the dimension of the data and polynomial in the number of Gaussians.,"Gaussian processes,
Statistics,
Electrical capacitance tomography,
Clustering algorithms,
Read only memory,
Probability,
History,
Psychology,
Geology,
Astrophysics"
Geometrically correct 3-D reconstruction of intravascular ultrasound images by fusion with biplane angiography-methods and validation,"In the rapidly evolving field of intravascular ultrasound (IVUS), the assessment of vessel morphology still lacks a geometrically correct three-dimensional (3-D) reconstruction. The IVUS frames are usually stacked up to form a straight vessel, neglecting curvature and the axial twisting of the catheter during the pullback. The authors' method combines the information about vessel cross sections obtained from IVUS with the information about the vessel geometry derived from biplane angiography. First, the catheter path is reconstructed from its biplane projections, resulting in a spatial model. The locations of the IVUS frames are determined and their orientations relative to each other are calculated using a discrete approximation of the Frenet-Serret formulas known from differential geometry. The absolute orientation of the frame set is established, utilizing the imaging catheter itself as an artificial landmark. The IVUS images are segmented, using the authors' previously developed algorithm. The fusion approach has been extensively validated in computer simulations, phantoms, and cadaveric pig hearts.","Three dimensional displays,
Ultrasonic imaging,
Catheters,
Image reconstruction,
Morphology,
Information geometry,
Angiography,
Image segmentation,
Computer simulation,
Imaging phantoms"
A path availability model for wireless ad-hoc networks,"Ad-hoc networks are expected to play an important role in future commercial and military communications systems. As such, scalable routing strategies capable of supporting greater user mobility and a wide range of applications are needed. This paper proposes a novel routing metric, which defines a probabilistic measure of the availability of network paths that are subject to link failures caused by node mobility in ad-hoc networks. It is shown how this measure can be used to select more stable paths and reduce the routing overhead caused by node mobility. A mobility model is first proposed and used to characterize the movement of ad-hoc network nodes. This model is then used to derive expressions for link and path availability. Finally, simulation results are reported which validate the proposed analytical model.","Ad hoc networks,
Routing,
Availability,
Analytical models,
Frequency,
Network topology,
Aggregates,
Information science,
Computer science,
Hospitals"
"Textons, contours and regions: cue integration in image segmentation","The paper makes two contributions: it provides (1) an operational definition of textons, the putative elementary units of texture perception, and (2) an algorithm for partitioning the image into disjoint regions of coherent brightness and texture, where boundaries of regions are defined by peaks in contour orientation energy and differences in texton densities across the contour. B. Julesz (1981) introduced the term texton, analogous to a phoneme in speech recognition, but did not provide an operational definition for gray-level images. We re-invent textons as frequently co-occurring combinations of oriented linear filter outputs. These can be learned using a K-means approach. By mapping each pixel to its nearest texton, the image can be analyzed into texton channels, each of which is a point set where discrete techniques such as Voronoi diagrams become applicable. Local histograms of texton frequencies can be used with a /spl chi//sup 2/ test for significant differences to find texture boundaries. Natural images contain both textured and untextured regions, so we combine this cue with that of the presence of peaks of contour energy derived from outputs of odd- and even-symmetric oriented Gaussian derivative filters. Each of these cues has a domain of applicability, so to facilitate cue combination we introduce a gating operator based on a statistical test for isotropy of Delaunay neighbors. Having obtained a local measure of how likely two nearby pixels are to belong to the same region, we use the spectral graph theoretic framework of normalized cuts to find partitions of the image into regions of coherent texture and brightness. Experimental results on a wide range of images are shown.","Image segmentation,
Nonlinear filters,
Peak to average power ratio,
Pixel,
Image analysis,
Testing,
Humans,
Computer science,
Image recognition,
Ear"
TCP performance in wireless multi-hop networks,"We investigate the interaction between TCP and MAC layer in a wireless multi-hop network. Using simulation, we provide new insight into two critical problems of TCP over wireless multi-hop. The first is the conflict between TCP data packets and TCP ACKs, which causes performance to degrade for window sizes greater than 1 packet. The second is the interaction between TCP and MAC layer backoff timers which causes severe unfairness and capture conditions. We show that these effects are particularly pronounced in two families of MAC protocols that have been extensively used in ad-hoc network simulation and implementations, namely CSMA and FAMA (a descendent of MACA). We then demonstrate that these problems are in part overcome by using MACAW, a MAC layer protocol which extends MACA by adding link level ACKs and a less aggressive backoff policy. We argue that link level protection, backoff policy and selective queue scheduling are critical elements for efficient and fair operation of ad-hoc networks under TCP. These conclusions are supported by extensive simulation and measurement experiments.","Intelligent networks,
Spread spectrum communication,
Media Access Protocol,
Protection,
Wireless application protocol,
Ad hoc networks,
Communication system control,
Error correction,
Laboratories,
Computer science"
Test case prioritization: an empirical study,Test case prioritization techniques schedule test cases for execution in an order that attempts to maximize some objective function. A variety of objective functions are applicable; one such function involves rate of fault detection-a measure of how quickly faults are detected within the testing process. An improved rate of fault detection during regression testing can provide faster feedback on a system under regression test and let debuggers begin their work earlier than might otherwise be possible. In this paper we describe several techniques for prioritizing test cases and report our empirical results measuring the effectiveness of these techniques for improving rate of fault detection. The results provide insights into the tradeoffs among various techniques for test case prioritization.,
Sparse Code Shrinkage: Denoising of Nongaussian Data by Maximum Likelihood Estimation,"Sparse coding is a method for finding a representation of data in which each of the components of the representation is only rarely significantly active. Such a representation is closely related to redundancy reduction and independent component analysis, and has some neurophysiological plausibility. In this article, we show how sparse coding can be used for denoising. Using maximum likelihood estimation of nongaussian variables corrupted by gaussian noise, we show how to apply a soft-thresholding (shrinkage) operator on the components of sparse coding so as to reduce noise. Our method is closely related to the method of wavelet shrinkage, but it has the important benefit over wavelet methods that the representation is determined solely by the statistical properties of the data. The wavelet representation, on the other hand, relies heavily on certain mathematical properties (like self-similarity) that may be only weakly related to the properties of natural data.",
Nonlinear elastic registration of brain images with tumor pathology using a biomechanical model [MRI],"A biomechanical model of the brain is presented, using a finite-element formulation. Emphasis is given to the modeling of the soft-tissue deformations induced by the growth of tumors and its application to the registration of anatomical atlases, with images from patients presenting such pathologies. First, an estimate of the anatomy prior to the tumor growth is obtained through a simulated biomechanical contraction of the tumor region. Then a normal-to-normal atlas registration to this estimated pre-tumor anatomy is applied. Finally, the deformation from the tumor-growth model is applied to the resultant registered atlas, producing an atlas that has been deformed to fully register to the patient images. The process of tumor growth is simulated in a nonlinear optimization framework, which is driven by anatomical features such as boundaries of brain structures. The deformation of the surrounding tissue is estimated using a nonlinear elastic model of soft tissue under the boundary conditions imposed by the skull, ventricles, and the falx and tentorium. A preliminary two-dimensional (2-D) implementation is presented in this paper, and tested on both simulated and patient data. One of the long-term goals of this work is to use anatomical brain atlases to estimate the locations of important brain structures in the brain and to use these estimates in pre-surgical and radiosurgical planning systems.","Neoplasms,
Pathology,
Brain modeling,
Magnetic resonance imaging,
Deformable models,
Anatomy,
Finite element methods,
Biological tissues,
Boundary conditions,
Skull"
Alternating cluster estimation: a new tool for clustering and function approximation,"Many clustering models define good clusters as extrema of objective functions. Optimization of these models is often done using an alternating optimization (AO) algorithm driven by necessary conditions for local extrema. We abandon the objective function model in favor of a generalized model called alternating cluster estimation (ACE). ACE uses an alternating iteration architecture, but membership and prototype functions are selected directly by the user. Virtually every clustering model can be realized as an instance of ACE. Out of a large variety of possible instances of non-AO models, we present two examples: 1) an algorithm with a dynamically changing prototype function that extracts representative data and 2) a computationally efficient algorithm with hyperconic membership functions that allows easy extraction of membership functions. We illustrate these non-AO instances on three problems: a) simple clustering of plane data where we show that creating an unmatched ACE algorithm overcomes some problems of fuzzy c-means (FCM-AO) and possibilistic c-means (PCM-AO); b) functional approximation by clustering on a simple artificial data set; and c) functional approximation on a 12 input 1 output real world data set. ACE models work pretty well in all three cases.",
Efficient calculation of lattice sums for free-space periodic Green's function,"An efficient method to calculate the lattice sums is presented for a one-dimensional (1-D) periodic array of line sources. The method is based on the recurrence relations for Hankel functions and the Fourier integral representation of the zeroth-order Hankel function. The lattice sums of arbitrary high order are then expressed by an integral of elementary functions, which is easily computed using a simple scheme of numerical integration. The calculated lattice sums are used to evaluate the free-space periodic Green's function. The numerical results show that the proposed method provides a highly accurate evaluation of the Green's function with far less computation time, even when the observation point is located near the plane of the array.","Lattices,
Green's function methods,
Electromagnetic scattering,
Periodic structures,
Numerical analysis,
Convergence of numerical methods,
Computer science,
Testing"
A method for ranking fuzzy numbers and its application to decision-making,"Since fuzzy numbers represent uncertain numeric values, it is difficult to rank them according to their magnitude. In the paper, a method for ranking fuzzy numbers is proposed. The method considers the overall possibility distributions of fuzzy numbers in their evaluations for ranking and provides users with a method of changing viewpoints for evaluations. Users represent their viewpoints with fuzzy sets. The method evaluates fuzzy numbers with a satisfaction function and the viewpoint given by users and then ranks the numbers according to their evaluation values. The satisfaction function is a measure of comparisons between fuzzy numbers. In order to illustrate the ranking method, two numeric examples are shown, and for the comparative study, our method is compared with four existing ranking methods through eight examples. As an example of potential applications, the proposed method is applied to a decision-making problem: a two-person game with fuzzy profit and loss. The ranking method is used to analyze player choices.","Decision making,
Game theory,
Fuzzy sets,
Fuzzy set theory,
Information technology,
Computer science"
A comparison study of static mapping heuristics for a class of meta-tasks on heterogeneous computing systems,"Heterogeneous computing (HC) environments are well suited to meet the computational demands of large, diverse groups of tasks (i.e., a meta-task). The problem of mapping (defined as matching and scheduling) these tasks onto the machines of an HC environment has been shown, in general, to be NP-complete, requiring the development of heuristic techniques. Selecting the best heuristic to use in a given environment, however, remains a difficult problem, because comparisons are often clouded by different underlying assumptions in the original studies of each heuristic. Therefore, a collection of eleven heuristics from the literature has been selected, implemented, and analyzed under one set of common assumptions. The eleven heuristics examined are opportunistic load balancing, user-directed assignment, fast greedy, min-min, max-min, greedy, genetic algorithm, simulated annealing, genetic simulated annealing, tabu, and A*. This study provides one even basis for comparison and insights into circumstances where one technique will outperform another. The evaluation procedure is specified, the heuristics are defined, and then selected results are compared.","Distributed computing,
Computer science,
Processor scheduling,
Computational modeling,
Load management,
Genetic algorithms,
Subcontracting,
Computer applications,
Collaboration,
Prototypes"
Statistical textural features for detection of microcalcifications in digitized mammograms,"Clustered microcalcifications on X-ray mammograms are an important sign for early detection of breast cancer. Texture-analysis methods can be applied to detect clustered microcalcifications in digitized mammograms. In this paper, a comparative study of texture-analysis methods is performed for the surrounding region-dependence method, which has been proposed by the authors, and conventional texture-analysis methods, such as the spatial gray level dependence method, the gray-level run-length method, and the gray-level difference method. Textural features extracted by these methods are exploited to classify regions of interest (ROI's) into positive ROI's containing clustered microcalcifications and negative ROI's containing normal tissues. A three-layer backpropagation neural network is used as a classifier. The results of the neural network for the texture-analysis methods are evaluated by using a receiver operating-characteristics (ROC) analysis. The surrounding region-dependence method is shown to be superior to the conventional texture-analysis methods with respect to classification accuracy and computational complexity.",
Extending the technology acceptance model to account for social influence: theoretical bases and empirical validation,"The Technology Acceptance Model (TAM) represents an important theoretical contribution toward understanding IS usage and IS acceptance behaviors. However, as noted by several IS researchers, TAM is incomplete in one important respect: it doesn't account for social influence in the adoption and utilization of new information systems. Davis (1986) and Davis et al. (1989) noted that it is important to account for subjective norm (SN), the construct denoting social influence. However, they observed that the conceptualization of SN based on TRA (Theory of Reasoned Action) has theoretical and psychometric problems. Specifically, they observed that it is difficult to distinguish if usage behavior is caused by the influence of referents on one's intent or by one's own attitude. They suggested that this problem may be circumvented by using an alternative theoretical basis for conceptualizing SN, specifically in terms of Kelman's (1958, 1961) processes of social influence (compliance, identification and internalization). Within the context of organizational enterprisewide implementation and adoption of collaboration and communication technologies, this study establishes theoretical and empirical bases for the above conceptualization originally suggested by Davis and his colleagues. The construct of social influence is operationalized in terms of Kelman's processes of internalization, identification and compliance. Analyses of field study data provide evidence of the reliability and validity of the proposed constructs, factor structures and measures. The findings enable future researchers to account for social influence in further investigating TAM.","EMP radiation effects,
Bismuth,
Tin,
Psychology,
Computational Intelligence Society,
Information systems,
Context,
Collaboration,
Communications technology,
Computers"
"Reducing power in superscalar processor caches using subbanking, multiple line buffers and bit-line segmentation","Modern microprocessors employ one or two levels of on-chip caches to bridge the burgeoning speed disparities between the processor and the RAM. These SRAM caches are a major source of power dissipation. We investigate architectural techniques, that do not compromise the processor cycle time, for reducing the power dissipation within the on-chip cache hierarchy in superscalar microprocessors. We use a detailed register-level simulator of a superscalar microprocessor that simulates the execution of the SPEC benchmarks and SPICE measurements for the actual layout of a 0.5 micron, 4-metal layer cache, optimized for a 300 MHz, clock. We show that a combination of subbanking, multiple line buffers and bit-line segmentation can reduce the on-chip cache power dissipation by as much as 75% in a technology-independent manner.",
Toward a common component architecture for high-performance scientific computing,"Describes work in progress to develop a standard for interoperability among high-performance scientific components. This research stems from the growing recognition that the scientific community needs to better manage the complexity of multidisciplinary simulations and better address scalable performance issues on parallel and distributed architectures. The driving force for this is the need for fast connections among components that perform numerically intensive work and for parallel collective interactions among components that use multiple processes or threads. This paper focuses on the areas we believe are most crucial in this context, namely an interface definition language that supports scientific abstractions for specifying component interfaces and a port connection model for specifying component interactions.","Component architectures,
Scientific computing,
Laboratories,
Object oriented modeling,
Computer science,
Parallel programming,
Yarn,
Context modeling,
Software systems,
Mathematics"
Optimized smooth handoffs in Mobile IP,"Mobile IP provides mobility support for hosts connected to the Internet without changing their IP addresses. Route optimization strategies complement Mobile IP, to alleviate triangle routing, by informing correspondent hosts of the mobile host's care-of address. We propose further strategies that are compatible with route optimization and its security model. First, foreign agents buffer packets for a mobile host and send them to its new location when it leaves. Second, hierarchical foreign agent management reduces the administrative overhead of frequent local handoffs, using an extension of the Mobile IP registration process so security can be maintained. Duplicate packets due to buffer handover are eliminated with the cooperation of mobile hosts. Simulation results show substantial performance improvements in terms of throughput, registration overhead, lost and duplicate packets during a handoff, without restrictions on physical placement of foreign agents.","Routing,
Birth disorders,
Base stations,
Internet,
Mobile computing,
Computer science,
Educational institutions,
Tunneling,
Authentication,
Data security"
Buffer block planning for interconnect-driven floorplanning,"We study buffer block planning for interconnect-driven floorplanning in deep submicron designs. We first introduce the concept of feasible region (FR) for buffer insertion, and derive closed-form formula for FR. We observe that the FR for a buffer is quite large in general even under fairly tight delay constraints. Therefore, FR gives us a lot of flexibility to plan for buffer locations. We then develop an effective buffer block planning (BBP) algorithm to perform buffer clustering such that the overall chip area and the buffer block number can be minimized. To the best of our knowledge, this is the first in-depth study on buffer planning for interconnect-driven floorplanning with both area and delay consideration.","Integrated circuit interconnections,
Wire,
Delay estimation,
Design optimization,
Routing,
Computer science,
Clustering algorithms,
Very large scale integration,
Circuit optimization,
Circuit synthesis"
Hidden Markov models based on multi-space probability distribution for pitch pattern modeling,"This paper discusses a hidden Markov model (HMM) based on multi-space probability distribution (MSD). The HMMs are widely-used statistical models to characterize the sequence of speech spectra and have successfully been applied to speech recognition systems. From these facts, it is considered that the HMM is useful for modeling pitch patterns of speech. However, we cannot apply the conventional discrete or continuous HMMs to pitch pattern modeling since the observation sequence of the pitch pattern is composed of one-dimensional continuous values and a discrete symbol which represents ""unvoiced"". MSD-HMM includes discrete HMMs and continuous mixture HMMs as special cases, and further can model the sequence of observation vectors with variable dimension including zero-dimensional observations, i.e., discrete symbols. As a result, MSD-HMMs can model pitch patterns without heuristic assumption. We derive a reestimation algorithm for the extended HMM and show that it can find a critical point of the likelihood function.","Hidden Markov models,
Probability distribution,
Speech recognition,
Context modeling,
Probability density function,
Computer science,
Laboratories,
Continuing education"
A multistage evolutionary algorithm for the timetable problem,"It is well known that timetabling problems can be very difficult to solve, especially when dealing with particularly large instances. Finding near-optimal results can prove to be extremely difficult, even when using advanced search methods such as evolutionary algorithms (EAs). The paper presents a method of decomposing larger problems into smaller components, each of which is of a size that the EA can effectively handle. Various experimental results using this method show that not only can the execution time be considerably reduced but also that the presented method can actually improve the quality of the solutions.","Evolutionary computation,
Search methods,
Processor scheduling,
Computer science,
Simulated annealing,
Genetic algorithms"
Fast spatio-temporal image reconstruction for dynamic PET,"In tomographic imaging, dynamic images are typically obtained by reconstructing the frames of a time sequence independently, one by one. A disadvantage of this frame-by-frame reconstruction approach is that it fails to account. For temporal correlations in the signal. Ideally, one should treat the entire image sequence as a single spatio-temporal signal. However, the resulting reconstruction task becomes computationally intensive. Fortunately, as the authors show in this paper, the spatio-temporal reconstruction problem call be greatly simplified by first applying a temporal Karhunen-Loeve (KL) transformation to the imaging equation. The authors show that if the regularization operator is chosen to be separable into space and time components, penalized weighted least squares reconstruction of the entire image sequence is approximately equivalent to frame-by-frame reconstruction in the space-KL domain. By this approach, spatio-temporal reconstruction can be achieved at reasonable computational cost. One can achieve further computational savings by discarding high-order KL components to avoid reconstructing them. Performance of the method is demonstrated through statistical evaluations of the bias-variance tradeoff obtained by computer simulation reconstruction.","Image reconstruction,
Positron emission tomography,
Least squares approximation,
Image sequences,
Equations,
Computational efficiency,
Computer simulation,
Principal component analysis,
High-resolution imaging,
Image resolution"
Distributed and mobility-adaptive clustering for multimedia support in multi-hop wireless networks,"A distributed algorithm is presented that partitions the nodes of a fully mobile network (multi-hop network) into clusters, thus giving the network a hierarchical organization. The algorithm is proven to be adaptive to changes in the network topology due to nodes' mobility and to nodes addition/removal. A new weight-based mechanism is introduced for the efficient cluster formation/maintenance that allows the cluster organization to be configured for specific applications and adaptive to changes in the network status, not available in previous solutions. Specifically, new and flexible criteria are defined that allow the choice of the nodes that coordinate the clustering process based on mobility parameters and/or their current status. Simulation results are provided that demonstrate up to an 85% reduction on the communication overhead associated with the cluster maintenance with respect to techniques used in clustering algorithms previously proposed.","Intelligent networks,
Spread spectrum communication,
Wireless networks,
Clustering algorithms,
Multimedia systems,
Computer science,
Electronic mail,
Distributed algorithms,
Partitioning algorithms,
Network topology"
The LRPD test: speculative run-time parallelization of loops with privatization and reduction parallelization,"Current parallelizing compilers cannot identify a significant fraction of parallelizable loops because they have complex or statically insufficiently defined access patterns. As parallelizable loops arise frequently in practice, we advocate a novel framework for their identification: speculatively execute the loop as a doall and apply a fully parallel data dependence test to determine if it had any cross-iteration dependences; if the test fails, then the loop is reexecuted serially. Since, from our experience, a significant amount of the available parallelism in Fortran programs can be exploited by loops transformed through privatization and reduction parallelization, our methods can speculatively apply these transformations and then check their validity at run-time. Another important contribution of this paper is a novel method for reduction recognition which goes beyond syntactic pattern matching: it detects at run-time if the values stored in an array participate in a reduction operation, even if they are transferred through private variables and/or are affected by statically unpredictable control flow. We present experimental results on loops from the PERFECT Benchmarks, which substantiate our claim that these techniques can yield significant speedups which are often superior to those obtainable by inspector/executor methods.","Testing,
Runtime,
Privatization,
Parallel processing,
Circuit simulation,
Computational modeling,
Algorithm design and analysis,
Computer science,
Pattern recognition,
Pattern matching"
Computerized detection of malignant tumors on digital mammograms,"This paper presents a tumor detection system for fully digital mammography. The processing scheme adopted in the proposed system focuses on the solution of two problems. One is how to detect tumors as suspicious regions with a very weak contrast to their background and another is how to extract features which characterize malignant tumors. For the first problem, a unique adaptive filter called the iris filter is proposed. It is very effective in enhancing approximately rounded opacities no matter what their contrasts might be. Clues for differentiation between malignant tumors and other tumors are believed to be mostly in their border areas. This paper proposes typical parameters which reflect boundary characteristics. To confirm the system performance for unknown samples, large scale experiments using 1212 CR images were performed. The results showed that the sensitivity of the proposed system was 90.5% and the average number of false positives per image was found to be only 1.3. These results show the effectiveness of the proposed system.","Malignant tumors,
Neoplasms,
Mammography,
Adaptive filters,
Iris,
Computer aided diagnosis,
Breast cancer,
Feature extraction,
System performance,
Large-scale systems"
Retrospective intermodality registration techniques for images of the head: surface-based versus volume-based,"A blinded evaluation of two groups of retrospective image registration techniques was performed using as a gold standard a prospective marker-based registration method, and we compared the performance of one group with the other. By grouping the techniques as volume-based or surface-based, we could make some interesting conclusions. In order to ensure blindness, all retrospective registrations were performed by participants who had no knowledge of the gold-standard results until after their results had been submitted. Image volumes of three modalities (X-ray CT, MRI and PET) were obtained from patients undergoing neurosurgery on whom bone-implanted fiducial markers were mounted. These volumes had all traces of the markers removed and were provided via the Internet to outside collaborators, who then performed retrospective registrations on the volumes, calculating transformations from CT to MRI and/or from PET to MRI. The accuracy of each registration was then evaluated. The accuracy is measured at multiple volumes of interest. The volume-based techniques in this study tended to give substantially more accurate and reliable results than the surface-based ones for the CT-to-MRI registration tasks, and slightly more accurate results for the PET-to-MRI tasks. Analysis of these results revealed that the rotational component of error was more pronounced for the surface-based group. It was also apparent that all of the registration techniques we examined have the potential to produce satisfactory results much of the time, but that visual inspection is necessary to guard against large errors.","Head,
Magnetic resonance imaging,
Computed tomography,
Positron emission tomography,
Image registration,
Performance evaluation,
Gold,
Blindness,
X-ray imaging,
Neurosurgery"
A controlled experiment to assess the benefits of estimating with analogy and regression models,"To have general validity, empirical results must converge. To be credible, an experimental science must understand the limitations and be able to explain the disagreements of empirical results. We describe an experiment to replicate previous studies which claim that estimation by analogy outperforms regression models. In the experiment, 68 experienced practitioners each estimated a project from a dataset of 48 industrial COTS projects. We applied two treatments, an analogy tool and a regression model, and we used the estimating performance when aided by the historical data as the control. We found that our results do not converge with previous results. The reason is that previous studies have used other datasets and partially different data analysis methods, and last but not least, the tools have been validated in isolation from the tool users. This implies that the results are sensitive to the experimental design: the characteristics of the dataset, the norms for removing outliers and other data points from the original dataset, the test metrics, significance levels, and the use of human subjects and their level of expertise. Thus, neither our results nor previous results are robust enough to claim any general validity.","Enterprise resource planning,
Testing,
Humans,
Costs,
Convergence,
Physics,
Data analysis,
Design for experiments,
Robustness,
Software performance"
Discriminating congestion losses from wireless losses using inter-arrival times at the receiver,"We present a simple scheme which enables a TCP receiver to distinguish congestion losses from corruption losses. The scheme works in the case where the last hop to the receiver is a wireless link and has the smallest bandwidth among all links on the connection path. We added our mechanism to TCP-Reno to evaluate the performance improvement. We compared our scheme against Ideal TCP-Reno which is TCP-Reno that can perfectly (but artificially) distinguish between congestion losses and wireless transmission losses. Under favourable conditions, our scheme performs similar to Ideal TCP-Reno and can lead to significant throughput improvement.","Propagation losses,
Throughput,
Electronic switching systems,
Computer science,
Postal services,
Read only memory,
Bandwidth,
Internet,
Wireless application protocol,
Electrical capacitance tomography"
Scenario-based reliability analysis of component-based software,"Software designers are motivated to utilize off-the-shelf software components for rapid application development. Such applications are expected to have high reliability as a result of deploying trusted components. The claims of high reliability need further investigation based on reliability analysis techniques that are applicable to component-based applications. This paper introduces a probabilistic model and a reliability analysis technique that is applicable to high-level designs. The technique is named scenario-based reliability analysis (SBRA). SBRA is specific to component-based software whose analysis is strictly based on execution scenarios. Using scenarios, we construct a probabilistic model named a ""component-dependency graph"" (CDG). CDGs are directed graphs that represent components, component reliabilities, link and interface reliabilities, transitions and transition probabilities. In CDGs, component interfaces and link reliabilities are treated as first-class elements of the model. Based on CDGs, an algorithm is presented to analyze the reliability of the application as the function of reliabilities of its components and interfaces. A case study illustrates the applicability of the algorithm. The SBRA is used to identify critical components and critical component interfaces, and to investigate the sensitivity of the application reliability to changes in the reliabilities of components and their interfaces.","Application software,
Software maintenance,
System testing,
Computerized monitoring,
NASA,
Computer science,
Software reliability,
Programming,
Assembly,
Software quality"
Impulse: building a smarter memory controller,"Impulse is a new memory system architecture that adds two important features to a traditional memory controller. First, Impulse supports application-specific optimizations through configurable physical address remapping. By remapping physical addresses, applications control how their data is accessed and cached, improving their cache and bus utilization. Second, Impulse supports prefetching at the memory controller, which can hide much of the latency of DRAM accesses. In this paper we describe the design of the Impulse architecture, and show how an Impulse memory system can be used to improve the performance of memory-bound programs. For the NAS conjugate gradient benchmark, Impulse improves performance by 67%. Because it requires no modification to processor, cache, or bus designs, Impulse can be adopted in conventional systems. In addition to scientific applications, we expect that Impulse will benefit regularly strided memory-bound applications of commercial importance, such as database and multimedia programs.","Random access memory,
Electronic switching systems,
Prefetching,
Delay,
Microprocessors,
Bandwidth,
Computer science,
Cities and towns,
Sparse matrices,
Databases"
Nonactive antenna compensation for fixed-array microwave imaging. II. Imaging results,"For pt. I see ibid., vol. 18, no. 6, p. 496 (1999). Model-based imaging techniques utilizing microwave signal illumination rely heavily on the ability to accurately represent the wave propagation with a suitable numerical model. To date, the highest quality images from the authors' prototype system have been achieved utilizing a single transmitter/single receiver measurement system where both antennas are manually repositioned to facilitate multiple illuminations of the imaging region, thus requiring long data acquisition times. In an effort to develop a system that can acquire data in a real time manner, a 32-channel network has been fabricated with all ports capable of being electronically selected for either transmit or receive mode. The presence of a complete array of antenna elements at data collection time perturbs the field distributions being measured, which can subsequently degrade the image reconstruction due to increased data-model mismatch. Incorporating the nonactive antenna-compensation model from Part I of this paper into the authors' hybrid element near field image reconstruction algorithm is shown to restore image quality when fixed antenna-array data acquisition is used. Improvements are most dramatic for inclusions located in near proximity to the antenna array itself, although cases of improvement in the recovery of centered heterogeneities are also illustrated. Increases in the frequency of illumination are found to warrant an increased need for nonactive antenna compensation. Quantitative measures of recovered inclusion shape and position reveal a systematic improvement in image reconstruction quality when the nonactive antenna-compensation model is employed. Improvements in electrical property value recovery of localized heterogeneities are also observed. Image reconstructions in freshly excised breast tissue illustrate the applicability of the approach when used with the authors' two-dimensional microwave imaging system.","Microwave imaging,
Microwave antennas,
Image reconstruction,
Lighting,
Antenna measurements,
Antennas and propagation,
Data acquisition,
Antenna arrays,
Shape measurement,
Microwave theory and techniques"
Linux as a case study: its extracted software architecture,"Many software systems do not have a documented system architecture. These are often large, complex systems that are difficult to understand and maintain. One approach to recovering the understanding of a system is to extract architectural documentation from the system implementation. To evaluate the effectiveness of this approach, we extracted architectural documentation from the Linux/sup TM/ kernel. The Linux kernel is a good candidate for a case study because it is a large (800 KLOC) system that is in widespread use and it is representative of many existing systems. Our study resulted in documentation that is useful for understanding the Linux system structure. Also, we learned several useful lessons about extracting a system's architecture.","Linux,
Computer aided software engineering,
Software architecture,
Software systems,
Documentation,
Computer architecture,
Kernel,
Costs,
Computer science,
Buildings"
Large datasets at a glance: combining textures and colors in scientific visualization,"We present a new method for using texture and color to visualize multivariate data elements arranged on an underlying height field. We combine simple texture patterns with perceptually uniform colors to increase the number of attribute values we can display simultaneously. Our technique builds multicolored perceptual texture elements (or pexels) to represent each data element. Attribute values encoded in an element are used to vary the appearance of its pexel. Texture and color patterns that form when the pexels are displayed can be used to rapidly and accurately explore the dataset. Our pexels are built by varying three separate texture dimensions: height, density, and regularity. Results from computer graphics, computer vision, and human visual psychophysics have identified these dimensions as important for the formation of perceptual texture patterns. The pexels are colored using a selection technique that controls color distance, linear separation, and color category. Proper use of these criteria guarantees colors that are equally distinguishable from one another. We describe a set of controlled experiments that demonstrate the effectiveness of our texture dimensions and color selection criteria. We then discuss new work that studies how texture and color can be used simultaneously in a single display.","Data visualization,
Displays,
Humans,
Psychology,
Computer vision,
Typhoons,
Surface topography,
Computer graphics,
Interference,
Asia"
Scale-space signatures for the detection of clustered microcalcifications in digital mammograms,"A method is described for the automated detection of microcalcifications in digitized mammograms. The method is based on the Laplacian scale-space representation of the mammogram only. First, possible locations of microcalcifications are identified as local maxima in the filtered image on a range of scales. For each finding, the size and local contrast is estimated, based on the Laplacian response denoted as the scale-space signature. A finding is marked as a microcalcification if the estimated contrast is larger than a predefined threshold which depends on the size of the finding. It is shown that the signature has a characteristic peak, revealing the corresponding image features. This peak can be robustly determined. The basic method is significantly improved by consideration of the statistical variation of the estimated contrast, which is the result of the complex noise characteristic of the mammograms. The method is evaluated with the Nijmegen database and compared to other methods using these mammograms. Results are presented as the free-response receiver operating characteristic (FROG) performance. At a rate of one false positive cluster per image the method reaches a sensitivity of 0.84, which is comparable to the best results achieved so far.","Cancer detection,
Laplace equations,
Breast cancer,
Lesions,
Mammography,
Visualization,
Noise robustness,
Image databases,
Spatial databases,
Europe"
Fluids in the universe: adaptive mesh refinement in cosmology,"Using galactic clusters, galactic formation, and the first stars as examples, the author demonstrates how adaptive mesh refinement techniques can be successfully applied to cosmological research. The method combines adaptive refinement of interesting regions with modern grid-based methods for solving hydrodynamic equations.","Adaptive mesh refinement,
Computational modeling,
Hydrodynamics,
Computational fluid dynamics,
Equations,
Gravity,
Lagrangian functions,
Timing,
Predictive models,
Fluctuations"
An architecture for a global Internet host distance estimation service,"There is an increasing need for Internet hosts to be able to quickly and efficiently learn the distance, in terms of metrics such as latency or bandwidth, between Internet hosts. For example, to select the nearest of multiple equal content Web servers. This paper explores technical issues related to the creation of a public infrastructure service to provide such information. In so doing, we suggest an architecture, called IDMaps, whereby Internet distance information is distributed over the Internet, using IP multicast groups, in the form of a virtual distance map. Systems listening to the groups can estimate the distance between any pair of IP addresses by running a spanning tree algorithm over the received distance map. We also presents the results of experiments that give preliminary evidence supporting the architecture. This work thus lays the initial foundation for future work in this new area.","Web and internet services,
Delay,
Web server,
Costs,
Bandwidth,
Web pages,
Protocols,
Computer science,
Computer architecture,
Laboratories"
EXPRESSION: a language for architecture exploration through compiler/simulator retargetability,"We describe EXPRESSION, a language supporting architectural design space exploration for embedded systems-on-chip (SOC) and automatic generation of a retargetable compiler/simulator toolkit. Key features of our language-driven design methodology include: a mixed behavioral/structural representation supporting a natural specification of the architecture, explicit specification of the memory, subsystem allowing novel memory organizations and hierarchies; clean syntax and ease of modification supporting architectural exploration; a single specification supporting consistency and completeness checking of the architecture; and efficient specification of architectural resource constraints allowing extraction of detailed reservation tables for compiler scheduling. We illustrate key features of EXPRESSION through simple examples and demonstrate its efficacy in supporting exploration and automatic software toolkit generation for an embedded SOC codesign flow.","Handicapped aids,
Hardware,
Costs,
Software tools,
Computational modeling,
Computer science,
System-on-a-chip,
Process design,
System-level design,
Libraries"
Classifying mammographic mass shapes using the wavelet transform modulus-maxima method,"In this article, multiresolution analysis, specifically the discrete wavelet transform modulus-maxima (mod-max) method, is utilized for the extraction of mammographic mass shape features. These shape features are used in a classification system to classify masses as round, nodular, or stellate. The multiresolution shape features are compared with traditional uniresolution shape features for their class discriminating abilities. The study involved 60 digitized mammographic images. The masses were segmented manually by radiologists, prior to introduction to the classification system. The uniresolution and multiresolution shape features were calculated using the radial distance measure of the mass boundaries. The discriminating power of the shape features were analyzed via linear discriminant analysis (LDA). The classification system utilized a simple Euclidean metric to determine class membership. The system was tested using the apparent and leave-one-out test methods. The classification system when using the multiresolution and uniresolution shape features resulted in classification rates of 83% and 80% for the apparent and leave one-out test methods, respectively. In comparison, when only the uniresolution shape features were used, the classification rates were 72 and 68% for the apparent and leave-one-out test methods, respectively.","Wavelet transforms,
Shape measurement,
Discrete wavelet transforms,
Cancer,
Wavelet analysis,
System testing,
Expert systems,
Linear discriminant analysis,
Breast neoplasms,
Multiresolution analysis"
Single and multiscale detection of masses in digital mammograms,"Scale is an important issue in the automated detection of masses in mammograms, due to the range of possible sizes masses can have. In this work, it was examined if detection of masses can be done at a single scale, or whether it is more appropriate to use the output of the detection method at different scales in a multiscale scheme. Three different pixel-based mass-detection methods were used for this purpose. The first method is based on convolution of a mammogram with the Laplacian of a Gaussian, the second method is based on correlation with a model of a mass, and the third is a new approach, based on statistical analysis of gradient-orientation maps. Experiments with simulated masses indicated that little can be gained by applying the methods at a number of scales. These results were confirmed by experiments on a set of 71 cases (132 mammograms) containing a malignant tumor. The performance of each method in a multiscale scheme was similar to the performance at the optimal single scale. A slight improvement was found for the correlation method when the output of different scales was combined. This was especially evident at low specificity levels. The correlation method and the gradient-orientation-analysis method have similar performances. A sensitivity of approximately 75% is reached at a level of one false positive per image. The method based on convolution with the Laplacian of the Gaussian performed considerably worse, in both a single and multiscale scheme.","Neoplasms,
Tellurium,
Cancer,
Convolution,
Laplace equations,
Correlation,
Mammography,
Statistical analysis,
Malignant tumors,
Computer aided diagnosis"
Physicists attempt to scale the ivory towers of finance,"Physicists have recently begun doing research in finance, and even though this movement is less than five years old, interesting and useful contributions have already emerged. This article reviews these developments in four areas, including empirical statistical properties of prices, random-process models for price dynamics, agent-based modeling, and practical applications.","Poles and towers,
Finance,
Recruitment,
Writing,
Frequency,
Econophysics,
Educational institutions,
Banking,
Physics computing,
Cultural differences"
Efficient mining of partial periodic patterns in time series database,"Partial periodicity search, i.e., search for partial periodic patterns in time-series databases, is an interesting data mining problem. Previous studies on periodicity search mainly consider finding full periodic patterns, where every point in time contributes (precisely or approximately) to the periodicity. However, partial periodicity is very common in practice since it is more likely that only some of the time episodes may exhibit periodic patterns. We present several algorithms for efficient mining of partial periodic patterns, by exploring some interesting properties related to partial periodicity such as the Apriori property and the max-subpattern hit set property, and by shared mining of multiple periods. The max-subpattern hit set property is a vital new property which allows us to derive the counts of all frequent patterns from a relatively small subset of patterns existing in the time series. We show that mining partial periodicity needs only two scans over the time series database, even for mining multiple periods. The performance study shows our proposed methods are very efficient in mining long periodic patterns.","Databases,
Cities and towns,
Data mining,
Computer science,
Read only memory,
Data analysis,
Algorithm design and analysis,
Councils,
Sun"
Lagrangian speckle model and tissue-motion estimation-theory [ultrasonography],"It is known that when a tissue is subjected to movements such as rotation, shearing, scaling, etc., changes in speckle patterns that result act as a noise source, often responsible for most of the displacement-estimate variance. From a modeling point of view, these changes can be thought of as resulting from two mechanisms: one is the motion of the speckles and the other, the alterations of their morphology. In this paper, the authors propose a new tissue-motion estimator to counteract these speckle decorrelation effects. The estimator is based on a Lagrangian description of the speckle motion. This description allows the authors to follow local characteristics of the speckle field as if they were a material property. This method leads to an analytical description of the decorrelation in a way which enables the derivation of an appropriate inverse filter for speckle restoration. The filter is appropriate for linear geometrical transformation of the scattering function (LT), i.e., a constant-strain region of interest (ROI). As the LT itself is a parameter of the filter, a tissue-motion estimator can be formulated as a nonlinear minimization problem, seeking the best match between the pre-tissue-motion image and a restored-speckle post-motion image. The method is tested, using simulated radio-frequency (RF) images of tissue undergoing axial shear.","Lagrangian functions,
Speckle,
Ultrasonography,
Filters,
Decorrelation,
Image restoration,
Radio frequency,
Shearing,
Morphology,
Motion estimation"
"The ""Parallel Vectors"" operator-a vector field visualization primitive","We propose an elementary operation on a pair of vector fields as a building block for defining and computing global line-type features of vector or scalar fields. While usual feature definitions often are procedural and therefore implicit, our operator allows precise mathematical definitions. It can serve as a basis for comparing feature definitions and for reuse of algorithms and implementations. Applications focus on vortex core methods.","Feature extraction,
Surface topography,
Data visualization,
Computer graphics,
Computer science,
Isosurfaces,
Differential algebraic equations,
Integral equations,
Differential equations,
Computational fluid dynamics"
Improved combinatorial algorithms for the facility location and k-median problems,"We present improved combinatorial approximation algorithms for the uncapacitated facility location and k-median problems. Two central ideas in most of our results are cost scaling and greedy improvement. We present a simple greedy local search algorithm which achieves an approximation ratio of 2.414+/spl epsiv/ in O/spl tilde/(n/sup 2///spl epsiv/) time. This also yields a bicriteria approximation tradeoff of (1+/spl gamma/, 1+2//spl gamma/) for facility cost versus service cost which is better than previously known tradeoffs and close to the best possible. Combining greedy improvement and cost scaling with a recent primal dual algorithm for facility location due to K. Jain and V. Vazirani (1999), we get an approximation ratio of 1.853 in O/spl tilde/(n/sup 3/) time. This is already very close to the approximation guarantee of the best known algorithm which is LP-based. Further combined with the best known LP-based algorithm for facility location, we get a very slight improvement in the approximation factor for facility location, achieving 1.728. We present improved approximation algorithms for capacitated facility location and a variant. We also present a 4-approximation for the k-median problem, using similar ideas, building on the 6-approximation of Jain and Vazirani. The algorithm runs in O/spl tilde/(n/sup 3/) time.","Linear programming,
Approximation algorithms,
Cost function,
Filtering algorithms,
Nonlinear filters,
Bismuth,
Electrical capacitance tomography"
Automatic clustering of software systems using a genetic algorithm,"Large software systems tend to have a rich and complex structure. Designers typically depict the structure of software systems as one or more directed graphs. For example, a directed graph can be used to describe the modules (or classes) of a system and their static interrelationships using nodes and directed edges, respectively. We call such graphs ""module dependency graphs"" (MDGs). MDGs can be large and complex graphs. One way of making them more accessible is to partition them, separating their nodes (i.e. modules) into clusters (i.e. subsystems). In this paper, we describe a technique for finding ""good"" MDG partitions. Good partitions feature relatively independent subsystems that contain modules which are highly interdependent. Our technique treats finding a good partition as an optimization problem, and uses a genetic algorithm (GA) to search the extraordinarily large solution space of all possible MDG partitions. The effectiveness of our technique is demonstrated by applying it to a medium-sized software system.","Software systems,
Genetic algorithms,
Mathematics,
Computer science,
Reverse engineering,
Software architecture,
Software design"
Will the real iris data please stand up?,"This correspondence points out several published errors in replicates of the well-known iris data, which was collected by Anderson (1935) but first published by Fisher (1936).","Iris,
Books,
Computer science,
Machine learning,
Databases,
Face detection,
Testing,
Pattern recognition,
Writing,
Mathematics"
Dynamically discovering likely program invariants to support program evolution,"Explicitly stated program invariants can help programmers by identifying program properties that must be preserved when modifying code. In practice, however, these invariants are usually implicit. An alternative to expecting programmers to fully annotate code with invariants is to automatically infer invariants from the program itself. This research focuses on dynamic techniques for discovering invariants from execution traces. This paper reports two results. First, it describes techniques for dynamically discovering invariants, along with an instrumenter and an inference engine that embody these techniques. Second, it reports on the application of the engine to two sets of target programs. In programs from Cries's work on program derivation, we rediscovered predefined invariants. In a C program lacking explicit invariants, we discovered invariants that assisted a software evolution task.","Programming profession,
Engines,
Testing,
Computer science,
Runtime,
Permission,
Application software,
Formal specifications,
Pattern analysis,
Pattern recognition"
Multimodal integration-a statistical view,"We present a statistical approach to developing multimodal recognition systems and, in particular, to integrating the posterior probabilities of parallel input signals involved in the multimodal system. We first identify the primary factors that influence multimodal recognition performance by evaluating the multimodal recognition probabilities. We then develop two techniques, an estimate approach and a learning approach, which are designed to optimize accurate recognition during the multimodal integration process. We evaluate these methods using Quickset, a speech/gesture multimodal system, and report evaluation results based on an empirical corpus collected with Quickset. From an architectural perspective, the integration technique presented offers enhanced robustness. It also is premised on more realistic assumptions than previous multimodal systems using semantic fusion. From a methodological standpoint, the evaluation techniques that we describe provide a valuable tool for evaluating multimodal systems.","Probability,
Speech recognition,
Humans,
Computer science,
Design optimization,
Speech analysis,
Robustness,
Decision making,
Uncertainty,
Hidden Markov models"
Selective value prediction,"Value prediction is a relatively new technique to increase instruction-level parallelism by breaking true data dependence chains. A value prediction architecture produces values, which may be later consumed by instructions that execute speculatively using the predicted value. This paper examines selective techniques for using value prediction in the presence of predictor capacity constraints and reasonable misprediction penalties. We examine prediction and confidence mechanisms in light of these constraints, and we minimize capacity conflicts through instruction filtering. The latter technique filters which instructions put values into the value prediction table. We examine filtering techniques based on instruction type, as well as giving priority to instructions belonging to the longest data dependence path in the processor's active instruction window. We apply filtering both to the producers of predicted values and the consumers. In addition, we examine the benefit of using different confidence levels for instructions using predicted values on the longest dependence path.","Delay,
Pipelines,
Computer science,
Filters,
Filtering,
Degradation"
Information technology enhanced learning in distance and conventional education,"The rapid growth of the Internet and the media-rich extensions of the World Wide Web allow new developments in the way instructors transfer knowledge to their students. There is no doubt that nothing will replace synchronous learning through face to face interaction but it is sometimes not feasible for students to attend conventional classes due to distance or time constraints. This paper presents a model for using information technology to enhance the learning experience for conventional on-campus students, as well as for those students whose circumstances require that they be asynchronous in time or space. The approach described emphasizes a solution which allows students to attend the class in real time via the Internet, or to access asynchronously digitally stored video material with hyperlinks to online training resources at any time. The proposed solution permits interaction in real-time and asynchronously among students and between students and instructor, which is a key for effective learning. In addition, the instructor maintains a significant level of spontaneity in using multimedia material prepared in advance or using conventional chalkboard or hand written materials via traditional overhead projection. The paper describes the technical issues involved and the chosen solutions to provide enhanced live and archived classes. This paper provides some comments on the evaluation of the learning experience using this method of delivery for on-campus and distance education students. Finally the authors share their vision on future trends to improve the proposed learning environment and the need for an optimal balance between expositive teaching and active learning for both synchronous and asynchronous activities.",Computer aided instruction
Recognizing surfaces using three-dimensional textons,"We study the recognition of surfaces made from different materials such as concrete, rug, marble or leather on the basis of their textural appearance. Such natural textures arise from spatial variation of two surface attributes: (1) reflectance and (2) surface normal. In this paper, we provide a unified model to address both these aspects of natural texture. The main idea is to construct a vocabulary of prototype tiny surface patches with associated local geometric and photometric properties. We call these 3D textons. Examples might be ridges, grooves, spots or stripes or combinations thereof Associated with each texton is an appearance vector, which characterizes the local irradiance distribution, represented as a set of linear Gaussian derivative filter outputs, under different lighting and viewing conditions. Given a large collection of images of different materials, a clustering approach is used to acquire a small (on the order of 100) 3D texton vocabulary. Given a few (1 to 4) images of any material, it can be characterized using these textons. We demonstrate the application of this representation for recognition of the material viewed under novel lighting and viewing conditions.","Lighting,
Concrete,
Filters,
Building materials,
Surface texture,
Computer science,
Read only memory,
Reactive power,
Electronic switching systems,
Network address translation"
Multinational evolutionary algorithms,"Since practical problems often are very complex with a large number of objectives, it can be difficult or impossible to create an objective function expressing all the criteria of good solutions. Sometimes a simpler function can be used where local optimas could be both valid and interesting. Because evolutionary algorithms are population based, they have the best potential for finding more of the best solutions among the possible solutions. However, standard EAs often converge to one solution and leave therefore only this single option for a final human selection. So far, at least two methods, sharing and tagging, have been proposed to solve the problem. The paper presents a new method for finding more quality solutions, not only global optimas but local as well. The method tries to adapt its search strategy to the problem by taking the topology of the fitness landscape into account. The idea is to use the topology to group the individuals into sub-populations, each covering a part of the fitness landscape.","Tagging,
Topology,
Computer science,
Evolutionary computation,
Humans,
Government,
Merging"
Supplying instantaneous video-on-demand services using controlled multicast,"We propose and evaluate the performance of a multicast technique, called controlled CIWP for supplying video-on-demand services. Similar to batching schemes, controlled CIWP allows two clients that request the same video to share a channel. Unlike batching, the controlled CIWP scheme does not delay the earlier request in order to promote sharing. Instead, sharing is ensured by permitting the client with the later arrival time to join an ongoing multicast session started for the earlier request. However, controlled CIWP does not let a client join an ongoing multicast session whenever possible. A threshold is used to control the frequency at which new multicast sessions are started. We derive the optimal threshold that maximizes the number of server channels required. Our simulation study shows that the controlled CIWP significantly improves the performance of the VOD service.","Delay,
Network servers,
Video sharing,
Bandwidth,
Streaming media,
Multicast algorithms,
Computer science,
Prefetching,
Educational institutions,
Weight control"
On the optimal placement of web proxies in the Internet,"Web caching or web proxy has been considered as the prime vehicle of coping with the ever-increasing demand for information retrieval over the Internet, the WWW being a typical example. Existing work on web proxy has primarily focused on content based caching; relatively less attention has been given to the development of proper placement strategies for the potential web proxies in the Internet. In this paper, we argue that the placement of web proxies is critical to the performance and further investigates the optimal placement policy of web proxies for a target web server in the Internet. The objective is to optimize a given performance measure for the target web server subject to system resources and traffic pattern. Specifically, we are interested in finding the optimal placement of multiple web proxies (M) among potential sites (N) under a given traffic pattern. We show this can be modeled a dynamic programming problem. We further obtain the optimal solution for the tree topology using O(N/sup 3/M/sup 2/) time.","Internet,
Web server,
Network servers,
Delay,
Vehicles,
Traffic control,
Computer science,
Information retrieval,
World Wide Web,
Dynamic programming"
An ordering algorithm for pattern presentation in fuzzy ARTMAP that tends to improve generalization performance,"We introduce a procedure, based on the max-min clustering method, that identifies a fixed order of training pattern presentation for fuzzy adaptive resonance theory mapping (ARTMAP). This procedure is referred to as the ordering algorithm, and the combination of this procedure with fuzzy ARTMAP is referred to as ordered fuzzy ARTMAP. Experimental results demonstrate that ordered fuzzy ARTMAP exhibits a generalization performance that is better than the average generalization performance of fuzzy ARTMAP, and in certain cases as good as, or better than the best fuzzy ARTMAP generalization performance. We also calculate the number of operations required by the ordering algorithm and compare it to the number of operations required by the training phase of fuzzy ARTMAP. We show that, under mild assumptions, the number of operations required by the ordering algorithm is a fraction of the number of operations required by fuzzy ARTMAP.","Resonance,
Clustering algorithms,
Clustering methods,
Sonar applications,
Radar applications,
Control systems,
Radar tracking,
Fuzzy logic,
Computer science,
Subspace constraints"
Stakeholder identification in the requirements engineering process,"Adequate, timely and effective consultation of relevant stakeholders is of paramount importance in the requirements engineering process. However, the thorny issue of making sure that all relevant stakeholders are consulted has received less attention than other areas which depend on it, such as scenario-based requirements, involving users in development, negotiating between different viewpoints and so on. The literature suggests examples of stakeholders, and categories of stakeholder, but does not provide help in identifying stakeholders for a specific system. In this paper, we discuss current work in stakeholder identification, propose an approach to identifying relevant stakeholders for a specific system, and propose future directions for the work.","Performance analysis,
Human computer interaction,
Informatics,
Design engineering,
Computer science,
Educational institutions,
Information systems,
Software engineering,
Engineering management,
Systems engineering and theory"
TCP and UDP performance over a wireless LAN,"We present a comprehensive set of measurements of a 2.4 GHz DSSS wireless LAN and analyze its behavior. We examine issues such as host and interface heterogeneity, bidirectional (TCP) traffic and error modeling, that have not been previously analyzed. We uncover multiple problems with TCP and UDP performance in this system. We investigate the causes of these problems (radio hardware, device drivers, network protocols) and discuss the effectiveness of proposed improvements.","Wireless LAN,
Testing,
Throughput,
Wireless communication,
Hardware,
Protocols,
Internet,
Traffic control,
Laboratories,
Computer science"
Stereo panorama with a single camera,"Full panoramic images, covering 360 degrees, can be created either by using panoramic cameras or by mosaicing together many regular images. Creating panoramic views in stereo, where one panorama is generated for the left eye, and another panorama is generated for the right eye is more problematic. Earlier attempts to mosaic images from a rotating pair of stereo cameras faced severe problems of parallax and of scale changes. A new family of multiple viewpoint image projections, the Circular Projections, is developed. Two panoramic images taken using such projections can serve as a panoramic stereo pair. A system is described to generates a stereo panoramic image using circular projections from images or video taken by a single rotating camera. The system works in real-time on a PC. It should be noted that the stereo images are created without computation of 3D structure, and the depth effects are created only in the viewer's brain.","Cameras,
Mirrors,
Layout,
Joining processes,
Computer science,
Real time systems,
Lenses,
Eyes,
Rendering (computer graphics),
Head"
Examples of 3D grasp quality computations,"Previous grasp quality research is mainly theoretical, and has assumed that contact types and positions are given, in order to preserve the generality of the proposed quality measures. The example results provided by these works either ignore hand geometry and kinematics entirely or involve only the simplest of grippers. We present a unique grasp analysis system that, when given a 3D object, hand, and pose for the hand, can accurately determine the types of contacts that will occur between the links of the hand and the object, and compute two measures of quality for the grasp. Using models of two articulated robotic hands, we analyze several grasps of a polyhedral model of a telephone handset, and we use a novel technique to visualize the 6D space used in these computations. In addition, we demonstrate the possibility of using this system for synthesizing high quality grasps by performing a search over a subset of possible hand configurations.","Geometry,
Solid modeling,
Power system modeling,
Kinematics,
Performance evaluation,
Grasping,
System testing,
Computer science,
Position measurement,
Orbital robotics"
Segmentation of medical images using LEGION,"Advances in visualization technology and specialized graphic workstations allow clinicians to virtually interact with anatomical structures contained within sampled medical-image datasets. A hindrance to the effective use of this technology is the difficult problem of image segmentation. In this paper, the authors utilize a recently proposed oscillator network called the locally excitatory globally inhibitory oscillator network (LEGION) whose ability to achieve fast synchrony with local excitation and desynchrony with global inhibition makes it an effective computational framework for grouping similar features and segregating dissimilar ones in an image. The authors extract an algorithm from LEGION dynamics and propose an adaptive scheme for grouping. They show results of the algorithm to two-dimensional (2-D) and three-dimensional (3-D) (volume) computerized topography (CT) and magnetic resonance imaging (MRI) medical-image datasets. In addition, the authors compare their algorithm with other algorithms for medical-image segmentation, as well as with manual segmentation. LEGION's computational and architectural properties make it a promising approach for real-time medical-image segmentation.","Image segmentation,
Biomedical imaging,
Local oscillators,
Magnetic resonance imaging,
Visualization,
Graphics,
Workstations,
Anatomical structure,
Computer networks,
Heuristic algorithms"
Deformable matching of hand shapes for user verification,"We present a method for personal authentication based on deformable matching of hand shapes. Authentication systems are already employed in domains that require some sort of user verification. Unlike previous methods on hand shape based verification, our method aligns the hand shapes before extracting a feature set. We also base the verification decision on the shape distance which is automatically computed during the alignment stage. The shape distance proves to be a more reliable classification criterion than the handcrafted feature sets used by previous systems. Our verification system attained a high level of accuracy: 96.5% genuine accept rate vs. false accept rate. This performance is further improved by learning an enrolment template shape for each user.","Shape,
Biometrics,
Feature extraction,
Iris,
Computer science,
World Wide Web,
Authentication,
Humans,
Access control,
Fingerprint recognition"
Detecting computer and network misuse through the production-based expert system toolset (P-BEST),"The paper describes an expert system development toolset called the Production-Based Expert System Toolset (P-BEST) and how it is employed in the development of a modern generic signature analysis engine for computer and network misuse detection. For more than a decade, earlier versions of P-BEST have been used in intrusion detection research and in the development of some of the most well known intrusion detection systems, but this is the first time the principles and language of P-BEST are described to a wide audience. We present rule sets for detecting subversion methods against which there are few defenses-specifically, SYN flooding and buffer overruns-and provide performance measurements. Together, these examples and performance measurements indicate that P-BEST based expert systems are well suited for real time misuse detection in contemporary computing environments. In addition, the simplicity of the P-BEST language and its close integration with the C programming language makes it easy to use while still being very powerful and flexible.","Computer networks,
Expert systems,
Intrusion detection,
Floods,
Measurement,
Operating systems,
Computer science,
Laboratories,
Engines,
Computer languages"
A simple and efficient rectification method for general motion,In this paper a new rectification method is proposed. The method is both simple and efficient and can deal with all possible camera motions. A minimal image size without any pixel loss is guaranteed. The only required information is the oriented fundamental matrix. The whole rectification process is carried out directly in the images. The idea consists of using a polar parametrization of the image around the epipole. The transfer between the images is obtained through the fundamental matrix. The proposed method has important advantages compared to the traditional rectification schemes. In some cases these approaches yield very large images or can not rectify at all. Even the recently proposed cylindrical rectification method can encounter problems in some cases. These problems are mainly due to the fact that the matching ambiguity is not reduced to half epipolar lines. Although this last method is more complex than the one proposed in this paper the resulting images are in general larger. The performance of the new approach is illustrated with some results on real image pairs.,"Cameras,
Pixel,
Image coding,
Information processing,
Computer science,
Hardware,
Calibration,
Head,
Geometry,
Layout"
Network emulation in the VINT/NS simulator,"Employing an emulation capability in network simulation provides the ability for real-world traffic to interact with a simulation. The benefits of emulation include the ability to expose experimental algorithms and protocols to live traffic loads, and to test real-world protocol implementations against repeatable interference generated in simulation. This paper describes the design and implementation of the emulation facility in the NS simulator a commonly-used publicly available network research simulator.","Emulation,
Intelligent networks,
Testing,
Traffic control,
Internet,
Computational modeling,
Software standards,
Multicast protocols,
Scheduling algorithm,
Computer science"
Online self-calibration for mobile robots,"This paper proposes a statistical method for calibrating the odometry of mobile robots. In contrast to previous approaches, which require explicit measurements of actual motion when calibrating a robot's odometry, the algorithm proposed here uses the robot's sensors to automatically calibrate the robot as it operates. An efficient, incremental maximum likelihood algorithm enables the robot to adapt to changes in its kinematics online, as they occur. The appropriateness of the approach is demonstrated in two large-scale environments, where the amount of odometric error is reduced by an order of magnitude.","Mobile robots,
Robot sensing systems,
Calibration,
Robotics and automation,
Orbital robotics,
Maximum likelihood estimation,
Motion measurement,
Life estimation,
Lifetime estimation,
Computer science"
A language and environment for architecture-based software development and evolution,"Software architectures have the potential to substantially improve the development and evolution of large, complex, multi-lingual, multi-platform, long-running systems. However, in order to achieve this potential, specific techniques for architecture-based modeling, analysis, and evolution must be provided. Furthermore, one cannot fully benefit from such techniques unless support for mapping an architecture to an implementation also exists. This paper motivates and presents one such approach, which is an outgrowth of our experience with systems developed and evolved according to the C2 architectural style. We describe an architecture description language (ADL) specifically designed to support architecture-based evolution and discuss the kinds of evolution the language supports. We then describe a component-based environment that enables modeling, analysis, and evolution of architectures expressed in the ADL, as well as mapping of architectural models to an implementation infrastructure. The architecture of the environment itself can be evolved easily to support multiple ADLs, kinds of analyses, architectural styles, and implementation platforms. Our approach is fully reflexive: the environment can be used to describe, analyze, evolve, and (partially) implement itself, using the very ADL it supports. An existing architecture is used throughout the paper to provide illustrations and examples.","Programming,
Software architecture,
Computer science,
Computer architecture,
Costs,
Connectors,
Architecture description languages,
Environmental economics,
Software tools,
Permission"
Hierarchical parallel coordinates for exploration of large datasets,"Our ability to accumulate large, complex (multivariate) data sets has far exceeded our ability to effectively process them in searching for patterns, anomalies and other interesting features. Conventional multivariate visualization techniques generally do not scale well with respect to the size of the data set. The focus of this paper is on the interactive visualization of large multivariate data sets based on a number of novel extensions to the parallel coordinates display technique. We develop a multi-resolution view of the data via hierarchical clustering, and use a variation of parallel coordinates to convey aggregation information for the resulting clusters. Users can then navigate the resulting structure until the desired focus region and level of detail is reached, using our suite of navigational and filtering tools. We describe the design and implementation of our hierarchical parallel coordinates system which is based on extending the XmdvTool system. Lastly, we show examples of the tools and techniques applied to large (hundreds of thousands of records) multivariate data sets.","Displays,
Data visualization,
Navigation,
Filtering,
Electrical capacitance tomography,
Computer science,
Large-scale systems,
Filters,
Visual databases,
Stacking"
Parallel hidden Markov models for American sign language recognition,"The major challenge that faces American Sign Language (ASL) recognition now is to develop methods that will scale well with increasing vocabulary size. Unlike in spoken languages, phonemes can occur simultaneously in ASL. The number of possible combinations of phonemes after enforcing linguistic constraints is approximately 5.5/spl times/10/sup 8/. Gesture recognition, which is less constrained than ASL recognition, suffers from the same problem. Thus, it is not feasible to train conventional hidden Markov models (HMMs) for large-scab ASL applications. Factorial HMMs and coupled HMMs are two extensions to HMMs that explicitly attempt to model several processes occuring in parallel. Unfortunately, they still require consideration of the combinations at training time. In this paper we present a novel approach to ASL recognition that aspires to being a solution to the scalability problems. It is based on parallel HMMs (PaHMMs), which model the parallel processes independently. Thus, they can also be trained independently, and do not require consideration of the different combinations at training time. We develop the recognition algorithm for PaHMMs and show that it runs in time polynomial in the number of states, and in time linear in the number of parallel processes. We run several experiments with a 22 sign vocabulary and demonstrate that PaHMMs can improve the robustness of HMM-based recognition even on a small scale. Thus, PaHMMs are a very promising general recognition scheme with applications in both gesture and ASL recognition.","Hidden Markov models,
Handicapped aids,
Speech recognition,
Humans,
Face recognition,
Information science,
Electronic switching systems,
Read only memory,
Ear,
Application software"
Anti-aliased three-dimensional cone-beam reconstruction of low-contrast objects with algebraic methods,"Examines the use of the algebraic reconstruction technique (ART) and related techniques to reconstruct 3-D objects from a relatively sparse set of cone-beam projections. Although ART has been widely used for cone-beam reconstruction of high-contrast objects, e.g., in computed angiography, the work presented here explores the more challenging low-contrast case which represents a little-investigated scenario for ART. Preliminary experiments indicate that for cone angles greater than 20/spl deg/, traditional ART produces reconstructions with strong aliasing artifacts. These artifacts are in addition to the usual off-midplane inaccuracies of cone-beam tomography with planar orbits. The authors find that the source of these artifacts is the nonuniform reconstruction grid sampling and correction by the cone-beam rays during the ART projection-backprojection procedure. A new method to compute the weights of the reconstruction matrix is devised, which replaces the usual constant-size interpolation filter by one whose size and amplitude is dependent on the source-voxel distance. This enables the generation of reconstructions free of cone-beam aliasing artifacts, at only little extra cost. An alternative analysis reveals that simultaneous ART (SART) also produces reconstructions without aliasing artifacts, however, at greater computational cost. Finally, the authors thoroughly investigate the influence of various ART parameters, such as volume initialization, relaxation coefficient /spl lambda/, correction scheme, number of iterations, and noise in the projection data on reconstruction quality. The authors find that ART typically requires only 3 iterations to render satisfactory reconstruction results.","Subspace constraints,
Angiography,
Tomography,
Orbits,
Sampling methods,
Transmission line matrix methods,
Interpolation,
Filters,
Costs,
Computational efficiency"
Synchrony and Desynchrony in Integrate-and-Fire Oscillators,"Due to many experimental reports of synchronous neural activity in the brain, there is much interest in understanding synchronization in networks of neural oscillators and its potential for computing perceptual organization. Contrary to Hopfield and Herz (1995), we find that networks of locally coupled integrate-and-fire oscillators can quickly synchronize. Furthermore, we examine the time needed to synchronize such networks. We observe that these networks synchronize at times proportional to the logarithm of their size, and we give the parameters used to control the rate of synchronization. Inspired by locally excitatory globally inhibitory oscillator network (LEGION) dynamics with relaxation oscillators (Terman & Wang, 1995), we find that global inhibition can play a similar role of desynchronization in a network of integrate-and-fire oscillators. We illustrate that a LEGION architecture with integrate-and-fire oscillators can be similarly used to address image analysis.",
Off-resonance correction of MR images,"In magnetic resonance imaging (MRI), the spatial inhomogeneity of the static magnetic field can cause degraded images if the reconstruction is based on inverse Fourier transformation. This paper presents and discusses a range of fast reconstruction algorithms that attempt to avoid such degradation by taking the field inhomogeneity into account. Some of these algorithms are new, others are modified versions of known algorithms. Speed and accuracy of all these algorithms are demonstrated using spiral MRI.","Magnetic resonance imaging,
Image reconstruction,
Magnetic fields,
Reconstruction algorithms,
Spirals,
Fourier transforms,
Degradation,
Image generation,
Algorithm design and analysis,
Mathematical model"
vUML: a tool for verifying UML models,"The Unified Modelling Language (UML) is a standardised notation for describing object oriented software designs. We present vUML, a tool that automatically verifies UML models where the behaviour of the objects is described using UML Statecharts diagrams. The tool uses the SPIN model checker to perform the verification, but the user does not have to know how to use SPIN or the PROMELA language. If an error is found during the verification, the tool creates a UML sequence diagram showing how to reproduce the error in the model.","Unified modeling language,
Object oriented modeling,
Software systems,
Computer science,
Electrical capacitance tomography,
Error analysis,
Counting circuits"
Identifying modules via concept analysis,"Describes a general technique for identifying modules in legacy code. The method is based on concept analysis - a branch of lattice theory that can be used to identify similarities among a set of objects based on their attributes. We discuss how concept analysis can identify potential modules using both ""positive"" and ""negative"" information. We present an algorithmic framework to construct a lattice of concepts from a program, where each concept represents a potential module. We define the notion of a concept partition, present an algorithm for discovering all concept partitions of a given concept lattice, and prove the algorithm to be correct.","Lattices,
Partitioning algorithms,
Information analysis,
Reverse engineering,
Software systems,
Computer languages,
Software tools,
Prototypes,
Computer science,
Queueing analysis"
Computing with words,"Computing with words is defined, in this paper, to be a symbolic generalization of fuzzy logic, which admits self-reference. It entails the randomization of declarative knowledge, which yields procedural knowledge. Such randomization can occur at two levels. First is termed weak randomization, which is essentially a domain-general pattern-matching operation. Second is termed strong randomization, which entails the application of one rule set to the semantics of another-possibly including itself. Strong randomization rests on top of weak randomization. Strong randomization is essentially a heuristic process. It is fully scalable, since it can in theory map out its own needed heuristics for evermore efficient search-including segmentation of the knowledge base. It is proven that strong learning must be knowledge-based, if effective. Computing with words does not preclude the use of predicate functions or procedural attachments. Also, the paradigm for computing with words does not directly compete with that for fuzzy logic. Rather, it serves to augment the utility of fuzzy logic through symbolic randomization. A countably infinite number of domain-specific logics or knowledge-based methods for randomization exist.",
Incremental learning methods with retrieving of interfered patterns,"There are many cases when a neural-network-based system must memorize some new patterns incrementally. However, if the network learns the new patterns only by referring to them, it probably forgets old memorized patterns, since parameters in the network usually correlate not only to the old memories but also to the new patterns. A certain way to avoid the loss of memories is to learn the new patterns with all memorized patterns. It needs, however, a large computational power. To solve this problem, we propose incremental learning methods with retrieval of interfered patterns (ILRI). In these methods, the system employs a modified version of a resource allocating network (RAN) which is one variation of a generalized radial basis function (GRBF). In ILRI, the RAN learns new patterns with a relearning of a few number of retrieved past patterns that are interfered with the incremental learning. We construct ILRI in two steps. In the first step, we construct a system which searches the interfered patterns from past input patterns stored in a database. In the second step, we improve the first system in such a way that the system does not need the database. In this case, the system regenerates the input patterns approximately in a random manner. The simulation results show that these two systems have almost the same ability, and the generalization ability is higher than other similar systems using neural networks and k-nearest neighbors.",
A new version of ant system for subset problems,"Early applications of Ant Colony Optimization (ACO) have been mainly concerned with solving ordering problems (e.g., traveling salesman problem). We introduce a new version of Ant System-an ACO algorithm for solving subset problems. The computational study involves the Multiple Knapsack Problem (MKP); the reported results show the potential power of the ACO approach for solving this type of subset problem.",
Finding maximal repetitions in a word in linear time,"A repetition in a word w is a subword with the period of at most half of the subword length. We study maximal repetitions occurring in w, that is those for which any extended subword of w has a bigger period. The set of such repetitions represents in a compact way all repetitions in w. We first prove a combinatorial result asserting that the sum of exponents of all maximal repetitions of a word of length n is bounded by a linear function in n. This implies, in particular that there is only a linear number of maximal repetitions in a word. This allows us to construct a linear-time algorithm for finding all maximal repetitions. Some consequences and applications of these results are discussed, as well as related works.",
Plagiarism in programming assignments,"The assessment of programming courses is usually carried out by means of programming assignments. Since it is simple to copy and edit computer programs, however, there will always be a temptation among some students following such courses to copy and modify the work of others. As the number of students in these courses is often high, it can be very difficult to detect this plagiarism. The authors have developed a package which will allow programming assignments to be submitted online, and which includes software to assist in detecting possible instances of plagiarism. In this paper, they discuss the concerns that motivated this work, describe the developed software, tailoring the software to different requirements and finally consider its implications for large group teaching.",
Temporal entity-relationship models-a survey,"The entity-relationship (ER) model, using varying notations and with some semantic variations, is enjoying a remarkable and increasing popularity in both the research community-the computer science curriculum-and in industry. In step with the increasing diffusion of relational platforms, ER modeling is growing in popularity. It has been widely recognized that temporal aspects of database schemas are prevalent and difficult to model using the ER model. As a result, how to enable the ER model to properly capture time-varying information has, for a decade and a half, been an active area in the database research community. This has led to the proposal of close to a dozen temporally enhanced ER models. This paper surveys all temporally enhanced ER models known to the authors. It provides a comprehensive overview of temporal ER modeling and it thus meets a need for consolidating and providing easy access to the research in temporal ER modeling. In the presentation of each model, the paper examines how the time-varying information is captured in the model and presents the new concepts and modeling constructs of the model. A total of 19 different design properties for temporally enhanced ER models are defined, and each model is characterized according to these properties.",
Tracking the left ventricle in echocardiographic images by learning heart dynamics,"In this paper a temporal learning-filtering procedure is applied to refine the left ventricle (LV) boundary detected by an active-contour model. Instead of making prior assumptions about the LV shape or its motion, this information is incrementally gathered directly from the images and is exploited to achieve more coherent segmentation. A Hough transform technique is used to find an initial approximation of the object boundary at the first frame of the sequence. Then, an active-contour model is used in a coarse-to-fine framework, for the estimation of a noisy LV boundary. The PCA transform is applied to form a reduced ordered orthonormal basis of the LV deformations based on a sequence of noisy boundary observations. Then this basis Is used to constrain the motion of the active contour in subsequent frames, and thus provide more coherent identification. Results of epicardial boundary identification in B-mode images are presented.",
Alias-free voxelization of geometric objects,"Introduces a new concept for alias-free voxelization of geometric objects based on a voxelization model (V-model). The V-model of an object is its representation in 3D continuous space by a trivariate density function. This function is sampled during the voxelization and the resulting values are stored in a volume buffer. This concept enables us to study general issues of sampling and rendering separately from object-specific design issues. It provides us with a possibility to design such V-models, which are correct from the point of view of both the sampling and rendering, thus leading to both alias-free volumetric representation and alias-free rendered images. We performed numerous experiments with different combinations of V-models and reconstruction techniques. We have shown that the V-model with a Gaussian surface density profile combined with tricubic interpolation and Gabor derivative reconstruction outperforms the previously published technique with a linear density profile. This enables higher fidelity of images rendered from volume data due to increased sharpness of edges and thinner surface patches.",
Multiple wavelet threshold estimation by generalized cross validation for images with correlated noise,"Denoising algorithms based on wavelet thresholding replace small wavelet coefficients by zero and keep or shrink the coefficients with absolute value above the threshold. The optimal threshold minimizes the error of the result as compared to the unknown, exact data. To estimate this optimal threshold, we use generalized cross validation. This procedure does not require an estimation for the noise energy. Originally, this method assumes uncorrelated noise. In this paper, we describe how we can extend it to images with correlated noise.",
Public speaking in virtual reality: facing an audience of avatars,"What happens when someone talks in public to an audience they know to be entirely computer generated-to an audience of avatars? If the virtual audience seems attentive, well-behaved, and interested, if they show positive facial expressions with complimentary actions such as clapping and nodding, does the speaker infer correspondingly positive evaluations of performance and show fewer signs of anxiety? On the other hand, if the audience seems hostile, disinterested, and visibly bored, if they have negative facial expressions and exhibit reactions such as head-shaking, loud yawning, turning away, falling asleep, and walking out, does the speaker infer correspondingly negative evaluations of performance and show more signs of anxiety? We set out to study this question during the summer of 1998. We designed a virtual public speaking scenario, followed by an experimental study. We wanted mainly to explore the effectiveness of virtual environments (VEs) in psychotherapy for social phobias. Rather than plunge straight in and design a virtual reality therapy tool, we first tackled the question of whether real people's emotional responses are appropriate to the behavior of the virtual people with whom they may interact. The project used DIVE (Distributive Interactive Virtual Environment) as the basis for constructing a working prototype of a virtual public speaking simulation. We constructed as a Virtual Reality Modeling Language (VRML) model, a virtual seminar room that matched the actual seminar room in which subjects completed their various questionnaires and met with the experimenters.",
Color edge detection with the compass operator,"The compass operator detects step edges without assuming that the regions on either side have constant color. Using distributions of pixel colors rather than the mean, the operator finds the orientation of a diameter that maximizes the difference between two halves of a circular window. Junctions can also be detected by exploiting their lack of bilateral symmetry. This approach is superior to a multi-dimensional gradient method in situations that often result in false negatives, and it localizes edges better as scale increases.",
Deformation analysis to detect and quantify active lesions in three-dimensional medical image sequences,"Evaluating precisely the temporal variations of lesion volumes is very important for at least three types of practical applications: pharmaceutical trials, decision making for drug treatment or surgery, and patient follow-up. Here, the authors present a volumetric analysis technique, combining precise rigid registration of three-dimensional (3-D) (volumetric) medical images, nonrigid deformation computation, and flow-field analysis. Their analysis technique has two outcomes: the detection of evolving lesions and the quantitative measurement of volume variations. The originality of the authors' approach is that no precise segmentation of the lesion is needed but the approximative designation of a region of interest (ROI) which can be automated. They distinguish between tissue transformation (image intensity changes without deformation) and expansion or contraction effects reflecting a change of mass within the tissue. A real lesion is generally the combination of both effects. The method is tested with synthesized volumetric image sequences and applied, in a first attempt to quantify in vivo a mass effect, to the analysis of a real patient case with multiple sclerosis (MS).",
Multi-frame optical flow estimation using subspace constraints,"Shows that the set of all flow fields in a sequence of frames imaging a rigid scene resides in a low-dimensional linear subspace. Based on this observation, we develop a method for simultaneous estimation of optical flow across multiple frames, which uses these subspace constraints. The multi-frame subspace constraints are strong constraints, and they replace commonly used heuristic constraints, such as spatial or temporal smoothness. The subspace constraints are geometrically meaningful and are not violated at depth discontinuities or when the camera motion changes abruptly. Furthermore, we show that the subspace constraints on flow fields apply for a variety of imaging models, scene models and motion models. Hence, the presented approach for constrained multi-frame flow estimation is general. However, our approach does not require prior knowledge of the underlying world or camera model. Although linear subspace constraints have been used successfully in the past for recovering 3D information, it has been assumed that 2D correspondences are given. However, correspondence estimation is a fundamental problem in motion analysis. In this paper, we use multi-frame subspace constraints to constrain the 2D correspondence estimation process itself, and not for 3D recovery.",
Scheduling fixed-priority tasks with preemption threshold,"In the context of fixed-priority scheduling, feasibility of a task set with non-preemptive scheduling does not imply the feasibility with preemptive scheduling and vice versa. We use the notion of preemption threshold, first introduced by Express Logic, in their ThreadX real-time operating system, to develop a scheduling model that subsumes both preemptive and non-preemptive fixed priority scheduling. Preemption threshold allows a task to only disable preemption of tasks up to a specified threshold priority. Tasks having priorities higher than the threshold are still allowed to preempt. With this new scheduling model, we show that schedulability is improved as compared to both the preemptive and nonpreemptive scheduling models. We develop the equations for computing the worst-case response times, using the concept of level-i busy period. Some useful results about the generalized model are presented and an algorithm for optimal assignment of priority and preemption threshold is designed based on these results.",
Mapping the Internet,"How can you determine what the Internet or even an intranet looks like? The answer is, of course, to draw it on screen. Once you can see the data succinctly, it becomes much easier to understand. The drawing itself can help locate bottlenecks and possible points of failure. Where is that newly acquired subsidiary connected? Which business units have connections to business partners? More important, visual displays of networks have another dimension-color. Color is an easy way to display link use, status, ownership, and network changes.",
Text-learning and related intelligent agents: a survey,"In surveying current research in the development of text-learning intelligent agents, the author focuses on three key criteria: what representation the particular application uses for documents, how it selects features, and what learning algorithm it uses. She then describes Personal WebWatcher, a content-based intelligent agent that uses text-learning for user-customized Web browsing.",
Non-malleable non-interactive zero knowledge and adaptive chosen-ciphertext security,"We introduce the notion of non-malleable non-interactive zero-knowledge (NIZK) proof systems. We show how to transform any ordinary NIZK proof system into one that has strong non-malleability properties. We then show that the elegant encryption scheme of Naor and Yung (1990) can be made secure against the strongest form of chosen-ciphertext attack by using a non-malleable NIZK proof instead of a standard NIZK proof. Our encryption scheme is simple to describe and works in the standard cryptographic model under, general assumptions. The encryption scheme can be realized assuming the existence of trapdoor permutations.","Security,
Public key cryptography,
Public key,
Laboratories,
Computer science,
Postal services,
US Department of Defense,
Privacy"
High-performance IP routing table lookup using CPU caching,"Wire-speed IP (Internet Protocol) routers require very fast routing table lookup for incoming IP packets. The routing table lookup operation is time consuming because the part of an IP address used in the lookup, i.e., the network address portion, is variable in length. This paper describes the routing table lookup algorithm used in a cluster-based parallel IP router project called Suez. The innovative aspect of this algorithm is its ability to use CPU caching hardware to perform routing table caching and lookup directly by carefully mapping IP addresses to virtual addresses. By running a detailed simulation model that incorporates the performance effects of the CPU memory hierarchy against a packet trace collected from a major network router, we show that the overall performance of the proposed algorithm can reach 87.87 million lookups per second for a 500-MHz Alpha processor with a 16-KByte L1 cache and a 1-MByte L2 cache. This result is one to two orders of magnitude faster than previously reported results on software-based routing table lookup implementations. This paper also reports the performance impacts of various architectural parameters in the proposed scheme and its storage costs, together with the measurements of an implementation of the proposed scheme on a Pentium-II machine running Linux.",
Temporal patterns (TRAPs) in ASR of noisy speech,"We study a new approach to processing temporal information for automatic speech recognition (ASR). Specifically, we study the use of rather long-time temporal patterns (TRAPs) of spectral energies in place of the conventional spectral patterns for ASR. The proposed neural TRAPs are found to yield significant amount of complementary information to that of the conventional spectral feature based ASR system. A combination of these two ASR systems is shown to result in improved robustness to several types of additive and convolutive environmental degradations.",
Using multivariate clustering to characterize ecoregion borders,"The authors present a geographic clustering technique which unambiguously locates, characterizes, and visualizes ecoregions and their borders. When coded with similarity colors, it can produce planar map views with sharpness contours that are visually rich in ecological information and represent integrated visualizations of complex and massive environmental data sets.",
A simple low-bandwidth broadcasting protocol for video-on-demand,"Broadcasting protocols can reduce the cost of video-on-demand services by using much less bandwidth to transmit videos that are simultaneously watched by many viewers. Unfortunately, the most efficient broadcasting protocols are also the most difficult to implement because they allocate a multitude of very low bandwidth streams to each video. We present a protocol that uses between three and seven high-bandwidth streams per video and never requires more than five percent more bandwidth than any other protocol to guarantee a given average waiting time.",
Parallel genetic algorithm taxonomy,"Genetic algorithms (GAs) are powerful search techniques that are used to solve difficult problems in many disciplines. Unfortunately, they can be very demanding in terms of computation load and memory. Parallel genetic algorithms (PGAs) are parallel implementations of GAs which can provide considerable gains in terms of performance and scalability. PGAs can easily be implemented on networks of heterogeneous computers or on parallel mainframes. We review the state of the art on PGAs and propose a new taxonomy also including a new form of PGA (the dynamic deme model) which was recently developed.",
Interactional coherence in CMC,"Text-only CMC has been claimed to be interactionally incoherent due to limitations imposed by messaging systems on turn-taking and reference, yet its popularity continues to grow. In an attempt to resolve this apparent paradox, this study evaluates the coherence of computer-mediated interaction by surveying research on cross-turn coherence. The results reveal a high degree of disrupted adjacency, overlapping exchanges, and topic decay. Two explanations are proposed to account for the popularity of CMC despite its relative incoherence: the ability of users to adapt to the medium, and the advantages of loosened coherence for heightened interactivity and language play.",
Fuzzy order statistics and their application to fuzzy clustering,"The median and the median absolute deviation (MAD) are robust statistics based on order statistics. Order statistics are extended to fuzzy sets to define a fuzzy median and a fuzzy MAD. The fuzzy c-means (FCM) clustering algorithm is defined for any p-norm (pFCM), including the l/sub 1/-norm (1FCM), The 1FCM clustering algorithm is implemented via the alternating optimization (AO) method and the clustering centers are shown to be the fuzzy median. The resulting AO-1FCM clustering algorithm is called the fuzzy c-medians (FCMED) clustering algorithm. An example illustrates the robustness of the FCMED.",
Activation detection in functional MRI using subspace modeling and maximum likelihood estimation,"A statistical method for detecting activated pixels in functional MRI (fMRI) data is presented. In this method, the fMRI time series measured at each pixel is modeled as the sum of a response signal which arises due to the experimentally controlled activation-baseline pattern, a nuisance component representing effects of no interest, and Gaussian white noise. For periodic activation-baseline patterns, the response signal is modeled by a truncated Fourier series with a known fundamental frequency but unknown Fourier coefficients. The nuisance subspace is assumed to be unknown. A maximum likelihood estimate is derived for the component of the nuisance subspace which is orthogonal to the response signal subspace. An estimate for the order of the nuisance subspace is obtained from an information theoretic criterion. A statistical test is derived and shown to be the uniformly most powerful (UMP) test invariant to a group of transformations which are natural to the hypothesis testing problem. The maximal invariant statistic used in this test has an F distribution. The theoretical F distribution under the null hypothesis strongly concurred with the experimental frequency distribution obtained by performing null experiments in which the subjects did not perform any activation task. Applications of the theory to motor activation and visual stimulation fMRI studies are presented.",
Classification of malignant and benign masses based on hybrid ART2LDA approach,"A new type of classifier combining an unsupervised and a supervised model was designed and applied to classification of malignant and benign masses on mammograms. The unsupervised model was based on an adaptive resonance theory (ART2) network which clustered the masses into a number of separate classes. The classes were divided into two types: one containing only malignant masses and the other containing a mix of malignant and benign masses. The masses from the malignant classes were classified by ART2. The masses from the mixed classes were input to a supervised linear discriminant classifier (LDA). In this way, some malignant masses were separated and classified by ART2 and the less distinguishable benign and malignant masses were classified by LDA. For the evaluation of classifier performance, 348 regions of interest (ROI's) containing biopsy proven masses (169 benign and 179 malignant) were used. Ten different partitions of training and test groups were randomly generated using an average of 73% of ROI's for training and 27% for testing. Classifier design, including feature selection and weight optimization, was performed with the training group. The test group was kept independent of the training group. The performance of the hybrid classifier was compared to that of an LDA classifier alone and a backpropagation neural network (BPN). Receiver operating characteristics (ROC) analysis was used to evaluate the accuracy of the classifiers. The average area under the ROC curve (A/sub z/) for the hybrid classifier was 0.81 as compared to 0.78 for the LDA and 0.80 for the BPN. The partial areas above a true positive fraction of 0.9 were 0.34, 0.27 and 0.31 for the hybrid, the LDA and the BPN classifier, respectively. These results indicate that the hybrid classifier is a promising approach for improving the accuracy of classification in CAD applications.",
A geometrical representation of McCulloch-Pitts neural model and its applications,"In this paper, a geometrical representation of McCulloch-Pitts neural model (1943) is presented, From the representation, a clear visual picture and interpretation of the model can be seen. Two interesting applications based on the interpretation are discussed. They are 1) a new design principle of feedforward neural networks and 2) a new proof of mapping abilities of three-layer feedforward neural networks.",
FDTD dispersion revisited: faster-than-light propagation,"The numerical dispersion relation that governs the propagation of fields in a finite-difference time-domain (FDTD) grid was derived several years ago. In this work a different interpretation is given for the governing equation. It is shown that the dispersion relation predicts faster-than-light propagation for coarsely resolved fields. Additionally, some spectral components that were previously believed to have zero phase velocity are shown to propagate, albeit with exponential decay.",
Starfish: fault-tolerant dynamic MPI programs on clusters of workstations,"This paper reports on the architecture and design of Starfish, an environment for executing dynamic (and static) MPI-2 programs on a cluster of workstations. Starfish is unique in being efficient, fault-tolerant, highly available, and dynamic as a system internally, and in supporting fault-tolerance and dynamicity for its application programs as well. Starfish achieves these goals by combining group communication technology with checkpoint/restart, and uses a novel architecture that is both flexible and portable and keeps group communication outside the critical data path, for maximum performance.",
A modeling technique for CMOS gates,"In this paper, a modeling technique for CMOS gates, based on the reduction of each gate to an equivalent inverter, is presented. The proposed method can be incorporated in existing timing simulators in order to improve their accuracy. The conducting and parasitic behavior of parallel and serially connected transistors is accurately analyzed and an equivalent transistor is extracted for each case, taking into account the actual operating conditions of each device in the structure. The proposed model incorporates short-channel effects, the influence of body effect and is developed for nonzero transition time inputs. The exact time point when the gate starts conducting is efficiently calculated improving significantly the accuracy of the method. A mapping algorithm for reducing every possible input pattern of a gate to an equivalent signal is introduced and the ""weight"" of each transistor position in the gate structure is extracted. Complex gates are treated by first mapping every possible structure to a NAND/NOR gate and then by collapsing this gate to an equivalent inverter. Results are validated by comparisons to SPICE and ILLIADS2 for three submicron technologies.",
"Branch prediction, instruction-window size, and cache size: performance trade-offs and simulation techniques","Design parameters interact in complex ways in modern processors, especially because out-of-order issue and decoupling buffers allow latencies to be overlapped. Trade-offs among instruction-window size, branch-prediction accuracy, and instruction- and data-cache size can change as these parameters move through different domains. For example, modeling unrealistic caches can under- or overstate the benefits of better prediction or a larger instruction window. Avoiding such pitfalls requires understanding how all these parameters interact. Because such methodological mistakes are common, this paper provides a comprehensive set of SimpleScalar simulation results from SPECint95 programs, showing the interactions among these major structures. In addition to presenting this database of simulation results, major mechanisms driving the observed trade-offs are described. The paper also considers appropriate simulation techniques when sampling full-length runs with the SPEC reference inputs. In particular, the results show that branch mispredictions limit the benefits of larger instruction windows, that better branch prediction and better instruction cache behavior have synergistic effects, and that the benefits of larger instruction windows and larger data caches trade off and have overlapping effects. In addition, simulations of only 50 million instructions can yield representative results if these short windows are carefully selected.",
A probabilistic roadmap approach for systems with closed kinematic chains,"We present a randomized approach to path planning for articulated robots that have closed kinematic chains. The approach extends the probabilistic roadmap technique which has previously been applied to rigid and elastic objects, and articulated robots without closed chains. It provides a framework for path planning problems that must satisfy closure constraints in addition to standard collision constraints. This expands the power of the probabilistic roadmap technique to include a variety of problems such as manipulation planning using two open-chain manipulators that cooperatively grasp an object, forming a system with a closed chain, and planning for reconfigurable robots where the robot links may be rearranged in a loop to ease manipulation or locomotion. We generate the vertices and edges in our probabilistic roadmap. We focus on the problem of planning the motions for a collection of attached links in a 2D environment with obstacles. The approach has been implemented and successfully demonstrated on several examples.","Kinematics,
Robots,
Motion planning,
Computer science,
Path planning,
Power system planning,
Manipulators,
Sampling methods,
Arm,
Animation"
Self-reconfiguration planning with compressible unit modules,We discuss a robotic system composed of crystalline modules. Crystalline modules can aggregate together to form distributed robot systems. Crystalline modules can move relative to each other by expanding and contracting. This actuation mechanism permits automated shape metamorphosis. We describe the crystalline module concept and show the basic motions that enable a crystalline robot system to self-reconfigure. We present an algorithm for general self-reconfiguration and describe simulation experiments.,
Use of ridge regression for improved estimation of kinetic constants from PET data,"The estimation of parameters in radio-tracer models from positron emission tomography (PET) data by nonlinear least squares (NLS) often leads to results with unacceptable mean square error (MSE) characteristics. The introduction of constraints on parameters has the potential to address this problem. We examine a ridge regression technique that augments the standard NLS criterion by the addition of a term which penalizes estimates which deviate from physiologically reasonable values. A variation on a plug-in methodology of Hoerl et al. (1975) is examined for data-dependent selection of the degree of reliance to place on the penalizing term. A simulation study is carried out to evaluate the performance of this approach in the context of estimation of kinetic constants in the three-compartment model used to analyze data from PET studies with fluoro-deoxyglucose (FDG). The results show that, over a range of realistic noise levels, the ridge regression procedure can be expected to reduce the root MSE of parameter estimates by 60%. This result is not found to be substantially dependent on the precise formulation of the penalty function used. Thus, the use of ridge regression for the estimation of kinetic parameters in PET studies is thus considered to be a promising tool.",
Generalized likelihood ratio detection for fMRI using complex data,"The majority of functional magnetic resonance imaging (fMRI) studies obtain functional information using statistical tests based on the magnitude image reconstructions. Recently, a complex correlation (CC) test was proposed based on the complex image data in order to take advantage of phase information in the signal. However, the CC test ignores additional phase information in the baseline component of the data. In this paper, a new detector for fMRI based on a generalized likelihood ratio test (GLRT) is proposed. The GLRT exploits the fact that the fMRI response signal as well as the baseline component of the data share a common phase. Theoretical analysis and Monte Carlo simulation are used to explore the performance of the new detector. At relatively low signal intensities, the GLRT outperforms both the standard magnitude data test and the CC test. At high signal intensities, the GLRT performs as well as the standard magnitude data test and significantly better than the CC test.","Testing,
Magnetic resonance imaging,
Detectors,
Blood,
Image reconstruction,
Signal to noise ratio,
High-resolution imaging,
Spatial resolution,
Signal resolution,
Statistical analysis"
Numerical simulation of macroscopic traffic equations,"The increasing need for efficient traffic optimization measures is making reliable, fast and robust methods for traffic simulation more and more important. Apart from developing cellular automata models of traffic flow, this need has stimulated studies of suitable numerical algorithms that can solve macroscopic traffic equations based on partial differential equations. The numerical integration of partial differential equations is a particularly difficult task, and there is no generally applicable method. In contrast to ordinary differential equations, the most natural explicit finite difference methods are often numerically unstable, even for very small discretizations of space and time. In general, numerical solutions to partial differential equations require special methods, which work only under certain conditions. Implicit integration methods are usually more stable but they require the frequent solution of linear systems with multidiagonal matrices. In this article, we discuss only explicit methods, because they are useful for the varying boundary conditions found in realistic traffic simulations, where data is continuously fed into the simulation. In addition, explicit methods are more flexible for the simulation of on- and off-ramps or entire road networks.","Numerical simulation,
Traffic control,
Partial differential equations,
Differential equations,
Optimization methods,
Robustness,
Finite difference methods,
Linear systems,
Boundary conditions,
Telecommunication traffic"
FingerCode: a filterbank for fingerprint representation and matching,"With the identity fraud in our society reaching unprecedented proportions and with an increasing emphasis on the emerging automatic positive personal identification applications, biometrics-based identification, especially fingerprint-based identification, is receiving a lot of attention. There are two major shortcomings of the traditional approaches to fingerprint representation. For a significant fraction of population, the representations based on explicit detection of complete ridge structures in the fingerprint are difficult to extract automatically. The widely used minutiae-based representation does not utilize a significant component of the rich discriminatory information, available in the fingerprints. The proposed filter-based algorithm uses a bank of Gabor filters to capture both the local and the global details in a fingerprint as a compact 640-byte fixed length FingerCode. The fingerprint matching is based on the Euclidean distance between the two corresponding FingerCodes and hence is extremely fast. Our initial results show identification accuracies comparable to the best results of minutiae-based algorithms published in the open literature. Finally, we show that the matching performance can be improved by combining the decisions of the matchers based on complementary fingerprint information.",
Software engineering programs are not computer science programs,"Software Engineering programs have become a source of contention in many universities. Computer Science departments, many of which have used that phrase to describe individual courses for decades, claim SE as part of their discipline. Yet some engineering faculties claim it as a new specialty among the engineering disciplines. This article discusses the differences between traditional CS programs and most engineering programs, and argues that we need SE programs that follow the traditional engineering approach to professional education.","Software engineering,
Computer science,
Educational programs,
Computer science education,
Power engineering and energy,
Design engineering,
Physics education,
Educational products,
Educational institutions,
Chemical engineering"
Simplified representation of vector fields,"Vector field visualization remains a difficult task. Many local and global visualization methods for vector fields such as flow data exist, but they usually require extensive user experience on setting the visualization parameters in order to produce images communicating the desired insight. We present a visualization method that produces simplified but suggestive images of the vector field automatically, based on a hierarchical clustering of the input data. The resulting clusters are then visualized with straight or curved arrow icons. The presented method has a few parameters with which users can produce various simplified vector field visualizations that communicate different insights on the vector data.","Data visualization,
Computational fluid dynamics,
Displays,
Weather forecasting,
Streaming media,
Mathematics,
Computer science,
Computational modeling,
Data engineering,
Fluid flow"
The hybrid tree: an index structure for high dimensional feature spaces,"Feature-based similarity searching is emerging as an important search paradigm in database systems. The technique used is to map the data items as points into a high-dimensional feature space which is indexed using a multidimensional data structure. Similarity searching then corresponds to a range search over the data structure. Although several data structures have been proposed for feature indexing, none of them is known to scale beyond 10-15 dimensional spaces. This paper introduces the hybrid tree-a multidimensional data structure for indexing high-dimensional feature spaces. Unlike other multidimensional data structures, the hybrid tree cannot be classified as either a pure data partitioning (DP) index structure (such as the R-tree, SS-tree or SR-tree) or a pure space partitioning (SP) one (such as the KDB-tree or hB-tree); rather it combines the positive aspects of the two types of index structures into a single data structure to achieve a search performance which is more scalable to high dimensionalities than either of the above techniques. Furthermore, unlike many data structures (e.g. distance-based index structures like the SS-tree and SR-tree), the hybrid tree can support queries based on arbitrary distance functions. Our experiments on ""real"" high-dimensional large-size feature databases demonstrate that the hybrid tree scales well to high dimensionality and large database sizes. It significantly outperforms both purely DP-based and SP-based index mechanisms as well as linear scans at all dimensionalities for large-sized databases.","Data structures,
Computer science,
Tree data structures,
Database systems,
Strontium,
Indexing,
Laboratories,
NASA,
Software libraries"
The segmentation of cursive handwriting: an approach based on off-line recovery of the motor-temporal information,"This paper presents a segmentation method that partly mimics the cognitive-behavioral process used by human subjects to recover motor-temporal information from the image of a handwritten word. The approach does not exploit any thinning or skeletonization procedure, but rather a different type of information is manipulated concerning the curvature function of the word contour. In this way, it is possible to detect the parts of the image where the original odometric information is lost or ambiguous (such as, for example, at an intersection of the handwritten lines) and interpret them to finally recover a part of the original temporal information. The algorithm scans the word, following the natural course of the line, and attempts to reproduce the same movement as executed by the writer during the generation of the word. It segments the cursive trace where the contour shows the slow-down of the original movement (corresponding to the maximum curvature points of the curve). At the end of the scanning process, a temporal sequence of motor strokes is obtained which plausibly composed the original intended movement.","Shape,
Image segmentation,
Humans,
Angular velocity,
Graphics,
Process planning,
Iris,
Computer science,
Nervous system"
Scheduling multimedia services in a low-power MAC for wireless and mobile ATM networks,"This paper describes the design and analysis of the scheduling algorithm for energy conserving medium access control (EC-MAC), which is a low-power medium access control (MAC) protocol for wireless and mobile ATM networks. We evaluate the scheduling algorithms that have been proposed for traditional ATM networks. Based on the structure of EC-MAC and the characteristics of wireless channel, we propose a new algorithm that can deal with the burst errors and the location-dependent errors. Most scheduling algorithms proposed for either wired or wireless networks were analyzed with homogeneous traffic or multimedia services with simplified traffic models. We analyze our scheduling algorithm with more realistic multimedia traffic models based on H.263 video traces and self-similar data traffic. One of the key goals of the scheduling algorithms is simplicity and fast implementation. Unlike the time-stamped based algorithms, our algorithm does not need to sort the virtual time, and thus, the complexity of the algorithm is reduced significantly.",
A theory of rate-based execution,"We present a task model for the real-time execution of event-driven tasks in which no a priori characterization of the actual arrival rates of events is known; only the expected arrival rates of events is known. The model, called rate-bared execution (RBE), is a generalization of Mok's sporadic task model. The RBE model is motivated naturally by distributed multimedia and digital signal processing applications. We derive necessary and sufficient conditions for determining the feasibility of an RBE task set and demonstrate that earliest deadline first (EDF) scheduling is an optimal scheduling algorithm for both preemptive and nonpreemptive execution environments, as well as hybrid environments wherein RBE tasks access shared resources. Our analysis of RBE tasks demonstrates a fundamental distinction between deadline based scheduling methods and static priority based methods. We show that for deadline-based scheduling methods, feasibility is solely a function of the distribution of task deadlines in time. This is contrasted with static priority schedulers where feasibility is a function of the actual arrival rates of work for tasks. Thus whereas the feasibility of static priority schedulers is a function of the periodicity of tasks, the feasibility of deadline schedulers is independent of task arrival processes and hence deadline schedulers are more suitable for use in distributed, event-driven, real-time systems.",
A meta-notation for protocol analysis,"Most formal approaches to security protocol analysis are based on a set of assumptions commonly referred to as the ""Dolev-Yao model"". In this paper, we use a multiset rewriting formalism, based on linear logic, to state the basic assumptions of this model. A characteristic of our formalism is the way that existential quantification provides a succinct way of choosing new values, such as new keys or nonces. We define a class of theories in this formalism that correspond to finite-length protocols, with a bounded initialization phase but allowing unboundedly many instances of each protocol role (e.g., client, sewer; initiator or responder). Undecidability is proved for a restricted class of these protocols, and PSPACE-completeness is claimed for a class further restricted to have no new data (nonces). Since it is a fragment of linear logic, we can use our notation directly as input to linear logic tools, allowing us to do proof search for attacks with relatively little programming effort, and to formally verify protocol transformations and optimizations.",
A two-tier resource management model for the Internet,"In this paper we propose a two-tier resource management model for the global Internet. Our solution resembles the current two-tier routing hierarchy and allows individual administrative domains to independently make their own decisions on strategies and protocols to use for internal resource management and QoS support. The aggregate traffic crossing domain borders is served according to relatively stable, long-lived bilateral agreements. End-to-end QoS support is achieved through the concatenation of such bilateral agreements. We describe in detail a realization of this two-tier model, where a bandwidth broker (BB) acts as the resource manager for each administrative domain, neighboring bandwidth brokers communicate with each other to establish inter-domain resource agreements. As an illustrative example in this paper we used a simplified RSVP as an intradomain resource allocation protocol for the aggregate traffic between border routers. Our simulation results show that this two-tier design can provide effective end-to-end QoS support for user applications.",
Fairness in routing and load balancing,"We consider the issue of network routing subject to explicit fairness conditions. The optimization of fairness criteria interacts in a complex fashion with the optimization of network utilization and throughput; in this work, we undertake an investigation of this relationship through the framework of approximation algorithms. In this work we consider the problem of selecting paths for routing so as to provide a bandwidth allocation that is as fair as possible (in the max-min sense). We obtain the first approximation algorithms for this basic optimization problem, for single-source unsplittable routings in an arbitrary directed graph. Special cases of our model include several fundamental load balancing problems, endowing them with a natural fairness criterion to which our approach can be applied. Our results form an interesting counterpart to the work of Megiddo (1974), who considered max-min fairness for single-source fractional flow. The optimization problems in our setting become NP-complete, and require the development of new techniques for relating fractional relaxations of routing to the equilibrium constraints imposed by the fairness criterion.",
Dynamic metrics for object oriented designs,"As object-oriented (OO) analysis and design techniques become more widely used, the demand on assessing the quality of OO designs increases substantially. Recently, there has been much research effort devoted to developing and empirically validating metrics for OO design quality. Complexity, coupling, and cohesion have received a considerable interest in the field. Despite the rich body of research and practice in developing design quality metrics, there has been less emphasis on dynamic metrics for OO designs. The complex dynamic behavior of many real-time applications motivates a shift in interest from traditional static metrics to dynamic metrics. This paper addresses the problem of measuring the quality of OO designs using dynamic metrics. We present a metrics suite to measure the quality of designs at an early development phase. The suite consists of metrics for dynamic complexity and object coupling based on execution scenarios. The proposed measures are obtained from executable design models. We apply the dynamic metrics to assess the quality of a pacemaker application. Results from the case study are used to compare static metrics to the proposed dynamic metrics and hence identify the need for empirical studies to explore the dependency of design quality on each.",
On the verification of broadcast protocols,"We analyze the model-checking problems for safety and liveness properties in parameterized broadcast protocols. We show that the procedure suggested previously for safety properties may not terminate, whereas termination is guaranteed for the procedure based on upward closed sets. We show that the model-checking problem for liveness properties is undecidable. In fact, even the problem of deciding if a broadcast protocol may exhibit an infinite behavior is undecidable.",
Nonactive antenna compensation for fixed-array microwave imaging. I. Model development,"Fixed-array microwave imaging with multisensor data acquisition can suffer from nonactive antenna element interactions which cause distortions in the measurements. In Part I of a two-part paper, the authors develop a nonactive antenna compensation model for incorporation in model-based near-field microwave image reconstruction methods. The model treats the nonactive members of the antenna array as impedance boundary conditions applied over a cylindrical surface of finite radius providing two parameters, the effective antenna radius and impedance factor, which can be determined empirically from measured data. Results show that the effective radius and impedance factor provide improved fits to experimental data in homogeneous phantoms where measurements are obtained with and without the presence of the nonactive antenna elements. Once deduced, these parameters are incorporated into the nonactive antenna compensation model and lead to systematic data-model match improvements in heterogeneous phantoms. While the improvements afforded by the nonactive antenna model are small on a per measurement basis, they are not insignificant. As shown in Part II (see ibid., vol. 18, no. 6, p. 508, 1999), inclusion of this new model for nonactive antenna compensation produces significantly higher quality image reconstructions from measurements obtained with a fixed-array data acquisition system over the frequency band 500-900 MHz.",
An algorithmic theory of learning: robust concepts and random projection,"We study the phenomenon of cognitive learning from an algorithmic standpoint. How does the brain effectively learn concepts from a small number of examples despite the fact that each example contains a huge amount of information? We provide a novel analysis for a model of robust concept learning (closely related to ""margin classifiers""), and show that a relatively small number of examples are sufficient to learn rich concept classes (including threshold functions, Boolean formulae and polynomial surfaces). As a result, we obtain simple intuitive proofs for the generalization bounds of Support Vector Machines. In addition, the new algorithm has several advantages-they are faster conceptually simpler and highly resistant to noise. For example, a robust half-space can be PAC-learned in linear time using only a constant number of training examples, regardless of the number of attributes. A general (algorithmic) consequence of the model, that ""more robust concepts are easier to learn"", is supported by a multitude of psychological studies.",
"Reconfigurable computing: what, why, and implications for design automation","Reconfigurable computing is emerging as an important new organizational structure for implementing computations. It combines the post-fabrication programmability of processors with the spatial computational style most commonly employed in hardware designs. The result changes traditional ""hardware"" and ""software"" boundaries, providing an opportunity for greater computational capacity and density within a programmable media. Reconfigurable computing must leverage traditional CAD technology for building spatial designs. Beyond that, however, reprogrammability introduces new challenges and opportunities for automation, including binding-time and specialization optimizations, regularity extraction and exploitation, and temporal partitioning and scheduling.",
Electrical conductivity imaging via contactless measurements,"A new imaging modality is introduced to image electrical conductivity of biological tissues via contactless measurements. This modality uses magnetic excitation to induce currents inside the body and measures the magnetic fields of the induced currents. In this study, the mathematical basis of the methodology is analyzed and numerical models are developed to simulate the imaging system. The induced currents are expressed using the A/spl I.oarr/-/spl phi/ formulation of the electric field where A/spl I.oarr/ is the magnetic vector potential and /spl phi/ is the scalar potential function. It is assumed that A/spl I.oarr/ describes the primary magnetic vector potential that exists in the absence of the body. This assumption considerably simplifies the solution of the secondary magnetic fields caused by induced currents. In order to solve /spl phi/ for objects of arbitrary conductivity distribution a three-dimensional (3-D) finite-element method (FEM) formulation is employed. A specific 7/spl times/7-coil system is assumed nearby the upper surface of a 10/spl times/10/spl times/5-cm conductive body. A sensitivity matrix, which relates the perturbation in measurements to the conductivity perturbations, is calculated. Singular-value decomposition of the sensitivity matrix shows various characteristics of the imaging system. Images are reconstructed using 500 voxels in the image domain, with truncated pseudoinverse. The noise level is assumed to produce a representative signal-to-noise ratio (SNR) of 80 dB. It is observed that it is possible to identify voxel perturbations (of volume 1 cm/sup 3/) at 2 cm depth. However, resolution gradually decreases for deeper conductivity perturbations.",
Worst case total dose radiation response of 0.35 /spl mu/m SOI CMOSFETs,"Through experimental results and analysis by TSUPREM4/MEDICI simulations, the worst case back gate total dose bias condition is established for body tied SOI NMOSFETs. Utilizing the worst-case bias condition, a recently proposed model that describes the back n-channel threshold voltage shift as a function of total dose, TSUPREM4/MEDICI simulations, and circuit level SPICE simulations, a methodology to model post-rad standby current is developed and presented. This methodology requires the extraction of fundamental starting material/material preparation constants, and then can be utilized to examine post-rad stand-by current at the device and circuit level as function of total dose. Good agreement between experimental results and simulations is demonstrated.",
A kurtosis-based dynamic approach to Gaussian mixture modeling,"We address the problem of probability density function estimation using a Gaussian mixture model updated with the expectation-maximization (EM) algorithm. To deal with the case of an unknown number of mixing kernels, we define a new measure for Gaussian mixtures, called total kurtosis, which is based on the weighted sample kurtoses of the kernels. This measure provides an indication of how well the Gaussian mixture fits the data. Then we propose a new dynamic algorithm for Gaussian mixture density estimation which monitors the total kurtosis at each step of the EM algorithm in order to decide dynamically on the correct number of kernels and possibly escape from local maxima. We show the potential of our technique in approximating unknown densities through a series of examples with several density estimation problems.",
Information measures in scale-spaces,"This article investigates Renyi's (1976) generalized entropies under linear and nonlinear scale-space evolutions of images. Scale-spaces are useful computer vision concepts for both scale analysis and image restoration. We regard images as densities and prove monotony and smoothness properties for the generalized entropies. The scale-space extended generalized entropies are applied to global scale selection and size estimations. Finally, we introduce an entropy-based fingerprint description for textures.",
A framework for online learning: the Virtual-U,"Much of the online post-secondary education available in North America and Europe has been created piecemeal. This situation arose because educators began adopting computer networking in the mid-1970s, soon after the invention of packet-switched networks (1969) and e-mail and computer conferencing (1971) for exchange of scientific information. In late 1993, the author set out to help design a system using the Internet that would encourage the adoption of a collaborative learning approach. She and her colleagues also wanted to develop embedded tools to meet the needs of both instructors and students. The goal of their system, now known as the Virtual-U (http://www.vu.vlei.com), was to provide a flexible framework to support advanced pedagogies based on active learning, collaboration, multiple perspectives, and knowledge building. With two years of field trials serving more than 8000 students and hosting 300 courses from 14 institutions, the Virtual-U provides a flexible yet well-organized framework for online, collaborative education. It brings together a multidisciplinary research team of educators, HCI specialists, engineers, computer scientists, and database and instructional designers, fulfilling the promise of integrated online learning.",
Exact rebinning methods for three-dimensional PET,"The high computational cost of data processing in volume PET imaging is still hindering the routine application of this successful technique, especially in the case of dynamic studies. This paper describes two new algorithms based on an exact rebinning equation, which can be applied to accelerate the processing of three-dimensional (3-D) PET data. The first algorithm, FOREPROJ, is a fast-forward projection algorithm that allows calculation of the 3-D attenuation correction factors (ACFs) directly from a two dimensional (2-D) transmission scan, without first reconstructing the attenuation map and then performing a 3-D forward projection. The use of FOREPROJ speeds up the estimation of the 3-D ACFs by more than a factor five. The second algorithm, FOREX, is a rebinning algorithm that is also more than five times faster, compared to the standard reprojection algorithm (3DRP) and does not suffer from the image distortions generated by the even faster approximate Fourier rebinning (FORE) method at large axial apertures. However, FOREX is probably not required by most existing scanners, as the axial apertures are not large enough to show improvements over FORE with clinical data. Both algorithms have been implemented and applied to data simulated for a scanner with a large axial aperture (30/spl deg/), and also to data acquired with the ECAT HR and the ECAT HR+ scanners. Results demonstrate the excellent accuracy achieved by these algorithms and the important speedup when the sinogram sizes are powers of two.",
Abstract syntax and variable binding,"We develop a theory of abstract syntax with variable binding. To every binding signature we associate a category of models consisting of variable sets endowed with compatible algebra and substitution structures. The syntax generated by the signature is the initial model. This gives a notion of initial algebra semantics encompassing the traditional one; besides compositionality, it automatically verifies the semantic substitution lemma.",
A spatiotemporal model of cyclic kinematics and its application to analyzing nonrigid motion with MR velocity images,"The authors present a method (DMESH) for nonrigid cyclic motion analysis using a series of velocity images covering the cycle acquired, for example, from phase-contrast magnetic resonance imaging. The method is based on fitting a dynamic finite-element mesh model to velocity samples of an extended region, at all time frames. The model offers a flexible tradeoff between accuracy and reproducibility with controllable built-in spatiotemporal smoothing, which is determined by the fineness of the initially defined mesh and the richness of included Fourier harmonics. The method can further provide a prediction of the analysis reproducibility, along with the estimated motion and deformation quantities. Experiments have been conducted to validate the method and to verify the reproducibility prediction. Use of the method for motion analysis using displacement information (e.g., from magnetic resonance tagging) has also been explored.",
Determination of parameters of synchronous motor with permanent magnets from measurement of load conditions,The paper describes the determination of the parameters of the two-axis model of a synchronous motor with permanent magnets using the results of the measurement of load conditions at different sine voltages and constant frequency. For the determination of parameters of the two-axis model the modified conventional method was used. The machine parameters are determined from measurement data by the results of the calculation of the induced voltage E/sub i/ and internal load angle /spl delta//sub i/. Computational difficulties at parameters determination owing to random inaccuracies in the measurements data are overcome with use of orthogonal polinoms approximation. Detailed determination of parameters values reveal an important influence of the interaction of the direct and quadrature axis excitation on the parameters variation.,
UML-based analysis of embedded systems using a mapping to VHDL,"Methods for developing and modeling embedded systems and rigorously verifying behavior before committing to code are increasingly important. A number of object-oriented techniques and notations have been introduced but recently, it appears that the Unified Modeling Language (UML) could be a notation broad enough in scope to represent a variety of domains and gain widespread use. Currently, however, UML is only a notation, with no formal semantics attached to the individual diagrams. In order to address this problem, we have developed a framework for deriving VHDL specifications from the class and state diagrams in order to capture the structure and the behavior of embedded systems. The derived VHDL specifications enable us to perform behavior simulation of the UML models.",
A new approach to abstract syntax involving binders,"The Fraenkel-Mostowski permutation model of set theory with atoms (FM-sets) can serve as the semantic basis of meta-logics for specifying and reasoning about formal systems involving name binding, /spl alpha/-conversion, capture avoiding substitution, and so on. We show that in FM-set theory one can express statements quantifying over 'fresh' names and we use this to give a novel set-theoretic interpretation of name abstraction. Inductively defined FM-sets involving this name abstraction set former (together with cartesian product and disjoint union) can correctly encode object-level syntax module e-conversion. In this way, the standard theory of algebraic data types can be extended to encompass signatures involving binding operators. In particular, there is an associated notion of structural recursion for defining syntax-manipulating functions (such as capture avoiding substitution, set of free variables, etc.) and a notion of proof by structural induction, both of which remain pleasingly close to informal practice.",
"Hysteresis, avalanches, and noise","In our studies of hysteresis and avalanches in the zero-temperature random-field Ising model, a simple model of magnetism, we often have had to do very large simulations. Previous simulations were usually limited to relatively small systems (up to 900/sup 2/ and 128/sup 3/), although there have been exceptions. In our simulations, we have found that larger systems (up to a billion spins) are crucial to extracting accurate values of the critical exponents and understanding important qualitative features of the physics. We show three algorithms for simulating these large systems. The first uses the brute-force method, which is the standard method for avalanche-propagation problems. This algorithm is simple but inefficient. We have developed two efficient and relatively straightforward algorithms that provide better results. The sorted-list algorithm decreases execution time, but requires considerable storage. The bits algorithm has an execution time that is similar to that of the sorted-list algorithm, but it requires far less storage.",
Factors systematically associated with errors in subjective estimates of software development effort: the stability of expert judgment,"Estimation of project development effort is most often performed by expert judgment rather than by using an empirically derived model (although such may be used by the expert to assist their decision). One question that can be asked about these estimates is how stable are they with respect to characteristics of the development process and product? This stability can be assessed in relation to the degree to which the project has advanced over time, the type of module for which the estimate is being made, and the characteristics of that module. In this paper we examine a set of expert-derived estimates for the effort required to develop a collection of modules from a large health-care system. Statistical tests are used to identify relationships between the type (screen or report) and characteristics of modules and the likelihood of the associated development effort being underestimated, approximately correct, or over-estimated. Distinct relationships are found that suggest that the estimation process being examined was not unbiased to such characteristics. This is a potentially useful finding in that it provides an opportunity for estimators to improve their prediction performance.",
Yet another result on multi-log/sub 2/N networks,"One-to-many connection (i.e., multicast) is an important communication primitive used in parallel processing and high-speed switching in order to simultaneously send data from an input to more than one output. We prove that for even (respectively, odd) n, a multi-log/sub 2/N network is strictly nonblocking for a one-to-many connection traffic if it is designed by vertically stacking at least (/spl delta/n)/4+1((/spl delta//2)(n-1)+1) planes of a log/sub 2/N network together, where N=2/sup n/, /spl delta/=2/sup [n/2]/, and [x] denotes the greatest integer less than or equal to x. We thus give answer to the open problem and introduce yet another strictly nonblocking multicast network. The characterized network has self-routing capability, regular topology, O(2log/sub 2/N+2log/sub 2/(log/sub 2/N)) stages, and fewer crosspoints than the Clos network for N/spl ges/512. We then extend multi log/sub 2/N multicast networks to the fanout restricted nonblocking networks. It turns out that the multi-log/sub 2/N network nonblocking in a strict-sense for a one-to-one connection traffic is also wide-sense nonblocking for a multicast traffic in which the fanout of any connection does not exceed /spl delta/, provided that for even (respectively, odd) n, the fanout capability of each log/sub 2/N network is restricted to stage (n/2)(((n-1)/2)+1) through n-1.",
Clustering large datasets in arbitrary metric spaces,"Clustering partitions a collection of objects into groups called clusters, such that similar objects fall into the same group. Similarity between objects is defined by a distance function satisfying the triangle inequality; this distance function along with the collection of objects describes a distance space. In a distance space, the only operation possible on data objects is the computation of distance between them. All scalable algorithms in the literature assume a special type of distance space, namely a k-dimensional vector space, which allows vector operations on objects. We present two scalable algorithms designed for clustering very large datasets in distance spaces. Our first algorithm BUBBLE is, to our knowledge, the first scalable clustering algorithm for data in a distance space. Our second algorithm BUBBLE-FM improves upon BUBBLE by reducing the number of calls to the distance function, which may be computationally very expensive. Both algorithms make only a single scan over the database while producing high clustering quality. In a detailed experimental evaluation, we study both algorithms in terms of scalability and quality of clustering. We also show results of applying the algorithms to a real life dataset.",
Searching for optima in non-stationary environments,Application of evolutionary algorithms to non-stationary problems is the subject of research discussed. We extended evolutionary algorithm by two mechanisms dedicated to non-stationary optimization: redundant genetic memory structures and a particular diversity maintenance technique-random immigrants mechanism. We made experiments with evolutionary optimization employing these two mechanisms (separately and together); the results of experiments are discussed and some observations are made.,
A robust software barcode reader using the Hough transform,"Nowadays barcodes are used in many different applications and environments. For most applications, such as access control, price calculation, etc., a handheld scanner is enough, but in other environments where the volume of information is very high and time is critical, hardware scanners aren't the best choice. In such situations, a powerful software barcode reader can process the barcode readers present in a scanned document without human interaction. The most commonly approach used to implement this kind of software scanners is to simulate the handheld scanner behavior by tracing one or more lines (the hardware laser beam) and measure the width of the barcode's lines and spaces. These methods present a serious handicap, since they are highly sensitive to eventual noise (human signatures, marks) that can be present in a code. In this paper we present a method based on the Hough transform which solves the aforementioned problem, and that can be easily adapted to read any 1D barcode.",
Parallel classification for data mining on shared-memory multiprocessors,Presents parallel algorithms for building decision-tree classifiers on shared-memory multiprocessor (SMP) systems. The proposed algorithms span the gamut of data and task parallelism. The data parallelism is based on attribute scheduling among processors. This basic scheme is extended with task pipelining and dynamic load balancing to yield faster implementations. The task-parallel approach uses dynamic subtree partitioning among processors. Our performance evaluation shows that the construction of a decision-tree classifier can be effectively parallelized on an SMP machine with good speedup.,
QoS provisioning with qContracts in web and multimedia servers,"The advent of performance-critical services such as online brokerage and e-commerce, as well as QoS-sensitive services such as streaming multimedia, makes existing FIFO servers incapable of meeting application QoS requirements. Re-designing server code to support QoS provisioning, on the other hand, is costly and time-consuming. To remedy this problem, we propose a new QoS-provisioning approach that does not require modification of server and QoS code. We develop a middleware, called qContracts, that can be transparently interposed between the server process and the operating system to achieve performance differentiation and soft QoS guarantees. The middleware enables reuse of existing legacy software in QoS-sensitive contexts, and off-loads QoS management concerns from future real-time service programmers. As an example, we show how the Apache web server is endowed with QoS support using qContracts on UNIX. Experimental results show the efficacy of the middleware in achieving the contracted QoS, while imposing less than 1% overhead.","Middleware,
Sockets,
Operating systems,
Programming profession,
Contracts,
Computer science,
Laboratories,
Argon,
Business,
Context-aware services"
Time optimal control of hybrid systems,"We discuss optimal control for hybrid systems. First, we present a general formulation of an optimal control problem for hybrid systems. Such a formulation allows us to use straightaway the maximum principle of Pontryagin. We focus on time optimal control for hybrid systems defined by a finite family of controlled linear systems and show how switching time can be achieved. An illustrative example ends the paper.",
"A characterization of the geometric architecture of the peritalar joint complex via MRI, an aid to classification of foot type","The purpose of this work is to study the architecture of the rearfoot using in vivo MR image data. Each data set used in this study is made of sixty sagittal slices of the foot acquired in a 1.5-T commercial GE MR system. The authors use the live-wire method to delineate boundaries and form the surfaces of the bones. In the first part of this work, they describe a new method to characterize the three-dimensional (3-D) relationships of four bones of the peritalar complex and apply this description technique to data sets from ten normal subjects and from seven pathological cases. In the second part, the authors propose a procedure to classify feet, based on the values of these new architectural parameters. They conclude that this noninvasive method offers a unique tool to characterize the 3-D architecture of the feet in live patients, based on a set of new architectural parameters. This can be integrated into a set of tools to improve diagnosis and treatment of foot malformations.",
Efficient code constructions for certain two-dimensional constraints,"Efficient encoding algorithms are presented for two types of constraints on two-dimensional binary arrays. The first constraint considered is that of t-conservative arrays, where each row and each column has at least t transitions of the form '0'/spl rarr/'1' or '1'/spl rarr/'0.' The second constraint is that of two-dimensional DC-free arrays, where in each row and each column the number of '0's equals the number of '1's.",
Parametric quantitative temporal reasoning,"We define Parameterized Real-Time Computation Tree Logic (PRTCTL), which allows quantitative temporal specifications to be parameterized over the natural numbers. Parameterized quantitative specifications are quantitative specifications in which concrete timing information has been abstracted away. Such abstraction allows designers to specify quantitative restrictions on the temporal ordering of events without having to use specific timing information from the model. A model checking algorithm for the logic is given which is polynomial for any fixed number of parameters. A subclass of formulae are identified for which the model checking problem is linear in the length of the formula and size of the structure. PRTCTL is generalised to allow quantitative reasoning about the number of occurrences of atomic events.",
Online scheduling to minimize average stretch,"We consider the classical problem of online job scheduling on uniprocessor and multiprocessor machines. For a given job, we measure the quality of service provided by an algorithm by the stretch of the job, which is defined as the ratio of the amount of time that the job spends in the system to the processing time of the job. For a given sequence of jobs, we measure the performance of an algorithm by the average stretch achieved by the algorithm over all the jobs in the sequence. The average stretch metric has been used to evaluate the performance of scheduling algorithms in many applications arising in databases, networks and systems; however no formal analysis of scheduling algorithms is known for the average stretch metric. The main contribution of the paper is to show that the shortest remaining processing time algorithm (SRPT) is O(l)-competitive with respect to average stretch for both uniprocessors as well as multiprocessors. For uniprocessors, we prove that SRPT is 2-competitive; we also establish an essentially matching lower bound on the competitive ratio of SRPT. For multiprocessors, we show that the competitive ratio of SRPT is at most 14. Furthermore, we establish constant-factor lower bounds on the competitive ratio of any online algorithm for both uniprocessors and multiprocessors.",
A dynamic bootstrap mechanism for rendezvous-based multicast routing,"Current multicast routing protocols can be classified into three types according to how the multicast tree is established: broadcast and prune (e.g., DVMRP, PIM-DM), membership advertisement (e.g., MO-SPF), and rendezvous-based (e.g., CBT, PIM-SM). Rendezvous-based protocols associate with each logical multicast group address, a physical unicast address, referred to as the 'core' or 'rendezvous point' (RP). Members first join a multicast tree rooted at this rendezvous point in order to receive data packets sent to the group. Rendezvous mechanisms are well suited to large wide-area networks because they distribute group-specific data and membership information only to those routers that are on the multicast distribution tree. However, rendezvous protocols require a bootstrap mechanism to map each logical multicast address to its current physical rendezvous point address. The bootstrap mechanism must adapt to network and router failures but should minimize unnecessary changes in the group-to-RP mapping. In addition, the bootstrap mechanism should be transparent to the hosts. This paper describes and analyzes the bootstrap mechanism developed for PIM-SM. The mechanism employs an algorithmic mapping of multicast group to rendezvous point address, based on a set of available RPs distributed throughout a multicast domain. The primary evaluation measures are convergence time, message distribution overhead, balanced assignment of groups to RPs, and host impact. The mechanism as a whole, and the design lessons in particular, are applicable to other rendezvous-based multicast routing protocols as well.",
Copy detection for intellectual property protection of VLSI designs,"We give the first study of copy detection techniques for VLSI CAD applications; these techniques are complementary to previous watermarking-based IP protection methods in finding and proving improper use of design IP. After reviewing related literature (notably in the text processing domain), we propose a generic methodology for copy detection based on determining basic elements within structural representations of solutions (IPs), calculating (context-independent) signatures for such elements, and performing fast comparisons to identify potential violators of IP rights. We give example implementations of this methodology in the domains of scheduling, graph coloring and gate-level layout; experimental results show the effectiveness of our copy detection schemes as well as the low overhead of implementation. We remark on open research areas, notably the potentially deep and complementary interaction between watermarking and copy detection.",
Time-frequency MEG-MUSIC algorithm,"The authors propose a method that incorporates the time-frequency characteristics of neural sources into magnetoencephalographic (MEG) source estimation. The method is based on the multiple-signal-classification (MUSIC) algorithm and it calculates a time-frequency matrix in which diagonal and off-diagonal terms are the auto and crosstime-frequency distributions of multichannel MEG recordings, respectively. The method averages this time-frequency matrix over the time-frequency region of interest. The locations of neural sources are then estimated by checking the orthogonality between the noise subspace of this averaged matrix and the sensor lead field. Accordingly, the method allows the authors to estimate the locations of neural sources from each time-frequency component. A computer simulation was performed to test the validity of the proposed method, and the results demonstrate its effectiveness.",
Interconnect estimation and planning for deep submicron designs,"This paper reports two sets of important results in our exploration of an interconnect-centric design flow for deep submicron (DSM) designs: (i) We obtain efficient yet accurate wiring area estimation models for optimal wire sizing (OWS). We also propose a simple metric to guide area-efficient performance optimization; (ii) Guided by our interconnect estimation models, we study the interconnect architecture planning problem for wire-width designs. We achieve a rather surprising result which suggests that two pre-determined wire widths per metal layer are sufficient to achieve near-optimal performance. This result will greatly simplify the routing architecture and tools for DSM designs. We believe that our interconnect estimation and planning results will have a significant impact on DSM designs.","Integrated circuit interconnections,
Wire,
Wiring,
Delay estimation,
Permission,
Computer science,
Very large scale integration,
Costs,
Capacitance,
Contracts"
20 years of covert channel modeling and analysis,"Covert channels emerged in mystery and departed in confusion. Covert channels are a means of communication between two processes that are not permitted to communicate, but do so anyway, a few bits at a time, by affecting shared resources. Information hiding is slightly different: the two communicating parties are allowed to talk, but the content is censored and restricted to certain subjects. The trick is to ""piggyback"" some contraband data invisibly on the legitimate content. The canonical example of this is to use the low-order two bits of each pixel in a picture for your secret message, since no one would notice if they were changed. When a similar idea was applied to smuggle information in network headers, we called it a network covert channel, mostly because the term ""information hiding"" hadn't been invented yet. The article traces the history of covert channel modeling from 1980 to the present (1999).",
Bandwidth modelling for network-aware applications,"Network-aware applications attempt to adjust their resource demands in response to changes in resource availability, e.g., if a server maintains a connection to a client, the server may want to adjust the amount of data sent to the client based on the effective bandwidth realized for the connection. Information about current and future network performance is therefore crucial for an adaptive application. This paper discusses three aspects of the coupling of applications and networks: (1) a network-aware application needs timely information about the status of the network; (2) a simple bandwidth estimation technique per forms reasonably well for TCP-Reno connections without timeouts; (3) enhancements proposed to TCP-Reno to reduce the number of timeouts (i.e., SACKs and its variants) increase the bandwidth but also improve the accuracy of bandwidth estimators developed by other researchers. The empirical observations reported in this paper are based on an in-vivo experiment in the Internet. Over a 6-month period, we logged the micro dynamics of random connections between a set of selected hosts. These results are encouraging for the developer of a network-aware application since they provide evidence that a simple widening of the interface between applications and network (protocol) may provide the information that allows an application to successfully adapt to changes in resource availability.",
Low complexity index-compressed vector quantization for image compression,"This paper proposes a novel lossless index compression algorithm that explores the interblock correlation in the index domain and the property of the codebook ordering. The goal of this algorithm is to improve the performance of the VQ scheme at a low bit rate while keeping low computation complexity. In this algorithm, the closest codeword in the codebook is searched for each input vector. Then, the resultant index is compared with the previously encoded indices in a predefined search order to see whether the same index value can be found in the neighboring region. Besides, the relative addressing technique is employed to encode the current index if the same index value can not be found in the region. According to the results, the newly proposed algorithm achieves significant reduction of bit rate without introducing extra coding distortion. It is concluded that our algorithm is very efficient and effective for image vector quantization.",
On individual and aggregate TCP performance,"/sup A/s the most widely used reliable transport in today's Internet, TCP has been extensively studied in the past. However previous research usually only considers a small or medium number of concurrent TCP flows. The TCP behavior under many competing TCP flows has not been sufficiently explored. In this paper we use extensive simulations to investigate the individual and aggregate TCP performance for a large number of concurrent TCP flows. First, we develop a simple yet realistic network model to abstract an Internet connection. Based on the model, we study the performance of a single TCP flow with many competing TCP flows by evaluating the best-known analytical model proposed in the literature. Finally, we examine the aggregate TCP behavior and derive general conclusions about overall throughput, goodput, and loss probability.",
A new Bayesian framework for object recognition,"We introduce an approach to feature-based object recognition, using maximum a posteriori (MAP) estimation under a Markov random field (MRF) model. This approach provides an efficienct solution for a wide class of priors that explicitly model dependencies between individual features of an object. These priors capture phenomena such as the fact that unmatched features due to partial occlusion are generally spatially correlated rather than independent. The main focus of this paper is a special case of the framework that yields a particularly efficient approximation method. We call this special case spatially coherent matching (SCM), as it reflects the spatial correlation among neighboring features of an object. The SCM method operates directly on the image feature map, rather than relying on the graph-based methods used in the general framework. We present some Monte Carlo experiments showing that SCM yields substantial improvements over Hausdorff matching for cluttered scenes and partially occluded objects.",
Efficient all-to-all broadcast in all-port mesh and torus networks,"All-to-all communication is one of the most dense communication patterns and occurs in many important applications in parallel computing. In this paper, we present a new all-to-all broadcast algorithm in all-port mesh and torus networks. Unlike existing all-to-all broadcast algorithms, the new algorithm takes advantage of overlapping of message switching time and transmission time, and achieves optimal transmission time for all-to-all broadcast. In addition, in most cases, the total communication delay is close to the lower bound of all-to-all broadcast within a small constant range. Finally, the algorithm is conceptually simple, and symmetrical for every message and every node so that it can be easily implemented in hardware and achieves the optimum in practice.",
Control education: time for radical change?,"Aims to identify, and review critically, some of the hidden assumptions in the conventional control engineering curriculum, to stimulate debate within the control community. Topics addressed include the role of mathematics; the philosophy of engineering; the use of computer-based tools; and the aspirations and requirements of students and employers. The article is prompted by a number of current issues in control engineering education, including: how to improve the structure and content of our curricula, and how to recruit more students to study control engineering; the implications for control education of the increasingly rapid pace of technological change-in particular, the role of computers and computer-aided control system design tools; the (at times acrimonious) debate within the control engineering profession on fuzzy and intelligent control; and the role of so-called ""nontechnical subjects"" in engineering education.",
Modeling and optimization of rotational C-arm stereoscopic X-ray angiography,"Stereoscopy can be an effective method for obtaining three-dimensional (3-D) spatial information from two-dimensional (2-D) projection X-ray images, without the need for tomographic reconstruction. This much-needed information is missed in many X-ray diagnostic and interventional procedures, such as the treatment of vascular aneurysms. Fast C-arm X-ray systems can obtain multiple angle sequences of stereoscopic image pairs from a single contrast injection and a single breath hold. To advance this solution, the authors developed a model of stereo angiography, performed perception experiments and related results to optimal acquisition. The model described horizontal disparity for the C-arm geometry that agreed very well with measurements from a geometric phantom. The perceptual accommodation-convergence conflict and geometry limited the effective stereoscopic field of view (SFOV). For a typical large image intensifier system, it was 28 cm/spl times/31 cm at the center of rotation (COR). In the model, blurring from finite focal-spot size and C-arm motion reduced depth resolution on the digital display. Near the COR, the predicted depth resolution was 3-11 mm for a viewing angle of 7/spl deg/, which agreed favorably with results from recently published studies. The model also described how acquisition parameters affected spatial warping of curves of equal apparent depth. Pincushioning and the difference between the acquisition and display geometry were found to introduce additional distortions to stereo displays. Preference studies on X-ray angiograms indicated that the ideal viewing angle should be small (1-2/spl deg/), which agreed with some previously published work. Perceptual studies indicated that stereo angiograms should have high artery contrast and that digital processing to increase contrast improved stereopsis. Digital subtraction angiograms, with different motion errors between the left and right-eye views, gave artifacts that confused stereopsis. The addition of background to subtracted images reduced this effect and provided other features for improved depth perception. Using the modeling results and typical clinical angiography requirements, the authors recommend acquisition protocols and engineering specifications that are achievable on current high-end systems.",
Tighter timing predictions by automatic detection and exploitation of value-dependent constraints,"Predicting the worst case execution time (WCET) of a real time program is a challenging task. Though much progress has been made in obtaining tighter timing predictions by using techniques that model the architectural features of a machine, significant overestimations of WCET can still occur. Even with perfect architectural modeling, dependencies on data values can constrain the outcome of conditional branches and the corresponding set of paths that can be taken in a program. While value-dependent constraint information has been used in the past by some timing analyzers, it has typically been specified manually, which is both tedious and error prone. The paper describes efficient techniques for automatically detecting value-dependent constraints by a compiler and automatically exploiting these constraints within a timing analyzer. The result is tighter timing analysis predictions without requiring additional interaction with a user.",
Approximate tree matching and shape similarity,"We present a framework for 2D shape contour (silhouette) comparison that can account for stretchings, occlusions and region information. Topological changes due to the original 3D scenarios and articulations are also addressed. To compare the degree of similarity between any two shapes, our approach is to represent each shape contour with a free tree structure derived from a shape axis (SA) model, which we have recently proposed. We then use a tree matching scheme to find the best approximate match and the matching cost. To deal with articulations, stretchings and occlusions, three local tree matching operations, merge, cut, and merge-and-cut, are introduced to yield optimally approximate matches, which can accommodate not only one-to-one but many-to-many mappings. The optimization process gives guaranteed globally optimal match efficiently. Experimental results on a variety of shape contours are provided.",
A two-phase feature selection method using both filter and wrapper,"Feature selection is an integral step of the data mining process to find an optimal subset of features. After examining the problems with both the filter and the wrapper approach to feature selection, we propose a two-phase (filter and wrapper) feature selection algorithm that can take advantage of both approaches. It begins by running GFSIC (Genetic Feature Selection with Inconsistency Criterion), a filter approach, to remove irrelevant features, then it runs SBFCV (Sensitivity-Based Feature selection with v-fold Cross-Validation), a wrapper approach, to remove redundant or useless features. Analysis and experimental studies show the effectiveness and scalability of the proposed algorithm. The generalization of the neural network is improved when the algorithm is used to pre-process the training data by eliminating irrelevant and useless features from the neural network's consideration.","Filters,
Neural networks,
Data mining,
Genetic algorithms,
Neurons,
Laboratories,
Computer science,
Information science,
Algorithm design and analysis,
Scalability"
Efficient topological exploration,"We consider the robot exploration of a planar graph-like world. The robot's goal is to build a complete map of its environment. The environment is modeled as an arbitrary undirected planar graph which is initially unknown to the robot. The robot cannot distinguish vertices and edges that it has explored from the unexplored ones. The robot is assumed to be able to autonomously traverse graph edges, recognize when it has reached a vertex, and enumerate edges incident upon the current vertex. The robot cannot measure distances nor does it have a compass, but it is equipped with a single marker that it can leave at a vertex and sense if the marker is present at a newly visited vertex. The total number of edges traversed while constructing a map of a graph is used as a measure of performance. We present an efficient algorithm for learning an unknown, undirected planar graph by a robot equipped with one marker. Experimental results obtained by running a large collection of example worlds are presented.",
A rearrangeable algorithm for the construction of delay-constrained dynamic multicast trees,"With the proliferation of multimedia group applications, the construction of multicast trees satisfying quality of service (QoS) requirements is becoming a problem of prime importance. Many of the multicast applications (such as video broadcasts and teleconferencing) require the network to support dynamic multicast sessions wherein the membership of the multicast group changes with time. We propose and evaluate an algorithm for on-line update of multicast trees to adjust to changes in group membership. The algorithm is based on a concept called quality factor (QF) that represents the usefulness of a portion of the multicast tree to the overall multicast session. When the usefulness of a particular region of the tree drops below a threshold, a rearrangement technique is used to suitably modify the tree. This algorithm aims to satisfy the delay-constraints of all current group members, at the same time minimizing the cost of the constructed tree. We compare the performance of our algorithm, by simulation, with that of an off-fine Steiner heuristic; with ARIES, a previously published algorithm for on-line update of unconstrained trees; and with the algorithm proposed by Hong, Lee and Park (see Proc. of IEEE INFOCOM, pp. 1433-40, 1998) for on-line update of delay-constrained trees. The simulation results indicate that our algorithm provides excellent cost-competitiveness that is better than that provided by the algorithm described by Hong et al., minimizes changes in the multicast tree after each update, and performs favorably even when compared with the unconstrained ARIES heuristic.",
Extending UML for modeling of multimedia applications,"An analysis of how visual modeling of structure and dynamic behavior of a multimedia application differs from modeling conventional software shows that aspects of the graphical user interface and time-dynamic behavior ought to be integral parts of a coherent multimedia application model. In this sense, we extend the model-view-controller paradigm towards multimedia. As a result, we present OMMMA-L, a visual language for the object-oriented modeling of multimedia applications, that is based on the Unified Modeling Language (UML). The structural and behavioral diagram types of UML have been analyzed and are adapted and extended according to multimedia application characteristics. A presentation diagram is introduced and integrated to adequately describe the visual presentation. In addition to explaining the different diagram types, we also give pragmatic guidelines on how to deploy and combine the various diagrams.",
Static analysis of binary code to isolate malicious behaviors,"We address the problem of static slicing on binary executables for the purposes of malicious code detection in COTS components. By operating directly on binary code without any assumption on the availability of source code, our approach is realistic and appropriate for the analysis of COTS software products. To be able to reason on such low-level code, we need a suite of program transformations that aim to get a high level imperative representation of the code. The intention is to significantly improve the analysability while preserving the original semantics. Next we apply slicing techniques to extract those code fragments that are critical from the security standpoint. Finally, these fragments are subjected to verification against behavioral specifications to statically decide whether they exhibit malicious behaviors or not.",
TetraFusion: information discovery on the Internet,The TetraFusion system described in this paper supports knowledge discovery from the World Wide Web by helping users perform data mining operations on sets of harvested URLs. Potential applications range from domain overviewing to science monitoring to competitive intelligence.,
Engagement in multimedia training systems,"Two studies examined user engagement in two types of multimedia training systems-a more passive medium, videotape, and a less passive medium, interactive software. Each study compared engagement in three formats: the text format contained text and still images, the audio format contained audio and still images, and the video format contained audio and video images. In both studies, engagement was lower in the text condition than in the video condition. However, there were no differences in engagement between text and audio in the videotape-based training and no differences between audio and video in the computer-based training.",
On the efficient scheduling of non-periodic tasks in hard real-time systems,"The paper presents linear time, online algorithms which guarantee and jointly schedule firm aperiodic, hard sporadic and periodic tasks in fixed priority real time systems. We develop and capitalize on a methodology which computes the spare capacity Z(a,b) exactly in time /spl Theta/(n), for arbitrary schedule intervals (a,b), which, to the best of our knowledge, is the first linear time algorithm reported in the literature. Previous state of the art methods incur pseudopolynomial time to guarantee online a single aperiodic and incur continuous overhead for slack maintenance. Our method guarantees and schedules firm tasks to receive FIFO or EDF service, incurring a one-time linear cost of /spl Theta/(n) and /spl Theta/(n+k) respectively, where k is the number of pending firm tasks.",
Importance weighted OWA aggregation of multicriteria queries,"The family of ordered weighted averaging (OWA) operators, as introduced by R.R. Yager (1988), appears to be very useful in flexible query answering, including information retrieval, where the information needs are often modeled by an aggregation (of the criteria in the query) between /spl and/ (pure AND) and V (pure OR). In this paper, we discuss and set up a general set of requirements for importance-weighted aggregation, and show that an importance weighting scheme suggested by Yager (1977) for OWA operators satisfies the requirements. The weighted arithmetic mean is shown to be order-equivalent to the special case of importance-weighted OWA operators where the importance-weighted satisfaction of the criteria are weighted evenly in the OWA aggregation.",
Route selection in mobile multimedia ad hoc networks,"The problem of routing packets in ad hoc networks is complex due to the lack of network infrastructure, shifting the reliance of packet forwarding onto the nodes of the network, all of which are mobile. In order to improve the robustness and adaptation of protocols for ad hoc networks to node mobility, we propose a new metric that bases route selection on the probability of the existence of the route (route availability). Specifically, we derive a simple closed form expression for the computation of the availability of a route, that takes into account node mobility and dependencies between links, which can then be used to satisfy the requirements of multimedia applications. In particular, we show how by efficiently collecting measures about the geographic location of other nodes, each node of an ad hoc network can locally and efficiently compute multiple paths to a given destination node, and, based on the introduced metric, choose the route that best meets the strict requirements of multimedia applications.",
Code decay analysis of legacy software through successive releases,"Prediction of problematic software components is an important activity today for many organizations as they manage their legacy systems and the maintenance problems they cause. This means that there is a need for methods and models to identify troublesome components. We apply a model for classification of software components as green, yellow and red according to the number of times they required corrective maintenance over successive releases. Further, we apply a principal component and box plot analysis to investigate the causes for the code decay and try to characterize the releases. The case study includes eight releases and 130 software components. The outcome indicates a large number of healthy components as well as a small set of troublesome components requiring extensive repair repeatedly. The analysis characterizes the releases and indicates that it is the relationship between components that causes many of the problems.",
Design of optimal stack filters under the MAE criterion,"The design of optimal stack filters under the MAE criterion is addressed in this paper. In our work, the Hasse diagram is adopted to represent the positive Boolean functions to solve the optimization problem. After problem transformation, the finding of the optimal stack filter is equivalent to the finding of the optimal on-set such that the total cost of the on-set is minimal. An efficient algorithm is developed that makes use of an important property of the optimal on-set to avoid fruitless search. It thereby greatly reduces the complexity in finding the corresponding optimal stack filter. A design example is illustrated in detail to demonstrate the optimization procedures. The proposed algorithm can generate the optimal stack filter in 1 s for the window size of 14 pixels. It can still generate the optimal stack filter for the window size of 21, although it takes about 4 h. Experimental results for real images reveal that the proposed algorithm essentially extends the maximum filter window size to make the stack filter optimization problem computationally tractable.",
Partitioning and searching dictionary for correction of optically read Devanagari character strings,"This paper describes a correction method for optically read Devanagari character strings which uses a partitioned word dictionary. The word dictionary is partitioned in order to reduce the search space besides preventing a forced match to the incorrect word. The envelop information of words consisting of the number of top, lower, core modifiers along with the number of core characters form the second level partitioning feature for short word partitions. The remaining words are further partitioned using tags. A tag is a string of fixed length associated with each partition. The search process uses a distance matrix for assigning a penalty for a mismatch. An improvement of approximately 20% in the recognition performance is obtained.",
Joint detection for potsherds of broken earthenware,"In this paper, we propose a new strategy for detecting the joint among two potsherds. Joint detection problems were studied in jigsaw puzzle assembling. However, the shape assumptions of a piece used in the past researches cannot be applied to joint detection to reconstruct broken earthenware. To detect the joint, the most similar section among two contours must be detected by partial verification. In our strategy, each contour is segmented for partial verification without making any assumption about the shape of a potsherd, unlike previous jigsaw puzzle assembling methods. Our strategy consists of five processes: the contour segmentation process, the segment description process, the segment verification process, the candidate extraction process, and the candidate verification process. We also present experimental results of our strategy with two-dimensional images of potsherds.",
ERUF: early regulation of unresponsive best-effort traffic,"We propose router mechanisms to regulate unresponsive best-effort traffic. By unresponsive traffic we mean flows that do not reduce their sending rate in response to congestion. The goal of the proposed mechanisms is to drop undeliverable packets as close to the periphery of the network as possible. The key ideas of our approach are: (1) edge routers keep track of incoming flows and their arrival rates; (2) core routers use random early detection (RED) for queue management and generate rate-limited source quenches on packet drops to advice sources to reduce their sending rates; and (3) edge routers snoop an source quenches passing through them and use them to control per-flow regulators. Regulators adjust their maximum sending rate using a multiplicative-decrease, additive-increase discipline. A decrease is triggered by the arrival of a source quench; an increase is triggered by non-arrival of source quenches for a time period. We examine the impact of these mechanisms for a variety of simulated network topologies and traffic patterns.",
Supporting Rich And Dynamic Communication In Large-Scale Collaborative Virtual Environments,"We focus on the problem of constructing collaborative virtual environments (CVEs) that scale to large numbers of simultaneous participants and yet which still afford rich and varied possibilities for communication. This is achieved by extending our previously defined spatial model of interaction for CVEs to include third-party objects that provide support for contextual factors in awareness calculations and that enhance scaleability. Third parties can have two effects on awareness: attenuation or amplification of existing awareness relationships, and the introduction of new aggregate awareness relationships. We propose a range of applications for third-party objects including world structuring regions, aggregate views, dynamic crowds of participants, common foci, representational and group services, and dynamic load management. We also discuss how the third-party concept relates to other approaches to structuring virtual environments such as tiles, zones, and locales. We then present an implementation, the MASSIVE-2 system, focusing on its network architecture which is based on a dynamic and selfconfiguring hierarchy of multicast groups. Finally, we describe four demonstration applications that have been developed in MASSIVE-2: an environment for staging a public poetry performance that includes semiprivate zones for social interaction; the Panoptican Plaza, which demonstrates a variety of differently bounded regions; a collaborative 3-D Web browser that groups pages into server regions; and the Arena, a performance space that supports both static and mobile crowd aggregations of groups of participants.",
A method for calibrating see-through head-mounted displays for AR,"In order to have a working augmented reality (AR) system, the see-through system must be calibrated such that the internal models of objects match their physical counterparts. By match, we mean they should have the same position, orientation, and size information as well as any intrinsic parameters (such as focal lengths in the case of cameras) that their physical counterparts have. To this end, a procedure must be developed which estimates the parameters of these internal models. This calibration method must be both accurate and simple to use. This paper reports on our efforts to implement a calibration method for a see-through head-mounted display. We use a dynamic system in which a user interactively modifies the camera parameters until the image of a calibration object matches the image of a corresponding physical object. The calibration method is dynamic in the sense that we do not require the user's head to be immobilized.",
Soft scheduling in high level synthesis,"In this paper, we establish a theoretical framework for a new concept of scheduling called soft scheduling. In contrasts to the traditional schedulers referred as hard schedulers, soft schedulers make soft decisions at a time, or decisions that can be adjusted later. Soft scheduling has a potential to alleviate the phase coupling problem that has plagued traditional high level synthesis (HLS), HLS for deep submicron design and VLIW code generation. We then develop a specific soft scheduling formulation, called threaded schedule, under which a linear, optimal (in the sense of online optimality) algorithm is guaranteed.",
Shared virtual memory: progress and challenges,"Shared virtual memory, a technique for supporting a shared address space in software on parallel systems, has undergone a decade of research, with significant maturing of protocols and communication layers having now been achieved. We provide a survey of the key developments in this research, placing the multitrack flow of ideas and results obtained so far in a comprehensive new framework. Four major research tracks are covered: relaxed consistency models; protocol laziness; architectural support; and application-driven research. Several related avenues are also discussed, such as fine grained software coherence, software protocols across multiprocessor nodes, and performance scalability. We summarize comparative performance results from the literature, discuss their limitations, and identify lessons learned so far, key outstanding questions, and important directions for future research in this area.",
Drawing the red line in Java,"Software-based protection has become a viable alternative to hardware-based protection in systems based on languages such as Java, but the absence of hardware mechanisms for protection has been coupled with an absence of a user/kernel boundary. We show why such a ""red line"" must be present in order for a Java virtual machine to be as effective and as reliable as an operating system. We discuss how the red line can be implemented using software mechanisms, and explain the ones we use in the Java system that we are building.",
Visualizing text data sets,"The authors present a visualization methodology which provides users with a way to alter perspectives and interpret visualization so that they can quickly identify trends, outliers, and possible clusters while tuning for a particular context. The technology developed for text mining is called Trust, or Text Representation Using Subspace Transformation. Trust provides an analysis environment that can supply meaningful representations of text documents; it also supports the functional ability to visually present a collection of documents in a meaningful context that allows for user insight and textual content. Contrary to other similar technologies, Trust applies a novel analysis ability that allows different subspaces to generate views, providing content information for the basis of the visualization and allowing an analyst to specify subspaces for it based on content.",
Routing and wavelength assignment for establishing dependable connections in WDM networks,"This paper considers the problem of establishing dependable connections (D-connections) for fast failure recovery in wavelength-routed wavelength-division multiplexed (WDM) networks with dynamic traffic demand. A D-connection is realized by a primary lightpath and one or more backup lightpaths. Real time applications usually require timeliness and fault-tolerance. It is not a difficult task to guarantee timely delivery of messages in WDM networks, as a lightpath with its entire bandwidth is dedicated to an application. However providing fault-tolerance is an important issue to be solved in these networks. In this paper we focus on backup multiplexing based, primary-backup lightpath routing in WDM networks. We present different methods to assign wavelengths to backup lightpaths, for a given primary lightpath. They are broadly classified into primary dependent backup wavelength assignment (PDBWA) and primary independent backup wavelength assignment (PIBWA) schemes. The PDBWA is further classified into PDBWA-S and PDBWA-D, depending on whether the wavelength of the backup lightpath is the same as that of the primary lightpath or not. All the above methods differ in their computational complexity and performance. To improve fairness among the connections with and without fault-tolerant requirements, we propose a method called backup threshold. We conduct extensive simulation experiments to study the performance of the proposed methods.",
"""Where are you pointing at?"" A study of remote collaboration in a wearable videoconference system","This paper reports on an empirical study aimed at evaluating the utility of a reality-augmenting telepointer in a wearable videoconference system. Results show that using this telepointer a remote expert can effectively guide and direct a field worker's manual activities. By analyzing verbal communication behavior and pointing gestures, we were able to determine that experts overwhelmingly preferred pointing for guiding workers through physical tasks.",
A new video interpolation technique based on motion-adaptive subsampling,"In video data transmission, a subsampling technique is used to reduce the number of transmitted frames. This paper presents a new technique to improve the performance of video interpolation by controlling the subsampling rate of a video sequence. Subsampling is determined by the motion complexity of a frame. In the simulation results, we find that the picture quality fluctuation is reduced.",
A probabilistic performance metric for real-time system design,"At the system level design of a real-time embedded system, a major issue is to identify from alternative architectures the best one which satisfies the timing constraints. This issue leads to the need of a metric that is capable of evaluating the overall system timing performance. Some of the previous work in the related areas focus on predicting the system's timing performance based on a fixed computation time model. These approaches are often too pessimistic. Those that do consider varying computation times for each task are only concerned with the timing behavior of each individual task. Such predictions may not properly capture the timing behavior of the entire system. In this paper, we introduce a metric that reflects the overall timing behavior of RTES. Applying this metric allows a comprehensive comparison of alternative system level designs.",
Integrating physics-based computing and visualization: modeling dust behavior,"Simulating physically realistic, complex dust behaviors is useful in interactive graphics applications, such as those used for education, entertainment, or training. Training in virtual environments is a major topic for research and applications, and generating dust behaviors in real time significantly increases the realism of the simulated training environment. We introduce a method for simulating the dust behaviors that a fast-traveling vehicle causes. Our method combines particle systems, rigid-body particle dynamics, computational fluid dynamics (CFD), rendering, and visualization techniques. Our work integrates physics-based computing and graphical visualization for applications in simulated virtual environments.",
FLB: Fast Load Balancing for distributed-memory machines,"This paper describes a novel compile-time list-based task scheduling algorithm for distributed-memory systems, called Fast Load Balancing (FLB). Compared to other typical list scheduling heuristics, FLB drastically reduces scheduling time complexity to O(V(log(W)+log (P))+E), where V and E are the number of tasks and edges in the task graph, respectively, W is the task graph width and P is the number of processors. It is proven that FLB is essentially equivalent to the existing ETF scheduling algorithm of O(W(E+V)P) time complexity. Experiments also show that FLB performs equally to other one-step algorithms of much higher cost, such as MCP. Moreover, FLB consistently outperforms multi-step algorithms such as DSC-LLB that also have higher cost.",
Direct queries for discovering network resource properties in a distributed environment,"The development and performance of network-aware applications depends on the availability of accurate predictions of network resource properties. Obtaining this information directly from the network is a scalable solution that provides the accurate performance predictions and topology information needed for planning and adapting application behavior across a variety of networks. The performance predictions obtained directly from the network are as accurate as application-level benchmarks, but the network-based technique provides the added advantages of scalability and topology discovery. We describe how to determine network properties directly from the network using SNMP. We provide an overview of SNMP and describe the features it provides that make it possible to extract both available bandwidth and network topology information from network devices. The available bandwidth predictions based on network queries using SNMP are compared with traditional predictions based on application history to demonstrate that they are equally useful. To demonstrate the feasibility of topology discovery, we present results for a large Ethernet at CMU.",
Workconserving vs. non-workconserving packet scheduling: an issue revisited,"Many packet schedulers for QoS networks are equipped with a rate control mechanism. The function of a rate control mechanism (rate controller) is to buffer packets from flows which exceed their negotiated traffic profile. It has been established that rate controllers lead to reduced buffer requirements at packet switches, and do not increase the worst-case delays in a deterministic service. On the other hand, rate controllers make a scheduler non-workconserving, and, thus, may yield higher average end-to-end delays. In this study, we show that by properly modifying a rate controller, one can design a scheduler which balances buffer requirements against average delays. We present a scheduler, called earliness-based earliest deadline first (EEDF), which achieves such a balancing using a tunable rate control mechanism. In simulation experiments, we compare EEDP with a rate-controlled EDF scheduler and a workconserving version of EDF.",
Methodology for flexible and efficient analysis of the performance of page segmentation algorithms,"The paper presents part of a new DIA performance analysis framework aimed at layout analysis algorithm developers. A new region representation scheme (an interval based description of isothetic polygons) and a corresponding comparison approach are introduced. These enable fast and accurate geometric comparison of ground truth with results of page segmentation, improving on current evaluation methods. Complex layouts are accurately described and layout analysis methods that handle them can be effectively evaluated. A further benefit of the new approach is that it measures the accuracy of the description of regions, an issue which is important for complex layouts involving non text regions.","Performance analysis,
Algorithm design and analysis,
Optical character recognition software,
Large-scale systems,
Image segmentation,
Spatial databases,
Computer science,
Text analysis,
Image analysis,
Error analysis"
Low-cost on-line test for digital filters,"A low-cost on-line test scheme for digital filters is proposed. The scheme uses an invariant of the digital filter, the frequency response at specific points, in order to detect possible malfunctioning of the circuit. The analysis performed indicates that 100% fault secureness is possible, if certain design constraints are followed.",
Managing complexity in a distributed digital library,"As the capabilities of distributed digital libraries increase, managing organizational and software complexity becomes a key issue. How can collections and indexes be updated without impacting queries currently in progress? How can the system handle several user interface clients for the same collections? Computer science professors from the University of Waikato have developed a software structure that successfully manages this complexity in the New Zealand Digital Library. This digital library has been a success in managing organizational and software complexity, The researchers' primary goal has been to minimize the effort required to keep the system operational and yet continue to expand its offering.",
Direct waveform synthesis for software radios,"This work presents results that show how direct digital synthesis (DDS) techniques can be extended to produce frequency modulated (FM) waveforms as well as many types of digitally modulated waveforms, such as PSK or QAM. We show how digitally modulated waveforms can be efficiently synthesized from tables of pre-computed samples. This work also reports on the implementation of these algorithms in a software radio system.",
Object ownership for dynamic alias protection,"Interobject references in object-oriented programs allow arbitrary aliases between objects. By breaching objects' encapsulation boundaries, these aliases can make programs hard to understand and especially hard to debug. We propose using an explicit, run-time notion of object ownership to control aliases between objects in dynamically typed languages. Dynamically checking object ownership as a program runs ensures the program maintains the encapsulation topology intended by the programmer.",
Preference-driven hierarchical hardware/software partitioning,"We present a hierarchical evolutionary approach to hardware/software partitioning for real-time embedded systems. In contrast to most previous approaches, we apply a hierarchical structure and dynamically determine the granularity of tasks and hardware modules to adaptively optimize the solution while keeping the search space as small as possible. Two new search operators are described, which exploit the proposed hierarchical structure. Efficient ranking is another problem addressed. Imprecisely specified multiple attribute utility theory has the advantage of constraining the solution space computation overhead. We propose a new technique to reduce the overhead. Experiment results show that our algorithm is both effective and efficient.",
Efficient network-flow based techniques for dynamic fault reconfiguration in FPGAs,"In this paper we consider a ""dynamic"" node covering frameworks for incorporating fault tolerance in SRAM-based segmented array FPGAs with spare row(s) and/or column(s) of cells. Two types of designs are considered: one that can support only node-disjoint (and hence nonintersecting) rectilinear reconfiguration paths, and the other that can support edge-disjoint (and hence possibly intersecting) rectilinear reconfiguration paths. The advantage of this approach is that reconfiguration paths are determined dynamically depending upon the actual set of faults and track segments are used as required, thus resulting in higher reconfigurability and lower track overheads compared to previously proposed ""static"" approaches. We provide optimal network flow based reconfiguration algorithms for both of our designs and present and analyze a technique for speeding up these algorithms, depending upon the fault size, by as much as 20 times. Finally, we present reconfigurability results for our FPGA designs that show much better fault tolerance for them compared to previous approaches-the reconfigurability of the edge-disjoint design is 90% or better and 100% most of the time, which implies near-optimal spare-cell utilization.",
Quality of service routing without global information exchange,"We propose a novel localized QoS routing approach, under which no global QoS state information exchange among network nodes is needed. This approach has many advantages over the conventional global QoS routing approach based on information exchange among routers. We develop two localized QoS routing schemes and demonstrate through simulations that the proposed localized QoS routing approach is indeed a viable alternative to the global QoS routing approach.",
Cell identification codes for tracking mobile users,"Location management is a crucial issue in wireless networks. The problem of tracking mobile users has been addressed by several studies, many of which attempt to reduce the wireless cost of users tracking. The basic idea shared by most of these works is that users update their location based on a pre-defined criterion. Unfortunately, some of these strategies require the use of information, such as the distance traveled from the last known location, that is not generally available to the user. For this reason, both the implementation of some of these strategies and the performance comparison to existing strategies is not clear. We propose to use cell identification codes (CIC) for tracking mobile users. Each cell periodically broadcasts a short message which identifies the cell and its orientation relatively to other cells in the network. This information is used by the users to efficiently update their location. We propose several cell identification encoding schemes, which are used to implement different tracking strategies. The best performance is achieved by a four-bit CIC, used to implement a distance-based tracking strategy in a two dimensional system. In addition, we propose a combination of timer and movement tracking strategy, based on either a one-bit or a two-bit CIC, depending on system topology and user mobility. An important property of our framework is that the overall performance cost, and hence its comparison to existing methods, is evaluated for each tracking strategy. The CIC-based strategies are shown to outperform the timer-based method over a wide range of parameters.",
Fast and accurate feature selection using hybrid genetic strategies,"When dealing with object classification, each object is defined by a set of features (characteristics) that classify the object to a particular class. The problem is how to choose the best subset of characteristics that provide an accurate classification. Previous research has shown that decision tables are as accurate as C4.5 for classification purposes. Two different genetic search techniques, CHC and CF/RSC, are applied to this problem. Results shows that CF/RSC and decision tables are a very good combination when dealing with large feature spaces. Results also suggest that CHC is better when used for problems with noise added to the features.",
The Hausdorff distance measure for feature selection in learning applications,"Recent advances in computing technology in terms of speed, cost, as well as access to tremendous amounts of computing power and the ability to process huge amounts of data in reasonable time has spurred increased interest in data mining applications. Machine learning has been one of the methods used in most of these data mining applications. It is widely acknowledged that about 80% of the resources in a majority of data mining applications are spent on cleaning and pre-processing the data. However, there have been relatively few studies on pre-processing data used as input in these data mining systems. In this study, we present a feature selection method based on the Hausdorff distance measure, and evaluate its effectiveness in pre-processing input data for inducing decision trees. The Hausdorff distance measure has been used extensively in computer vision and graphics applications to determine the similarity of patterns. Two real-world financial credit scoring data sets are used to illustrate the performance of the proposed method.",
Cell replication and redundancy elimination during placement for cycle time optimization,"Presents a new timing-driven approach for cell replication tailored to the practical needs of standard cell layout design. Cell replication methods have been studied extensively in the context of generic partitioning problems. However, until now, it has remained unclear what practical benefit can be obtained from this concept in a realistic environment for timing-driven layout synthesis. Therefore, this paper presents a timing-driven cell replication procedure, demonstrates its incorporation into a standard cell placement and routing tool, and examines its benefit on the final circuit performance in comparison with conventional gate or transistor sizing techniques. Furthermore, we demonstrate that cell replication can deteriorate the stuck-at fault testability of circuits and show that stuck-at redundancy elimination must be integrated into the placement procedure. Experimental results demonstrate the usefulness of the proposed methodology and suggest that cell replication should be an integral part of the physical design flow complementing traditional gate sizing techniques.",
Group support systems and deceptive communication,"Electronic communication is becoming more pervasive worldwide with the spread of the Internet, especially through the World Wide Web and electronic mail. Yet, as with all human communication, electronic communication is vulnerable to deceit on the part of senders, and to the less than stellar performance of most people at defecting deceit aimed at them. Despite considerable research over the years into both computer-mediated communication and into deception, there has been little if any research at the intersection of these two research streams. In this paper, we review these two streams and suggest a research model for investigating their intersection, focusing on group support systems as an electronic medium. We focus specifically on research questions concerning how successful people are at deceiving others through computer-mediated communication, and how successful people are at detecting such deception. We also suggest three propositions guiding research in this area.",
Qualitative probabilities for image interpretation,"Two basic problems in image interpretation are: a) determining which interpretations are the most plausible amongst many possibilities; and b) controlling the search for plausible interpretations. We address these issues using a Bayesian approach, with the plausibility ordering and search pruning based on the posterior probabilities of interpretations. However, due to the need for detailed quantitative prior probabilities and the need to evaluate complex integrals over various conditional distributions, a full Bayesian approach is currently impractical except in tightly constrained domains. To circumvent these difficulties we introduce the notion of qualitative probabilistic analysis. In particular, given spatial and contrast resolution parameters, we consider only the asymptotic order of the posterior probability for any interpretation as these resolutions are made finer. We introduce this approach for a simple card-world domain, and present computational results for blocks-world images.",
Low-energy directed architecture selection and task scheduling for system-level design,"Most of the current methods for designing power/energy efficient digital systems are addressing either the system hardware or system software. This is usually performed quite late in the design process. This paper presents a method for minimizing the energy consumption at system level, early in the design process. Our approach to low-energy system design uses constraint programing to achieve this goal. Considering a system composed of several communicating tasks running on a multiprocessor architecture, our method tries to find for a given deadline, the best schedule and configuration of processors from the energy consumption point of view. Finally, we present a set of experiments which confirm the importance of addressing the energy consumption at system level.",
Load distribution in a CORBA environment,"The design and implementation of a CORBA load distribution service for distributed scientific computing applications running in a network of workstations is described. The proposed approach is based on integrating load distribution into the CORBA naming service which in turn relies on information provided by the underlying WINNER resource management system developed for typical networked Unix workstation environments. The necessary extensions to the naming service, the WINNER features for collecting load information and the placement decisions are described. A prototypical implementation of the complete system is presented, and performance results obtained for the parallel optimization of a mathematical test function are discussed.",
"Comments on ""Choquet fuzzy integral-based hierarchical networks for decision analysis"" [with reply]","We remark on an error in the above paper by Chiang (ibid. vol.7 (1999)). The purpose of this note is to present the correct formulas for partial derivatives of fuzzy integral-based neural nodes with respect to densities of Sugeno measures. In reply, Chiang agree with the correction.",
Efficient content-based retrieval: experimental results,"Extensive testing has shown that the bare-bones triangle inequality algorithm could be used to sharply reduce the number of images needed to be directly compared to a query image for a given distance measure, and that adding the triangle-trie for a two-stage algorithm can be used to search for matches faster than even the bare-bones triangle inequality algorithm. We have developed a method for using the triangle inequality algorithm for combinations of distance measures, thus allowing for database systems which combine flexibility and speed. There are a number of open problems concerning the various data structures and algorithms we describe: key selection, number of keys, trie depth and bin size. More generally, the statistical behavior of distance measures over different sets of images influences the behavior of all the algorithms, and this needs to be explored.",
Face detection using a mixture of factor analyzers,"We present a probabilistic method to detect human faces using a mixture of factor analyzers. One characteristic of this mixture model is that it concurrently performs clustering and, within each cluster, local dimensionality reduction. A wide range of face images including ones in different poses, with different expressions and under different lighting conditions are used as the training set to capture the variations of human faces. In order to fit the mixture model to the sample face images, the parameters are estimated using an EM algorithm. Experimental results show that faces in different poses, with different facial expressions, and under different lighting conditions are accurately detected by our method.",
Reusing use case descriptions for requirements specification: towards use case patterns,"We present reusable patterns appearing in use case based requirements analysis processes. In the processes, analysts have interviews with and questionnaires for the stakeholders to get information about their problems, and then they compose the requirements specifications that are readable to the stakeholders. We adopt the use case approach to specify the requirements. We abstract the requirements descriptions written in use cases into patterns, so that we can reuse the experience in requirements analysis. From a case study of a simple example problem, we design questionnaire forms and a set of patterns of reusable structured use cases.",
Optimal control for hybrid systems: an hysteresis example,"In this paper, the authors present an approach based on the maximum principle of Pontryagin to solve optimal control for hybrid dynamical systems (HDS). In the first step a general hybrid optimal control problem is formulated. Secondly, our method can then be used to solve an hysteresis example and get analytic form of the solutions.",
Models and algorithms for optical and optoelectronic parallel computers,This paper briefly reviews some of the more popular parallel computer models - pipelined optical bus and OTIS (optical transpose interconnect system) models - that employ optical interconnects. The interconnect topology and some simple algorithms for each model are also described.,
Preprocessing and structural feature extraction for a multi-fonts Arabic/Persian OCR,"English and Chinese are languages which have attracted tremendous interest from character recognition researchers. In contrast, research in the field of character recognition for Arabic/Persian scripts faces major problems, mainly related to their unique characteristics, like being cursive, the multiple shapes of one character in different positions in a word, and the connectivity of characters on the baseline. The work proposed in this paper consists of three major phases. After digitizing the text, the original image is transformed into a gray-scale image using a 300-dpi scanner. Different pre-processing steps are then applied to the image file. In the next phase, sub-words of all words are recognized and global features for each word are extracted. Contour tracing plays a very important role in the feature extraction phase.",
Software development based on software pattern evolution,"This paper discusses a technique to model software patterns for supporting pattern based software development. Software development can be considered as the evolution of the artifacts to be produced. Software patterns are general structures that frequently appear in the artifacts and the patterns are also being evolved as the artifacts are being done. By specifying how to evolve software patterns as software processes progress, we can get a support for developing an artifact from the artifacts that were produced in the previous steps. In our approach, we consider that a software pattern consists of a pattern structure (a class diagram and/or an object diagram) and manipulation operations on the pattern structure. These operations are for pattern instantiation (applying a pattern to an actual problem) and for pattern evolution (evolving the artifacts of the previous steps into a new one). We model them with the object-oriented technique encapsulating these operations into patterns.",
"Adding more ""DL"" to IDL: towards more knowledgeable component inter-operability","In an open component market place, interface description languages (IDLs), such as CORBA's, provide for the consumer only a weak guarantee (concerning type signatures) that a software service will work in a particular context as anticipated. Stronger guarantees, regarding the intended semantics of the service, would help, especially if formalized in a language that allowed effective, automatic and static checking of compatibility between the server and the client's service descriptions. We propose an approach based on a family of formalisms called description logics (DLs), providing three examples of the use of DLs to augment IDL: (1) for the CORBA Cos Relationship service; (2) for capturing information models described using STEP Express, the ISO standard language used in the manufacturing domain (and a basis of the OMG PDM effort); and (3) constraints involving methods. While traditional formal specification techniques are more powerful, DLs offer certain advantages: they have decidable, even efficient reasoning algorithms, yet they still excel at modeling natural domains, and are thus well-suited for specifying application and domain-specific services.",
Multiscale image registration using scale trace correlation,"This paper presents a method for registering images at different magnifications (scales) by treating the problem not only as one of scaling the image coordinates but also as one inherently involving multiresolution information. While some existing methods for multiresolution registration do consider the way the resolution (scale) affects the image, they often consider the image one scale at a time, using geometric properties within that scale. Others use multiscale information, usually to produce more robust results, but only to register same-magnification images (e.g., stereo). A scale trace is the set of values that a single pixel takes on as magnification decreases and the effective point specifications corresponding increases. As such, scale traces capture information across multiple scales. This paper presents a method that uses correlation of scale traces through multiresolution images to find correspondences between images of the same scene at differing magnifications. Because the method relies solely on the multiscale properties of the images themselves and not on spatial correspondence, it is less sensitive to discrete sampling of potential scale factors. Sample results demonstrate that the method is able to accurately identify both the relative position and relative magnification (scale) of the two images.",
Fuzzy models and potential outliers,"Outliers or distorted attributes very often severely interfere with data analysis algorithms that try to extract few meaningful rules. Most methods to deal with outliers try to completely ignore them. This can be potentially harmful since the very outlier that was ignored might have described a rare but still extremely interesting phenomena. We describe an approach that tries to build an interpretable model while still maintaining all the information in the data. This is achieved through a two stage process. A first phase builds an outlier model for data points of low relevance, followed by a second stage which uses this model as filter and generates a simpler model, describing only examples with higher relevance, thus representing a more general concept. The outlier model on the other hand may point out potential areas of interest to the user. Preliminary experiments using an existing algorithm to construct fuzzy rule sets from data indicate that the two models in fact have lower complexity and sometimes even offer superior performance.",
Extracting nonrigid motion and 3D structure of hurricanes from satellite image sequences without correspondences,"Image sequences capturing Hurricane Luis through meteorological satellites (GOES-8 and GOES-9) are used to estimate hurricane-top heights (structure) and hurricane winds (motion). This problem is difficult not only due to the absence of correspondence but also due to the lack of depth cues in the 2D hurricane images (scaled orthographic projection). In this paper, we present a structure and motion analysis system, called SMAS. In this system, the hurricane images are first segmented into small square areas. We assume that each small area is undergoing similar nonrigid motion. A suitable nonrigid motion model for cloud motion is first defined. Then, non-linear least-square method is used to fit the nonrigid motion model for each area in order to estimate the structure, motion model, and 3D nonrigid motion correspondences. Finally, the recovered hurricane-top heights and winds are presented along with an error analysis. Both structure and 3D motion correspondences are estimated to subpixel accuracy. Our results are very encouraging, and have many potential applications in earth and space sciences, especially in cloud models for weather prediction.",
An efficient algorithm for leader-election in synchronous distributed systems,"Leader election is an important problem in distributed computing. H. Garcia-Molina's (1982) Bully algorithm is a classic solution to leader election in synchronous systems with crash failures. In this paper, we indicate the problems with the Bully algorithm and re-write it to use a failure detector instead of explicit time-outs. We show that this algorithm is more efficient than Garcia-Molina's one in terms of processing time.",
Adaptive Performance Prediction for Distributed Data-Intensive Applications,"The computational grid is becoming the platform of choice for large-scale distributed data-intensive applications. Accurately predicting the transfer times of remote data files, a fundamental component of such applications, is critical to achieving application performance. In this paper, we introduce a performance prediction method, AdRM (Adaptive Regression Modeling), to determine file transfer times for network-bound distributed data-intensive applications. We demonstrate the effectiveness of the AdRM method on two distributed data applications, SARA (Synthetic Aperture Radar Atlas) and SRB (Storage Resource Broker), and discuss how it can be used for application scheduling. Our experiments use the Network Weather Service [36, 37], a resource performance measurement and forecasting facility, as a basis for the performance prediction model. Our initial findings indicate that the AdRM method can be effective in accurately predicting data transfer times in wide-area multi-user grid environments.",
A model for adaptive monitoring configurations,"With the increased availability and complexity of distributed systems comes a greater need for solutions to assist in the management of distributed system components. Despite the significant contributions made towards the development of management tools that monitor and control distributed systems, little has been done to address issues such as the cost of management and how it can adapt to the dynamic changes in user requirements as well as system resources. We present an adaptive model in which an initial optimal configuration of management agents is determined according to a set of user/system requirements. These agents can later be dynamically reconfigured to adapt to changes in resource availability and user/system constraints, with minimal effect on the behaviour of the managed system components. Algorithm, prototype, and experimental results are presented.",
The probability of multiple correct packet reception in coded synchronous frequency-hopped spread-spectrum networks,"We present a computationally efficient method of evaluating the probability of multiple correct packet reception in coded synchronous frequency-hopped spread-spectrum (FHSS) networks. We show that the approximation using the independent receiver operation assumption (IROA), which has been frequently employed in the literature without rigorous validation, produces reasonable results in most network load conditions when compared to the exact computations derived from our proposed method. Specifically, the expected value of the absolute error was in the range of 0.0055%-18.21% in the investigated scenarios.",
Fusion of fixation and odometry for vehicle navigation,"Fixation is shown to be a visual routine which reduces dead-reckoning errors accumulated by an autonomous guided vehicle (AGV). By fixating on a landmark as the vehicle moves one can improve the navigation accuracy even if the scene coordinates of the landmark are unknown. In contrast with previous methods which assume that the coordinates of the landmark are known, our method enables any point of the observed scene to be selected as a landmark, and not just pre-measured points. Moreover, in contrast with other methods, in fixation only one point needs to be tracked. This disposes of the need to be able to identify which of the landmarks is currently being tracked, through a matching algorithm or by other means. Thus the incorporation of fixation into the navigation process does not involve long computation times and may be done while the vehicle is continuously moving. In addition to the basic method, we suggest an ""emergency procedure"" for obtaining absolute position once the vehicle gets lost. We support our findings with both experimental and simulation results.",
Parallel matrix multiplication on a linear array with a reconfigurable pipelined bus system,"The known fast sequential algorithms for multiplying two N/spl times/N matrices (over an arbitrary ring) have time complexity O(N/sup /spl alpha//), where 2",
Development of a real-time radon monitoring system for simultaneous measurements in multiple sites,"A real-time radon monitoring system that can simultaneously measure radon concentrations in multiple sites was developed and tested. The system consists of maximum of four radon detectors, optical fiber cables and a data acquisition personal computer. The radon detector uses a plastic scintillation counter that collects radon daughters in the chamber electrostatically. The applied voltage on the photocathode for the photomultiplier tube (PMT) acts as an electrode for radon daughters. The thickness of the plastic scintillator was thin, 50 /spl mu/m, so as to minimize the background counts due to the environmental gamma rays or beta particles. The energy discriminated signals from the radon detectors are fed to the data acquisition personal computer via optical fiber cables. The system made it possible to measure the radon concentrations in multiple sites simultaneously.",
Autonomy and software technology on NASA's Deep Space One,"NASA's Deep Space One mission is unprecedented. Traditionally, NASA's space missions have been justified by science data return as the primary, if not the sole consideration. DS1 is the first NASA mission whose main purpose is to demonstrate the flight readiness of a set of technologies. The article shows how various AI related technologies are helping to launch NASA into the exciting new era of autonomous space vehicles. The article presents five autonomy-technology experiments and related software engineering activities on DS1 which are paving the way for the use of autonomy capabilities in future NASA missions: proving technologies, reducing perceived risk, and ameliorating first-user costs. NASA is entering the era of autonomous space systems, and the results achieved on DS1 are already leading to applications of the autonomy technologies described here, as well as inspiring additional autonomy-technology development work.",
AceDB: a genome database management system,"The Human Genome Project has spawned many databases using custom software or with conventional data management systems. Surprisingly, amidst this wealth of genome database systems, few ready-made systems are available to a group that wishes to set up its own genome database or even to an individual interested in experimenting with them to learn more about bioinformatics. Many genome database systems are unavailable for distribution or are so specialized to their task that it is impractical to adapt them for use at another site. Jean Thierry-Mieg and Richard Durbin disproved this generalization by designing AceDB-an object-oriented database system-from the ground up to represent genomes and other complex biological data. Application programmers can use its APIs to fetch, examine, and modify data maintained in a variety of AceDB genome databases.",
A parabolic operator for parameter optimization problems,"Parameter optimization has been a prime target for evolutionary algorithms for a number of years. Genetic algorithms, evolution strategies, and evolutionary programming have dealt with a variety of nonlinear programming problems. There is a growing evidence that evolutionary algorithms are well suited for optimization of real valued multi-modal difficult functions of many variables. Despite this success story, there are still many open, interesting questions. One of them deals with a relationship between the recombination operators and the landscape of the problem; it seems that different problems ""require"" different operators. We propose a new multi-parent crossover operator: a parabolic crossover, which works very well for certain types of landscapes. Additionally, this operator maintains an interesting balance between its exploratory and exploitative capabilities and has potential for further generalizations.",
IEEE Transactions on Software Engineering,,
Guided crossover: a new operator for genetic algorithm based optimization,"Genetic algorithms (GAs) have been extensively used in different domains as a means of doing global optimization in a simple yet reliable manner. They have a much better chance of getting to global optima than gradient-based methods which usually converge to local sub-optima. However, GAs have a tendency of getting only moderately close to the optima in a small number of iterations. To get very close to the optima, the GA needs a very large number of iterations, whereas gradient-based optimizers usually get very close to local optima in a relatively small number of iterations. In this paper we describe a new crossover operator which is designed to endow the GA with gradient-like abilities without actually computing any gradients and without sacrificing global optimality. The operator works by using guidance from all members of the GA population to select a direction for exploration. Empirical results in several engineering design domains demonstrate that the operator can significantly improve the steady state error of the GA optimizer.",
A torus assignment for an interconnection network recursive diagonal torus,"Recursive Diagonal Torus (RDT) is a class of interconnection network consisting of recursively overlaid two-dimensional square diagonal tori for massively parallel computers, with up to 216 nodes. Connection structures of the RDT vary according to the assignment of upper rank diagonal tori into a node. Although traditional simple assignment called RDT(2,4,1)//spl alpha/ shows enough performance under the uniform traffic, the congestion of low rank tori degrades the performance when local communication is dominant. RDT(2,4,1)//spl beta/ torus assignment is proposed, focusing on improving the performance for local communication. With a simplified simulation algorithm, the result shows that RDT(2,4,1)//spl beta/ improves the average distance compared with RDT(2,4,1)//spl alpha/ assignment when considering local area.",
Darwinian invention and problem solving by means of genetic programming,Demonstrates that genetic programming is capable of automatically creating computer programs from a high-level statement of a problem's requirements and of producing human-competitive results.,
Analog Neural Nets with Gaussian or Other Common Noise Distributions Cannot Recognize Arbitrary Regular Languages,"We consider recurrent analog neural nets where the output of each gate is subject to gaussian noise or any other common noise distribution that is nonzero on a sufficiently large part of the state-space. We show that many regular languages cannot be recognized by networks of this type, and we give a precise characterization of languages that can be recognized. This result implies severe constraints on possibilities for constructing recurrent analog neural nets that are robust against realistic types of analog noise. On the other hand, we present a method for constructing feedfor-ward analog neural nets that are robust with regard to analog noise of this type.",
The effects of a mobile agent on file service,"Implementing an application as a mobile agent may improve the application's functionality and performance, but may have a detrimental effect on overall system performance. We consider the effect of moving an application from a client to a file server (as an agent), both on the application and the server. Under what circumstances does application performance improve, and does it come at the expense of other (non-mobile) background applications using the same server? We use a trace-driven simulation to measure the effect of mobile code, allowing system parameters such as the size of the server memory and server speed relative to client speed to be varied. We found that several factors influence the benefit of mobile agents. Server memory does not appear to be a significant problem; relatively small server caches have a high hit rate even when shared with mobile agents. The relative CPU performance of the client and server has a bigger effect on system performance: mobile agents should not be run on the server if its CPU is a bottleneck.",
A new rule extraction method from neural networks,"This paper presents a method of extracting rules from multilayered neural networks (NN) formed using a random optimization (search) method (ROM). The objective of this study is to extract rules from NN, achieving 100% recognition accuracy in a pattern recognition system. NNs to be extracted rules are formed using ROM. A hybrid algorithm of NN and ROM performs a formation of a small-sized NN system, which is suitable for a rule extraction. In this paper iris data is used as inputs. ROM is utilized to reduce the number of connection weights in NN. The network weights survived after the ROM training represent regularities to perform pattern classification. The rules are then extracted from the networks in which hidden units use signum and sigmoid functions to produce binary outputs. It enables us to extract simple logical functions from the network. By means of computer simulation, the effectiveness of this approach is examined.",
Improving speech recognition performance by using multi-model approaches,"Most current speech recognition systems are built upon a single type of model, e.g. an HMM or certain type of segment based model, and furthermore typically employs only one type of acoustic feature e.g. MFCCs and their variants. This entails that the system may not be robust should the modeling assumptions be violated. Recent research efforts have investigated the use of multi-scale/multi-band acoustic features for robust speech recognition. This paper described a multi-model approach as an alternative and complement to the multi-feature approaches. The multi-model approach seeks a combination of different types of acoustic models, thereby integrating the capabilities of each individual model for capturing discriminative information. An example system built upon the combination of the standard HMM technique with a segment-based modeling technique was implemented. Experiments for both isolated-word and continuous speech recognition have shown improved performances over each of the individual models considered in isolation.",
Reconfigurable systolic Viterbi decoder,"This paper introduces a new algorithm which saves the power consumption of the systolic Viterbi decoder. The new algorithm dynamically changes the truncated path length of a Viterbi decoder according to the channel condition, resulting in reduction of power consumption. This algorithm is based on the observation that the truncated path length and bit error rate are closely related. If we set the truncated path length short, we can reduce the size of the decoder even though the system performance is sacrificed. We propose a reconfiguration of the truncated path length according to channel state. It is shown that power consumption of a systolic Viterbi decoder with convolutional code for a constraint length K=3 and a code rate R=1/2 can be eliminated over 20 percent at the bit error rate of 10/sup -3/ in a Rayleigh fading channel. Furthermore, the longer the truncated path length becomes, the more effective the proposed method is.",
Metrics for evaluating database selection techniques,"The increasing availability of online databases and other information resources in digital libraries has created the need for efficient and effective algorithms for selecting databases to search. A number of techniques have been proposed for query routing or database selection. We have developed a methodology and metrics that can be used to directly compare competing techniques. They can also be used to isolate factors that influence the performance of these techniques so that we can better understand performance issues. We describe the methodology we have used to examine the performance of database selection algorithms such as gGIOSS and CORI. In addition we develop the theory behind a ""random"" database selection algorithm and show how it can be used to help analyze the behavior of realistic database selection algorithms.",
Utilization bound re-visited,"Utilization bound is a well-known concept first introduced in the seminal paper of Liu and Layland (1973) which provides a simple and practical way to test the schedulability of a real-time task set. The original utilization bound for the fixed-priority scheduler was given as a function of the number of tasks in the periodic task set. We define the utilization bound as a function of the information about the task set. By making use of more than just the number of tasks, we obtain various improvements over the Liu and Layland bound. In particular, we give a more intuitive derivation of the bound as a function of the number of harmonic chains in the task periods which allows us to derive a simpler algorithm to calculate such bounds. We derive algorithms that yield better bounds as a function of the period parameters of the task set. We also give a generalization of the bound for tasks whose deadlines are smaller than their periods.",
Self-adaptation and global convergence: a counter-example,"The self-adaptation of the mutation distribution is a distinguishing feature of evolutionary algorithms that optimize over continuous variables. It is widely recognized that self-adaptation accelerates the search for optima and enhances the ability to locate optima accurately, but it is generally unclear whether these optima are global ones or not. Here, it is proven that the probability of convergence to the global optimum is less than one in general, even if the objective function is continuous.",
Teaching architectural design in an undergraduate software engineering curriculum,"In the fall of 1996, the Rochester Institute of Technology (USA) began admitting students to the nations first baccalaureate program in software engineering. One of the curriculum's primacy themes is software design, ranging from the very concrete (e.g., code, data structures and algorithms) to the more abstract (e.g., subsystems, architectural designs). At each of these levels the authors are able to address the variety of software quality issues that arise at different points in the software development process. This paper describes the structure of their introductory course in software architectures and their experience teaching it. They also speculate on how the course could evolve over time.",
Extended retiming: optimal scheduling via a graph-theoretical approach,"Many iterative or recursive applications commonly found in DSP and image processing applications can be represented by data-flow graphs (DFGs). This graph is then used to perform DFG scheduling, where the starting times for executing the application's individual tasks are determined. The minimum length of time required to execute all tasks once is called the schedule length of the DFG. A great deal of research has been done attempting to optimize such applications by applying various graph transformation techniques to the DFG in order to minimize this schedule length. One of the most effective of these techniques is retiming. We demonstrate that the traditional retiming technique does not always achieve optimal schedules and propose a new graph transformation technique, extended retiming, which will. We also present an algorithm for finding an extended retiming which transforms a DFG into one with minimal schedule length. Finally, we demonstrate a constant-time algorithm which verifies the existence of a retimed DFG with the minimum schedule length.",
A client/server case study for software engineering students,"A goal of the Studio course in the Master of Software Engineering program at Carnegie Mellon University is to bridge the gap between experience and academics. One way to transfer experience to young software engineers is through case studies designed to focus students on specific software engineering problems. This paper discusses my experience with developing a case study to improve students' analytical capabilities and introduce the importance of considering maintenance and implementation issues in software design. The case study, developed as a classroom assignment, proved an effective tool to teach software engineering students that there are more things to consider than performance specifications.",
Towards understanding constraint-handling methods in evolutionary algorithms,"The experimental results reported in many papers suggest that making an appropriate a priori choice of an evolutionary method for a nonlinear parameter optimization problem remains an open question. It seems that the most promising approach at this stage of research is experimental, involving a design of a scalable test suite of constrained optimization problems, in which many features could be easily tuned. Then it would be possible to evaluate merits and drawbacks of the available methods as well as test new methods efficiently. In this paper we discuss a recently proposed test-case generator for constrained parameter optimization techniques. This generator is capable of creating various test cases with different characteristics and is very useful for analyzing and comparing different constraint-handling techniques.",
An in-depth look at flow aggregation for efficient quality of service,"We investigate the preservation of quality of service guarantees to a flow of packets in the presence of flow aggregation. For efficiency, multiple flows, known as the constituent flows, are merged together resulting in a single aggregate flow. Packet schedulers located after the network point where the aggregation occurred are aware of the aggregate flow, but are unaware of its constituent flows. In spite of this, we show that quality of service guarantees may still be offered to the constituent flows provided the aggregation is performed fairly. In earlier work, we showed that the end-to-end delay is preserved (and in some cases improved) under flow aggregation, when the packet delay is coupled with the reserved rate of the flow. We go beyond these results by showing that, even when the delay is de-coupled from the reserved rate, the end-to-end delay is preserved under flow aggregation.",
Triple-head coincidence imaging,"Coincidence imaging with a triple-head gamma-camera offers the possibility of greater sensitivity than dual-head coincidence gamma-camera systems. Using data collected from computer simulations, this paper compares both systems and reports on the sensitivity improvements that can be expected using a triple-head system. It is found that the sensitivity profiles for triple-head systems remain uniform across the field-of-view, unlike the profiles for a dual-head system which decrease as one moves away from the center of the field-of-view. The actual increase in sensitivity depends upon the configuration of the heads and the size and location of the object. Increases of 80% for a small organ orbit or 65% for a large organ orbit are achievable.",
Synthesis of two-level dynamic CMOS circuits,"CMOS circuits are presently used for the realization of VLSI circuits because of low power consumption and high integration density. But, because of the use of both n-block and D-block, static full complementary CMOS circuits are not area efficient. The problem is overcome in dynamic circuits using either a p-block or a n-block in the realization of digital circuits. However, cascading of dynamic circuits leads to the serious problem of clock skew. To overcome this problem, domino and NORA techniques have been proposed. This paper proposes a novel approach for the synthesis of two-level dynamic circuits with the minimum number of gates and with an optimized number of transistors using domino and NORA techniques. The NORA implementation is completely inverter-free. The approach is based on a novel concept called unate decomposition, which decomposes any given Boolean function in terms of unate functions allowing seamless realization using dynamic CMOS gates.",
A genetic algorithm for the cryptanalysis of Chor-Rivest knapsack public key cryptosystem (PKC),"We develop a genetic algorithm as a method for cryptanalysing the Chor-Rivest knapsack public key cryptosystem (PKC) (B. Chor and R.L. Rivest, 1988). As far as we know there is no feasible attack known on it (A.J. Menezes, 1997). The results show how the algorithm is effectively used to break this scheme by examining a very small fraction of the space of possible solutions. The algorithm found the exact solution in all attempted cases.",
Efficient regular data structures and algorithms for location and proximity problems,"Investigates data structures obtained by a recursive partitioning of the input domain into regions of equal size. One of the most well-known examples of such a structure is the quadtree, which is used in this paper as a basis for more complex data structures; we also provide multidimensional versions of the stratified tree of P. van Emde Boas (1997). We show that, under the assumption that the input points have limited precision (i.e. are drawn from an integer grid of size u), these data structures yield efficient solutions to many important problems. In particular, they allow us to achieve O(log log u) time per operation for finding the dynamic approximate nearest neighbor (under insertions and deletions) and the exact online closest pair (under insertions only) in any constant dimension. They allow O(log log u) point location in a given planar shape or in its expansion (dilation by a ball of a given radius). Finally, we provide a linear-time (optimal) algorithm for computing the expansion of a shape represented by a quadtree. This result shows that the spatial order imposed by this regular data structure is sufficient to optimize the dilation by a ball operation.",
Incorporating error recovery into the imprecise computation model,"We describe optimal algorithms for incorporating error recovery in the imprecise computation model. In this model each task comprises a mandatory and an optional part. The mandatory part must be completed within the task's deadline even in the presence of faults and a reward function is associated with the execution of each optional part. We address the problem of optimal scheduling in an imprecise computation environment so as to maximize total reward while simultaneously guaranteeing timely completion of recovery operations when faults occur. Furthermore, in order to prevent run-time overhead we enforce that no changes in the optimal schedule should be necessary as long as no error is detected in mandatory parts. Independent imprecise computation tasks as well as tasks with an end-to-end deadline and linear precedence constraints are considered. We present polynomial-time optimal algorithms for models with upper and lower bounds on execution times of the optional parts and for reward functions represented by general nondecreasing linear and concave functions.",
Testing and analysis of a flexible feeding system,"Flexible parts feeding techniques have begun to gain industry acceptance. However, one barrier to effective flexible feeding solutions is a dearth of knowledge of the under lying dynamics involved in flexible part feeders. The paper presents the results of testing the CWRU flexible parts feeder. Data was collected for extended periods while feeding a variety of parts. The data was examined to determine throughput and statistical properties. In addition, tests were performed to examine other aspects of the system. A new metric for specifying the throughput of vision-based flexible feeders is presented, interesting system phenomena are examined, and a statistical analysis of the data is performed.",
Providing support for multiple collection types in a fuzzy object oriented spatial data model,"Fuzzy set approaches are particularly suitable for issues of modeling uncertainty in spatial data. Previous work of the authors describes a framework to support uncertainty by using an object-oriented approach to modeling spatial data. The original research focused on how to incorporate spatial data into a fuzzy object model. This paper expands upon that by discussing the implications of incorporating all collection types described in the ODMG object database standard in this framework. In addition, we will look at how future collection types may be incorporated into the framework.",
Multiprocessor memory reference generation using Cerberus,"Presents Cerberus, an efficient system for simulating the execution of shared-memory, multiprocessor programs on a uniprocessor workstation. Using EDS (execution-driven simulation), it generates address traces which can be used to drive cache simulations on-the-fly, eliminating the large disk space requirements needed by trace files. It is fast because it links the program to be traced together with the cache or statistics gathering tool into a single executable, which eliminates the context switching needed by communicating processes. It is flexible because it has a simple interface which allows users to easily add any kind of module to use the generated trace information. It compares favorably to other existing tracers; it runs on a commonly available workstation and it is accurate, allowing cycle-by-cycle interactions between the simulated processors. The resulting slowdown from Cerberus is approximately 31 in uniprocessor mode and 45-50 in multiprocessor mode, relative to the workloads run natively on the same machines. We demonstrate that EDS uses only 5% of the total execution cycles when combined with a cache simulator, and we show that EDS is just as efficient as using trace-driven simulation.",
Interactive lens visualization techniques,"The paper describes new techniques for minimally immersive visualization of 3D scalar and vector fields, and visualization of document corpora. In our glyph based visualization system, the user interacts with the 3D volume of glyphs using a pair of button-enhanced 3D position and orientation trackers. The user may also examine the volume using an interactive lens, which is a rectangle that slices through the 3D volume and displays scalar information on its surface. A lens allows the display of scalar data in the 3D volume using a contour diagram, and a texture based volume rendering.",
Building evolution friendliness into cellular automaton dynamics: the cytomatrix neuron model,"The cytomatrix neuron is a softened cellular automaton, roughly motivated by interactions that could occur in a molecular or cellular complex. Input signals are combined in space and time by subcells that exert graded influences on each other. Output is triggered if a readout element is located in a suitably activated subcell. Multiple parameters are open to evolution. Extensive experimentation with the model shows that the dynamics can be molded to produce different structures of generalization. Dimensionality can be increased by increasing the number of dynamical parameters open to variation and selection. Learning algorithms that vary the greatest number of parameters were found to have a greater variability in the structures of generalization and to yield higher performance values and learning rates. Here we focus on n-bit exclusive-OR tasks that are known to be hard due to their linear inseparability. The system successfully learned 2 bit and 4 bit X-OR functions. The higher dimensional algorithms exhibited a relatively good performance on the 8 bit X-OR function.",
The impact of computer mediated communication on information overload in distributed teams,"Investigates the impact of computer-mediated communication (CMC) in distributed teams on real or perceived information overload. Typically, these organizations rely on CMC for communication because teams working on joint projects are spatially dispersed and have to communicate across space and time zones. Many of the problems observed in virtual teams seem to be related to communication in general, and it is unclear how this is affected by the communication medium used. Two surprisingly contradicting statements from users of these systems are: ""I do not have the necessary information to do my job"" and ""I am swamped with information"". This paper extracts a cause-effect relationship based on established theories. Supporting theory and literature used to analyse the topic can be grouped into three areas: media choice theories, media consequence theories and information overload literature. There is little overlap in these research domains. Prior research on information load neglected the impact on organizational infrastructure, particularly the computer information systems environment. However, this is an area where severe degrees of overload are experienced or perceived by users. The reviewed literature and theories are summarized and expanded into a research model. A survey conducted within an IT service provider is reported to perform a preliminary test of the model. Based on the empirical data, appropriate modifications of the model were made and areas of interest for further research were identified. The paper finally discusses practical implications of the research results.",
Reduction of active losses with reconfiguration of electricity distribution networks,"Electricity distribution networks in densely populated urban areas are, in order to increase the reliability of supply, usually designed as meshed networks, though they always operate in radial configuration. Reconfiguration of distribution networks is achieved through switching operations on switching devices of distribution network branches (switching on/off of a branch). It is often the case that the existing configuration is not the configuration with minimum power losses. With the new computer program, developed by the authors, it is possible to find the network configuration that enables operation with minimum losses. The program proposes the best network configuration to the dispatcher, and the latter makes his own decision whether to change the network configuration or not. The mathematical algorithm for the network reconfiguration is presented in the paper, as well as results of the reconfiguration of the electricity distribution network of the city of Maribor, Slovenia.",
Intra-address space protection using segmentation hardware,"The technological evolution towards extensible software systems and component-based software development requires efficient, flexible and easy-to-use protection mechanisms to isolate software modules residing in the same address space. While a number of software-based approaches have been proposed in the last several years, no clear winner emerges that satisfactorily addresses all the design issues involved in establishing multiple protection domains within an address space. This paper describes a novel intra-address space protection mechanism based on the segmentation and paging hardware in Intel's x86 architecture. Measurements from the prototype implementation indicate that a null protected procedure call and return costs about 147 CPU cycles on a Pentium 200 MHz machine.",
Neural network processing for adaptive array antennas,"Currently, several algorithms can be used to perform the direction finding or angle of arrival of signals from mobile users. One drawback of these algorithms is the difficulty of their implementation in real-time because of their intensive computational complexity. Neural networks, on the other hand, due to their high-speed computational capability, can yield results in real-time. Moreover, conventional beamformers require highly calibrated antennas with identical element properties. Performance degradation often occurs due to the fact that these algorithms poorly adapt to element failure or other sources of errors. Neural network-based array antennas do not suffer from this shortcoming.",
Dynamic vectorization: a mechanism for exploiting far-flung ILP in ordinary programs,"Several ILP limit studies indicate the presence of considerable ILP across dynamically far-apart instructions in program execution. This paper proposes a hardware mechanism, dynamic vectorization (DV), as a tool for quickly building up a large logical instruction window. Dynamic vectorization converts repetitive dynamic instruction sequences into vector form, enabling the processing of instructions from beyond the corresponding program loop to be overlapped with the loop. This enables vector-like execution of programs with relatively complex static control flow that may not be amenable to static, compile time vectorization. Experimental evaluation shows that a large fraction of the dynamic instructions of four of the six SPECInt92 programs can be captured in vector form. Three of these programs exhibit significant potential for ILP improvements from dynamic vectorization, with speedups of more than a factor of 2 in a scenario of realistic branch prediction and perfect memory disambiguation. Under perfect branch prediction conditions, a fourth program also shows well over a factor of 2 speedup from DV. The speedups are due to the overlap of post-loop processing with loop processing.",
Matching attributes in a fuzzy case based reasoning,"This paper describes a fuzzy expert case-based reasoning system. The idea is to combine methodologies from both technologies to come up with a system that utilizes inference procedures with matching algorithms used by case-based reasoning systems. We describe the system and, with examples, show how it can be utilized to solve problems in a more natural way than some of the existing case-based reasoning systems.",
A development system for creating real-time machine vision hardware using field programmable gate arrays,"In this paper, we introduce a new development system for creating real-time image processing hardware using custom computing machines with multiple Field Programmable Gate Array (FPGA) chips. Three distinct processes are accomplished within the development system: design entry, verification, and translation. A library of modules that implement common low-level machine vision functions is used to create complex designs based on a dataflow graph representation. The library's low-level image processing modules contain both gate-level and chip-level hardware components, of which the gate-level components are compiled into the functionality of available FPGA chips. Standard interfaces are established for input/output of the modules, allowing for the creation of sophisticated software support tools. Experimental results verify the utility of this development system for easily creating real-time machine vision hardware using multiple FPGA-based custom computing machines.",
A load balancing algorithm for a distributed multimedia game server architecture,"Network game servers, in which a huge number of users can play a common game through interconnecting network, can be designed in a distributed system architecture with the benefit of scalability, reliability, and cost effectiveness. In order to develop an efficient and scalable network game server system, the paper investigates the issue of load balancing for dynamic change of workload in a network game. We propose a dynamic load balancing algorithm for network games which takes into account the geographical relationship among game units and the short response time of frequent user interactions. We also present simulation results of the proposed algorithm.",
A hybrid swing up controller for a two-link brachiating robot,"We report on a ""hybrid"" scheme for regulating the swing up behavior of a two degree of freedom brachiating robot. In this controller, a previous ""target dynamics"" controller and a mechanical energy regulator are combined. The proposed controller guarantees the boundedness of the total energy of the system. Simulations suggest that this hybrid controller achieves much better regulation of the desired swing motion than the target dynamics method by itself.",
Current status of DIII-D real-time digital plasma control,This paper describes the current status of real-time digital plasma control for the DIII-D tokamak. The digital plasma control system (PCS) has been in place at DIII-D since the early 1990s and continues to expand and improve in it's capabilities to monitor and control plasma parameters for DIII-D fusion science experiments. The PCS monitors over 200 tokamak parameters from the DIII-D experiment using a real-time data acquisition system that acquires a new set of samples once every 60 /spl mu/s. This information is then used in a number of feedback control algorithms to compute and control a variety of parameters including those affecting plasma shape and position. A number of recent improvements to the control capabilities of the PCS along with general system enhancements have contributed to the advancement and understanding of fusion energy science at DIII-D.,
Balancing Steiner minimum trees and shortest-path trees in the rectilinear plane,"We consider the problem of balancing shortest-path trees and Steiner minimum trees in the rectilinear plane. This problem has applications in telecommunication, wire routing and VLSI physical design. We present a polynomial time approximation algorithm which simultaneously approximates both the shortest-path tree and the Steiner minimum tree. Previous results guarantee a balanced approximation ratio of about 2.4142. Our algorithm improves the value to 2.0806.",
Stability bounds on step-size for the partial update LMS algorithm,"Partial updating of LMS filter coefficients is an effective method for reducing the computational load and the power consumption in adaptive filter implementations. Only in the recent past has any work been done on deriving conditions for filter stability, convergence rate, and steady state error for the partial update LMS algorithm. Douglas (see IEEE Trans. Circuits and Systems-II: Analog and Digital Signal Processing, vol.44, p.209-16, 1997) derived approximate bounds on the step-size parameter /spl mu/ which ensure stability in-the-mean of the alternating even/odd index coefficient updating strategy. Unfortunately, due to the restrictiveness of the assumptions, these bounds are unreliable when fast convergence (large /spl mu/) is desired. In this paper, tighter bounds on /spl mu/ are derived which guarantee convergence in the mean of the coefficient sequence for the case of wide sense stationary signals.",
Replacing a hospital information system: an example of a real-world case study,"Real-world case studies are important to complement the academic skills and knowledge acquired by computer science students. In this paper we relate our experiences with a course specifically designed to address this issue. The objectives of the course are threefold: to train management and communication skills, to integrate and apply knowledge gained at different previous courses, and to learn by experience the difference between a real-world problem and a textbook problem. Students' evaluations show that the objectives of the course are met and that it is regarded as very useful. We found that the three objectives mutually reinforce each other, which is a decisive factor for the success of the course.",
Molecular dynamics simulation,"The article presents a discussion on molecular dynamics (MD) simulation. MD requires a description of the molecules and the forces that act between them; a well known example is the Lennard-Jones potential, in which spherical particles repel one another at close range but otherwise attract. The MD simulation itself amounts to numerically integrating the equations of motion for systems of between a few hundred and a few million particles over many thousand (or more) timesteps. The paths the particles follow during the computation represent actual molecular trajectories. What does the future hold? MD simulation covers length scales ranging from the atomistic to entire microstructures. It has proved capable of studying a broad range of phenomena associated with both simple and complex molecules. It is free of many of the simplifying assumptions that tend to dominate theory and other modeling techniques. So, after making the reasonable extrapolation that computer power will continue to grow at its present rate, the author has little doubt that MD is destined to play an ever-increasing role in both science and engineering.",
Data-driven shape-from-shading using curvature consistency,"This paper makes two contributions to the problem of needle-map recovery using shape-from-shading. Firstly, we provide a geometric update procedure which allows the image irradiance equation to be satisfied as a hard-constraint. This improves the data-closeness of the recovered needle-map. Secondly, we consider how topographic constraints can be lured to impose local consistency on the recovered needle-map. We present several alternative curvature consistency models, and provide an experimental assessment of the new shape-from-shading framework on both real-world images and synthetic images with known ground-truth surface-normals. The main conclusion drawn from our analysis is that the new framework allows rapid development of more appropriate constraints on the SFS problem.",
A DCT-domain H.263 based video combiner for multipoint continuous presence video conferencing,This paper proposes an H.263 based DCT-domain video combiner which is suitable for a multipoint continuous presence videoconference system and supports up to six conferees. The main issues of the H.263 video combiner are discussed. A software-based combiner is implemented and tested for various test sequences. The combined videos have promising quality and the combiner is considered very efficient for practical usage.,
A flexible rerouting protocol in ATM networks,"The paper introduces a protocol for rerouting calls in an ATM network, referred to as the flexible trerouting protocol (FRP). Failure of a VP triggers construction of an alternate VP out of VPs that are not in use, named stand-by VPs, by simple operations at the endpoints of the stand-by VPs. VC rerouting from the failed VP to the alternate VP is performed concurrently. FRP is designed to also allow recovery from repeated failures that may occur during the construction and rerouting process.",
Handover prioritised schemes for optimal capacity and overload management in cellular mobile systems,"This paper analyses the capacity maximisation in the presence of new calls and handovers (HO) in cellular mobile systems. The analysis of HO prioritisation, leads to the definition of the optimal system configuration (e.g., number of guard channels). It is shown that a 'guard channels' based scheme does not maximise but approximates the maximum capacity. A new HO prioritised scheme, that maximises the cell capacity, is then proposed. The scheme uses 'unequally shared channels', which are allocated to new calls and HO with a different priority guided by a control parameter. Finally, an analysis on the system behaviour during overload conditions leads to the identification of the optimal scheme control.",
Focusing on mobility,"In this paper we motivate the importance of the field of mobile computing and survey current practical and formal approaches. We argue that the existing formalisms are not sufficiently general and powerful because they do not model all necessary concepts of mobility adequately. The main contribution of the paper is, therefore, to identify and define the fundamental concepts of mobile systems by providing a precise, mathematical foundation. The model we present is an extended variant of existing, compositional network models for control and data flow of non-mobile systems, enriched by the concept of locations as places containing components. To model the migration of a component from one location to another, the containment relation may change dynamically over time. Based on this formal model, we define a number of fundamental properties and characteristics such as network transparency. Finally, we demonstrate how existing description techniques may be extended in the context of mobility, and sketch a supporting CASE tool.",
MP-LOCKs: replacing H/W synchronization primitives with message passing,"Shared memory programs guarantee the correctness of concurrent accesses to shared data using interprocessor synchronization operations. The most common synchronization operators are locks, which are traditionally implemented via a mix of shared memory accesses and hardware synchronization primitives like test-and-set. In this paper, we argue that synchronization operations implemented using fast message passing and kernel-embedded lock managers are an attractive alternative to dedicated synchronization hardware. We propose three message passing lock (MP-LOCK) algorithms (centralized, distributed, and reactive) and provide implementation guidelines. MP-LOCKs reduce the design complexity and runtime occupancy of DSM controllers and can exploit software's inherent flexibility to adapt to differing applications lock access patterns. We compared the performance of MP-LOCKs with two common shared memory lock algorithms: test-and-test-and-set and MCS locks and found that MP-LOCKs scale better. For machines with 16 to 32 nodes, applications using MP-LOCKs ran up to 186% faster than the same applications with shared memory locks. For small systems (up to 8 nodes), three applications with MP-LOCKs slow down by no more than 18%, while the other two slowed by no more than 180% due to higher software overhead. We conclude that locks based on message passing should be considered as a replacement for hardware locks in future scalable multiprocessors that support efficient message passing mechanisms.",
Efficiently computing frequent tree-like topology patterns in a Web environment,"Data mining has recently emerged in a number of real life applications, such as banking, supermarketing, and interneting. The authors investigate a novel data mining problem derived from the applications of WWW. They present an efficient algorithm for solving the problem. The initial implementation suggests that the algorithm is very efficient and scalable in practice.",
Making sharing pervasive: Ubiquitous computing for shared note taking,"As a variety of low-cost note-taking devices becomes pervasive, shared notes can help work groups better communicate ideas and information. To explore this idea further, we carried out three related case studies of how members of a large research group shared meeting notes. The group found value in combining personal notes and presentation slides with a single, unifying document, such as regular meeting minutes. The minutes provided structure when there were too many sources of notes. We used this insight in our design of NotePals, a note-sharing system with a lightweight process, an interface, and hardware that distinguish it from previous systems. We have developed note-taking applications that run on inexpensive personal digital assistants and other ink-based capture devices, such as the paper-based CrossPadâ„¢. Experience with using NotePals has shown that shared notes can add value to meeting, conference, and class records.",
Applying model checking to concurrent object-oriented software,"Model checking is a formal verification technique which checks the consistency between a requirement specification and a behavior model of the system by exploring the state space of the model. We apply model checking to formal verification of concurrent object-oriented systems, using an existing model checker SPIN which has been successful in verifying parallel systems. First, we propose an Actor-based modeling language, called APromela, by extending a modeling language Promela which is a modeling language supported in SPIN. APromela supports not only all the primitives of Promela, but additional primitives needed to model concurrent object-oriented systems, such as class definition, object instantiation, message send, and synchronization. Second, we provide translation rules for mapping APromela's such modeling primitives to Promela's. By giving an example of specification, translation, and verification, we also demonstrate the applicability of our proposed approach, and discuss the limitations and further research issues.",
Medical diagnostic systems using ensembles of neural SOFM classifiers,"A design for medical diagnostic systems composed of ensembles of neural self organizing feature map (SOFM) classifiers is presented. Each SOFM classifier was fed with a different feature set extracted from the raw data and their results were combined using (i) majority voting and (ii) a confidence measure derived from the SOFM which weighted the contribution of each feature set to the final classification result. The following two diagnostic systems were developed: (i) a decision support system for the assessment of electromyographic (EMG) signals, and (ii) a system for the characterisation of carotid plaques from ultrasound images. The results in this work shows that combining the classification results of multiple classifiers using as input multiple feature sets, in conjunction with the use of a confidence measure can improve the overall classification performance of the system.",
Towards a real-world oriented workflow system-based on practical experiments for three years,"Current commercial workflow systems handle only information that is in digital format. However, when they are applied to actual business in an office, we sometimes want to manage not only digital information but also human activities and documents, under some constraints such as the work context and computer environment. In this study, a workflow that handles information found in the real world as well as digital information in the system is called a ""real-world oriented workflow"". To study the workflow, we implemented a prototype workflow system for research budget management in a national facility as a case study. Based on this case study, this paper describes the design principles and methods of supporting the real-world oriented workflow. The effectiveness of the prototype workflow system, which was used for actual business for three years, is shown.",
Recovery in distributed extended long-lived transaction models,"We address the recovery and the rollback problem in distributed collaborative transactions. We propose a solution to the problem in a generalized ARIES framework. We modified its existing data structures and provided additional data structures for recovery of distributed extended long-lived transactions. In the proposed model the transactions communicate and collaborate only by exchanging messages. The messages are logged along with usual database actions. In recovery of distributed extended transactions these message logs and message tables are extensively used. The recovery algorithms work in a distributed environment, under extended transaction models, with different kinds of failures and transaction rollback.",
A System Dependence Net generator for Ada programs,An explicit representation of various dependence relationships in a program has many applications in software engineering. The System Dependence Net is a model to represent program dependences in a concurrent program which consists of multiple procedures. The paper explains how to generate a System Dependence Net for Ada 95 programs automatically.,
DASS: a discovery agent supporting system,"A discovery agent, which may be a human or a computing system to conduct and perform knowledge discovery from scientific data or from databases, makes extensive use of domain knowledge in order to increase the efficiency and effectiveness of the discovery process. This knowledge accumulates and becomes huge with the lapse of time or with the evolution of the discovery process. This huge amount of domain knowledge makes it difficult for the automatic or even human discovery agent to effectively use it and retrieve the relevant entities. It thus creates an urgent need to develop a supporting system addressing this issue. In this respect, we propose a supporting system named DASS (Discovery Agent Supporting System). DASS supports the discovery agent in finding the relevant entities of knowledge from its own domain knowledge base and probably from other knowledge based systems relevant to the field and available online. We give an outline of the architecture of DASS.",
An inclusive and extensible architecture for electronic brokerage,"The output and experience of a large European project in Electronic Brokerage called ""Generic Architecture for Information Availability"" (GAIA) is presented. The paper describes a reference model and functional architecture for value-added mediation in Electronic Commerce. The customers are provided with a uniform way of accessing heterogeneous suppliers without changes in the supplier software. A way of employing distributed objects for large scale service integration and multi-enterprise transactions is shown. The issues encountered during the implementation of the CORBA-based pilot prototype and the application of the architecture in 3 diverse domains are presented.",
First-order logic vs. fixed-point logic in finite set theory,"The ordered conjecture states that least fixed-point logic LFP is strictly more expressive than first-order logic FO on every infinite class of ordered finite structures. It has been established that either way of settling this conjecture would resolve open problems in complexity theory. In fact, this holds true even for the particular instance of the ordered conjecture on the class of BIT-structures, that is, ordered finite structures with a built-in BIT predicate. Using a well known isomorphism from the natural numbers to the hereditarily finite sets that maps BIT to the membership relation between sets, the ordered conjecture on BIT-structures can be translated to the problem of comparing the expressive power of FO and LFP in the context of finite set theory. The advantage of this approach is that we can use set-theoretic concepts and methods to identify certain fragments of LFP for which the restriction of the ordered conjecture is already hard to settle, as well as other restricted fragments of LFP that actually collapse to FO. These results advance the state of knowledge about the ordered conjecture on BIT-structures and contribute to the delineation of the boundary where this conjecture becomes hard to settle.",
On the problem of granulometry for a degraded Boolean image model,"We consider a geometric coverage process consisting of a random number of disks, or grains, having random radii and positions in the plane. Our objective is granulometry: estimation of a parameter of the disk radius distribution, which is important in diverse applications such bio-assay, ballistics, and numerical taxonomy. These disks are only incompletely observed due to mutual occlusion, spatial blurring and additive noise. We use a measurement channel paradigm to derive an expectation-maximization (EM) type estimation algorithm and a distortion-rate lower bound on estimation error.",
New binocular vision system for human-robot communications,"Concerns artificial vision for human-robot communications. This paper reported on a new binocular stereovision system to solve the problems of virtual images and occlusion. It introduced the principle of the rotation disparity mechanism, the theoretical principle of detecting and canceling virtual images, and the concept of a technique to detect occlusion. An experimental prototype system was introduced to demonstrate these principles.",
RETWINE: a new distributed environment for microelectronics instrumentation learning and measurement,"The paper proposes an original new way for integrating training of advanced measurement equipment in continuing education for students or engineers. It uses some of the WWW features such as the ability to have multimedia documents, the simplicity of the HTML syntax and particularly the Web's networked structure. The basic idea is that on the one side there is an instrumentation pool (i.e., a number of instruments located anywhere in the world). On the other side a detailed description of the instrument and its front panel are available on the Web and can be accessed by any Web browser for a specific declared user's group. This link permits students or engineers to learn the use of advanced measurement equipment, and to perform real measurements with instruments not available at the university.",
Feasibility study of in situ imaging of Ir-192 source during HDR brachytherapy procedure using a small gamma imager based on a Hamamatsu R3292 PSPMT,"Preliminary results obtained utilizing a compact gamma camera fitted with a pinhole collimator to track the position of a high activity 5.3 Ci pellet of Ir-192 used in high dose rate (HDR) brachytherapy are presented. HDR brachytherapy procedures depend on delivering fractionated therapy doses to designated target volumes by timed insertion of a small (2-4 mm long) seed source of Ir-192 in body cavities or tumors. The Ir-192 source is attached to a wire and pulled/pushed in fine catheters that are operatively inserted in body cavities or tumor volumes. Presently, HDR procedures rely on a computer controlled mechanical delivery system to track the location of the source in the body. The objective of this study is to provide real-time images of the Ir-192 source in-vivo to monitor that the source is delivered to predefined locations for specified dwell times. The compact gamma camera is based on a 5"" Hamamatsu R3292 position sensitive photomultiplier tube with a spatial resolution of about 1 mm. Several scintillator sensors were tested based on thin CsI(Na) crystals and a plastic scintillator (such as Bicron BC 400). The low efficiency of the plastic scintillation detector provided acceptable images of the 5.3 Ci Ir-192 source viewed with a pinhole collimator (dia 0.5 and 1.0 mm) located 16 cm from the source. This gamma camera approach provides a unique solution to the problem of verifying HDR brachytherapy treatments in the clinic.",
Fusion of 2D face alignment and 3D head pose estimation for robust and real-time performance,"In order to perform coherent and robust interpretation of moving faces, one must resolve the inconsistency and ambiguities observed among multiple sources of highly noisy visual measurements. In this work, we describe a method for perceptual fusion through the estimation of observation covariance. We demonstrate the approach through a working system that performs real-time face detection and tracking in live video sequences. The use of perceptual fusion aims to address the difficult problem of simultaneous 2D face alignment and 3D head pose estimation during tracking.",
Learning of the way of abstraction in real robots,"Real robots should be able to adapt flexibly to various environments. The main problem is how to abstract useful information from a huge amount of information in the environment. This is called the frame problem. The paper proposes a new architecture which can learn how to perform abstraction while executing the task. We call the architecture the situation transition network system (STNS). By this architecture, a robot can acquire a necessary and sufficient symbol system for the current task and environment. Furthermore, this symbol system is flexible enough to adapt to changes of the environment. STNS performs cognitive learning and behavior learning parallelly while executing the task. In cognitive learning, it extracts situations and maintains them dynamically in the continuous state space on the basis of rewards from the environment. A situation can be regarded as an empirically obtained symbol. In behavior learning, it constructs a MDP (Markov decision problem) model of the environment on the abstracted situation representation. This model is used for planning of behavior. The validity of STNS is shown in computer simulations.",
Towards integration of state machines and object-oriented languages,"The goal of the paper is to obtain a one-to-one correspondence between state machines as e.g. used in UML and object oriented programming languages. A proposal is made for a language mechanism that makes it possible for an object to change its virtual bindings at run time. A state of an object may then be represented as a set of virtual bindings. One advantage of object orientation is that it provides an integrating perspective on many phases of software development, including analysis, design and implementation. For the static set of OO language constructs there is almost a one-to-one correspondence between analysis/design notations and OO programming languages. No such correspondence exists for the dynamic aspects, but the proposed state mechanism is a contribution to a better correspondence. The proposal is based on previous work by A. Taivalsaari (1993) and compared to the more complex features for changing object behavior as found in CLOS, Smalltalk, and predicate classes, it is simple and efficient to implement.",
Formal development of secure email,"Developing systems that are assured to be secure requires precise and accurate descriptions of specifications, designs, implementations, and security properties. Formal specification and verification have long been recognized as giving the highest degree of assurance. In this paper, we describe a software development process that integrates formal verification and synthesis. We demonstrate this process by developing assured sender and receiver C++ code for a secure electronic mail system, Privacy Enhanced Mail. We use higher-order logic for system-requirements specification, design specifications and design verification. We use a combination of higher-order logic and category theory and tools supporting these formalism to refine specifications and synthesize code. Much of our work is applicable to other secure email protocols, as our development is parameter used, component-based, and reusable.",
An intelligent agent for Web-based process redesign,"Process redesign is an expensive, time consuming and labor-intensive activity. Analysis of the redesign process indicates the ""first generation"" computer-based tools are inadequate for redesign today. Knowledge-based systems and intelligent agents have the ability to address the key, intellectual activities required for effective process redesign. An intelligent redesign agent called KOPeR is developed and employed, in an ""industrial strength"" reengineering engagement, to redesign processes from the procurement domain. This paper describes the KOPeR design and implementation and highlights its use as a redesign agent in the field. The field results reveal insights into the use, utility and potential of this agent technology, and the paper closes with a number of promising future directions for related research.","Intelligent agent,
Knowledge based systems,
Business process re-engineering,
Problem-solving,
Humans,
Pathology,
Automation,
Performance evaluation,
Procurement,
Paper technology"
A predictive paging scheme based on the movement direction of a mobile host,"This paper introduces a predictive paging scheme, called the movement direction paging scheme (MDP), which is based on the movement direction of a mobile host in order to reduce the paging cost in personal communication services (PCS). In the movement direction paging scheme, the visiting location registers (VLR) store the history of movement directions for a mobile host in its database. The history of movement direction for a mobile host is stored with compact records revised by the mapping and reduction function of the mobile switching center (MSC). The future movement direction of a mobile host can be predicted by using these records stored at the database of the VLR. The proposed scheme of movement direction paging is combined with a movement-based registration scheme and related to the basic velocity paging scheme (BVP). Analytical models of the basic velocity paging scheme and the paging scheme for performance evaluation are provided and the results demonstrate the cost-effectiveness of the proposed scheme under various parameters.",
An off-line character recognition method employing model-dependent pattern normalization by an elastic membrane model,"This paper proposes a model-dependent pattern normalization method that employs an elastic membrane model to absorb pattern deformation in handwritten character patterns. This method prepares an elastic membrane from an input pattern and induces model-dependent pattern normalization by placing the membrane on each potential field representing a standard pattern. We composed a recognition system from non-linear normalization, coarse classification, model-dependent normalization and fine classification, then show that the system with the model-dependent normalization is more effective than that without it for both the ETL off-line handwritten character patterns database and the HANDS on-line patterns database.",
A forward-looking electronics and computer engineering technology program,"A successful electronics and computer engineering technology program has been offered at Arizona State University (USA) since 1973. Engineering technology programs graduate students who can readily contribute to the workforce in industry and society, The immediate contribution is achieved by imparting state-of-the-art knowledge and skills; continued success and lifelong learning are assured by a knowledge of the fundamental concepts in engineering and the physical sciences acquired in their studies. A balanced combination of courses in engineering technology, mathematics, natural sciences and general studies produces these results, Core courses provide a working knowledge in a broad category of subjects. The breadth acquired through these basic courses is complemented by a depth of optional elective courses. Graduates are trained in engineering design, are taught appropriate communication skills, and are given extensive hands-on laboratory experience, The curriculum mandates all students develop a design project from definition through final product fabrication and submit a formal project report, Both the undergraduate and graduate programs provide opportunities to do research and analysis and to develop technical skills required for a successful career. To meet the challenges imposed by the changing demands of industry, continuous refinements to keep the program current and viable have been ongoing in the curriculum since its inception.",
The role of women in the history of computing,"Long before the electronic computing era, women were already a part of the information processing industry. For the first fifty years of information processing, women had an important role to play-from the women data entry operators of the early 1900s to the six women programmers of ENIAC in the 1940s and the scientific computation women computists of the 1950s. Sometimes an extraordinary partnership occurred, with women an integral part of a team. Sometimes a stroke of fate placed a woman at the right place at the right time to be a part of computing history. The paper provides a personal overview of the role of women in the history of information processing and computing, then gives a perspective on the workplace issues of supply and demand that continue to affect that role. Concern is expressed about the future role of women in computing and the sciences, with suggestions for consideration of new ways to approach the shortfall.",
A framework for user assisted design space exploration,"Much effort in hardware/software codesign has been devoted to developing ""push-button"" types of tools for automatic hardware/software partitioning. However, given the highly complex nature of embedded system design, user guided design exploration can be more effective. In this paper, we propose a framework for designer assisted partitioning that can be used in conjunction with any given search strategy. A key component of this framework is the visualization of the design space, without enumerating all possible design configurations. Furthermore, this design space representation provides a straightforward way for a designer to identify promising partitions and hence guide the subsequent exploration process. Experiments have shown the effectiveness of this approach.",
Effective Monte Carlo simulation on System-V massively parallel associative string processing architecture,We show that the latest version of massively parallel processing associative string processing architecture (System-V) is applicable for fast Monte Carlo simulation if an effective on-processor random number generator is implemented. Our lagged Fibonacci generator can produce 10/sup 8/ random numbers on a processor string of 12 K PE-s. The time dependent Monte Carlo algorithm of the one-dimensional non-equilibrium kinetic Ising model performs 80 faster than the corresponding serial algorithm on a 300 MHz UltraSparc.,
Tensor-product wavelet vs. Mallat decomposition: a comparative analysis,"The two-dimensional tensor product wavelet transform is compared to the Mallat representation for the purpose of data compression. It is shown that the tensor product wavelet transform will always provide a coding gain greater than or equal to that of the Mallat representation. Further, the costs of obtaining the tensor product wavelet transform are outlined.",
Management support for globally operating virtual organizations: the case of KLM Distribution,"In search for new organizational forms that incorporate the appropriate level of flexibility to offer a high variety of products and services, while keeping costs and lead times low, the virtual organization has emerged. The capabilities of information and communication technology (ICT) enable the virtual organization to link and coordinate a wide variety of globally dispersed business partners. The virtual organization in action is difficult to manage. We introduce a management support tool, called Modular Network Design (MND). MND supports managers of a virtual organization in four steps: determination and analysis of customer requirements; tracking of possibilities to satisfy customer requirements; allocation of production tasks among network partners; and ongoing assessment and adjustment of activities and allocation procedures. The applicability of MND, and the evaluation of ICT use, is illustrated with a case study at KLM Distribution, the center of a globally operating virtual organization. This business unit of KLM Royal Dutch Airlines distributes worldwide aircraft spare-parts for KLM and its partners and customers. The case study describes how MND supports the management of the virtual organization and contributes to better planning of individual, customized transport orders.",
A framework for management and control of distributed applications using agents and IP-multicast,As more and more applications on the Internet become network aware the need and possibility to remotely control them becomes larger. This paper presents a framework for control and management of distributed applications and components. This is done using IP-multicast and an agent based application architecture. The target of the framework is to allow for resource discovery of both controllable elements and available control points in the these elements as well as real-time control. All this is done in scalable and secure way based on IP-multicast and asymmetric cryptography. The presented framework is also independent of the underlying transport mechanism to allow for flexibility and easy deployment. The framework bandwidth usage and introduced control delay is presented. Details on the reference implementation of the framework and example usage scenarios where the framework is used to create bandwidth adaptive applications and better group awareness is also presented.,
Variation of direct and quadrature axis inductances of single-sided linear reluctance motor,"The paper describes the variation of the direct and quadrature axis inductance, the moving force and power factor of a single-sided linear reluctance motor (LRM) in dependency on different combination of direct and quadrature axis excitation. The magnetic conditions in the motor were analysed by the finite element method. The moving force was calculated by the virtual work method. Calculation results of moving forces at constant direct axis current components show a linear response of moving force in dependency on quadrature axis current component.",
Security analysis of Tramel,An operational formal specification of the Tramel system is presented. Tramel is used by NASA's Jet Propulsion Laboratory to support asynchronous inter-task communication of distributed software across varying architectures and operating systems. Security analysis of communications between non-Tramel programs and Tramel is explored using an operational trace-based specification model.,
The 1-Steiner tree problem in lambda-3 geometry plane,"In this paper, we extend the 1-Steiner idea of Georgakopoulos and Papadimitriou [1987] to the Steiner tree problem in lambda-3 geometry plane. Our extension to the lambda-3 geometry plane and that of Kahng and Robins [1992] to the rectilinear plane are similar in principle, but different in many nontrivial details. After presenting an efficient algorithm for solving the 1-Steiner tree problem, we apply the iterated 1-Steiner heuristic to compute approximations to the Steiner minimum tree problem in lambda-3 geometry plane. Computational results on standard benchmarks show that our algorithm compares favorably with previously published heuristics.",
Contact analysis of spatial fixed-axes pairs using configuration spaces,"We present the first configuration space computation algorithm for pairs of rigid parts that move along fixed spatial axes. The motivation is contact analysis for mechanical design of spatial systems and of planar systems with axis misalignment. The part geometry is specified in a parametric boundary representation using planes, cylinders, and spheres. Our strategy is to exploit the specialized part geometry and the 2D structure of the configuration space to: 1) derive low-degree algebraic contact equations in the two part motion parameters, which can readily be solved to obtain contact curves, and 2) to use a practical planar configuration space construction algorithm. We demonstrate a preliminary implementation on three representative pairs, none of which is covered by other contact analysis algorithms. We show how the program is used in answering design questions.",
A PCI bus based correlation matrix memory and its application to k-NN classification,"This paper describes a PCI bus based implementation of a binary correlation matrix memory (CMM) neural network and its application and performance for use as a k-NN based pattern classification system. The system expands on earlier VME based systems incorporating FPGA based implementation through greater integration and lower cost. Experimental results for several benchmarks show that, compared with a simple k-NN method, the CMM hardware gave speed up of 8-98.8 times during recall process with a classification performance which is 99%-100% that of a conventional k-NN implementation.",
Teaching software project management: a response-interaction approach,"Southern Polytechnic State University has recently implemented a new Master of Science in Software Engineering degree, which includes a course in software project management in its core requirements. This paper addresses an innovative approach to teaching this course through what is described as response-interaction. Also included are the results of the first offering of this course.",
Two-variable descriptions of regularity,"We prove that the class of all languages that are definable in /spl Sigma//sub 1//sup 1/(FO/sup 2/), that is, in (non-monadic) existential second-order logic with only two first-order variables, coincides with the regular languages. This provides an alternative logical description of regularity to both the traditional one in terms of monadic second-order logic, due to Buchi and Trakhtenbrot, and the more recent ones in terms of prefix fragments of /spl Sigma//sub 1//sup 1/, due to Eiter, Gottlob and Gurevich. Our result extends to more general settings than words. Indeed, definability in /spl Sigma//sub 1//sup 1/(FO/sup 2/) coincides with recognizability by appropriate notions of automata on a large class of objects, including /spl omega/-words, trees, pictures and, more generally, all weakly deterministic, triangle-free transition systems.",
Analyses of college students' concept by fuzzy structural modeling (FSM) method with planar lattice neural network,"Since college students will meet the first stage of real learning in software programming, it is very important for teachers to effectively give students informatics education at university level. Accordingly, the teachers have to select educational contents and curriculums and to improve those according to student comprehension. In order to facilitate the teachers' informatics education, we propose to structure student concepts with regard to information processing by using a fuzzy structural modeling method with planar lattice neural networks. It was found that our technique would provide useful information for teachers to improve their education.",
Recognition of printed Arabic words with fuzzy ARTMAP neural network,"This paper presents a new method for the recognition of Arabic text using global features and fuzzy ARTMAP neural network. The method is divided into three major steps. The first step is digitization and pre-processing to create connected component. The second step is concerned with feature extraction, where global features of the input word are used to extract features such as number of subwords, number of peaks within the subword, number and position of the complementary character, etc., to avoid the difficulty of segmentation stage. The third step is the classification and is composed of a single fuzzy ARTMAP. The method was evaluated with 3255 images of 217 Arabic words with different fonts (each word has 15 samples), and the mean correct classification rate was 95.25%.",
Weight estimation for the learning of modular perceptron networks,"We propose a weight estimation method for feedforward neural networks. The proposed method includes two steps: (1) weight vector orientation estimation and (2) weight vector length estimation, such that the iteration learning of modular perceptron networks (MPN) can be reduced. We have applied the proposed method to the divide-and-conquer learning (DCL) for two-spiral problems (TPS) on an MPN. The experimental results show that the number of pattern presentations can be reduced 79.05% (from 190,980 to 40,012), and the number of subnets in MPN can also be reduced 20.38% (from 26.5 to 21.1).",
Using a single address space operating system for distributed computing and high performance,"Recent 64-bit microprocessors have made a huge 18.4 quintillion byte address space potentially available to programs. This has led to the design of Operating Systems that provide a single virtual address space in which all code and data reside in and that spans all levels of storage and all nodes of a distributed system. These operating systems called SASOSs, have characteristics that can be used to support synchronization and coherency in a distributed system in ways that provide an improved program development environment and higher performance than that available from conventional operating systems. Sombrero, our SASOS design, makes use of its hardware support for object-grained protection, separate thread related protection domains and implicit protection domain crossing to provide synchronization and coherency support for distributed object copy set management not available in SASOSs built on stock processors. Its design, which provides direct system level support for object oriented programming includes a number of system architectural features targeted at modern distributed computing.",
Cascaded execution: Speeding up unparallelized execution on shared-memory multiprocessors,"Both inherently sequential code and limitations of analysis techniques prevent full parallelization of many applications by parallelizing compilers. Amdahl's Law tells us that as parallelization becomes increasingly effective, any unparallelized loop becomes an increasingly dominant performance bottleneck. We present a technique for speeding up the execution of unparallelized loops by cascading their sequential execution across multiple processors: only a single processor executes the loop body at any one time, and each processor executes only a portion of the loop body before passing control to another. Cascaded execution allows otherwise idle processors to optimize their memory state for the eventual execution of their next portion of the loop, resulting in significantly reduced overall loop body execution times. We evaluate cascaded execution using loop nests from wave5, a Spec95fp benchmark application, and a synthetic benchmark. Running on a PC with 4 Pentium Pro processors and an SGI Power Onyx with 8 R10000 processors, we observe an overall speedup of 1.35 and 1.7, respectively, for the wave5 loops we examined and speedups as high as 4.5 for individual loops. Our extrapolated results using the synthetic benchmark show a potential for speedups as large as 16 on future machines.",
An emotional viseme compiler for facial animation,"The animation of a three dimensional synthetic human face has been the object of much research in the past few years. Many systems now exist for this purpose, which rely on the artistic and animation skills of animators. Methods for the generation of lip movements to accompany a speech soundtrack have also been developed. These systems rely on the extraction of phonemes from the speech signal and converting them to ""visemes"" or visual lip shapes for a synthetic human face. The generation of human emotional expressions has also been developed in the recent past. This paper combines some of these developments to present a system, which is capable of appropriately combining emotional cues automatically with phonemes to generate emotional visual speech on a synthetic human face.",
Self-timed design: An avenue to complex computer systems,"Asynchronism could prove to be the way to build huge systems, except for incoming hazards. This paper introduces an original methodology for hazard-free self-timed design, assuming the worst conditions for robustness. Hazards are classified under three types. On top of logic hazards that resort to implementation, equation hazards are eliminated by an optimal covering. A new variable, labeled state trajectory is proposed: its integrity guarantees immunity to function hazards. The method was fruitfully applied to the VLSI CMOS implementation of a router for a parallel machine. Peculiar full-custom cells are designed. The measured performances of the circuit are presented, as well as some more global machine performances for communication.",
Exploring evolutionary learning in a simulated hockey environment,"As a test bed for studying evolutionary and other machine learning techniques, we have developed a simulated hockey game called Shock in which players attempt to shoot a puck into their enemy's goal during a fixed time period. Multiple players may participate-one can be controlled by a human user, while the others are guided by artificial controllers. In previous work, we introduced the Shock environment and presented players that received global input (as if from an overhead camera) and were trained on a restricted task, using an evolutionary hill climbing algorithm, with a staged learning approach (A. Blair and E. Sklar, 1998). Here, we expand upon this work by developing players which instead receive input from local, Braitenberg-style sensors (V. Braitenberg, 1984). These players are able to learn the task with fewer restrictions, using a simpler fitness measure based purely on whether or not a goal was scored. Moreover, they evolve to develop robust strategies for moving around the rink and scoring goals.",
Econophysics: can physicists contribute to the science of economics?,"How can computational physicists contribute to the search for solutions to the puzzles posed by modern economics that economists themselves cannot solve? An approach-not very commonly used in economics-is to begin empirically, with real data that you can analyze in some detail, but without prior models. In economics, a great deal of real data is available. If you, moreover, have at your disposal the tools of computational physics and the computing power to carry out any number of approaches, this abundance of data is a great advantage. A careful analysis of any system involves studying the propagation of correlations from one unit of the system to the next. We learned that these correlations propagate both directly and indirectly. At one time, it was imagined that scale-free phenomena are relevant to only a fairly narrow slice of physical phenomena. However, the range of systems that apparently display power-law and hence scale-invariant correlations has increased dramatically in recent years. Such systems range from base-pair correlations in noncoding DNA, lung inflation, and interbeat intervals of the human heart, to complex systems involving large numbers of interacting subunits that display free will. In particular, economic time series, e.g., stock market indices or currency exchange rates, depend on the evolution of a large number of strongly interacting systems far from equilibrium, and belong to the class of complex evolving systems. Thus, the statistical properties of economic time series have attracted the interests of many physicists.",
SEGWorld: a WWW-based infrastructure to support the development of shared software engineering artifacts,"Software engineering tasks, during both development and maintenance, typically involve teamwork supported by computers. Team members rarely work on isolated computers; networked computers are commonly used. An underlying assumption of our research is that software engineering teams will work more effectively if adequately supported by network-based groupware technology. This research is investigating the provision of such network-based support for software engineering teams. The immediate objective is to provide WWW-based support, in the form of shared information spaces, specifically for students working on Software Engineering Group (SEG) projects in the Department of Computer Science at Durham. The long term objectives are to develop more flexible support for group working among university students and their staff supervisors for project work. This paper reports on our development and use, over a two year period, of SEGWorld-a WWW based infrastructure incorporating shared information spaces. Hypotheses relating to the student use of the infrastructure and shared information spaces have been formulated. Results of the use of this WWW-based infrastructure are presented.",
The relationship of slicing and debugging to program understanding,"The paper describes a study that explores the relationship between program slicing and code understanding gained while debugging. The study consisted of an experiment that compared the program understanding abilities of two classes of debuggers: those who slice while debugging and those who do not. For debugging purposes, a slice can be thought of as a minimal subprogram of the original code that contains the program faults. Those who only examine statements within a slice for correctness are considered slicers; all others are considered non-slicers. Using accuracy of reconstruction as a measure of understanding, it was determined that slicers have a better understanding of the code after debugging.",
Design of a visual environment for evaluating and customizing medical image compression techniques,"This paper discusses the design of the Image Compression Laboratory (ICL), a visual environment supporting radiologists in interactively compressing medical images but still maintaining the diagnostic information. In ICL lossy image compression techniques and the interactive image interpretation approach are merged following the cooperative visual environment approach. In this way the radiologists themselves can perform the compression using their professional skill and knowledge.",
The impact of copyright on the development of cutting edge binary reverse engineering technology,"Reverse engineering of executable code is a growing area of software engineering research and technology development due to a variety of reasons, including the porting of programs to newer and faster machines. In this paper we discuss three core object code reverse engineering technologies: emulation, decompilation, and binary translation, and present their uses in the last decades. These uses point at an economic need for such techniques to the benefit of users of the technology. We then present the extent of copyright protection for binary code and its implications on the development of binary code manipulation tools. Further, we argue that copyright laws should not hinder the development of computer and software technology at a time when hardware is developing at increasingly fast rates and software needs to be made available on such new machines; i.e. economic considerations need to be taken into account.",
Guaranteeing quality of service in interactive video-on-demand servers,One of the main challenges in disk system design for interactive VOD servers is how to achieve a good quality of service (QoS) guarantee. This paper proposes a queueing model for analyzing the I/O bandwidth required in order to achieve a certain level of QoS for a disk system design scheme that provides VCR emulation. The disk system design scheme employs a practice that requests no extra bandwidth to support interactive operations such as fast forward search and fast backward search. The proposed queueing model provides a complete analysis tool for pinning down the resource requirement issue and its validity is verified through simulation.,
Some computational properties of intersection types,"This paper presents a new method for comparing computation-properties of /spl lambda/-terms typeable with intersection types with respect to terms typeable with Curry types. In particular, strong normalization and /spl lambda/-definability are investigated. A translation is introduced from intersection typing derivations to Curry typeable terms; the main feature of the proposed technique is that the translation is preserved by /spl beta/-reduction. This allows to simulate a computation starting from a term typeable in the intersection discipline by means of a computation starting from a simply typeable term. Our approach naturally leads to prove strong normalization in the intersection system by means of purely syntactical techniques. In addition, the presented method enables us to give a proof of a conjecture proposed by Leivant in 1990, namely that all functions uniformly definable using intersection types are already definable using Curry types.",
Learning to acquire and select useful landmarks for route following,"The paper describes a prototypical system for optimal landmark acquisition and selection. Our landmark-learning approach does not require any type of environment model to be supplied to the robot in advance, and represents a step towards robots interacting with real environments. The approach is fitted and tested for the TMGA system (P. Zingaretti et al., 1998), which the authors developed for landmark tracking by adaptive, stereo template matching. Two complementary strategies, properly managed, are followed to construct a suitable subset of landmarks: the selection of the more discriminant landmarks and the selection of the landmarks that are more invariant in a neighbourhood. The robustness of the TMGA system in analysing the discriminant power of each landmark and the analysis of the disparity map and of the spatial activity maps of the stereo images are used for identifying discriminant and invariant landmarks. The experimental results show that the number of matching failures, and consequent landmark changes during the following of a route is comparable with (not much greater than) those obtained using an a-priori subset.",
Educational applications and benefits of a compact multitasking kernel for microcontrollers,"This paper describes the educational applications and benefits of a compact multitasking kernel for Motorola 68HC11 and 68HC12 microcontroller-based systems used in teaching at Queen's University. The kernel is implemented in assembly language and in C; neither version exceeds 200 lines of code. The base kernel enables students to apply their knowledge of both microprocessor systems and operating systems by adding new services and writing code to test those services. The most interesting application, however; is the use of a speech chip to audibly trace the dynamic execution of the kernel. Prerecorded voice messages are played back at strategic points to identity key events and conditions. This capability can also serve as a debugging aid when adding new services to the kernel.",
Development of an intelligent case-based system for help desk operations,"Help desks are computer-aided environments that assist customer response centers in providing front-line phone support to external and internal customers. With the proliferation of diverse software and hardware, the centers need to handle more and more incoming calls for help. Since customer support centers are often the first point of contact for customers, it is important to ensure customer satisfaction when a call is placed. The objective of this project is to develop a case-based help desk system to support both call center personnel and customers. The process of system development, including knowledge acquisition, knowledge representation, system implementation, and verification are discussed with an emphasis on the structured and automated development method applied to the process.",
Incremental maintenance of nested relational views,"Incremental view maintenance techniques are required for many new types of data models that are being increasingly used in industry. One of these models is the nested relational model that is used in the modelling of complex objects in databases. In this paper, we derive a group of expressions for incrementally evaluating query expressions in the nested relational model. We also present an algorithm to propagate base relation updates to a materialized view when the view is defined as a complex query.",
An investigation of out-of-core parallel discrete-event simulation,"In large-scale discrete event simulations, the size of a computer's physical memory limits the size of system to be simulated. Demand paging policies that support virtual memory are generally ineffective. Use of parallel processors to execute the simulation compounds the problems, as memory can be tied down due to synchronization needs. We show that by taking more direct control of disks it is possible to break through the memory bottleneck, without significantly increasing overall execution time. We model one approach to conducting out-of-core parallel simulation, identifying relationships between execution, memory, and I/O costs that admit good performance.",
Effect of a sparse architecture on generalization behaviour of connectionist networks: a comparative study,"Generalization, the ability of achieving equal performance with respect to the training patterns for the design of the system as well as the unknown test patterns outside the training set, is considered to be the most desirable aspect of a cognitive learning system. For connectionist classifiers, generalization depends on several factors like the network architecture and size, learning algorithm, complexity of the problem and the quality and quantity of the training samples. Various studies on the generalization behaviour of a neural classifier suggest that the architecture and the size of the network should match the size and complexity of the training sample set of the particular problem. The popular ideas for improving generalization is network pruning by removing redundant nodes or growing by adding nodes to reach the optimum size for matching. In this work a feedforward multilayer connectionist model proposed earlier by Chakraborty et al. (1997) with a sparse fractal connection structure between the neurons of adjacent layers has been studied for its suitability compared to other architectures for achieving good generalization. A comparative study with another feedforward multilayer model with network pruning algorithms for pattern classification problem revealed that it is easier to achieve good generalization with the proposed sparse fractal architecture.",
Industrial application of artificial neural networks,"This paper reports on three case studies where application-driven research has led to advances not only of an applied nature, but also to significant basic or fundamental results. The application areas reported encompass pattern classification, pattern recognition and time series prediction.",
A real-time prefetching method for continuous media playback,"Continuous media (CM) such as digitally-coded video and sound impose time constraints on multimedia systems to prevent hiccups. In this paper we propose a new CM playback method that is suitable for handling CM streams with various playback rates and a rather short duration of playback. Our proposed method is aimed at providing both good response times and high disk utilization. For better disk utilization, the SCAN algorithm is employed for retrieving CM data and then a real-time bulk-scan is conducted. Along with this bulk-scan, we propose an EDF-style algorithm with a priority-driven property. Owing to the flexibility of disk scanning, our method is able to provide a good response time as well as high disk utilization. We show performance advantages through experimental comparisons between our method and earlier round-style methods.",
Design and evolution of software architecture in practice,"With special focus on software architectural issues, we report from the first two major phases of a software development project. Our experience suggests that explicit focus on software architecture in these phases was an important key to success. More specifically: Demands for stability, flexibility and proper work organisation in an initial prototyping phase of a project are facilitated by having an explicit architecture. However, the architecture should also allow for certain degrees of freedom for experimentation. Furthermore, in a following evolutionary development phase, architectural redesign is necessary and should be firmly based on experience gained from working within the prototype architecture. Finally, to get it right, the architecture needs to be prototyped, or iterated upon, throughout evolutionary development cycles. In this architectural prototyping process, we address the difficult issue of identifying and evolving functional components in the architecture and point to an architectural strategy a set of architectures, their context and evolution-that was helpful in this respect.",
MysterX: a Scheme toolkit for building interactive applications with COM,"MysterX is an object oriented Scheme toolkit for building applications from off-the-shelf COM components. While the COM support in languages such as Haskell and Mercury requires the use of an interface compiler to generate stub code, MysterX uses the reflective capabilities of OLE Automation to make value marshalling decisions at run time. MysterX hosts COM components in windows that display Dynamic HTML, without requiring a separate browser. Scheme code can manipulate HTML elements and their style properties in such windows to create interesting visual effects. Event handlers written in Scheme can be associated with HTML elements and COM objects. By integrating these diverse technologies, MysterX can be used to write complete GUI applications.",
TRIUMF-a system for remote multimedia interviewing,"Modern communication technologies enable recruitment interviews to be held remotely through the use of video conferencing. However, the effective deployment of such services, at a standard comparable to traditional face-to-face interviews, requires an integrated hardware and software environment to support a wide range of activities during recruitment interviews. In this paper, we present a system for such an environment, and demonstrate how multimedia communications are managed in the environment and how they can be used to assist the interviews in ways that would not be easily achievable in traditional face-to-face interviews.",
Using the WWW as a medium for marketing research in financial services: the case of Xenon Laboratories,"The nature of how marketing research is conducted has changed dramatically over the past thirty years. This has been a function both of how the prevailing view of best practice in marketing has developed, and also of the state of technology available to researchers. While it is clear that the WWW will increasingly be used as a medium for conducting marketing research, the full implications of this new medium are not yet fully understood. Existing research has examined the implications of the WWW in terms of the potential dramatic reductions in both the time and cost involved in testing new financial service products. The potential for a financial services firm to analyse the market environment and competitors' strategies is examined using a case study based on real time marketing research. The case is that of the Canadian Internet financial services provider, Xenon Laboratories, which conducts marketing research into the market for credit/charge cards.",
Network service selection for distributed multimedia applications,"An important question in the development of system support for distributed multimedia is the type of network service offered to applications. This paper compares two network service disciplines: weighted fair queueing (WFQ) and non-preemptive earliest deadline first (NEDF). We show that, for a broad class of high-bandwidth distributed multimedia applications, WFQ outperforms NEDF in terms of network throughput while still providing an application-adequate end-to-end service. This result holds despite the fact that NEDF offers applications far greater flexibility in terms of control over end-to-end delivery delay.",
Progressing from small group work to cooperative learning: a case study from computer science,"Although the industrial partners of academe are unanimous in their desire to hire engineering graduates who are experienced in working productively in small groups, implementing small group work in a computer science class can be difficult. The obvious assignment, a group programming project, proved to be a poor choice when implemented in the author's computer graphics class. An examination of the literature in this area shows that a group programming project has many features in common with a group term paper, the assignment which has been uniquely identified as the worst choice for small group work. Fortunately, there are better choices for cooperative learning in computer science. Assignments with ""the three S's"": same problem, specific choice and simultaneous reporting of group choices, work well. This was implemented in a class by having students work multiple choice quizzes designed to require high level learning skills. Quizzes were first worked by individuals, then by small groups. The small group answers are then compared and discussed in class. This generates the type of interaction between the professor and students which creates positive cooperative learning experiences. Promising preliminary results have been seen with this method, from both the student and the professor's perspective.",
On the overfitting of the five-layered bottleneck network,"In autoassociative learning for the bottleneck neural network, the problem of overfitting is pointed out. This overfitting is pathological in the sense that it does not disappear even if the sample size goes to infinity. However, it is not observed in the real learning process. Thus we study the basin of the overfitting solution. First, the existence of overfitting is confirmed. Then it is shown that the basin of the overfitting solution is small compared with the normal solution.",
The problems you're having may not be the problems you think you're having: results from a latency study of Windows NT,"This paper is intended to catalyze discussions on two intertwined systems topics. First, it presents early results from a latency study of Windows NT that identifies some specific causes of long thread scheduling latencies, many of which delay the dispatching of runnable threads for tens of milliseconds. Reasons for these delays, including technical, methodological, and economic are presented and possible solutions are discussed. Secondly, and equally importantly, it is intended to serve as a cautionary tale against believing one's own intuition about the causes of poor system performance. We went into this study believing we understood a number of the causes of these delays, with our beliefs informed more by conventional wisdom and hunches than data. In nearly all cases the reasons we discovered via instrumentation and measurement surprised us. In fact, some directly contradicted ""facts"" we thought we ""knew"".",
Distributed objects in a large scale text processing system (industrial case study),"We describe the distributed object oriented architecture of TREVI, a system designed to help overcome the information overload problem. TREVI provides a comprehensive framework for the processing and dissemination of documents coming from news streams and other external sources. TREVI is designed to perform a variety of linguistic and text processing tasks, according to the informational needs expressed in user profiles. Many different user profiles may be simultaneously present, and the sheer volume of information that has to be managed by the system poses serious scalability and performance concerns. We show how the use of a distributed, component based architecture allows us to cope with these challenges, and furthermore provides an excellent base for future system extension, allowing for a rapid response to changing user informational needs.",
Collaborative scientific visualization in networked immersive virtual environment,"A collaborative scientific visualization environment was constructed by connecting the CABIN immersive multi-screen display system to several immersive projection displays and to supercomputers via a broadband network. In this environment, users in remote places are able to discuss the displayed phenomena while looking at the same data. In particular, a video avatar method was developed to realize natural communication in a shared virtual world. This method was applied to the visualization of several kinds of scientific data, and its effectiveness was evaluated.",
Adaptive data broadcast strategy for transactions with multiple data requests in mobile computing environments,"Data broadcast in mobile environments has received much attention in recent years and a large number of algorithms have been proposed. However, existing work in the literature only assume a single data item per transaction. In this paper we propose an adaptive data broadcast strategy designed for the case where there may be more than one data item per transaction, for a mobile environment with both push and pull channels. The basic rationale of the strategy is to increase the number of transactions that can be completed by improving the use of the on-demand channel. Extensive simulation shows that the algorithm performs well in a variety of operating environments.",
Softly focusing on data,"A computational approach providing a focus for unsupervised, reactive data mining is suggested. In data mining, achieving focus is an important issue. This is because there are too many attributes and values in a real database to consider them all. A soft focus is suggested, as both the data and the focus product may be imprecise. An approach is suggested for unsupervised searching controlled by progressive reduction of cognitive dissonance. Both crisp and non-crisp data are subject to discovery. Soft completing tools are needed because of the need to granulize data and to establish crisp boundaries in a non-crisp world. Issues involve: coherence measures, granularization, user-intelligible results, unsupervised recognition of interesting results, and concept-equivalent formation.",
Real-time depth warping for 3-D scene reconstruction,"This paper explores a new depth-recovery technique targeted for two environments: visualization on non-regular screen surfaces, and real-time depth recovery in an unknown scene. The algorithm is based on the computation of pre-warped images that are projected into the scene and re-imaged by synchronized cameras. The real-time control of the pre-warp and the type of imagery projected into the scene allows for a novel solution that recovers scene geometry and can be used in both visualization (projection of imagery onto screen surfaces) and computer vision environments.",
Mobile agent-based shared registry system for electronic commerce of Internet domain names,"Internet domain name management is currently developing into a very significant market. We present the design and implementation of the domain name exchange (DNX) platform, the primary purpose of which is to support an integrated market for domain names. Our platform is based on mobile agent technology, which enables flexible and efficient electronic trade of domain names. Our registration model is the shared registry system model aiming at an open and fair registration process. We propose with DNX to enhance the basic registration service by a set of value-added services, such as intelligent registration, cross-top level domain registration and active event notification. Our platform implementation is based on a secured-Java mobile agent platform that provides the basic tools for programming the DNX system. Furthermore, the DNX platform makes possible the co-existence of an electronic domain name trading system with other electronic commerce components, such as conflict and dispute resolution systems.",
A dynamic channel assignment scheme with two thresholds for load balancing in cellular networks,"A channel assignment problem is to develop efficient channel assignment schemes that can maximize the number of calls served in cellular network systems. This problem is one of the most important problems in the design and operation of cellular networks. But the non-uniformity of demand (or traffic) in different cells may lead to a gross imbalance in the system performance. So, the system should be able to cope with such traffic overheads in certain cells. Previous schemes with one threshold have a redundant channel borrowing problem. However, in our scheme, the problem of such redundant channel borrowing and imbalance in the system performance is solved using two thresholds.",
Inter-enterprise workflow management systems,"Electronic commerce is conceived as one of the major channels for performing commerce on a global scale, and the area is rapidly evolving. Currently, transferring and processing electronic data is handled separately from the enterprise support system, e.g., workflows, thus blurring the actual functionality of the supply chain. We propose an extension of a standard workflow model to enable the description and control of the full life cycle of a supply chain.",
Enabling alarm correlation for a mobile agent based system and network management - a wrapper concept,"Solutions for system and network management given today are afflicted with two main shortcomings, namely the centralised approach they are based on and the inadequate addressing of heterogeneous environments. While the latter is the main concern of, for example, Web-based management activities, the aspect of centralisation is not being covered to full extend. With regard to the explosive growth of computer systems and networks both in size and complexity, new approaches therefore have to be evaluated which address all problems caused by complexity and heterogeneity. Mobile agent technology is being discussed as a promising new technology for various application areas, with management being one of the most promising. In this paper, problems arising when trying to apply this new technology to system and network management are identified and discussed. The focus is on how to handle the overall complexity with mobile agents, which are limited in size, knowledge, and functionality. An architecture is presented which introduces various types of mobile agents. By determining the required correlation sequence and identifying the co-operation of agents, it is shown how an alarm correlation with mobile agents can be established, which forms the basis for addressing management tasks with mobile agent technology.",
