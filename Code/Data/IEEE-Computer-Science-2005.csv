Title,Abstract,Keywords
Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions,"This paper presents an overview of the field of recommender systems and describes the current generation of recommendation methods that are usually classified into the following three main categories: content-based, collaborative, and hybrid recommendation approaches. This paper also describes various limitations of current recommendation methods and discusses possible extensions that can improve recommendation capabilities and make recommender systems applicable to an even broader range of applications. These extensions include, among others, an improvement of understanding of users and items, incorporation of the contextual information into the recommendation process, support for multicriteria ratings, and a provision of more flexible and less intrusive types of recommendations.","Recommender systems,
Collaborative work,
Filtering,
Books,
Motion pictures,
Hybrid power systems,
Collaboration,
Business,
Context modeling,
Cognitive science"
Survey of clustering algorithms,"Data analysis plays an indispensable role for understanding various phenomena. Cluster analysis, primitive exploration with little or no prior knowledge, consists of research developed across a wide variety of communities. The diversity, on one hand, equips us with many tools. On the other hand, the profusion of options causes confusion. We survey clustering algorithms for data sets appearing in statistics, computer science, and machine learning, and illustrate their applications in some benchmark data sets, the traveling salesman problem, and bioinformatics, a new field attracting intensive efforts. Several tightly related topics, proximity measure, and cluster validation, are also discussed.","Clustering algorithms,
Machine learning algorithms,
Data analysis,
Humans,
Statistics,
Computer science,
Machine learning,
Application software,
Traveling salesman problems,
Bioinformatics"
Behavior recognition via sparse spatio-temporal features,"A common trend in object recognition is to detect and leverage the use of sparse, informative feature points. The use of such features makes the problem more manageable while providing increased robustness to noise and pose variation. In this work we develop an extension of these ideas to the spatio-temporal case. For this purpose, we show that the direct 3D counterparts to commonly used 2D interest point detectors are inadequate, and we propose an alternative. Anchoring off of these interest points, we devise a recognition algorithm based on spatio-temporally windowed data. We present recognition results on a variety of datasets including both human and rodent behavior.","Object recognition,
Object detection,
Detectors,
Mice,
Computer vision,
Robustness,
Spatiotemporal phenomena,
Video sequences,
Prototypes,
Computer science"
Overview of the face recognition grand challenge,"Over the last couple of years, face recognition researchers have been developing new techniques. These developments are being fueled by advances in computer vision techniques, computer design, sensor design, and interest in fielding face recognition systems. Such advances hold the promise of reducing the error rate in face recognition systems by an order of magnitude over Face Recognition Vendor Test (FRVT) 2002 results. The face recognition grand challenge (FRGC) is designed to achieve this performance goal by presenting to researchers a six-experiment challenge problem along with data corpus of 50,000 images. The data consists of 3D scans and high resolution still imagery taken under controlled and uncontrolled conditions. This paper describes the challenge problem, data corpus, and presents baseline performance and preliminary results on natural statistics of facial imagery.","Face recognition,
Image recognition,
Image resolution,
Computer vision,
Protocols,
Lighting control,
Testing,
NIST,
Computer science,
Drives"
Telos: enabling ultra-low power wireless research,"We present Telos, an ultra low power wireless sensor module (""mote"") for research and experimentation. Telos is the latest in a line of motes developed by UC Berkeley to enable wireless sensor network (WSN) research. It is a new mote design built from scratch based on experiences with previous mote generations. Telos' new design consists of three major goals to enable experimentation: minimal power consumption, easy to use, and increased software and hardware robustness. We discuss how hardware components are selected and integrated in order to achieve these goals. Using a Texas Instruments MSP430 microcontroller, Chipcon IEEE 802.15.4-compliant radio, and USB, Telos' power profile is almost one-tenth the consumption of previous mote platforms while providing greater performance and throughput. It eliminates programming and support boards, while enabling experimentation with WSNs in both lab, testbed, and deployment settings.","Wireless sensor networks,
Robustness,
Microcontrollers,
Energy consumption,
Hardware,
Read-write memory,
Magnetic sensors,
Temperature sensors,
Computer science,
Instruments"
The pyramid match kernel: discriminative classification with sets of image features,"Discriminative learning is challenging when examples are sets of features, and the sets vary in cardinality and lack any sort of meaningful ordering. Kernel-based classification methods can learn complex decision boundaries, but a kernel over unordered set inputs must somehow solve for correspondences epsivnerally a computationally expensive task that becomes impractical for large set sizes. We present a new fast kernel function which maps unordered feature sets to multi-resolution histograms and computes a weighted histogram intersection in this space. This ""pyramid match"" computation is linear in the number of features, and it implicitly finds correspondences based on the finest resolution histogram cell where a matched pair first appears. Since the kernel does not penalize the presence of extra features, it is robust to clutter. We show the kernel function is positive-definite, making it valid for use in learning algorithms whose optimal solutions are guaranteed only for Mercer kernels. We demonstrate our algorithm on object recognition tasks and show it to be accurate and dramatically faster than current approaches","Kernel,
Histograms,
Face detection,
Computer vision,
Image edge detection,
Shape,
Support vector machines,
Support vector machine classification,
Learning systems,
Computer science"
A possibilistic fuzzy c-means clustering algorithm,"In 1997, we proposed the fuzzy-possibilistic c-means (FPCM) model and algorithm that generated both membership and typicality values when clustering unlabeled data. FPCM constrains the typicality values so that the sum over all data points of typicalities to a cluster is one. The row sum constraint produces unrealistic typicality values for large data sets. In this paper, we propose a new model called possibilistic-fuzzy c-means (PFCM) model. PFCM produces memberships and possibilities simultaneously, along with the usual point prototypes or cluster centers for each cluster. PFCM is a hybridization of possibilistic c-means (PCM) and fuzzy c-means (FCM) that often avoids various problems of PCM, FCM and FPCM. PFCM solves the noise sensitivity defect of FCM, overcomes the coincident clusters problem of PCM and eliminates the row sum constraints of FPCM. We derive the first-order necessary conditions for extrema of the PFCM objective function, and use them as the basis for a standard alternating optimization approach to finding local minima of the PFCM objective functional. Several numerical examples are given that compare FCM and PCM to PFCM. Our examples show that PFCM compares favorably to both of the previous models. Since PFCM prototypes are less sensitive to outliers and can avoid coincident clusters, PFCM is a strong candidate for fuzzy rule-based system identification.","Clustering algorithms,
Prototypes,
Phase change materials,
Fuzzy systems,
Knowledge based systems,
Government,
Strontium,
Engineering management,
Computer science,
Fuzzy logic"
Combining multiple clusterings using evidence accumulation,"We explore the idea of evidence accumulation (EAC) for combining the results of multiple clusterings. First, a clustering ensemble - a set of object partitions, is produced. Given a data set (n objects or patterns in d dimensions), different ways of producing data partitions are: 1) applying different clustering algorithms and 2) applying the same clustering algorithm with different values of parameters or initializations. Further, combinations of different data representations (feature spaces) and clustering algorithms can also provide a multitude of significantly different data partitionings. We propose a simple framework for extracting a consistent clustering, given the various partitions in a clustering ensemble. According to the EAC concept, each partition is viewed as an independent evidence of data organization, individual data partitions being combined, based on a voting mechanism, to generate a new n Ã— n similarity matrix between the n patterns. The final data partition of the n patterns is obtained by applying a hierarchical agglomerative clustering algorithm on this matrix. We have developed a theoretical framework for the analysis of the proposed clustering combination strategy and its evaluation, based on the concept of mutual information between data partitions. Stability of the results is evaluated using bootstrapping techniques. A detailed discussion of an evidence accumulation-based clustering algorithm, using a split and merge strategy based on the k-means clustering algorithm, is presented. Experimental results of the proposed method on several synthetic and real data sets are compared with other combination strategies, and with individual clustering results produced by well-known clustering algorithms.",
Coordinated multi-robot exploration,"In this paper, we consider the problem of exploring an unknown environment with a team of robots. As in single-robot exploration the goal is to minimize the overall exploration time. The key problem to be solved in the context of multiple robots is to choose appropriate target points for the individual robots so that they simultaneously explore different regions of the environment. We present an approach for the coordination of multiple robots, which simultaneously takes into account the cost of reaching a target point and its utility. Whenever a target point is assigned to a specific robot, the utility of the unexplored area visible from this target position is reduced. In this way, different target locations are assigned to the individual robots. We furthermore describe how our algorithm can be extended to situations in which the communication range of the robots is limited. Our technique has been implemented and tested extensively in real-world experiments and simulation runs. The results demonstrate that our technique effectively distributes the robots over the environment and allows them to quickly accomplish their mission.",
Energy-efficient target coverage in wireless sensor networks,"A critical aspect of applications with wireless sensor networks is network lifetime. Power-constrained wireless sensor networks are usable as long as they can communicate sensed data to a processing node. Sensing and communications consume energy, therefore judicious power management and sensor scheduling can effectively extend network lifetime. To cover a set of targets with known locations when ground access in the remote area is prohibited, one solution is to deploy the sensors remotely, from an aircraft. The lack of precise sensor placement is compensated by a large sensor population deployed in the drop zone, that would improve the probability of target coverage. The data collected from the sensors is sent to a central node (e.g. cluster head) for processing. In this paper we propose un efficient method to extend the sensor network life time by organizing the sensors into a maximal number of set covers that are activated successively. Only the sensors from the current active set are responsible for monitoring all targets and for transmitting the collected data, while all other nodes are in a low-energy sleep mode. By allowing sensors to participate in multiple sets, our problem formulation increases the network lifetime compared with related work [M. Cardei et al], that has the additional requirements of sensor sets being disjoint and operating equal time intervals. In this paper we model the solution as the maximum set covers problem and design two heuristics that efficiently compute the sets, using linear programming and a greedy approach. Simulation results are presented to verify our approaches.","Energy efficiency,
Intelligent networks,
Wireless sensor networks,
Sensor phenomena and characterization,
Computer science,
Scheduling,
Energy management,
Monitoring,
Transceivers,
Application software"
Efficient pipeline for image-based patient-specific analysis of cerebral aneurysm hemodynamics: technique and sensitivity,"Hemodynamic factors are thought to be implicated in the progression and rupture of intracranial aneurysms. Current efforts aim to study the possible associations of hemodynamic characteristics such as complexity and stability of intra-aneurysmal flow patterns, size and location of the region of flow impingement with the clinical history of aneurysmal rupture. However, there are no reliable methods for measuring blood flow patterns in vivo. In this paper, an efficient methodology for patient-specific modeling and characterization of the hemodynamics in cerebral aneurysms from medical images is described. A sensitivity analysis of the hemodynamic characteristics with respect to variations of several variables over the expected physiologic range of conditions is also presented. This sensitivity analysis shows that although changes in the velocity fields can be observed, the characterization of the intra-aneurysmal flow patterns is not altered when the mean input flow, the flow division, the viscosity model, or mesh resolution are changed. It was also found that the variable that has the greater impact on the computed flow fields is the geometry of the vascular structures. We conclude that with the proposed modeling pipeline clinical studies involving large numbers cerebral aneurysms are feasible.","Pipelines,
Image analysis,
Aneurysm,
Hemodynamics,
Sensitivity analysis,
Stability,
History,
Fluid flow measurement,
Blood flow,
In vivo"
A Consensus Support System Model for Group Decision-Making Problems With Multigranular Linguistic Preference Relations,"The group decision-making framework with linguistic preference relations is studied. In this context, we assume that there exist several experts who may have different background and knowledge to solve a particular problem and, therefore, different linguistic term sets (multigranular linguistic information) could be used to express their opinions. The aim of this paper is to present a model of consensus support system to assist the experts in all phases of the consensus reaching process of group decision-making problems with multigranular linguistic preference relations. This consensus support system model is based on i) a multigranular linguistic methodology, ii) two consensus criteria, consensus degrees and proximity measures, and iii) a guidance advice system. The multigranular linguistic methodology permits the unification of the different linguistic domains to facilitate the calculus of consensus degrees and proximity measures on the basis of experts' opinions. The consensus degrees assess the agreement amongst all the experts' opinions, while the proximity measures are used to find out how far the individual opinions are from the group opinion. The guidance advice system integrated in the consensus support system model acts as a feedback mechanism, and it is based on a set of advice rules to help the experts change their opinions and to find out which direction that change should follow in order to obtain the highest degree of consensus possible. There are two main advantages provided by this model of consensus support system. Firstly, its ability to cope with group decision-making problems with multigranular linguistic preference relations, and, secondly, the figure of the moderator, traditionally presents in the consensus reaching process, is replaced by the guidance advice system, and in such a way, the whole group decision-making process is automated",
A simple multimembered evolution strategy to solve constrained optimization problems,"This work presents a simple multimembered evolution strategy to solve global nonlinear optimization problems. The approach does not require the use of a penalty function. Instead, it uses a simple diversity mechanism based on allowing infeasible solutions to remain in the population. This technique helps the algorithm to find the global optimum despite reaching reasonably fast the feasible region of the search space. A simple feasibility-based comparison mechanism is used to guide the process toward the feasible region of the search space. Also, the initial stepsize of the evolution strategy is reduced in order to perform a finer search and a combined (discrete/intermediate) panmictic recombination technique improves its exploitation capabilities. The approach was tested with a well-known benchmark. The results obtained are very competitive when comparing the proposed approach against other state-of-the art techniques and its computational cost (measured by the number of fitness function evaluations) is lower than the cost required by the other techniques compared.",
Mining version histories to guide software changes,"We apply data mining to version histories in order to guide programmers along related changes: ""Programmers who changed these functions also changed...."" Given a set of existing changes, the mined association rules 1) suggest and predict likely further changes, 2) show up item coupling that is undetectable by program analysis, and 3) can prevent errors due to incomplete changes. After an initial change, our ROSE prototype can correctly predict further locations to be changed; the best predictive power is obtained for changes to existing software. In our evaluation based on the history of eight popular open source projects, ROSE's topmost three suggestions contained a correct location with a likelihood of more than 70 percent.",
Using linear programming to Decode Binary linear codes,"A new method is given for performing approximate maximum-likelihood (ML) decoding of an arbitrary binary linear code based on observations received from any discrete memoryless symmetric channel. The decoding algorithm is based on a linear programming (LP) relaxation that is defined by a factor graph or parity-check representation of the code. The resulting ""LP decoder"" generalizes our previous work on turbo-like codes. A precise combinatorial characterization of when the LP decoder succeeds is provided, based on pseudocodewords associated with the factor graph. Our definition of a pseudocodeword unifies other such notions known for iterative algorithms, including ""stopping sets,"" ""irreducible closed walks,"" ""trellis cycles,"" ""deviation sets,"" and ""graph covers."" The fractional distance d/sub frac/ of a code is introduced, which is a lower bound on the classical distance. It is shown that the efficient LP decoder will correct up to /spl lceil/d/sub frac//2/spl rceil/-1 errors and that there are codes with d/sub frac/=/spl Omega/(n/sup 1-/spl epsi//). An efficient algorithm to compute the fractional distance is presented. Experimental evidence shows a similar performance on low-density parity-check (LDPC) codes between LP decoding and the min-sum and sum-product algorithms. Methods for tightening the LP relaxation to improve performance are also provided.","Linear programming,
Maximum likelihood decoding,
Iterative decoding,
Parity check codes,
Iterative algorithms,
Linear code,
Computer science,
Performance analysis,
Associate members,
Error correction codes"
Fields of Experts: a framework for learning image priors,"We develop a framework for learning generic, expressive image priors that capture the statistics of natural scenes and can be used for a variety of machine vision tasks. The approach extends traditional Markov random field (MRF) models by learning potential functions over extended pixel neighborhoods. Field potentials are modeled using a Products-of-Experts framework that exploits nonlinear functions of many linear filter responses. In contrast to previous MRF approaches all parameters, including the linear filters themselves, are learned from training data. We demonstrate the capabilities of this Field of Experts model with two example applications, image denoising and image inpainting, which are implemented using a simple, approximate inference scheme. While the model is trained on a generic image database and is not tuned toward a specific application, we obtain results that compete with and even outperform specialized techniques.",
Fundamental limits on detection in low SNR under noise uncertainty,"In this paper we consider the problem of detecting whether a frequency band is being used by a known primary user. We derive fundamental bounds on detection performance in low SNR in the presence of noise uncertainty - the noise is assumed to be white, but we know its distribution only to within a particular set. For clarity of analysis, we focus on primary transmissions that are BPSK-modulated random data without any pilot tones or training sequences. The results should all generalize to more general primary transmissions as long as no deterministic component is present. Specifically, we show that for every 'moment detector' there exists an SNR below which detection becomes impossible in the presence of noise uncertainty. In the neighborhood of that SNR wall, we show how the sample complexity of detection approaches infinity. We also show that if our radio has a finite dynamic range (upper and lower limits to the voltages we can quantize), then at low enough SNR, any detector can be rendered useless even under moderate noise uncertainty.",
Spectral segmentation with multiscale graph decomposition,"We present a multiscale spectral image segmentation algorithm. In contrast to most multiscale image processing, this algorithm works on multiple scales of the image in parallel, without iteration, to capture both coarse and fine level details. The algorithm is computationally efficient, allowing to segment large images. We use the normalized cut graph partitioning framework of image segmentation. We construct a graph encoding pairwise pixel affinity, and partition the graph for image segmentation. We demonstrate that large image graphs can be compressed into multiple scales capturing image structure at increasingly large neighborhood. We show that the decomposition of the image segmentation graph into different scales can be determined by ecological statistics on the image grouping cues. Our segmentation algorithm works simultaneously across the graph scales, with an inter-scale constraint to ensure communication and consistency between the segmentations at each scale. As the results show, we incorporate long-range connections with linear-time complexity, providing high-quality segmentations efficiently. Images that previously could not be processed because of their size have been accurately segmented thanks to this method.",
Location Privacy in Mobile Systems: A Personalized Anonymization Model,"This paper describes a personalized k-anonymity model for protecting location privacy against various privacy threats through location information sharing. Our model has two unique features. First, we provide a unified privacy personalization framework to support location k-anonymity for a wide range of users with context-sensitive personalized privacy requirements. This framework enables each mobile node to specify the minimum level of anonymity it desires as well as the maximum temporal and spatial resolutions it is willing to tolerate when requesting for k-anonymity preserving location-based services (LBSs). Second, we devise an efficient message perturbation engine which runs by the location protection broker on a trusted server and performs location anonymization on mobile users' LBS request messages, such as identity removal and spatio-temporal cloaking of location information. We develop a suite of scalable and yet efficient spatio-temporal cloaking algorithms, called CliqueCloak algorithms, to provide high quality personalized location k-anonymity, aiming at avoiding or reducing known location privacy threats before forwarding requests to LBS provider(s). The effectiveness of our CliqueCloak algorithms is studied under various conditions using realistic location data synthetically generated using real road maps and traffic volume data",
The Grid Economy,"This work identifies challenges in managing resources in a Grid computing environment and proposes computational economy as a metaphor for effective management of resources and application scheduling. It identifies distributed resource management challenges and requirements of economy-based Grid systems, and discusses various representative economy-based systems, both historical and emerging, for cooperative and competitive trading of resources such as CPU cycles, storage, and network bandwidth. It presents an extensive, service-oriented Grid architecture driven by Grid economy and an approach for its realization by leveraging various existing Grid technologies. It also presents commodity and auction models for resource allocation. The use of commodity economy model for resource management and application scheduling in both computational and data grids is also presented.","Resource management,
Grid computing,
Processor scheduling,
Environmental management,
Distributed computing,
Laboratories,
Computer science,
Software engineering,
Application software,
Concurrent computing"
Routing and interface assignment in multi-channel multi-interface wireless networks,"Multiple channels are available for use in IEEE 802.11. Multiple channels can increase the available network capacity, but require new protocols to exploit the available capacity. This paper studies the problem of improving the capacity of multi-channel wireless networks by using multiple interfaces. We consider the scenario when multiple interfaces are available, but the number of available interfaces is lesser than the number of available channels. We provide a classification of interface assignment strategies, and propose a new strategy that does not require modifications to IEEE 802.11. We also identify routing heuristics that are suitable for use with the proposed interface assignment strategy.","Intelligent networks,
Wireless networks,
Routing protocols,
Hardware,
Media Access Protocol,
Wireless LAN,
Spread spectrum communication,
Costs,
Wireless application protocol,
Computer science"
SyncScan: practical fast handoff for 802.11 infrastructure networks,"Wireless access networks scale by replicating base stations geographically and then allowing mobile clients to seamlessly ""hand off"" from one station to the next as they traverse the network. However, providing the illusion of continuous connectivity requires selecting the right moment to handoff and the right base station to transfer to. Unfortunately, 802.11-based networks only attempt a handoff when a client's service degrades to a point where connectivity is threatened. Worse, the overhead of scanning for nearby base stations is routinely over 250 ms - during which incoming packets are dropped - far longer than what can be tolerated by highly interactive applications such as voice telephony. In this paper we describe SyncScan, a low-cost technique for continuously tracking nearby base stations by synchronizing short listening periods at the client with periodic transmissions from each base station. We have implemented this SyncScan algorithm using commodity 802.11 hardware and we demonstrate that it allows better handoff decisions and over an order of magnitude improvement in handoff delay. Finally, our approach only requires trivial implementation changes, is incrementally deployable and is completely backward compatible with existing 802.11 standards.",
Comparison and validation of tissue modelization and statistical classification methods in T1-weighted MR brain images,"This paper presents a validation study on statistical nonsupervised brain tissue classification techniques in magnetic resonance (MR) images. Several image models assuming different hypotheses regarding the intensity distribution model, the spatial model and the number of classes are assessed. The methods are tested on simulated data for which the classification ground truth is known. Different noise and intensity nonuniformities are added to simulate real imaging conditions. No enhancement of the image quality is considered either before or during the classification process. This way, the accuracy of the methods and their robustness against image artifacts are tested. Classification is also performed on real data where a quantitative validation compares the methods' results with an estimated ground truth from manual segmentations by experts. Validity of the various classification methods in the labeling of the image as well as in the tissue volume is estimated with different local and global measures. Results demonstrate that methods relying on both intensity and spatial information are more robust to noise and field inhomogeneities. We also demonstrate that partial volume is not perfectly modeled, even though methods that account for mixture classes outperform methods that only consider pure Gaussian classes. Finally, we show that simulated data results can also be extended to real data.","Brain modeling,
Testing,
Noise robustness,
Magnetic resonance,
Magnetic noise,
Magnetic resonance imaging,
Image quality,
Image segmentation,
Labeling,
Magnetic field measurement"
The effect of network topology on the spread of epidemics,"Many network phenomena are well modeled as spreads of epidemics through a network. Prominent examples include the spread of worms and email viruses, and, more generally, faults. Many types of information dissemination can also be modeled as spreads of epidemics. In this paper we address the question of what makes an epidemic either weak or potent. More precisely, we identify topological properties of the graph that determine the persistence of epidemics. In particular, we show that if the ratio of cure to infection rates is larger than the spectral radius of the graph, then the mean epidemic lifetime is of order log n, where n is the number of nodes. Conversely, if this ratio is smaller than a generalization of the isoperimetric constant of the graph, then the mean epidemic lifetime is of order e/sup na/, for a positive constant a. We apply these results to several network topologies including the hypercube, which is a representative connectivity graph for a distributed hash table, the complete graph, which is an important connectivity graph for BGP, and the power law graph, of which the AS-level Internet graph is a prime example. We also study the star topology and the Erdos-Renyi graph as their epidemic spreading behaviors determine the spreading behavior of power law graphs.","Network topology,
Viruses (medical),
Hypercubes,
Computer worms,
Sufficient conditions,
Computer science,
Graph theory,
Stochastic processes,
Impedance,
Power system faults"
Software reuse research: status and future,"This paper briefly summarizes software reuse research, discusses major research contributions and unsolved problems, provides pointers to key publications, and introduces four papers selected from The Eighth International Conference on Software Reuse (ICSR8).","Software quality,
Software reusability,
Software engineering,
Computer science,
Finance,
Productivity,
Reliability engineering,
Software systems,
Software libraries,
Software architecture"
Local Gabor binary pattern histogram sequence (LGBPHS): a novel non-statistical model for face representation and recognition,"For years, researchers in face recognition area have been representing and recognizing faces based on subspace discriminant analysis or statistical learning. Nevertheless, these approaches are always suffering from the generalizability problem. This paper proposes a novel non-statistics based face representation approach, local Gabor binary pattern histogram sequence (LGBPHS), in which training procedure is unnecessary to construct the face model, so that the generalizability problem is naturally avoided. In this approach, a face image is modeled as a ""histogram sequence"" by concatenating the histograms of all the local regions of all the local Gabor magnitude binary pattern maps. For recognition, histogram intersection is used to measure the similarity of different LGBPHSs and the nearest neighborhood is exploited for final classification. Additionally, we have further proposed to assign different weights for each histogram piece when measuring two LGBPHSes. Our experimental results on AR and FERET face database show the validity of the proposed approach especially for partially occluded face images, and more impressively, we have achieved the best result on FERET face database.","Histograms,
Face recognition,
Pattern recognition,
Statistical learning,
Image databases,
Support vector machines,
Noise robustness,
Computer science,
Research and development,
Content addressable storage"
Automatic detection of red lesions in digital color fundus photographs,"The robust detection of red lesions in digital color fundus photographs is a critical step in the development of automated screening systems for diabetic retinopathy. In this paper, a novel red lesion detection method is presented based on a hybrid approach, combining prior works by Spencer et al. (1996) and Frame et al. (1998) with two important new contributions. The first contribution is a new red lesion candidate detection system based on pixel classification. Using this technique, vasculature and red lesions are separated from the background of the image. After removal of the connected vasculature the remaining objects are considered possible red lesions. Second, an extensive number of new features are added to those proposed by Spencer-Frame. The detected candidate objects are classified using all features and a k-nearest neighbor classifier. An extensive evaluation was performed on a test set composed of images representative of those normally found in a screening set. When determining whether an image contains red lesions the system achieves a sensitivity of 100% at a specificity of 87%. The method is compared with several different automatic systems and is shown to outperform them all. Performance is close to that of a human expert examining the images for the presence of red lesions.","Lesions,
Diabetes,
Biomedical imaging,
Medical diagnostic imaging,
Cities and towns,
Retinopathy,
Blindness,
Robustness,
Object detection,
Performance evaluation"
Year,,
Privacy and rationality in individual decision making,"Traditional theory suggests consumers should be able to manage their privacy. Yet, empirical and theoretical research suggests that consumers often lack enough information to make privacy-sensitive decisions and, even with sufficient information, are likely to trade off long-term privacy for short-term benefits","Privacy,
Decision making,
Protection,
Psychology,
Information security,
Computer security,
Cognitive science,
Problem-solving,
Public policy,
Demography"
Efficient aggregation of encrypted data in wireless sensor networks,"Wireless sensor networks (WSNs) are ad-hoc networks composed of tiny devices with limited computation and energy capacities. For such devices, data transmission is a very energy-consuming operation. It thus becomes essential to the lifetime of a WSN to minimize the number of bits sent by each device. One well-known approach is to aggregate sensor data (e.g., by adding) along the path from sensors to the sink. Aggregation becomes especially challenging if end-to-end privacy between sensors and the sink is required. In this paper, we propose a simple and provably secure additively homomorphic stream cipher that allows efficient aggregation of encrypted data. The new cipher only uses modular additions (with very small moduli) and is therefore very well suited for CPU-constrained devices. We show that aggregation based on this cipher can be used to efficiently compute statistical values such as mean, variance and standard deviation of sensed data, while achieving significant bandwidth gain.","Cryptography,
Intelligent networks,
Wireless sensor networks,
Computer networks,
Privacy,
Computer science,
Data communication,
Monitoring,
Authentication,
Ad hoc networks"
Search biases in constrained evolutionary optimization,"A common approach to constraint handling in evolutionary optimization is to apply a penalty function to bias the search toward a feasible solution. It has been proposed that the subjective setting of various penalty parameters can be avoided using a multiobjective formulation. This paper analyzes and explains in depth why and when the multiobjective approach to constraint handling is expected to work or fail. Furthermore, an improved evolutionary algorithm based on evolution strategies and differential variation is proposed. Extensive experimental studies have been carried out. Our results reveal that the unbiased multiobjective approach to constraint handling may not be as effective as one may have assumed.","Constraint optimization,
Failure analysis,
Evolutionary computation,
Functional programming,
Computer science"
Actions sketch: a novel action representation,"In this paper, we propose to model an action based on both the shape and the motion of the performing object. When the object performs an action in 3D, the points on the outer boundary of the object are projected as 2D (x, y) contour in the image plane. A sequence of such 2D contours with respect to time generates a spatiotemporal volume (STV) in (x, y, t), which can be treated as 3D object in the (x, y, t) space. We analyze STV by using the differential geometric surface properties to identify action descriptors capturing both spatial and temporal properties. A set of action descriptors is called an action sketch. The first step in our approach is to generate STV by solving the point correspondence problem between consecutive frames. The correspondences are determined using a two-step graph theoretical approach. After the STV is generated, actions descriptors are computed by analyzing the differential geometric properties of STV. Finally, using these descriptors, we perform action recognition, which is also formulated as graph theoretical problem. Several experimental results are presented to demonstrate our approach.","Shape,
Legged locomotion,
Spatiotemporal phenomena,
Humans,
Computer vision,
Hidden Markov models,
Data mining,
Trajectory,
Computer science,
Surface treatment"
Combined ultrasound and optoacoustic system for real-time high-contrast vascular imaging in vivo,"In optoacoustic imaging, short laser pulses irradiate highly scattering human tissue and adiabatically heat embedded absorbing structures, such as blood vessels, to generate ultrasound transients by means of the thermoelastic effect. We present an optoacoustic vascular imaging system that records these transients on the skin surface with an ultrasound transducer array and displays the images online. With a single laser pulse a complete optoacoustic B-mode image can be acquired. The optoacoustic system exploits the high intrinsic optical contrast of blood and provides high-contrast images without the need for contrast agents. The high spatial resolution of the system is determined by the acoustic propagation and is limited to the submillimeter range by our 7.5-MHz linear array transducer. A Q-switched alexandrite laser emitting short near-infrared laser pulses at a wavelength of 760 nm allows an imaging depth of a few centimeters. The system provides real-time images at frame-rates of 7.5 Hz and optionally displays the classically generated ultrasound image alongside the optoacoustic image. The functionality of the system was demonstrated in vivo on human finger, arm and leg. The proposed system combines the merits and most compelling features of optics and ultrasound in a single high-contrast vascular imaging device.",
HOT SAX: efficiently finding the most unusual time series subsequence,"In this work, we introduce the new problem of finding time series discords. Time series discords are subsequences of a longer time series that are maximally different to all the rest of the time series subsequences. They thus capture the sense of the most unusual subsequence within a time series. Time series discords have many uses for data mining, including improving the quality of clustering, data cleaning, summarization, and anomaly detection. Discords are particularly attractive as anomaly detectors because they only require one intuitive parameter (the length of the subsequence) unlike most anomaly detection algorithms that typically require many parameters. We evaluate our work with a comprehensive set of experiments. In particular, we demonstrate the utility of discords with objective experiments on domains as diverse as Space Shuttle telemetry monitoring, medicine, surveillance, and industry, and we demonstrate the effectiveness of our discord discovery algorithm with more than one million experiments, on 82 different datasets from diverse domains.","Telemetry,
Computer science,
Data mining,
Detectors,
Detection algorithms,
Space shuttles,
Surveillance,
Aerospace industry,
Cleaning,
Monitoring"
Cluster-head election using fuzzy logic for wireless sensor networks,"Wireless sensor networks (WSNs) present a new generation of real-time embedded systems with limited computation, energy and memory resources that are being used in a wide variety of applications where traditional networking infrastructure is practically infeasible. Appropriate cluster-head node election can drastically reduce the energy consumption and enhance the lifetime of the network. In this paper, a fuzzy logic approach to cluster-head election is proposed based on three descriptors-energy, concentration and centrality. Simulation shows that depending upon network configuration, a substantial increase in network lifetime can be accomplished as compared to probabilistically selecting the nodes as cluster-heads using only local information.","Nominations and elections,
Fuzzy logic,
Wireless sensor networks,
Energy consumption,
Base stations,
Computer science,
Monitoring,
Chemical and biological sensors,
Biosensors,
Acoustic sensors"
Spatio-temporal nonrigid registration for ultrasound cardiac motion estimation,"We propose a new spatio-temporal elastic registration algorithm for motion reconstruction from a series of images. The specific application is to estimate displacement fields from two-dimensional ultrasound sequences of the heart. The basic idea is to find a spatio-temporal deformation field that effectively compensates for the motion by minimizing a difference with respect to a reference frame. The key feature of our method is the use of a semi-local spatio-temporal parametric model for the deformation using splines, and the reformulation of the registration task as a global optimization problem. The scale of the spline model controls the smoothness of the displacement field. Our algorithm uses a multiresolution optimization strategy to obtain a higher speed and robustness. We evaluated the accuracy of our algorithm using a synthetic sequence generated with an ultrasound simulation package, together with a realistic cardiac motion model. We compared our new global multiframe approach with a previous method based on pairwise registration of consecutive frames to demonstrate the benefits of introducing temporal consistency. Finally, we applied the algorithm to the regional analysis of the left ventricle. Displacement and strain parameters were evaluated showing significant differences between the normal and pathological segments, thereby illustrating the clinical applicability of our method.","Ultrasonic imaging,
Motion estimation,
Image reconstruction,
Heart,
Parametric statistics,
Optimization methods,
Displacement control,
Robustness,
Packaging,
Algorithm design and analysis"
Fast replanning for navigation in unknown terrain,"Mobile robots often operate in domains that are only incompletely known, for example, when they have to move from given start coordinates to given goal coordinates in unknown terrain. In this case, they need to be able to replan quickly as their knowledge of the terrain changes. Stentz' Focussed Dynamic A/sup */ (D/sup */) is a heuristic search method that repeatedly determines a shortest path from the current robot coordinates to the goal coordinates while the robot moves along the path. It is able to replan faster than planning from scratch since it modifies its previous search results locally. Consequently, it has been extensively used in mobile robotics. In this article, we introduce an alternative to D/sup */ that determines the same paths and thus moves the robot in the same way but is algorithmically different. D/sup */ Lite is simple, can be rigorously analyzed, extendible in multiple ways, and is at least as efficient as D/sup */. We believe that our results will make D/sup */-like replanning methods even more popular and enable robotics researchers to adapt them to additional applications.","Navigation,
Robot kinematics,
Mobile robots,
Robot sensing systems,
Costs,
Search methods,
Path planning,
Motion planning,
Computer science,
Land vehicles"
Rapid gridding reconstruction with a minimal oversampling ratio,"Reconstruction of magnetic resonance images from data not falling on a Cartesian grid is a Fourier inversion problem typically solved using convolution interpolation, also known as gridding. Gridding is simple and robust and has parameters, the grid oversampling ratio and the kernel width, that can be used to trade accuracy for computational memory and time reductions. We have found that significant reductions in computation memory and time can be obtained while maintaining high accuracy by using a minimal oversampling ratio, from 1.125 to 1.375, instead of the typically employed grid oversampling ratio of two. When using a minimal oversampling ratio, appropriate design of the convolution kernel is important for maintaining high accuracy. We derive a simple equation for choosing the optimal Kaiser-Bessel convolution kernel for a given oversampling ratio and kernel width. As well, we evaluate the effect of presampling the kernel, a common technique used to reduce the computation time, and find that using linear interpolation between samples adds negligible error with far less samples than is necessary with nearest-neighbor interpolation. We also develop a new method for choosing the optimal presampled kernel. Using a minimal oversampling ratio and presampled kernel, we are able to perform a three-dimensional (3-D) reconstruction in one-eighth the time and requiring one-third the computer memory versus using an oversampling ratio of two and a Kaiser-Bessel convolution kernel, while maintaining the same level of accuracy.","Kernel,
Image reconstruction,
Convolution,
Interpolation,
Magnetic resonance imaging,
Magnetic resonance,
Fourier transforms,
Grid computing,
Sampling methods,
Robustness"
Tracking of migrating cells under phase-contrast video microscopy with combined mean-shift processes,"In this paper, we propose a combination of mean-shift-based tracking processes to establish migrating cell trajectories through in vitro phase-contrast video microscopy. After a recapitulation on how the mean-shift algorithm permits efficient object tracking we describe the proposed extension and apply it to the in vitro cell tracking problem. In this application, the cells are unmarked (i.e., no fluorescent probe is used) and are observed under classical phase-contrast microscopy. By introducing an adaptive combination of several kernels, we address several problems such as variations in size and shape of the tracked objects (e.g., those occurring in the case of cell membrane extensions), the presence of incomplete (or noncontrasted) object boundaries, partially overlapping objects and object splitting (in the case of cell divisions or mitoses). Comparing the tracking results automatically obtained to those generated manually by a human expert, we tested the stability of the different algorithm parameters and their effects on the tracking results. We also show how the method is resistant to a decrease in image resolution and accidental defocusing (which may occur during long experiments, e.g., dozens of hours). Finally, we applied our methodology on cancer cell tracking and showed that cytochalasin-D significantly inhibits cell motility.",
Adaptive routing for intermittently connected mobile ad hoc networks,"The vast majority of mobile ad hoc networking research makes a very large assumption - that communication can only take place between nodes that are simultaneously accessible within the same connected cloud (i.e., that communication is synchronous). In reality, this assumption is likely to be a poor one, particularly for sparsely or irregularly populated environments. We present the context-aware routing (CAR) algorithm. CAR is a novel approach to the provision of asynchronous communication in partially-connected mobile ad hoc networks, based on the intelligent placement of messages. We discuss the details of the algorithm, and then present simulation results demonstrating that it is possible for nodes to exploit context information in making local decisions that lead to good delivery ratios and latencies with small overheads.","Routing,
Mobile ad hoc networks,
Clouds,
Context,
Mobile communication,
Delay,
Ad hoc networks,
Algorithm design and analysis,
Computer science,
Educational institutions"
MV routing and capacity building in disruption tolerant networks,"Disruption-tolerant networks (DTNs) differ from other types of networks in that capacity is exclusively created by the movements of participants. This implies that understanding and influencing the participants' motions can have a significant impact on network performance. In this paper, we introduce the routing protocol MV, which learns structure in the movement patterns of network participants and uses it to enable informed message passing. We also propose the introduction of autonomous agents as additional participants in DTNs. These agents adapt their movements in response to variations in network capacity and demand. We use multi-objective control methods from robotics to generate motions capable of optimizing multiple network performance metrics simultaneously. We present experimental evidence that these strategies, individually and in conjunction, result in significant performance improvements in DTNs.","Intelligent networks,
Disruption tolerant networking,
Routing protocols,
Autonomous agents,
Measurement,
Bandwidth,
Delay,
Computer science,
Message passing,
Motion control"
"Recognizing partially occluded, expression variant faces from single training image per person with SOM and soft k-NN ensemble","Most classical template-based frontal face recognition techniques assume that multiple images per person are available for training, while in many real-world applications only one training image per person is available and the test images may be partially occluded or may vary in expressions. This paper addresses those problems by extending a previous local probabilistic approach presented by Martinez, using the self-organizing map (SOM) instead of a mixture of Gaussians to learn the subspace that represented each individual. Based on the localization of the training images, two strategies of learning the SOM topological space are proposed, namely to train a single SOM map for all the samples and to train a separate SOM map for each class, respectively. A soft k nearest neighbor (soft k-NN) ensemble method, which can effectively exploit the outputs of the SOM topological space, is also proposed to identify the unlabeled subjects. Experiments show that the proposed method exhibits high robust performance against the partial occlusions and variant expressions.",
Accuracy of fluorescent tomography in the presence of heterogeneities:study of the normalized born ratio,"We studied the performance of three-dimensional fluorescence tomography of diffuse media in the presence of heterogeneities. Experimental measurements were acquired using an imaging system consisting of a parallel plate-imaging chamber and a lens coupled charge coupled device camera, which enables conventional planar imaging as well as fluorescence tomography. To simulate increasing levels of background heterogeneity, we employed phantoms made of a fluorescent tube surrounded by several absorbers in different combinations of absorption distribution. We also investigated the effect of low absorbing thin layers (such as membranes). We show that the normalized Born approach accurately retrieves the position and shape of the fluorochrome even at high background heterogeneity. We also demonstrate that the quantification is relatively insensitive to a varying degree of heterogeneity and background optical properties. Findings are further contrasted to images obtained with the standard Born expansion and with a normalized approach that divides the fluorescent field with excitation measurements through a homogeneous medium.","Fluorescence,
Tomography,
Optical imaging,
Charge-coupled image sensors,
Current measurement,
Charge measurement,
Lenses,
Imaging phantoms,
Absorption,
Biomembranes"
Video summarization and scene detection by graph modeling,"We propose a unified approach for video summarization based on the analysis of video structures and video highlights. Two major components in our approach are scene modeling and highlight detection. Scene modeling is achieved by normalized cut algorithm and temporal graph analysis, while highlight detection is accomplished by motion attention modeling. In our proposed approach, a video is represented as a complete undirected graph and the normalized cut algorithm is carried out to globally and optimally partition the graph into video clusters. The resulting clusters form a directed temporal graph and a shortest path algorithm is proposed to efficiently detect video scenes. The attention values are then computed and attached to the scenes, clusters, shots, and subshots in a temporal graph. As a result, the temporal graph can inherently describe the evolution and perceptual importance of a video. In our application, video summaries that emphasize both content balance and perceptual quality can be generated directly from a temporal graph that embeds both the structure and attention information.","Layout,
Clustering algorithms,
Partitioning algorithms,
Motion detection,
Motion analysis,
Algorithm design and analysis,
Councils,
Computer science,
Asia,
Entropy"
Realistic simulation of the 3-D growth of brain tumors in MR images coupling diffusion with biomechanical deformation,"We propose a new model to simulate the three-dimensional (3-D) growth of glioblastomas multiforma (GBMs), the most aggressive glial tumors. The GBM speed of growth depends on the invaded tissue: faster in white than in gray matter, it is stopped by the dura or the ventricles. These different structures are introduced into the model using an atlas matching technique. The atlas includes both the segmentations of anatomical structures and diffusion information in white matter fibers. We use the finite element method (FEM) to simulate the invasion of the GBM in the brain parenchyma and its mechanical interaction with the invaded structures (mass effect). Depending on the considered tissue, the former effect is modeled with a reaction-diffusion or a Gompertz equation, while the latter is based on a linear elastic brain constitutive equation. In addition, we propose a new coupling equation taking into account the mechanical influence of the tumor cells on the invaded tissues. The tumor growth simulation is assessed by comparing the in-silico GBM growth with the real growth observed on two magnetic resonance images (MRIs) of a patient acquired with 6 mo difference. Results show the feasibility of this new conceptual approach and justifies its further evaluation.","Brain modeling,
Deformable models,
Neoplasms,
Equations,
Medical treatment,
Tumors,
Magnetic resonance,
Hospitals,
Inverse problems,
Diffusion bonding"
Direct reconstruction of kinetic parameter images from dynamic PET data,"Our goal in this paper is the estimation of kinetic model parameters for each voxel corresponding to a dense three-dimensional (3-D) positron emission tomography (PET) image. Typically, the activity images are first reconstructed from PET sinogram frames at each measurement time, and then the kinetic parameters are estimated by fitting a model to the reconstructed time-activity response of each voxel. However, this ""indirect"" approach to kinetic parameter estimation tends to reduce signal-to-noise ratio (SNR) because of the requirement that the sinogram data be divided into individual time frames. In 1985, Carson and Lange proposed, but did not implement, a method based on the expectation-maximization (EM) algorithm for direct parametric reconstruction. The approach is ""direct"" because it estimates the optimal kinetic parameters directly from the sinogram data, without an intermediate reconstruction step. However, direct voxel-wise parametric reconstruction remained a challenge due to the unsolved complexities of inversion and spatial regularization. In this paper, we demonstrate and evaluate a new and efficient method for direct voxel-wise reconstruction of kinetic parameter images using all frames of the PET data. The direct parametric image reconstruction is formulated in a Bayesian framework, and uses the parametric iterative coordinate descent (PICD) algorithm to solve the resulting optimization problem. The PICD algorithm is computationally efficient and is implemented with spatial regularization in the domain of the physiologically relevant parameters. Our experimental simulations of a rat head imaged in a working small animal scanner indicate that direct parametric reconstruction can substantially reduce root-mean-squared error (RMSE) in the estimation of kinetic parameters, as compared to indirect methods, without appreciably increasing computation.","Image reconstruction,
Kinetic theory,
Positron emission tomography,
Parameter estimation,
Iterative algorithms,
Time measurement,
Signal to noise ratio,
Bayesian methods,
Computational modeling,
Head"
Genetic tuning of fuzzy rule deep structures preserving interpretability and its interaction with fuzzy rule set reduction,"Tuning fuzzy rule-based systems for linguistic fuzzy modeling is an interesting and widely developed task. It involves adjusting some of the components of the knowledge base without completely redefining it. This contribution introduces a genetic tuning process for jointly fitting the fuzzy rule symbolic representations and the meaning of the involved membership functions. To adjust the former component, we propose the use of linguistic hedges to perform slight modifications keeping a good interpretability. To alter the latter component, two different approaches changing their basic parameters and using nonlinear scaling factors are proposed. As the accomplished experimental study shows, the good performance of our proposal mainly lies in the consideration of this tuning approach performed at two different levels of significance. The paper also analyzes the interaction of the proposed tuning method with a fuzzy rule set reduction process. A good interpretability-accuracy tradeoff is obtained combining both processes with a sequential scheme: first reducing the rule set and subsequently tuning the model.","Fuzzy sets,
Genetics,
Fuzzy systems,
Surface structures,
Fuzzy logic,
Computer science,
Knowledge based systems,
Proposals,
Modeling,
Artificial intelligence"
From Physiological Signals to Emotions: Implementing and Comparing Selected Methods for Feature Extraction and Classification,"Little attention has been paid so far to physiological signals for emotion recognition compared to audio-visual emotion channels, such as facial expressions or speech. In this paper, we discuss the most important stages of a fully implemented emotion recognition system including data analysis and classification. For collecting physiological signals in different affective states, we used a music induction method which elicits natural emotional reactions from the subject. Four-channel biosensors are used to obtain electromyogram, electrocardiogram, skin conductivity and respiration changes. After calculating a sufficient amount of features from the raw signals, several feature selection/reduction methods are tested to extract a new feature set consisting of the most significant features for improving classification performance. Three well-known classifiers, linear discriminant function, k-nearest neighbour and multilayer perceptron, are then used to perform supervised classification","Feature extraction,
Emotion recognition,
Speech,
Data analysis,
Multiple signal classification,
Biosensors,
Skin,
Conductivity,
Testing,
Multilayer perceptrons"
Social networks applied,"Social networks have interesting properties. They influence our lives enormously without us being aware of the implications they raise. The authors investigate the following areas concerning social networks: how to exploit our unprecedented wealth of data and how we can mine social networks for purposes such as marketing campaigns; social networks as a particular form of influence, i.e.., the way that people agree on terminology and this phenomenon's implications for the way we build ontologies and the Semantic Web; social networks as something we can discover from data; the use of social network information to offer a wealth of new applications such as better recommendations for restaurants, trustworthy email senders, or (maybe) blind dates; investigation of the richness and difficulty of harvesting FOAF (friend-of-a-friend) information; and by looking at how information processing is bound to social context, the resulting ways that network topology's definition determines its outcomes.","Social network services,
Semantic Web,
Computational modeling,
Power system modeling,
Shape,
Terminology,
Ontologies,
Service oriented architecture,
Information processing,
Marketing and sales"
A principled approach to detecting surprising events in video,"Primates demonstrate unparalleled ability at rapidly orienting towards important events in complex dynamic environments. During rapid guidance of attention and gaze towards potential objects of interest or threats, often there is no time for detailed visual analysis. Thus, heuristic computations are necessary to locate the most interesting events in quasi real-time. We present a new theory of sensory surprise, which provides a principled and computable shortcut to important information. We develop a model that computes instantaneous low-level surprise at every location in video streams. The algorithm significantly correlates with eye movements of two humans watching complex video clips, including television programs (17,936 frames, 2,152 saccadic gaze shifts). The system allows more sophisticated and time-consuming image analysis to be efficiently focused onto the most surprising subsets of the incoming data.","Event detection,
Humans,
Computer science,
Bioinformatics,
Streaming media,
Image analysis,
Computer vision,
Brain modeling,
Neuroscience,
Genomics"
Resource Allocation for Autonomic Data Centers using Analytic Performance Models,"Large data centers host several application environments (AEs) that are subject to workloads whose intensity varies widely and unpredictably. Therefore, the servers of the data center may need to be dynamically redeployed among the various AEs in order to optimize some global utility function. Previous approaches to solving this problem suffer from scalability limitations and cannot easily address the fact that there may be multiple classes of workloads executing on the same AE. This paper presents a solution that addresses these limitations. This solution is based on the use of analytic queuing network models combined with combinatorial search techniques. The paper demonstrates the effectiveness of the approach through simulation experiments. Both online and batch workloads are considered",
Automatic analysis of multimodal group actions in meetings,"This paper investigates the recognition of group actions in meetings. A framework is employed in which group actions result from the interactions of the individual participants. The group actions are modeled using different HMM-based approaches, where the observations are provided by a set of audiovisual features monitoring the actions of individuals. Experiments demonstrate the importance of taking interactions into account in modeling the group actions. It is also shown that the visual modality contains useful information, even for predominantly audio-based events, motivating a multimodal approach to meeting analysis.","Speech analysis,
Information analysis,
Application software,
Hidden Markov models,
Monitoring,
Multimedia databases,
Signal processing,
Visual databases,
Speech processing,
Speech recognition"
Incremental Online Learning in High Dimensions,"Locally weighted projection regression (LWPR) is a new algorithm for incremental nonlinear function approximation in high-dimensional spaces with redundant and irrelevant input dimensions. At its core, it employs nonparametric regression with locally linear models. In order to stay computationally efficient and numerically robust, each local model performs the regression analysis with a small number of univariate regressions in selected directions in input space in the spirit of partial least squares regression. We discuss when and how local learning techniques can successfully work in high-dimensional spaces and review the various techniques for local dimensionality reduction before finally deriving the LWPR algorithm. The properties of LWPR are that it (1) learns rapidly with second-order learning methods based on incremental training, (2) uses statistically sound stochastic leave-one-out cross validation for learning without the need to memorize training data, (3) adjusts its weighting kernels based on only local information in order to minimize the danger of negative interference of incremental learning, (4) has a computational complexity that is linear in the number of inputs, and (5) can deal with a large number ofâ€”possibly redundantâ€”inputs, as shown in various empirical evaluations with up to 90 dimensional data sets. For a probabilistic interpretation, predictive variance and confidence intervals are derived. To our knowledge, LWPR is the first truly incremental spatially localized learning method that can successfully and efficiently operate in very high-dimensional spaces.",
The exploration/exploitation tradeoff in dynamic cellular genetic algorithms,"This paper studies static and dynamic decentralized versions of the search model known as cellular genetic algorithm (cGA), in which individuals are located in a specific topology and interact only with their neighbors. Making changes in the shape of such topology or in the neighborhood may give birth to a high number of algorithmic variants. We perform these changes in a methodological way by tuning the concept of ratio. Since the relationship (ratio) between the topology and the neighborhood shape defines the search selection pressure, we propose to analyze in depth the influence of this ratio on the exploration/exploitation tradeoff. As we will see, it is difficult to decide which ratio is best suited for a given problem. Therefore, we introduce a preprogrammed change of this ratio during the evolution as a possible additional improvement that removes the need of specifying a single ratio. A later refinement will lead us to the first adaptive dynamic kind of cellular models to our knowledge. We conclude that these dynamic cGAs have the most desirable behavior among all the evaluated ones in terms of efficiency and accuracy; we validate our results on a set of seven different problems of considerable complexity in order to better sustain our conclusions.",
Accelerating popular tomographic reconstruction algorithms on commodity PC graphics hardware,"The task of reconstructing an object from its projections via tomographic methods is a time-consuming process due to the vast complexity of the data. For this reason, manufacturers of equipment for medical computed tomography (CT) rely mostly on special application specified integrated circuits (ASICs) to obtain the fast reconstruction times required in clinical settings. Although modern CPUs have gained sufficient power in recent years to be competitive for two-dimensional (2D) reconstruction, this is not the case for three-dimensional (3D) reconstructions, especially not when iterative algorithms must be applied. The recent evolution of commodity PC computer graphics boards (GPUs) has the potential to change this picture in a very dramatic way. In this paper we will show how the new floating point GPUs can be exploited to perform both analytical and iterative reconstruction from X-ray and functional imaging data. For this purpose, we decompose three popular three-dimensional (3D) reconstruction algorithms (Feldkamp filtered backprojection, the simultaneous algebraic reconstruction technique, and expectation maximization) into a common set of base modules, which all can be executed on the GPU and their output linked internally. Visualization of the reconstructed object is easily achieved since the object already resides in the graphics hardware, allowing one to run a visualization module at any time to view the reconstruction results. Our implementation allows speedups of over an order of magnitude with respect to CPU implementations, at comparable image quality.",
A study on several Machine-learning methods for classification of Malignant and benign clustered microcalcifications,"In this paper, we investigate several state-of-the-art machine-learning methods for automated classification of clustered microcalcifications (MCs). The classifier is part of a computer-aided diagnosis (CADx) scheme that is aimed to assisting radiologists in making more accurate diagnoses of breast cancer on mammograms. The methods we considered were: support vector machine (SVM), kernel Fisher discriminant (KFD), relevance vector machine (RVM), and committee machines (ensemble averaging and AdaBoost), of which most have been developed recently in statistical learning theory. We formulated differentiation of malignant from benign MCs as a supervised learning problem, and applied these learning methods to develop the classification algorithm. As input, these methods used image features automatically extracted from clustered MCs. We tested these methods using a database of 697 clinical mammograms from 386 cases, which included a wide spectrum of difficult-to-classify cases. We analyzed the distribution of the cases in this database using the multidimensional scaling technique, which reveals that in the feature space the malignant cases are not trivially separable from the benign ones. We used receiver operating characteristic (ROC) analysis to evaluate and to compare classification performance by the different methods. In addition, we also investigated how to combine information from multiple-view mammograms of the same case so that the best decision can be made by a classifier. In our experiments, the kernel-based methods (i.e., SVM, KFD, and RVM) yielded the best performance (A/sub z/=0.85, SVM), significantly outperforming a well-established, clinically-proven CADx approach that is based on neural network (A/sub z/=0.80).",
Authenticated routing for ad hoc networks,"Initial work in ad hoc routing has considered only the problem of providing efficient mechanisms for finding paths in very dynamic networks, without considering security. Because of this, there are a number of attacks that can be used to manipulate the routing in an ad hoc network. In this paper, we describe these threats, specifically showing their effects on ad hoc on-demand distance vector and dynamic source routing. Our protocol, named authenticated routing for ad hoc networks (ARAN), uses public-key cryptographic mechanisms to defeat all identified attacks. We detail how ARAN can secure routing in environments where nodes are authorized to participate but untrusted to cooperate, as well as environments where participants do not need to be authorized to participate. Through both simulation and experimentation with our publicly available implementation, we characterize and evaluate ARAN and show that it is able to effectively and efficiently discover secure routes within an ad hoc network.","Ad hoc networks,
Routing protocols,
Computer science,
National security,
Costs,
Cryptographic protocols,
Public key cryptography,
Mobile ad hoc networks,
Centralized control,
IP networks"
Real-time neuroevolution in the NERO video game,"In most modern video games, character behavior is scripted; no matter how many times the player exploits a weakness, that weakness is never repaired. Yet, if game characters could learn through interacting with the player, behavior could improve as the game is played, keeping it interesting. This paper introduces the real-time Neuroevolution of Augmenting Topologies (rtNEAT) method for evolving increasingly complex artificial neural networks in real time, as a game is being played. The rtNEAT method allows agents to change and improve during the game. In fact, rtNEAT makes possible an entirely new genre of video games in which the player trains a team of agents through a series of customized exercises. To demonstrate this concept, the Neuroevolving Robotic Operatives (NERO) game was built based on rtNEAT. In NERO, the player trains a team of virtual robots for combat against other players' teams. This paper describes results from this novel application of machine learning, and demonstrates that rtNEAT makes possible video games like NERO where agents evolve and adapt in real time. In the future, rtNEAT may allow new kinds of educational and training applications through interactive and adapting games.","Games,
Network topology,
Machine learning,
Neural networks,
Robots,
Artificial intelligence,
Toy industry,
Computer science education,
Industrial training,
Artificial neural networks"
Intrathoracic airway trees: segmentation and airway morphology analysis from low-dose CT scans,"The segmentation of the human airway tree from volumetric computed tomography (CT) images builds an important step for many clinical applications and for physiological studies. Previously proposed algorithms suffer from one or several problems: leaking into the surrounding lung parenchyma, the need for the user to manually adjust parameters, excessive runtime. Low-dose CT scans are increasingly utilized in lung screening studies, but segmenting them with traditional airway segmentation algorithms often yields less than satisfying results. In this paper, a new airway segmentation method based on fuzzy connectivity is presented. Small adaptive regions of interest are used that follow the airway branches as they are segmented. This has several advantages. It makes it possible to detect leaks early and avoid them, the segmentation algorithm can automatically adapt to changing image parameters, and the computing time is kept within moderate values. The new method is robust in the sense that it works on various types of scans (low-dose and regular dose, normal subjects and diseased subjects) without the need for the user to manually adjust any parameters. Comparison with a commonly used region-grow segmentation algorithm shows that the newly proposed method retrieves a significantly higher count of airway branches. A method that conducts accurate cross-sectional airway measurements on airways is presented as an additional processing step. Measurements are conducted in the original gray-level volume. Validation on a phantom shows that subvoxel accuracy is achieved for all airway sizes and airway orientations.","Morphology,
Computed tomography,
Image segmentation,
Lungs,
Humans,
Runtime,
Leak detection,
Robustness,
Volume measurement,
Imaging phantoms"
The monitoring and early detection of Internet worms,"After many Internet-scale worm incidents in recent years, it is clear that a simple self-propagating worm can quickly spread across the Internet and cause severe damage to our society. Facing this great security threat, we need to build an early detection system that can detect the presence of a worm in the Internet as quickly as possible in order to give people accurate early warning information and possible reaction time for counteractions. This paper first presents an Internet worm monitoring system. Then, based on the idea of ""detecting the trend, not the burst"" of monitored illegitimate traffic, we present a ""trend detection"" methodology to detect a worm at its early propagation stage by using Kalman filter estimation, which is robust to background noise in the monitored data. In addition, for uniform-scan worms such as Code Red, we can effectively predict the overall vulnerable population size, and estimate accurately how many computers are really infected in the global Internet based on the biased monitored data. For monitoring a nonuniform scan worm, especially a sequential-scan worm such as Blaster, we show that it is crucial for the address space covered by the worm monitoring system to be as distributed as possible.","Internet,
Computer worms,
Computerized monitoring,
Societies,
IP networks,
Telecommunication traffic,
Computer science,
Intrusion detection,
Computer hacking,
Traffic control"
Interactive graph cut based segmentation with shape priors,"Interactive or semi-automatic segmentation is a useful alternative to pure automatic segmentation in many applications. While automatic segmentation can be very challenging, a small amount of user input can often resolve ambiguous decisions on the part of the algorithm. In this work, we devise a graph cut algorithm for interactive segmentation which incorporates shape priors. While traditional graph cut approaches to interactive segmentation are often quite successful, they may fail in cases where there are diffuse edges, or multiple similar objects in close proximity to one another. Incorporation of shape priors within this framework mitigates these problems. Positive results on both medical and natural images are demonstrated.","Shape,
Image segmentation,
Biomedical imaging,
Medical treatment,
Bladder,
Computer science,
Application software,
Level set,
Biomedical applications of radiation,
Visualization"
Discriminative learning of Markov random fields for segmentation of 3D scan data,"We address the problem of segmenting 3D scan data into objects or object classes. Our segmentation framework is based on a subclass of Markov random fields (MRFs) which support efficient graph-cut inference. The MRF models incorporate a large set of diverse features and enforce the preference that adjacent scan points have the same classification label. We use a recently proposed maximum-margin framework to discriminatively train the model from a set of labeled scans; as a result we automatically learn the relative importance of the features for the segmentation task. Performing graph-cut inference in the trained MRF can then be used to segment new scenes very efficiently. We test our approach on three large-scale datasets produced by different kinds of 3D sensors, showing its applicability to both outdoor and indoor environments containing diverse objects.","Markov random fields,
Layout,
Image segmentation,
Computer science,
Testing,
Large-scale systems,
Indoor environments,
Mobile robots,
Robot sensing systems,
Robot localization"
Power scaling for cognitive radio,"In this paper we explore the idea of using cognitive radios to reuse locally unused spectrum for their own transmissions. We impose the constraint that they cannot general e unacceptable levels of interference to licensed systems on the same frequency. Using received SNR as a proxy for distance, we prove that a cognitive radio can vary its transmit power while maintaining a guarantee of service to primary users. We consider the aggregate interference caused by multiple cognitive radios and show that aggregation causes a change in the effective decay rate of the interference. We examine the effects of heterogeneous propagation path loss functions and justify the feasibility of multiple secondary users with dynamic transmit powers. Finally, we prove the fundamental constraint on a cognitive radio's transmit power is the minimum SNR it can detect and explore the effect of this power cap.","Cognitive radio,
Interference constraints,
Radio transmitters,
Aggregates,
Propagation losses,
FCC,
Computer science,
Frequency,
Time measurement,
Manufacturing"
CLUE: cluster-based retrieval of images by unsupervised learning,"In a typical content-based image retrieval (CBIR) system, target images (images in the database) are sorted by feature similarities with respect to the query. Similarities among target images are usually ignored. This paper introduces a new technique, cluster-based retrieval of images by unsupervised learning (CLUE), for improving user interaction with image retrieval systems by fully exploiting the similarity information. CLUE retrieves image clusters by applying a graph-theoretic clustering algorithm to a collection of images in the vicinity of the query. Clustering in CLUE is dynamic. In particular, clusters formed depend on which images are retrieved in response to the query. CLUE can be combined with any real-valued symmetric similarity measure (metric or nonmetric). Thus, it may be embedded in many current CBIR systems, including relevance feedback systems. The performance of an experimental image retrieval system using CLUE is evaluated on a database of around 60,000 images from COREL. Empirical results demonstrate improved performance compared with a CBIR system using the same image similarity measure. In addition, results on images returned by Google's Image Search reveal the potential of applying CLUE to real-world image data and integrating CLUE as a part of the interface for keyword-based image retrieval systems.",
An evolutionary algorithm with guided mutation for the maximum clique problem,"Estimation of distribution algorithms sample new solutions (offspring) from a probability model which characterizes the distribution of promising solutions in the search space at each generation. The location information of solutions found so far (i.e., the actual positions of these solutions in the search space) is not directly used for generating offspring in most existing estimation of distribution algorithms. This paper introduces a new operator, called guided mutation. Guided mutation generates offspring through combination of global statistical information and the location information of solutions found so far. An evolutionary algorithm with guided mutation (EA/G) for the maximum clique problem is proposed in this paper. Besides guided mutation, EA/G adopts a strategy for searching different search areas in different search phases. Marchiori's heuristic is applied to each new solution to produce a maximal clique in EA/G. Experimental results show that EA/G outperforms the heuristic genetic algorithm of Marchiori (the best evolutionary algorithm reported so far) and a MIMIC algorithm on DIMACS benchmark graphs.",
EECS: an energy efficient clustering scheme in wireless sensor networks,"Data gathering is a common but critical operation in many applications of wireless sensor networks. Innovative techniques that improve energy efficiency to prolong the network lifetime are highly required. Clustering is an effective topology control approach in wireless sensor networks, which can increase network scalability and lifetime. In this paper, we propose a novel clustering schema EECS for wireless sensor networks, which better suits the periodical data gathering applications. Our approach elects cluster heads with more residual energy through local radio communication while achieving well cluster head distribution; further more it introduces a novel method to balance the load among the cluster heads. Simulation results show that EECS outperforms LEACH significantly with prolonging the network lifetime over 35%.",
Detecting irregularities in images and in video,"We address the problem of detecting irregularities in visual data, e.g., detecting suspicious behaviors in video sequences, or identifying salient patterns in images. The term ""irregular"" depends on the context in which the ""regular"" or ""valid"" are defined. Yet, it is not realistic to expect explicit definition of all possible valid configurations for a given context. We pose the problem of determining the validity of visual data as a process of constructing a puzzle: We try to compose a new observed image region or a new video segment (""the query"") using chunks of data (""pieces of puzzle"") extracted from previous visual examples (""the database ""). Regions in the observed data which can be composed using large contiguous chunks of data from the database are considered very likely, whereas regions in the observed data which cannot be composed from the database (or can be composed, but only using small fragmented pieces) are regarded as unlikely/suspicious. The problem is posed as an inference process in a probabilistic graphical model. We show applications of this approach to identifying saliency in images and video, and for suspicious behavior recognition.","Image databases,
Visual databases,
Video sequences,
Data mining,
Object detection,
Statistical analysis,
Legged locomotion,
Computer science,
Image segmentation,
Graphical models"
Robust nonrigid registration to capture brain shift from intraoperative MRI,"We present a new algorithm to register 3-D preoperative magnetic resonance (MR) images to intraoperative MR images of the brain which have undergone brain shift. This algorithm relies on a robust estimation of the deformation from a sparse noisy set of measured displacements. We propose a new framework to compute the displacement field in an iterative process, allowing the solution to gradually move from an approximation formulation (minimizing the sum of a regularization term and a data error term) to an interpolation formulation (least square minimization of the data error term). An outlier rejection step is introduced in this gradual registration process using a weighted least trimmed squares approach, aiming at improving the robustness of the algorithm. We use a patient-specific model discretized with the finite element method in order to ensure a realistic mechanical behavior of the brain tissue. To meet the clinical time constraint, we parallelized the slowest step of the algorithm so that we can perform a full 3-D image registration in 35 s (including the image update time) on a heterogeneous cluster of 15 personal computers. The algorithm has been tested on six cases of brain tumor resection, presenting a brain shift of up to 14 mm. The results show a good ability to recover large displacements, and a limited decrease of accuracy near the tumor resection cavity.","Robustness,
Magnetic resonance imaging,
Least squares approximation,
Clustering algorithms,
Neoplasms,
Registers,
Magnetic resonance,
Iterative algorithms,
Magnetic noise,
Magnetic field measurement"
Sensor relocation in mobile sensor networks,"Recently there has been a great deal of research on using mobility in sensor networks to assist in the initial deployment of nodes. Mobile sensors are useful in this environment because they can move to locations that meet sensing coverage requirements. This paper explores the motion capability to relocate sensors to deal with sensor failure or respond to new events. We define the problem of sensor relocation and propose a two-phase sensor relocation solution: redundant sensors are first identified and then relocated to the target location. We propose a Grid-Quorum solution to quickly locate the closest redundant sensor with low message complexity, and propose to use cascaded movement to relocate the redundant sensor in a timely, efficient and balanced way. Simulation results verify that the proposed solution outperforms others in terms of relocation time, total energy consumption, and minimum remaining energy.","Intelligent networks,
Sensor phenomena and characterization,
Costs,
Intelligent sensors,
Energy consumption,
Delay,
Network topology,
Computer science,
Surveillance,
Smart homes"
MNP: Multihop Network Reprogramming Service for Sensor Networks,"Reprogramming of sensor networks is an important and challenging problem as it is often necessary to reprogram the sensors in place. In this paper, we propose a multihop reprogramming service designed for Mica-2/XSM motes. One of the problems in reprogramming is the issue of message collision. To reduce the problem of collision and hidden terminal problem, we propose a sender selection algorithm that attempts to guarantee that in a neighborhood there is at most one source transmitting the program at a time. Further, our sender selection is greedy in that it tries to select the sender that is expected to have the most impact. We also use pipelining to enable fast data propagation. MNP is energy efficient because it reduces the active radio time of a sensor node by putting the node into ""sleep"" state when its neighbors are transmitting a segment that is not of interest. Finally, we argue that it is possible to tune our service according to the remaining battery level of a sensor, i.e., it can be tuned so that the probability that a sensor is given the responsibility of transmitting the code is proportional to its remaining battery life",
User-level performance of channel-aware scheduling algorithms in wireless data networks,"Channel-aware scheduling strategies, such as the Proportional Fair algorithm for the CDMA 1xEV-DO system, provide an effective mechanism for improving throughput performance in wireless data networks by exploiting channel fluctuations. The performance of channel-aware scheduling algorithms has mostly been explored at the packet level for a static user population, often assuming infinite backlogs. In the present paper, we focus on the performance at the flow level in a dynamic setting with random finite-size service demands. We show that in certain cases the user-level performance may be evaluated by means of a multiclass Processor-Sharing model where the total service rate varies with the total number of users. The latter model provides explicit formulas for the distribution of the number of active users of the various classes, the mean response times, the blocking probabilities, and the throughput. In addition we show that, in the presence of channel variations, greedy, myopic strategies which maximize throughput in a static scenario, may result in sub-optimal throughput performance for a dynamic user configuration and cause potential instability effects.","Scheduling algorithm,
Intelligent networks,
Throughput,
Delay,
Processor scheduling,
Multiaccess communication,
Telecommunication traffic,
Mathematics,
Computer science,
Fluctuations"
ICDAR 2005 text locating competition results,"This paper describes the results of the ICDAR 2005 competition for locating text in camera captured scenes. For this we used the same data as the ICDAR 2003 competition, which has been kept private until now. This allows a direct comparison with the 2003 entries. The main result is that the leading 2005 entry has improved significantly on the leading 2003 entry, with an increase in average f-score from 0.5 to 0.62, where the f-score is the same adapted information retrieval measure used for the 2003 competition. The paper also discusses the Web-based deployment and evaluation of text locating systems, and one of the leading entries has now been deployed in this way. This mode of usage could lead to more complete and more immediate knowledge of the strengths and weaknesses of each newly developed system.","Testing,
Layout,
Robustness,
Character recognition,
Computer science,
Information retrieval,
Pattern recognition,
Digital cameras,
Roads,
Books"
Tutorial on agent-based modeling and simulation,"Agent-based modeling and simulation (ABMS) is a new approach to modeling systems comprised of autonomous, interacting agents. ABMS promises to have far reaching effects on the way that businesses use computers to support decision making and researchers use electronic laboratories to support their research. Some have gone so far as to contend that ABMS is a third way of doing science besides deductive and inductive reasoning. Computational advances have made possible a growing number of agent-based applications in a variety of fields. Applications range from modeling agent behavior in the stock market and supply chains, to predicting the spread of epidemics and the threat of biowarfare, from modeling consumer behavior to understanding the fall of ancient civilizations, to name a few. This tutorial describes the theoretical and practical foundations of ABMS, identifies toolkits and methods for developing ABMS models, and provides some thoughts on the relationship between ABMS and traditional modeling techniques.",
Unbounded transactional memory,"Hardware transactional memory should support unbounded transactions: transactions of arbitrary size and duration. We describe a hardware implementation of unbounded transactional memory, called UTM, which exploits the common case for performance without sacrificing correctness on transactions whose footprint can be nearly as large as virtual memory. We performed a cycle-accurate simulation of a simplified architecture, called LTM. LTM is based on UTM but is easier to implement, because it does not change the memory subsystem outside of the processor. LTM allows nearly unbounded transactions, whose footprint is limited only by physical memory size and whose duration by the length of a timeslice. We assess UTM and LTM through microbenchmarking and by automatically converting the SPECjvm98 Java benchmarks and the Linux 2.4.19 kernel to use transactions instead of locks. We use both cycle-accurate simulation and instrumentation to understand benchmark behavior. Our studies show that the common case is small transactions that commit, even when contention is high, but that some applications contain very large transactions. For example, although 99.9% of transactions in the Linux study touch 54 cache lines or fewer, some transactions touch over 8000 cache lines. Our studies also indicate that hardware support is required, because some applications spend over half their time in critical regions. Finally, they suggest that hardware support for transactions can make Java programs run faster than when run using locks and can increase the concurrency of the Linux kernel by as much as a factor of 4 with no additional programming work.",
Mobile-assisted localization in wireless sensor networks,"The localization problem is to determine an assignment of coordinates to nodes in a wireless ad-hoc or sensor network that is consistent with measured pairwise node distances. Most previously proposed solutions to this problem assume that the nodes can obtain pairwise distances to other nearby nodes using some ranging technology. However, for a variety of reasons that include obstructions and lack of reliable omnidirectional ranging, this distance information is hard to obtain in practice. Even when pairwise distances between nearby nodes are known, there may not be enough information to solve the problem uniquely. This paper describes MAL, a mobile-assisted localization method which employs a mobile user to assist in measuring distances between node pairs until these distance constraints form a ""globally rigid'* structure that guarantees a unique localization. We derive the required constraints on the mobile's movement and the minimum number of measurements it must collect; these constraints depend on the number of nodes visible to the mobile in a given region. We show how to guide the mobile's movement to gather a sufficient number of distance samples for node localization. We use simulations and measurements from an indoor deployment using the Cricket location system to investigate the performance of MAL, finding in real-world experiments that MAL's median pairwise distance error is less than 1.5% of the true node distance.","Intelligent networks,
Wireless sensor networks,
Coordinate measuring machines,
Reflection,
Computer science,
Artificial intelligence,
Laboratories,
Intelligent sensors,
Distance measurement,
Rotation measurement"
STACS: new active contour scheme for cardiac MR image segmentation,"The paper presents a novel stochastic active contour scheme (STACS) for automatic image segmentation designed to overcome some of the unique challenges in cardiac MR images such as problems with low contrast, papillary muscles, and turbulent blood flow. STACS minimizes an energy functional that combines stochastic region-based and edge-based information with shape priors of the heart and local properties of the contour. The minimization algorithm solves, by the level set method, the Euler-Lagrange equation that describes the contour evolution. STACS includes an annealing schedule that balances dynamically the weight of the different terms in the energy functional. Three particularly attractive features of STACS are: 1) ability to segment images with low texture contrast by modeling stochastically the image textures; 2) robustness to initial contour and noise because of the utilization of both edge and region-based information; 3)ability to segment the heart from the chest wall and the undesired papillary muscles due to inclusion of heart shape priors. Application of STACS to a set of 48 real cardiac MR images shows that it can successfully segment the heart from its surroundings such as the chest wall and the heart structures (the left and right ventricles and the epicardium.) We compare STACS' automatically generated contours with manually-traced contours, or the ""gold standard,"" using both area and edge similarity measures. This assessment demonstrates very good and consistent segmentation performance of STACS.",
Cone-beam reconstruction using the backprojection of locally filtered projections,"This paper describes a flexible new methodology for accurate cone beam reconstruction with source positions on a curve (or set of curves). The inversion formulas employed by this methodology are based on first backprojecting a simple derivative in the projection space and then applying a Hilbert transform inversion in the image space. The local nature of the projection space filtering distinguishes this approach from conventional filtered-backprojection methods. This characteristic together with a degree of flexibility in choosing the direction of the Hilbert transform used for inversion offers two important features for the design of data acquisition geometries and reconstruction algorithms. First, the size of the detector necessary to acquire sufficient data for accurate reconstruction of a given region is often smaller than that required by previously documented approaches. In other words, more data truncation is allowed. Second, redundant data can be incorporated for the purpose of noise reduction. The validity of the inversion formulas along with the application of these two properties are illustrated with reconstructions from computer simulated data. In particular, in the helical cone beam geometry, it is shown that 1) intermittent transaxial truncation has no effect on the reconstruction in a central region which means that wider patients can be accommodated on existing scanners, and more importantly that radiation exposure can be reduced for region of interest imaging and 2) at maximum pitch the data outside the Tam-Danielsson window can be used to reduce image noise and thereby improve dose utilization. Furthermore, the degree of axial truncation tolerated by our approach for saddle trajectories is shown to be larger than that of previous methods.",
Is mutation an appropriate tool for testing experiments? [software testing],"The empirical assessment of test techniques plays an important role in software testing research. One common practice is to instrument faults, either manually or by using mutation operators. The latter allows the systematic, repeatable seeding of large numbers of faults; however, we do not know whether empirical results obtained this way lead to valid, representative conclusions. This paper investigates this important question based on a number of programs with comprehensive pools of test cases and known faults. It is concluded that, based on the data available thus far, the use of mutation operators is yielding trustworthy results (generated mutants are similar to real faults). Mutants appear however to be different from hand-seeded faults that seem to be harder to detect than real faults.","Genetic mutations,
Fault detection,
Software testing,
Performance evaluation,
Permission,
Design engineering,
Systems engineering and theory,
Computer science,
Instruments,
Debugging"
Simultaneous single event charge sharing and parasitic bipolar conduction in a highly-scaled SRAM design,"A novel mechanism for upset is seen in a commercially available 0.25 /spl mu/m 10-T SEE hardened SRAM cell. Unlike traditional multiple node charge collection in which diffusions near a single event strike collect the deposited carriers, this new mechanism involves direct drift-diffusion collection at an NFET transistor in conjunction with parasitic bipolar conduction in nearby PFET transistors. The charge collection with the parasitic bipolar conduction compromise the SEE hardened design, thus causing upsets. The mechanism was identified using laser testing and three-dimensional TCAD simulations.","Random access memory,
Circuit simulation,
Single event upset,
Computational modeling,
CMOS technology,
Circuit testing,
State feedback,
Laboratories,
SPICE,
Computer simulation"
Perspectives of granular computing,"Granular computing emerges as a new multi-disciplinary study and has received much attention in recent years. A conceptual framework is presented by extracting shared commonalities from many fields. The framework stresses multiple views and multiple levels of understanding in each view. It is argued that granular computing is more about a philosophical way of thinking and a practical methodology of problem solving. By effectively using levels of granularity, granular computing provides a systematic, natural way to analyze, understand, represent, and solve real world problems. With granular computing, one aims at structured thinking at the philosophical level, and structured problem solving at the practical level.",
Neighborhood preserving embedding,"Recently there has been a lot of interest in geometrically motivated approaches to data analysis in high dimensional spaces. We consider the case where data is drawn from sampling a probability distribution that has support on or near a submanifold of Euclidean space. In this paper, we propose a novel subspace learning algorithm called neighborhood preserving embedding (NPE). Different from principal component analysis (PCA) which aims at preserving the global Euclidean structure, NPE aims at preserving the local neighborhood structure on the data manifold. Therefore, NPE is less sensitive to outliers than PCA. Also, comparing to the recently proposed manifold learning algorithms such as Isomap and locally linear embedding, NPE is defined everywhere, rather than only on the training data points. Furthermore, NPE may be conducted in the original space or in the reproducing kernel Hilbert space into which data points are mapped. This gives rise to kernel NPE. Several experiments on face database demonstrate the effectiveness of our algorithm",
Stereo correspondence by dynamic programming on a tree,"Dynamic programming on a scanline is one of the oldest and still popular methods for stereo correspondence. While efficient, its performance is far from the state of the art because the vertical consistency between the scanlines is not enforced. We re-examine the use of dynamic programming for stereo correspondence by applying it to a tree structure, as opposed to the individual scanlines. The nodes of this tree are all the image pixels, but only the ""most important"" edges of the 4 connected neighbourhood system are included. Thus our algorithm is truly a global optimization method because disparity estimate at one pixel depends on the disparity estimates at all the other pixels, unlike the scanline based methods. We evaluate our algorithm on the benchmark Middlebury database. The algorithm is very fast; it takes only a fraction of a second for a typical image. The results are considerably better than that of the scanline based methods. While the results are not the state of the art, our algorithm offers a good trade off in terms of accuracy and computational efficiency.",
Time-delay- and time-reversal-based robust capon beamformers for ultrasound imaging,"Currently, the nonadaptive delay-and-sum (DAS) beamformer is extensively used for ultrasound imaging, despite the fact that it has lower resolution and worse interference suppression capability than the adaptive standard Capon beamformer (SCB) if the steering vector corresponding to the signal of interest (SOI) is accurately known. The main problem which restricts the use of SCB, however, is that SCB lacks robustness against steering vector errors that are inevitable in practice. Whenever this happens, the performance of SCB may become worse than that of DAS. Therefore, a robust adaptive beamformer is desirable to maintain the robustness of DAS and adaptivity of SCB. In this paper we consider a recent promising robust Capon beamformer (RCB) for ultrasound imaging. We propose two ways of implementing RCB, one based on time delay and the other based on time reversal. RCB extends SCB by allowing the array steering vector to be within an uncertainty set. Hence, it restores the appeal of SCB including its high resolution and superb interference suppression capabilities, and also retains the attractiveness of DAS including its robustness against steering vector errors. The time-delay-based RCB can tolerate the misalignment of data samples and the time-reversal-based RCB can withstand the uncertainty of the Green's function. Both time-delay-based RCB and time-reversal-based RCB can be efficiently computed at a comparable cost to SCB. The excellent performances of the proposed robust adaptive beamforming approaches are demonstrated via a number of simulated and experimental examples.","Ultrasonic imaging,
Robustness,
Interference suppression,
Uncertainty,
Image resolution,
Signal resolution,
High-resolution imaging,
Delay effects,
Green's function methods,
Costs"
Use of relative code churn measures to predict system defect density,"Software systems evolve over time due to changes in requirements, optimization of code, fixes for security and reliability bugs etc. Code churn, which measures the changes made to a component over a period of time, quantifies the extent of this change. We present a technique for early prediction of system defect density using a set of relative code churn measures that relate the amount of churn to other variables such as component size and the temporal extent of churn. Using statistical regression models, we show that while absolute measures of code chum are poor predictors of defect density, our set of relative measures of code churn is highly predictive of defect density. A case study performed on Windows Server 2003 indicates the validity of the relative code churn measures as early indicators of system defect density. Furthermore, our code churn metric suite is able to discriminate between fault and not fault-prone binaries with an accuracy of 89.0 percent.","Density measurement,
Time measurement,
Software systems,
Software measurement,
Software quality,
Software engineering,
History,
Automatic control,
Control systems,
Computer science"
Toward automated segmentation of the pathological lung in CT,"Conventional methods of lung segmentation rely on a large gray value contrast between lung fields and surrounding tissues. These methods fail on scans with lungs that contain dense pathologies, and such scans occur frequently in clinical practice. We propose a segmentation-by-registration scheme in which a scan with normal lungs is elastically registered to a scan containing pathology. When the resulting transformation is applied to a mask of the normal lungs, a segmentation is found for the pathological lungs. As a mask of the normal lungs, a probabilistic segmentation built up out of the segmentations of 15 registered normal scans is used. To refine the segmentation, voxel classification is applied to a certain volume around the borders of the transformed probabilistic mask. Performance of this scheme is compared to that of three other algorithms: a conventional, a user-interactive and a voxel classification method. The algorithms are tested on 10 three-dimensional thin-slice computed tomography volumes containing high-density pathology. The resulting segmentations are evaluated by comparing them to manual segmentations in terms of volumetric overlap and border positioning measures. The conventional and user-interactive methods that start off with thresholding techniques fail to segment the pathologies and are outperformed by both voxel classification and the refined segmentation-by-registration. The refined registration scheme enjoys the additional benefit that it does not require pathological (hand-segmented) training data.","Pathology,
Lungs,
Computed tomography,
Image segmentation,
Coronary arteriosclerosis,
Cancer detection,
Diseases,
Biomedical imaging,
Medical diagnostic imaging,
Design automation"
Vessel tree reconstruction in thoracic CT scans with application to nodule detection,"Vessel tree reconstruction in volumetric data is a necessary prerequisite in various medical imaging applications. Specifically, when considering the application of automated lung nodule detection in thoracic computed tomography (CT) scans, vessel trees can be used to resolve local ambiguities based on global considerations and so improve the performance of nodule detection algorithms. In this study, a novel approach to vessel tree reconstruction and its application to nodule detection in thoracic CT scans was developed by using correlation-based enhancement filters and a fuzzy shape representation of the data. The proposed correlation-based enhancement filters depend on first-order partial derivatives and so are less sensitive to noise compared with Hessian-based filters. Additionally, multiple sets of eigenvalues are used so that a distinction between nodules and vessel junctions becomes possible. The proposed fuzzy shape representation is based on regulated morphological operations that are less sensitive to noise. Consequently, the vessel tree reconstruction algorithm can accommodate vessel bifurcation and discontinuities. A quantitative performance evaluation of the enhancement filters and of the vessel tree reconstruction algorithm was performed. Moreover, the proposed vessel tree reconstruction algorithm reduced the number of false positives generated by an existing nodule detection algorithm by 38%.","Computed tomography,
Filters,
Image reconstruction,
Reconstruction algorithms,
Detection algorithms,
Shape,
Multi-stage noise shaping,
Biomedical imaging,
Lungs,
Eigenvalues and eigenfunctions"
Toffoli network synthesis with templates,"Reversible logic functions can be realized as networks of Toffoli gates. The synthesis of Toffoli networks can be divided into two steps. First, find a network that realizes the desired function. Second, transform the network such that it uses fewer gates, while realizing the same function. This paper addresses the above synthesis approach. We present a basic method and, based on that, a bidirectional synthesis algorithm which produces a network of Toffoli gates realizing a given reversible specification. An asymptotically optimal modification of the basic synthesis algorithm employing generalized mEXOR gates is also presented. Transformations are then applied using template matching. The basis for a template is a network of gates that realizes the identity function. If a sequence of gates in the synthesized network matches a sequence comprised of more than half the gates in a template, then a transformation using the remaining gates in the template can be applied resulting in a reduction in the gate count for the synthesized network. All templates with up to six gates are described in this paper. Experimental results including an exhaustive examination of all 3-variable reversible functions and a collection of benchmark problems are presented. The paper concludes with suggestions for further research.","Network synthesis,
Quantum computing,
CMOS logic circuits,
Optical computing,
Computer science,
Logic functions,
Computer applications,
Nanotechnology,
Optical feedback,
Hamming distance"
List-coloring based channel allocation for open-spectrum wireless networks,,"Channel allocation,
Wireless networks,
Availability,
Space technology,
Telecommunication traffic,
Frequency,
Time-varying channels,
White spaces,
FCC,
Computer science"
Speaker verification using sequence discriminant support vector machines,"This paper presents a text-independent speaker verification system using support vector machines (SVMs) with score-space kernels. Score-space kernels generalize Fisher kernels and are based on underlying generative models such as Gaussian mixture models (GMMs). This approach provides direct discrimination between whole sequences, in contrast with the frame-level approaches at the heart of most current systems. The resultant SVMs have a very high dimensionality since it is related to the number of parameters in the underlying generative model. To address problems that arise in the resultant optimization we introduce a technique called spherical normalization that preconditions the Hessian matrix. We have performed speaker verification experiments using the PolyVar database. The SVM system presented here reduces the relative error rates by 34% compared to a GMM likelihood ratio system.","Support vector machines,
Kernel,
Hidden Markov models,
Support vector machine classification,
Databases,
Error analysis,
Computer science,
Speaker recognition,
Heart,
Speech"
Robust quantification of in vitro angiogenesis through image analysis,"An automated image analysis method for quantification of in vitro angiogenesis is presented. The method is designed for in vitro angiogenesis assays that are based on co-culturing endothelial cells with fibroblasts. Such assays are used in many current studies in which anti-angiogenic agents for the treatment of cancer are being sought. This search requires accurate quantification of the stimulatory and inhibitory effects of the different agents. The quantification method gives lengths and sizes of the tubule complexes as well as the numbers of junctions in each of them. The method is tested with a set of test images obtained with a commercially available in vitro angiogenesis assay. The results correctly indicate the inhibitory effect of suramin and the stimulatory effect of vascular endothelial growth factor. Moreover, the image analysis method is shown to be robust against variations in illumination. We have implemented a software package that utilizes the methods. The software as well as a set of test images are available at http://www.cs.tut.fi/sgn/csb/angioquant/.","Robustness,
In vitro,
Image analysis,
Cancer,
Testing,
Fibroblasts,
Length measurement,
In vivo,
Signal processing,
Pathology"
Anti-collusion forensics of multimedia fingerprinting using orthogonal modulation,"Digital fingerprinting is a method for protecting digital data in which fingerprints that are embedded in multimedia are capable of identifying unauthorized use of digital content. A powerful attack that can be employed to reduce this tracing capability is collusion, where several users combine their copies of the same content to attenuate/remove the original fingerprints. In this paper, we study the collusion resistance of a fingerprinting system employing Gaussian distributed fingerprints and orthogonal modulation. We introduce the maximum detector and the thresholding detector for colluder identification. We then analyze the collusion resistance of a system to the averaging collusion attack for the performance criteria represented by the probability of a false negative and the probability of a false positive. Lower and upper bounds for the maximum number of colluders K/sub max/ are derived. We then show that the detectors are robust to different collusion attacks. We further study different sets of performance criteria, and our results indicate that attacks based on a few dozen independent copies can confound such a fingerprinting system. We also propose a likelihood-based approach to estimate the number of colluders. Finally, we demonstrate the performance for detecting colluders through experiments using real images.",
Defending against distributed denial-of-service attacks with max-min fair server-centric router throttles,"Our work targets a network architecture and accompanying algorithms for countering distributed denial-of-service (DDoS) attacks directed at an Internet server. The basic mechanism is for a server under stress to install a router throttle at selected upstream routers. The throttle can be the leaky-bucket rate at which a router can forward packets destined for the server. Hence, before aggressive packets can converge to overwhelm the server, participating routers proactively regulate the contributing packet rates to more moderate levels, thus forestalling an impending attack. In allocating the server capacity among the routers, we propose a notion of level-k max-min fairness. We first present a control-theoretic model to evaluate algorithm convergence under a variety of system parameters. In addition, we present packet network simulation results using a realistic global network topology, and various models of good user and attacker distributions and behavior. Using a generator model of web requests parameterized by empirical data, we also evaluate the impact of throttling in protecting user access to a web server. First, for aggressive attackers, the throttle mechanism is highly effective in preferentially dropping attacker traffic over good user traffic. In particular, level-k max-min fairness gives better good-user protection than recursive pushback of max-min fair rate limits proposed in the literature. Second, throttling can regulate the experienced server load to below its design limit - in the presence of user dynamics - so that the server can remain operational during a DDoS attack. Lastly, we present implementation results of our prototype on a Pentium III/866 MHz machine. The results show that router throttling has low deployment overhead in time and memory.","Computer crime,
Network servers,
Web server,
Protection,
Stress,
Traffic control,
Computer science,
Telecommunication traffic,
IP networks,
Network topology"
A multidimensional scaling framework for mobile location using time-of-arrival measurements,"Localization of mobile phones is now a very popular research topic. A simple algorithm is devised for mobile location estimation using time-of-arrival measurements of the signal from the mobile station received at three or more base stations, via modifying the classical multidimensional scaling technique, which has been developed for analyzing data obtained from physical, biological, and behavioral science. The bias and variance of the proposed algorithm are also derived. Computer simulations are included to corroborate the theoretical development and to contrast the estimator performance with several conventional approaches as well as the Crame/spl acute/r-Rao lower bound.","Multidimensional systems,
Time measurement,
Synchronization,
Position measurement,
Base stations,
FCC,
Global Positioning System,
Signal processing,
Mobile handsets,
Data analysis"
A scalable collaborative filtering framework based on co-clustering,"Collaborative filtering-based recommender systems have become extremely popular due to the increase in Web-based activities such as e-commerce and online content distribution. Current collaborative filtering (CF) techniques such as correlation and SVD based methods provide good accuracy, but are computationally expensive and can be deployed only in static off-line settings. However, a number of practical scenarios require dynamic real-time collaborative filtering that can allow new users, items and ratings to enter the system at a rapid rate. In this paper, we consider a novel CF approach based on a proposed weighted co-clustering algorithm (Banerjee et al., 2004) that involves simultaneous clustering of users and items. We design incremental and parallel versions of the co-clustering algorithm and use it to build an efficient real-time CF framework. Empirical evaluation demonstrates that our approach provides an accuracy comparable to that of the correlation and matrix factorization based approaches at a much lower computational cost.","Recommender systems,
Clustering algorithms,
Real time systems,
Computer science,
Online Communities/Technical Collaboration,
Information filtering,
Information filters,
Algorithm design and analysis,
Computational efficiency,
Internet"
Global A-Optimal Robot Exploration in SLAM,"It is well-known that the Kalman filter for simultaneous localization and mapping (SLAM) converges to a fully correlated map in the limit of infinite time and data [1]. However, the rate of convergence of the map has a strong dependence on the order of the observations. We show that conventional exploration algorithms for collecting map data are sub-optimal in both the objective function and choice of optimization procedure. We show that optimizing the a-optimal information measure results in a more accurate map than existing approaches, using a greedy, closed-loop strategy. Secondly, we demonstrate that by restricting the planning to an appropriate policy class, we can tractably find non-greedy, global planning trajectories that produce more accurate maps, explicitly planning to close loops even in open-loop scenarios.","Simultaneous localization and mapping,
Robot sensing systems,
Sensor phenomena and characterization,
Orbital robotics,
Trajectory,
Computer science,
Convergence,
Mobile robots,
Strategic planning,
Artificial intelligence"
Priors for people tracking from small training sets,"We advocate the use of scaled Gaussian process latent variable models (SGPLVM) to learn prior models of 3D human pose for 3D people tracking. The SGPLVM simultaneously optimizes a low-dimensional embedding of the high-dimensional pose data and a density function that both gives higher probability to points close to training data and provides a nonlinear probabilistic mapping from the low-dimensional latent space to the full-dimensional pose space. The SGPLVM is a natural choice when only small amounts of training data are available. We demonstrate our approach with two distinct motions, golfing and walking. We show that the SGPLVM sufficiently constrains the problem such that tracking can be accomplished with straightforward deterministic optimization.","Humans,
Training data,
Biological system modeling,
Density functional theory,
Computer science,
Gaussian processes,
Legged locomotion,
Simultaneous localization and mapping,
Constraint optimization,
Clothing"
Victim replication: maximizing capacity while hiding wire delay in tiled chip multiprocessors,"In this paper, we consider tiled chip multiprocessors (CMP) where each tile contains a slice of the total on-chip L2 cache storage and tiles are connected by an on-chip network. The L2 slices can be managed using two basic schemes: 1) each slice is treated as a private L2 cache for the tile; 2) all slices are treated as a single large L2 cache shared by all tiles. Private L2 caches provide the lowest hit latency but reduce the total effective cache capacity, as each tile creates local copies of any line it touches. A shared L2 cache increases the effective cache capacity for shared data, but incurs long hit latencies when L2 data is on a remote tile. We present a new cache management policy, victim replication, which combines the advantages of private and shared schemes. Victim replication is a variant of the shared scheme which attempts to keep copies of local primary cache victims within the local L2 cache slice. Hits to these replicated copies reduce the effective latency of the shared L2 cache, while retaining the benefits of a higher effective capacity for shared data. We evaluate the various schemes using full-system simulation of both single-threaded and multi-threaded benchmarks running on an 8-processor tiled CMP. We show that victim replication reduces the average memory access latency of the shared L2 cache by an average of 16% for multi-threaded benchmarks and 24% for single-threaded benchmarks, providing better overall performance than either private or shared schemes.","Wire,
Delay,
Tiles,
Cache storage,
Yarn,
Computer science,
Artificial intelligence,
Laboratories,
Throughput,
Clocks"
Wireless packet scheduling based on the cumulative distribution function of user transmission rates,"In this paper, we present a new wireless scheduling algorithm based on the cumulative distribution function (cdf) and its simple modification that limits the maximum starving time. This cdf-based scheduling (CS) algorithm selects the user for transmission based on the cdf of user rates, in such a way that the user whose rate is high enough, but least probable to become higher, is selected first. We prove that the CS algorithm is equivalent to a scheduling algorithm that regards the user rates as independent and identically distributed, and the average throughput of a user is independent of the probability distribution of other users. So, we can evaluate the exact user throughput only if we know the user's own distribution, which is a distinctive feature of this proposed algorithm. In addition, we try a modification on the CS algorithm to limit the maximum starving time, and prove that the modification does not affect the average interservice time. This CS with starving-time limitation (CS-STL) algorithm turns out to limit the maximum starving time at the cost of a negligible throughput loss.","Scheduling algorithm,
Distribution functions,
Throughput,
Computer science,
Probability distribution,
Costs,
Telecommunication traffic,
Delay,
Data communication"
"What is a robot companion - friend, assistant or butler?","The study presented in this paper explored people's perceptions and attitudes towards the idea of a future robot companion for the home. A human-centred approach was adopted using questionnaires and human-robot interaction trials to derive data from 28 adults. Results indicated that a large proportion of participants were in favour of a robot companion and saw the potential role as being an assistant, machine or servant. Few wanted a robot companion to be a friend. Household tasks were preferred to child/animal care tasks. Humanlike communication was desirable for a robot companion, whereas humanlike behaviour and appearance were less essential. Results are discussed in relation to future research directions for the development of robot companions.",
Automated detection of prostatic adenocarcinoma from high-resolution ex vivo MRI,"Prostatic adenocarcinoma is the most commonly occurring cancer among men in the United States, second only to skin cancer. Currently, the only definitive method to ascertain the presence of prostatic cancer is by trans-rectal ultrasound (TRUS) directed biopsy. Owing to the poor image quality of ultrasound, the accuracy of TRUS is only 20%-25%. High-resolution magnetic resonance imaging (MRI) has been shown to have a higher accuracy of prostate cancer detection compared to ultrasound. Consequently, several researchers have been exploring the use of high resolution MRI in performing prostate biopsies. Visual detection of prostate cancer, however, continues to be difficult owing to its apparent lack of shape, and the fact that several malignant and benign structures have overlapping intensity and texture characteristics. In this paper, we present a fully automated computer-aided detection (CAD) system for detecting prostatic adenocarcinoma from 4 Tesla ex vivo magnetic resonance (MR) imagery of the prostate. After the acquired MR images have been corrected for background inhomogeneity and nonstandardness, novel three-dimensional (3-D) texture features are extracted from the 3-D MRI scene. A Bayesian classifier then assigns each image voxel a ""likelihood"" of malignancy for each feature independently. The ""likelihood"" images generated in this fashion are then combined using an optimally weighted feature combination scheme. Quantitative evaluation was performed by comparing the CAD results with the manually ascertained ground truth for the tumor on the MRI. The tumor labels on the MR slices were determined manually by an expert by visually registering the MR slices with the corresponding regions on the histology slices. We evaluated our CAD system on a total of 33 two-dimensional (2-D) MR slices from five different 3-D MR prostate studies. Five slices from two different glands were used for training. Our feature combination scheme was found to outperform the individual texture features, and also other popularly used feature combination methods, including AdaBoost, ensemble averaging, and majority voting. Further, in several instances our CAD system performed better than the experts in terms of accuracy, the expert segmentations being determined solely from visual inspection of the MRI data. In addition, the intrasystem variability (changes in CAD accuracy with changes in values of system parameters) was significantly lower than the corresponding intraobserver and interobserver variability. CAD performance was found to be very similar for different training sets. Future work will focus on extending the methodology to guide high-resolution MRI-assisted in vivo prostate biopsies.","Magnetic resonance imaging,
Ultrasonic imaging,
Biopsy,
Prostate cancer,
Cancer detection,
Neoplasms,
Skin cancer,
Image quality,
Shape,
Magnetic resonance"
Forensic analysis of nonlinear collusion attacks for multimedia fingerprinting,"Digital fingerprinting is a technology for tracing the distribution of multimedia content and protecting them from unauthorized redistribution. Unique identification information is embedded into each distributed copy of multimedia signal and serves as a digital fingerprint. Collusion attack is a cost-effective attack against digital fingerprinting, where colluders combine several copies with the same content but different fingerprints to remove or attenuate the original fingerprints. In this paper, we investigate the average collusion attack and several basic nonlinear collusions on independent Gaussian fingerprints, and study their effectiveness and the impact on the perceptual quality. With unbounded Gaussian fingerprints, perceivable distortion may exist in the fingerprinted copies as well as the copies after the collusion attacks. In order to remove this perceptual distortion, we introduce bounded Gaussian-like fingerprints and study their performance under collusion attacks. We also study several commonly used detection statistics and analyze their performance under collusion attacks. We further propose a preprocessing technique of the extracted fingerprints specifically for collusion scenarios to improve the detection performance.","Forensics,
Fingerprint recognition,
Protection,
Data security,
Nonlinear distortion,
Spread spectrum communication,
Watermarking,
Signal processing,
Gaussian processes,
Statistical analysis"
"The Semantic Grid: Past, Present, and Future","Grid computing offers significant enhancements to our capabilities for computation, information processing, and collaboration, and has exciting ambitions in many fields of endeavor. We argue that the full richness of the Grid vision, with its application in e-Science, e-Research, or e-Business, requires the ""Semantic Grid."" The Semantic Grid is an extension of the current Grid in which information and services are given well-defined meaning, better enabling computers and people to work in cooperation. To this end, we outline the requirements of the Semantic Grid, discuss the state of the art in achieving them, and identify the key research challenges in realizing this vision.","Grid computing,
Computer vision,
Semantic Web,
Collaboration,
Application software,
Software agents,
Contracts,
Problem-solving,
Collaborative software,
Information processing"
Wearable Sensors for Reliable Fall Detection,"Unintentional falls are a common cause of severe injury in the elderly population. By introducing small, non-invasive sensor motes in conjunction with a wireless network, the Ivy Project aims to provide a path towards more independent living for the elderly. Using a small device worn on the waist and a network of fixed motes in the home environment, we can detect the occurrence of a fall and the location of the victim. Low-cost and low-power MEMS accelerometers are used to detect the fall while RF signal strength is used to locate the person",
An insight-based methodology for evaluating bioinformatics visualizations,"High-throughput experiments, such as gene expression microarrays in the life sciences, result in very large data sets. In response, a wide variety of visualization tools have been created to facilitate data analysis. A primary purpose of these tools is to provide biologically relevant insight into the data. Typically, visualizations are evaluated in controlled studies that measure user performance on predetermined tasks or using heuristics and expert reviews. To evaluate and rank bioinformatics visualizations based on real-world data analysis scenarios, we developed a more relevant evaluation method that focuses on data insight. This paper presents several characteristics of insight that enabled us to recognize and quantify it in open-ended user tests. Using these characteristics, we evaluated five microarray visualization tools on the amount and types of insight they provide and the time it takes to acquire it. The results of the study guide biologists in selecting a visualization tool based on the type of their microarray data, visualization designers on the key role of user interaction techniques, and evaluators on a new approach for evaluating the effectiveness of visualizations for providing insight. Though we used the method to analyze bioinformatics visualizations, it can be applied to other domains.","Bioinformatics,
Data visualization,
Data analysis,
Graphical user interfaces,
Gene expression,
Character recognition,
Software systems,
System testing,
Proteins,
Biology computing"
Local discriminant embedding and its variants,"We present a new approach, called local discriminant embedding (LDE), to manifold learning and pattern classification. In our framework, the neighbor and class relations of data are used to construct the embedding for classification problems. The proposed algorithm learns the embedding for the submanifold of each class by solving an optimization problem. After being embedded into a low-dimensional subspace, data points of the same class maintain their intrinsic neighbor relations, whereas neighboring points of different classes no longer stick to one another. Via embedding, new test data are thus more reliably classified by the nearest neighbor rule, owing to the locally discriminating nature. We also describe two useful variants: two-dimensional LDE and kernel LDE. Comprehensive comparisons and extensive experiments on face recognition are included to demonstrate the effectiveness of our method.",
STCP: a generic transport layer protocol for wireless sensor networks,"We consider the issue of designing a generic transport layer protocol for energy-constrained sensor networks. We present the requirements for such a transport protocol and propose sensor transmission control protocol (STCP). STCP is a generic, scalable and reliable transport layer protocol where a majority of the functionalities are implemented at the base station. STCP offers controlled variable reliability, congestion detection and avoidance, and supports multiple applications in the same network. We present the design and implementation of STCP and evaluate the protocol with different scenarios and network characteristics.","Transport protocols,
Wireless application protocol,
Wireless sensor networks,
Base stations,
Sensor phenomena and characterization,
Temperature sensors,
Computer networks,
Computer science,
Media Access Protocol,
Delay"
The smallest grammar problem,"This paper addresses the smallest grammar problem: What is the smallest context-free grammar that generates exactly one given string /spl sigma/? This is a natural question about a fundamental object connected to many fields such as data compression, Kolmogorov complexity, pattern identification, and addition chains. Due to the problem's inherent complexity, our objective is to find an approximation algorithm which finds a small grammar for the input string. We focus attention on the approximation ratio of the algorithm (and implicitly, the worst case behavior) to establish provable performance guarantees and to address shortcomings in the classical measure of redundancy in the literature. Our first results are concern the hardness of approximating the smallest grammar problem. Most notably, we show that every efficient algorithm for the smallest grammar problem has approximation ratio at least 8569/8568 unless P=NP. We then bound approximation ratios for several of the best known grammar-based compression algorithms, including LZ78, B ISECTION, SEQUENTIAL, LONGEST MATCH, GREEDY, and RE-PAIR. Among these, the best upper bound we show is O(n/sup 1/2/). We finish by presenting two novel algorithms with exponentially better ratios of O(log/sup 3/n) and O(log(n/m/sup */)), where m/sup */ is the size of the smallest grammar for that input. The latter algorithm highlights a connection between grammar-based compression and LZ77.","Approximation algorithms,
Data compression,
Computer science,
Compressors,
Pattern matching,
Compression algorithms,
Upper bound,
Source coding"
Delay efficient sleep scheduling in wireless sensor networks,"Medium access techniques for wireless sensor networks raise the important question of providing periodic energy-efficient radio sleep cycles while minimizing the end-to-end communication delays. This study aims to minimize the communication latency given that each sensor has a duty cycling requirement of being awake for only 1/k time slots on an average. As a first step we consider the single wake-up schedule case, where each sensor can choose exactly one of the k slots to wake up. We formulate a novel graph-theoretical abstraction of this problem in the general setting of a low-traffic wireless sensor network with arbitrary communication flows and prove that minimizing the end-to-end communication delays is in general NP-hard. However, we are able to derive and analyze optimal solutions for two special cases: tree topologies and ring topologies. Several heuristics for arbitrary topologies are proposed and evaluated by simulations. Our simulations suggest that distributed heuristics may perform poorly because of the global nature of the constraints involved. We also show that by carefully choosing multiple wake-up slots for each sensor significant delay savings can be obtained over the single wake-up schedule case while maintaining the same duty cycling. Using this technique, we propose algorithms that offer a desirable bound of d+O(k) on the delay for specialized topologies like the tree and grid and a weaker guarantee of O((d+k)log n) for arbitrary graphs, where d is the shortest path between 2 nodes in the underlying topology and n is the total number of nodes.","Sleep,
Intelligent networks,
Wireless sensor networks,
Delay,
Network topology,
Energy efficiency,
Telecommunication traffic,
Tree graphs,
Processor scheduling,
Computer science"
"Full-chip analysis of leakage power under process variations, including spatial correlations","In this paper, we present a method for analyzing the leakage current, and hence the leakage power, of a circuit under process parameter variations that can include spatial correlations due to intra-chip variation. A lognormal distribution is used to approximate the leakage current of each gate and the total chip leakage is determined by summing up the lognormals. In this work, both subthreshold leakage and gate tunneling leakage are considered. The proposed method is shown to be effective in predicting the CDF/PDF of the total chip leakage. The average errors for mean and sigma values are -1.3% and -4.1%.","Circuits,
Tunneling,
Subthreshold current,
Leakage current,
CMOS technology,
Permission,
Gate leakage,
Random variables,
Computer science,
Power engineering and energy"
Computer-aided diagnostic scheme for distinction between benign and malignant nodules in thoracic low-dose CT by use of massive training artificial neural network,"Low-dose helical computed tomography (LDCT) is being applied as a modality for lung cancer screening. It may be difficult, however, for radiologists to distinguish malignant from benign nodules in LDCT. Our purpose in this study was to develop a computer-aided diagnostic (CAD) scheme for distinction between benign and malignant nodules in LDCT scans by use of a massive training artificial neural network (MTANN). The MTANN is a trainable, highly nonlinear filter based on an artificial neural network. To distinguish malignant nodules from six different types of benign nodules, we developed multiple MTANNs (multi-MTANN) consisting of six expert MTANNs that are arranged in parallel. Each of the MTANNs was trained by use of input CT images and teaching images containing the estimate of the distribution for the ""likelihood of being a malignant nodule"", i.e., the teaching image for a malignant nodule contains a two-dimensional Gaussian distribution and that for a benign nodule contains zero. Each MTANN was trained independently with ten typical malignant nodules and ten benign nodules from each of the six types. The outputs of the six MTANNs were combined by use of an integration ANN such that the six types of benign nodules could be distinguished from malignant nodules. After training of the integration ANN, our scheme provided a value related to the ""likelihood of malignancy"" of a nodule, i.e., a higher value indicates a malignant nodule, and a lower value indicates a benign nodule. Our database consisted of 76 primary lung cancers in 73 patients and 413 benign nodules in 342 patients, which were obtained from a lung cancer screening program on 7847 screenees with LDCT for three years in Nagano, Japan. The performance of our scheme for distinction between benign and malignant nodules was evaluated by use of receiver operating characteristic (ROC) analysis. Our scheme achieved an Az (area under the ROC curve) value of 0.882 in a round-robin test. Our scheme correctly identified 100% (76/76) of malignant nodules as malignant, whereas 48% (200/413) of benign nodules were identified correctly as benign. Therefore, our scheme may be useful in assisting radiologists in the diagnosis of lung nodules in LDCT.","Computer networks,
Cancer,
Artificial neural networks,
Lungs,
Computed tomography,
Education,
Nonlinear filters,
Gaussian distribution,
Databases,
Performance analysis"
Just In Time Dynamic Voltage Scaling: Exploiting Inter-Node Slack to Save Energy in MPI Programs,"Recently, improving the energy efficiency of HPC machines has become important. As a result, interest in using powerscalable clusters, where frequency and voltage can be dynamically modified, has increased. On power-scalable clusters, one opportunity for saving energy with little or no loss of performance exists when the computational load is not perfectly balanced. This situation occurs frequently, as balancing load between nodes is one of the long standing problems in parallel and distributed computing. In this paper we present a system called Jitter, which reduces the frequency on nodes that are assigned less computation and therefore have slack time. This saves energy on these nodes, and the goal of Jitter is to attempt to ensure that they arrive ""just in time"" so that they avoid increasing overall execution time. For example, in Aztec, from the ASCI Purple suite, our algorithm uses 8% less energy while increasing execution time by only 2.6%.","Dynamic voltage scaling,
Frequency,
Gears,
Computer science,
Energy efficiency,
Jitter,
Energy consumption,
Permission,
Microprocessors,
Performance loss"
GLIDER: gradient landmark-based distributed routing for sensor networks,"We present gradient landmark-based distributed routing (GLIDER), a novel naming/addressing scheme and associated routing algorithm, for a network of wireless communicating nodes. We assume that the nodes are fixed (though their geographic locations are not necessarily known), and that each node can communicate wirelessly with some of its geographic neighbors - a common scenario in sensor networks. We develop a protocol which in a preprocessing phase discovers the global topology of the sensor field and, as a byproduct, partitions the nodes into routable tiles - regions where the node placement is sufficiently dense and regular that local greedy methods can work well. Such global topology includes not just connectivity but also higher order topological features, such as the presence of holes. We address each node by the name of the tile containing it and a set of local coordinates derived from connectivity graph distances between the node and certain landmark nodes associated with its own and neighboring tiles. We use the tile adjacency graph for global route planning and the local coordinates for realizing actual inter- and intra-tile routes. We show that efficient load-balanced global routing can be implemented quite simply using such a scheme.","Network topology,
Tiles,
Wireless sensor networks,
Hardware,
Routing protocols,
Computer science,
Land use planning,
Graph theory,
Combinatorial mathematics,
IP networks"
Visible Korean Human: Improved serially sectioned images of the entire body,"The data from the Visible Human Project (VHP) and the Chinese Visible Human (CVH), which are the serially sectioned images of the entire cadaver, are being used to produce three-dimensional (3-D) images and software. The purpose of our research, the Visible Korean Human (VKH), is to produce an enhanced version of the serially sectioned images of an entire cadaver that can be used to upgrade the 3-D images and software. These improvements are achieved without drastically changing the methods developed for the VHP and CVH; thus, a complementary solution was found. A Korean male cadaver was chosen without anything perfused into the cadaver; the entire body was magnetic resonance (MR) and computed tomography (CT) scanned at 1.0-mm intervals to produce MR and CT images. After scanning, entire body of the cadaver was embedded and serially sectioned at 0.2-mm intervals; each sectioned surface was inputted into a personal computer to produce anatomical images (pixel size: 0.2 mm) without any missing images. Eleven anatomical organs in the anatomical images were segmented to produce segmented images. The anatomical and segmented images were stacked and reconstructed to produce 3-D images. The VKH is an ongoing research; we will produce a female version of the VKH and provide more detailed segmented images. The data from the VHP, CVH, and VKH will provide valuable resources to the medical image library of 3-D images and software in the field of medical education and clinical trials.","Humans,
Cadaver,
Image segmentation,
Computed tomography,
Biomedical imaging,
Magnetic resonance,
Surface reconstruction,
Microcomputers,
Pixel,
Image reconstruction"
Matching and anatomical labeling of human airway tree,"Matching of corresponding branchpoints between two human airway trees, as well as assigning anatomical names to the segments and branchpoints of the human airway tree, are of significant interest for clinical applications and physiological studies. In the past, these tasks were often performed manually due to the lack of automated algorithms that can tolerate false branches and anatomical variability typical for in vivo trees. In this paper, we present algorithms that perform both matching of branchpoints and anatomical labeling of in vivo trees without any human intervention and within a short computing time. No hand-pruning of false branches is required. The results from the automated methods show a high degree of accuracy when validated against reference data provided by human experts. 92.9% of the verifiable branchpoint matches found by the computer agree with experts' results. For anatomical labeling, 97.1% of the automatically assigned segment labels were found to be correct.",
Vector boosting for rotation invariant multi-view face detection,"In this paper, we propose a novel tree-structured multiview face detector (MVFD), which adopts the coarse-to-fine strategy to divide the entire face space into smaller and smaller subspaces. For this purpose, a newly extended boosting algorithm named vector boosting is developed to train the predictors for the branching nodes of the tree that have multicomponents outputs as vectors. Our MVFD covers a large range of the face space, say, +/-45/spl deg/ rotation in plane (RIP) and +/-90/spl deg/ rotation off plane (ROP), and achieves high accuracy and amazing speed (about 40 ms per frame on a 320 /spl times/ 240 video sequence) compared with previous published works. As a result, by simply rotating the detector 90/spl deg/, 180/spl deg/ and 270/spl deg/, a rotation invariant (360/spl deg/ RIP) MVFD is implemented that achieves real time performance (11 fps on a 320 /spl times/ 240 video sequence) with high accuracy.",
Development of UB Hand 3: Early Results,"The first part of this paper describes the development of a humanoid robot hand based on an endoskeleton made of rigid links connected with elastic hinges, actuated by sheath routed tendons and covered by continuous compliant pulps. The project is called UB Hand 3 (University of Bologna Hand, 3rd version) and aims to reduce the mechanical complexity of robotic end effectors yet maintaining full anthropomorphic aspect and a good level of dexterity. In the second part this paper focuses on the early experiences of the UB Hand 3 in performing manipulation tasks.","Fingers,
Humans,
Humanoid robots,
Testing,
Fasteners,
Prototypes,
Mechanical engineering,
Computer science,
Tendons,
End effectors"
Error correction via linear programming,Suppose we wish to transmit a vector f Ïµ Rn reliably. A frequently discussed approach consists in encoding f with an m by n coding matrix A. Assume now that a fraction of the entries of Af are corrupted in a completely arbitrary fashion by an error e. We do not know which entries are affected nor do we know how they are affected. Is it possible to recover f exactly from the corrupted m-dimensional vector y = Af + e?,"Error correction,
Linear programming,
Linear code,
Vectors,
Mathematics,
Error correction codes,
Encoding,
Particle measurements,
Functional analysis,
Decoding"
Locational marginal price sensitivities,"Within an optimal power flow market clearing framework, this paper provides expressions to compute the sensitivities of locational marginal prices with respect to power demands. Sensitivities with respect to other parameters can also be obtained. An example and a case study are used to illustrate the expressions derived.","Load flow,
Voltage,
Power demand,
Reactive power,
Power generation,
Mathematics,
ISO standards,
Computer science,
Statistics,
Upper bound"
Network localization in partially localizable networks,"Knowing the positions of the nodes in a network is essential to many next generation pervasive and sensor network functionalities. Although many network localization systems have recently been proposed and evaluated, there has been no systematic study of partially localizable networks, i.e., networks in which there exist nodes whose positions cannot be uniquely determined. There is no existing study which correctly identifies precisely which nodes in a network are uniquely localizable and which are not. This absence of a sufficient uniqueness condition permits the computation of erroneous positions that may in turn lead applications to produce flawed results. In this paper, in addition to demonstrating the relevance of networks that may not be fully localizable, we design the first framework for two dimensional network localization with an efficient component to correctly determine which nodes are localizable and which are not. Implementing this system, we conduct comprehensive evaluations of network localizability, providing guidelines for both network design and deployment. Furthermore, we study an integration of traditional geographic routing with geographic routing over virtual coordinates in the partially localizable network setting. We show that this novel cross-layer integration yields good performance, and argue that such optimizations will be likely be necessary to ensure acceptable application performance in partially localizable networks.","Intelligent networks,
Global Positioning System,
Routing,
Satellites,
Computer science,
Australia,
Sensor systems,
Guidelines,
Next generation networking,
Pervasive computing"
Dynamic-Domain RRTs: Efficient Exploration by Controlling the Sampling Domain,"Sampling-based planners have solved difficult problems in many applications of motion planning in recent years. In particular, techniques based on the Rapidly-exploring Random Trees (RRTs) have generated highly successful single-query planners. Even though RRTs work well on many problems, they have weaknesses which cause them to explore slowly when the sampling domain is not well adapted to the problem. In this paper we characterize these issues and propose a general framework for minimizing their effect. We develop and implement a simple new planner which shows significant improvement over existing RRT-based planners. In the worst cases, the performance appears to be only slightly worse in comparison to the original RRT, and for many problems it performs orders of magnitude better.","Sampling methods,
Motion planning,
Application software,
Orbital robotics,
Motion control,
Computer science,
Urban planning,
Robots,
Computer aided manufacturing,
Pharmaceuticals"
Pollution in P2P file sharing systems,"One way to combat P2P file sharing of copyrighted content is to deposit into the file sharing systems large volumes of polluted files. Without taking sides in the file sharing debate, in this paper we undertake a measurement study of the nature and magnitude of pollution in the FastTrack P2P network, currently the most popular P2P file sharing system. We develop a crawling platform which crawls the majority of the FastTrack Network's 20,000+ supernodes in less than 60 minutes. From the raw data gathered by the crawler for popular audio content, we obtain statistics on the number of unique versions and copies available in a 24-hour period. We develop an automated procedure to detect whether a given version is polluted or not, and we show that the probabilities of false positives and negatives of the detection procedure are very small. We use the data from the crawler and our pollution detection algorithm to determine the fraction of versions and fraction of copies that are polluted for several recent and old songs. We observe that pollution is pervasive for recent popular songs. We also identify and describe a number of anti-pollution mechanisms.","Peer to peer computing,
Information science,
Pollution measurement,
Internet,
Telecommunication traffic,
Crawlers,
Marketing and sales,
Current measurement,
Statistics,
Probability"
Efficient simulation of blood flow past complex endovascular devices using an adaptive embedding technique,"The simulation of blood flow past endovascular devices such as coils and stents is a challenging problem due to the complex geometry of the devices. Traditional unstructured grid computational fluid dynamics relies on the generation of finite element grids that conform to the boundary of the computational domain. However, the generation of such grids for patient-specific modeling of cerebral aneurysm treatment with coils or stents is extremely difficult and time consuming. This paper describes the application of an adaptive grid embedding technique previously developed for complex fluid structure interaction problems to the simulation of endovascular devices. A hybrid approach is used: the vessel walls are treated with body conforming grids and the endovascular devices with an adaptive mesh embedding technique. This methodology fits naturally in the framework of image-based computational fluid dynamics and opens the door for exploration of different therapeutic options and personalization of endovascular procedures.",
Exploiting structural duplication for lifetime reliability enhancement,"Increased power densities (and resultant temperatures) and other effects of device scaling are predicted to cause significant lifetime reliability problems in the near future. In this paper, we study two techniques that leverage microarchitectural structural redundancy for lifetime reliability enhancement. First, in structural duplication (SD), redundant microarchitectural structures are added to the processor and designated as spares. Spare structures can be turned on when the original structure fails, increasing the processor's lifetime. Second, graceful performance degradation (GPD) is a technique which exploits existing microarchitectural redundancy for reliability. Redundant structures that fail are shut down while still maintaining functionality, thereby increasing the processor's lifetime, but at a lower performance. Our analysis shows that exploiting structural redundancy can provide significant reliability benefits, and we present guidelines for efficient usage of these techniques by identifying situations where each is more beneficial. We show that GPD is the superior technique when only limited performance or cost resources can be sacrificed for reliability. Specifically, on average for our systems and applications, GPD increased processor reliability to 1.42 times the base value for less than a 5% loss in performance. On the other hand, for systems where reliability is more important than performance or cost, SD is more beneficial. SD increases reliability to 3.17 times the base value for 2.25 times the base cost, for our applications. Finally, a combination of the two techniques (SD+GPD) provides the highest reliability benefit.","Redundancy,
Microarchitecture,
Costs,
Temperature,
Process design,
Rivers,
Computer science,
Degradation,
Maintenance,
Guidelines"
Class-specific material categorisation,"Although a considerable amount of work has been published on material classification, relatively little of it studies situations with considerable variation within each class. Many experiments use the exact same sample, or different patches from the same image, for training and test sets. Thus, such studies are vulnerable to effectively recognising one particular sample of a material as opposed to the material category. In contrast, this paper places firm emphasis on the capability to generalise to previously unseen instances of materials. We adopt an appearance-based strategy, and conduct experiments on a new database which contains several samples of each of eleven material categories, imaged under a variety of pose, illumination and scale conditions. Together, these sources of intra-class variation provide a stern challenge indeed for recognition. Somewhat surprisingly, the difference in performance between various state-of-the-art texture descriptors proves rather small in this task. On the other hand, we clearly demonstrate that very significant gains can be achieved via different SVM-based classification techniques. Selecting appropriate kernel parameters proves crucial. This motivates a novel recognition scheme based on a decision tree. Each node contains an SVM to split one class from all others with a kernel parameter optimal for that particular node. Hence, each decision is made using a different, optimal, class-specific metric. Experiments show the superiority of this approach over several state-of-the-art classifiers",
Real-world environment models for mobile network evaluation,"Simulation environments are an important tool for the evaluation of new concepts in networking. The study of mobile ad hoc networks depends on understanding protocols from simulations, before these protocols are implemented in a real-world setting. To produce a real-world environment within which an ad hoc network can be formed among a set of nodes, there is a need for the development of realistic, generic and comprehensive mobility, and signal propagation models. In this paper, we propose the design of a mobility and signal propagation model that can be used in simulations to produce realistic network scenarios. Our model allows the placement of obstacles that restrict movement and signal propagation. Movement paths are constructed as Voronoi tessellations with the corner points of these obstacles as Voronoi sites. Our mobility model also introduces a signal propagation model that emulates properties of fading in the presence of obstacles. As a result, we have developed a complete environment in which network protocols can be studied on the basis of numerous performance metrics. Through simulation, we show that the proposed mobility model has a significant impact on network performance, especially when compared with other mobility models. In addition, we also observe that the performance of ad hoc network protocols is effected when different mobility scenarios are utilized.","Protocols,
Ad hoc networks,
Atmospheric modeling,
Mobile ad hoc networks,
Fading,
Signal design,
Measurement,
Testing,
Computer science"
HBD layout isolation techniques for multiple node charge collection mitigation,"A three-dimensional (3D) technology computer-aided design (TCAD) model was used to simulate charge collection at multiple nodes. Guard contacts are shown to mitigate the charge collection and to more quickly restore the well potential, especially in PMOS devices. Mitigation of the shared charge collection in NMOS devices is accomplished through isolation of the P-wells using a triple-well option. These techniques have been partially validated through heavy-ion testing of three versions of flip-flop shift register chains.",
Estimating the numbers of end users and end user programmers,"In 1995, Boehm predicted that by 2005, there would be ""55 million performers"" of ""end user programming"" in the United States. The original context and method which generated this number had two weaknesses, both of which we address. First, it relies on undocumented, judgment-based factors to estimate the number of end user programmers based on the total number of end users; we address this weakness by identifying specific end user sub-populations and then estimating their sizes. Second, Boehm's estimate relies on additional undocumented, judgment-based factors to adjust for rising computer usage rates; we address this weakness by integrating fresh Bureau of Labor Statistics (BLS) data and projections as well as a richer estimation method. With these improvements to Boehm's method, we estimate that in 2012 there will be 90 million end users in American workplaces. Of these, we anticipate that over 55 million will use spreadsheets or databases (and therefore may potentially program), while over 13 million will describe themselves as programmers, compared to BLS projections of fewer than 3 million professional programmers. We have validated our improved method by generating estimates for 2001 and 2003, then verifying that our estimates are consistent with existing estimates from other sources.","Programming profession,
Computer science,
Statistics,
Employment,
Databases,
Prediction methods,
Software performance,
Computer industry,
Costs,
Programming environments"
Segmentation of kidney from ultrasound images based on texture and shape priors,"This work presents a novel texture and shape priors based method for kidney segmentation in ultrasound (US) images. Texture features are extracted by applying a bank of Gabor filters on test images through a two-sided convolution strategy. The texture model is constructed via estimating the parameters of a set of mixtures of half-planed Gaussians using the expectation-maximization method. Through this texture model, the texture similarities of areas around the segmenting curve are measured in the inside and outside regions, respectively. We also present an iterative segmentation framework to combine the texture measures into the parametric shape model proposed by Leventon and Faugeras. Segmentation is implemented by calculating the parameters of the shape model to minimize a novel energy function. The goal of this energy function is to partition the test image into two regions, the inside one with high texture similarity and low texture variance, and the outside one with high texture variance. The effectiveness of this method is demonstrated through experimental results on both natural images and US data compared with other image segmentation methods and manual segmentation.","Image segmentation,
Ultrasonic imaging,
Testing,
Shape measurement,
Feature extraction,
Gabor filters,
Convolution,
Parameter estimation,
Gaussian processes,
Area measurement"
Hierarchical fixed priority pre-emptive scheduling,"This paper focuses on the hierarchical scheduling of systems where a number of separate applications reside on a single processor. It addresses the particular case where fixed priority pre-emptive scheduling is used at both global and local levels, with a server associated with each application. Using response time analysis, an exact schedulability test is derived for application tasks. This test improves on previously published work. The analysis is extended to the case of harmonic tasks that can be bound to the release of their server. These tasks exhibit improved schedulability indicating that it is advantageous to choose server periods that enable some tasks to be bound to the release of their server. The use of periodic, sporadic and deferrable servers is considered with the conclusion that the simple periodic server dominates both sporadic and deferrable servers when the metric is application task schedulability","Processor scheduling,
Microprocessors,
Real time systems,
Testing,
Automotive electronics,
Control systems,
Time factors,
Dynamic scheduling,
Computer science,
Application software"
Global and local consistencies in distributed fault diagnosis for discrete-event systems,"In this paper, we present a unified framework for distributed diagnosis. We first introduce the concepts of global and local consistency in terms of supremal global and local supports, then present two distributed diagnosis problems based on them. After that, we provide algorithms to achieve supremal global and local supports respectively, and discuss in detail the advantages and disadvantages of each. Finally, we present an industrial example to demonstrate our distributed diagnosis approach.",
Optimal dimensionality reduction of sensor data in multisensor estimation fusion,"When there exists the limitation of communication bandwidth between sensors and a fusion center, one needs to optimally precompress sensor outputs-sensor observations or estimates before the sensors' transmission in order to obtain a constrained optimal estimation at the fusion center in terms of the linear minimum error variance criterion, or when an allowed performance loss constraint exists, one needs to design the minimum dimension of sensor data. This paper will answer the above questions by using the matrix decomposition, pseudo-inverse, and eigenvalue techniques.","Sensor fusion,
Bandwidth,
Sensor systems,
Performance loss,
Propagation losses,
Matrix decomposition,
Eigenvalues and eigenfunctions,
Mathematics,
Computer science,
System performance"
"Test-driven development concepts, taxonomy, and future direction","Test-driven development creates software in very short iterations with minimal upfront design. This strategy requires writing automated tests prior to developing functional code in small, rapid iterations. Although developers have been applying TDD in various forms for several decades, this software development strategy has continued to gain increased attention as one of the core extreme programming practices.",
Partial volume effect compensation for quantitative brain SPECT imaging,"Partial volume (PV) effects degrade the quantitative accuracy of SPECT brain images. In this paper, we extended a PV compensation (PVC) method originally developed for brain PET, the geometric transfer matrix (GTM) method, to brain SPECT using iterative reconstruction-based compensations. In the GTM method a linear transform between the true regional activities and the measured results was assumed. Elements of the GTM were calculated by projecting and reconstructing maps with uniform regions representing different structures. However, with iterative reconstruction methods, especially when reconstruction-based compensation for detector response was applied, we found that it was important to treat the region maps as a perturbation to the reconstructed image in the estimation of the GTM. This modified method, termed perturbation-based GTM (pGTM) was evaluated using Monte Carlo (MC) simulated and experimentally acquired data. Results showed great improvement of the quantitative accuracy in brain SPECT imaging. For MC simulated data, PVC using pGTM reduced the underestimation of striatal activities from 30% to less than 1.2%. For experimental data, PVC using pGTM reduced the underestimation of striatal activities from 36% to less than 7.8%. The underestimation of the striatum to background activity ratio was also improved from 31% to 2.7%.","Image reconstruction,
Reconstruction algorithms,
Iterative methods,
Degradation,
Positron emission tomography,
Detectors,
Brain modeling,
Diseases,
Attenuation,
Scattering"
Relevance vector machine for automatic detection of clustered microcalcifications,"Clustered microcalcifications (MC) in mammograms can be an important early sign of breast cancer in women. Their accurate detection is important in computer-aided detection (CADe). In this paper, we propose the use of a recently developed machine-learning technique - relevance vector machine (RVM) - for detection of MCs in digital mammograms. RVM is based on Bayesian estimation theory, of which a distinctive feature is that it can yield a sparse decision function that is defined by only a very small number of so-called relevance vectors. By exploiting this sparse property of the RVM, we develop computerized detection algorithms that are not only accurate but also computationally efficient for MC detection in mammograms. We formulate MC detection as a supervised-learning problem, and apply RVM as a classifier to determine at each location in the mammogram if an MC object is present or not. To increase the computation speed further, we develop a two-stage classification network, in which a computationally much simpler linear RVM classifier is applied first to quickly eliminate the overwhelming majority, non-MC pixels in a mammogram from any further consideration. The proposed method is evaluated using a database of 141 clinical mammograms (all containing MCs), and compared with a well-tested support vector machine (SVM) classifier. The detection performance is evaluated using free-response receiver operating characteristic (FROC) curves. It is demonstrated in our experiments that the RVM classifier could greatly reduce the computational complexity of the SVM while maintaining its best detection accuracy. In particular, the two-stage RVM approach could reduce the detection time from 250 s for SVM to 7.26 s for a mammogram (nearly 35-fold reduction). Thus, the proposed RVM classifier is more advantageous for real-time processing of MC clusters in mammograms.","Support vector machines,
Support vector machine classification,
Computer networks,
Breast cancer,
Bayesian methods,
Estimation theory,
Detection algorithms,
Object detection,
Databases,
Computational complexity"
Mixture models with adaptive spatial regularization for segmentation with an application to FMRI data,"Mixture models are often used in the statistical segmentation of medical images. For example, they can be used for the segmentation of structural images into different matter types or of functional statistical parametric maps (SPMs) into activations and nonactivations. Nonspatial mixture models segment using models of just the histogram of intensity values. Spatial mixture models have also been developed which augment this histogram information with spatial regularization using Markov random fields. However, these techniques have control parameters, such as the strength of spatial regularization, which need to be tuned heuristically to particular datasets. We present a novel spatial mixture model within a fully Bayesian framework with the ability to perform fully adaptive spatial regularization using Markov random fields. This means that the amount of spatial regularization does not have to be tuned heuristically but is adaptively determined from the data. We examine the behavior of this model when applied to artificial data with different spatial characteristics, and to functional magnetic resonance imaging SPMs.","Image segmentation,
Scanning probe microscopy,
Magnetic resonance imaging,
Histograms,
Brain,
Markov random fields,
Hospitals,
Biomedical imaging,
Councils,
Bayesian methods"
Localized measurement of optical attenuation coefficients of atherosclerotic plaque constituents by quantitative optical coherence tomography,"Optical coherence tomography (OCT) is a novel, high-resolution diagnostic tool that is capable of imaging the arterial wall and plaques. The differentiation between different types of atherosclerotic plaque is based on qualitative differences in gray levels and structural appearance. We hypothesize that a quantitative data analysis of the OCT signal allows measurement of light attenuation by the local tissue components, which can facilitate quantitative spatial discrimination between plaque constituents. High-resolution OCT images (at 800 nm) of human atherosclerotic arterial segments obtained at autopsy were histologically validated. Using a new, simple analysis algorithm, which incorporates the confocal properties of the OCT system, the light attenuation coefficients for these constituents were determined: for diffuse intimal thickening (5.5/spl plusmn/1.2 mm/sup -1/) and lipid-rich regions (3.2/spl plusmn/1.1 mm/sup -1/), the attenuation differed significantly from media (9.9/spl plusmn/1.8 mm/sup -1/), calcifications (11.1/spl plusmn/4.9 mm/sup -1/) and thrombi (11.2/spl plusmn/2.3 mm/sup -1/) (p<0.01). These proof of principle studies show that simple quantitative analysis of the OCT signals allows spatial determination of the intrinsic optical attenuation coefficient of atherosclerotic tissue components within regions of interest. Combining morphological imaging by OCT with the observed differences in optical attenuation coefficients of the various regions may enhance discrimination between various plaque types.","Optical attenuators,
Attenuation measurement,
Tomography,
Optical imaging,
High-resolution imaging,
Arteries,
Data analysis,
Humans,
Image segmentation,
Autopsy"
Standardized evaluation methodology for 2-D-3-D registration,"In the past few years, a number of two-dimensional (2-D) to three-dimensional (3-D) (2-D-3-D) registration algorithms have been introduced. However, these methods have been developed and evaluated for specific applications, and have not been directly compared. Understanding and evaluating their performance is therefore an open and important issue. To address this challenge we introduce a standardized evaluation methodology, which can be used for all types of 2-D-3-D registration methods and for different applications and anatomies. Our evaluation methodology uses the calibrated geometry of a 3-D rotational X-ray (3DRX) imaging system (Philips Medical Systems, Best, The Netherlands) in combination with image-based 3-D-3-D registration for attaining a highly accurate gold standard for 2-D X-ray to 3-D MR/CT/3DRX registration. Furthermore, we propose standardized starting positions and failure criteria to allow future researchers to directly compare their methods. As an illustration, the proposed methodology has been used to evaluate the performance of two 2-D-3-D registration techniques, viz. a gradient-based and an intensity-based method, for images of the spine. The data and gold standard transformations are available on the internet (http://www.isi.uu.nl/Research/Databases/).",
A real time cognitive radio testbed for physical and link layer experiments,"Cognitive radios have been advanced as a technology for the opportunistic use of under-utilized spectrum. Cognitive radio is able to sense the spectrum and detect the presence of primary users. However, primary users of the spectrum are skeptical about the robustness of this sensing process and have raised concerns with regards to interference from cognitive radios. Furthermore, while a number of techniques have been advanced to aid the sensing process, none of these techniques have been verified in a practical system. To alleviate these concerns, a real time testbed is required, which can aid the comparison of these techniques and enable the measurement and evaluation of key interference and performance metrics. In this paper, we present such a testbed, which is based on the BEE2, a multi-FPGA emulation engine. The BEE2 can connect to 18 radio front-ends, which can be configured as primary or secondary users. Inherent parallelism of the FPGAs allows the simultaneous operation of multiple radios, which can communicate and exchange information via high speed low latency links","Cognitive radio,
Testing,
Radio spectrum management,
Frequency,
FCC,
Computer science,
Interference,
Robustness,
Measurement,
Emulation"
"Control channel based MAC-layer configuration, routing and situation awareness for cognitive radio networks","In a cognitive radio (CR) network, MAC-layer configuration involves determining a common set of channels to facilitate communication among participating nodes. Further, the availability of multiple channels and frequent channel switches add to the complexity of route selection. Knowledge of the global network topology can be used to solve the above-described problems. In this paper, we propose a distributed algorithm for gathering global network topology information for a CR network. We outline approaches that utilize the gathered topology information for MAC-layer configuration and efficient routing of packets. In addition, situation awareness is achieved by sharing the physical location information among the nodes in the network. The proposed algorithm determines the global network topology in O(N2 ) timeslots, where N is the maximum number of nodes deployed. With 80 available channels for communication, the algorithm terminates within 0.8 second","Radio control,
Routing,
Cognitive radio,
Chromium,
Frequency,
Network topology,
Switches,
Communication switching,
Computer science,
Distributed algorithms"
Link scheduling in sensor networks: distributed edge coloring revisited,"We consider the problem of link scheduling in a sensor network employing a TDMA MAC protocol. Our link scheduling algorithm involves two phases. In the first phase, we assign a color to each edge in the network such that no two edges incident on the same node are assigned the same color. We propose a distributed edge coloring algorithm that needs at most (/spl delta/+1) colors, where /spl delta/ is the maximum degree of the graph. To the best of our knowledge, this is the first distributed algorithm that can edge color a graph with at most (/spl delta/+1) colors. In the second phase, we map each color to a unique timeslot and attempt to identify a direction of transmission along each edge such that the hidden terminal and the exposed terminal problems are avoided. Next, considering topologies for which a feasible solution does not exist, we obtain a direction of transmission for each edge using additional timeslots, if necessary. Finally, we show that reversing the direction of transmission along every edge leads to another feasible direction of transmission. Using both the transmission assignments we obtain a TDMA MAC schedule, which enables two-way communication between every pair of neighbors. For acyclic topologies, we show that at most 2(/spl delta/+1) timeslots are required. Through simulations we show that for sparse graphs with cycles the number of timeslots assigned is close to 2(/spl delta/+1).","Intelligent networks,
Media Access Protocol,
Time division multiple access,
Access protocols,
Wireless sensor networks,
Base stations,
Delay,
Processor scheduling,
Computer science,
Remote monitoring"
Variational optical flow computation in real time,"This paper investigates the usefulness of bidirectional multigrid methods for variational optical flow computations. Although these numerical schemes are among the fastest methods for solving equation systems, they are rarely applied in the field of computer vision. We demonstrate how to employ those numerical methods for the treatment of variational optical flow formulations and show that the efficiency of this approach even allows for real-time performance on standard PCs. As a representative for variational optic flow methods, we consider the recently introduced combined local-global method. It can be considered as a noise-robust generalization of the Horn and Schunck technique. We present a decoupled, as well as a coupled, version of the classical Gau/spl szlig/-Seidel solver, and we develop several multigrid implementations based on a discretization coarse grid approximation. In contrast, with standard bidirectional multigrid algorithms, we take advantage of intergrid transfer operators that allow for nondyadic grid hierarchies. As a consequence, no restrictions concerning the image size or the number of traversed levels have to be imposed. In the experimental section, we juxtapose the developed multigrid schemes and demonstrate their superior performance when compared to unidirectional multigrid methods and nonhierachical solvers. For the well-known 316/spl times/252 Yosemite sequence, we succeeded in computing the complete set of dense flow fields in three quarters of a second on a 3.06-GHz Pentium4 PC. This corresponds to a frame rate of 18 flow fields per second which outperforms the widely-used Gau/spl szlig/-Seidel method by almost three orders of magnitude.",
Particle swarm optimization approaches to coevolve strategies for the iterated prisoner's dilemma,"This paper presents and investigates the application of coevolutionary training techniques based on particle swarm optimization (PSO) to evolve playing strategies for the nonzero sum problem of the iterated prisoner's dilemma (IPD). Three different coevolutionary PSO techniques are used, differing in the way that IPD strategies are presented: A neural network (NN) approach in which the NN is used to predict the next action, a binary PSO approach in which the particle represents a complete playing strategy, and finally, a novel approach that exploits the symmetrical structure of man-made strategies. The last technique uses a PSO algorithm as a function approximator to evolve a function that characterizes the dynamics of the IPD. These different PSO approaches are compared experimentally with one another, and with popular man-made strategies. The performance of these approaches is evaluated in both clean and noisy environments. Results indicate that NNs cooperate well, but may develop weak strategies that can cause catastrophic collapses. The binary PSO technique does not have the same deficiency, instead resulting in an overall state of equilibrium in which some strategies are allowed to exploit the population, but never dominate. The symmetry approach is not as successful as the binary PSO approach in maintaining cooperation in both noisy and noiseless environments-exhibiting selfish behavior against the benchmark strategies and depriving them of receiving almost any payoff. Overall, the PSO techniques are successful at generating a variety of strategies for use in the IPD, duplicating and improving on existing evolutionary IPD population observations.","Particle swarm optimization,
Neural networks,
Working environment noise,
Game theory,
Computer science,
Mathematical model,
Environmental economics,
Mathematics,
Information technology,
Africa"
Graph-theoretic analysis of structured peer-to-peer systems: routing distances and fault resilience,"This paper examines graph-theoretic properties of existing peer-to-peer networks and proposes a new infrastructure based on optimal-diameter de Bruijn graphs. Since generalized de Bruijn graphs exhibit very short average distances and high resilience to node failure, they are well suited for distributed hash tables (DHTs). Using the example of Chord, CAN, and de Bruijn, we study the routing performance, graph expansion, clustering properties, and bisection width of each graph. Having confirmed that de Bruijn graphs offer the best diameter and highest connectivity among the existing peer-to-peer structures, we offer a very simple incremental building process that preserves optimal properties of de Bruijn graphs under uniform user joins/departures. We call the combined peer-to-peer architecture optimal diameter routing infrastructure.","Peer to peer computing,
Routing,
Resilience,
Proposals,
Computer science,
Buildings,
IP networks,
Tree graphs,
Delay,
Fault tolerance"
Efficient deployment algorithms for ensuring coverage and connectivity of wireless sensor networks,"Sensor deployment is a critical issue since it reflects the cost and detection capability of a wireless sensor network. Although lots of work has addressed this issue, most of them assume that the sensing field is an open space and there is a special relationship between the communication range and sensing range of sensors. In this work, we consider the sensing field as an arbitrary-shaped region possibly with obstacles. Besides, we allow an arbitrary relationship between the communication range and sensing range, thus eliminating the constraints of existing results. Our approach is to partition the sensing field into smaller sub-regions based on the shape of the field, and then to deploy sensors in these sub-regions. Simulation results show that our method requires fewer sensors compared to existing results.","Wireless sensor networks,
Costs,
Shape,
Computerized monitoring,
Art,
Computer science,
Transceivers,
Actuators,
Wireless communication,
Base stations"
Thermal via planning for 3-D ICs,"Heat dissipation is one of the most serious challenges in 3-D IC designs. One effective way of reducing circuit temperature is to introduce thermal through-the-silicon (TTS) vias. In this paper, we extended the TTS-via planning in a multilevel routing framework by Cong and Zhang (2005), but use a much enhanced TTS-via planning algorithm. We formulate the TTS-via minimization problem with temperature constraints as a constrained nonlinear programming problem (NLP) based on the thermal resistive model and develop an efficient heuristic algorithm, named m-ADVP, which solves a sequence of simplified via planning subproblems in alternating direction in a multilevel framework. The vertical via distribution is formulated as a convex programming problem, and the horizontal via planning is based on two efficient techniques: path counting and heat propagation. Experimental results show that the m-ADVP algorithm is more than 200/spl times/ faster than the direct solution to the NPL formulation for via planning with very similar solution quality (within 1% of TS-vias count). However, compared to a recent work of multilevel TS-via planning algorithm based on temperature profiling (Cong and Zhang, 2005), our algorithm can reduce the total TS-via number by over 68% for the same required temperature with similar runtime.","Temperature,
Cooling,
Heat sinks,
Three-dimensional integrated circuits,
Thermal conductivity,
Routing,
Optimization methods,
Resistance heating,
Computer science,
Speech synthesis"
Self-Adapting Linear Algebra Algorithms and Software,"One of the main obstacles to the efficient solution of scientific problems is the problem of tuning software, both to the available architecture and to the user problem at hand. We describe approaches for obtaining tuned high-performance kernels and for automatically choosing suitable algorithms. Specifically, we describe the generation of dense and sparse Basic Linear Algebra Subprograms (BLAS) kernels, and the selection of linear solver algorithms. However, the ideas presented here extend beyond these areas, which can be considered proof of concept.","Linear algebra,
Software algorithms,
Kernel,
Computer architecture,
Hardware,
Computer science,
Software libraries,
Laboratories,
Iterative algorithms,
Sparse matrices"
Unsupervised 3D object recognition and reconstruction in unordered datasets,"This paper presents a system for fully automatic recognition and reconstruction of 3D objects in image databases. We pose the object recognition problem as one of finding consistent matches between all images, subject to the constraint that the images were taken from a perspective camera. We assume that the objects or scenes are rigid. For each image, we associate a camera matrix, which is parameterised by rotation, translation and focal length. We use invariant local features to find matches between all images, and the RANSAC algorithm to find those that are consistent with the fundamental matrix. Objects are recognised as subsets of matching images. We then solve for the structure and motion of each object, using a sparse bundle adjustment algorithm. Our results demonstrate that it is possible to recognise and reconstruct 3D objects from an unordered image database with no user input at all.","Object recognition,
Image reconstruction,
Image recognition,
Cameras,
Image databases,
Layout,
Feature extraction,
Computer science,
Sparse matrices,
Computer vision"
Fast generation of digitally reconstructed radiographs using attenuation fields with application to 2D-3D image registration,"Generation of digitally reconstructed radiographs (DRRs) is computationally expensive and is typically the rate-limiting step in the execution time of intensity-based two-dimensional to three-dimensional (2D-3D) registration algorithms. We address this computational issue by extending the technique of light field rendering from the computer graphics community. The extension of light fields, which we call attenuation fields (AFs), allows most of the DRR computation to be performed in a preprocessing step; after this precomputation step, DRRs can be generated substantially faster than with conventional ray casting. We derive expressions for the physical sizes of the two planes of an AF necessary to generate DRRs for a given X-ray camera geometry and all possible object motion within a specified range. Because an AF is a ray-based data structure, it is substantially more memory efficient than a huge table of precomputed DRRs because it eliminates the redundancy of replicated rays. Nonetheless, an AF can require substantial memory, which we address by compressing it using vector quantization. We compare DRRs generated using AFs (AF-DRRs) to those generated using ray casting (RC-DRRs) for a typical C-arm geometry and computed tomography images of several anatomic regions. They are quantitatively very similar: the median peak signal-to-noise ratio of AF-DRRs versus RC-DRRs is greater than 43 dB in all cases. We perform intensity-based 2D-3D registration using AF-DRRs and RC-DRRs and evaluate registration accuracy using gold-standard clinical spine image data from four patients. The registration accuracy and robustness of the two methods is virtually identical whereas the execution speed using AF-DRRs is an order of magnitude faster.",
Optimal frequency-hopping sequences via cyclotomy,"Using cyclotomic numbers, a simple construction is presented for frequency-hopping (FH) sequences having optimal autocorrelation with respect to the well-known Lempel-Greenberger bound. Some optimal families of FH sequences are constructed. The simplicity of this technique makes it attractive for practical use.","Frequency,
Autocorrelation,
Spread spectrum radar,
Multiaccess communication,
Collision mitigation,
Bonding,
Computer science"
DTI segmentation using an information theoretic tensor dissimilarity measure,"In recent years, diffusion tensor imaging (DTI) has become a popular in vivo diagnostic imaging technique in Radiological sciences. In order for this imaging technique to be more effective, proper image analysis techniques suited for analyzing these high dimensional data need to be developed. In this paper, we present a novel definition of tensor ""distance"" grounded in concepts from information theory and incorporate it in the segmentation of DTI. In a DTI, the symmetric positive definite (SPD) diffusion tensor at each voxel can be interpreted as the covariance matrix of a local Gaussian distribution. Thus, a natural measure of dissimilarity between SPD tensors would be the Kullback-Leibler (KL) divergence or its relative. We propose the square root of the J-divergence (symmetrized KL) between two Gaussian distributions corresponding to the diffusion tensors being compared and this leads to a novel closed form expression for the ""distance"" as well as the mean value of a DTI. Unlike the traditional Frobenius norm-based tensor distance, our ""distance"" is affine invariant, a desirable property in segmentation and many other applications. We then incorporate this new tensor ""distance"" in a region based active contour model for DTI segmentation. Synthetic and real data experiments are shown to depict the performance of the proposed model.",
Shared risk link Group (SRLG)-diverse path provisioning under hybrid service level agreements in wavelength-routed optical mesh networks,"The static provisioning problem in wavelength-routed optical networks has been studied for many years. However, service providers are still facing the challenges arising from the special requirements for provisioning services at the optical layer. In this paper, we incorporate some realistic constraints into the static provisioning problem, and formulate it under different network resource availability conditions. We consider three classes of shared risk link group (SRLG)-diverse path protection schemes: dedicated, shared, and unprotected. We associate with each connection request a lightpath length constraint and a revenue value. When the network resources are not sufficient to accommodate all the connection requests, the static provisioning problem is formulated as a revenue maximization problem, whose objective is maximizing the total revenue value. When the network has sufficient resources, the problem becomes a capacity minimization problem with the objective of minimizing the number of used wavelength-links. We provide integer linear programming (ILP) formulations for these problems. Because solving these ILP problems is extremely time consuming, we propose a tabu search heuristic to solve these problems within a reasonable amount of time. We also develop a rerouting optimization heuristic, which is based on previous work. Experimental results are presented to compare the solutions obtained by the tabu search heuristic and the rerouting optimization heuristic. For both problems, the tabu search heuristic outperforms the rerouting optimization heuristic.",
Path vector face routing: geographic routing with local face information,"Existing geographic routing algorithms depend on the planarization of the network connectivity graph for correctness, and the planarization process gives rise to a well-defined notion of ""faces"". In this paper, we demonstrate that we can improve routing performance by storing a small amount of local face information at each node. We present a protocol, path vector exchange (PVEX), that maintains local face information at each node efficiently, and a new geographic routing algorithm, greedy path vector face routing (GPVFR), that achieves better routing performance in terms of both path stretch and hop stretch than existing geographic routing algorithms by exploiting available local face information. Our simulations demonstrate that GPVFR/PVEX achieves significantly reduced path and hop stretch than greedy perimeter stateless routing (GPSR) and somewhat better performance than greedy other adaptive face routing (GOAFR+) over a wide range of network topologies. The cost of this improved performance is a small amount of additional storage, and the bandwidth required for our algorithm is comparable to GPSR and GOAFR+ in quasi-static networks.","Planarization,
Routing protocols,
Costs,
Bandwidth,
Radio network,
Switches,
Computer science,
Artificial intelligence,
Laboratories,
Network topology"
A comparison of human and model observers in multislice LROC studies,"Model and human observers have been compared in a series of localization receiver operating characteristic (LROC) studies involving single-slice and multislice image displays. The task was detection of Ga-avid lymphomas within single photon emission computed tomography (SPECT)-reconstructed transverse slices of a mathematical phantom, and the studies involved four reconstruction strategies: the filtered-backprojection (FBP) and ordered-subset expectation-maximization (OSEM) algorithms with two- and three-dimensional postreconstruction filtering. The human-observer data was drawn from studies performed by Wells et al. (2000), while multiclass versions of the nonprewhitening (NPW), channelized nonprewhitening (CNPW), and channelized Hotelling (CH) model observers, each capable of performing the tumor search task, were applied. The channelized observers were evaluated with multiple square-channel models and both with and without internal noise. For the multislice studies, two different capacities for integrating the slice information were also tested. The CH observer gave good quantitative agreement with the human data from both image-display studies when the internal-noise model was used. The CNPW observer performed similarly with the iterative strategies. Wells et al. had shown that human observers are imperfect integrators of multislice information, and this is characterized as increased internal noise with the model observers.","Humans,
Neoplasms,
Biomedical imaging,
Image reconstruction,
Testing,
Displays,
Tumors,
Medical diagnostic imaging,
Signal detection,
Associate members"
Localized topology control algorithms for heterogeneous wireless networks,"Most existing topology control algorithms assume homogeneous wireless networks with uniform maximal transmission power, and cannot be directly applied to heterogeneous wireless networks where the maximal transmission power of each node may be different. We present two localized topology control algorithms for heterogeneous networks: Directed Relative Neighborhood Graph (DRNG) and Directed Local Spanning Subgraph (DLSS). In both algorithms, each node independently builds its neighbor set by adjusting the transmission power, and defines the network topology by using only local information. We prove that: 1) both DRNG and DLSS can preserve network connectivity; 2) the out-degree of any node in the resulting topology generated by DRNG or DLSS is bounded by a constant; and 3) DRNG and DLSS can preserve network bi-directionality. Simulation results indicate that DRNG and DLSS significantly outperform existing topology control algorithms for heterogeneous networks in several aspects.",
Model-based segmentation of medical imagery by matching distributions,"The segmentation of deformable objects from three-dimensional (3-D) images is an important and challenging problem, especially in the context of medical imagery. We present a new segmentation algorithm based on matching probability distributions of photometric variables that incorporates learned shape and appearance models for the objects of interest. The main innovation over similar approaches is that there is no need to compute a pixelwise correspondence between the model and the image. This allows for a fast, principled algorithm. We present promising results on difficult imagery for 3-D computed tomography images of the male pelvis for the purpose of image-guided radiotherapy of the prostate.","Image segmentation,
Biomedical imaging,
Shape,
Computed tomography,
Probability distribution,
Pixel,
Medical treatment,
Cancer,
Biomedical engineering,
Systems engineering and theory"
A robust algorithm for point set registration using mixture of Gaussians,"This paper proposes a novel and robust approach to the point set registration problem in the presence of large amounts of noise and outliers. Each of the point sets is represented by a mixture of Gaussians and the point set registration is treated as a problem of aligning the two mixtures. We derive a closed-form expression for the L/sub 2/distance between two Gaussian mixtures, which in turn leads to a computationally efficient registration algorithm. This new algorithm has an intuitive interpretation, is simple to implement and exhibits inherent statistical robustness. Experimental results indicate that our algorithm achieves very good performance in terms of both robustness and accuracy.","Gaussian processes,
Noise robustness,
Iterative closest point algorithm,
Kernel,
Closed-form solution,
Application software,
Iterative methods,
Shape,
Cost function,
Information science"
Supervised Learning of Places from Range Data using AdaBoost,"This paper addresses the problem of classifying places in the environment of a mobile robot into semantic categories. We believe that semantic information about the type of place improves the capabilities of a mobile robot in various domains including localization, path-planning, or human-robot interaction. Our approach uses AdaBoost, a supervised learning algorithm, to train a set of classifiers for place recognition based on laser range data. In this paper we describe how this approach can be applied to distinguish between rooms, corridors, doorways, and hallways. Experimental results obtained in simulation and with real robots demonstrate the effectiveness of our approach in various environments.",
"""One size fits all"": an idea whose time has come and gone","The last 25 years of commercial DBMS development can be summed up in a single phrase: ""one size fits all"". This phrase refers to the fact that the traditional DBMS architecture (originally designed and optimized for business data processing) has been used to support many data-centric applications with widely varying characteristics and requirements. In this paper, we argue that this concept is no longer applicable to the database market, and that the commercial world will fracture into a collection of independent database engines, some of which may be unified by a common front-end parser. We use examples from the stream-processing market and the data-warehouse market to bolster our claims. We also briefly discuss other markets for which the traditional architecture is a poor fit and argue for a critical rethinking of the current factoring of systems services into products.",
Robust anisotropic Gaussian fitting for volumetric characterization of Pulmonary nodules in multislice CT,"This paper proposes a robust statistical estimation and verification framework for characterizing the ellipsoidal (anisotropic) geometrical structure of pulmonary nodules in the Multislice X-ray computed tomography (CT) images. Given a marker indicating a rough location of a target, the proposed solution estimates the target's center location, ellipsoidal boundary approximation, volume, maximum/average diameters, and isotropy by robustly and efficiently fitting an anisotropic Gaussian intensity model. We propose a novel multiscale joint segmentation and model fitting solution which extends the robust mean shift-based analysis to the linear scale-space theory. The design is motivated for enhancing the robustness against margin-truncation induced by neighboring structures, data with large deviations from the chosen model, and marker location variability. A chi-square-based statistical verification and analytical volumetric measurement solutions are also proposed to complement this estimation framework. Experiments with synthetic one-dimensional and two-dimensional data clearly demonstrate the advantage of our solution in comparison with the /spl gamma/-normalized Laplacian approach (Linderberg, 1998) and the standard sample estimation approach (Matei, 2001). A quasi-real-time three-dimensional nodule characterization system is developed using this framework and validated with two clinical data sets of thin-section chest CT images. Our experiments with 1310 nodules resulted in 1) robustness against intraoperator and interoperator variability due to varying marker locations, 2) 81% correct estimation rate, 3) 3% false acceptance and 5% false rejection rates, and 4) correct characterization of clinically significant nonsolid ground-glass opacity nodules. This system processes each 33-voxel volume-of-interest by an average of 2 s with a 2.4-GHz Intel CPU. Our solution is generic and can be applied for the analysis of blob-like structures in various other applications.","Robustness,
Anisotropic magnetoresistance,
Computed tomography,
X-ray imaging,
Image analysis,
Cancer,
Image segmentation,
Image resolution,
Volume measurement,
Laplace equations"
Implementation and extensibility of an analytic placer,"Automated cell placement is a critical problem in very large scale integration (VLSI) physical design. New analytical placement methods that simultaneously spread cells and optimize wirelength have recently received much attention from both academia and industry. A novel and simple objective function for spreading cells over the placement area is described in the patent of Naylor et al. (U.S. Pat. 6301693). When combined with a wirelength objective function, this allows efficient simultaneous cell spreading and wirelength optimization using nonlinear optimization techniques. In this paper, we implement an analytic placer (APlace) according to these ideas (which have other precedents in the open literature), and conduct in-depth analysis of characteristics and extensibility of the placer. Our contributions are as follows. 1) We extend the objective functions described in (Naylor et al., U.S. Patent 6301693) with congestion information and implement a top-down hierarchical (multilevel) placer (APlace) based on them. For IBM-ISPD04 circuits, the half-perimeter wirelength of APlace outperforms that of FastPlace, Dragon, and Capo, respectively, by 7.8%, 6.5%, and 7.0% on average. For eight IBM-PLACE v2 circuits, after the placements are detail-routed using Cadence WRoute, the average improvement in final wirelength is 12.0%, 8.1%, and 14.1% over QPlace, Dragon, and Capo, respectively. 2) We extend the placer to address mixed-size placement and achieve an average of 4% wirelength reduction on ten ISPD'02 mixed-size benchmarks compared to results of the leading-edge solver, FengShui. 3) We extend the placer to perform timing-driven placement. Compared with timing-driven industry tools, evaluated by commercial detailed routing and static timing analysis, we achieve an average of 8.4% reduction in cycle time and 7.5% reduction in wirelength for a set of six industry testcases. 4) We also extend the placer to perform input/output-core coplacement and constraint handing for mixed-signal designs. Our paper aims to, and empirically demonstrates, that the APlace framework is a general, and extensible platform for ""spatial embedding"" tasks across many aspects of system physical implementation.","Very large scale integration,
Circuits,
Computer science,
Partitioning algorithms,
Annealing,
Optimization methods,
Routing,
Timing,
Performance analysis,
Testing"
Detecting application-level failures in component-based Internet services,"Most Internet services (e-commerce, search engines, etc.) suffer faults. Quickly detecting these faults can be the largest bottleneck in improving availability of the system. We present Pinpoint, a methodology for automating fault detection in Internet services by: 1) observing low-level internal structural behaviors of the service; 2) modeling the majority behavior of the system as correct; and 3) detecting anomalies in these behaviors as possible symptoms of failures. Without requiring any a priori application-specific information, Pinpoint correctly detected 89%-96% of major failures in our experiments, as compared with 20%-70% detected by current application-generic techniques.",
A general exact reconstruction for cone-beam CT via backprojection-filtration,"In this paper, we prove a generalized backprojection-filtration formula for exact cone-beam image reconstruction with an arbitrary scanning locus. Our proof is independent of the shape of the scanning locus, as long as the object is contained in a region where there is a chord through any interior point. As special cases, this generalized formula can be applied with cone-beam scanning along nonstandard spiral and saddle curves, as well as in an n-PI window setting. The algorithmic implementation and numerical results are described to support the correctness of our general claim.","Computed tomography,
Spirals,
Image reconstruction,
Radiology,
Cities and towns,
Shape,
Mathematics,
Imaging phantoms,
Biomedical imaging,
Angiography"
Power and energy profiling of scientific applications on distributed systems,"Power consumption is a troublesome design constraint for emergent systems such as IBM's BlueGene /L. If current trends continue, future petaflop systems will require 100 megawatts of power to maintain high-performance. To address this problem the power and energy characteristics of high-performance systems must be characterized. To date, power-performance profiles for distributed systems have been limited to interactive commercial workloads. However, scientific workloads are typically non-interactive (batched) processes riddled with interprocess dependences and communication. We present a framework for direct, automatic profiling of power consumption for non-interactive, parallel scientific applications on high-performance distributed systems. Though our approach is general, we use our framework to study the power-performance efficiency of the NAS parallel benchmarks on a 32-node Beowulf cluster. We provide profiles by component (CPU, memory, disk, and NIC), by node (for each of 32 nodes), and by system scale (2, 4, 8, 16, and 32 nodes). Our results indicate power profiles are often regular corresponding to application characteristics and for fixed problem size increasing the number of nodes always increases energy consumption but does not always improve performance. This finding suggests smart schedulers could be used to optimize for energy while maintaining performance.","Energy consumption,
Costs,
Earth,
Temperature,
Application software,
Kirk field collapse effect,
Laboratories,
Computer science,
Maintenance engineering,
Design engineering"
A Two-Stage Template Approach to Person Detection in Thermal Imagery,We present a two-stage template-based method to detect people in widely varying thermal imagery. The approach initially performs a fast screening procedure using a generalized template to locate potential person locations. Next an AdaBoosted ensemble classifier using automatically tuned filters is employed to test the hypothesized person locations. We demonstrate and evaluate the approach using a challenging dataset of thermal imagery,
VARS: a vehicle ad-hoc network reputation system,"Using mobile ad-hoc networks in an automotive environment (VANET) opens a new set of applications, such as the distribution of information about local traffic or road conditions. This can increase traffic safety and improve mobility. One of the main challenges is to forward event related messages in such a way that the information can be trusted by receiving nodes. Authentication does not solve the problem as it does not target the quality of messages. One promising solution might be given by reputation systems. However, conventional centralized trust establishment approaches are not well suited for use within distributed networks such as those envisioned for automotive scenarios. Therefore, we present VARS, a completely distributed approach based on reputation. Our work is based on the following assumptions: cars move at a high average speed; VANETs may become very large, in the order of thousands or even millions of nodes, so that authenticated identities are not feasible; a solution has to be completely decentralized; available bandwidth for communication remains limited, while processing power and memory continue to increase. We introduce major architecture concepts that enable VARS to operate efficiently in the given environment, present the most relevant algorithms and provide some simulation results. First results show that our scheme works in general.","Reactive power,
Vehicles,
Ad hoc networks,
Automotive engineering,
Telecommunication traffic,
Computer science,
Mobile ad hoc networks,
Application software,
Roads,
Safety"
Dimensions and principles of declassification,"Computing systems often deliberately release (or declassify) sensitive information. A principal security concern for systems permitting information release is whether this release is safe: is it possible that the attacker compromises the information release mechanism and extracts more secret information than intended? While the security community has recognised the importance of the problem, the state-of-the-art in information release is, unfortunately, a number of approaches with somewhat unconnected semantic goals. We provide a road map of the main directions of current research, by classifying the basic goals according to what information is released, who releases information, where in the system information is released, and when information can be released. With a general declassification framework as a long-term goal, we identify some prudent principles of declassification. These principles shed light on existing definitions and may also serve as useful ""sanity checks"" for emerging models.",
A Quantitative Evaluation of Video-based 3D Person Tracking,"The Bayesian estimation of 3D human motion from video sequences is quantitatively evaluated using synchronized, multi-camera, calibrated video and 3D ground truth poses acquired with a commercial motion capture system. While many methods for human pose estimation and tracking have been proposed, to date there has been no quantitative comparison. Our goal is to evaluate how different design choices influence tracking performance. Toward that end, we independently implemented two fairly standard Bayesian person trackers using two variants of particle filtering and propose an evaluation measure appropriate for assessing the quality of probabilistic tracking methods. In the Bayesian framework we compare various image likelihood functions and prior models of human motion that have been proposed in the literature. Our results suggest that in constrained laboratory environments, current methods perform quite well. Multiple cameras and background subtraction, however, are required to achieve reliable tracking suggesting that many current methods may be inappropriate in more natural settings. We discuss the implications of the study and the directions for future research that it entails","Humans,
Bayesian methods,
Biological system modeling,
Filtering,
Motion estimation,
Video sequences,
Cameras,
Image sequences,
Motion measurement,
Testing"
DynDE: a differential evolution for dynamic optimization problems,"This paper presents an approach of using differential evolution (DE) to solve dynamic optimization problems. Careful setting of parameters is necessary for DE algorithms to successfully solve optimization problems. This paper describes DynDE, a multipopulation DE algorithm developed specifically to solve dynamic optimization problems that doesn't need any parameter control strategy for the F or CR parameters. Experimental evidence has been gathered to show that this new algorithm is capable of efficiently solving the moving peaks benchmark.","Chromium,
Fluctuations,
Position measurement,
Entropy,
Mathematics,
Computer science,
Algorithm design and analysis,
Design optimization"
Sparse image coding using a 3D non-negative tensor factorization,"We introduce an algorithm for a non-negative 3D tensor factorization for the purpose of establishing a local parts feature decomposition from an object class of images. In the past, such a decomposition was obtained using non-negative matrix factorization (NMF) where images were vectorized before being factored by NMF. A tensor factorization (NTF) on the other hand preserves the 2D representations of images and provides a unique factorization (unlike NMF which is not unique). The resulting ""factors"" from the NTF factorization are both sparse (like with NMF) but also separable allowing efficient convolution with the test image. Results show a superior decomposition to what an NMF can provide on all fronts - degree of sparsity, lack of ghost residue due to invariant parts and efficiency of coding of around an order of magnitude better. Experiments on using the local parts decomposition for face detection using SVM and Adaboost classifiers demonstrate that the recovered features are discriminatory and highly effective for classification.","Image coding,
Tensile stress,
Filters,
Principal component analysis,
Convolution,
Face detection,
Face recognition,
Independent component analysis,
Computer science,
Matrix decomposition"
Interplay between intensity standardization and inhomogeneity correction in MR image processing,"Image intensity standardization is a postprocessing method designed for correcting acquisition-to-acquisition signal intensity variations (nonstandardness) inherent in magnetic resonance (MR) images. Inhomogeneity correction is a process used to suppress the low frequency background nonuniformities (inhomogeneities) of the image domain that exist in MR images. Both these procedures have important implications in MR image analysis. The effects of these postprocessing operations on improvement of image quality in isolation has been well documented. However, the combined effects of these two processes on MR images and how the processes influence each other have not been studied thus far. In this paper, we evaluate the effect of inhomogeneity correction followed by standardization and vice-versa on MR images in order to determine the best sequence to follow for enhancing image quality. We conducted experiments on several clinical and phantom data sets (nearly 4000 three-dimensional MR images were analyzed) corresponding to four different MRI protocols. Different levels of artificial nonstandardness, and different models and levels of artificial background inhomogeneity were used in these experiments. Our results indicate that improved standardization can be achieved by preceding it with inhomogeneity correction. There is no statistically significant difference in image quality obtained between the results of standardization followed by correction and that of correction followed by standardization from the perspective of inhomogeneity correction. The correction operation is found to bias the effect of standardization. We demonstrate this bias both qualitatively and quantitatively by using two different methods of inhomogeneity correction. We also show that this bias in standardization is independent of the specific inhomogeneity correction method used. The effect of this bias due to correction was also seen in magnetization transfer ratio (MTR) images, which are naturally endowed with the standardness property. Standardization, on the other hand, does not seem to influence the correction operation. It is also found that longer sequences of repeated correction and standardization operations do not considerably improve image quality. These results were found to hold for the clinical and the phantom data sets, for different MRI protocols, for different levels of artificial nonstandardness, for different models and levels of artificial inhomogeneity, for different correction methods, and for images that were endowed with inherent standardness as well as for those that were standardized by using the intensity standardization method. Overall, we conclude that inhomogeneity correction followed by intensity standardization is the best sequence to follow from the perspective of both image quality and computational efficiency.","Standardization,
Image processing,
Image quality,
Imaging phantoms,
Magnetic resonance imaging,
Protocols,
Design methodology,
Signal design,
Magnetic resonance,
Frequency"
Modeling UMTS discontinuous reception mechanism,"This paper investigates the discontinuous reception (DRX) mechanism of universal mobile telecommunications system (UMTS). DRX is exercised between the network and a mobile station (MS) to save the power of the MS. The DRX mechanism is controlled by two parameters: the inactivity timer threshold and the DRX cycle. Analytic and simulation models are proposed to study the effects of these two parameters on output measures including the expected queue length, the expected packet waiting time, and the power saving factor. Our study quantitatively shows how to select appropriate inactivity timer and DRX cycle values for various traffic patterns.","3G mobile communication,
Computer science,
Telecommunication traffic,
Energy consumption,
IEEE news,
Power system modeling,
Data communication,
Asynchronous transfer mode,
Multiaccess communication,
Councils"
Buffer sizes for large multiplexers: TCP queueing theory and instability analysis,"In large multiplexers with many TCP flows, the aggregate traffic flow behaves predictably; this is a basis for the fluid model of Misra, Gong and Towsley V. Misra et al., (2000) and for a growing literature on fluid models of congestion control. In this paper we argue that different fluid models arise from different buffer-sizing regimes. We consider the large buffer regime (buffer size is bandwidth-delay product), an intermediate regime (divide the large buffer size by the square root of the number of flows), and the small buffer regime (buffer size does not depend on number of flows). Our arguments use various techniques from queueing theory. We study the behaviour of these fluid models (on a single bottleneck Kink, for a collection of identical long-lived flows). For what parameter regimes is the fluid model stable, and when it is unstable what is the size of oscillations and the impact on goodput? Our analysis uses an extension of the Poincare-Linstedt method to delay-differential equations. We find that large buffers with drop-tail have much the same performance as intermediate buffers with either drop-tail or AQM; that large buffers with RED are better at least for window sizes less than 20 packets; and that small buffers with either drop-tail or AQM are best over a wide range of window sizes, though the buffer size must be chosen carefully. This suggests that buffer sizes should be much much smaller than is currently recommended.",
Multiround algorithms for scheduling divisible loads,"Divisible load applications occur in many fields of science and engineering and can be easily parallelized in a master-worker fashion, but pose several scheduling challenges. While a number of approaches have been proposed that allocate load to workers in a single round, using multiple rounds improves overlap of computation with communication. Unfortunately, multiround algorithms are difficult to analyze and have thus received only limited attention. In this paper, we answer three open questions in the multiround divisible load scheduling area: 1) how to account for latencies, 2) how to account for heterogeneous platforms, and 3) how many rounds should be used. To answer 1), we derive the first closed-form optimal schedule for a homogeneous platform with both computation and communication latencies, for a given number of rounds. To answer 2) and 3), we present a novel algorithm, UMR. We evaluate UMR in a variety of realistic scenarios.","Scheduling algorithm,
Processor scheduling,
Optimal scheduling,
Delay,
Grid computing,
Costs,
Algorithm design and analysis,
Computer applications,
Concurrent computing,
Computational efficiency"
Imaging performance of a-PET: a small animal PET camera,"The evolution of positron emission tomography (PET) imaging for small animals has led to the development of dedicated PET scanner designs with high resolution and sensitivity. The animal PET scanner achieves these goals for imaging small animals such as mice and rats. The scanner uses a pixelated Anger-logic detector for discriminating 2 /spl times/ 2 /spl times/ 10 mm/sup 3/ crystals with 19-mm-diameter photomultiplier tubes. With a 19.7-cm ring diameter, the scanner has an axial length of 11.9 cm and operates exclusively in three-dimensional imaging mode, leading to very high sensitivity. Measurements show that the scanner design achieves a spatial resolution of 1.9 mm at the center of the field-of-view. Initially designed with gadolinium orthosilicate but changed to lutetium-yttrium orthosilicate, the scanner now achieves a sensitivity of 3.6% for a point source at the center of the field-of-view with an energy window of 250-665 keV. Iterative image reconstruction, together with accurate data corrections for scatter, random, and attenuation, are incorporated to achieve high-quality images and quantitative data. These results are demonstrated through our contrast recovery measurements as well as sample animal studies.",
Semantics-based dynamic service composition,"Complex services may be dynamically composed through combining distributed components on demand (i.e., when requested by a user) in order to provide new services without preinstallation. Several systems have been proposed to dynamically compose services. However, they require users to request services in a manner that is not intuitive to the users. In order to allow a user to request a service in an intuitive form (e.g., using a natural language), this paper proposes a semantics-based service composition architecture. The proposed architecture obtains the semantics of the service requested in an intuitive form, and dynamically composes the requested service based on the semantics of the service. To compose a service based on its semantics, the proposed architecture supports semantic representation of components [through a component model named Component Service Model with Semantics (CoSMoS)], discovers components required to compose a service [through a middleware named Component Runtime Environment (CoRE)], and composes the requested service based on its semantics and the semantics of the discovered components [through a service composition mechanism named Semantic Graph-Based Service Composition (SeGSeC)]. This paper presents the design, implementation and empirical evaluation of the proposed architecture.",
Predicting the number of fatal soft errors in Los Alamos national laboratory's ASC Q supercomputer,"Early in the deployment of the Advanced Simulation and Computing (ASC) Q supercomputer, a higher-than-expected number of single-node failures was observed. The elevated rate of single-node failures was hypothesized to be caused primarily by fatal soft errors, i.e., board-level cache (B-cache) tag (BTAG) parity errors caused by cosmic-ray-induced neutrons that led to node crashes. A series of experiments was undertaken at the Los Alamos Neutron Science Center (LANSCE) to ascertain whether fatal soft errors were indeed the primary cause of the elevated rate of single-node failures. Observed failure data from Q are consistent with the results from some of these experiments. Mitigation strategies have been developed, and scientists successfully use Q for large computations in the presence of fatal soft errors and other single-node failures.","Laboratories,
Supercomputers,
Error correction codes,
Computer errors,
Neutrons,
Computational modeling,
Life testing,
Semiconductor device testing,
Random access memory,
Runtime"
Exploring the space of a human action,"One of the fundamental challenges of recognizing actions is accounting for the variability that arises when arbitrary cameras capture humans performing actions. In this paper, we explicitly identify three important sources of variability: (1) viewpoint, (2) execution rate, and (3) anthropometry of actors, and propose a model of human actions that allows us to investigate all three. Our hypothesis is that the variability associated with the execution of an action can be closely approximated by a linear combination of action bases in joint spatio-temporal space. We demonstrate that such a model bounds the rank of a matrix of image measurements and that this bound can be used to achieve recognition of actions based only on imaged data. A test employing principal angles between subspaces that is robust to statistical fluctuations in measurement data is presented to find the membership of an instance of an action. The algorithm is applied to recognize several actions, and promising results have been obtained.","Space exploration,
Humans,
Cameras,
Computer vision,
Anthropometry,
Object recognition,
Laboratories,
Computer science,
Linear approximation,
Image recognition"
A wireless sensor network for structural health monitoring: performance and experience,"While sensor network research has made significant strides in the past few years, the literature has relatively few examples of papers that have evaluated and validated a complete experimental system. In this paper, we discuss our deployment experiences and evaluate the performance of a multi-hop wireless data acquisition system (called Wisden) for structural health monitoring (SHM) on a large seismic test structure used by civil engineers. Our experiments indicate that, with the latest sensor network hardware, Wisden can reliably deliver time-synchronized tri-axial structural vibration data reliably across multiple hops with low latencies for sampling rates up to 200 Hz. This performance was achieved by iteratively refining the system design using a series of test deployments. Our experiences suggested the need for careful onset detection in order to preserve the fidelity of the structure's frequency response. Furthermore, the high damping characteristics of large structures motivated an exploration of the processing, sampling, and communication limits of current platforms.","Wireless sensor networks,
Data acquisition,
Frequency response,
System testing,
Sampling methods,
Sensor systems,
Computerized monitoring,
Computer science,
Civil engineering,
Data engineering"
The bi-elliptical deformable contour and its application to automated tongue segmentation in Chinese medicine,"Automated tongue image segmentation, in Chinese medicine, is difficult due to two special factors: 1) there are many pathological details on the surface of the tongue, which have a large influence on edge extraction; 2) the shapes of the tongue bodies captured from various persons (with different diseases) are quite different, so they are impossible to describe properly using a predefined deformable template. To address these problems, in this paper, we propose an original technique that is based on a combination of a bi-elliptical deformable template (BEDT) and an active contour model, namely the bi-elliptical deformable contour (BEDC). The BEDT captures gross shape features by using the steepest decent method on its energy function in the parameter space. The BEDC is derived from the BEDT by substituting template forces for classical internal forces, and can deform to fit local details. Our algorithm features fully automatic interpretation of tongue images and a consistent combination of global and local controls via the template force. We apply the BEDC to a large set of clinical tongue images and present experimental results.","Tongue,
Image segmentation,
Biomedical imaging,
Shape,
Pathology,
Diseases,
Active contours,
Deformable models,
Energy capture,
Automatic control"
Synthesis of High-Performance Parallel Programs for a Class of ab Initio Quantum Chemistry Models,"This paper provides an overview of a program synthesis system for a class of quantum chemistry computations. These computations are expressible as a set of tensor contractions and arise in electronic structure modeling. The input to the system is a a high-level specification of the computation, from which the system can synthesize high-performance parallel code tailored to the characteristics of the target architecture. Several components of the synthesis system are described, focusing on performance optimization issues that they address.","Chemistry,
Physics computing,
Tensile stress,
Concurrent computing,
Quantum computing,
Optimizing compilers,
Computer science,
Computer architecture,
Laboratories,
Quantum mechanics"
Wavelet-based cascaded adaptive filter for removing baseline drift in pulse waveforms,This paper presents an energy ratio-based method and a wavelet-based cascaded adaptive filter (CAF) for detecting and removing baseline drift from pulse waveforms. Experiments on 50 simulated and five hundred real pulse signals demonstrate that this CAF outperforms traditional filters both in removing baseline drift and in preserving the diagnostic information of pulse waveforms.,
Facial Action Unit Detection using Probabilistic Actively Learned Support Vector Machines on Tracked Facial Point Data,"A system that could enable fast and robust facial expression recognition would have many applications in behavioral science, medicine, security and human-machine interaction. While working toward that goal, we do not attempt to recognize prototypic facial expressions of emotions but analyze subtle changes in facial behavior by recognizing facial muscle action units (AUs, i.e., atomic facial signals) instead. By detecting AUs we can analyse many more facial communicative signals than emotional expressions alone. This paper proposes AU detection by classifying features calculated from tracked ?ducial facial points. We use a Particle Filtering tracking scheme using factorized likelihoods and a novel observation model that combines a rigid and a morphologic model. The AUs displayed in a video are classi?ed using Probabilistic Actively Learned Support VectorMachines (PAL-SVM).When tested on 167 videos from the MMI web-based facial expression database, the proposed method achieved very high recognition rates for 16 different AUs. To ascertain data independency we also performed a validation using another benchmark database. When trained on the MMI-Facial expression database and tested on the Cohn-Kanade database, the proposed method achieved a recognition rate of 84% when detecting 9 AUs occurring alone or in combination in input image sequences.","Face detection,
Support vector machines,
Face recognition,
Image databases,
Emotion recognition,
Signal analysis,
Testing,
Robustness,
Behavioral science,
Biomedical imaging"
Virtual angiography for visualization and validation of computational models of aneurysm hemodynamics,"It has recently become possible to simulate aneurysmal blood flow dynamics in a patient-specific manner via the coupling of three-dimensional (3-D) X-ray angiography and computational fluid dynamics (CFD). Before such image-based CFD models can be used in a predictive capacity, however, it must be shown that they indeed reproduce the in vivo hemodynamic environment. Motivated by the fact that there are currently no techniques for adequately measuring complex blood velocity fields in vivo, in this paper we describe how cine X-ray angiograms may be simulated for the purpose of indirectly validating patient-specific CFD models. Mimicking the radiological procedure, a virtual angiogram is constructed by first simulating the time-varying injection of contrast agent into a precomputed, patient-specific CFD model. A time-series of images is then constructed by simulating the attenuation of X-rays through the computed 3-D contrast-agent flow dynamics. Virtual angiographic images and residence time maps, here derived from an image-based CFD model of a giant aneurysm, are shown to be in excellent agreement with the corresponding clinical images and residence time maps, but only when the interaction between the quasisteady contrast agent injection and the pulsatile flow are properly accounted for. These virtual angiographic techniques pave the way for validating image-based CFD models against routinely available clinical data, and provide a means of visualizing complex, 3-D blood flow dynamics in a clinically relevant manner. They also clearly show how the contrast agent injection perturbs the normal blood flow patterns, further highlighting the potential utility of image-based CFD as a window into the true aneurysmal hemodynamics.",
Exploring the energy-time tradeoff in MPI programs on a power-scalable cluster,"Recently, energy has become an important issue in high-performance computing. For example, supercomputers that have energy in mind, such as BlueGene/L, have been built; the idea is to improve the energy efficiency of nodes. Our approach, which uses off-the-shelf, high-performance cluster nodes that are frequency scalable, allows energy saving by scaling down the CPU. This paper investigates the energy consumption and execution time of applications from a standard benchmark suite (NAS) on a power-scalable cluster. We study via direct measurement and simulation both intra-node and inter-node effects of memory and communication bottlenecks, respectively. Additionally, we compare energy consumption and execution time across different numbers of nodes. Our results show that a power-scalable cluster has the potential to save energy by scaling the processor down to lower energy levels. Furthermore, we found that for some programs, it is possible to both consume less energy and execute in less time when using a larger number of nodes, each at reduced energy. Additionally, we developed and validated a model that enables us to predict the energy-time tradeoff of larger clusters.",
Heterogeneity and load balance in distributed hash tables,"Existing solutions to balance load in DHTs incur a high overhead either in terms of routing state or in terms of load movement generated by nodes arriving or departing the system. In this paper, we propose a set of general techniques and use them to develop a protocol based on Chord, called Y/sub 0/, that achieves load balancing with minimal overhead under the typical assumption that the load is uniformly distributed in the identifier space. In particular, we prove that Y/sub 0/ can achieve near-optimal load balancing, while moving little load to maintain the balance and increasing the size of the routing tables by at most a constant factor. Using extensive simulations based on real-world and synthetic capacity distributions, we show that Y/sub 0/ reduces the load imbalance of Chord from O(log n) to a less than 3.6 without increasing the number of links that a node needs to maintain. In addition, we study the effect of heterogeneity on both DHTs, demonstrating significantly reduced average route length as node capacities become increasingly heterogeneous. For a real-word distribution of node capacities, the route length in Y/sub 0/ is asymptotically less than half the route length in the case of a homogeneous system.","Peer to peer computing,
Intrusion detection,
Load management,
Computer science,
Query processing,
Identity management systems,
Routing protocols,
Discrete wavelet transforms,
Costs"
Future interconnection environment,"The emergence of the Web provided an unprecedented AI research and application platform. By providing access to human-readable content stored in any computer connected to the Internet, it revolutionized business, scientific research, government, and public information services around the globe. Networks pervade nature, society, and virtual worlds, giving structure and function to a variety of resources and behaviors. Discovering the rules that govern the future interconnection environment is a major challenge. The China Knowledge Grid Research Group, is exploring the operating principles of this future interconnection environment.","Application software,
Artificial intelligence,
Computer network management,
Machine intelligence,
Semantic Web,
Distributed computing,
Computer networks,
Intelligent networks,
Web and internet services,
Web pages"
Beyond pairwise clustering,"We consider the problem of clustering in domains where the affinity relations are not dyadic (pairwise), but rather triadic, tetradic or higher. The problem is an instance of the hypergraph partitioning problem. We propose a two-step algorithm for solving this problem. In the first step we use a novel scheme to approximate the hypergraph using a weighted graph. In the second step a spectral partitioning algorithm is used to partition the vertices of this graph. The algorithm is capable of handling hyperedges of all orders including order two, thus incorporating information of all orders simultaneously. We present a theoretical analysis that relates our algorithm to an existing hypergraph partitioning algorithm and explain the reasons for its superior performance. We report the performance of our algorithm on a variety of computer vision problems and compare it to several existing hypergraph partitioning algorithms.",
"Combining evidence from source, suprasegmental and spectral features for a fixed-text speaker verification system","This paper proposes a text-dependent (fixed-text) speaker verification system which uses different types of information for making a decision regarding the identity claim of a speaker. The baseline system uses the dynamic time warping (DTW) technique for matching. Detection of the end-points of an utterance is crucial for the performance of the DTW-based template matching. A method based on the vowel onset point (VOP) is proposed for locating the end-points of an utterance. The proposed method for speaker verification uses the suprasegmental and source features, besides spectral features. The suprasegmental features such as pitch and duration are extracted using the warping path information in the DTW algorithm. Features of the excitation source, extracted using the neural network models, are also used in the text-dependent speaker verification system. Although the suprasegmental and source features individually may not yield good performance, combining the evidence from these features seem to improve the performance of the system significantly. Neural network models are used to combine the evidence from multiple sources of information.","Speaker recognition,
Data mining,
Neural networks,
Speech recognition,
Shape,
Information resources,
System testing,
Computer science,
Information systems,
Natural languages"
Adaptive dynamic collision checking for single and multiple articulated robots in complex environments,"Static collision checking amounts to testing a given configuration of objects for overlaps. In contrast, the goal of dynamic checking is to determine whether all configurations along a continuous path are collision-free. While there exist effective methods for static collision detection, dynamic checking still lacks methods that are both reliable and efficient. A common approach is to sample paths at some fixed, prespecified resolution and statically test each sampled configuration. But this approach is not guaranteed to detect collision whenever one occurs, and trying to increase its reliability by refining the sampling resolution along the entire path results in slow checking. This paper introduces a new method for testing path segments in c-space or collections of such segments, that is both reliable and efficient. This method locally adjusts the sampling resolution by comparing lower bounds on distances between objects in relative motion with upper bounds on lengths of curves traced by points of these moving objects. Several additional techniques and heuristics increase the checker's efficiency in scenarios with many moving objects (e.g., articulated arms and/or multiple robots) and high geometric complexity. The new method is general, but particularly well suited for use in probabilistic roadmap (PRM) planners, where it is critical to determine as quickly as possible whether given path segments collide, or not. Extensive tests, in particular on randomly generated path segments and on multisegment paths produced by PRM planners, show that the new method compares favorably with a fixed-resolution approach at ""suitable"" resolution, with the enormous advantage that it never fails to detect collision.","Robots,
Testing,
Sampling methods,
Road accidents,
Motion planning,
Laboratories,
Computer science,
Upper bound,
Arm,
Graphics"
Alignment of sparse freehand 3-D ultrasound with preoperative images of the liver using models of respiratory motion and deformation,"We present a method for alignment of an interventional plan to optically tracked two-dimensional intraoperative ultrasound (US) images of the liver. Our clinical motivation is to enable the accurate transfer of information from three-dimensional (3D) preoperative imaging modalities [magnetic resonance (MR) or computed tomography (CT)] to intraoperative US to aid needle placement for thermal ablation of liver metastases. An initial rigid registration to intraoperative coordinates is obtained using a set of US images acquired at maximum exhalation. A preprocessing step is applied to both the preoperative images and the US images to produce evidence of corresponding structures. This yields two sets of images representing classification of regions as vessels. The registration then proceeds using these images. The preoperative images and plan are then warped to correspond to a single US slice acquired at an unknown point in the breathing cycle where the liver is likely to have moved and deformed relative to the preoperative image. Alignment is constrained using a patient-specific model of breathing motion and deformation. Target registration error is estimated by carrying out simulation experiments using resliced MR volumes to simulate real US and comparing the registration results to a ""bronze-standard"" registration performed on the full MR volume. Finally, the system is tested using real US and verified using visual inspection.","Ultrasonic imaging,
Liver,
Deformable models,
Computed tomography,
Optical imaging,
Magnetic resonance imaging,
Magnetic resonance,
Needles,
Metastasis,
System testing"
Graphical model architectures for speech recognition,"This article discusses the foundations of the use of graphical models for speech recognition as presented in J. R. Deller et al. (1993), X. D. Huang et al. (2001), F. Jelinek (19970, L. R. Rabiner and B. -H. Juang (1993) and S. Young et al. (1990) giving detailed accounts of some of the more successful cases. Our discussion employs dynamic Bayesian networks (DBNs) and a DBN extension using the Graphical Model Toolkit's (GMTK's) basic template, a dynamic graphical model representation that is more suitable for speech and language systems. While this article concentrates on speech recognition, it should be noted that many of the ideas presented here are also applicable to natural language processing and general time-series analysis.","Graphical models,
Speech recognition,
Random variables,
Probability distribution,
Speech processing,
Natural languages,
Bayesian methods,
Computer architecture,
Computer science,
Social network services"
Active application oriented vertical handoff in next-generation wireless networks,"The coexistence of heterogeneous wireless networks providing service anywhere at anytime is an inevitable trend in the development of next-generation wireless data networks. Vertical handoff is the switching of the mobile terminal (MT) among different types of wireless networks. How and when to carry out vertical handoff directly affects the performance and quality of network services. In this paper, we propose a novel vertical handoff scheme in which the MT can request and initiate the handoff actively, contrary to other schemes where the MTs participate passively during the handoff process. Our active application oriented scheme provides an efficient interface management for multi-interface MTs to reduce the power consumption caused by unnecessary interface activation. By treating the application running in the MT as the main vertical handoff decision factor, the proposed scheme is able to switch the MT at the right time to the most suitable network to minimize the waste of network resources. Finally, simulation results are presented to show the improved performance over passive schemes.","Intelligent networks,
Next generation networking,
Wireless networks,
Switches,
Application software,
Energy consumption,
Quality of service,
Computer science,
Energy management,
Base stations"
Large margin nearest neighbor classifiers,"The nearest neighbor technique is a simple and appealing approach to addressing classification problems. It relies on the assumption of locally constant class conditional probabilities. This assumption becomes invalid in high dimensions with a finite number of examples due to the curse of dimensionality. Severe bias can be introduced under these conditions when using the nearest neighbor rule. The employment of a locally adaptive metric becomes crucial in order to keep class conditional probabilities close to uniform, thereby minimizing the bias of estimates. We propose a technique that computes a locally flexible metric by means of support vector machines (SVMs). The decision function constructed by SVMs is used to determine the most discriminant direction in a neighborhood around the query. Such a direction provides a local feature weighting scheme. We formally show that our method increases the margin in the weighted space where classification takes place. Moreover, our method has the important advantage of online computational efficiency over competing locally adaptive techniques for nearest neighbor classification. We demonstrate the efficacy of our method using both real and simulated data.","Nearest neighbor searches,
Support vector machines,
Support vector machine classification,
Error analysis,
Employment,
Computational efficiency,
Computational modeling,
Engineering profession,
Software engineering,
Computer science"
Security for Grids,"Securing a Grid environment presents a distinctive set of challenges. This work groups the activities that need to be secured into four categories: naming and authentication; secure communication; trust, policy, and authorization; and enforcement of access control. It examines the current state of the art in securing these activities and introduces new technologies that promise to meet the security requirements of Grids more completely.","Security,
Grid computing,
Authorization,
Computer science,
Resource management,
Control systems,
Authentication,
Access control,
Communication system control,
Collaborative work"
REPLICA: a bitstream manipulation filter for module relocation in partial reconfigurable systems,"The feature of partial reconfiguration provided by currently available field programmable gate arrays (FPGAs) makes it possible to change hardware modules while others keep working. The combination of this feature and the high gate capacity enables the integration of dynamic systems that can be adapted to changing demands during runtime. Placing the dynamically changing modules along a horizontal communication infrastructure does not only provide communication facilities it also enables the relocation of pre-synthesized modules by bitstream manipulations. The exact placement of an incoming module is determined according to the current resource allocation, which results in an online placement problem. In order to prevent any extra configuration overhead for the relocation process, we developed the REPLICA (relocation per online configuration alteration) filter, which is capable of performing the necessary bitstream manipulations during the regular download process. The filter architecture, a configuration manager and an evaluation example are presented in this paper.","Filters,
Field programmable gate arrays,
Resource management,
Runtime,
Computer architecture,
Software tools,
Power generation,
Computer science,
Software engineering,
Road transportation"
"Behavioral diversity, choices and noise in the iterated prisoner's dilemma","Real-world dilemmas rarely involve just two choices and perfect interactions without mistakes. In the iterated prisoner's dilemma (IPD) game, intermediate choices or mistakes (noise) have been introduced to extend its realism. This paper studies the IPD game with both noise and multiple levels of cooperation (intermediate choices) in a coevolutionary environment, where players can learn and adapt their strategies through an evolutionary algorithm. The impact of noise on the evolution of cooperation is first examined. It is shown that the coevolutionary models presented in this paper are robust against low noise (when mistakes occur with low probability). That is, low levels of noise have little impact on the evolution of cooperation. On the other hand, high noise (when mistakes occur with high probability) creates misunderstandings and discourages cooperation. However, the evolution of cooperation in the IPD with more choices in a coevolutionary learning setting also depends on behavioral diversity. This paper further investigates the issue of behavioral diversity in the coevolution of strategies for the IPD with more choices and noise. The evolution of cooperation is more difficult to achieve if a coevolutionary model with low behavioral diversity is used for IPD games with higher levels of noise. The coevolutionary model with high behavioral diversity in the population is more resistant to noise. It is shown that strategy representations can have a significant impact on the evolutionary outcomes because of different behavioral diversities that they generate. The results further show the importance of behavioral diversity in coevolutionary learning.",
An autonomous earth-observing sensorWeb,"We discuss the various features of the Earth-observing sensorWeb developed by the Jet Propulsion Laboratory and Goddard Space Flight Center, which provide the key science data about eruptions within hours for the volcanologists around the world. Onboard AI software evaluates the request, orients the spacecraft, and operates the science instruments to acquire high-resolution images with hyperspectral data for science analysis.","Space technology,
Instruments,
Event detection,
Space missions,
Intelligent sensors,
Geoscience,
Propulsion,
Laboratories,
Intelligent systems,
Aerospace engineering"
Structured light in scattering media,"Virtually all structured light methods assume that the scene and the sources are immersed in pure air and that light is neither scattered nor absorbed. Recently, however, structured lighting has found growing application in underwater and aerial imaging, where scattering effects cannot be ignored. In this paper, we present a comprehensive analysis of two representative methods - light stripe range scanning and photometric stereo - in the presence of scattering. For both methods, we derive physical models for the appearances of a surface immersed in a scattering medium. Based on these models, we present results on (a) the condition for object detectability in light striping and (b) the number of sources required for photometric stereo. In both cases, we demonstrate that while traditional methods fail when scattering is significant, our methods accurately recover the scene (depths, normals, albedos) as well as the properties of the medium. These results are in turn used to restore the appearances of scenes as if they were captured in clear air. Although we have focused on light striping and photometric stereo, our approach can also be extended to other methods such as grid coding, gated and active polarization imaging.","Light scattering,
Optical scattering,
Layout,
Photometry,
Optical imaging,
Surface reconstruction,
Optical attenuators,
Sun,
Robots,
Computer science"
Shapelets correlated with surface normals produce surfaces,"This paper addresses the problem of deducing the surface shape of an object given just the surface normals. Many shape measurement algorithms such as shape from shading and shape from texture only return the surface normals of an object, often with an ambiguity of pi in the surface tilt. The surface shape has to be inferred from these normals, typically via some integration process. However; reconstruction through the integration of surface gradients is sensitive to noise and the choice of integration paths across the surface. In addition, existing techniques cannot accommodate ambiguities in tilt. This paper presents a new approach to the reconstruction of surfaces from surface normals using basis functions, referred to here as shapelets. The surface gradients of the shapelets are correlated with the gradients of the surface and the correlations summed to form the reconstruction. This results in a simple reconstruction process that is very robust to noise. Where there is an ambiguity of it in the surface tilt, reconstructions of reduced quality are still possible up to a positive/negative shape ambiguity. Intriguingly, some form of reconstruction is also possible using just slant information","Surface reconstruction,
Noise shaping,
Shape measurement,
Surface texture,
Noise robustness,
Computer vision,
Stability,
Computer science,
Software engineering,
Australia"
On Learning Vector-Valued Functions,"In this letter, we provide a study of learning in a Hilbert space of vector-valued functions. We motivate the need for extending learning theory of scalar-valued functions by practical considerations and establish some basic results for learning vector-valued functions that should prove useful in applications. Specifically, we allow an output space Y to be a Hilbert space, and we consider a reproducing kernel Hilbert space of functions whose values lie in Y. In this setting, we derive the form of the minimal norm interpolant to a finite set of data and apply it to study some regularization functionals that are important in learning theory. We consider specific examples of such functionals corresponding to multiple-output regularization networks and support vector machines, for both regression and classification. Finally, we provide classes of operator-valued kernels of the dot product and translation-invariant type.",
Dynamic taint propagation for Java,"Improperly validated user input is the underlying root cause for a wide variety of attacks on Web-based applications. Static approaches for detecting this problem help at the time of development, but require source code and report a number of false positives. Hence, they are of little use for securing fully deployed and rapidly evolving applications. We propose a dynamic solution that tags and tracks user input at runtime and prevents its improper use to maliciously affect the execution of the program. Our implementation can be transparently applied to Java classfiles, and does not require source code. Benchmarks show that the overhead of this runtime enforcement is negligible and can prevent a number of attacks",
Approximation algorithms for two optimal location problems in sensor networks,"This paper study two problems that arise in optimization of sensor networks: First, we devise provable approximation schemes for locating a base station and constructing a network among a set of sensors each of which has a data stream to get to the base station. Subject to power constraints at the sensors, our goal is to locate the base station and establish a network in order to maximize the lifespan of the network. Second, we study optimal sensor placement problems for quality coverage of given domains cluttered with obstacles. We assume ""line-of-site"", sensors, that sense a point only if the straight segment connecting the sensor to this point (the ""line-of-site"") does not cross any obstacle. so obstacles occludes area from using line-of-site sensors, the goal is to minimize the number of sensors required in order to have each point ""well covered"" according to precise criteria (e.g., that each point is seen by two sensors that form at least angle a, or that each point is seen by three sensors that form a triangle containing the point).","Approximation algorithms,
Intelligent networks,
Wireless sensor networks,
Base stations,
Batteries,
Robustness,
Joining processes,
Streaming media,
Computer science,
Engineering profession"
Nonphotolithographic nanoscale memory density prospects,"Technologies are now emerging to construct molecular-scale electronic wires and switches using bottom-up self-assembly. This opens the possibility of constructing nanoscale circuits and memories where active devices are just a few nanometers square and wire pitches may be on the order of ten nanometers. The features can be defined at this scale without using photolithography. The available assembly techniques have relatively high defect rates compared to conventional lithographic integrated circuits and can only produce very regular structures. Nonetheless, with proper memory organization, it is reasonable to expect these technologies to provide memory densities in excess of 10/sup 11/ b/cm/sup 2/ with modest active power requirements under 0.6 W/Tb/s for random read operations.",
Mode Characteristics of radio-frequency atmospheric glow discharges,"Building on recent experimental and numerical evidence of different glow modes in atmospheric plasmas, this paper reports a systematic study of these modes in radio-frequency (RF) glow discharges in atmospheric helium. Using a one-dimensional (1-D) hybrid computer model, we present detailed characterization of three glow modes, namely the /spl alpha/ mode, the /spl alpha/-/spl gamma/ transitional mode, and the /spl gamma/-mode in a 13.56-MHz atmospheric glow discharge over a wide range of root mean square (RMS) current density from 5 mA/cm/sup 2/ to 110 mA/cm/sup 2/. Our focus is on sheath dynamics through spatial and temporal profiles of charged densities, electric field, electron mean energy, sheath thickness, and sheath voltage, and when appropriate our results are compared against experimental data of atmospheric glow discharges and that of glow discharges at reduced gas pressure below 1 torr. Fundamental characteristics of the three glow modes are shown to be distinctively different, and these can be used as a hitherto unavailable route to tailor the operation of radio-frequency atmospheric glow discharges to their intended applications.","Radio frequency,
Glow discharges,
Plasma applications,
Plasma properties,
Electrons,
Plasma stability,
Voltage,
Plasma measurements,
Atmospheric modeling"
Fluorescent protein tomography scanner for small animal imaging,"Microscopy of fluorescent proteins has enabled unprecedented insights into visualizing gene expression in living systems. Imaging deeper into animals, however, has been limited due to the lack of accurate imaging methods for the visible. We present a novel system designed to perform tomographic imaging of fluorescent proteins through whole animals. The tomographic method employed a multiangle, multiprojection illumination scheme, while detection was achieved using a highly sensitive charge-coupled device camera with appropriate filters. Light propagation was modeled using a modified solution to the diffusion equation to account for the high absorption and high scattering of tissue at the visible wavelengths. We show that the technique can quantitatively detect fluorescence with sub millimeter spatial resolution both in phantoms and in tissues. We conclude that the method could be applied in tomographic imaging of fluorescent proteins for in vivo targeting of different diseases and abnormalities.","Fluorescence,
Proteins,
Tomography,
Animals,
Optical imaging,
Microscopy,
Visualization,
Gene expression,
Lighting,
Cameras"
Differential evolution methods for unsupervised image classification,"A clustering method that is based on differential evolution is developed in this paper. The algorithm finds the centroids of a user specified number of clusters, where each cluster groups together similar patterns. The application of the proposed clustering algorithm to the problem of unsupervised classification and segmentation of images is investigated. To illustrate its wide applicability, the proposed algorithm is then applied to synthetic, MRI and satellite images. Experimental results show that the differential evolution clustering algorithm performs very well compared to other state-of-the-art clustering algorithms in all measured criteria. Additionally, the paper presents a different formulation to the multi-objective fitness function to eliminate the need to tune objective weights. A gbest DE is also proposed with encouraging results.","Image classification,
Clustering algorithms,
Partitioning algorithms,
Iterative algorithms,
Image analysis,
Image segmentation,
Pixel,
Computer science,
Africa,
Clustering methods"
MiNT: a miniaturized network testbed for mobile wireless research,"Most mobile wireless networking research today relies on simulations. However, fidelity of simulation results has always been a concern, especially when the protocols being studied are affected by the propagation and interference characteristics of the radio channels. Inherent difficulty in faithfully modeling the wireless channel characteristics has encouraged several researchers to build wireless network testbeds. A full-fledged wireless testbed is spread over a large physical space because of the wide coverage area of radio signals. This makes a large-scale testbed difficult and expensive to set up, configure, and manage. This paper describes a miniaturized 802.11b-based, multi-hop wireless network testbed called MiNT. MiNT occupies a significantly small space, and dramatically reduces the efforts required in setting up a multi-hop wireless network used for wireless application/protocol testing and evaluation. MiNT is also a hybrid simulation platform that can execute ns-2 simulation scripts with the link, MAC and physical layer in the simulator replaced by real hardware. We demonstrate the fidelity of MiNT by comparing experimental results on it with similar experiments conducted on a non-miniaturized testbed. We also compare the results of experiments conducted using hybrid simulation on MiNT with those obtained using pure simulation. Finally, using a case study we show the usefulness of MiNT in wireless application testing and evaluation.","Testing,
Wireless networks,
Computational modeling,
Wireless application protocol,
Routing protocols,
Access protocols,
Spread spectrum communication,
Radio propagation,
Computer science,
USA Councils"
Comparing Feature Sets for Acted and Spontaneous Speech in View of Automatic Emotion Recognition,"We present a data-mining experiment on feature selection for automatic emotion recognition. Starting from more than 1000 features derived from pitch, energy and MFCC time series, the most relevant features in respect to the data are selected from this set by removing correlated features. The features selected for acted and realistic emotions are analyzed and show significant differences. All features are computed automatically and we also contrast automatically with manually units of analysis. A higher degree of automation did not prove to be a disadvantage in terms of recognition accuracy","Emotion recognition,
Feature extraction,
Mel frequency cepstral coefficient,
Speech recognition,
Automation,
Time measurement,
Statistics,
Natural languages,
Application software,
Computer science"
CRAWDAD: A Community Resource for Archiving Wireless Data at Dartmouth,"Wireless network researchers are seriously starved for data about how real users, applications, and devices use real networks under real network conditions. CRAWDAD (Community Resource for Archiving Wireless Data at Dartmouth) is a new National Science Foundation-funded project to build a wireless-network data archive for the research community. It will host wireless data and provide tools and documents to make collecting and using the data easy. This resource should help researchers identify and evaluate real and interesting problems in mobile and pervasive computing. To learn more about CRAWDAD and discuss its direction, about 30 interested people gathered at a workshop held in conjunction with MobiCom 2005.",
Correcting errors beyond the Guruswami-Sudan radius in polynomial time,"We introduce a new family of error-correcting codes that have a polynomial-time encoder and a polynomial-time list-decoder, correcting a fraction of adversarial errors up to /spl tau//sub M/ = 1 - /sup M+1//spl radic/(M/sup M/R/sup M/) where R is the rate of the code and M /spl ges/ 1 is an arbitrary integer parameter. This makes it possible to decode beyond the Guruswami-Sudan radius of 1 /spl radic/R for all rates less than 1/16. Stated another way, for any /spl epsiv/ > 0, we can list-decode in polynomial time a fraction of errors up to 1 - /spl epsiv/ with a code of length n and rate /spl Omega/(/spl epsiv//log(1//spl epsiv/)), defined over an alphabet of size n/sup M/ = n/sup O(log(1//spl epsiv/))/. Notably, this error-correction is achieved in the worst-case against adversarial errors: a probabilistic model for the error distribution is neither needed nor assumed. The best results so far for polynomial-time list-decoding of adversarial errors required a rate of O(/spl epsiv//sup 2/) to achieve the correction radius of 1 - /spl epsiv/. Our codes and list-decoders are based on two key ideas. The first is the transition from bivariate polynomial interpolation, pioneered by Sudan and Guruswami-Sudan [1999], to multivariate interpolation decoding. The second idea is to part ways with Reed-Solomon codes, for which numerous prior attempts at breaking the O(/spl epsiv//sup 2/) rate barrier in the worst-case were unsuccessful. Rather than devising a better list-decoder for Reed-Solomon codes, we devise better codes. Standard Reed-Solomon encoders view a message as a polynomial f(X) over a field F/sub q/, and produce the corresponding codeword by evaluating f(X) at n distinct elements of F/sub q/. Herein, given f(X), we first compute one or more related polynomials g/sub 1/(X), g/sub 2/(X), ..., g/sub M-1/(X) and produce the corresponding codeword by evaluating all these polynomials. Correlation between f(X) and g/sub i/(X), carefully designed into our encoder, then provides the additional information we need to recover the encoded message from the output of the multivariate interpolation process.",
"Automotive communications-past, current and future","This paper presents a state-of-practice (SOP) overview of automotive communication technologies, including the latest technology developments. These networking technologies are classified in four major groups: (1) current wired, (2) multimedia, (3) upcoming wired and (4) wireless. Within these groups a few technologies stand out as strong candidates for future automotive networks. The goal of this paper is to give an overview of automotive applications relying on communications, identify the key networking technologies used in various automotive applications, present their properties and attributes, and indicate future challenges in the area of automotive communications",
High-precision floating-point arithmetic in scientific computation,"IEEE 64-bit floating-point arithmetic is sufficient for most scientific applications, but a rapidly growing body of scientific computing applications requires a higher level of numeric precision. Software packages have yielded interesting scientific results that suggest numeric precision in scientific computations could be as important to program design as algorithms and data structures.",
Optimal peer selection for P2P downloading and streaming,"In a P2P system, a client peer may select one or more server peers to download a specific file. In a P2P resource economy, the server peers charge the client for the downloading. A server peer's price would naturally depend on the specific object being downloaded, the duration of the download, and the rate at which the download is to occur. The optimal peer selection problem is to select, from the set of peers that have the desired object, the subset of peers and download rates that minimizes cost. In this paper we examine a number of natural peer selection problems for both P2P downloading and P2P streaming. For downloading, we obtain the optimal solution for minimizing the download delay subject to a budget constraint, as well as the corresponding Nash equilibrium. For the streaming problem, we obtain a solution that minimizes cost subject to continuous playback while allowing for one or more server peers to fail during the streaming process. The methodologies developed in this paper are applicable to a variety of P2P resource economy problems.","Peer to peer computing,
Bandwidth,
Information science,
File servers,
Distributed computing,
Computer science,
Cost function,
Nash equilibrium,
Application software,
Biotechnology"
Cross-generalization: learning novel classes from a single example by feature replacement,"We develop an object classification method that can learn a novel class from a single training example. In this method, experience with already learned classes is used to facilitate the learning of novel classes. Our classification scheme employs features that discriminate between class and non-class images. For a novel class, new features are derived by selecting features that proved useful for already learned classification tasks, and adapting these features to the new classification task. This adaptation is performed by replacing the features from already learned classes with similar features taken from the novel class. A single example of a novel class is sufficient to perform feature adaptation and achieve useful classification performance. Experiments demonstrate that the proposed algorithm can learn a novel class from a single training example, using 10 additional familiar classes. The performance is significantly improved compared to using no feature adaptation. The robustness of the proposed feature adaptation concept is demonstrated by similar performance gains across 107 widely varying object categories.",
A novel local thresholding algorithm for trabecular bone volume fraction mapping in the limited spatial resolution regime of in vivo MRI,"Recent advances in micro-magnetic resonance imaging have shown the possibility of in vivo assessment of trabecular bone architecture. However, the small feature size and relatively low signal-to-noise ratio (SNR) achievable in vivo cause the intensity histogram to be unimodal. The critical first step in the processing of these images is the extraction of bone volume fraction for each voxel. Here, we propose a local threshold algorithm (LTA) that determines the marrow intensity value in the neighborhood of each voxel based on nearest-neighbor statistics. Using the local marrow intensities we threshold the image and scale the intensities of voxels partially occupied by bone to produce a marrow volume fraction map of the trabecular bone region. We show that structural parameters derived with the LTA are highly correlated with those obtained with the previously published histogram deconvolution algorithm (HDA) and that the LTA is robust to image noise corruption. The LTA is found to correctly identify trabeculae with a significantly higher reliability than HDA. Finally, we demonstrate that the LTA is superior in preserving connectivity by showing for 75 in vivo images that the genus of the trabecular bone surface is always higher than when processed with the HDA.","Cancellous bone,
Spatial resolution,
In vivo,
Magnetic resonance imaging,
Histograms,
Signal to noise ratio,
Statistics,
Structural engineering,
Deconvolution,
Noise robustness"
Radioactive source detection by sensor networks,"Detection limits of sensor networks for moving radioactive sources are characterized, using Bayesian methods in conjunction with computer simulation. These studies involve point sources moving at constant velocity, emulating vehicular conveyance on a straight road. For networks involving ten nodes, respective Bayesian methods are implementable in real time. We probe the increased computational requirements incurred by larger numbers of nodes and source trajectory parameters. The complexity appears quadratic in the number of nodes and, also, numerous trajectory parameters may be used. We investigate the consequences of different levels of background radiation. Simulations are shown to be useful for ranking candidate node layouts. We study the detection capabilities of individual sensors and the scalability of detection with sensor density; near the detection limit, increasing the number of sensors can accrue subproportional network sensitivity.","Bayesian methods,
Parameter estimation,
Sensor phenomena and characterization,
Radiation detectors,
Computer networks,
Computer simulation,
Roads,
Probes,
Computational modeling,
Scalability"
Interactive online undergraduate laboratories using J-DSP,"An interactive Web-based simulation tool called Java-DSP (J-DSP) for use in digital signal processing (DSP)-related electrical engineering courses is described. J-DSP is an object-oriented simulation environment that enables students and distance learners to perform online signal processing simulations, visualize Web-based interactive demos, and perform computer laboratories from remote locations. J-DSP is accompanied by a series of hands-on laboratory exercises that complement classroom and textbook content. The laboratories cover several fundamental concepts, including z transforms, digital filter design, spectral analysis, multirate signal processing, and statistical signal processing. Online assessment instruments for the evaluation of the J-DSP software and the associated laboratory exercises have been developed. Pre/postassessment data have been collected and analyzed for each laboratory in an effort to assess the impact of the tool on student learning.",
TASK: sensor network in a box,"Sensornet systems research is being conducted with various applications and deployment scenarios in mind. In many of these scenarios, the presumption is that the sensornet will be deployed and managed by users who do not have a background in computer science. In this paper we describe the ""tiny application sensor kit"" (TASK), a system we have designed for use by end-users with minimal sensornet sophistication. We describe the requirements that guided our design, the architecture of the system and results from initial deployments. Based on our experience to date we present preliminary design principles and research challenges that arise in delivering sensornet research to end users.","Intelligent networks,
Application software,
Biosensors,
Computer science,
Wireless sensor networks,
Computerized monitoring,
Sensor phenomena and characterization,
Sensor systems and applications,
Computer architecture,
Ad hoc networks"
FRIP: a region-based image retrieval tool using automatic image segmentation and stepwise Boolean AND matching,"We present our region-based image retrieval tool, finding region in the picture (FRIP), that is able to accommodate, to the extent possible, region scaling, rotation, and translation. Our goal is to develop an effective retrieval system to overcome a few limitations associated with existing systems. To do this, we propose adaptive circular filters used for semantic image segmentation, which are based on both Bayes' theorem and texture distribution of image. In addition, to decrease the computational complexity without losing the accuracy of the search results, we extract optimal feature vectors from segmented regions and apply them to our stepwise Boolean AND matching scheme. The experimental results using real world images show that our system can indeed improve retrieval performance compared to other global property-based or region-of-interest-based image retrieval methods.","Image retrieval,
Image segmentation,
Content based retrieval,
Shape,
Information retrieval,
Image databases,
Computer science,
Color,
Adaptive filters,
Computational complexity"
Static techniques for concept location in object-oriented code,"Concept location in source code is the process that identifies where a software system implements a specific concept. While it is well accepted that concept location is essential for the maintenance of complex procedural code like code written in C, it is much less obvious whether it is also needed for the maintenance of the object-oriented code. After all, the object-oriented code is structured into classes and well-designed classes already implement concepts, so the issue seems to be reduced to the selection of the appropriate class. The objective of our work is to see if the techniques for concept location are still needed (they are) and whether object-oriented structuring facilitates concept location (it does not). This paper focuses on static concept location techniques that share common prerequisites and are search the source code using regular expression matching, or static program dependencies, or information retrieval. The paper analyses these techniques to see how they compare to each other in terms of their respective strengths and weaknesses.","Software systems,
Information retrieval,
Software maintenance,
Testing,
Computer science,
Documentation,
Software performance,
Natural languages,
Libraries,
Databases"
DyNoC: A dynamic infrastructure for communication in dynamically reconfugurable devices,"A new paradigm to support the communication among modules dynamically placed on a reconfigurable device at run-time is presented. Based on the network on chip (NoC) infrastructure, we developed a dynamic communication infrastructure as well as routing methodologies capable to handle routing in a NoC with obstacles created by dynamically placed components. We prove the unrestricted reachability of components and pins, the deadlock-freeness and we finally show the feasibility of our approach by means on real life example applications.","Runtime,
Network-on-a-chip,
Routing,
Pins,
Field programmable gate arrays,
Computer science,
System recovery,
Switching circuits,
Switches,
Communication switching"
Truthful and near-optimal mechanism design via linear programming,"We give a general technique to obtain approximation mechanisms that are truthful in expectation. We show that for packing domains, any /spl alpha/-approximation algorithm that also bounds the integrality gap of the IF relaxation of the problem by a can be used to construct an /spl alpha/-approximation mechanism that is truthful in expectation. This immediately yields a variety of new and significantly improved results for various problem domains and furthermore, yields truthful (in expectation) mechanisms with guarantees that match the best known approximation guarantees when truthfulness is not required. In particular, we obtain the first truthful mechanisms with approximation guarantees for a variety of multi-parameter domains. We obtain truthful (in expectation) mechanisms achieving approximation guarantees of O(/spl radic/m) for combinatorial auctions (CAs), (1 + /spl epsi/ ) for multiunit CAs with B = /spl Omega/(log m) copies of each item, and 2 for multiparameter knapsack problems (multiunit auctions). Our construction is based on considering an LP relaxation of the problem and using the classic VCG mechanism by W. Vickrey (1961), E. Clarke (1971) and T. Groves (1973) to obtain a truthful mechanism in this fractional domain. We argue that the (fractional) optimal solution scaled down by a, where a is the integrality gap of the problem, can be represented as a convex combination of integer solutions, and by viewing this convex combination as specifying a probability distribution over integer solutions, we get a randomized, truthful in expectation mechanism. Our construction can be seen as a way of exploiting VCG in a computational tractable way even when the underlying social-welfare maximization problem is NP-hard.",
Balancing search and target response in cooperative unmanned aerial vehicle (UAV) teams,"This paper considers a heterogeneous team of cooperating unmanned aerial vehicles (UAVs) drawn from several distinct classes and engaged in a search and action mission over a spatially extended battlefield with targets of several types. During the mission, the UAVs seek to confirm and verifiably destroy suspected targets and discover, confirm, and verifiably destroy unknown targets. The locations of some (or all) targets are unknown a priori, requiring them to be located using cooperative search. In addition, the tasks to be performed at each target location by the team of cooperative UAVs need to be coordinated. The tasks must, therefore, be allocated to UAVs in real time as they arise, while ensuring that appropriate vehicles are assigned to each task. Each class of UAVs has its own sensing and attack capabilities, so the need for appropriate assignment is paramount. In this paper, an extensive dynamic model that captures the stochastic nature of the cooperative search and task assignment problems is developed, and algorithms for achieving a high level of performance are designed. The paper focuses on investigating the value of predictive task assignment as a function of the number of unknown targets and number of UAVs. In particular, it is shown that there is a tradeoff between search and task response in the context of prediction. Based on the results, a hybrid algorithm for switching the use of prediction is proposed, which balances the search and task response. The performance of the proposed algorithms is evaluated through Monte Carlo simulations.",
Some new characters on the wire-tap channel of type II,"The noiseless wire-tap channel of type II with coset coding scheme was provided by Ozarow and Wyner. In this correspondence, the user is split into multiple parties who are coordinated in coding their data symbols by using the same encoder. The adversary can tap not only partial transmitted symbols but also partial data symbols. We are interested in the equivocation of the data symbols to this adversary who has more power than that of Ozarow and Wyner. The generalized Hamming weight of Wei and the dimension/length profile (DLP) of Forney are extended to two-code formats: relative generalized Hamming weight and relative dimension/length profile (RDLP). Upper and lower bounds of the new concepts are investigated. They are useful to design a perfect secrecy coding scheme for the coordinated multiparty model. Under a general secrecy standard, the coordinated model can provide a higher transmission rate than an uncoordinated (time-sharing) model.","Hamming weight,
Linear code,
Information retrieval,
Uncertainty,
Time sharing computer systems,
Cryptography,
Redundancy,
Decoding,
Computer science,
Mathematics"
Maximum-likelihood techniques for joint segmentation-classification of multispectral chromosome images,"Traditional chromosome imaging has been limited to grayscale images, but recently a 5-fluorophore combinatorial labeling technique (M-FISH) was developed wherein each class of chromosomes binds with a different combination of fluorophores. This results in a multispectral image, where each class of chromosomes has distinct spectral components. In this paper, we develop new methods for automatic chromosome identification by exploiting the multispectral information in M-FISH chromosome images and by jointly performing chromosome segmentation and classification. We 1) develop a maximum-likelihood hypothesis test that uses multispectral information, together with conventional criteria, to select the best segmentation possibility; 2) use this likelihood function to combine chromosome segmentation and classification into a robust chromosome identification system; and 3) show that the proposed likelihood function can also be used as a reliable indicator of errors in segmentation, errors in classification, and chromosome anomalies, which can be indicators of radiation damage, cancer, and a wide variety of inherited diseases. We show that the proposed multispectral joint segmentation-classification method outperforms past grayscale segmentation methods when decomposing touching chromosomes. We also show that it outperforms past M-FISH classification techniques that do not use segmentation information.","Image segmentation,
Biological cells,
Gray-scale,
Image analysis,
Multispectral imaging,
Labeling,
System testing,
Robustness,
Cancer,
Diseases"
ASKALON: a Grid application development and computing environment,"We present the ASKALON environment whose goal is to simplify the development and execution of workflow applications on the Grid. ASKALON is centered around a set of high-level services for transparent and effective Grid access, including a Scheduler for optimized mapping of workflows onto the Grid, an Enactment Engine for reliable application execution, a Resource Manager covering both computers and application components, and a Performance Prediction service based on training phase and statistical methods. A sophisticated XML-based programming interface that shields the user from the Grid middleware details allows the high-level composition of workflow applications. ASKALON is used to develop and port scientific applications as workflows in the Austrian Grid project. We present experimental results using two real-world scientific applications to demonstrate the effectiveness of our approach.",
Half-time image reconstruction in thermoacoustic tomography,Thermoacoustic tomography (TAT) is an emerging imaging technique with great potential for a wide range of biomedical imaging applications. We propose and investigate reconstruction approaches for TAT that are based on the half-time reflectivity tomography paradigm. We reveal that half-time reconstruction approaches permit for the explicit control of statistically complementary information that can result in the optimal reduction of image variances. We also show that half-time reconstruction approaches can mitigate image artifacts due to heterogeneous acoustic properties of an object. Reconstructed images and numerical results produced from simulated and experimental TAT measurement data are employed to demonstrate these effects.,"Image reconstruction,
Tomography,
Reflectivity,
Biomedical measurements,
Apertures,
Biomedical imaging,
Acoustic measurements,
Biological tissues,
Reconstruction algorithms,
Biomedical engineering"
Video data mining: semantic indexing and event detection from the association perspective,"Advances in the media and entertainment industries, including streaming audio and digital TV, present new challenges for managing and accessing large audio-visual collections. Current content management systems support retrieval using low-level features, such as motion, color, and texture. However, low-level features often have little meaning for naive users, who much prefer to identify content using high-level semantics or concepts. This creates a gap between systems and their users that must be bridged for these systems to be used effectively. To this end, in this paper, we first present a knowledge-based video indexing and content management framework for domain specific videos (using basketball video as an example). We will provide a solution to explore video knowledge by mining associations from video data. The explicit definitions and evaluation measures (e.g., temporal support and confidence) for video associations are proposed by integrating the distinct feature of video data. Our approach uses video processing techniques to find visual and audio cues (e.g., court field, camera motion activities, and applause), introduces multilevel sequential association mining to explore associations among the audio and visual cues, classifies the associations by assigning each of them with a class label, and uses their appearances in the video to construct video indices. Our experimental results demonstrate the performance of the proposed approach.","Data mining,
Indexing,
Event detection,
Content management,
Visual databases,
Streaming media,
Knowledge management,
Computer science,
Digital TV,
Content based retrieval"
On-line density-based appearance modeling for object tracking,"Object tracking is a challenging problem in real-time computer vision due to variations of lighting condition, pose, scale, and view-point over time. However, it is exceptionally difficult to model appearance with respect to all of those variations in advance; instead, on-line update algorithms are employed to adapt to these changes. We present a new on-line appearance modeling technique which is based on sequential density approximation. This technique provides accurate and compact representations using Gaussian mixtures, in which the number of Gaussians is automatically determined. This procedure is performed in linear time at each time step, which we prove by amortized analysis. Features for each pixel and rectangular region are modeled together by the proposed sequential density approximation algorithm, and the target model is updated in scale robustly. We show the performance of our method by simulations and tracking in natural videos","Target tracking,
Density functional theory,
Performance analysis,
Noise robustness,
Histograms,
Computer science,
Educational institutions,
Computer vision,
Approximation algorithms,
Videos"
A new beacon order adaptation algorithm for IEEE 802.15.4 networks,"The new IEEE 802.15.4 standard enables deployment of low-rate low-power personal area networks. It provides a mechanism for adaptation of the protocols duty cycle during runtime. In this paper we propose BOAA, a new algorithm for beacon order adaptation in IEEE 802.15.4 star-topology networks. By observing the communication frequency, the coordinator in such a network determines the required duty cycle and adapts the beacon interval accordingly. Investigations reveal that the algorithm enables power saving with a trade-off according to message delay","Spread spectrum communication,
Protocols,
Delay,
Wireless LAN,
Computerized monitoring,
Condition monitoring,
Energy consumption,
Wireless sensor networks,
Computer science,
Personal area networks"
A novel approach to the 2-D blind deconvolution problem in medical ultrasound,"The finite frequency bandwidth of ultrasound transducers and the nonnegligible width of transmitted acoustic beams are the most significant factors that limit the resolution of medical ultrasound imaging. Consequently, in order to recover diagnostically important image details, obscured due to the resolution limitations, an image restoration procedure should be applied. The present study addresses the problem of ultrasound image restoration by means of the blind-deconvolution techniques. Given an acquired ultrasound image, algorithms of this kind perform either concurrent or successive estimation of the point-spread function (PSF) of the imaging system and the original image. A blind-deconvolution algorithm is proposed, in which the PSF is recovered as a preliminary stage of the restoration problem. As the accuracy of this estimation affects all the following stages of the image restoration, it is considered as the most fundamental and important problem. The contribution of the present study is twofold. First, it introduces a novel approach to the problem of estimating the PSF, which is based on a generalization of several fundamental concepts of the homomorphic deconvolution. It is shown that a useful estimate of the spectrum of the PSF can be obtained by applying a proper smoothing operator to both log-magnitude and phase of the spectra of acquired radio-frequency (RF) images. It is demonstrated that the proposed approach performs considerably better than the existing homomorphic (cepstrum-based) deconvolution methods. Second, the study shows that given a reliable estimate of the PSF, it is possible to deconvolve it out of the RF-image and obtain an estimate of the true tissue reflectivity function, which is relatively independent of the properties of the imaging system. The deconvolution was performed using the maximum a-posteriori (MAP) estimation framework for a number of statistical priors assumed for the reflectivity function. It is shown in a series of in vivo experiments that reconstructions based on the priors, which tend to emphasize the ""sparseness"" of the tissue structure, result in solutions of higher resolution and contrast.","Deconvolution,
Ultrasonic imaging,
Image restoration,
Biomedical imaging,
Medical diagnostic imaging,
Image resolution,
Radio frequency,
Reflectivity,
Bandwidth,
Biomedical transducers"
Design and implementation of the AEGIS single-chip secure processor using physical random functions,"Secure processors enable new applications by ensuring private and authentic program execution even in the face of physical attack. In this paper, we present the AEGIS secure processor architecture, and evaluate its RTL implementation on FPGAs. By using physical random functions, we propose a new way of reliably protecting and sharing secrets that is more secure than existing solutions based on non-volatile memory. Our architecture gives applications the flexibility of trusting and protecting only a portion of a given process, unlike prior proposals which require a process to be protected in entirety. We also put forward a specific model of how secure applications can be programmed in a high-level language and compiled to run on our system. Finally, we evaluate a fully functional FPGA implementation of our processor, assess the implementation tradeoffs, compare performance, and demonstrate the benefits of partially protecting a program.",
Combinatorial auction-based protocols for resource allocation in grids,"In this paper, we introduce the combinatorial auction model for resource management in grids. We propose a combinatorial auction-based resource allocation protocol in which a user bids a price value for each of the possible combinations of resources required for its tasks execution. The protocol involves an approximation algorithm for solving the combinatorial auction problem. We implement the new protocol in a simulated environment and study its economic efficiency and its effect on the system performance.",
Battery model for embedded systems,"This paper explores the recovery and rate capacity effect for batteries used in embedded systems. It describes the prominent battery models with their advantages and drawbacks. It then throws new light on the battery recovery behavior, which can help determine optimum discharge profiles and hence result in significant improvement in battery lifetime. Finally it proposes a fast and accurate stochastic model which draws the positives from the earlier models and minimizes the drawbacks. The parameters for this model are determined by a pretest, which takes into account the newfound background into recovery and rate capacity hence resulting in higher accuracy. Simulations conducted suggest close correspondence with experimental results and a maximum error of 2.65%.","Batteries,
Embedded system,
Predictive models,
Stochastic processes,
Life estimation,
Mobile computing,
Voltage control,
Computer science,
Computational modeling,
Hardware"
Reduction of noise-induced streak artifacts in X-ray computed tomography through spline-based penalized-likelihood sinogram smoothing,"We present a statistically principled sinogram smoothing approach for X-ray computed tomography (CT) with the intent of reducing noise-induced streak artifacts. These artifacts arise in CT when some subset of the transmission measurements capture relatively few photons because of high attenuation along the measurement lines. Attempts to reduce these artifacts have focused on the use of adaptive filters that strive to tailor the degree of smoothing to the local noise levels in the measurements. While these approaches involve loose consideration of the measurement statistics to determine smoothing levels, they do not explicitly model the statistical distributions of the measurement data. We present an explicitly statistical approach to sinogram smoothing in the presence of photon-starved measurements. It is an extension of a nonparametric sinogram smoothing approach using penalized Poisson-likelihood functions that we have previously developed for emission tomography. Because the approach explicitly models the data statistics, it is naturally adaptive-it will smooth more variable measurements more heavily than it does less variable measurements. We find that it significantly reduces streak artifacts and noise levels without comprising image resolution.","Noise reduction,
Computed tomography,
X-ray imaging,
Spline,
Smoothing methods,
Statistical distributions,
Attenuation measurement,
Noise level,
Adaptive filters,
Noise measurement"
Compensating for intraoperative soft-tissue deformations using incomplete surface data and finite elements,"Image-guided liver surgery requires the ability to identify and compensate for soft tissue deformation in the organ. The predeformed state is represented as a complete three-dimensional surface of the organ, while the intraoperative data is a range scan point cloud acquired from the exposed liver surface. The first step is to rigidly align the coordinate systems of the intraoperative and preoperative data. Most traditional rigid registration methods minimize an error metric over the entire data set. In this paper, a new deformation-identifying rigid registration (DIRR) is reported that identifies and aligns minimally deformed regions of the data using a modified closest point distance cost function. Once a rigid alignment has been established, deformation is accounted for using a linearly elastic finite element model (FEM) and implemented using an incremental framework to resolve geometric nonlinearities. Boundary conditions for the incremental formulation are generated from intraoperatively acquired range scan surfaces of the exposed liver surface. A series of phantom experiments is presented to assess the fidelity of the DIRR and the combined DIRR/FEM approaches separately. The DIRR approach identified deforming regions in 90% of cases under conditions of realistic surgical exposure. With respect to the DIRR/FEM algorithm, subsurface target errors were correctly located to within 4 mm in phantom experiments.",
A Performance Comparison of Data Encryption Algorithms,"The principal goal guiding the design of any encryption algorithm must be security against unauthorized attacks. However, for all practical applications, performance and the cost of implementation are also important concerns. A data encryption algorithm would not be of much use if it is secure enough but slow in performance because it is a common practice to embed encryption algorithms in other applications such as e-commerce, banking, and online transaction processing applications. Embedding of encryption algorithms in other applications also precludes a hardware implementation, and is thus a major cause of degraded overall performance of the system. In this paper, the four of the popular secret key encryption algorithms, i.e., DES, 3DES, AES (Rijndael), and the Blowfish have been implemented, and their performance is compared by encrypting input files of varying contents and sizes, on different Hardware platforms. The algorithms have been implemented in a uniform language, using their standard specifications, to allow a fair comparison of execution speeds. The performance results have been summarized and a conclusion has been presented. Based on the experiments, it has been concluded that the Blowfish is the best performing algorithm among the algorithms chosen for implementation.","Cryptography,
Data security,
NIST,
Algorithm design and analysis,
Costs,
Banking,
Hardware,
Design engineering,
Educational institutions,
Mechanical engineering"
Generic vertical handoff decision function for heterogeneous wireless,"As mobile wireless networks increase in popularity and pervasiveness, we are facing the challenge of integration of diverse wireless networks such as WLANs and WWANs. Consequently, it is becoming progressively more important to arrive at a vertical handoff solution where users can move among various types of networks efficiently and seamlessly. The ability to remain connected as a mobile device roams across different types of networks still remains an unachieved objective. Frequently, just choosing the best network to connect to, is a challenging problem due to the large number of network characteristics that need to be considered. Identifying these decision factors is therefore one of the principal objectives for seamless mobility. In this paper, we discuss the different factors and metric qualities that give an indication of whether or not a handoff is needed. We then describe a vertical handoff decision function, VHDF, which enables devices to assign weights to different network factors such as monetary cost, quality of service, power requirements, personal preference, etc.","Wireless networks,
Wireless LAN,
Bandwidth,
Land mobile radio cellular systems,
Computer networks,
Pervasive computing,
Telecommunication computing,
Cost function,
Internet,
Information science"
Cost-based scheduling of scientific workflow applications on utility grids,"Over the last few years, grid technologies have progressed towards a service-oriented paradigm that enables a new way of service provisioning based on utility computing models. Users consume these services based on their QoS (quality of service) requirements. In such ""pay-per-use"" grids, workflow execution cost must be considered during scheduling based on users' QoS constraints. In this paper, we propose a cost-based workflow scheduling algorithm that minimizes execution cost while meeting the deadline for delivering results. It can also adapt to the delays of service executions by rescheduling unexecuted tasks. We also attempt to optimally solve the task scheduling problem in branches with several sequential tasks by modeling the branch as a Markov decision process and using the value iteration method","Optimal scheduling,
Processor scheduling,
Scheduling algorithm,
Workflow management software,
Grid computing,
Quality of service,
Costs,
Computer networks,
Time factors,
Delay"
Dynamic feature traces: finding features in unfamiliar code,"This paper introduces an automated technique for feature location: helping developers map features to relevant source code. Like several other automated feature location techniques, ours is based on execution-trace analysis. We hypothesize that these techniques, which rely on making binary judgments about a code element's relevance to a feature, are overly sensitive to the quality of the input. The main contribution of this paper is to provide a more robust alternative, whose most distinguishing characteristic is that it employs ranking heuristics to determine a code element's relevance to a feature. We believe that our technique is less sensitive with respect to the quality of the input and we claim that it is more effective when used by developers unfamiliar with the target system. We validate our claim by applying our technique to three systems with comprehensive test suites. A developer unfamiliar with the target system spent a limited amount of effort preparing the test suite for analysis. Our results show that under these circumstances our ranking-based technique compares favorably to a technique based on binary judgements.","System testing,
Computer science,
Software maintenance,
Robustness,
Software engineering,
Feedback loop,
Performance analysis,
Design for testability,
Visualization,
Reconnaissance"
NavTracks: supporting navigation in software maintenance,"In this paper, we present NavTracks, a tool that supports browsing through software. NavTracks keeps track of the navigation history of software developers, forming associations between related files. These associations are then used as the basis for recommending potentially related files as a developer navigates the software system. We present the reasoning behind NavTracks, its basic algorithm, a case study, and propose some future work.","Navigation,
Software maintenance,
Software systems,
Cognitive science,
Computer science,
Software tools,
History,
Human computer interaction,
Software engineering,
Space exploration"
Conjugate phase MRI reconstruction with spatially variant sample density correction,"A new image reconstruction method to correct for the effects of magnetic field inhomogeneity in non-Cartesian sampled magnetic resonance imaging (MRI) is proposed. The conjugate phase reconstruction method, which corrects for phase accumulation due to applied gradients and magnetic field inhomogeneity, has been commonly used for this case. This can lead to incomplete correction, in part, due to the presence of gradients in the field inhomogeneity function. Based on local distortions to the k-space trajectory from these gradients, a spatially variant sample density compensation function is introduced as part of the conjugate phase reconstruction. This method was applied to both simulated and experimental spiral imaging data and shown to produce more accurate image reconstructions. Two approaches for fast implementation that allow the use of fast Fourier transforms are also described. The proposed method is shown to produce fast and accurate image reconstructions for spiral sampled MRI.",
Real-time forest fire detection with wireless sensor networks,"In this paper, we propose a wireless sensor network paradigm for real-time forest fire detection. The wireless sensor network can detect and forecast forest fire more promptly than the traditional satellite-based detection approach. This paper mainly describes the data collecting and processing in wireless sensor networks for real-time forest fire detection. A neural network method is applied to in-network data processing. We evaluate the performance of our approach by simulations.","Fires,
Wireless sensor networks,
Neural networks,
Temperature sensors,
Data processing,
Disaster management,
Smoke detectors,
Computer science,
Monitoring,
Satellites"
On the impossibility of obfuscation with auxiliary input,"Barak et al. formalized the notion of obfuscation, and showed that there exist (contrived) classes of functions that cannot be obfuscated. In contrast, Canetti and Wee showed how to obfuscate point functions, under various complexity assumptions. Thus, it would seem possible that most programs of interest can be obfuscated even though in principle general purpose obfuscators do not exist. We show that this is unlikely to be the case. In particular; we consider the notion of obfuscation w.r.t. auxiliary input, which corresponds to the setting where the adversary, which is given the obfuscated circuit, may have some additional a priori information. This is essentially the case of interest in any usage of obfuscation we can imagine. We prove that there exist many natural classes of functions that cannot be obfuscated w.r.t. auxiliary input, both when the auxiliary input is dependent of the function being obfuscated and even when the auxiliary input is independent of the function being obfuscated. We also give a positive result. In particular; we show that any obfuscator for the class of point functions is also an obfuscator with independent auxiliary input.","Circuits,
Polynomials,
Protocols,
Boolean functions,
Security,
Context modeling,
History"
Mixture model analysis of DNA microarray images,"In this paper, we propose a new methodology for analysis of microarray images. First, a new gridding algorithm is proposed for determining the individual spots and their borders. Then, a Gaussian mixture model (GMM) approach is presented for the analysis of the individual spot images. The main advantages of the proposed methodology are modeling flexibility and adaptability to the data, which are well-known strengths of GMM. The maximum likelihood and maximum a posteriori approaches are used to estimate the GMM parameters via the expectation maximization algorithm. The proposed approach has the ability to detect and compensate for artifacts that might occur in microarray images. This is accomplished by a model-based criterion that selects the number of the mixture components. We present numerical experiments with artificial and real data where we compare the proposed approach with previous ones and existing software tools for microarray image analysis and demonstrate its advantages.","Image analysis,
DNA,
Fluorescence,
Maximum likelihood detection,
Maximum likelihood estimation,
Software tools,
Image color analysis,
Gene expression,
Computer science,
Parameter estimation"
Optimization of geometrical calibration in pinhole SPECT,"Previously, we developed a method to determine the acquisition geometry of a pinhole camera. This information is needed for the correct reconstruction of pinhole single photon emission computed tomography images. The method uses a calibration phantom consisting of three point sources and their positions in the field of view (FOV) influence the accuracy of the geometry estimate. This work proposes two particular configurations of point sources with specific positions and orientations in the FOV for optimal image reconstruction accuracy. For the proposed calibration setups, inaccuracies of the geometry estimate due to noise in the calibration data, only cause subresolution inaccuracies in reconstructed images. The calibration method also uses a model of the point source configuration, which is only known with limited accuracy. The study demonstrates, however, that, with the proposed calibration setups, the error in reconstructed images is comparable to the error in the phantom model.","Calibration,
Detectors,
Geometry,
Image reconstruction,
Imaging phantoms,
Cameras,
Single photon emission computed tomography,
Nuclear medicine,
Technological innovation"
Energy minimization via graph cuts: settling what is possible,"The recent explosion of interest in graph cut methods in computer vision naturally spawns the question: what energy functions can be minimized via graph cuts? This question was first attacked by two papers of Kolmogorov and Zabih, in which they dealt with functions with pair-wise and triplewise pixel interactions. In this work, we extend their results in two directions. First, we examine the case of k-wise pixel interactions; the results are derived from a purely algebraic approach. Second, we discuss the applicability of provably approximate algorithms. Both of these developments should help researchers best understand what can and cannot be achieved when designing graph cut based algorithms.","Computer vision,
Minimization methods,
Explosions,
Application software,
Computer science,
Algorithm design and analysis,
Polynomials,
Stereo vision,
Sufficient conditions,
NP-complete problem"
"MMAC: a mobility-adaptive, collision-free MAC protocol for wireless sensor networks","Mobility in wireless sensor networks poses unique challenges to the medium access control (MAC) protocol design. Previous MAC protocols for sensor networks assume static sensor nodes and focus on energy-efficiency. In this paper, we present a mobility-adaptive, collision-free medium access control protocol (MMAC) for mobile sensor networks. MMAC caters for both weak mobility (e.g., topology changes, node joins, and node failures) and strong mobility (e.g., concurrent node joins and failures, and physical mobility of nodes). MMAC is a scheduling-based protocol and thus it guarantees collision avoidance. MMAC allows nodes the transmission rights at particular time-slots based on the traffic information and mobility pattern of the nodes. Simulation results indicate that the performance of MMAC is equivalent to that of TRAMA in static sensor network environments. In sensor networks with mobile nodes or high network dynamics, MMAC outperforms existing MAC protocols, like TRAM A and S-MAC, in terms of energy-efficiency, delay, and packet delivery.","Media Access Protocol,
Wireless application protocol,
Wireless sensor networks,
Access protocols,
Sensor phenomena and characterization,
Traffic control,
Energy efficiency,
Bandwidth,
Computer science,
Programmable control"
"Mixed reality in education, entertainment, and training","Transferring research from the laboratory to mainstream applications requires the convergence of people, knowledge, and conventions from divergent disciplines. Solutions involve more than combining functional requirements and creative novelty. To transform technical capabilities of emerging mixed reality (MR) technology into the mainstream involves the integration and evolution of unproven systems. For example, real-world applications require complex scenarios (a content issue) involving an efficient iterative pipeline (a production issue) and driving the design of a story engine (a technical issue) that provides an adaptive experience with an after-action review process (a business issue). This article describes how a multi-disciplinary research team transformed core MR technology and methods into diverse urban terrain applications. These applications are used for military training and situational awareness, as well as for community learning to significantly increase the entertainment, educational, and satisfaction levels of existing experiences in public venues.","Virtual reality,
Auditory displays,
Space technology,
Computer displays,
Computer science education,
Portals,
Haptic interfaces,
Marine animals,
Oceans,
Augmented reality"
Comparison between MAP and postprocessed ML for image reconstruction in emission tomography when anatomical knowledge is available,"Previously, the noise characteristics obtained with penalized-likelihood reconstruction [or maximum a posteriori (MAP)] have been compared to those obtained with postsmoothed maximum-likelihood (ML) reconstruction, for emission tomography applications requiring uniform resolution. It was found that penalized-likelihood reconstruction was not superior to postsmoothed ML. In this paper, a similar comparison is made, but now for applications where the noise suppression is tuned with anatomical information. It is assumed that limited but exact anatomical information is available. Two methods were compared. In the first method, the anatomical information is incorporated in the prior of a MAP-algorithm and is, therefore, imposed during MAP-reconstruction. The second method starts from an unconstrained ML-reconstruction, and imposes the anatomical information in a postprocessing step. The theoretical analysis was verified with simulations: small lesions were inserted in two different objects, and noisy PET data were produced and reconstructed with both methods. The resulting images were analyzed with bias-noise curves, and by computing the detection performance of the nonprewhitening observer and a channelized Hotelling observer. Our analysis and simulations indicate that the postprocessing method is inferior, unless the noise correlations between neighboring pixels are taken into account. This can be done by applying a so-called prewhitening filter. However, because the prewhitening filter is shift variant and object dependent, it seems that MAP reconstruction is the more efficient method.","Image reconstruction,
Tomography,
Computational modeling,
Filters,
Image resolution,
Analytical models,
Spatial resolution,
Nuclear medicine,
Image quality,
Imaging phantoms"
US domestic extremist groups on the Web: link and content analysis,"Although US domestic extremist and hate groups might not be as well-known as some international groups, they nevertheless pose a significant threat to homeland security. Increasingly, these groups are using the Internet as a tool for facilitating recruitment, linking with other extremist groups, reaching global audiences, and spreading hate materials that encourage violence and terrorism. A study of semiautomated methodologies to capture and organize domestic extremist Web site data revealed interorganizational structures and cluster affinities that coincided with both domain expert knowledge and earlier manual research.",
Binary and multivariate stochastic models of consensus formation,"We consider stochastic dynamic models studied via computer simulation. We review some basic results for the voter model which is probably the simplest model of collective behavior. Specifically, we focus on the dynamical effect of who interacts with whom - that is, the consequences of different interaction networks. We also consider R. Axelrod's (1997) model for the dissemination of culture.","Stochastic processes,
Cultural differences,
Complex networks,
Metastasis,
Power system modeling,
Intelligent networks,
Instruments,
Globalization,
Polarization,
Multidimensional systems"
Quantum circuit simplification using templates,"Optimal synthesis of quantum circuits is intractable and heuristic methods must be employed. Templates are a general approach to reversible quantum circuit simplification. We consider the use of templates to simplify a quantum circuit initially found by other means. We present and analyze templates in the general case, and then provide particular details for circuits composed of NOT, CNOT and controlled-sqrt-of-NOT gates. We introduce templates for this set of gates and apply them to simplify both known quantum realizations of Toffoli gates and circuits found by earlier heuristic Fredkin and Toffoli gate synthesis algorithms. While the number of templates is quite small, the reduction in quantum cost is often significant.","Quantum computing,
Circuit synthesis,
Costs,
Computer science,
Niobium,
Optimization methods,
Vectors,
Logic,
Q measurement,
Tensile stress"
Tardiness bounds under global EDF scheduling on a multiprocessor,"This paper considers the scheduling of soft real-time sporadic task systems under global EDF on an identical multiprocessor. Prior research on global EDF has focused mostly on hard real-time systems, where, to ensure that all deadlines are met, approximately 50% of the available processing capacity will have to be sacrificed in the worst case. This may be overkill for soft real-time systems that can tolerate bounded tardiness. In this paper, we derive tardiness bounds under preemptive and non-preemptive global EDF on multiprocessors when the total utilization of a task system is not restricted and may equal the number of processors. Our tardiness bounds depend on per-task utilizations and execution costs - the lower these values, the lower the tardiness bounds. As a final remark, we note that global EDF may be superior to partitioned EDF for multiprocessor-based soft real-time systems in that the latter does not offer any scope to improve system utilization even if bounded tardiness can be tolerated",
"On the representation, measurement, and discovery of fuzzy associations","The use of fuzzy sets to describe associations between data extends the types of relationships that may be represented, facilitates the interpretation of rules in linguistic terms, and avoids unnatural boundaries in the partitioning of the attribute domains. In addition, the partial membership values provide a method for incorporating the distribution of the data into the assessment of a rule. This paper investigates techniques to identify and evaluate associations in a relational database that are expressible by fuzzy if-then rules. Extensions of the classical confidence measure based on the /spl alpha/-cut decompositions of the fuzzy sets are proposed to incorporate the distribution of the data into the assessment of a relationship and identify robustness in an association. A rule learning strategy that discovers both the presence and the type of an association is presented.","Fuzzy sets,
Association rules,
Relational databases,
Robustness,
Measurement standards,
Data mining,
Computer science"
Emergency Broadcast Protocol for Inter-Vehicle Communications,"The most important goal in transportation systems is to reduce the dramatically high number of accidents and fatal consequences. One of the most important factors that would make it possible to reach this goal is the design of effective broadcast protocols. In this paper we present an emergency broadcast protocol designed for sensor inter-vehicle communications and based in geographical routing. Sensors installed in cars continuously gather important information and in any emergency detection raise the need for immediate broadcast. The highway is divided in virtual cells, which moves as the vehicles moves. The cell members choose a cell reflector that behaves for a certain time interval as a base station that handle the emergency messages coming from members of the same cell, or close members from neighbor cells. Besides that the cell reflector serves as an intermediate node in the routing of emergency messages coming from its neighbor cell reflectors and does a prioritization of all messages in order to decide which is the first to be forwarded. After this the message is forwarded through the other cell reflectors. Finally the destination cell reflector sends the message to the destination node. Our simulation results show that our proposed protocol is more effective compared to existing inter-vehicles protocols","Broadcasting,
Mobile communication,
Road transportation,
Routing protocols,
Wireless networks,
Vehicle safety,
Floods,
Computer science,
Accidents,
Road vehicles"
Using erasure codes efficiently for storage in a distributed system,"Erasure codes provide space-optimal data redundancy to protect against data loss. A common use is to reliably store data in a distributed system, where erasure-coded data are kept in different nodes to tolerate node failures without losing data. In this paper, we propose a new approach to maintain ensure-encoded data in a distributed system. The approach allows the use of space efficient k-of-n erasure codes where n and k are large and the overhead n-k is small. Concurrent updates and accesses to data are highly optimized: in common cases, they require no locks, no two-phase commits, and no logs of old versions of data. We evaluate our approach using an implementation and simulations for larger systems.","Protection,
Computer science,
Data engineering,
Reliability engineering,
Costs,
Computer crashes,
Laboratories,
Milling machines,
Redundancy,
Maintenance"
Is Search Really Necessary to Generate High-Performance BLAS?,"A key step in program optimization is the estimation of optimal values for parameters such as tile sizes and loop unrolling factors. Traditional compilers use simple analytical models to compute these values. In contrast, library generators like ATLAS use global search over the space of parameter values by generating programs with many different combinations of parameter values, and running them on the actual hardware to determine which values give the best performance. It is widely believed that traditional model-driven optimization cannot compete with search-based empirical optimization because tractable analytical models cannot capture all the complexities of modern high-performance architectures, but few quantitative comparisons have been done to date. To make such a comparison, we replaced the global search engine in ATLAS with a model-driven optimization engine and measured the relative performance of the code produced by the two systems on a variety of architectures. Since both systems use the same code generator, any differences in the performance of the code produced by the two systems can come only from differences in optimization parameter values. Our experiments show that model-driven optimization can be surprisingly effective and can generate code with performance comparable to that of code generated by ATLAS using global search.","Program processors,
Optimizing compilers,
Analytical models,
Libraries,
Computer science,
Programming profession,
Hardware,
Search engines,
Linear algebra"
Criterion for SEU occurrence in SRAM deduced from circuit and device Simulations in case of neutron-induced SER,"A reliable criterion for SEU occurrence simulation is presented. It expresses the relationship existing at threshold between the magnitude and duration of the ion-induced parasitic pulse. This criterion can be obtained by both three-dimensional device and SPICE simulations. Using this criterion, the simulated and experimental SER on 130 and 250 nm technologies are shown to be in good agreement.","Random access memory,
Circuit simulation,
Computer aided software engineering,
SPICE,
Protons,
Neutrons,
Space vector pulse width modulation,
Target tracking,
Particle tracking"
Effect of island distribution on error rate performance in patterned media,"The size, shape, and distribution of islands in a patterned medium depends on the patterning process adopted. The off-track performance in this case is mainly dominated by inter-track interference due to the neighboring islands. In this paper, the effect that the island distribution has on the off-track performance is investigated with respect to read channel bit-error-rate performance.","Error analysis,
Shape,
Magnetic separation,
Writing,
Lattices,
Solid modeling,
Computer science,
Mathematics,
Statistical distributions,
Interference"
Efficient texture analysis of SAR imagery,"We address the problem of efficiency in texture analysis for synthetic aperture radar (SAR) imagery. Motivated by the statistical occupancy model, we introduce the notion of patch reoccurrences. Using the reoccurrences, we propose the use of approximate textural features in analysis of SAR images. We describe how the proposed approximate features can be extracted for two popular texture analysis methods-the gray-level cooccurrence matrix and Gabor wavelets. Results on image texture classification show that the proposed method can provide an improved efficiency in the analysis of SAR imagery, without introducing any significant degradation in the classification results.","Image analysis,
Image texture analysis,
Sea surface,
Surface texture,
Sea ice,
Wavelet packets,
Synthetic aperture radar,
Wavelet analysis,
Image texture,
Computer science"
Safraless decision procedures,"The automata-theoretic approach is one of the most fundamental approaches to developing decision procedures in mathematical logics. To decide whether a formula in a logic with the tree-model property is satisfiable, one constructs an automaton that accepts all (or enough) tree models of the formula and then checks that the language of this automaton is nonempty. The standard approach translates formulas into alternating parity tree automata, which are then translated, via Safra's determinization construction, into nondeterministic parity automata. This approach is not amenable to implementation because of the difficulty of implementing Safra's construction and the nonemptiness test for nondeterministic parity tree automata. In this paper, we offer an alternative to the standard automata-theoretic approach. The crux of our approach is avoiding the use of Safra's construction and of nondeterministic parity tree automata. Our approach goes instead via universal co-Buchi tree automata and nondeterministic Buchi tree automata. Our translations are significantly simpler than the standard approach, less difficult to implement, and have practical advantages like being amenable to optimizations and a symbolic implementation. We also show that our approach yields better complexity bounds.",
Video behaviour profiling and abnormality detection without manual labelling,A novel framework is developed for automatic behaviour profiling and abnormality sampling/detection without any manual labelling of the training dataset. Natural grouping of behaviour patterns is discovered through unsupervised model selection and feature selection on the eigenvectors of a normalised affinity matrix. Our experiments demonstrate that a behaviour model trained using an unlabelled dataset is superior to those trained using the same but labelled dataset in detecting abnormality from an unseen video,"Labeling,
Layout,
Sampling methods,
Pattern recognition,
Robustness,
Humans,
Feature extraction,
Computer science,
Prototypes,
Bayesian methods"
"Target Acquisition, Localization, and Surveillance Using a Fixed-Wing Mini-UAV and Gimbaled Camera","Target acquisition and continuous surveillance using fixed-wing UAVs is a difficult task due to the many degrees of freedom inherent in aircraft and gimbaled cameras. Mini-UAVs further complicate the problem by introducing severe restrictions on the size and weight of electro-optical sensor assemblies. We present a field-tested mini-UAV gimbal mechanism and flightpath generation algorithm as well as a human-UAV interaction scheme in which the operator manually flies the UAV to produce an estimate of the target position, then allows the aircraft to fly itself and control the gimbal while the operator re fines or moves the target position as required.",
VSched: Mixing Batch And Interactive Virtual Machines Using Periodic Real-time Scheduling,"We are developing Virtuoso, u system ,for distributed computing using virtual machines (VMs). Virtuoso must be uble to mix batch und interactive VMs on the same physical hardwure, while satisfiing constraint on re- sponsiveness und compute rates for each workload. VSched is the component of Virtuoso that provides this capability. VSched is an entirely user-level tool that interacts with the stock Linux kernel running below any type-11 virtual machine monitor to schedule VMs (indeed, any process) using a periodic real-time scheduling model. This abstraction allows compute rate and responsivness constraints to be straightforwardly described using a period und a slice within the period, and it allows,for just and simple admission control. This paper makes the case,for periodic real-time scheduling for VM-based computing environments, and then describes and evaluate.s VSched. It also applies VSched to scheduling parallel worklouds, showing that it can help a BSP application maintain a fixed stable performance despite externally caused loud imbalance.",
Agreement-Based Resource Management,"One of the criteria for the Grid infrastructure is the ability to share resources with nontrivial qualities of service. However, sharing resources in Grids is complicated in that is requires the ability bridge the differing policy requirements of the resource owners to create a consistent cross-organizational policy domain that delivers the necessary capability to the end user while respecting the policy requirements of the resource owner. Further complicating the management of Grid resources is the need to coordinate resource usage, the diversity of resource types and the variety of different management modes that may be used. We present a unifying resource management framework in which we can address these issues. The fundamental underlying concept in this framework is the representation of various resource management activities in terms of an agreement. Agreements abstract local management policy by representing an underlying resource strictly in terms of policy terms which it is willing to assert, and in doing so provides the basis for building a variety of alternative Grid resource management strategies. We introduce the concepts of agreement based resource management. We present a general agreement model and examine current resource management systems in the context of this model. We then discuss how agreement based resource management is being used as the basis for standards activities and next generation resource management services.","Resource management,
Grid computing,
Computer network management,
Computer networks,
Context modeling,
Computer science,
Application software,
Quality of service,
Bridges,
Processor scheduling"
An adaptive tissue characterization network for model-free visualization of dynamic contrast-enhanced magnetic resonance image data,"Dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) has become an important source of information to aid cancer diagnosis. Nevertheless, due to the multi-temporal nature of the three-dimensional volume data obtained from DCE-MRI, evaluation of the image data is a challenging task and tools are required to support the human expert. We investigate an approach for automatic localization and characterization of suspicious lesions in DCE-MRI data. It applies an artificial neural network (ANN) architecture which combines unsupervised and supervised techniques for voxel-by-voxel classification of temporal kinetic signals. The algorithm is easy to implement, allows for fast training and application even for huge data sets and can be directly used to augment the display of DCE-MRI data. To demonstrate that the system provides a reasonable assessment of kinetic signals, the outcome is compared with the results obtained from the model-based three-time-points (3TP) technique which represents a clinical standard protocol for analysing breast cancer lesions. The evaluation based on the DCE-MRI data of 12 cases indicates that, although the ANN is trained with imprecisely labeled data, the approach leads to an outcome conforming with 3TP without presupposing an explicit model of the underlying physiological process.","Adaptive systems,
Data visualization,
Magnetic resonance,
Artificial neural networks,
Cancer,
Lesions,
Kinetic theory,
Magnetic resonance imaging,
Information resources,
Humans"
Transition phase classification and prediction,"Most programs are repetitive, where similar behavior can be seen at different execution times. Proposed on-line systems automatically group these similar intervals of execution into phases, where the intervals in a phase have homogeneous behavior and similar resource requirements. These systems are driven by algorithms that dynamically classify intervals of execution into phases and predict phase changes. In this paper, we examine several improvements to dynamic phase classification and prediction. The first improvement is to appropriately deal with phase transitions. This modification identifies phase transitions for what they are, instead of classifying them into a new phase, which increases phase prediction accuracy. We also describe an adaptive system that dynamically adjusts classification thresholds and splits phases with poor homogeneity. This modification increases the homogeneity of the hardware metrics across the intervals in each phase. We improve phase prediction accuracy by applying confidence to phase prediction, and we develop architectures that can accurately predict the outcome of the next phase change, and the length of the next phase.","Accuracy,
Hardware,
Computer architecture,
Optimizing compilers,
Computer science,
Heuristic algorithms,
Adaptive systems,
Random access memory,
Statistics,
Yarn"
A visually guided swimming robot,"We describe recent results obtained with AQUA, a mobile robot capable of swimming, walking and amphibious operation. Designed to rely primarily on visual sensors, the AQUA robot uses vision to navigate underwater using servo-based guidance, and also to obtain high-resolution range scans of its local environment. This paper describes some of the pragmatic and logistic obstacles encountered, and provides an overview of some of the basic capabilities of the vehicle and its associated sensors. Moreover, this paper presents the first ever amphibious transition from walking to swimming.","Robot sensing systems,
Remotely operated vehicles,
Navigation,
Legged locomotion,
Computer science,
Communication system control,
Aquatic automobiles,
Robot vision systems,
Computer vision,
Marine vehicles"
Modeling the vulnerability discovery process,Security vulnerabilities in servers and operating systems are software defects that represent great risks. Both software developers and users are struggling to contain the risk posed by these vulnerabilities. The vulnerabilities are discovered by both developers and external testers throughout the life-span of a software system. A few models for the vulnerability discovery process have just been published recently. Such models will allow effective resource allocation for patch development and are also needed for evaluating the risk of vulnerability exploitation. Here we examine these models for the vulnerability discovery process. The models are examined both analytically and using actual data on vulnerabilities discovered in three widely-used systems. The applicability of the proposed models and significance of the parameters involved are discussed. The limitations of the proposed models are examined and major research challenges are identified,"Operating systems,
Software systems,
Software testing,
Resource management,
Investments,
Software reliability,
Computer science,
Computer security,
Life testing,
System testing"
Quasiconvex optimization for robust geometric reconstruction,"Geometric reconstruction problems in computer vision are often solved by minimizing a cost function that combines the reprojection errors in the 2D images. In this paper, we show that, for various geometric reconstruction problems, their reprojection error functions share a common and quasiconvex formulation. Based on the quasiconvexity, we present a novel quasiconvex optimization framework in which the geometric reconstruction problems are formulated as a small number of small-scale convex programs that are ready to solve. Our final reconstruction algorithm is simple and has intuitive geometric interpretation. In contrast to existing random sampling or local minimization approaches. Our algorithm is deterministic and guarantees a predefined accuracy of the minimization result. We demonstrate the effectiveness of our algorithm by experiments on both synthetic and real data","Robustness,
Image reconstruction,
Cameras,
Cost function,
Computer errors,
Computer vision,
Minimization methods,
Computer science,
Reconstruction algorithms,
Image sampling"
Multi-view reconstruction using photo-consistency and exact silhouette constraints: a maximum-flow formulation,"This paper describes a novel approach for reconstructing a closed continuous surface of an object from multiple calibrated color images and silhouettes. Any accurate reconstruction must satisfy (1) photo-consistency and (2) silhouette consistency constraints. Most existing techniques treat these cues identically in optimization frameworks where silhouette constraints are traded off against photo-consistency and smoothness priors. Our approach strictly enforces silhouette constraints, while optimizing photo-consistency and smoothness in a global graph-cut framework. We transform the reconstruction problem into computing max-flow/min-cut in a geometric graph, where any cut corresponds to a surface satisfying exact silhouette constraints (its silhouettes should exactly coincide with those of the visual hull); a minimum cut is the most photo-consistent surface amongst them. Our graph-cut formulation is based on the rim mesh, (the combinatorial arrangement of rims or contour generators from many views) which can be computed directly from the silhouettes. Unlike other methods, our approach enforces silhouette constraints without introducing a bias near the visual hull boundary and also recovers the rim curves. Results are presented for synthetic and real datasets.","Image reconstruction,
Layout,
Shape,
Surface reconstruction,
Constraint optimization,
Color,
Optimization methods,
Computer vision,
Computer science,
Surface treatment"
Manifold clustering,"Manifold learning has become a vital tool in data driven methods for interpretation of video, motion capture, and handwritten character data when they lie on a low dimensional, nonlinear manifold. This work extends manifold learning to classify and parameterize unlabeled data which lie on multiple, intersecting manifolds. This approach significantly increases the domain to which manifold learning methods can be applied, allowing parameterization of example manifolds such as figure eights and intersecting paths which are quite common in natural data sets. This approach introduces several technical contributions which may be of broader interest, including node-weighted multidimensional scaling and a fast algorithm for weighted low-rank approximation for rank-one weight matrices. We show examples for intersecting manifolds of mixed topology and dimension and demonstrations on human motion capture data.","Manifolds,
Clustering algorithms,
Multidimensional systems,
Approximation algorithms,
Independent component analysis,
Humans,
Computer science,
Data engineering,
Drives,
Learning systems"
Molecular imaging of small animals with a triple-head SPECT system using pinhole collimation,"Pinhole collimation yields high sensitivity when the distance from the object to the aperture is small, as in the case of imaging small animals. Fine-resolution images may be obtained when the magnification is large since this mitigates the effect of detector resolution. Large magnifications in pinhole single-photon emission computed tomography (SPECT) may be obtained by using a collimator whose focal length is many times the radius of rotation. This may be achieved without truncation if the gamma camera is large. We describe a commercially available clinical scanner mated with pinhole collimation and an external linear stage. The pinhole collimation gives high magnification. The linear stage allows for helical pinhole SPECT. We have used the system to image radiolabeled molecules in phantoms and small animals.","Molecular imaging,
Animals,
Collimators,
Apertures,
High-resolution imaging,
Detectors,
Image resolution,
Computed tomography,
Cameras,
Imaging phantoms"
Towards Autonomous Topological Place Detection Using the Extended Voronoi Graph,"Autonomous place detection has long been a major hurdle to topological map-building techniques. Theoretical work on topological mapping has assumed that places can be reliably detected by a robot, resulting in deterministic actions. Whether or not deterministic place detection is always achievable is controversial; however, even topological mapping algorithms that assume non-determinism benefit from highly reliable place detection. Unfortunately, topological map-building implementations often have hand-coded place detection algorithms that are brittle and domain dependent. This paper presents an algorithm for reliable autonomous place detection that is sensor and domain independent. A preliminary implementation of this algorithm for an indoor robot has demonstrated reliable place detection in real-world environments, with no a priori environmental knowledge. The implementation uses a local, scrolling 2D occupancy grid and a real-time calculated Voronoi graph to find the skeleton of the free space in the local surround. In order to utilize the place detection algorithm in non-corridor environments, we also introduce the extended Voronoi graph (EVG), which seamlessly transitions from a skeleton of a midline in corridors to a skeleton that follows walls in rooms larger than the local scrolling map.","Intelligent robots,
Skeleton,
Navigation,
Intelligent agent,
Detection algorithms,
Robot sensing systems,
Intelligent sensors,
Animals,
Large-scale systems,
Reliability theory"
Locally adaptive support-weight approach for visual correspondence search,"In this paper, we present a new area-based method for visual correspondence search that focuses on the dissimilarity computation. Local and area-based matching methods generally measure the similarity (or dissimilarity) between the image pixels using local support windows. In this approach, an appropriate support window should be selected adaptively for each pixel to make the measure reliable and certain. Finding the optimal support window with an arbitrary shape and size is, however, very difficult and generally known as an NP-hard problem. For this reason, unlike the existing methods that try to find an optimal support window, we adjusted the support-weight of each pixel in a given support window. The adaptive support-weight of a pixel is computed based on the photometric and geometric relationship with the pixel under consideration. Dissimilarity is then computed using the raw matching costs and support-weights of both support windows, and the correspondence is finally selected by the WTA (winner-takes-all) method. The experimental results for the rectified real images show that the proposed method successfully produces piecewise smooth disparity maps while preserving sharp depth discontinuities accurately.","Shape,
Pixel,
Costs,
Computer vision,
NP-hard problem,
Robot vision systems,
Laboratories,
Computer science,
Area measurement,
Photometry"
BugNet: continuously recording program execution for deterministic replay debugging,"Significant time is spent by companies trying to reproduce and fix the bugs that occur for released code. To assist developers, we propose the BugNet architecture to continuously record information on production runs. The information collected before the crash of a program can be used by the developers working in their execution environment to deterministically replay the last several million instructions executed before the crash. BugNet is based on the insight that recording the register file contents at any point in time, and then recording the load values that occur after that point can enable deterministic replaying of a program's execution. BugNet focuses on being able to replay the application's execution and the libraries it uses, but not the operating system. But our approach provides the ability to replay an application's execution across context switches and interrupts. Hence, BugNet obviates the need for tracking program I/O, interrupts and DMA transfers, which would have otherwise required more complex hardware support. In addition, BugNet does not require a final core dump of the system state for replaying, which significantly reduces the amount of data that must be sent back to the developer.","Debugging,
Computer bugs,
Computer crashes,
Hardware,
Core dumps,
Computer architecture,
Software systems,
Application software,
Computer science,
Continuous production"
Context saving and restoring for multitasking in reconfigurable systems,"Today's Field Programmable Gate Arrays (FPGAs) can be reconfigured partially, which makes it possible to share resources between various functional modules (hardware tasks) over time. This concept is well known in the area of conventional operating systems. However, in order to transfer resource sharing concepts to operating systems on FPGAs, several underlying mechanisms have to be developed. One of these mechanisms is to suspend hardware tasks and restart them at another time and/or another area of the FPGA. Addressing this problem, this paper discusses ways to save and restore the state information of a hardware task. Afterwards, an implementation of a state relocation mechanisms is presented that uses the standard configuration port. In contrast to similar approaches, we significantly reduce the amount of readback data by reading only those configuration frames that contain state information. We finally determine the time overhead for task relocation, which is essential for most multitasking concepts, like defragmentation.","Multitasking,
Hardware,
Field programmable gate arrays,
Registers,
Operating systems,
Resource management,
Circuits,
Runtime,
Computer science,
Software engineering"
A reputation-based mechanism for isolating selfish nodes in ad hoc networks,"For ad hoc networks to realize their potential in commercial deployments, it is important that they incorporate adequate security measures. Selfish behavior of autonomous network nodes could greatly disrupt network operation. Such behavior should be discouraged, detected, and isolated. In this paper, we propose a reputation-based mechanism to detect and isolate selfish nodes in an ad hoc network. The proposed mechanism allows a node to autonomously evaluate the ""reputation"" of its neighbors based on the completion of the requested service. The underlying principle is that when a node forwards a packet through one of its neighbors, it holds that neighbor responsible for the correct delivery of the packet to the destination. Our mechanism is efficient and immune to node collusion since, unlike most contemporary mechanisms for reputation-based trust, it does not depend on exchanging reputation information among nodes. We also explore various reputation functions and report on their effectiveness in isolating selfish nodes and reducing false positives. Our simulation results demonstrate that the choice of the reputation function greatly impacts performance and that the proposed mechanism, with a carefully selected function, is successful in isolating selfish nodes while maintaining false positives at a reasonably low level.","Intelligent networks,
Ad hoc networks,
Routing,
Computer science,
Computer security,
Electric variables measurement,
Spread spectrum communication,
Mobile communication,
Mobile ad hoc networks,
Collaboration"
Narrow passage sampling for probabilistic roadmap planning,"Probabilistic roadmap (PRM) planners have been successful in path planning of robots with many degrees of freedom, but sampling narrow passages in a robot's configuration space remains a challenge for PRM planners. This paper presents a hybrid sampling strategy in the PRM framework for finding paths through narrow passages. A key ingredient of the new strategy is the bridge test, which reduces sample density in many unimportant parts of a configuration space, resulting in increased sample density in narrow passages. The bridge test can be implemented efficiently in high-dimensional configuration spaces using only simple tests of local geometry. The strengths of the bridge test and uniform sampling complement each other naturally. The two sampling strategies are combined to construct the hybrid sampling strategy for our planner. We implemented the planner and tested it on rigid and articulated robots in 2-D and 3-D environments. Experiments show that the hybrid sampling strategy enables relatively small roadmaps to reliably capture the connectivity of configuration spaces with difficult narrow passages.","Sampling methods,
Orbital robotics,
Testing,
Path planning,
Bridges,
Computer science,
Sun,
Computational geometry,
Virtual prototyping,
Computational biology"
Cross-Platform Performance Prediction of Parallel Applications Using Partial Execution,"Performance prediction across platforms is increasingly important as developers can choose from a wide range of execution platforms. The main challenge remains to perform accurate predictions at a low-cost across different architectures. In this paper, we derive an affordable method approaching cross-platform performance translation based on relative performance between two platforms. We argue that relative performance can be observed without running a parallel application in full. We show that it suffices to observe very short partial executions of an application since most parallel codes are iterative and behave predictably manner after a minimal startup period. This novel prediction approach is observation-based. It does not require program modeling, code analysis, or architectural simulation. Our performance results using real platforms and production codes demonstrate that prediction derived from partial executions can yield high accuracy at a low cost. We also assess the limitations of our model and identify future research directions on observationbased performance prediction.","Computer architecture,
Hardware,
Application software,
Computer science,
Predictive models,
Permission,
Mathematics,
Laboratories,
Analytical models,
Production"
Despeckling of medical ultrasound images using data and rate adaptive lossy compression,"A novel technique for despeckling the medical ultrasound images using lossy compression is presented. The logarithm of the input image is first transformed to the multiscale wavelet domain. It is then shown that the subband coefficients of the log-transformed ultrasound image can be successfully modeled using the generalized Laplacian distribution. Based on this modeling, a simple adaptation of the zero-zone and reconstruction levels of the uniform threshold quantizer is proposed in order to achieve simultaneous despeckling and quantization. This adaptation is based on: 1) an estimate of the corrupting speckle noise level in the image; 2) the estimated statistics of the noise-free subband coefficients; and 3) the required compression rate. The Laplacian distribution is considered as a special case of the generalized Laplacian distribution and its efficacy is demonstrated for the problem under consideration. Context-based classification is also applied to the noisy coefficients to enhance the performance of the subband coder. Simulation results using a contrast detail phantom image and several real ultrasound images are presented. To validate the performance of the proposed scheme, comparison with two two-stage schemes, wherein the speckled image is first filtered and then compressed using the state-of-the-art JPEG2000 encoder, is presented. Experimental results show that the proposed scheme works better, both in terms of the signal to noise ratio and the visual quality.","Biomedical imaging,
Ultrasonic imaging,
Image coding,
Laplace equations,
Noise level,
Wavelet domain,
Adaptation model,
Image reconstruction,
Quantization,
Speckle"
"Theories, methods and tools in program comprehension: past, present and future","Program comprehension research can be characterized by both the theories that provide rich explanations about how programmers comprehend software, as well as the tools that are used to assist in comprehension tasks. During this talk the author review some of the key cognitive theories of program comprehension that have emerged. Using these theories as a canvas, the author then explores how tools that are popular today have evolved to support program comprehension. Specifically, the author discusses how the theories and tools are related and reflect on the research methods that were used to construct the theories and evaluate the tools. The reviewed theories and tools will be further differentiated according to human characteristics, program characteristics, and the context for the various comprehension tasks. Finally, the author predicts how these characteristics will change in the future and speculate on how a number of important research directions could lead to improvements in program comprehension tools and methods.","Programming profession,
Software tools,
Software engineering,
Computer science,
Electronic mail,
Humans,
Silver,
Buildings,
History,
Conferences"
A tree based router search engine architecture with single port memories,"Pipelined forwarding engines are used in core router to meet speed demands. Tree-based searches are pipelined across a number of stages to achieve high throughput, but this results in unevenly distributed memory. To address this imbalance, conventional approaches use either complex dynamic memory allocation schemes or over-provision each of the pipeline stages. This paper describes the microarchitecture of a novel network search processor which provides both high execution throughput and balanced memory distributor by dividing the tree into subtrees and allocating each subtree separately, allowing searches to begin at any pipeline stage. The architecture is validated by implementing and simulating state of the art solutions for IPv4 lookup, VPN forwarding and packet classification. The new pipeline scheme and memory allocator can provide searches with a memory allocation, efficiency that is within 1% of non-pipelined schemes.","Search engines,
Computer architecture,
Pipelines,
Computer science,
Throughput,
Virtual private networks,
High-speed networks,
Costs,
Databases,
Microarchitecture"
Trading off prediction accuracy and power consumption for context-aware wearable computing,"Context-aware mobile computing requires wearable sensors to acquire information about the user. Continuous sensing rapidly depletes the -wearable system's energy, which is a critically constrained resource. In this paper, we analyze the trade-off between power consumption and prediction accuracy of context classifiers working on dual-axis accelerometer data collected from the eWaich sensing and notification platform. We improve power consumption techniques by providing competitive classification performance even in the low frequency region of 1-10 Hz and for the highly erratic wrist based sensing location. Furthermore, we propose and analyze a collection of selective sampling strategies in order to reduce the number of required sensor readings and the computation cycles even further. Our results indicate that optimized sampling schemes can increase the deployment lifetime of a wearable computing platform by a factor of four without a significant loss in prediction accuracy.","Accuracy,
Energy consumption,
Wearable computers,
Sampling methods,
Wearable sensors,
Accelerometers,
Biomedical monitoring,
Mobile computing,
Frequency estimation,
Computer science"
A performance vs. cost framework for evaluating DHT design tradeoffs under churn,"Protocols for distributed hash tables (DHTs) incorporate features to achieve low latency for lookup requests in the face of churn, continuous changes in membership. These protocol features can include a directed identifier space, parallel lookups, pro-active flooding of membership changes, and stabilization protocols for maintaining accurate routing. In addition, DHT protocols have parameters that can be tuned to achieve different tradeoffs between lookup latency and communication cost due to maintenance traffic. The relative importance of the features and parameters is not well understood, because most previous work evaluates protocols on static networks. This paper presents a performance versus cost framework (PVC) that allows designers to compare the effects of different protocol features and parameter values. PVC views a protocol as consuming a certain amount of network bandwidth in order to achieve a certain lookup latency, and helps reveal the efficiency with which protocols use additional network resources to improve latency. To demonstrate the value of PVC, this paper simulates Chord, Kademlia, Kelips, OneHop, and Tapestry under different workloads and uses PVC to understand which features are more important under churn. PVC analysis shows that the key to efficiently using additional bandwidth is for a protocol to adjust its routing table size. It also shows that routing table stabilization is wasteful and can be replaced with opportunistic learning through normal lookup traffic. These insights combined demonstrate that PVC is a valuable tool for DHT designers.","Costs,
Delay,
Routing protocols,
Telecommunication traffic,
Bandwidth,
Gas insulated transmission lines,
Computer science,
Artificial intelligence,
Laboratories,
Floods"
Deobfuscation: reverse engineering obfuscated code,"In recent years, code obfuscation has attracted attention as a low cost approach to improving software security by making it difficult for attackers to understand the inner workings of proprietary software systems. This paper examines techniques for automatic deobfuscation of obfuscated programs, as a step towards reverse engineering such programs. Our results indicate that much of the effects of code obfuscation, designed to increase the difficulty of static analyses, can be defeated using simple combinations of straightforward static and dynamic analyses. Our results have applications to both software engineering and software security. In the context of software engineering, we show how dynamic analyses can be used to enhance reverse engineering, even for code that has been designed to be difficult to reverse engineer. For software security, our results serve as an attack model for code obfuscators, and can help with the development of obfuscation techniques that are more resilient to straightforward reverse engineering.","Reverse engineering,
Software engineering,
Protection,
Costs,
Application software,
Computer viruses,
Computer science,
Computer security,
Software systems,
Watermarking"
Motivation and nonmajors in computer science: identifying discrete audiences for introductory courses,"Traditional introductory computer science (CS) courses have had little success engaging non-computer science majors. At the Georgia Institute of Technology, Atlanta, where introductory CS courses are a requirement for CS majors and nonmajors alike, two tailored introductory courses were introduced as an alternative to the traditional course. The results were encouraging: more nonmajors succeeded (completed and passed) in tailored courses than in the traditional course, students expressed fewer negative reactions to the course content, and many reported that they would be interested in taking another tailored CS course. The authors present findings from a pilot study of the three courses and briefly discuss some of the issues surrounding the tailored courses for nonmajors: programming, context, choice of language, and classroom culture.",Computer science education
Agent-based evolutionary approach for interpretable rule-based knowledge extraction,"An agent-based evolutionary approach is proposed to extract interpretable rule-based knowledge. In the multiagent system, each fuzzy set agent autonomously determines its own fuzzy sets information, such as the number and distribution of the fuzzy sets. It can further consider the interpretability of fuzzy systems with the aid of hierarchical chromosome formulation and interpretability-based regulation method. Based on the obtained fuzzy sets, the Pittsburgh-style approach is applied to extract fuzzy rules that take both the accuracy and interpretability of fuzzy systems into consideration. In addition, the fuzzy set agents can cooperate with each other to exchange their fuzzy sets information and generate offspring agents. The parent agents and their offspring compete with each other through the arbitrator agent based on the criteria associated with the accuracy and interpretability to allow them to remain competitive enough to move into the next population. The performance with emphasis upon both the accuracy and interpretability based on the agent-based evolutionary approach is studied through some benchmark problems reported in the literature. Simulation results show that the proposed approach can achieve a good tradeoff between the accuracy and interpretability of fuzzy systems.","Fuzzy systems,
Fuzzy sets,
Data mining,
Multiagent systems,
Biological cells,
Computer science,
Evolutionary computation,
Decision making,
Fuzzy reasoning"
Delineating fluid-filled region boundaries in optical coherence tomography images of the retina,"We evaluate the ability of a deformable model to yield accurate shape descriptions of fluid-filled regions associated with age-related macular degeneration. Calculation of retinal thickness and volume by the current optical coherence tomography (OCT) system includes fluid-filled regions or lesions along with actual retinal tissue. In order to quantify these lesions independently from the retinal tissue, they must be outlined. A deformable model was applied to OCT images of retinas demonstrating cystoids and subretinal fluid spaces. Several implementation issues were addressed in order to choose appropriate parameters. The use of a nonlinear anisotropic diffusion filter to suppress speckle noise while at the same time preserving the edges of the original image was explored. Once the contours of the lesions were outlined, quantitative analysis of the surface area and volume of the lesions was performed. The deformable model could accurately outline fluid-filled regions within the retina. The detection method tested proved effective in capturing the complexity of fluid-filled regions in OCT images. Deformable models combined with nonlinear anisotropic diffusion filtering show promise in the detection of retinal features of interest for diagnosis in clinical OCT images. Thus, fluid-filled region detection may significantly aid in analysis of treatments and diagnosis.","Tomography,
Retina,
Deformable models,
Lesions,
Adaptive optics,
Nonlinear optics,
Optical filters,
Coherence,
Anisotropic magnetoresistance,
Shape"
Registration-assisted segmentation of real-time 3-D echocardiographic data using deformable models,"Real-time three-dimensional (3-D) echocardiography is a new imaging modality that presents the unique opportunity to visualize the complex 3-D shape and motion of the left ventricle (LV) in vivo and to measure the associated global and local function parameters. To take advantage of this opportunity in routine clinical practice, automatic segmentation of the LV in the 3-D echocardiographic data, usually hundreds of megabytes large, is essential. We report a new segmentation algorithm for this task. Our algorithm has two distinct stages, initialization of a deformable model and its refinement, which are connected by a dual ""voxel + wiremesh"" template. In the first stage, mutual-information-based registration of the voxel template with the image to be segmented helps initialize the wiremesh template. In the second stage, the wiremesh is refined iteratively under the influence of external and internal forces. The internal forces have been customized to preserve the nonsymmetric shape of the wiremesh template in the absence of external forces, defined using the gradient vector flow approach. The algorithm was validated against expert-defined segmentation and demonstrated acceptable accuracy. Our segmentation algorithm is fully automatic and has the potential to be used clinically together with real-time 3-D echocardiography for improved cardiovascular disease diagnosis.","Deformable models,
Image segmentation,
Echocardiography,
Ultrasonic imaging,
Cardiovascular diseases,
Biomedical engineering,
Shape measurement,
Iterative algorithms,
Heart,
Radiology"
VolumeShop: an interactive system for direct volume illustration,"Illustrations play a major role in the education process. Whether used to teach a surgical or radiologic procedure, to illustrate normal or aberrant anatomy, or to explain the functioning of a technical device, illustration significantly impacts learning. Although many specimens are readily available as volumetric data sets, particularly in medicine, illustrations are commonly produced manually as static images in a time-consuming process. Our goal is to create a fully dynamic three-dimensional illustration environment which directly operates on volume data. Single images have the aesthetic appeal of traditional illustrations, but can be interactively altered and explored. In this paper we present methods to realize such a system which combines artistic visual styles and expressive visualization techniques. We introduce a novel concept for direct multi-object volume visualization which allows control of the appearance of inter-penetrating objects via two-dimensional transfer functions. Furthermore, a unifying approach to efficiently integrate many non-photorealistic rendering models is presented. We discuss several illustrative concepts which can be realized by combining cutaways, ghosting, and selective deformation. Finally, we also propose a simple interface to specify objects of interest through three-dimensional volumetric painting. All presented methods are integrated into VolumeShop, an interactive hardware-accelerated application for direct volume illustration.","Interactive systems,
Rendering (computer graphics),
Data visualization,
Biomedical imaging,
Computer graphics,
Medical diagnostic imaging,
Bladder,
Image generation,
Computer science education,
Surgery"
Sequential parameter optimization,"Sequential parameter optimization is a heuristic that combines classical and modern statistical techniques to improve the performance of search algorithms. To demonstrate its flexibility, three scenarios are discussed: (1) no experience how to choose the parameter setting of an algorithm is available, (2) a comparison with other algorithms is needed, and (3) an optimization algorithm has to be applied effectively and efficiently to a complex real-world optimization problem. Although sequential parameter optimization relies on enhanced statistical techniques such as design and analysis of computer experiments, it can be performed algorithmically and requires basically the specification of the relevant algorithm's parameters","Algorithm design and analysis,
Performance analysis,
Computer science,
Design optimization,
Design for experiments,
Stochastic processes,
Modems,
Statistics,
Evolutionary computation,
Genetic algorithms"
Determination of mechanical and electronic shifts for pinhole SPECT using a single point source,"The effects of uncompensated electronic and mechanical shifts may compromise the resolution of pinhole single photon emission computed tomography. The resolution degradation due to uncompensated shifts is estimated through simulated data. A method for determining the transverse mechanical and axial electronic shifts is described and evaluated. This method assumes that the tilt of the detector and the radius of rotation (ROR) are previously determined using another method. When this assumption is made, it is possible to determine the rest of the calibration parameters using a single point source. A method that determines the electronic and mechanical shifts as well as the tilt has been previously described; this method requires three point sources. It may be reasonable in most circumstances to calibrate tilt much less frequently than the mechanical shifts since the tilt is a property of the scanner whereas the mechanical shift may change every time the collimator is replaced. An alternative method for determining the ROR may also be used. Lastly, we take the view that the transverse electronic shift and the focal length change slowly and find these parameters independently.","Image reconstruction,
Single photon emission computed tomography,
Calibration,
Collimators,
Biomedical imaging,
Radiology,
Biomedical engineering,
Degradation,
Computational modeling,
Detectors"
The conceptual cohesion of classes,"While often defined in informal ways, software cohesion reflects important properties of modules in a software system. Cohesion measurement has been used for quality assessment, fault proneness prediction, software modularization, etc. Existing approaches to cohesion measurement in object-oriented software are largely based on the structural information of the source code, such as attribute references in methods. These measures reflect particular interpretations of cohesion and try to capture different aspects of cohesion and no single cohesion metric or suite is accepted as standard measurement for cohesion. The paper proposes a new set of measures for the cohesion of individual classes within an OO software system, based on the analysis of the semantic information embedded in the source code, such as comments and identifiers. A case study on open source software is presented, which compares the new measures with an extensive set of existing metrics. The differences and similarities among the approaches and results are discussed and analyzed.","Software measurement,
Software quality,
Open source software,
Software systems,
Particle measurements,
Software reusability,
Computer science,
Quality assessment,
Measurement standards,
Information analysis"
The determination of the optimal length of crystal blanks in quartz crystal resonators,"It is well understood that the strong coupling of thickness-shear and flexural vibrations in piezoelectric crystal plates only occurs at specific length at which the vibration mode conversion, like the flexural mode gradually converting to thickness-shear mode while the thickness-shear mode converting to higher-order flexural mode, happens. It is important to avoid the strong coupling of modes in a crystal resonator that uses thickness-shear vibrations to enhance the energy trapping. To achieve such a design goal, the length of a crystal blank should be carefully chosen such that the coupling is at its weakest, which usually is in the middle of two strong coupling points. Through a closer examination of the frequency spectra, or the frequency-length relationship in this study, we can see that the strong coupling points appear periodically. This implies that we can find exact locations with the plate theory that predicts the resonance frequency. Based on this observation, we first use the first-order Mindlin plate theory with the precise thickness-shear frequency, which is normalized to one, to find corresponding wavenumbers. Then the length as a variable is solved from the coupled frequency equation for exact coupling points in a crystal plate of AT-cut quartz. The optimal length of a crystal blank in the simplest resonator model is calculated for the coupled thickness-shear, flexural, and extensional vibrations. The solutions and the method will be important in the determination of optimal length of a crystal blank in the resonator design process.","Vibrations,
Equations,
Cutoff frequency,
Process design,
Materials science and technology,
Resonance,
Resonant frequency,
Analytical models,
Anisotropic magnetoresistance,
Electrodes"
Is Levenberg-Marquardt the most efficient optimization algorithm for implementing bundle adjustment?,"In order to obtain optimal 3D structure and viewing parameter estimates, bundle adjustment is often used as the last step of feature-based structure and motion estimation algorithms. Bundle adjustment involves the formulation of a large scale, yet sparse minimization problem, which is traditionally solved using a sparse variant of the Levenberg-Marquardt optimization algorithm that avoids storing and operating on zero entries. This paper argues that considerable computational benefits can be gained by substituting the sparse Levenberg-Marquardt algorithm in the implementation of bundle adjustment with a sparse variant of Powell's dog leg non-linear least squares technique. Detailed comparative experimental results provide strong evidence supporting this claim","Least squares methods,
Large-scale systems,
Iterative algorithms,
Equations,
Motion estimation,
Minimization methods,
Leg,
Cameras,
Computer science,
Parameter estimation"
EGT for multiple view geometry and visual servoing: robotics vision with pinhole and panoramic cameras,"The Epipolar Geometry Toolbox (EGT) for MATLAB is a software package targeted to research and education in computer vision and robotics visual servoing. It provides the user with a wide set of functions for designing multicamera systems for both pinhole and panoramic cameras. Functions include camera placement and visualization, computation, and estimation of epipolar geometry entities. The compatibility of EGT with the Robotics Toolbox enables users to address general vision-based control issues. Two applications of EGT to visual servoing tasks are examined in this article. Several epipolar geometry estimation algorithms have been implemented.","Robot vision systems,
Computational geometry,
Visual servoing,
Cameras,
MATLAB,
Software packages,
Computer science education,
Computer vision,
Educational robots,
Visualization"
The class blueprint: visually supporting the understanding of glasses,"Understanding source code is an important task in the maintenance of software systems. Legacy systems are not only limited to procedural languages, but are also written in object-oriented languages. In such a context, understanding classes is a key activity as they are the cornerstone of the object-oriented paradigm and the primary abstraction from which applications are built. Such an understanding is however difficult to obtain because of reasons such as the presence of late binding and inheritance. A first level of class understanding consists of the understanding of its overall structure, the control flow among its methods, and the accesses on its attributes. We propose a novel visualization of classes called class blueprint that is based on a semantically enriched visualization of the internal structure of classes. This visualization allows a software engineer to build a first mental model of a class that he validates via opportunistic code-reading. Furthermore, we have identified visual patterns that represent recurrent situations and as such convey additional, information to the viewer. The contributions of this article are the class blueprint, a novel visualization of the internal structure of classes, the identification of visual patterns, and the definition of a vocabulary based on these visual patterns. We have performed several case studies of which one is presented in depth, and validated the usefulness of the approach in a controlled experiment.","Visualization,
Software maintenance,
Software systems,
Application software,
Cognitive science,
Vocabulary,
Programming profession,
Reverse engineering,
Phase measurement,
Software measurement"
An analytical model for the energy hole problem in many-to-one sensor networks,,"Analytical models,
Intelligent networks,
Energy consumption,
Sensor phenomena and characterization,
Telecommunication traffic,
Traffic control,
Computer science,
Spread spectrum communication,
Relays,
Mathematical model"
Comparison of clustering algorithms in the context of software evolution,"To aid software analysis and maintenance tasks, a number of software clustering algorithms have been proposed to automatically partition a software system into meaningful subsystems or clusters. However, it is unknown whether these algorithms produce similar meaningful clusterings for similar versions of a real-life software system under continual change and growth. This paper describes a comparative study of six software clustering algorithms. We applied each of the algorithms to subsequent versions from five large open source systems. We conducted comparisons based on three criteria respectively: stability (Does the clustering change only modestly as the system undergoes modest updating?), authoritative-ness (Does the clustering reasonably approximate the structure an authority provides?) and extremity of cluster distribution (Does the clustering avoid huge clusters and many very small clusters?). Experimental results indicate that the studied algorithms exhibit distinct characteristics. For example, the clusterings from the most stable algorithm bear little similarity to the implemented system structure, while the clusterings from the least stable algorithm has the best cluster distribution. Based on obtained results, we claim that current automatic clustering algorithms need significant improvement to provide continual support for large software projects.","Clustering algorithms,
Software algorithms,
Software maintenance,
Software systems,
Partitioning algorithms,
Computer architecture,
Documentation,
Algorithm design and analysis,
Open source software,
Computer science"
"High-performance, power-aware distributed computing for scientific applications","The PowerPack framework enables distributed systems to profile, analyze, and conserve energy in scientific applications using dynamic voltage scaling. For one common benchmark, the framework achieves more than 30 percent energy savings with minimal performance impact.","Distributed computing,
Energy consumption,
Biological system modeling,
Computational modeling,
Power system reliability,
Temperature,
Costs,
Weather forecasting,
Large-scale systems,
DNA"
VESTA: A statistical model-checker and analyzer for probabilistic systems,We give a brief overview of a statistical model-checking and analysis tool VESTA. VESTA is a tool for statistical analysis of probabilistic systems. It supports statistical model-checking and statistical evaluation of expected values of temporal expressions.,"Probabilistic logic,
Logic testing,
Stochastic processes,
Discrete event simulation,
Computer science,
Statistical analysis,
Database languages,
Independent component analysis,
Algorithm design and analysis,
Computational modeling"
Design and implementation tradeoffs for wide-area resource discovery,"This paper describes the design and implementation of SWORD, a scalable resource discovery service for wide-area distributed systems. In contrast to previous systems, SWORD allows users to describe desired resources as a topology of interconnected groups with required intragroup, intergroup, and per-node characteristics, along with the utility that the application derives from various ranges of values of those characteristics. This design gives users the flexibility to find geographically distributed resources for applications that are sensitive to both node and network characteristics, and allows the system to rank acceptable configurations based on their quality for that application. We explore a variety of architectures to deliver SWORD's functionality in a scalable and highly-available manner. A 1000-node ModelNet evaluation using a workload of measurements collected from PlanetLab shows that an architecture based on 4-node server cluster sites at network peering facilities outperforms a decentralized DHT-based resource discovery infrastructure for all but the smallest number of sites. While such a centralized architecture shows significant promise, we find that our decentralized implementation, both in emulation and running continuously on over 200 PlanetLab nodes, performs well while benefiting from the DHT's self-healing properties.","Peer to peer computing,
Distributed computing,
Computer science,
Topology,
Extraterrestrial measurements,
Computer networks,
Grid computing,
Design engineering,
Computer architecture,
Emulation"
Group rekeying for filtering false data in sensor networks: a predistribution and local collaboration-based approach,"When a sensor network is deployed in hostile environments, the adversary may compromise some sensor nodes, and use the compromised nodes to inject false sensing reports or modify the reports sent by other nodes. In order to defend against the attacks with low cost, researchers have proposed symmetric group key-based en-route filtering schemes, such as SEF [F. Ye et al., March 2004] and I-LHAP [S. Zhu et al., 2004]. However, if the adversary has compromised a large number of nodes, many group keys can be captured, and the filtering schemes may become ineffective or even useless. To deal with node compromise, the compromised nodes should be identified and the innocent nodes should update their group keys. Some existing intruder identification schemes can be used to identify the compromised nodes, but most existing group rekeying schemes are not suitable for sensor networks since they have large overhead and are not scalable. To address the problem, we propose a family of predistribution and local collaboration-based group rekeying (PCGR) schemes. These schemes are designed based on the ideas that future group keys can be preloaded to the sensor nodes before deployment, and neighbors can collaborate to protect and appropriately use the preloaded keys. Extensive analyses and simulations are conducted to evaluate the proposed schemes, and the results show that the proposed schemes can achieve a good level of security, outperform most previous group rekeying schemes, and significantly improve the effectiveness of filtering false data.","Filtering,
Intelligent networks,
Collaboration,
Sensor systems,
Data security,
Monitoring,
Computer science,
Costs,
Protection,
Analytical models"
System test case prioritization of new and regression test cases,"Test case prioritization techniques have been shown to be beneficial for improving regression-testing activities. With prioritization, the rate of fault detection is improved, thus allowing testers to detect faults earlier in the system-testing phase. Most of the prioritization techniques to date have been code coverage-based. These techniques may treat all faults equally. We build upon prior test case prioritization research with two main goals: (1) to improve user-perceived software quality in a cost effective way by considering potential defect severity and (2) to improve the rate of detection of severe faults during system-level testing of new code and regression testing of existing code. We present a value-driven approach to system-level test case prioritization called the prioritization of requirements for test (PORT). PORT prioritizes system test cases based upon four factors: requirements volatility, customer priority, implementation complexity, and fault proneness of the requirements. We conducted a PORT case study on four projects developed by students in advanced graduate software testing class. Our results show that PORT prioritization at the system level improves the rate of detection of severe faults. Additionally, customer priority was shown to be one of the most important prioritization factors contributing to the improved rate of fault detection.","System testing,
Computer aided software engineering,
Software testing,
Fault detection,
Software quality,
Costs,
Software engineering,
Computer science,
Statistical analysis,
Phase detection"
Scan Matching in the Hough Domain,"Scan matching is used as a building block in many robotic applications, for localization and simultaneous localization and mapping (SLAM). Although many techniques have been proposed for scan matching in the past years, more efficient and effective scan matching procedures allow for improvements of such associated problems. In this paper we present a new scan matching method that, exploiting the properties of the Hough domain, allows for combining advantages of dense scan matching algorithms with feature-based ones.","Acoustic noise,
Working environment noise,
Simultaneous localization and mapping,
Noise robustness,
Feature extraction,
Sensor phenomena and characterization,
Remuneration,
Computer science,
Robots,
Application software"
IAM-OnDB - an on-line English sentence database acquired from handwritten text on a whiteboard,In this paper we present IAM-OnDB - a new large online handwritten sentences database. It is publicly available and consists of text acquired via an electronic interface from a whiteboard. The database contains about 86 K word instances from an 11 K dictionary written by more than 200 writers. We also describe a recognizer for unconstrained English text that was trained and tested using this database. This recognizer is based on hidden Markov models (HMMs). In our experiments we show that by using larger training sets we can significantly increase the word recognition rate. This recognizer may serve as a benchmark reference for future research.,"Handwriting recognition,
Image databases,
Hidden Markov models,
Image recognition,
Text recognition,
Computer science,
Dictionaries,
Benchmark testing,
Writing,
Personal communication networks"
Adaptive tuning of the sampling domain for dynamic-domain RRTs,"Sampling based planners have become increasingly efficient in solving the problems of classical motion planning and its applications. In particular, techniques based on the rapidly-exploring random trees (RRTs) have generated highly successful single-query planners. Recently, a variant of this planner called dynamic-domain RRT was introduced by Yershova et al. (2005). It relies on a new sampling scheme that improves the performance of the RRT approach on many motion planning problems. One of the drawbacks of this method is that it introduces a new parameter that requires careful tuning. In this paper we analyze the influence of this parameter and propose a new variant of the dynamic-domain RRT, which iteratively adapts the sampling domain for the Voronoi region of each node during the search process. This allows automatic tuning of the parameter and significantly increases the robustness of the algorithm. The resulting variant of the algorithm has been tested on several path planning problems.","Sampling methods,
Robustness,
Iterative algorithms,
Motion planning,
Robots,
Geometry,
Computer science,
Urban planning,
Application software,
Testing"
Inference of non-overlapping camera network topology by measuring statistical dependence,"We present an approach for inferring the topology of a camera network by measuring statistical dependence between observations in different cameras. Two cameras are considered connected if objects seen departing in one camera is seen arriving in the other. This is captured by the degree of statistical dependence between the cameras. The nature of dependence is characterized by the distribution of observation transformations between cameras, such as departure to arrival transition times, and color appearance. We show how to measure statistical dependence when the correspondence between observations in different cameras is unknown. This is accomplished by non-parametric estimates of statistical dependence and Bayesian integration of the unknown correspondence. Our approach generalizes previous work which assumed restricted parametric transition distributions and only implicitly dealt with unknown correspondence. Results are shown on simulated and real data. We also describe a technique for learning the absolute locations of the cameras with Global Positioning System (GPS) side information","Network topology,
Smart cameras,
Global Positioning System,
Surveillance,
Monitoring,
Computer science,
Artificial intelligence,
Laboratories,
Bayesian methods,
Traffic control"
A Lower Bound on Convergence of a Distributed Network Consensus Algorithm,"This paper gives a lower bound on the convergence rate of a class of network consensus algorithms. Two different approaches using directed graphs as a main tool are introduced: one is to compute the ""scrambling constants"" of stochastic matrices associated with ""neighbor shared graphs"" and the other is to analyze random walks on a sequence of graphs. Both approaches prove that the time to reach consensus within a dynamic network is logarithmic in the relative error and is in worst case exponential in the size of the network.",
A method to track cortical surface deformations using a laser range scanner,"This paper reports a novel method to track brain shift using a laser-range scanner (LRS) and nonrigid registration techniques. The LRS used in this paper is capable of generating textured point-clouds describing the surface geometry/intensity pattern of the brain as presented during cranial surgery. Using serial LRS acquisitions of the brain's surface and two-dimensional (2-D) nonrigid image registration, we developed a method to track surface motion during neurosurgical procedures. A series of experiments devised to evaluate the performance of the developed shift-tracking protocol are reported. In a controlled, quantitative phantom experiment, the results demonstrate that the surface shift-tracking protocol is capable of resolving shift to an accuracy of approximately 1.6 mm given initial shifts on the order of 15 mm. Furthermore, in a preliminary in vivo case using the tracked LRS and an independent optical measurement system, the automatic protocol was able to reconstruct 50% of the brain shift with an accuracy of 3.7 mm while the manual measurement was able to reconstruct 77% with an accuracy of 2.1 mm. The results suggest that a LRS is an effective tool for tracking brain surface shift during neurosurgery.","Surface emitting lasers,
Protocols,
Neurosurgery,
Surface reconstruction,
Image reconstruction,
Surface texture,
Geometrical optics,
Cranial,
Laser surgery,
Two dimensional displays"
Automated processing of shoeprint images based on the Fourier transform for use in forensic science,The development of a system for automatically sorting a database of shoeprint images based on the outsole pattern in response to a reference shoeprint image is presented. The database images are sorted so that those from the same pattern group as the reference shoeprint are likely to be at the start of the list. A database of 476 complete shoeprint images belonging to 140 pattern groups was established with each group containing two or more examples. A panel of human observers performed the grouping of the images into pattern categories. Tests of the system using the database showed that the first-ranked database image belongs to the same pattern category as the reference image 65 percent of the time and that a correct match appears within the first 5 percent of the sorted images 87 percent of the time. The system has translational and rotational invariance so that the spatial positioning of the reference shoeprint images does not have to correspond with the spatial positioning of the shoeprint images of the database. The performance of the system for matching partial-prints was also determined.,
Sink equilibria and convergence,"We introduce the concept of a sink equilibrium. A sink equilibrium is a strongly connected component with no outgoing arcs in the strategy profile graph associated with a game. The strategy profile graph has a vertex set induced by the set of pure strategy profiles; its arc set corresponds to transitions between strategy profiles that occur with nonzero probability. (Here our focus will just be on the special case in which the strategy profile graph is actually a best response graph; that is, its arc set corresponds exactly to best response moves that result from myopic or greedy behaviour). We argue that there is a natural convergence process to sink equilibria in games where agents use pure strategies. This leads to an alternative measure of the social cost of a lack of coordination, the price of sinking, which measures the worst case ratio between the value of a sink equilibrium and the value of the socially optimal solution. We define the value of a sink equilibrium to be the expected social value of the steady state distribution induced by a random walk on that sink. We illustrate the value of this measure in three ways. Firstly, we show that it may more accurately reflects the inefficiency of uncoordinated solutions in competitive games when the use of pure strategies is the norm. In particular, we give an example (a valid-utility game) in which the game converges to solutions which are a factor n worse than socially optimal. The price of sinking is indeed n, but the price of anarchy is close to 1. Secondly, sink equilibria always exist. Thus, even in games in which pure strategy Nash equilibria (PSNE) do not exist, we can still calculate the price of sinking. Thirdly, we show that bounding the price of sinking can have important implications for the speed of convergence to socially good solutions in games where the agents make best response moves in a random order. We present two examples to illustrate our ideas. (i) Unsplittable selfish routing (and weighted congestion games):we prove that the price of sinking for the weighted unsplittable flow version of the selfish routing problem (for bounded-degree polynomial latency functions) is at most O(2/sup 2d/ d/sup 2d + 3/). In comparison, we give instances of these games without any PSNE. Moreover, our proof technique implies fast convergence to socially good (approximate) solutions. This is in contrast to the negative result of Fabrikant, Papadimitriou, and Talwar (2004) showing the existence of exponentially long best-response paths. (ii) Valid-utility games: we show that for valid-utility games the price of sinking is at most n+1; thus the worst case price of sinking in a valid-utility game is between it and n+1. We use our proof to show fast convergence to constant factor approximate solutions in basic-utility games. In addition, we present a hardness result which shows that, in general, there might be states that are exponentially far from any sink equilibrium in valid-utility games. We prove this by showing that the problem of finding a sink equilibrium (or a PSNE) in valid-utility games is PLS-complete.","Convergence,
Routing,
Cost function,
Steady-state,
Polynomials,
Delay,
Performance analysis,
Control systems,
Nash equilibrium,
Computer science"
Prospective motion correction of X-ray images for coronary interventions,"A method for prospective motion correction of X-ray imaging of the heart is presented. A 3D+t coronary model is reconstructed from a biplane coronary angiogram obtained during free breathing. The deformation field is parameterized by cardiac and respiratory phase, which enables the estimation of the state of the arteries at any phase of the cardiac-respiratory cycle. The motion of the three-dimensional (3-D) coronary model is projected onto the image planes and used to compute a dewarping function for motion correcting the images. The use of a 3-D coronary model facilitates motion correction of images acquired with the X-ray system at arbitrary orientations. The performance of the algorithm was measured by tracking the motion of selected left coronary landmarks using a template matching cross-correlation. In three patients, we motion corrected the same images used to construct their 3D+t coronary model. In this best case scenario, the algorithm reduced the motion of the landmarks by 84%-85%, from mean RMS displacements of 12.8-14.6 pixels to 2.1-2.2 pixels. Prospective motion correction was tested in five patients by building the coronary model from one dataset, and correcting a second dataset. The patient's cardiac and respiratory phase are monitored and used to calculate the appropriate correction parameters. The results showed a 48%-63% reduction in the motion of the landmarks, from a mean RMS displacement of 11.5-13.6 pixels to 4.4-7.1 pixels.","X-ray imaging,
Heart,
Image reconstruction,
Phase estimation,
State estimation,
Arteries,
Motion measurement,
Tracking,
Testing,
Buildings"
Result verification and trust-based scheduling in peer-to-peer grids,"Peer-to-peer grids that seek to harvest idle cycles available throughout the Internet are vulnerable to hosts that fraudulently accept computational tasks and then maliciously return arbitrary results. Current strategies employed by popular cooperative computing grids, such as SETI@Home, rely heavily on task replication to check results. However, result verification through replication suffers from two potential shortcomings: (1) susceptibility to collusion in which a group of malicious hosts conspire to return the same bad results and (2) high fixed overhead incurred by running redundant copies of the task. In this paper, we first propose a scheme called Quiz to combat collusion. The basic idea of Quiz is to insert indistinguishable quiz tasks with verifiable results known to the client within a package containing several normal tasks. The client can then accept or reject the normal task results based on the correctness of quiz results. Our second contribution is the promotion of trust-based task scheduling in peer-to-peer grids. By coupling a reputation system with the basic verification schemes - Replication and Quiz - a client can potentially avoid malicious hosts and also reduce the overhead of verification for trusted hosts.","Peer to peer computing,
Grid computing,
Processor scheduling,
Distributed computing,
Computer crime,
Internet,
Information science,
Packaging,
Web server,
Open systems"
Service-oriented environments for dynamically interacting with mesoscale weather,"Within a decade after John von Neumann and colleagues conducted the first experimental weather forecast on the ENIAC computer in the late 1940s, numerical models of the atmosphere become the foundation of modern-day weather forecasting and one of the driving application areas in computer science. This article describes research that is enabling a major shift toward dynamically adaptive responses to rapidly changing environmental conditions.","Weather forecasting,
Numerical models,
Meteorology,
Atmosphere,
Floods,
Tornadoes,
Economic forecasting,
Environmental economics,
Computer science,
Wind"
Computer-aided placement of deep brain stimulators: from planningto intraoperative guidance,"In current practice, optimal placement of deep-brain stimulators (DBSs) used to treat movement disorders in patients with Parkinson's disease and essential tremor is an iterative procedure. A target is chosen preoperatively based on anatomical landmarks identified on magnetic resonance images. This point is used as an initial position that is refined intraoperatively using both microelectrode recordings and macrostimulation. In this paper, we report on our current progress toward developing a system for the computer-assisted preoperative selection of target points and for the intraoperative adjustment of these points. The system consists of a deformable atlas of optimal target points that can be used to select automatically the preoperative target, of an electrophysiological atlas, and of an intraoperative interface. Results we have obtained show that automatic prediction of target points is an achievable goal. Our results also indicate that electrophysiological information could be used to resolve structures not visible in anatomic images, thus improving both preoperative and intraoperative guidance. Our intraoperative system has reached the stage of a working prototype and we compare targeting accuracy as well as the number of paths needed to reach the targets with our system and with the method in current clinical use.",
Evaluation of fully 3-D emission mammotomography with a compact cadmium zinc telluride detector,"A compact, dedicated cadmium zinc telluride (CZT) gamma camera coupled with a fully three-dimensional (3-D) acquisition system may serve as a secondary diagnostic tool for volumetric molecular imaging of breast cancers, particularly in cases when mammographic findings are inconclusive. The developed emission mammotomography system comprises a medium field-of-view, quantized CZT detector and 3-D positioning gantry. The intrinsic energy resolution, sensitivity and spatial resolution of the detector are evaluated with Tc-99m (140 keV) filled flood sources, capillary line sources, and a 3-D frequency-resolution phantom. To mimic realistic human pendant, uncompressed breast imaging, two different phantom shapes of an average sized breast, and three different lesion diameters are imaged to evaluate the system for 3-D mammotomography. Acquisition orbits not possible with conventional emission, or transmission, systems are designed to optimize the viewable breast volume while improving sampling of the breast and anterior chest wall. Complications in camera positioning about the patient necessitate a compromise in these two orbit design criteria. Image quality is evaluated with signal-to-noise ratios and contrasts of the lesions, both with and without additional torso phantom background. Reconstructed results indicate that 3-D mammotomography, incorporating a compact CZT detector, is a promising, dedicated breast imaging technique for visualization of tumors <1 cm in diameter. Additionally, there are no outstanding trajectories that consistently yield optimized quantitative lesion imaging parameters. Qualitatively, imaging breasts with realistic torso backgrounds (out-of-field activity) substantially alters image characteristics and breast morphology unless orbits which improve sampling are utilized. In practice, the sampling requirement may be less strict than initially anticipated.","Cadmium compounds,
Zinc compounds,
Breast,
Detectors,
Imaging phantoms,
Lesions,
Orbits,
Image sampling,
Cameras,
Torso"
Online learning of probabilistic appearance manifolds for video-based recognition and tracking,"This paper presents an online learning algorithm to construct from video sequences an image-based representation that is useful for recognition and tracking. For a class of objects (e.g., human faces), a generic representation of the appearances of the class is learned off-line. From video of an instance of this class (e.g., a particular person), an appearance model is incrementally learned on-line using the prior generic model and successive frames from the video. More specifically, both the generic and individual appearances are represented as an appearance manifold that is approximated by a collection of sub-manifolds (named pose manifolds) and the connectivity between them. In turn, each sub-manifold is approximated by a low-dimensional linear sub-space while the connectivity is modeled by transition probabilities between pairs of sub-manifolds. We demonstrate that our online learning algorithm constructs an effective representation for face tracking, and its use in video-based face recognition compares favorably to the representation constructed with a batch technique.","Video sequences,
Principal component analysis,
Image recognition,
Computer science,
Machine learning,
Machine learning algorithms,
Face recognition,
Streaming media,
Manifolds,
Humans"
A recursive greedy algorithm for walks in directed graphs,"Given an arc-weighted directed graph G = (V, A, /spl lscr/) and a pair of nodes s, t, we seek to find an s-t walk of length at most B that maximizes some given function f of the set of nodes visited by the walk. The simplest case is when we seek to maximize the number of nodes visited: this is called the orienteering problem. Our main result is a quasi-polynomial time algorithm that yields an O(log OPT) approximation for this problem when f is a given submodular set function. We then extend it to the case when a node v is counted as visited only if the walk reaches v in its time window [R(v), D(v)]. We apply the algorithm to obtain several new results. First, we obtain an O(log OPT) approximation for a generalization of the orienteering problem in which the profit for visiting each node may vary arbitrarily with time. This captures the time window problem considered earlier for which, even in undirected graphs, the best approximation ratio known [Bansal, N et al. (2004)] is O(log/sup 2/ OPT). The second application is an O(log/sup 2/ k) approximation for the k-TSP problem in directed graphs (satisfying asymmetric triangle inequality). This is the first non-trivial approximation algorithm for this problem. The third application is an O(log/sup 2/ k) approximation (in quasi-poly time) for the group Steiner problem in undirected graphs where k is the number of groups. This improves earlier ratios (Garg, N et al.) by a logarithmic factor and almost matches the inapproximability threshold on trees (Halperin and Krauthgamer, 2003). This connection to group Steiner trees also enables us to prove that the problem we consider is hard to approximate to a ratio better than /spl Omega/(log/sup 1-/spl epsi// OPT), even in undirected graphs. Even though our algorithm runs in quasi-poly time, we believe that the implications for the approximability of several basic optimization problems are interesting.","Greedy algorithms,
Optimized production technology,
Approximation algorithms,
Vehicles,
Traveling salesman problems,
Routing,
Steiner trees,
Tree graphs,
Floors,
Delay"
How Do We Evaluate Artificial Immune Systems?,"The field of Artificial Immune Systems (AIS) concerns the study and development of computationally interesting abstractions of the immune system. This survey tracks the development of AIS since its inception, and then attempts to make an assessment of its usefulness, defined in terms of â€˜distinctivenessâ€™ and â€˜effectiveness.â€™ In this paper, the standard types of AIS are examinedâ€”Negative Selection, Clonal Selection and Immune Networksâ€”as well as a new breed of AIS, based on the immunological â€˜danger theory.â€™ The paper concludes that all types of AIS largely satisfy the criteria outlined for being useful, but only two types of AIS satisfy both criteria with any certainty.","danger theory,
Artificial immune systems,
critical evaluation,
negative selection,
clonal selection,
immune network models"
Using structural context to recommend source code examples,"When coding to a framework, developers often become stuck, unsure of which class to subclass, which objects to instantiate and which methods to call. Example code that demonstrates the use of the framework can help developers make progress on their task. In this paper, we describe an approach for locating relevant code in an example repository that is based on heuristically matching the structure of the code under development to the example code. Our tool improves on existing approaches in two ways. First, the structural context needed to query the repository is extracted automatically from the code, freeing the developer from learning a query language or from writing their code in a particular style. Second, the repository can be generated easily from existing applications. We demonstrate the utility of this approach by reporting on a case study involving two subjects completing four programming tasks within the Eclipse integrated development environment framework.","Database languages,
Writing,
Permission,
Computer science,
Utility programs,
Programming environments,
Programming profession,
Software tools,
Application software,
Protocols"
A graph-based approach to Web services composition,"Automatic composition of Web services has drawn a great deal of attention recently. By composition, we mean taking advantage of currently existing Web services to provide a new service that does not exist on its own. Therefore, in order to have a more complex service, we can use some semantically related simpler Web services and execute them in such a way that the whole set provides the desired service. There are Web service specification languages that specify semantic properties of Web services. These languages are helpful in searching for those Web services that can participate in a composition. This work is aimed at searching among Web services in order to find those whose composition provides a specific behavior. Those Web services found after this search are incrementally composed together to build a new service that realizes that behavior. Our technique takes advantage of graph structures and also a particular formalism called interface automata.",
A New Convergence Proof of Fuzzy c-Means,"In this letter, we give a new, more direct derivation of the convergence properties of the fuzzy c-means (FCM) algorithm, using the equivalence between the original and reduced FCM criterion. From the point of view of the reduced criterion, the FCM algorithm is simply a steepest descent algorithm with variable steplength. We prove that steplength adjustment follows from the majorization principle for steplength. By applying the majorization principle we give a straightforward proof of global convergence. Further convergence properties follow immediately using known results of optimization theory","Convergence,
Clustering algorithms,
Fuzzy sets,
Equations,
Optimization methods,
Computer science,
Closed-form solution"
Monitoring k-nearest neighbor queries over moving objects,"Many location-based applications require constant monitoring of k-nearest neighbor (k-NN) queries over moving objects within a geographic area. Existing approaches to this problem have focused on predictive queries, and relied on the assumption that the trajectories of the objects are fully predictable at query processing time. We relax this assumption, and propose two efficient and scalable algorithms using grid indices. One is based on indexing objects, and the other on queries. For each approach, a cost model is developed, and a detailed analysis along with the respective applicability is presented. The object-indexing approach is further extended to multi-levels to handle skewed data. We show by experiments that our grid-based algorithms significantly outperform R-tree-based solutions. Extensive experiments are also carried out to study the properties and evaluate the performance of the proposed approaches under a variety of settings.","Indexing,
Computerized monitoring,
Costs,
Delay effects,
Computer science,
Application software,
Trajectory,
Query processing,
Personal digital assistants,
Mobile handsets"
"Visualization of vasculature with convolution surfaces: method, validation and evaluation","We present a method for visualizing vasculature based on clinical computed tomography or magnetic resonance data. The vessel skeleton as well as the diameter information per voxel serve as input. Our method adheres to these data, while producing smooth transitions at branchings and closed, rounded ends by means of convolution surfaces. We examine the filter design with respect to irritating bulges, unwanted blending and the correct visualization of the vessel diameter. The method has been applied to a large variety of anatomic trees. We discuss the validation of the method by means of a comparison to other visualization methods. Surface distance measures are carried out to perform a quantitative validation. Furthermore, we present the evaluation of the method which has been accomplished on the basis of a survey by 11 radiologists and surgeons.","Convolution,
Data visualization,
Computed tomography,
Skeleton,
Surface reconstruction,
Rendering (computer graphics),
Isosurfaces,
Biomedical imaging,
Medical treatment,
Magnetic noise"
WSDL-based automatic test case generation for Web services testing,"Web services promote the specification based cooperation and collaboration among distributed applications in an open environment. To ensure the quality of the services that are published, bound, invoked and integrated at runtime, test cases have to be automatically generated and testing executed, monitored and analyzed at runtime. This paper presents the research to generate Web services test cases automatically based on the Web services specification language WSDL (Web Services Description Language), which carries the basic information of a service including its interface operations and the data transmitted. The WSDL file is first parsed and transformed into the structured DOM tree. Then, test cases are generated from two perspectives: test data generation and test operation generation. Test data are generated by analyzing the message data types according to standard XML schema syntax. Operation flows are generated based on the operation dependency analysis. Three types of dependencies are defined: input dependency, output dependency, and input/output dependency. Finally, the generated test cases are documented in XML based test files called service test specification.","Automatic testing,
Computer aided software engineering,
Web services,
Runtime,
Simple object access protocol,
Computer science,
System testing,
Collaboration,
Application software,
Computerized monitoring"
Face membership authentication using SVM classification tree generated by membership-based LLE data partition,"This paper presents a new membership authentication method by face classification using a support vector machine (SVM) classification tree, in which the size of membership group and the members in the membership group can be changed dynamically. Unlike our previous SVM ensemble-based method, which performed only one face classification in the whole feature space, the proposed method employed a divide and conquer strategy that first performs a recursive data partition by membership-based locally linear embedding (LLE) data clustering, then does the SVM classification in each partitioned feature subset. Our experimental results show that the proposed SVM tree not only keeps the good properties that the SVM ensemble method has, such as a good authentication accuracy and the robustness to the change of members, but also has a considerable improvement on the stability under the change of membership group size.","Authentication,
Support vector machines,
Support vector machine classification,
Classification tree analysis,
Robust stability,
Humans,
Face recognition,
Permission,
Data security,
Computer science education"
SMART: a scan-based movement-assisted sensor deployment method in wireless sensor networks,"The efficiency of sensor networks depends on the coverage of the monitoring area. Although in general a sufficient number of sensors are used to ensure a certain degree of redundancy in coverage so that sensors can rotate between active and sleep modes, a good sensor deployment is still necessary to balance the workload of sensors. In a sensor network with locomotion facilities, sensors can move around to self-deploy. The movement-assisted sensor deployment deals with moving sensors from an initial unbalanced state to a balanced state. Therefore, various optimization problems can be defined to minimize different parameters, including total moving distance, total number of moves, communication/computation cost, and convergence rate. In this paper, we propose a Scan-based Movement-Assisted sensoR deploymenT method (SMART) that uses scan and dimension exchange to achieve a balanced state. SMART also addresses a unique problem called communication holes in sensor networks. Using the concept of load balancing, SMART achieves good performance especially when applied to uneven distribution sensor networks, and can be a complement to the existing sensor deployment methods. Extensive simulation has been done to verify the effectiveness of the proposed scheme.","Wireless sensor networks,
Intelligent sensors,
Intelligent networks,
Force sensors,
Cost function,
Convergence,
Load management,
Condition monitoring,
Parallel processing,
Computer science"
A unifying approach to hard and probabilistic clustering,"We derive the clustering problem from first principles showing that the goal of achieving a probabilistic, or ""hard"", multi class clustering result is equivalent to the algebraic problem of a completely positive factorization under a doubly stochastic constraint. We show that spectral clustering, normalized cuts, kernel K-means and the various normalizations of the associated affinity matrix are particular instances and approximations of this general principle. We propose an efficient algorithm for achieving a completely positive factorization and extend the basic clustering scheme to situations where partial label information is available.","Stochastic processes,
Kernel,
Chromium,
Computer science,
Clustering algorithms,
Labeling,
Particle measurements,
Euclidean distance,
Matrices,
Computer vision"
Combining particle swarm optimisation with angle modulation to solve binary problems,"The optimisation process of a particular problem generally has many influencing factors including the parameter choices, problem constraints as well as the complexity of the optimisation algorithm and optimisation problem among others. The dimensionality of a problem influences the computational complexity in converging to a valid solution. With problems defined in larger and more abstract dimensions, complexity becomes a problem as the solutions presented by the algorithm are more likely to be sub-optimal. An interesting and unique manner to reduce the complexity of binary problems is developed in this paper: angle modulation is applied to generate a bit string to solve binary problems, using particle swarm optimisation (PSO) to evolve the function coefficients of a trigonometric model. Instead of evolving a high dimensional bit vector, angle modulation reduces the problem to a four-dimensional problem defined in continuous space. Experimental results show that the angle modulation method is faster than the standard binary PSO, and that accuracy is improved for most benchmark functions used","Particle swarm optimization,
Computer science,
Africa,
Constraint optimization,
Computational complexity,
Optimization methods,
Guidelines,
Birds"
New constructions of quaternary low correlation zone sequences,"In this paper, given a composite integer n, we propose a method of constructing quaternary low correlation zone (LCZ) sequences of period 2/sup n/-1 from binary sequences of the same length with ideal autocorrelation. These new sequences are optimal with respect to the bound by Tang, Fan, and Matsufuji. The correlation distributions of these new quaternary LCZ sequences constructed from m-sequences and Gordon-Mills-Welch (GMW) sequences are derived.","Delay effects,
Multiaccess communication,
Binary sequences,
Autocorrelation,
Local area networks,
Wireless communication,
Computer science,
Galois fields"
A novel layered graph model for topology formation and routing in dynamic spectrum access networks,"This paper studies a fundamental problem in dynamic spectrum access (DSA) networks: given a set of detected spectrum bands that can be temporarily used by each node in a DSA network, how to form a topology by selecting spectrum bands for each radio interface of each node, called topology formation in this paper. We propose a novel layered graph to model the temporarily available spectrum bands, called spectrum opportunities (SOPs) in this paper, and use this layered graph model to develop effective and efficient routing and interface assignment algorithms to form near-optimal topologies for DSA networks. We have evaluated the performance of our layered graph approach and compared it to a sequential interface assignment algorithm. The numerical results show that the layered graph approach significantly outperforms the sequential interface assignment","Network topology,
Routing,
Intelligent networks,
Wireless networks,
FCC,
Cellular phones,
Interference,
Frequency,
Computer science,
Computer interfaces"
Fragile watermarking for authenticating 3-D polygonal meshes,"Designing a powerful fragile watermarking technique for authenticating three-dimensional (3-D) polygonal meshes is a very difficult task. Yeo and Yeung were first to propose a fragile watermarking method to perform authentication of 3-D polygonal meshes. Although their method can authenticate the integrity of 3-D polygonal meshes, it cannot be used for localization of changes. In addition, it is unable to distinguish malicious attacks from incidental data processings. In this paper, we trade off the causality problem in Yeo and Yeung's method for a new fragile watermarking scheme. The proposed scheme can not only achieve localization of malicious modifications in visual inspection, but also is immune to certain incidental data processings (such as quantization of vertex coordinates and vertex reordering). During the process of watermark embedding, a local mesh parameterization approach is employed to perturb the coordinates of invalid vertices while cautiously maintaining the visual appearance of the original model. Since the proposed embedding method is independent of the order of vertices, the hidden watermark is immune to some attacks, such as vertex reordering. In addition, the proposed method can be used to perform region-based tampering detection. The experimental results have shown that the proposed fragile watermarking scheme is indeed powerful.","Watermarking,
Protection,
Robustness,
Authentication,
Data processing,
Quantization,
Mesh generation,
Information science,
Algorithm design and analysis,
Graphics"
On logic-based intelligent systems,"First, a new matrix product, called the semitensor product of matrices, is introduced. Then the logic operators are expressed in matrix form. Based on this form, the fuzzy logic is deduced in an axiomatic form. Finally, the logic-based intelligent control is considered.","Intelligent systems,
Fuzzy logic,
Intelligent control,
Humans,
Machine intelligence,
Computer science,
Artificial intelligence,
Production,
Artificial neural networks,
Support vector machines"
Belief in information flow,"Information leakage traditionally has been defined to occur when uncertainty about secret data is reduced. This uncertainty-based approach is inadequate for measuring information flow when an attacker is making assumptions about secret inputs and these assumptions might be incorrect; such attacker beliefs are an unavoidable aspect of any satisfactory definition of leakage. To reason about information flow based on beliefs, a model is developed that describes how attacker beliefs change due to the attacker's observation of the execution of a probabilistic (or deterministic) program. The model leads to a new metric for quantitative information flow that measures accuracy rather than uncertainty of beliefs.","Fluid flow measurement,
Information security,
Computer science,
Mechanical factors,
Government,
Authentication"
Reduction of quality (RoQ) attacks on Internet end-systems,"Current computing systems depend on adaptation mechanisms to ensure that they remain in quiescent operating regions. These regions are often defined using efficiency, fairness, and stability properties. To that end, traditional research works in scalable server architectures and protocols have focused on promoting these properties by proposing even more sophisticated adaptation mechanisms, without the proper attention to security implications. In this paper, we exemplify such security implications by exposing the vulnerabilities of admission control mechanisms that are widely deployed in Internet end systems to reduction of quality (RoQ) attacks. RoQ attacks target the transients of a system's adaptive behavior as opposed to its limited steady-state capacity. We show that a well orchestrated RoQ attack on an end-system admission control policy could introduce significant inefficiencies that could potentially deprive an Internet end-system from much of its capacity, or significantly reduce its service quality, while evading detection by consuming an unsuspicious, small fraction of that system's hijacked capacity. We develop a control theoretic model for assessing the impact of RoQ attacks on an end-system's admission controller. We quantify the damage inflicted by an attacker through deriving appropriate metrics. We validate our findings through real Internet experiments performed in our lab.","Internet,
Admission control,
Web server,
Resource management,
Delay,
Computer science,
Stability,
Computer architecture,
Protocols,
Mechanical factors"
Clustering aggregation,"We consider the following problem: given a set of clusterings, find a clustering that agrees as much as possible with the given clusterings. This problem, clustering aggregation, appears naturally in various contexts. For example, clustering categorical data is an instance of the problem: each categorical variable can be viewed as a clustering of the input rows. Moreover, clustering aggregation can be used as a meta-clustering method to improve the robustness of clusterings. The problem formulation does not require a-priori information about the number of clusters, and it gives a natural way for handling missing values. We give a formal statement of the clustering-aggregation problem, we discuss related work, and we suggest a number of algorithms. For several of the methods we provide theoretical guarantees on the quality of the solutions. We also show how sampling can be used to scale the algorithms for large data sets. We give an extensive empirical evaluation demonstrating the usefulness of the problem and of the solutions.","Clustering algorithms,
Information technology,
Computer science,
Robustness,
Sampling methods,
Partitioning algorithms,
Data analysis"
Analysis of Zeno behaviors in a class of hybrid systems,"This note investigates conditions for existence of Zeno behaviors (where a system undergoes an unbounded number of discrete transitions in a finite length of time) in a class of hybrid systems. Zeno behavior occurs, for example, when a controller unsuccessfully attempts to satisfy an invariance specification by switching the system among different configurations faster and faster. Two types of Zeno systems are investigated: (1) strongly Zeno systems where all runs of the system are Zeno and (2) (weakly) Zeno systems where only some runs of the system are Zeno. For constant-rate and bounded-rate hybrid systems and some nonlinear generalizations, necessary and sufficient conditions for both Zenoness and strong Zenoness are derived. The analysis is based on studying the trajectory set of a certain ""equivalent"" continuous-time system that is associated with the dynamic equations of the hybrid system. The relation between the possibility of existence of Zeno behaviors in a system and the problem of existence of non-Zeno safety controllers (that keep the system in a specified region of its operating space) is also examined. It is shown that in certain Zeno systems, a minimally-interventive safety controller may not exist, even if a safety controller exists, disproving a conjecture made earlier in the literature.","Control systems,
Safety,
Control system synthesis,
NASA,
Optimal control,
Sufficient conditions,
Nonlinear dynamical systems,
Nonlinear equations,
Computer science,
Space technology"
A simulation approach to structure-based software reliability analysis,"Structure-based techniques enable an analysis of the influence of individual components on the application reliability. In an effort to ensure analytical tractability, prevalent structure-based analysis techniques are based on assumptions which preclude the use of these techniques for reliability analysis during the testing and operational phases. In this paper, we develop simulation procedures to assess the impact of individual components on the reliability of an application in the presence of fault detection and repair strategies that may be employed during testing. We also develop simulation procedures to analyze the application reliability for various operational configurations. We illustrate the potential of simulation procedures using several examples. Based on the results of these examples, we provide novel insights into how testing and repair strategies can be tailored depending on the application structure to achieve the desired reliability in a cost-effective manner. We also discuss how the results could be used to explore alternative operational configurations of a software application taking into consideration the application structure so as to cause minimal interruption in the field.","Analytical models,
Software reliability,
Application software,
Testing,
Runtime,
Flow graphs,
Computer science,
Fault detection,
Discrete event simulation,
Programming"
"Equilibria, prudent Compromises,and the ""Waiting"" game","While evaluation of many e-negotiation agents are carried out through empirical studies, this work supplements and complements existing literature by analyzing the problem of designing market-driven agents (MDAs) in terms of equilibrium points and stable strategies. MDAs are negotiation agents designed to make prudent compromises taking into account factors such as time preference, outside option, and rivalry. This work shows that 1) in a given market situation, an MDA negotiates optimally because it makes minimally sufficient concession, and 2) by modeling negotiation of MDAs as a game /spl Gamma/ of incomplete information, it is shown that the strategies adopted by MDAs are stable. In a bilateral negotiation, it is proven that the strategy pair of two MDAs forms a sequential equilibrium for /spl Gamma/. In a multilateral negotiation, it is shown that the strategy profile of MDAs forms a market equilibrium for /spl Gamma/.","Stochastic processes,
Information science,
Grid computing,
Design optimization,
Software agents,
Analytical models,
Proposals,
Computational modeling,
Constraint optimization,
Stability criteria"
Towards ultimate motion estimation: combining highest accuracy with real-time performance,"Although variational methods are among the most accurate techniques for estimating the optical flow, they have not yet entered the field of real-time vision. Main reason is the great popularity of standard numerical schemes that are easy to implement, however, at the expense of being too slow for real-time performance. In our paper we address this problem in two ways: (i) we present an improved version of the highly accurate technique of Brox et al. (2004). Thereby we show that a separate robustification of the constancy assumptions is very useful, in particular if the I-norm is used as penalizer. As a result, a method is obtained that yields the lowest angular errors in the literature, (ii) We develop an efficient numerical scheme for the proposed approach that allows real-time performance for sequences of size 160 /spl times/ 720. To this end, we combine two hierarchical strategies: a coarse-to-fine warping strategy as implementation of a fixed point iteration for a non-convex optimisation problem and a nonlinear full multigrid method - a so called full approximation scheme (FAS) - for solving the highly nonlinear equation systems at each warping level. In the experimental section the advantage of the proposed approach becomes obvious: Outperforming standard numerical schemes by two orders of magnitude frame rates of six high quality flow fields per second are obtained on a 3.06 GHz Pentium4 PC.",
Visual quality measures for Characterizing Planar robot grasps,"This paper presents and analyzes 12 quality measures that characterize robotic grips according to their stability and reliability. The measures are designed to assess three-finger grips of two-dimensional parts performed in a real environment, taking into account both theoretical aspects and unavoidable uncertainties of a grasping action. They build on the existing literature and on physical and mechanical considerations. The measures constitute a feature space that pattern recognition methods can use in order to classify robotic grips according to their quality. Six of the measures depend on the actual finger configuration of the gripper, and they have shown to be critical for better characterization. The kinematics of the Barrett Hand have been used. As a validation step, the measures are merged in two global quality values (with different practical applicability) that can be used to rank feasible candidate grips.","Intelligent robots,
Fingers,
Stability analysis,
Extraterrestrial measurements,
Pattern recognition,
Orbital robotics,
Kinematics,
Informatics,
Laboratories,
Computer science"
Robust L/sub 1/ norm factorization in the presence of outliers and missing data by alternative convex programming,"Matrix factorization has many applications in computer vision. Singular value decomposition (SVD) is the standard algorithm for factorization. When there are outliers and missing data, which often happen in real measurements, SVD is no longer applicable. For robustness iteratively re-weighted least squares (IRLS) is often used for factorization by assigning a weight to each element in the measurements. Because it uses L/sub 2/ norm, good initialization in IRLS is critical for success, but is nontrivial. In this paper, we formulate matrix factorization as a L/sub 1/ norm minimization problem that is solved efficiently by alternative convex programming. Our formulation 1) is robust without requiring initial weighting, 2) handles missing data straightforwardly, and 3) provides a framework in which constraints and prior knowledge (if available) can be conveniently incorporated. In the experiments we apply our approach to factorization-based structure from motion. It is shown that our approach achieves better results than other approaches (including IRLS) on both synthetic and real data.","Robustness,
Cost function,
Matrix decomposition,
Application software,
Computer vision,
Singular value decomposition,
Noise measurement,
Maximum likelihood estimation,
Least squares approximation,
Computer science"
Adaptive Temporal Radio Maps for Indoor Location Estimation,"In this paper, we present a novel method to adapt the temporal radio maps for indoor location estimation by offsetting the variational environmental factors using data mining techniques and reference points. Environmental variations, which cause the signals to change from time to time even at the same location, present a challenging task for indoor location estimation in the IEEE 802.11b infrastructure. In such a dynamic environment, the radio maps obtained in one time period may not be applicable in other time periods. To solve this problem, we apply a regression analysis to learn the temporal predictive relationship between the signal-strength values received by sparsely located reference points and that received by the mobile device. This temporal prediction model can then be used for online localization based on the newly observed signal-strength values at the client side and the reference points. We show that this technique can effectively accommodate the variations of signal-strength values over different time periods without the need to rebuild the radio maps repeatedly. We also show that the location of mobile device can be accurately determined using this technique with lower density in the distribution of the reference points",
Intravital leukocyte detection using the gradient inverse coefficient of variation,"The problem of identifying and counting rolling leukocytes within intravital microscopy is of both theoretical and practical interest. Currently, methods exist for tracking rolling leukocytes in vivo, but these methods rely on manual detection of the cells. In this paper we propose a technique for accurately detecting rolling leukocytes based on Bayesian classification. The classification depends on a feature score, the gradient inverse coefficient of variation (GICOV), which serves to discriminate rolling leukocytes from a cluttered environment. The leukocyte detection process consists of three sequential steps: the first step utilizes an ellipse matching algorithm to coarsely identify the leukocytes by finding the ellipses with a locally maximal GICOV. In the second step, starting from each of the ellipses found in the first step, a B-spline snake is evolved to refine the leukocytes boundaries by maximizing the associated GICOV score. The third and final step retains only the extracted contours that have a GICOV score above the analytically determined threshold. Experimental results using 327 rolling leukocytes were compared to those of human experts and currently used methods. The proposed GICOV method achieves 78.6% leukocyte detection accuracy with 13.1% false alarm rate.",
Decentralized Schemes for Size Estimation in Large and Dynamic Groups,"Large-scale and dynamically changing distributed systems such as the grid, peer-to-peer overlays, etc., need to collect several kinds of global statistics in a decentralized manner. In this paper, we tackle a specific statistic collection problem called group size estimation, for estimating the number of non-faulty processes present in the global group at any given point of time. We present two new decentralized algorithms for estimation in dynamic groups, analyze the algorithms, and experimentally evaluate them using real-life traces. One scheme is active: it spreads a gossip into the overlay first, and then samples the receipt times of this gossip at different processes. The second scheme is passive: it measures the density of processes when their identifiers are hashed into a real interval. Both schemes have low latency, scalable per-process overheads, and provide high levels of probabilistic accuracy for the estimate. They are implemented as part of a size estimation utility called PeerCounter that can be incorporated modularly into standard peer-to-peer overlays. We present experimental results from both the simulations and PeerCounter, running on a cluster of 33 Linux servers","Peer to peer computing,
Delay estimation,
Large-scale systems,
Statistical distributions,
Protocols,
Computer science,
Algorithm design and analysis,
Routing,
Multicast algorithms,
Global communication"
Boosting Data Center Performance Through Non-Uniform Power Allocation,"Data center power management is evolving from ad hoc methods based on maximum node power usage to systematic methods that employ power-scalable components. In addition, it is possible to exploit the power and throughput relationship to increase the total work performed and safely overprovision the rack space while staying below an aggregate power limit. This research describes a general framework for boosting throughput at a local level while load-balancing the available aggregate power under a set of operating constraints. Our solution is useful for those data centers that cannot expand the number of power circuits or seek effective usage of their available power budget due to unplanned power fluctuations. The framework is particularly well suited for environments with a heterogeneous workload and hence, a non-uniform power allocation requirement. Based on a representative workload for a two minute period, this paper shows a non-uniform power allocation scheme increases throughput by over 16% versus a uniform power allocation mechanism",
Just Right Outsourcing: Understanding and Managing Risk,"The risks associated with outsourcing have been the principal limitation on the growth of business process outsourcing, especially cross border outsourcing. Technological improvements in risk management have lead to the dramatic increase in outsourcing in India. Further progress now comes from redesigning work flows and dividing work among multiple vendors, increasing the range of tasks that are now appropriate candidates for outsourcing.",
"Routing worm: a fast, selective attack worm based on IP address information","Most well-known worms, such as Code Red, Slammer, Blaster, and Sasser, infected vulnerable computers by scanning the entire IPv4 address space. In this paper, we present an advanced worm called ""routing worm"", which implements two advanced attacking techniques. First, a routing worm uses BGP routing tables to only scan the Internet mutable address space, which allows it propagate three times faster than a traditional worm. Second, and more importantly, the geographic information of BGP routing prefixes enables a routing worm to conduct pinpoint ""selective attacks"" by imposing heavy damage to vulnerable computers in a specific country, company, Internet service provider, or autonomous system, without collateral damage done to others. Because of the inherent publicity of BGP routing tables, attackers can easily deploy routing worms, which distinguishes the routing worm from other ""worst-case"" worms. Compared to a traditional worm, a routing worm could possibly cause more severe congestion to the Internet backbone since all scans sent out by a routing worm are Internet mutable (and can only be dropped at the destinations). In addition, it is harder to quickly detect a routing-worm infected computer since we cannot distinguish illegal scans from regular connections without waiting for traffic responses. In order to defend against routing worms and all scanning worms, an effective way is to upgrade the current Internet from IPv4 to IPv6, although such an upgrade will require a tremendous effort and is still a controversial issue.","Routing,
Computer worms,
Web and internet services,
Computer science,
Spine,
Computer security,
Bandwidth,
Conferences"
Increasing register file immunity to transient errors,"Transient errors are a major reason for system downtime in many systems. In prior research, the register file has largely been neglected, but since it is accessed very frequently, the probability of transient errors is high. These errors can quickly spread to different parts of the system, and cause an application crash or silent data corruption. The paper addresses the reliability of register files in superscalar processors. We propose to duplicate actively used physical registers in unused physical registers. If the protection mechanism (parity or ECC) used for the primary copy indicates an error, the duplicate can provide the data, as long as it is not corrupted. We implement two strategies based on register duplication. In the ""conservative strategy"", we limit ourselves with the given register usage behavior, and duplicate register contents only on otherwise unused registers. Consequently, there is no impact on the original performance when there is no error, except for the protection mechanism used for the primary copy. Experiments with two different versions of this strategy show that, with the more powerful conservative scheme, 78% of the accesses are to the physical registers with duplicates. The ""aggressive strategy"" sacrifices some performance to increase the number of register accesses with duplicates. It does so by marking the registers not used for a long time as ""dead"" and using them for duplicating actively used registers. Experiments with this strategy indicate that it takes the fraction of reliable register accesses to 84%, and degrades the overall performance by only 0.21% on average.","Registers,
Protection,
Computer errors,
Computer crashes,
Error correction codes,
Packaging,
Error probability,
Error correction,
Computer science,
Degradation"
"Architecture and details of a high quality, large-scale analytical placer","Modern design requirements have brought additional complexities to netlists and layouts. Millions of components, whitespace resources, and fixed/movable blocks are just a few to mention in the list of complexities. With these complexities in mind, placers are faced with the burden of finding an arrangement of placeable objects under strict wirelength, timing, and power constraints. In this paper we describe the architecture and novel details of our high quality, large-scale analytical placer. The performance of our placer has been recently recognized in the recent ISPD-2005 placement contest, and in this paper we disclose many of the technical details that we believe are key factors to its performance. We describe (i) a new clustering architecture, (ii) a dynamically adaptive analytical solver, and (iii) better legalization schemes and novel detailed placement methods. We also provide extensive experimental results on a number of benchmark sets. On average, our results are better than the best published results by 3%, 14%, and 6% for the IBM ISPD '04, ICCAD '04, and ISPD '05 benchmark sets respectively. One of the goals of this paper is to also provide enough details to enable possible future replication of our methods.","Large-scale systems,
Timing,
Computer architecture,
Computer science,
Very large scale integration,
Circuits,
Runtime,
Engines,
Service oriented architecture"
Experiments with an improved iris segmentation algorithm,"Iris is claimed to be one of the best biometrics. We have collected a large data set of iris images, intentionally sampling a range of quality broader than that used by current commercial iris recognition systems. We have re-implemented the Daugman-like iris recognition algorithm developed by Masek. We have also developed and implemented an improved iris segmentation and eyelid detection stage of the algorithm, and experimentally verified the improvement in recognition performance using the collected dataset. Compared to Masek's original segmentation approach, our improved segmentation algorithm leads to an increase of over 6% in the rank-one recognition rate.","Image segmentation,
Iris recognition,
Biometrics,
Neodymium,
Eyelids,
Probes,
Encoding,
Computer science,
Image sampling,
Change detection algorithms"
Localization for anisotropic sensor networks,"In this paper, we consider the issue of localization in anisotropic sensor networks. Anisotropic networks are differentiated from isotropic networks in that they possess properties that vary according to the direction of measurement. Anisotropic characteristics result from various factors such as the geographic shape of the region (non-convex region), the different node densities, the irregular radio patterns, and the anisotropic terrain conditions. In order to characterize anisotropic features, we devise a linear mapping method that transforms proximity measurements between sensor nodes into a geographic distance embedding space by using the truncated singular value decomposition-based (TSVD-based) pseudo-inverse technique. This transformation retains as much topological information as possible and reduces the effect of measurement noises on the estimates of geographic distances. We show via simulation that the proposed localization method outperforms DV-hop, DV-distance (D. Niculescu, 2001), and MDS-map (Y. Shang et al., 2003), and makes robust and accurate estimates of sensor locations in both isotropic and anisotropic sensor networks.","Anisotropic magnetoresistance,
Wireless sensor networks,
Shape,
Sensor phenomena and characterization,
Communication system security,
Monitoring,
Sensor systems,
Global Positioning System,
Position measurement,
Computer science"
Feature hierarchies for object classification,"The paper describes a method for automatically extracting informative feature hierarchies for object classification, and shows the advantage of the features constructed hierarchically over previous methods. The extraction process proceeds in a top-down manner: informative top-level fragments are extracted first, and by a repeated application of the same feature extraction process the classification fragments are broken down successively into their own optimal components. The hierarchical decomposition terminates with atomic features that cannot be usefully decomposed into simpler features. The entire hierarchy, the different features and sub-features, and their optimal parameters, are learned during a training phase using training examples. Experimental comparisons show that these feature hierarchies are significantly more informative and better for classification compared with similar nonhierarchical features as well as previous methods for using feature hierarchies.","Feature extraction,
Training data,
Computer vision,
Object detection,
Computer science,
Mathematics,
Face detection,
Eyelids,
Mutual information,
Lighting"
3D information visualization for time dependent data on maps,"The visual analysis of time dependent data is an essential task in many application fields. However, visualizing large time dependent data collected within a spatial context is still a challenging task. In this paper, we therefore describe an approach for visualizing spatio-temporal data on maps. The approach is based on two commonly used concepts: 3D information visualization and information hiding. These concepts are realized by means of novel embeddings of 3D icons into a map display for representing spatio-temporal data, and an integration of event-based methods for reducing the amount of information to be represented. Our approach is capable of visualizing multiple time dependent attributes on maps, and of emphasizing the characteristics constituted by either linear or cyclic temporal dependencies.","Data visualization,
Data analysis,
Geographic Information Systems,
Animation,
Computer science,
Application software,
Three dimensional displays,
Visual databases,
Statistical analysis,
Event detection"
Adaptive local search parameters for real-coded memetic algorithms,"This paper presents a real-coded memetic algorithm that combines a high diversity global exploration with an adaptive local search method to the most promising individuals that adjusts the local search probability and the local search depth. In our proposal we use the individual fitness to decide when local search will be applied (local search probability) and how many effort should be applied (the local search depth), focusing the local search effort on the most promising regions. We divide the individuals of the population into three different categories and we assign different values of the above local search parameters to the individual in function of the category to which that individual belongs. In this study, we analyze the performance of our proposal when tackling the test problems proposed for the Special Session of the IEEE Congress on Evolutionary Computation 2005",
Edge-based image restoration,"In this paper, we propose a new image inpainting algorithm that relies on explicit edge information. The edge information is used both for the reconstruction of a skeleton image structure in the missing areas, as well as for guiding the interpolation that follows. The structure reconstruction part exploits different properties of the edges, such as the colors of the objects they separate, an estimate of how well one edge continues into another one, and the spatial order of the edges with respect to each other. In order to preserve both sharp and smooth edges, the areas delimited by the recovered structure are interpolated independently, and the process is guided by the direction of the nearby edges. The novelty of our approach lies primarily in exploiting explicitly the constraint enforced by the numerical interpretation of the sequential order of edges, as well as in the pixel filling method which takes into account the proximity and direction of edges. Extensive experiments are carried out in order to validate and compare the algorithm both quantitatively and qualitatively. They show the advantages of our algorithm and its readily application to real world cases.","Image restoration,
Image reconstruction,
Mathematics,
Computer science,
Data mining,
Skeleton,
Interpolation,
Filling,
Cultural differences,
Biomedical imaging"
Reducing the Size of Fuzzy Concept Lattices by Hedges,"We study concept lattices with hedges. The principal aim is to control, in a parametrical way, the size of a concept lattice. The paper presents theoretical insight, comments, and examples. We show that a concept lattice with hedges is indeed a complete lattice which is isomorphic to an ordinary concept lattice. We describe the isomorphism and its inverse. These mappings serve as translation procedures. As a consequence, we obtain a theorem characterizing the structure of concept lattices with hedges which generalizes the so-called main theorem of concept lattices. Furthermore, the isomorphism and its inverse enable us to compute a concept lattice with hedges using algorithms for ordinary concept lattices. Further insight is provided in case one uses hedges only for attributes. We demonstrate by experiments that the size reduction using hedges as a parameter is smooth","Lattices,
Data mining,
Data analysis,
Computer science,
Size control,
Algebra"
Predicting unroll factors using supervised classification,"Compilers base many critical decisions on abstracted architectural models. While recent research has shown that modeling is effective for some compiler problems, building accurate models requires a great deal of human time and effort. This paper describes how machine learning techniques can be leveraged to help compiler writers model complex systems. Because learning techniques can effectively make sense of high dimensional spaces, they can be a valuable tool for clarifying and discerning complex decision boundaries. In this work we focus on loop unrolling, a well-known optimization for exposing instruction level parallelism. Using the Open Research Compiler as a testbed, we demonstrate how one can use supervised learning techniques to determine the appropriateness of loop unrolling. We use more than 2,500 loops - drawn from 72 benchmarks - to train two different learning algorithms to predict unroll factors (i.e., the amount by which to unroll a loop) for any novel loop. The technique correctly predicts the unroll factor for 65% of the loops in our dataset, which leads to a 5% overall improvement for the SPEC 2000 benchmark suite (9% for the SPEC 2000 floating point benchmarks).","Pipeline processing,
Machine learning,
Humans,
Optimizing compilers,
Support vector machines,
Support vector machine classification,
History,
Computer science,
Artificial intelligence,
Laboratories"
"InsightVideo: toward hierarchical video content organization for efficient browsing, summarization and retrieval","Hierarchical video browsing and feature-based video retrieval are two standard methods for accessing video content. Very little research, however, has addressed the benefits of integrating these two methods for more effective and efficient video content access. In this paper, we introduce InsightVideo, a video analysis and retrieval system, which joins video content hierarchy, hierarchical browsing and retrieval for efficient video access. We propose several video processing techniques to organize the content hierarchy of the video. We first apply a camera motion classification and key-frame extraction strategy that operates in the compressed domain to extract video features. Then, shot grouping, scene detection and pairwise scene clustering strategies are applied to construct the video content hierarchy. We introduce a video similarity evaluation scheme at different levels (key-frame, shot, group, scene, and video.) By integrating the video content hierarchy and the video similarity evaluation scheme, hierarchical video browsing and retrieval are seamlessly integrated for efficient content access. We construct a progressive video retrieval scheme to refine user queries through the interactions of browsing and retrieval. Experimental results and comparisons of camera motion classification, key-frame extraction, scene detection, and video retrieval are presented to validate the effectiveness and efficiency of the proposed algorithms and the performance of the system.","Content based retrieval,
Video compression,
Layout,
Computer science,
Cameras,
Standards organizations,
Feature extraction,
Gunshot detection systems,
Motion detection,
Computer networks"
Stereopsis-guided brain shift compensation,"Brain deformation models have proven to be a powerful tool in compensating for soft tissue deformation during image-guided neurosurgery. The accuracy of these models can be improved by incorporating intraoperative measurements of brain motion. We have designed and implemented a passive intraoperative stereo vision system capable of estimating the three-dimensional shape of the surgical scene in near real-time. This intraoperative shape is compared with the cortical surface in the co-registered preoperative magnetic resonance (MR) volume for the estimation of the cortical motion resulting from the open cranial surgery. The estimated cortical motion is then used to guide a full brain model, which updates a preoperative MR volume. We have found that the stereo vision system is accurate to within approximately 1 mm. Based on data from two representative clinical cases, we show that stereopsis guidance improves the accuracy of brain shift compensation both at and below the cortical surface.","Brain modeling,
Power system modeling,
Stereo vision,
Shape,
Surgery,
Motion estimation,
Deformable models,
Biological tissues,
Neurosurgery,
Motion measurement"
A three-dimensional registration method for automated fusion of micro PET-CT-SPECT whole-body images,"Micro positron emission tomography (PET) and micro single-photon emission computed tomography (SPECT), used for imaging small animals, have become essential tools in developing new pharmaceuticals and can be used, among other things, to test new therapeutic approaches in animal models of human disease, as well as to image gene expression. These imaging techniques can be used noninvasively in both detection and quantification. However, functional images provide little information on the structure of tissues and organs, which makes the localization of lesions difficult. Image fusion techniques can be exploited to map the functional images to structural images, such as X-ray computed tomography (CT), to support target identification and to facilitate the interpretation of PET or SPECT studies. Furthermore, the mapping of two functional images of SPECT and PET on a structural CT image can be beneficial for those in vivo studies that require two biological processes to be monitored simultaneously. This paper proposes an automated method for registering PET, CT, and SPECT images for small animals. A calibration phantom and a holder were used to determine the relationship among three-dimensional fields of view of various modalities. The holder was arranged in fixed positions on the couches of the scanners, and the spatial transformation matrix between the modalities was held unchanged. As long as objects were scanned together with the holder, the predetermined matrix could register the acquired tomograms from different modalities, independently of the imaged objects. In this work, the PET scan was performed by Concorde's microPET R4 scanner, and the SPECT and CT data were obtained using the Gamma Medica's X-SPECT/CT system. Fusion studies on phantoms and animals have been successfully performed using this method. For microPET-CT fusion, the maximum registration errors were 0.21 mm /spl plusmn/ 0.14 mm, 0.26 mm /spl plusmn/ 0.14 mm, and 0.45 mm /spl plusmn/ 0.34 mm in the X (right-left), Y (upper lower), and Z (rostral-caudal) directions, respectively; for the microPET-SPECT fusion, they were 0.24 mm /spl plusmn/ 0.14 mm, 0.28 mm /spl plusmn/ 0.15 mm, and 0.54 mm /spl plusmn/ 0.35 mm in the X, Y, and Z directions, respectively. The results indicate that this simple method can be used in routine fusion studies.","Computed tomography,
Positron emission tomography,
Animal structures,
X-ray imaging,
Imaging phantoms,
Whole-body PET,
Pharmaceuticals,
Testing,
Humans,
Diseases"
Security enhancement for a dynamic ID-based remote user authentication scheme,"In a paper recently published in the IEEE transaction on consumer electronics, Das, Saxena, and Gulati proposed a dynamic ID-based remote user authentication scheme using smart cards that allows the users to choose and change their passwords freely, and does not maintain any verifier table. It can protect against ID-theft, replaying, forgery, guessing, insider, and stolen verifier attacks. However, this paper shows that Das, Saxena, and Gulati's scheme has some attacks. Therefore, we propose a slight modification to their scheme to improve their weaknesses. As a result, the improved scheme can enhance the security of Das, Saxena, and Gulati's scheme. In addition, the proposed scheme does not add many computational costs additionally. Compare with their scheme, our scheme is also efficient.","Authentication,
Smart cards,
Computer science,
Protection,
Forgery,
Computational efficiency,
Law,
Legal factors,
National security,
Asia"
Improved K-means clustering algorithm for exploring local protein sequence motifs representing common structural property,"Information about local protein sequence motifs is very important to the analysis of biologically significant conserved regions of protein sequences. These conserved regions can potentially determine the diverse conformation and activities of proteins. In this work, recurring sequence motifs of proteins are explored with an improved K-means clustering algorithm on a new dataset. The structural similarity of these recurring sequence clusters to produce sequence motifs is studied in order to evaluate the relationship between sequence motifs and their structures. To the best of our knowledge, the dataset used by our research is the most updated dataset among similar studies for sequence motifs. A new greedy initialization method for the K-means algorithm is proposed to improve traditional K-means clustering techniques. The new initialization method tries to choose suitable initial points, which are well separated and have the potential to form high-quality clusters. Our experiments indicate that the improved K-means algorithm satisfactorily increases the percentage of sequence segments belonging to clusters with high structural similarity. Careful comparison of sequence motifs obtained by the improved and traditional algorithms also suggests that the improved K-means clustering algorithm may discover some relatively weak and subtle sequence motifs, which are undetectable by the traditional K-means algorithms. Many biochemical tests reported in the literature show that these sequence motifs are biologically meaningful. Experimental results also indicate that the improved K-means algorithm generates more detailed sequence motifs representing common structures than previous research. Furthermore, these motifs are universally conserved sequence patterns across protein families, overcoming some weak points of other popular sequence motifs. The satisfactory result of the experiment suggests that this new K-means algorithm may be applied to other areas of bioinformatics research in order to explore the underlying relationships between data samples more effectively.","Clustering algorithms,
Protein sequence,
Sequences,
Computer science,
Testing,
Cancer,
Bioinformatics,
Diseases,
Information science,
Biology"
Practical security for disconnected nodes,"Endpoints in a delay tolerant network (DTN) [K. Fall, 2003] must deal with long periods of disconnection, large end-to-end communication delays, and opportunistic communication over intermittent links. This makes traditional security mechanisms inefficient and sometimes unsuitable. We study three specific problems that arise naturally in this context: initiation of a secure channel by a disconnected user using an opportunistic connection, mutual authentication over an opportunistic link, and protection of disconnected users from attacks initiated by compromised identities. We propose a security architecture for DTN based on hierarchical identity based cryptography (HIBC) that provides efficient and practical solutions to these problems.","Disruption tolerant networking,
Authentication,
Personal digital assistants,
Identity-based encryption,
Public key,
Protection,
Electronic mail,
Cryptography,
Delay,
Computer science"
Automatic measurement of a QoS metric for Web service recommendation,"Web services have enabled businesses and organizations to collaborate without platform interoperability and programming language barriers. Quality of service (QoS) of a Web service is an important factor that differentiates similar services offered by different service providers. Such a measure would allow Web service clients to choose and bind to a suitable Web service at run time (based on QoS attributes). Some researchers have proposed the integration of the QoS measure on the Web service directory server. However, a mechanism to maintain the QoS metric has not been defined yet. In this paper, we propose such a mechanism. This mechanism involves automated measurement of QoS attributes on both the client and provider sides, when the service is being used, and updating the QoS-aware Web services directory with this information. We describe a prototype we developed for this purpose and present the results of using this prototype for gathering QoS measurements at run time.","Web services,
Quality of service,
Time measurement,
Prototypes,
Software engineering,
Web and internet services,
Computer science,
Collaborative software,
Computer languages,
Standards development"
JRipples: a tool for program comprehension during incremental change,"Incremental software change adds new functionality to software. It is the foundation of software evolution, maintenance, iterative development, agile development, and other software processes. Highly interactive tool JRipples provides the programmer with the organizational support that makes the incremental change process easier and more systematic. JRipples supports impact analysis and change propagation, the two most difficult activities of the incremental change.","Programming profession,
Software maintenance,
Databases,
Software tools,
Java,
Displays,
Computer science,
Software engineering,
Humans,
Error analysis"
Networked infomechanical systems: a mobile embedded networked sensor platform,"Networked infomechanical systems (NIMS) introduces a new actuation capability for embedded networked sensing. By exploiting a constrained actuation method based on rapidly deployable infrastructure, NIMS suspends a network of wireless mobile and fixed sensor nodes in three-dimensional space. This permits run-time adaptation with variable sensing location, perspective, and even sensor type. Discoveries in NIMS environmental investigations have raised requirements for 1) new embedded platforms integrating many diverse sensors with actuators, and 2) advances for in-network sensor data processing. This is addressed with a new and generally applicable processor-preprocessor architecture described in this paper. Also this paper describes the successful integration of R, a powerful statistical computing environment, into the embedded NIMS node platform.","Sensor systems,
Sensor phenomena and characterization,
Sampling methods,
Monitoring,
Computer science,
Wireless sensor networks,
Runtime,
Actuators,
Mobile computing,
Statistics"
Robust active appearance models and their application to medical image analysis,"Active appearance models (AAMs) have been successfully used for a variety of segmentation tasks in medical image analysis. However, gross disturbances of objects can occur in routine clinical setting caused by pathological changes or medical interventions. This poses a problem for AAM-based segmentation, since the method is inherently not robust. In this paper, a novel robust AAM (RAAM) matching algorithm is presented. Compared to previous approaches, no assumptions are made regarding the kind of gray-value disturbance and/or the expected magnitude of residuals during matching. The method consists of two main stages. First, initial residuals are analyzed by means of a mean-shift-based mode detection step. Second, an objective function is utilized for the selection of a mode combination not representing the gross outliers. We demonstrate the robustness of the method in a variety of examples with different noise conditions. The RAAM performance is quantitatively demonstrated in two substantially different applications, diaphragm segmentation and rheumatoid arthritis assessment. In all cases, the robust method shows an excellent behavior, with the new method tolerating up to 50% object area covered by gross gray-level disturbances.","Biomedical imaging,
Image analysis,
Active appearance model,
Image segmentation,
Magnetic resonance imaging,
Image texture analysis,
Noise robustness,
Shape,
Computer graphics,
Image motion analysis"
Performance analysis of MPI collective operations,"Previous studies of application usage show that the performance of collective communications are critical for high-performance computing and are often overlooked when compared to the point-to-point performance. In this paper, we analyze and attempt to improve intra-cluster collective communication in the context of the widely deployed MPI programming paradigm by extending accepted models of point-to-point communication, such as Hockney, LogP/LogGP, and PLogP. The predictions from the models were compared to the experimentally gathered data and our findings were used to optimize the implementation of collective operations in the FT-MPI library.","Performance analysis,
High performance computing,
Predictive models,
Libraries,
Topology,
System testing,
Lifting equipment,
Laboratories,
Computer science,
Application software"
Combining Local and Global Image Features for Object Class Recognition,"Object recognition is a central problem in computer vision research. Most object recognition systems have taken one of two approaches, using either global or local features exclusively. This may be in part due to the difficulty of combining a single global feature vector with a set of local features in a suitable manner. In this paper, we show that combining local and global features is beneficial in an application where rough segmentations of objects are available. We present a method for classification with local features using non-parametric density estimation. Subsequently, we present two methods for combining local and global features. The first uses a ""stacking"" ensemble technique, and the second uses a hierarchical classification system. Results show the superior performance of these combined methods over the component classifiers, with a reduction of over 20% in the error rate on a challenging marine science application.","Image recognition,
Object recognition,
Computer vision,
Robustness,
Face detection,
Laboratories,
Computer science,
Sea measurements,
Aquaculture,
Error analysis"
Capacity and codes for embedding information in gray-scale signals,"Gray-scale signals can be represented as sequences of integer-valued symbols. If such a symbol has alphabet {0,1,...,2/sup B/-1} it can be represented by B binary digits. To embed information in these sequences, we are allowed to distort the symbols. The distortion measure that we consider here is squared error, however, errors larger than m are not allowed. The embedded message must be recoverable with error probability zero. In this setup, there is a so-called ""rate-distortion function"" that tells us what the largest embedding rate is, given a certain distortion level and parameter m. First, we determine this rate-distortion function for m=1 and for m/spl rarr//spl infin/. Next we compare the performance of ""low-bits modulation"" to the rate-distortion function for m/spl rarr//spl infin/. Then embedding codes are proposed based on i) ternary Hamming codes and on the ii) ternary Golay code. We show that all these codes are optimal in the sense that they achieve the smallest possible distortion at a given rate for fixed block length for any m.","Gray-scale,
Laboratories,
Codes,
Distortion measurement,
Error probability,
Communication system control,
Computer science,
Artificial intelligence,
Probability distribution,
Decoding"
Secure software development by example,"When trying to incorporate security into a program, software developers face either too much theoretical information that they cannot apply or exhaustive and discouraging recommendation lists. This article gives an overview of security concerns at each step of a project's life cycle.","Programming,
Information security,
Application software,
Computer security,
Privacy,
Data security,
Databases,
Computer science,
Buffer overflow,
Iterative methods"
Linear diophantine equations over polynomials and soft decoding of Reed-Solomon codes,"This paper generalizes the classical Knuth-Schoumlnhage algorithm computing the greatest common divisor (gcd) of two polynomials for solving arbitrary linear Diophantine systems over polynomials in time, quasi-linear in the maximal degree. As an application, the following weighted curve fitting problem is considered: given a set of points in the plane, find an algebraic curve (satisfying certain degree conditions) that goes through each point the prescribed number of times. The main motivation for this problem comes from coding theory, namely, it is ultimately related to the list decoding of Reed-Solomon codes. This paper presents a new fast algorithm for the weighted curve fitting problem, based on the explicit construction of a Groebner basis. This gives another fast algorithm for the soft decoding of Reed-Solomon codes different from the procedure proposed by Feng, which works in time (w/r) O(1)nlog2n, where r is the rate of the code, and w is the maximal weight assigned to a vertical line","Equations,
Decoding,
Polynomials,
Curve fitting,
Codes,
Computer science,
Interpolation,
Linear systems,
Arithmetic,
Ground support"
Where do goals come from: the underlying principles of goal-oriented requirements engineering,"Goal is a widely used concept in requirements engineering methods. Several kinds of goals, such as achievement, maintenance and soft goals, have been defined in these methods. These methods also define heuristics for the identification of organizational goals that drive the requirements process. In this paper, we propose a set of principles that explain the nature of goal-oriented behavior. These principles are based on regulation mechanisms as defined in general systems thinking and cybernetics. We use these principles to analyze the existing definitions of these different kinds of goals and to propose more precise definitions. We establish the commonalities and differences between these kinds of goals, and propose extension for goal identification heuristics.","Computer science,
Cybernetics,
Gas insulated transmission lines,
Systems engineering and theory,
Organizing,
Artificial intelligence,
Problem-solving,
Stability,
Knowledge acquisition"
Trajectory clustering and its applications for video surveillance,"In this paper we present a trajectory clustering method suited for video surveillance and monitoring systems. The clusters are dynamic and built in real-time as the trajectory data is acquired, without the need of an off-line processing step. We show how the obtained clusters can be successfully used both to give proper feedback to the low-level tracking system and to collect valuable information for the high-level event analysis modules.","Video surveillance,
Hidden Markov models,
Clustering algorithms,
Clustering methods,
Layout,
Vector quantization,
Application software,
Mathematics,
Computer science,
Computerized monitoring"
An augmented reality system for patient-specific guidance of cardiac catheter ablation procedures,"We present a system to assist in the treatment of cardiac arrhythmias by catheter ablation. A patient-specific three-dimensional (3-D) anatomical model, constructed from magnetic resonance images, is merged with fluoroscopic images in an augmented reality environment that enables the transfer of electrocardiography (ECG) measurements and cardiac activation times onto the model. Accurate mapping is realized through the combination of: a new calibration technique, adapted to catheter guided treatments; a visual matching registration technique, allowing the electrophysiologist to align the model with contrast-enhanced images; and the use of virtual catheters, which enable the annotation of multiple ECG measurements on the model. These annotations can be visualized by color coding on the patient model. We provide an accuracy analysis of each of these components independently. Based on simulation and experiments, we determined a segmentation error of 0.6 mm, a calibration error in the order of 1 mm and a target registration error of 1.04 /spl plusmn/ 0.45 mm. The system provides a 3-D visualization of the cardiac activation pattern which may facilitate and improve diagnosis and treatment of the arrhythmia. Because of its low cost and similar advantages we believe our approach can compete with existing commercial solutions, which rely on dedicated hardware and costly catheters. We provide qualitative results of the first clinical use of the system in 11 ablation procedures.","Augmented reality,
Catheters,
Electrocardiography,
Calibration,
Visualization,
Magnetic resonance,
Electrophysiology,
Independent component analysis,
Image segmentation,
Costs"
A service-centric model for wireless sensor networks,"Most of the current research in wireless sensor networks (WSNs) is constraint driven and focuses on optimizing the use of limited resources (e.g., power) at each sensor. While such constraints are important, there is a energy for more general performance metrics to assess the effectiveness of WSNs. There is also a need for a unified formal model that would enable comparison of different types of WSNs and provide a framework for WSN operations. We propose a new service-centric model that focuses on services provided by a WSN and views a WSN as a service provider. A WSN is modeled at different levels of abstraction. For each level, a set of services and a set of metrics are defined. Services and their interfaces are defined in a formal way to facilitate automatic composition of services, and enable interoperability and multitasking of WSNs at the different levels. A two-way mapping between two neighboring levels is then defined as a decomposition (from higher to lower level) and composition (from lower to higher level). A composite mapping between metrics at different levels connects high-level, mission-oriented metrics and low-level, capability-oriented metrics. The service-centric model consists of mission, network, region, sensor, and capability layers. Each layer has associated semantics that use lower level components as syntactic units (except for the capability layer). Within each layer there are four planes or functionality sets; communication, management, application, and generational learning. The combination of layers and planes enables a service-based visualization paradigm that can provide better understanding of the WSN. The service-centric model provides a holistic approach to measuring and presenting WSNs effectiveness. In addition, it presents a general and flexible framework in which various more specific WSN models can be represented and evaluated.","Wireless sensor networks,
Constraint optimization,
Computer science,
Communication system traffic control,
Traffic control,
Measurement,
Multitasking,
Data visualization,
Predictive models,
Communication system control"
Globally optimal solutions for energy minimization in stereo vision using reweighted belief propagation,"A wide range of low level vision problems have been formulated in terms of finding the most probable assignment of a Markov random field (or equivalently the lowest energy configuration). Perhaps the most successful example is stereo vision. For the stereo problem, it has been shown that finding the global optimum is NP hard but good results have been obtained using a number of approximate optimization algorithms. In this paper, we show that for standard benchmark stereo pairs, the global optimum can be found in about 30 minutes using a variant of the belief propagation (BP) algorithm. We extend previous theoretical results on reweighted belief propagation to account for possible ties in the beliefs and using these results we obtain easily checkable conditions that guarantee that the BP disparities are the global optima. We verify experimentally that these conditions are typically met for the standard benchmark stereo pairs and discuss the implications of our results for further progress in stereo.","Stereo vision,
Belief propagation,
Computer science,
Power engineering and energy,
Markov random fields,
Filters,
Energy resolution,
Energy measurement,
Cameras,
Polynomials"
Internet Multicast Video Delivery,"Internet video delivery has been motivating research in multicast routing, quality of service (QoS), and the service model of the Internet itself for the last 15 years. Multicast delivery has the potential to deliver a large amount of content that currently cannot be delivered through broadcast. IP and overlay multicast are two architectures proposed to provide multicast support. A large body of research has been done with IP multicast and QoS mechanisms for IP multicast since the late 1980s. In the past five years, overlay multicast research has gained momentum with a vision to accomplish ubiquitous multicast delivery that is efficient and scales in the dimensions of the number of groups, number of receivers, and number of senders. This work presents an overview of the issues facing both IP and overlay multicast and the approaches that researchers are taking to solve them. Many of these approaches take advantage of a rich interface, beyond a single rate video stream, between the coding and delivery mechanisms. The semantics of this interface is an important question for future research and we discuss this with insight from experience on delivery technologies.","Streaming media,
Quality of service,
Web and internet services,
Video on demand,
Computer science,
Routing,
Multimedia communication,
Broadcasting,
Multicast protocols,
Peer to peer computing"
Flexible web-based educational system for teaching computer architecture and organization,"An important problem in teaching courses in computer architecture and organization is to find a way to help students to make a cognitive leap from the blackboard description of a computer system to its utilization as a programmable device. Computer simulators developed to tackle this problem vary in scope, target architecture, user interface, and support for distance learning. Usually, they include the processor only, lacking the whole-system perspective. The existing simulators mainly focus on the programmer's view of the machine and do not provide the designer's perspective. This paper presents an educational computer system and its Web-based simulator, designed to help teaching and learning computer architecture and organization courses. The educational computer system is designed to cover a broad spectrum of topics taught in lower division courses. It offers a unique environment that exposes students to both the programmer and the designer's perspective of the computer system. The Web-based simulator features an interactive animation of program execution and allows students to navigate through different levels of the educational computer system's hierarchy-starting from the top level with block representation down to the implementation level with standard sequential and combinational logic blocks.","Computer science education,
Internet,
Computer aided instruction,
Simulation"
Information-theoretic approaches for sensor selection and placement in sensor networks for target localization and tracking,"In this paper, we describes the information-theoretic approaches to sensor selection and sensor placement in sensor networks for target localization and tracking. We have developed a sensor selection heuristic to activate the most informative candidate sensor for collaborative target localization and tracking. The fusion of the observation by the selected sensor with the prior target location distribution yields nearly the greatest reduction of the entropy of the expected posterior target location distribution. Our sensor selection heuristic is computationally less complex and thus more suitable to sensor networks with moderate computing power than the mutual information sensor selection criteria. We have also developed a method to compute the posterior target location distribution with the minimum entropy that could be achieved by the fusion of observations of the sensor network with a given deployment geometry. We have found that the covariance matrix of the posterior target location distribution with the minimum entropy is consistent with the Cramer-Rao lower bound (CRB) of the target location estimate. Using the minimum entropy of the posterior target location distribution, we have characterized the effect of the sensor placement geometry on the localization accuracy.","Entropy,
Mutual information,
Target tracking,
Random variables,
Bayesian methods,
Estimation error,
Measurement uncertainty"
Service Level Agreement based Allocation of Cluster Resources: Handling Penalty to Enhance Utility,"Jobs submitted into a cluster have varying requirements depending on user-specific needs and expectations. Therefore, in utility-driven cluster computing, cluster resource management systems (RMSs) need to be aware of these requirements in order to allocate resources effectively. Service level agreements (SLAs) can be used to differentiate different value of jobs as they define service conditions that the cluster RMS agrees to provide for each different job. The SLA acts as a contract between a user and the cluster whereby the user is entitled to compensation whenever the cluster RMS fails to deliver the required service. In this paper, we present a proportional share allocation technique called LibraSLA that takes into account the utility of accepting new jobs into the cluster based on their SLA. We study how LibraSLA performs with respect to several SLA requirements that include: (i) deadline type whether the job can be delayed, (ii) deadline when the job needs to be finished, (iii) budget to be spent for finishing the job, and (iv) penalty rate for compensating the user for failure to meet the deadline","Resource management,
Grid computing,
Quality of service,
Delay,
Finishing,
Admission control,
Laboratories,
Computer science,
Software engineering,
Contracts"
Automated verification of selected equivalences for security protocols,"In the analysis of security protocols, methods and tools for reasoning about protocol behaviors have been quite effective. We aim to expand the scope of those methods and tools. We focus on proving equivalences P/spl ap/Q in which P and Q are two processes that differ only in the choice of some terms. These equivalences arise often in applications. We show how to treat them as predicates on the behaviors of a process that represents P and Q at the same time. We develop our techniques in the context of the applied pi calculus and implement them in the tool ProVerif.","Security,
Calculus,
Broadcasting,
Cryptographic protocols,
Cryptography,
Writing"
Device-centric spectrum management,"Efficient spectrum allocation in open spectrum systems is a challenging problem, particularly for devices with constrained communication resources such as sensor and mobile ad hoc networks. We propose a device-centric spectrum management scheme with low communication costs, where users observe local interference patterns and act independently according to preset spectrum rules. We propose five rules that tradeoff performance with implementation complexity and communication costs, and derive a lower bound on each user's allocation based on these rules. Experimental results show that our proposed rule-based approach reduces communication costs from efficient collaborative approaches by a factor of 3-4 while providing good performance","Radio spectrum management,
Mobile communication,
Costs,
Collaboration,
Collaborative work,
Computer science,
Interference constraints,
Mobile ad hoc networks,
Licenses,
Communication system traffic control"
Bounding worst-case data cache behavior by analytically deriving cache reference patterns,"While caches have become invaluable for higher-end architectures due to their ability to hide, in part, the gap between processor speed and memory access times, caches (and particularly data caches) limit the timing predictability for data accesses that may reside in memory or in cache. This is a significant problem for real-time systems. The objective our work is to provide accurate predictions of data cache behavior of scalar and nonscalar references whose reference patterns are known at compile time. Such knowledge about cache behavior provides the basis for significant improvements in bounding the worst-case execution time (WCET) of real-time programs, particularly for hard-to-analyze data caches. We exploit the power of the cache miss equations (CME) framework but lift a number of limitations of traditional CME to generalize the analysis to more arbitrary programs. We further devised a transformation, coined ""forced"" loop fusion, which facilitates the analysis across sequential loops. Our contributions result in exact data cache reference patterns minus; in contrast to approximate cache miss behavior of prior work. Experimental results indicate improvements on the accuracy of worst-case data cache behavior up to two orders of magnitude over the original approach. In fact, our results closely bound and sometimes even exactly match those obtained by trace-driven simulation for worst-case inputs. The resulting WCET bounds of timing analysis confirm these findings in terms of providing tight bounds. Overall, our contributions lift analytical approaches to predict data cache behavior to a level suitable for efficient static timing analysis and, subsequently, real-time schedulability of tasks with predictable WCET.","Pattern analysis,
Equations,
Timing,
Real time systems,
Delay,
Computer science,
Embedded system,
Computer architecture,
Random access memory,
Processor scheduling"
A knowledge creation info-structure to acquire and crystallize the tacit knowledge of health-care experts,"Tacit knowledge of health-care experts is an important source of experiential know-how, yet due to various operational and technical reasons, such health-care knowledge is not entirely harnessed and put into professional practice. Emerging knowledge-management (KM) solutions suggest strategies to acquire the seemingly intractable and nonarticulated tacit knowledge of health-care experts. This paper presents a KM methodology, together with its computational implementation, to 1) acquire the tacit knowledge possessed by health-care experts; 2) represent the acquired tacit health-care knowledge in a computational formalism-i.e., clinical scenarios-that allows the reuse of stored knowledge to acquire tacit knowledge; and 3) crystallize the acquired tacit knowledge so that it is validated for health-care decision-support and medical education systems.","Crystallization,
Medical services,
Decision making,
Guidelines,
Problem-solving,
Knowledge management,
Collaborative work,
Computer science,
Pediatrics,
Electronic mail"
Network resilience through multi-topology routing,,"Resilience,
Routing,
Network topology,
Costs,
Multiprotocol label switching,
Robustness,
IP networks,
Computer science,
Electronic mail,
Scalability"
Algebraic constructions of constant composition codes,"Constant-composition codes are a special class of constant-weight codes. They include permutation codes as a subclass. In this correspondence, two classes of optimal constant-composition codes are constructed. Both constructions employ perfect nonlinear functions, but they are different.","Hamming weight,
History,
Councils,
Computer science,
Mathematics,
Codes"
The educational use of home robots for children,"Korea, with the world's top class infrastructure in IT, has entered in full scale, the experimental phase of e-learning, and speedily extended to even m-learning. Backed by its IT environment and advances in robot technology, Korea developed the world's first available e-learning home robot and demonstrated the prospect of robots as a new educational media. This study compared the effects of traditional media-assisted learning and Web-based instruction(WBI), with the effects of home robot-assisted learning, which automatically updates e-contents and software through wireless network. The results showed that children felt a home robot was friendlier than other media assisted learning programs. Compared to other learning programs, the home robot was superior in promoting and improving students concentration, interest, and academic achievement.","Educational robots,
Service robots,
Electronic learning,
Robotics and automation,
Internet,
Intelligent robots,
Research and development,
Human computer interaction,
Computer science education,
Educational technology"
Swapping to Remote Memory over InfiniBand: An Approach using a High Performance Network Block Device,"Traditionally, operations with memory on other nodes (remote memory) in cluster environments interconnected with technologies like Gigabit Ethernet have been expensive with latencies several magnitudes slower than local memory accesses. Modern RDMA capable networks such as InfiniBand and Quadrics provide low latency of a few microseconds and high bandwidth of up to 10 Gbps. This has significantly reduced the latency gap between access to local memory and remote memory in modern clusters. Remote idle memory can be exploited to reduce the memory pressure on individual nodes. This is akin to adding an additional level in the memory hierarchy between local memory and the disk, with potentially dramatic performance improvements especially for memory intensive applications. In this paper, we take on the challenge to design a remote paging system for remote memory utilization in InfiniBand clusters. We present the design and implementation of a high performance networking block device (HPBD) over InfiniBand fabric, which serves as a swap device of kernel virtual memory (VM) system for efficient page transfer to/from remote memory servers. Our experiments show that using HPBD, quick sort performs only 1.45 times slower than local memory system, and up to 21 times faster than local disk. And our design is completely transparent to user applications. To the best of our knowledge, it is the first work of a remote pager design using InfiniBand for remote memory utilization","Delay,
Throughput,
Transaction databases,
Space technology,
Memory management,
System performance,
Computer science,
Ethernet networks,
Bandwidth,
Fabrics"
Implementation of an analytically based scatter correction in SPECT reconstructions,"Photon scattering is one of the main effects contributing to the degradation of image quality and to quantitative inaccuracy in nuclear imaging. We have developed a scatter correction based on a simplified version of the analytic photon distribution (APD) method, and have implemented it in an iterative image reconstruction algorithm. The scatter distributions generated using this approach were compared to those obtained using the original APD method. Reconstructions were performed using computer simulations, phantom experiments, and patient data. Images corrected for scatter, attenuation, and collimator blurring were compared to images corrected only for attenuation and collimator blurring. In the simulation studies, results were compared to an ideal situation in which only the primary (unscattered) photon data were reconstructed. Results showed that in all cases, the scatter-corrected images demonstrated substantially improved image contrast relative to no scatter correction. For simulated data, scatter-corrected images had very similar contrast and noise properties to the primary-only reconstructions. Additional work is required to further reduce the computation times to clinically viable amounts.","Electromagnetic scattering,
Particle scattering,
Image reconstruction,
Single photon emission computed tomography,
Attenuation,
Collimators,
Computational modeling,
Degradation,
Image quality,
Nuclear imaging"
Modelling requirements variability across product lines,"The explicit definition of variability in software product lines is a key difference between the development of single software systems and software product line engineering. More and more companies maintain several software product lines which focus on different types of products, market segments, and/or domains. Those product lines typically share commonalities and variability. The companies thus face the problem of managing communality and variability across different product lines. In this paper, we identify essential requirements for the documentation of requirements variability across product lines. We propose a meta model for structuring the variability information, sketch a prototypical realisation for managing variability across product lines in DOORS, and illustrate the use of the meta model in a small example. We further report on experiences made with the proposed variability modelling approach.","Software systems,
Hard disks,
Prototypes,
DVD,
Software maintenance,
Design engineering,
Information management,
Systems engineering and theory,
Computer science,
Information systems"
The role of refactorings in API evolution,"Frameworks and libraries change their APIs. Migrating an application to the new API is tedious and disrupts the development process. Although some tools and ideas have been proposed to solve the evolution of APIs, most updates are done manually. To better understand the requirements for migration tools we studied the API changes of three frameworks and one library. We discovered that the changes that break existing applications are not random, but they tend to fall into particular categories. Over 80% of these changes are refactorings. This suggests that refactoring-based migration tools should be used to update applications.","Application software,
Software libraries,
Software maintenance,
Programming profession,
Computer science,
Software systems,
Costs,
Computer languages,
Operating systems"
A generic framework for efficient subspace clustering of high-dimensional data,"Subspace clustering has been investigated extensively since traditional clustering algorithms often fail to detect meaningful clusters in high-dimensional data spaces. Many recently proposed subspace clustering methods suffer from two severe problems: First, the algorithms typically scale exponentially with the data dimensionality and/or the subspace dimensionality of the clusters. Second, for performance reasons, many algorithms use a global density threshold for clustering, which is quite questionable since clusters in subspaces of significantly different dimensionality will most likely exhibit significantly varying densities. In this paper, we propose a generic framework to overcome these limitations. Our framework is based on an efficient filter-refinement architecture that scales at most quadratic w.r.t. the data dimensionality and the dimensionality of the subspace clusters. It can be applied to any clustering notions including notions that are based on a local density threshold. A broad experimental evaluation on synthetic and real-world data empirically shows that our method achieves a significant gain of runtime and quality in comparison to state-of-the-art subspace clustering algorithms.","Clustering algorithms,
Partitioning algorithms,
Clustering methods,
Data mining,
Gene expression,
Computer science,
Runtime,
Scalability,
Principal component analysis,
Diseases"
Autonomous Terrain Mapping and Classification Using Hidden Markov Models,"This paper presents a new approach for terrain mapping and classification using mobile robots with 2D laser range finders. Our algorithm generates 3D terrain maps and classifies navigable and non-navigable regions on those maps using Hidden Markov models. The maps generated by our approach can be used for path planning, navigation, local obstacle avoidance, detection of changes in the terrain, and object recognition. We propose a map segmentation algorithm based on Markov Random Fields, which removes small errors in the classification. In order to validate our algorithms, we present experimental results using two robotic platforms.","Terrain mapping,
Hidden Markov models,
Path planning,
Navigation,
Mobile robots,
Application software,
Robot sensing systems,
Computer science,
Clouds,
Laboratories"
An experimental study of routing and data aggregation in sensor networks,"Several sensor network applications, such as environmental monitoring, require data aggregation to an observer. For this purpose, a data aggregation tree, rooted at the observer, is constructed in the network. Node clustering can be employed to further balance load among sensor nodes and prolong the network lifetime. In this paper, we design and implement a system, iHEED, in which node clustering is integrated with multi-hop routing for TinyOS. We consider simple data aggregation operators, such as AVG or MAX. We use a simple energy consumption model to keep track of the battery consumption of cluster heads and regular nodes. We perform experiments on a sensor network testbed to quantify the advantages of integrating hierarchical routing with data aggregation. Our results indicate that the network lifetime is prolonged by a factor of 2 to 4, and successful transmissions are almost doubled. Clustering plays a dominant role in delaying the first node death, while aggregation plays a dominant role in delaying the last node death",
Precise robot-assisted guide positioning for distal locking of intramedullary nails,"This paper presents a novel image-guided robot-based system to assist orthopedic surgeons in performing distal locking of long bone intramedullary nails. The system consists of a bone-mounted miniature robot fitted with a drill guide that provides rigid mechanical guidance for hand-held drilling of the distal screws' pilot holes. The robot is automatically positioned so that the drill guide and nail distal locking axes coincide, using a single fluoroscopic X-ray image. Since the robot is rigidly attached to the intramedullary nail or bone, no leg immobilization or real-time tracking is required. We describe the system and protocol and present a method for accurate and robust drill guide and nail hole localization and registration. The in vitro system accuracy experiments for fronto-parallel viewing show a mean angular error of 1.3/spl deg/ (std=0.4/spl deg/) between the computed drill guide axes and the actual locking holes axes, and a mean 3.0 mm error (std=1.1mm) in the entry and exit drill point, which is adequate for successfully locking the nail.","Robots,
Nails,
Robotics and automation,
Bones,
Orthopedic surgery,
Drilling,
Fasteners,
X-ray imaging,
Legged locomotion,
Leg"
GPS: a general peer-to-peer simulator and its use for modeling BitTorrent,"Peer-to-Peer (P2P) systems have become popular over the past few years. However, their large scale and the open nature of the system makes studying them challenging. This paper presents an extensible framework for simulating P2P networks efficiently and accurately. Efficiency is accomplished by using message level simulation rather than packet level simulation. Moreover, accuracy is maintained by tracking the network infrastructure and using a flow model to accomplish accurate estimate of the message behavior. A second contribution of the paper is to model the BitTorrent (BT) protocol. BT is a widely-used protocol that is significantly more complex than other P2P protocols because file download occurs in chunks from many other peers concurrently. Thus, contrary to models of other P2P systems such as Gnutella or Freenet, which focus on finding the location of a file in the network, BT's complexity occurs in downloading files (locating files in fact occurs out of band using Websites that host the Torrent files). We validate the model against a packet level simulator and also using a real, but small scale, BitTorrent experiment. The simulator is object oriented and extensible for simulating other P2P protocols and applications.","Global Positioning System,
Peer to peer computing,
Object oriented modeling,
Protocols,
Computational modeling,
Large-scale systems,
File servers,
Computer science,
Application software,
Network servers"
A motion-tolerant dissolve detection algorithm,"Gradual shot change detection is one of the most important research issues in the field of video indexing/retrieval. Among the numerous types of gradual transitions, the dissolve-type gradual transition is considered the most common one, but it is also the most difficult one to detect. In most of the existing dissolve detection algorithms, the false/miss detection problem caused by motion is very serious. In this paper, we present a novel dissolve-type transition detection algorithm that can correctly distinguish dissolves from disturbance caused by motion. We carefully model a dissolve based on its nature and then use the model to filter out possible confusion caused by the effect of motion. Experimental results show that the proposed algorithm is indeed powerful.","Detection algorithms,
Gunshot detection systems,
Indexing,
Motion detection,
Computer science,
Filters,
Change detection algorithms,
Organizing,
Databases,
Information retrieval"
Distribution of cathode current density and breaking capacity of medium voltage vacuum interrupters with axial magnetic field,"A high-speed photographic technique incorporating computer-aided reconstruction of the cathode current density was applied to butt solid contacts with an external axial magnetic field (AMF) at relatively low current density j<2 kA/cm/sup 2/. In the present work, we have further developed this technique. This advanced technique is applicable for the analysis of the current density distribution on the complicated electrodes of commercial vacuum interrupters (VIs) carrying currents up to I=50 kA at current density up to j/spl sim/4 kA/cm/sup 2/. The experiments have been carried out for three types of electrode designs generating AMFs with different configurations. The results obtained by this new technique proved a previously derived conclusion that current density tends to distribute evenly across that part of the contact surface where the AMF induction fits the inequality B/sub z//sup (1)/","Cathodes,
Current density,
Medium voltage,
Interrupters,
Magnetic fields,
Electrodes,
Solids,
Magnetic analysis,
Vacuum technology,
Induction generators"
JHAVE: supporting algorithm visualization,"JHAVE fosters the use of algorithm visualization as an effective pedagogical tool for computer science educators, helping students to better understand algorithms. The Java-hosted algorithm visualization environment (JHAVE) is not an AV system itself but rather a support environment for a variety of AV systems (called AV engines by JHAVE). In broad terms, JHAVE gives such an engine a drawing context on which it can render its pictures in any way. In return, JHAVE provides the engine with effortless ways to synchronize its graphical displays with i) a standard set of VCR-like controls, ii) information and pseudocode windows, iii) input generators, iv) stop-and-think questions, and v) meaningful content generation tools.","Visualization,
Computer graphics,
Computer science,
Animation,
Computer science education,
Computer displays,
Uniform resource locators,
Materials science and technology,
Computational modeling,
Computer simulation"
The radial trifocal tensor: a tool for calibrating the radial distortion of wide-angle cameras,We present a technique to linearly estimate the radial distortion of a wide-angle lens given three views of a real-world plane. The approach can also be used with pure rotation as in this case all points appear as lying on a plane. The three views can even be recorded using three different cameras as long as the deviation from the pin-hole model for each camera is distortion along radial lines. We introduce the 1D radial camera which projects scene points onto radial lines and the radial trifocal tensor which encodes the multi-view relations between radial lines. Given at least seven triplets of corresponding points the radial trifocal tensor can be computed linearly. This allows recovery of the radial cameras and the projective reconstruction of the plane up to a two fold ambiguity. This 2D reconstruction is unaffected by radial distortion and can be used in different ways to compute the radial distortion parameters. We propose to use the division model as in this case we obtain a linear algorithm that computes the radial distortion coefficients and the 3 remaining degrees of freedom of the homography relating the reconstructed 2D plane to the undistorted image. Each feature point that has at least one corresponding point yields one linear constraint on those unknowns. Our method is validated on real-world images. We successfully calibrate several wide-angle cameras.,"Tensile stress,
Cameras,
Nonlinear distortion,
Calibration,
Lenses,
Layout,
Image reconstruction,
Computer vision,
Computer science,
Mirrors"
Three-dimensional reconstruction and quantification of cervical carcinoma invasion fronts from histological serial sections,"The analysis of the three-dimensional (3-D) structure of tumoral invasion fronts of carcinoma of the uterine cervix is the prerequisite for understanding their architectural-functional relationship. The variation range of the invasion patterns known so far reaches from a smooth tumor-host boundary surface to more diffusely spreading patterns, which all are supposed to have a different prognostic relevance. As a very decisive limitation of previous studies, all morphological assessments just could be done verbally referring to single histological sections. Therefore, the intention of this paper is to get an objective quantification of tumor invasion based on 3-D reconstructed tumoral tissue data. The image processing chain introduced here is capable to reconstruct selected parts of tumor invasion fronts from histological serial sections of remarkable extent (90-500 slices). While potentially gaining good accuracy and reasonably high resolution, microtome cutting of large serial sections especially may induce severe artifacts like distortions, folds, fissures or gaps. Starting from stacks of digitized transmitted light color images, an overall of three registration steps are the main parts of the presented algorithm. By this, we achieved the most detailed 3-D reconstruction of the invasion of solid tumors so far. Once reconstructed, the invasion front of the segmented tumor is quantified using discrete compactness.","Neoplasms,
Magnetic force microscopy,
Scanning electron microscopy,
Transmission electron microscopy,
Image reconstruction,
Magnetic resonance imaging,
Computed tomography,
Image processing,
Image analysis,
Bioinformatics"
"Redundancy, Efficiency and Robustness in Multi-Robot Coverage","Area coverage is an important task for mobile robots, with many real-world applications. Motivated by potential efficiency and robustness improvements, there is growing interest in the use of multiple robots in coverage. Previous investigations of multi-robot coverage focuses on completeness and eliminating redundancy, but does not formally address robustness, nor examine the impact of the initial positions of robots on the coverage time. Indeed, a common assumption is that non-redundancy leads to improved coverage time. We address robustness and efficiency in a family of multi-robot coverage algorithms, based on spanning-tree coverage of approximate cell decomposition. We analytically show that the algorithms are robust, in that as long as a single robot is able to move, the coverage will be completed. We also show that non-redundant (non-back tracking) versions of the algorithms have a worst-case coverage time virtually identical to that of a single robotâ€”thus no performance gain is guaranteed in non-redundant coverage. Moreover, this worst-case is in fact common in real-world applications. Surprisingly, however, redundant coverage algorithms lead to guaranteed performance which halves the coverage time even in the worst case.","Robustness,
Robot sensing systems,
Mobile robots,
Computer science,
Application software,
Algorithm design and analysis,
Performance gain,
Cleaning,
Shape,
Actuators"
Downloading textual hidden web content through keyword queries,"An ever-increasing amount of information on the Web today is available only through search interfaces: the users have to type in a set of keywords in a search form in order to access the pages from certain Web sites. These pages are often referred to as the hidden Web or the deep Web. Since there are no static links to the hidden Web pages, search engines cannot discover and index such pages and thus do not return them in the results. However, according to recent studies, the content provided by many hidden Web sites is often of very high quality and can be extremely valuable to many users. In this paper, we study how we can build an effective hidden Web crawler that can autonomously discover and download pages from the hidden Web. Since the only ""entry point"" to a hidden Web site is a query interface, the main challenge that a hidden Web crawler has to face is how to automatically generate meaningful queries to issue to the site. We provide a theoretical framework to investigate the query generation problem for the hidden Web and we propose effective policies for generating queries automatically. Our policies proceed iteratively, issuing a different query in every iteration. We experimentally evaluate the effectiveness of these policies on 4 real hidden Web sites and our results are very promising. For instance, in one experiment, one of our policies downloaded more than 90% of a hidden Web site (that contains 14 million documents) after issuing fewer than 100 queries","Search engines,
Crawlers,
Computer science,
Information systems,
Web pages,
Software libraries,
Information analysis,
Indexing,
Information retrieval,
Content based retrieval"
A hybrid channel assignment approach using an efficient evolutionary strategy in wireless mobile networks,"In wireless mobile communication systems, radio spectrum is a limited resource. However, efficient use of available channels has been shown to improve the system capacity. The role of a channel assignment scheme is to allocate channels to cells or mobiles in such a way as to minimize call blocking or call dropping probabilities, and also to maximize the quality of service. Channel assignment is known to be an NP-hard optimization problem. In this paper, we have developed an evolutionary strategy (ES) which optimizes the channel assignment. The proposed ES approach uses an efficient problem representation as well as an appropriate fitness function. Our paper deals with a novel hybrid channel assignment based scheme called D-ring. Our D-ring method yields a faster running time and simpler objective function. We also propose a novel way of generating the initial population which reduces the number of channels reassignments and therefore yields a faster running time and may generate a possibly better initial parent. We have obtained better results (as well as faster running time) than a similar approach in literature.","Intelligent networks,
Base stations,
Mobile communication,
Frequency,
Interference constraints,
Wireless communication,
Quality of service,
Computer science,
Interchannel interference,
Power control"
A framework for intelligent sensor network with video camera for structural health monitoring of bridges,"Wireless sensor network (WSN) gives the characteristics of an effective, feasible and fairly reliable monitoring system which shows promise for structural health monitoring (SHM) applications. Monitoring of civil structures generates a large amount of sensor data that is used for structural anomaly detection. Efficiently dealing with this large amount of data in a resource-constrained WSN is a challenge. This paper proposes a, WSN based, novel framework that triggers smart events from sensor data. These events are useful for both intelligent data recording and video camera control. The operation of this framework consists of active & passive sensing modes. In passive mode, selected nodes can intelligently interpret local sensor data to trigger appropriate events. In active mode, most of the sensing nodes perform high frequency sampling and record useful data. Unnecessary data is suppressed which improves the lifespan of the network and simplifies data management.","Intelligent sensors,
Smart cameras,
Bridges,
Wireless sensor networks,
Computerized monitoring,
Condition monitoring,
Sensor systems,
Cables,
Data acquisition,
Computer science"
A stabilizing receding horizon regulator for nonholonomic mobile robots,This paper presents a receding horizon (RH) controller used for regulating a nonholonomic mobile robot. The RH control stability is guaranteed by adding a terminal-state penalty to the cost function and a terminal-state region to optimization constraints. A suboptimal solution to the optimization problem is sufficient to achieve stability. A new terminal-state penalty and its corresponding terminal-state constraints are found. Implementation and simulation results are provided to verify the proposed control strategy.,"Regulators,
Mobile robots,
Robot sensing systems,
Navigation,
Robotics and automation,
Stability,
Computer science,
Robot vision systems,
Dead reckoning,
Optical fibers"
A new and open model to share laboratories on the Internet,"A new model for sharing real laboratories on the Internet and creating a virtual on-line laboratory has been developed and validated in the field of electronic measurements. Testing theories through practice is an important approach to scientific teaching, and appropriate solutions have not yet been found to support this activity in Web-based education. The on-line laboratory addresses this issue. It allows the execution via Web of real experiments and manages concurrency among users who remotely drive instruments and carry out experiments. The experimental setup can be distributed in different real laboratories, spread on a wide-area network, and controlled by local computers. Users can practice through the network and transparently to the actual locations of the devices under test in a multiuser concurrent way. The proposed paradigm increases the possibility of practice in both academic and industrial education and enriches the current experience in e-learning.","Laboratories,
Internet,
Computer science education,
Concurrent computing,
Instruments,
Virtual reality,
Testing,
Electronic learning,
Costs,
Continuing education"
A flexible high-rate USB2 data acquisition system for PET and SPECT imaging,"A new flexible data acquisition system has been developed to instrument gamma-ray imaging detectors designed by the Jefferson Lab Detector and Imaging Group. Hardware consists of 16-channel data acquisition modules installed on USB2 carrier boards. Carriers have been designed to accept one, two, and four modules. Application trigger rate and channel density determines the number of acquisition boards and readout computers used. Each channel has an independent trigger, gated integrator and a 2.5 MHz 12-bit ADC. Each module has an FPGA for analog control and signal processing. Processing includes a 5 ns 40-bit trigger time stamp and programmable triggering, gating, ADC timing, offset and gain correction, charge and pulse-width discrimination, sparsification, event counting, and event assembly. The carrier manages global triggering and transfers module data to a USB buffer. High-granularity time-stamped triggering is suitable for modular detectors. Time stamped events permit dynamic studies, complex offline event assembly, and high-rate distributed data acquisition. A sustained USB data rate of 20 Mbytes/s, a sustained trigger rate of 300 kHz for 32 channels, and a peak trigger rate of 2.5 MHz to FIFO memory were achieved. Different trigger, gating, processing, and event assembly techniques were explored. Target applications include >100 kHz coincidence rate PET detectors, dynamic SPECT detectors, miniature and portable gamma detectors for small-animal and clinical use","Data acquisition,
Positron emission tomography,
Gamma ray detection,
Gamma ray detectors,
Assembly,
Optical imaging,
Universal Serial Bus,
Instruments,
Nuclear imaging,
Hardware"
A comparison of a similarity-based and a feature-based 2-D-3-D registration method for neurointerventional use,"Two-dimensional (2-D)-to-three-dimensional (3-D) registration can improve visualization which may aid minimally invasive neurointerventions. Using clinical and phantom studies, two state-of-the-art approaches to rigid registration are compared quantitatively: an intensity-based algorithm using the gradient difference similarity measure; and an iterative closest point (ICP)-based algorithm. The gradient difference approach was found to be more accurate, with an average registration accuracy of 1.7 mm for clinical data, compared to the ICP-based algorithm with an average accuracy of 2.8 mm. In phantom studies, the ICP-based algorithm proved more reliable, but with more complicated clinical data, the gradient difference algorithm was more robust. Average computation time for the ICP-based algorithm was 20 s per registration, compared with 14 min and 50 s for the gradient difference algorithm.",
Geometric modeling of the human normal cerebral arterial system,"We propose an anatomy-based approach for an efficient construction of a three-dimensional human normal cerebral arterial model from segmented and skeletonized angiographic data. The centerline-based model is used for an accurate angiographic data representation. A vascular tree is represented by tubular segments and bifurcations whose construction takes into account vascular anatomy. A bifurcation is defined quantitatively and the algorithm calculating it is given. The centerline is smoothed by means of a sliding average filter. As the vessel radius is sensitive to quality of data as well as accuracy of segmentation and skeletonization, radius outlier removal and radius regression algorithms are formulated and applied. In this way, the approach compensates for some inaccuracies introduced during segmentation and skeletonization. To create the frame of vasculature, we use two different topologies: tubular and B-subdivision based. We also propose a technique to prevent vessel twisting. The analysis of the vascular model is done on a variety of data containing 258 vascular segments and 131 bifurcations. Our approach gives acceptable results from anatomical, topological and geometrical standpoints as well as provides fast visualization and manipulation of the model. The approach is applicable for building a reference cerebrovascular atlas, developing applications for simulation and planning of interventional radiology procedures and vascular surgery, and in education.","Solid modeling,
Humans,
Bifurcation,
Biomedical imaging,
Anatomy,
Topology,
Data visualization,
Radiology,
Surgery,
Biological system modeling"
Combining generative models and Fisher kernels for object recognition,"Learning models for detecting and classifying object categories is a challenging problem in machine vision. While discriminative approaches to learning and classification have, in principle, superior performance, generative approaches provide many useful features, one of which is the ability to naturally establish explicit correspondence between model components and scene features - this, in turn, allows for the handling of missing data and unsupervised learning in clutter. We explore a hybrid generative/discriminative approach using 'Fisher kernels' by Jaakkola and Haussler (1999) which retains most of the desirable properties of generative methods, while increasing the classification performance through a discriminative setting. Furthermore, we demonstrate how this kernel framework can be used to combine different types of features and models into a single classifier. Our experiments, conducted on a number of popular benchmarks, show strong performance improvements over the corresponding generative approach and are competitive with the best results reported in the literature.",
Close encounters: spatial distances between people and a robot of mechanistic appearance,"This paper presents the results from two empirical exploratory studies of human-robot interaction in the context of an initial encounter with a robot of mechanistic appearance. The first study was carried out with groups of children, and the second with single adults. The analysis concentrates on the personal space zones and initial distances between robot and humans, the context of the encounters and the human's perception of the robot as a social being. We discuss the results of these observations and analyses, and also compare the child and adult data. The child groups showed a dominant response to prefer the 'social zone' distance, comparable to distances people adopt when talking to other humans. From the single adult studies a small majority preferred the 'personal zone', reserved for talking to friends. However, significant minorities deviate from this pattern. Implications for future work are discussed","Human robot interaction,
Orbital robotics,
Cognitive robotics,
Speech,
Tellurium,
Adaptive systems,
Computer science,
Educational institutions,
Contracts,
Psychology"
A Trust based Access Control Framework for P2P File-Sharing Systems,"Peer-to-peer (P2P) file sharing systems have become popular as a new paradigm for information exchange. However, the decentralized and anonymous characteristics of P2P environments make the task of controlling access to sharing information more difficult, which cannot be done by traditional access control methods. In this paper, we identify access control requirements in such environments and propose a trust based access control framework for P2P file-sharing systems. The framework integrates aspects of trust and recommendation models, fairness based participation schemes and access control schemes, and applies them to P2P file-sharing systems. We believe that the proposed scheme is realistic and argue that our approach preserves P2P decentralized structure and peers' autonomy property whist enabling collaboration between peers.","Access control,
Peer to peer computing,
Network servers,
Telecommunication traffic,
Traffic control,
Computer networks,
Centralized control,
Collaboration,
Distributed computing,
Application software"
CirculaFloor [locomotion interface],"The CirculaFloor locomotion interface's movable tiles employ a holonomic mechanism to achieve omnidirectional motion. Users can thus maintain their position while walking in a virtual environment. The CirculaFloor method exploits both the treadmill and footpad, creating an infinite omnidirectional surface using a set of movable tiles. The tiles provide a sufficient area for walking, and thus precision tracing of the foot position is not required. This method has the potential to create an uneven surface by mounting an up-and-down mechanism on each tile. This article is available with a short video documentary on CD-ROM.","Legged locomotion,
Hardware,
Motion measurement,
Position measurement,
Sensor systems,
Virtual environment,
Information science,
Computer architecture,
Computational modeling"
Correction of adversarial errors in networks,"We design codes to transmit information over a network, some subset of which is controlled by a malicious adversary. The computationally unbounded, hidden adversary knows the message to be transmitted, and can observe and change information over the part of the network being controlled. The network nodes do not share resources such as shared randomness or a private key. We first consider a unicast problem in a network with |epsiv parallel, unit-capacity, directed edges. The rate-region has two parts. If the adversary controls a fraction p < 0.5 of the |epsiv edges, the maximal throughput equals (1 - p) |epsiv|. We describe low-complexity codes that achieve this rate-region. We then extend these results to investigate more general multicast problems in directed, acyclic networks","Error correction,
Intelligent networks,
Decoding,
Computer errors,
Computer science,
Computer networks,
Unicast,
Throughput,
Channel coding,
Size control"
On the power of quantum memory,"We address the question whether quantum memory is more powerful than classical memory. In particular, we consider a setting where information about a random n-bit string X is stored in s classical or quantum bits, for s","Quantum mechanics,
Privacy,
Cryptography,
Information security,
Cryptographic protocols,
Information theory,
Context,
Time measurement,
Information processing,
Computer science"
On-line Conservative Learning for Person Detection,"We present a novel on-line conservative learning framework for an object detection system. All algorithms operate in an on-line mode, in particular we also present a novel on-line AdaBoost method. The basic idea is to start with a very simple object detection system and to exploit a huge amount of unlabeled video data by being very conservative in selecting training examples. The key idea is to use reconstructive and discriminative classifiers in an iterative co-training fashion to arrive at increasingly better object detectors. We demonstrate the framework on a surveillance task where we learn person detectors that are tested on two surveillance video sequences. We start with a simple moving object classifier and proceed with incremental PCA (on shape and appearance) as a reconstructive classifier, which in turn generates a training set for a discriminative on-line AdaBoost classifier","Object detection,
Visual system,
Surveillance,
Computer vision,
Robustness,
Detectors,
Layout,
Computer science education,
Educational programs,
Educational technology"
Power assignment for k-connectivity in wireless ad hoc networks,"The problem min-power k-connectivity seeks a power assignment to the nodes in a given wireless ad hoc network such that the produced network topology is k-connected and the total power is the lowest. In this paper, we present several approximation algorithms for this problem. Specifically, we propose a 3k-approximation algorithm for any k /spl ges/ 3, a (k + 12H (k))-approximation algorithm for k(2k - 1) /spl les/ n where n is the network size, a (k + 2 [(k + 1)/2])-approximation algorithm for 2 /spl les/ k /spl les/ 7, a 6-approximation algorithm for k = 3, and a 9-approximation algorithm for k = 4.","Intelligent networks,
Ad hoc networks,
Approximation algorithms,
Network topology,
Computer science,
Mobile ad hoc networks,
Wireless sensor networks,
Energy consumption,
Batteries,
Energy conservation"
Multiple case studies to enhance project-based learning in a computer architecture course,"The IEEE/Association for Computing Machinery (ACM) Computing Curricula and the Accreditation Board of Engineering and Technology (ABET) Evaluation Criteria 2000 emphasize the use of recurrent concepts and system design/evaluation through projects and case studies in the curriculum of Computer and Electrical Engineering. In addition, efficient teamwork, autonomy, and initiative are commonly required qualifications for a professional in this field. Project-based learning approaches that require the students to handle realistic case studies are adequate to pursue these objectives. However, these pedagogical approaches tend to be rejected because they promote deep learning but focus on a restricted set of concepts, whereas many engineering curricula require a broad range of concepts to be covered in each course. The introduction of multiple case studies carried out simultaneously in the same course by different teams of students can broaden the set of concepts studied, but collaboration at different levels must be strongly enforced to achieve effective learning. This paper describes a multiple-case-study project design that has been applied to a computer architecture course for four years. After systematically evaluating the experience, the authors conclude that students achieve a deep learning of the concepts required in their own case study, while they are able to generalize their knowledge to case studies of different characteristics from those considered during the course. Furthermore, a number of collaborative skills and attitudes are developed as a consequence of the proposed environment based on multiple levels of collaboration.",
Improving handoff performance in wireless overlay networks by switching between two-layer IPv6 and one-layer IPv6 addressing,"In wireless/mobile networks, users freely change their service points, while they are communicating with other users. In order to support the mobility of mobile nodes, Mobile IPv6 (MIPv6) is proposed by the Internet Engineering Task Force (IETF), in which a mobile node must inform its home agent the binding of its home address and the current care-of-address (CoA). The home agent forwards packets to CoA when it receives packets for the mobile node. There is a significant problem in MIPv6 due to its inability to support micromobility caused by long delays and high packet losses during a handoff. Hierarchical Mobile IPv6 (HMIPv6) is proposed to separate mobility into micromobility [within one domain or within the same mobility anchor point (MAP)] and macromobility (between domains or between MAPs). HMIPv6 reduces handoff latency by employing a hierarchical network structure and minimizing the location update signaling with external network. The two-layer network structure of HMIPv6 is very suitable for supporting the vertical handoff and the horizontal handoff in wireless overlay networks. Wireless overlay networks can consist of two layers. Functions of access routers can be implemented in low-layer networks and functions of MAPs can be implemented in high-layer networks. Or we can have an access router collocated with a low-layer network and an MAP collocated with a high-layer network. In either way, the micromobility in HMIPv6 is equal to the horizontal handoff in wireless overlay networks and the macromobility in HMIPv6 is equal to the vertical handoff in wireless overlay networks. However, a significant delay still occurs in macromobility management. This paper considers handover operations. We analyze the handoff delay and find that the DAD time represents a large portion of handoff delay. We also note that we can assign a unique on-link care-of address (LCoA) to each mobile node and switch between one-layer IPv6 and two-layer IPv6 addressing. By this way, we propose a Stealth-time HMIP (SHMIP) which can reduce the effect of the DAD time on the handoff delay and, thus, reduce the handoff time significantly. To further reduce packet losses, we also adopt prehandoff notification to request previous MAP to buffer packets for the mobile node. By simulations, we show that the proposed scheme can realize low handoff delays and low packet losses during macromobility.","Intelligent networks,
Delay effects,
Wireless LAN,
Switches,
Base stations,
Internet,
Computer science,
Mobile computing,
Computer networks,
Wireless networks"
A look into the future of nanoelectronics,"On the occasion of the 25th anniversary of the VLSI Symposium, it is appropriate to reflect on the past and peer into the future. It is clear that continuing scaling in the coming decade is no longer an evolution but rather a revolution involving materials science based engineering at the device level and computer science at the systems level - not ""living apart together"" but intensely interlinked in the design world. The challenges are daunting: materials and device breakthroughs, innovations in circuit and system architecture, new design tools and skills are urgently needed if we are to reconcile nanoscale realities with the promises of nomadic connectivity and ""embedded-everywhere"" systems. Close interactions among a multitude of disciplines are mandatory. But while ""getting it all together"" is not for the faint of heart, life has never been more exciting for the scientist with a sharp eye and an open mind.","Nanoelectronics,
Very large scale integration,
Materials science and technology,
Design engineering,
Computer science,
Technological innovation,
Circuits and systems,
Computer architecture,
Nanoscale devices,
Heart"
Mitigating malicious control packet floods in ad hoc networks,"We investigate the impact of hacker attacks by malicious nodes on the overall network performance. These malicious nodes mimic normal nodes in all aspects, except that they do route discoveries much more frequently than other nodes. We show, using simulations, that the basic route discovery mechanism used in many ad hoc network protocols can be exploited by as few as one malicious or compromised node to bring down throughput dramatically. We propose an adaptive statistical packet dropping mechanism to mitigate such situations and reduce the loss of throughput. The proposed mechanism works even when the identity of the malicious node is unknown and does not use any additional network bandwidth. It is simple to implement and maintains or improves network throughput when there are no malicious nodes, but the network is congested with excess traffic.",
VioCluster: Virtualization for Dynamic Computational Domains,"A large organization, such as a university, commonly supplies computational power through multiple independently administered computational domains (e.g. clusters). Each computational domain faces the conflict between dynamic workload and static capacity. This is clearly inefficient at times when some clusters have idle nodes while others experience excessive workload. An opportunity arises to resolve this conflict by dynamically adapting the capacity of clusters by borrowing idle machines of peer domains. In this paper, we present the design, implementation, and evaluation of VioCluster, a virtualization based computational resource sharing platform. Through machine and network virtualization, VioCluster enables virtual computational domains that safely ""trade"" machines between them without infringing on the autonomy of either domain. Our performance evaluation results show that dynamic machine trading between virtual domains increases their resource utilization and decreases their job wait times","Peer to peer computing,
Virtual manufacturing,
Resource management,
Hardware,
Software packages,
Virtual machining,
Computer science,
Power supplies,
Resource virtualization,
Platform virtualization"
Using the inner-distance for classification of articulated shapes,"We propose using the inner-distance between landmark points to build shape descriptors. The inner-distance is defined as the length of the shortest path between landmark points within the shape silhouette. We show that the inner-distance is articulation insensitive and more effective at capturing complex shapes with part structures than Euclidean distance. To demonstrate this idea, it is used to build a new shape descriptor based on shape contexts. After that, we design a dynamic programming based method for shape matching and comparison. We have tested our approach on a variety of shape databases including an articulated shape dataset, MPEG7 CE-Shape-1, Kimia silhouettes, a Swedish leaf database and a human motion silhouette dataset. In all the experiments, our method demonstrates effective performance compared with other algorithms.",
Speech and crosstalk detection in multichannel audio,"The analysis of scenarios in which a number of microphones record the activity of speakers, such as in a round-table meeting, presents a number of computational challenges. For example, if each participant wears a microphone, speech from both the microphone's wearer (local speech) and from other participants (crosstalk) is received. The recorded audio can be broadly classified in four ways: local speech, crosstalk plus local speech, crosstalk alone and silence. We describe two experiments related to the automatic classification of audio into these four classes. The first experiment attempted to optimize a set of acoustic features for use with a Gaussian mixture model (GMM) classifier. A large set of potential acoustic features were considered, some of which have been employed in previous studies. The best-performing features were found to be kurtosis, ""fundamentalness,"" and cross-correlation metrics. The second experiment used these features to train an ergodic hidden Markov model classifier. Tests performed on a large corpus of recorded meetings show classification accuracies of up to 96%, and automatic speech recognition performance close to that obtained using ground truth segmentation.","Crosstalk,
Microphones,
Hidden Markov models,
Automatic speech recognition,
Speech recognition,
Speech analysis,
Laboratories,
Audio recording,
Video recording,
Computer science"
Test prioritization using system models,"During regression testing, a modified system is retested using the existing test suite. Because the size of the test suite may be very large, testers are interested in detecting faults in the system as early as possible during the retesting process. Test prioritization tries to order test cases for execution so the chances of early detection of faults during retesting are increased. The existing prioritization methods are based on the code of the system. System modeling is a widely used technique to model state-based systems. In this paper, we present methods of test prioritization based on state-based models after changes to the model and the system. The model is executed for the test suite and information about model execution is used to prioritize tests. Execution of the model is inexpensive as compared to execution of the system; therefore the overhead associated with test prioritization is relatively small. In addition, we present an analytical framework for evaluation of test prioritization methods. This framework may reduce the cost of evaluation as compared to the existing evaluation framework that is based on experimentation (observation). We have performed an experimental study in which we compared different test prioritization methods. The results of the experimental study suggest that system models may improve the effectiveness of test prioritization with respect to early fault detection.",
Thema: Byzantine-fault-tolerant middleware for Web-service applications,"Distributed applications composed of collections of Web services may call for diverse levels of reliability in different parts of the system. Byzantine fault tolerance (BFT) is a general strategy that has recently been shown to be practical for the development of certain classes of survivable, client-server, distributed applications; however, little research has been done on incorporating it into selective parts of multi-tier, distributed applications like Web services that have heterogeneous reliability requirements. To understand the impacts of combining BFT and Web services, we have created Thema, a new BFT middleware system that extends the BFT and Web services technologies to provide a structured way to build Byzantine-fault-tolerant, survivable Web services that application developers can use like other Web services. From a reliability perspective, our enhancements are also novel in that they allow Byzantine-fault-tolerant services: (1) to support the multi-tiered requirements of Web services, and (2) to provide standardized Web services support for their own clients (through WSDL interfaces and SOAP communication). In this paper we study key architectural implications of combining BFT with Web services and provide a performance evaluation of Thema using the TPC-W benchmark.","Middleware,
Web services,
Computer crashes,
Application software,
Fault tolerance,
Security,
Computer science,
Simple object access protocol,
Diversity reception,
Availability"
"The unique games conjecture, integrality gap for cut problems and embeddability of negative type metrics into l/sub 1/","In this paper, we disprove the following conjecture due to Goemans (1997) and Linial (2002): ""Every negative type metric embeds into with constant distortion."" We show that for every /spl delta/ > 0, and for large enough n, there is an n-point negative type metric which requires distortion at-least (log log n) /sup 1/6-/spl delta// to embed into l/sub 1/. Surprisingly, our construction is inspired by the Unique Games Conjecture (UGC) of Khot (2002), establishing a previously unsuspected connection between PCPs and the theory of metric embeddings. We first prove that the UGC implies super-constant hardness results for (non-uniform) sparsest cut and minimum uncut problems. It is already known that the UGC also implies an optimal hardness result for maximum cut (2004). Though these hardness results depend on the UGC, the integrality gap instances rely ""only"" on the PCP reductions for the respective problems. Towards this, we first construct an integrality gap instance for a natural SDP relaxation of unique games. Then, we ""simulate"" the PCP reduction and ""translate""the integrality gap instance of unique games to integrality gap instances for the respective cut problems! This enables us to prove a (log log n) /sup 1/6-/spl delta// integrality gap for (nonuniform) sparsest cut and minimum uncut, and an optimal integrality gap for maximum cut. All our SDP solutions satisfy the so-called ""triangle inequality"" constraints. This also shows, for the first time, that the triangle inequality constraints do not add any power to the Goemans-Williamson's SDP relaxation of maximum cut. The integrality gap for sparsest cut immediately implies a lower bound for embedding negative type metrics into l/sub i/. It also disproves the non-uniform version of Arora, Rao and Vazirani's Conjecture (2004), asserting that the integrality gap of the sparsest cut SDP, with the triangle inequality constraints, is bounded from above by a constant.","Partitioning algorithms,
Iterative algorithms,
User-generated content,
Algorithm design and analysis,
Approximation algorithms,
Educational institutions,
Embedded computing,
Game theory,
NP-hard problem,
Extraterrestrial measurements"
Straggling and extreme cases in the energy deposition by ions in submicron silicon volumes,"The variations in the energy deposited by ions in thin silicon layers and submicron volumes are calculated using Monte Carlo and convolution methods. For the /spl delta/-electron spectra we use a detailed spectrum, considering solid state effects. For each ion, the energy it deposits in a sensitive (to single events) volume (SV) is found by subtracting the energy of the /spl delta/-electrons when escaping the SV from the energy the ion loses in the SV. The behavior of the resulting straggling functions is studied in detail as function of the SV shape and angle of incidence of the ion beam. They are used for estimating the probabilities of single events, in particular those due to rare high energy deposition events. The latter are compared with the probability for events due to nuclear reactions of the ion with the electronic material nuclei.",
"A Secure Image Steganography using LSB, DCT and Compression Techniques on Raw Images","Steganography is an important area of research in recent years involving a number of applications. It is the science of embedding information into the cover image viz., text, video, and image (payload) without causing statistically significant modification to the cover image. The modern secure image steganography presents a challenging task of transferring the embedded information to the destination without being detected. In this paper we present an image based steganography that combines Least Significant Bit(LSB), Discrete Cosine Transform(DCT), and compression techniques on raw images to enhance the security of the payload. Initially, the LSB algorithm is used to embed the payload bits into the cover image to derive the stego-image. The stego-image is transformed from spatial domain to the frequency domain using DCT. Finally quantization and runlength coding algorithms are used for compressing the stego-image to enhance its security. It is observed that secure images with low MSE and BER are transferred without using any password, in comparison with earlier works.","Steganography,
Discrete cosine transforms,
Image coding,
Payloads,
Modems,
Information security,
Frequency domain analysis,
Cryptography,
Watermarking,
Filtering"
PETS Metrics: On-Line Performance Evaluation Service,This paper presents the PETS Metrics On-line Evaluation Service for computational visual surveillance algorithms. The service allows researchers to submit their algorithm results for evaluation against a set of applicable metrics. The results of the evaluation processes are publicly displayed allowing researchers to instantly view how their algorithm performs against previously submitted algorithms. The approach has been validated using seven motion segmentation algorithms.,
Tomographic fluorescence imaging in tissue phantoms: a novel reconstruction algorithm and imaging geometry,"A novel image reconstruction algorithm has been developed and demonstrated for fluorescence-enhanced frequency-domain photon migration (FDPM) tomography from measurements of area illumination with modulated excitation light and area collection of emitted fluorescence light using a gain modulated image-intensified charge-coupled device (ICCD) camera. The image reconstruction problem was formulated as a nonlinear least-squares-type simple bounds constrained optimization problem based upon the penalty/modified barrier function (PMBF) method and the coupled diffusion equations. The simple bounds constraints are included in the objective function of the PMBF method and the gradient-based truncated Newton method with trust region is used to minimize the function for the large-scale problem (39919 unknowns, 2973 measurements). Three-dimensional (3-D) images of fluorescence absorption coefficients were reconstructed using the algorithm from experimental reflectance measurements under conditions of perfect and imperfect distribution of fluorophore within a single target. To our knowledge, this is the first time that targets have been reconstructed in three-dimensions from reflectance measurements with a clinically relevant phantom.",
GangSim: a simulator for grid scheduling studies,"Large distributed grid systems pose new challenges in job scheduling due to complex workload characteristics and system characteristics. Due to the numerous parameters that must be considered and the complex interactions that can occur between different resource allocation policies, analytical modeling of system behavior appears impractical. Thus, we have developed the GangSim simulator to support studies of scheduling strategies in grid environments, with a particular focus on investigations of the interactions between local and community resource allocation policies. The GangSim implementation is derived in part from the Ganglia distributed monitoring framework, an implementation approach that permits mixing of simulated and real grid components. We present examples of the studies that GangSim permits, showing in particular how we can use GangSim to study the behavior of VO schedulers as a function of scheduling policy, resource usage policies, and workloads. We also present the results of experiments conducted on an operational Grid, Grid3, to evaluate GangSim's accuracy. These latter studies point to the need for more accurate modeling of various aspects of local site behavior.","Resource management,
Computerized monitoring,
Analytical models,
Scheduling algorithm,
Mathematics,
Computer science,
Laboratories,
System testing,
Computational modeling,
Control systems"
Recognizing human actions in videos acquired by uncalibrated moving cameras,"Most work in action recognition deals with sequences acquired by stationary cameras with fixed viewpoints. Due to the camera motion, the trajectories of the body parts contain not only the motion of the performing actor but also the motion of the camera. In addition to the camera motion, different viewpoints of the same action in different environments result in different trajectories, which can not be matched using standard approaches. In order to handle these problems, we propose to use the multi-view geometry between two actions. However, well known epipolar geometry of the static scenes where the cameras are stationary is not suitable for our task. Thus, we propose to extend the standard epipolar geometry to the geometry of dynamic scenes where the cameras are moving. We demonstrate the versatility of the proposed geometric approach for recognition of actions in a number of challenging sequences","Humans,
Videos,
Cameras,
Geometry,
Computer science,
Layout,
Image motion analysis,
Optical computing,
Surveillance,
Content based retrieval"
The Strong correlation Between Code Signatures and Performance,"A recent study examined the use of sampled hardware counters to create sampled code signatures. This approach is attractive because sampled code signatures can be quickly gathered for any application. The conclusion of their study was that there exists a fuzzy correlation between sampled code signatures and performance predictability. The paper raises the question of how much information is lost in the sampling process, and our paper focuses on examining this issue. We first focus on showing that there exists a strong correlation between code signatures and performance. We then examine the relationship between sampled and full code signatures, and how these affect performance predictability. Our results confirm that there is a fuzzy correlation found in recent work for the SPEC programs with sampled code signatures, but that a strong correlation exists with full code signatures. In addition, we propose converting the sampled instruction counts, used in the prior work, into sampled code signatures representing loop and procedure execution frequencies. These sampled loop and procedure code signatures allow phase analysis to more accurately and easily find patterns, and they correlate better with performance","Counting circuits,
Sampling methods,
Hardware,
Performance analysis,
Databases,
Computer science,
Instruments,
Lifting equipment,
Application software,
Frequency conversion"
A fully polynomial-time approximation scheme for feasibility analysis in static-priority systems with arbitrary relative deadlines,"Current feasibility tests for the static-priority scheduling on uniprocessors of periodic task systems run in pseudo-polynomial time. We present a fully polynomial-time approximation scheme (FPTAS) for feasibility analysis in static-priority systems with arbitrary relative deadlines. This test is an approximation with respect to the amount of a processor's capacity that must be ""sacrificed"" for the test to become exact. We show that an arbitrary level of accuracy, /spl epsi/, may be chosen for the approximation scheme, and present a runtime bound that is polynomial in terms of /spl epsi/ and the number of tasks, n.",
Swimming and Crawling with an Amphibious Snake Robot,"We present AmphiBot I, an amphibious snake robot capable of crawling and swimming. Experiments have been carried out to characterize how the speed of locomotion depends on the frequencies, amplitudes, and phase lags of undulatory gaits, both in water and on ground. Using this characterization, we can identify the fastest gaits for a given medium. Results show that the fastest gaits are different from one medium to the other, with larger optimal regions in parameter space for the crawling gaits. Swimming gaits are faster than crawling gaits for the same frequencies. For both media, the fastest locomotion is obtained with total phase lags that are smaller than one. These results are compared with data from fishes and from amphibian snakes.",
The value of visualization,"The field of visualization is getting mature. Many problems have been solved, and new directions are sought for. In order to make good choices, an understanding of the purpose and meaning of visualization is needed. Especially, it would be nice if we could assess what a good visualization is. In this paper an attempt is made to determine the value of visualization. A technological viewpoint is adopted, where the value of visualization is measured based on effectiveness and efficiency. An economic model of visualization is presented, and benefits and costs are established. Next, consequences (brand limitations of visualization are discussed (including the use of alternative methods, high initial costs, subjective/less, and the role of interaction), as well as examples of the use of the model for the judgement of existing classes of methods and understanding why they are or are not used in practice. Furthermore, two alternative views on visualization are presented and discussed: viewing visualization as an art or as a scientific discipline. Implications and future directions are identified.",
On the eigenspectrum of the gram matrix and the generalization error of kernel-PCA,"In this paper, the relationships between the eigenvalues of the m/spl times/m Gram matrix K for a kernel /spl kappa/(/spl middot/,/spl middot/) corresponding to a sample x/sub 1/,...,x/sub m/ drawn from a density p(x) and the eigenvalues of the corresponding continuous eigenproblem is analyzed. The differences between the two spectra are bounded and a performance bound on kernel principal component analysis (PCA) is provided showing that good performance can be expected even in very-high-dimensional feature spaces provided the sample eigenvalues fall sufficiently quickly.","Eigenvalues and eigenfunctions,
Principal component analysis,
Kernel,
Symmetric matrices,
Statistical learning,
Support vector machines,
Gaussian processes,
Machine learning,
Algorithm design and analysis,
Computer science"
Online selecting discriminative tracking features using particle filter,"The paper proposes a method to keep the tracker robust to background clutters by online selecting discriminative features from a large feature space. Furthermore, the feature selection procedure is embedded into the particle filtering process with the aid of existed ""background"" particles. Feature values from background patches and object observations are sampled during tracking and Fisher discriminant is employed to rank the classification capacity of each feature based on sampled values. Top-ranked discriminative features are selected into the appearance model and simultaneously invalid features are removed out to adjust the object representation adaptively. The implemented tracker with online discriminative feature selection module embedded shows promising results on experimental video sequences.","Particle tracking,
Particle filters,
Histograms,
Computational efficiency,
Space technology,
Robustness,
Gabor filters,
Computer science,
Filtering,
Video sequences"
Voronoi treemaps,"Treemaps are a well known method for the visualization of attributed hierarchical data. Previously proposed treemap layout algorithms are limited to rectangular shapes, which cause problems with the aspect ratio of the rectangles as well as with identifying the visualized hierarchical structure. The approach of Voronoi treemaps presented in this paper eliminates these problems through enabling subdivisions of and in polygons. Additionally, this allows for creating treemap visualizations within areas of arbitrary shape, such as triangles and circles, thereby enabling a more flexible adaptation of treemaps for a wider range of applications.",
An information-theoretic criterion for intrasubject alignment of FMRI time series: motion corrected independent component analysis,"A three-dimensional image registration method for motion correction of functional magnetic resonance imaging (fMRI) time-series, based on independent component analysis (ICA), is described. We argue that movement during fMRI data acquisition results in a simultaneous increase in the joint entropy of the observed time-series and a decrease in the joint entropy of a nonlinear function of the derived spatially independent components calculated by ICA. We propose this entropy difference as a reliable criterion for motion correction and refer to a method that maximizes this as motion-corrected ICA (MCICA). Specifically, a given motion-corrupted volume may be corrected by determining the linear combination of spatial transformations of the motion-corrupted volume that maximizes the proposed criterion. In essence, MCICA consists of designing an adaptive spatial resampling filter which maintains maximum temporal independence among the recovered components. In contrast with conventional registration methods, MCICA does not require registration of motion-corrupted volumes to a single reference volume which can introduce artifacts because corrections are applied without accounting for variability due to the task-related activation. Simulations demonstrate that MCICA is robust to activation level, additive noise, random motion in the reference volumes and the exact number of independent components extracted. When the method was applied to real data with minimal estimated motion, the method had little effect and, hence, did not introduce spurious changes in the data. However, in a data series from a motor fMRI experiment with larger motion, preprocessing the data with the proposed method resulted in the emergence of activation in primary motor and supplementary motor cortices. Although mutual information (MI) was not explicitly optimized, the MI between all subsequent volumes and the first one was consistently increased for all volumes after preprocessing the data with MCICA. We suggest MCICA represents a robust and reliable method for preprocessing of fMRI time-series corrupted with motion.","Motion analysis,
Independent component analysis,
Entropy,
Image registration,
Magnetic resonance imaging,
Data acquisition,
Adaptive filters,
Noise robustness,
Additive noise,
Data mining"
Augmented virtuality based on stereoscopic reconstruction in multimodal image-guided neurosurgery: methods and performance evaluation,"Displaying anatomical and physiological information derived from preoperative medical images in the operating room is critical in image-guided neurosurgery. This paper presents a new approach referred to as augmented virtuality (AV) for displaying intraoperative views of the operative field over three-dimensional (3-D) multimodal preoperative images onto an external screen during surgery. A calibrated stereovision system was set up between the surgical microscope and the binocular tubes. Three-dimensional surface meshes of the operative field were then generated using stereopsis. These reconstructed 3-D surface meshes were directly displayed without any additional geometrical transform over preoperative images of the patient in the physical space. Performance evaluation was achieved using a physical skull phantom. Accuracy of the reconstruction method itself was shown to be within 1 mm (median: 0.76 mm /spl plusmn/ 0.27), whereas accuracy of the overall approach was shown to be within 3 mm (median: 2.29 mm /spl plusmn/ 0.59), including the image-to-physical space registration error. We report the results of six surgical cases where AV was used in conjunction with augmented reality. AV not only enabled vision beyond the cortical surface but also gave an overview of the surgical area. This approach facilitated understanding of the spatial relationship between the operative field and the preoperative multimodal 3-D images of the patient.",
User behavior analysis of a video-on-demand service with a wide variety of subjects and lengths,"This paper presents the analysis performed on the www.lne.es video-on-demand service (LNE TV), which is part of the digital version of one of the most important newspapers in Spain. Its principal special characteristic is the wide range of subjects (news, music, culture, tourism, nature, sports, etc) and lengths of the offered contents (from 2 minutes to 2 hours), which make it an interesting case study. Elements about user behavior have been analyzed such as session analysis; delivered time, pause distribution, jumps length, etc. The study points out interesting results about length dependence, interactions appearing, popularity of the videos, etc., which are compared with the results obtained in previous works, generally developed on educational environments (services or users). A behavior user model has been performed using the results of the analyses. This model is oriented to services with different types of information and lengths for the videos. This study has been performed thanks to an access log database with more than 150,000 requests of almost 900 videos stored over a period of 4 years. The conclusions of the study are essential to improve the service configuration and content selection. Moreover, they can be used to develop service models for video-on-demand services, which can help administrators to predict future situations and avoid performance problems.","Videos,
Performance analysis,
Web and internet services,
Load modeling,
TV,
Databases,
Computer science,
Predictive models,
Web sites,
Bandwidth"
Test suite reduction with selective redundancy,"Software testing is a critical part of software development. Test suite sizes may grow significantly with subsequent modifications to the software over time. Due to time and resource constraints for testing, test suite minimization techniques attempt to remove those test cases from the test suite that have become redundant over time since the requirements covered by them are also covered by other test cases in the test suite. Prior work has shown that test suite minimization techniques can severely compromise the fault detection effectiveness of test suites. In this paper, we present a novel approach to test suite reduction that attempts to selectively keep redundant tests in the reduced suites. We implemented our technique by modifying an existing heuristic for test suite minimization. Our experiments show that our approach can significantly improve the fault detection effectiveness of reduced suites without severely affecting the extent of test suite size reduction.","Fault detection,
Software testing,
Redundancy,
Computer science,
Programming,
Time factors,
Life testing,
Resource management,
Software development management,
Flow graphs"
Entropy and the law of small numbers,"Two new information-theoretic methods are introduced for establishing Poisson approximation inequalities. First, using only elementary information-theoretic techniques it is shown that, when S/sub n/=/spl Sigma//sub i=1//sup n/X/sub i/ is the sum of the (possibly dependent) binary random variables X/sub 1/,X/sub 2/,...,X/sub n/, with E(X/sub i/)=p/sub i/ and E(S/sub n/)=/spl lambda/, then D(P(S/sub n/)/spl par/Po(/spl lambda/)) /spl les//spl Sigma//sub i=1//sup n/p/sub i//sup 2/+[/spl Sigma//sub i=1//sup n/H(X/sub i/)-H(X/sub 1/,X/sub 2/,...,X/sub n/)] where D(P(S/sub n/)/spl par/Po(/spl lambda/)) is the relative entropy between the distribution of S/sub n/ and the Poisson (/spl lambda/) distribution. The first term in this bound measures the individual smallness of the X/sub i/ and the second term measures their dependence. A general method is outlined for obtaining corresponding bounds when approximating the distribution of a sum of general discrete random variables by an infinitely divisible distribution. Second, in the particular case when the X/sub i/ are independent, the following sharper bound is established: D(P(S/sub n/)/spl par/Po(/spl lambda/))/spl les/1//spl lambda/ /spl Sigma//sub i=1//sup n/ ((p/sub i//sup 3/)/(1-p/sub i/)) and it is also generalized to the case when the X/sub i/ are general integer-valued random variables. Its proof is based on the derivation of a subadditivity property for a new discrete version of the Fisher information, and uses a recent logarithmic Sobolev inequality for the Poisson distribution.","Entropy,
Random variables,
Mathematics,
Computer science,
Laboratories,
Probability distribution,
Statistical distributions,
Topology"
"Scalable, memory efficient, high-speed IP lookup algorithms","One of the central issues in router performance is IP address lookup based on longest prefix matching. IP address lookup algorithms can be evaluated on a number of metrics-lookup time, update time, memory usage, and to a less important extent, the time to construct the data structure used to support lookups and updates. Many of the existing methods are geared toward optimizing a specific metric, and do not scale well with the ever expanding routing tables and the forthcoming IPv6 where the IP addresses are 128 bits long. In contrast, our effort is directed at simultaneously optimizing multiple metrics and provide solutions that scale to IPv6, with its longer addresses and much larger routing tables. In this paper, we present two IP address lookup schemes-Elevator-Stairs algorithm and logW-Elevators algorithm. For a routing table with N prefixes, The Elevator-Stairs algorithm uses optimal O(N) memory, and achieves better lookup and update times than other methods with similar memory requirements. The logW-Elevators algorithm gives O(logW) lookup time, where W is the length of an IP address, while improving upon update time and memory usage. Experimental results using the MAE-West router with 29 487 prefixes show that the Elevator-Stairs algorithm gives an average throughput of 15.7 Million lookups per second (Mlps) using 459KB of memory, and the logW-Elevators algorithm gives an average throughput of 21.41Mlps with a memory usage of 1259KB.",
Dynamic task scheduling using genetic algorithms for heterogeneous distributed computing,"An algorithm has been developed to dynamically schedule heterogeneous tasks on heterogeneous processors in a distributed system. The scheduler operates in an environment with dynamically changing resources and adapts to variable system resources. It operates in a batch fashion and utilises a genetic algorithm to minimise the total execution time. We have compared our scheduler to six other schedulers, three batch-mode and three immediate-mode schedulers. We have performed simulations with randomly generated task sets, using uniform, normal, and Poisson distributions, whilst varying the communication overheads between the clients and scheduler. We have achieved more efficient results than all other schedulers across a range of different scenarios while scheduling 10,000 tasks on up to 50 heterogeneous processors.",
Lifetime maximization for multicasting in energy-constrained wireless networks,"We consider the problem of maximizing the lifetime of a given multicast connection in a wireless network of energy-constrained (e.g., battery-operated) nodes, by choosing ideal transmission power levels for the nodes relaying the connection. We distinguish between two basic operating modes: In a static power assignment, the power levels of the nodes are set at the beginning and remain unchanged until the nodes are depleted of energy. In a dynamic power schedule, the powers can be adjusted during operation. We show that while lifetime-maximizing static power assignments can be found in polynomial time, for dynamic schedules the problem becomes NP-hard. We introduce two approximation heuristics for the dynamic case, and experimentally verify that the lifetime of a dynamically adjusted multicast connection can be made several times longer than what can be achieved by the best possible static assignment.","Intelligent networks,
Wireless networks,
Transceivers,
Computer science,
Dynamic scheduling,
Joining processes,
Energy consumption,
Broadcasting,
Relays,
Polynomials"
Decision strategies that maximize the area under the LROC curve,"For the 2-class detection problem (signal absent/present), the likelihood ratio is an ideal observer in that it minimizes Bayes risk for arbitrary costs and it maximizes the area under the receiver operating characteristic (ROC) curve [AUC]. The AUC-optimizing property makes it a valuable tool in imaging system optimization. If one considered a different task, namely, joint detection and localization of the signal, then it would be similarly valuable to have a decision strategy that optimized a relevant scalar figure of merit. We are interested in quantifying performance on decision tasks involving location uncertainty using the localization ROC (LROC) methodology. Therefore, we derive decision strategies that maximize the area under the LROC curve, A/sub LROC/. We show that these decision strategies minimize Bayes risk under certain reasonable cost constraints. The detection-localization task is modeled as a decision problem in three increasingly realistic ways. In the first two models, we treat location as a discrete parameter having finitely many values resulting in an (L+1) class classification problem. In our first simple model, we do not include search tolerance effects and in the second, more general, model, we do. In the third and most general model, we treat location as a continuous parameter and also include search tolerance effects. In all cases, the essential proof that the observer maximizes A/sub LROC/ is obtained with a modified version of the Neyman-Pearson lemma. A separate form of proof is used to show that in all three cases, the decision strategy minimizes the Bayes risk under certain reasonable cost constraints.","Costs,
Radiology,
Signal detection,
Uncertainty,
Biomedical imaging,
Biomedical image processing,
Risk analysis,
Performance analysis,
Measurement,
Optimization methods"
Implementation of a web-based educational tool for digital signal processing teaching using the technological acceptance model,"This paper presents an exploratory study about the improvement and validation of a Web-based educational tool. The tool, designed with Shockwave and Macromedia Director, is used as a teaching methodology in an undergraduate course using modern microprocessors, architectures, and applications. An information system theory, called the Technological Acceptance Model (TAM), has been applied to detect both the use of the tool and the external variables that have a significant influence over it. The obtained results illustrate the strengths and weaknesses to be reinforced and have been taken into account to implement the final version of the tool. The proposed method may be extended to similar tools and experiments to fill the lack of scientific studies in the validation and acceptance of computer-based educational tools.","Internet,
Computer aided instruction,
Signal processing,
Computer science education,
Electrical engineering education"
A high throughput string matching architecture for intrusion detection and prevention,"Network intrusion detection and prevention systems have emerged as one of the most effective ways of providing security to those connected to the network, and at the heart of almost every modern intrusion detection system is a string matching algorithm. String matching is one of the most critical elements because it allows for the system to make decisions based not just on the headers, but the actual content flowing through the network. Unfortunately, checking every byte of every packet to see if it matches one of a set of ten thousand strings becomes a computationally intensive task as network speeds grow into the tens, and eventually hundreds, of gigabits/second. To keep up with these speeds a specialized device is required, one that can maintain tight bounds on worst case performance, that can be updated with new rules without interrupting operation, and one that is efficient enough that it could be included on chip with existing network chips or even into wireless devices. We have developed an approach that relies on a special purpose architecture that executes novel string matching algorithms specially optimized for implementation in our design. We show how the problem can be solved by converting the large database of strings into many tiny state machines, each of which searches for a portion of the rules and a portion of the bits of each rule. Through the careful co-design and optimization of our architecture with a new string matching algorithm we show that it is possible to build a system that is 10 times more efficient than the currently best known approaches.",
Web service discovery based on behavior signatures,"Web service discovery is a key problem as the number of services is expected to increase dramatically. Service discovery at the present time is based primarily on keywords, or interfaces of Web services through the use of ontology. We argue that ""behavior signatures"" as operational level description should play an important role in the service discovery process. In this paper, we propose a new behavior model for Web services using automata and logic formalisms. Roughly, the model associates messages with activities and adopts the IOPR model in OWL-S to describe activities. A new query language is developed to express temporal and semantic properties on service behaviors. Query evaluation algorithms are developed; in particular, an optimization approach using RE-tree and heuristics is shown to improve the performance. Specifically, experimental results show that the use of RE-tree reduces query evaluation time by an order of magnitude and with heuristics it enhances the performance by two orders of magnitude. This is clearly an encouraging starting point.","Web services,
Computer science,
Query processing,
Credit cards,
Ontologies,
Automata,
Logic,
Database languages,
Books"
Deformation invariant image matching,"We propose a novel framework to build descriptors of local intensity that are invariant to general deformations. In this framework, an image is embedded as a 2D surface in 3D space, with intensity weighted relative to distance in x-y. We show that as this weight increases, geodesic distances on the embedded surface are less affected by image deformations. In the limit, distances are deformation invariant. We use geodesic sampling to get neighborhood samples for interest points, and then use a geodesic-intensity histogram (GIH) as a deformation invariant local descriptor. In addition to its invariance, the new descriptor automatically finds its support region. This means it can safely gather information from a large neighborhood to improve discriminability. Furthermore, we propose a matching method for this descriptor that is invariant to affine lighting changes. We have tested this new descriptor on interest point matching for two data sets, one with synthetic deformation and lighting change, and another with real non-affine deformations. Our method shows promising matching results compared to several other approaches","Image matching,
Image sampling,
Histograms,
Surface treatment,
Jacobian matrices,
Automation,
Computer science,
Educational institutions,
Testing,
Object recognition"
A robust visual tracking system for patient motion detection in SPECT: hardware solutions,"Our overall research goal is to devise a robust method of tracking and compensating patient motion by combining an emission data based approach with a visual tracking system (VTS) that provides an independent estimate of motion. Herein, we present the latest hardware configuration of the VTS, a test of the accuracy of motion tracking by it, and our solution for synchronization between the SPECT and the optical acquisitions. The current version of the VTS includes stereo imaging with sets of optical network cameras with attached light sources, a SPECT/VTS calibration phantom, a black stretchable garment with reflective spheres to track chest motion, and a computer to control the cameras. The computer also stores the JPEG files generated by the optical cameras with synchronization to the list-mode acquisition of events on our SPECT system. Five Axis PTZ 2130 network cameras (Axis Communications AB, Lund, Sweden) were used to track motion of spheres with a highly retroreflective coating using stereo methods. The calibration phantom is comprised of seven reflective spheres designed such that radioactivity can be added to the tip of the mounts holding the spheres. This phantom is used to determine the transformation to be applied to convert the motion detected by the VTS into the SPECT coordinates system. The ability of the VTS to track motion was assessed by comparing its results to those of the Polaris infrared tracking system (Northern Digital Inc., Waterloo, ON, Canada). The difference in the motions assessed by the two systems was generally less than 1 mm. Synchronization was assessed in two ways. First, optical cameras were aimed at a digital clock and the elapsed time estimated by the cameras was compared to the actual time shown by the clock in the images. Second, synchronization was also assessed by moving a radioactive and reflective sphere three times during concurrent VTS and SPECT acquisitions and comparing the time at which motion occurred in the optical and SPECT images. The results show that optical and SPECT images stay synchronized within a 150-ms range. The 100-Mbit network load is less than 10%, and the computer's CPU load is between 15% and 25%; thus, the VTS can be improved by adding more cameras or by increasing the image size and/or resolution while keeping an acquisition rate of 30 images per second per camera.","Robustness,
Tracking,
Motion detection,
Hardware,
Cameras,
Synchronization,
Imaging phantoms,
Calibration,
Computer networks,
Optical computing"
A game-theoretic and dynamical-systems analysis of selection methods in coevolution,"We use evolutionary game theory (EGT) to investigate the dynamics and equilibria of selection methods in coevolutionary algorithms. The canonical selection method used in EGT is equivalent to the standard ""fitness-proportional"" selection method used in evolutionary algorithms. All attractors of the EGT dynamic are Nash equilibria; we focus on simple symmetric variable-sum games that have polymorphic Nash-equilibrium attractors. Against the dynamics of proportional selection, we contrast the behaviors of truncation selection, (/spl mu/,/spl lambda/),(/spl mu/+/spl lambda/), linear ranking, Boltzmann, and tournament selection. Except for Boltzmann selection, each of the methods we test unconditionally fail to achieve polymorphic Nash equilibrium. Instead, we find point attractors that lack game-theoretic justification, cyclic dynamics, or chaos. Boltzmann selection converges onto polymorphic Nash equilibrium only when selection pressure is sufficiently low; otherwise, we obtain attracting limit-cycles or chaos. Coevolutionary algorithms are often used to search for solutions (e.g., Nash equilibria) of games of strategy; our results show that many selection methods are inappropriate for finding polymorphic Nash solutions to variable-sum games. Another application of coevolution is to model other systems; our results emphasize the degree to which the model's behavior is sensitive to implementation details regarding selection-details that we might not otherwise believe to be critical.","Algorithm design and analysis,
Game theory,
Computer science,
Evolutionary computation,
Nash equilibrium,
Chaos,
Stochastic processes,
Genetics,
Nonlinear dynamical systems,
Testing"
ASyMTRe: Automated Synthesis of Multi-Robot Task Solutions through Software Reconfiguration,"This paper describes a methodology for automat ically synthesizing task solutions for heterogeneous multi-robot teams. In contrast to prior approaches that require a manual pre definition of how the robot team will accomplish its task (while perhaps automating who performs which task), our approach automates both the how and the who to generate task solution approaches that were not explicitly defined by the designer a priori. The advantages of this new approach are that it: (1) enables the robot team to synthesize new task solutions that use fundamentally different combinations of robot behaviors for different team compositions, and (2) provides a general mechanism for sharing sensory information across networked robots, so that more capable robots can assist less capable robots in accomplishing their objectives. Our approach, which we call ASyMTRe (Automated Synthesis of Multi-robot Task solutions through software Reconfiguration, pronounced â€œAsymmetryâ€), is based on mapping environmental, perceptual, and motor control schemas to the required flow of information through the multi-robot system, automatically reconfiguring the connections of schemas within and across robots to synthesize valid and efficient multi-robot behaviors for accomplishing the team objectives. We validate this approach by presenting the results of applying our methodology to two different teaming scenarios: altruistic cooperation involving multi-robot transportation, and coalescent cooperation involving multi-robot box pushing.",
Applications of computers to dance,"As computer technology has developed and become less expensive, many artists have found ways to use it to enhance their performances with interactive multimedia. This has included incorporating computer-generated images and sound with live dance performance and using sensors that let the live dancers' movements control imagery, sound, and a wide variety of special effects. This overview describes some of the current applications of computer graphics to dance including visualizing choreography, composing, editing and animating dance notation, and enhancing live performance.",
Cloning by accident: an empirical study of source code cloning across software systems,"One of the key goals of open source development is the sharing of knowledge, experience, and solutions that pertain to a software system and its problem domain. Source code cloning is one way in which expertise can be reused across systems; cloning is known to have been used in several open source projects, such as the SCSI drivers of the Linux kernel. In this paper, we discuss two case studies in which we performed clone detection on several open source systems within the same domain. In the first case study we examined nine text editors written in C, and in the second study we examined eight X-Windows window managers written in C and C++. To our surprise, we found little evidence of ""true"" cloning activity, but we did notice a significant number of ""accidental"" clones - that is, code fragments that are similar due to the precise protocols they must use when interacting with a given API or set of libraries. We further discuss the nature of ""true"" versus ""accidental"" clones, as well as the details of our case studies.",
Assessment of vulnerable plaque composition by matching the deformation of a parametric plaque model to measured plaque deformation,"Intravascular ultrasound (IVUS) elastography visualizes local radial strain of arteries in so-called elastograms to detect rupture-prone plaques. However, due to the unknown arterial stress distribution these elastograms cannot be directly interpreted as a morphology and material composition image. To overcome this limitation we have developed a method that reconstructs a Young's modulus image from an elastogram. This method is especially suited for thin-cap fibroatheromas (TCFAs), i.e., plaques with a media region containing a lipid pool covered by a cap. Reconstruction is done by a minimization algorithm that matches the strain image output, calculated with a parametric finite element model (PFEM) representation of a TCFA, to an elastogram by iteratively updating the PFEM geometry and material parameters. These geometry parameters delineate the TCFA media, lipid pool and cap regions by circles. The material parameter for each region is a Young's modulus, E/sub M/, E/sub L/, and E/sub C/, respectively. The method was successfully tested on computer-simulated TCFAs (n=2), one defined by circles, the other by tracing TCFA histology, and additionally on a physical phantom (n=1) having a stiff wall (measured E/sub M/=16.8 kPa) with an eccentric soft region (measured E/sub L/=4.2 kPa). Finally, it was applied on human coronary plaques in vitro (n=1) and in vivo (n=1). The corresponding simulated and measured elastograms of these plaques showed radial strain values from 0% up to 2% at a pressure differential of 20, 20, 1, 20, and 1 mmHg respectively. The used/reconstructed Young's moduli [kPa] were for the circular plaque E/sub L/=50/66, E/sub M/=1500/1484, E/sub C/=2000/2047, for the traced plaque E/sub L/=25/1, E/sub M/=1000/1148, E/sub C/=1500/1491, for the phantom E/sub L/=4.2/4 kPa, E/sub M/=16.8/16, for the in vitro plaque E/sub L/=n.a./29, E/sub M/=n.a./647, E/sub C/=n.a./1784 kPa and for the in vivo plaque E/sub L/=n.a./2, E/sub M/=n.a./188, E/sub C/=n.a./188 kPa.",
Flexible hardware abstraction for wireless sensor networks,"We present a flexible hardware abstraction architecture (HAA) that balances conflicting requirements of wireless sensor networks (WSNs) applications and the desire for increased portability and streamlined development of applications. Our three-layer design gradually adapts the capabilities of the underlying hardware platforms to the selected platform-independent hardware interface between the operating system core and the application code. At the same time, it allows the applications to utilize a platform's full capabilities-exported at the second layer, when the performance requirements outweigh the need for cross-platform compatibility. We demonstrate the practical value of our approach by presenting how it can be applied to the most important hardware modules that are found in a typical WSN platform. We support our claims using concrete examples from existing hardware abstractions in TinyOS and our implementation of the MSP430 platform that follows the architecture proposed in this paper.",
Information structures to secure control of rigid formations with leader-follower architecture,"This paper is concerned with rigid formations of mobile autonomous agents that have leader-follower architecture. In a previous paper, Baillieul and Suri gave a proposition as a necessary condition for stable rigidity. They also gave a separate theorem as a sufficient condition for stable rigidity. This paper suggests an approach to analyze rigid formations that have leader-follower architecture. It proves that the third condition in the proposition given by Baillieul and Suri is redundant, and it proves that this proposition is a necessary and sufficient condition for stable rigidity. Simulation results are also presented to illustrate rigidity.",
"Architecture of a dual-modality, high-resolution, fully digital positron emission tomography/computed tomography (PET/CT) scanner for small animal imaging","Contemporary positron emission tomography (PET) scanners are commonly implemented with very large scale integration analog front-end electronics to reduce power consumption, space, noise, and cost. Analog processing yields excellent results in dedicated applications, but offers little flexibility for sophisticated signal processing or for more accurate measurements with newer, fast scintillation crystals. Design goals of the new Sherbrooke PET/computed tomography (CT) scanner are: 1) to achieve 1 mm resolution in both emission (PET) and transmission (CT) imaging using the same detector channels; 2) to be able to count and discriminate individual X-ray photons in CT mode. These requirements can be better met by sampling the analog signal from each individual detector channel as early as possible, using off-the-shelf, 8-b, 100-MHz, high-speed analog-to-digital converters (ADC) and digital processing in field programmable gate arrays (FPGAs). The core of the processing units consists of Xilinx SpartanIIe that can hold up to 16 individual channels. The initial architecture is designed for 1024 channels, but modularity allows extending the system up to 10 K channels or more. This parallel architecture supports count rates in excess of a million hits/s/scintillator in CT mode and up to 100 K events/s/scintillator in PET mode, with a coincidence time window of less than 10 ns full-width at half-maximum.","Positron emission tomography,
Computed tomography,
Computer architecture,
Animals,
High-resolution imaging,
Signal processing,
X-ray imaging,
Field programmable gate arrays,
Very large scale integration,
Energy consumption"
Flattening maps for the visualization of multibranched vessels,"We present two novel algorithms which produce flattened visualizations of branched physiological surfaces, such as vessels. The first approach is a conformal mapping algorithm based on the minimization of two Dirichlet functionals. From a triangulated representation of vessel surfaces, we show how the algorithm can be implemented using a finite element technique. The second method is an algorithm which adjusts the conformal mapping to produce a flattened representation of the original surface while preserving areas. This approach employs the theory of optimal mass transport. Furthermore, a new way of extracting center lines for vessel fly-throughs is provided.",
Layer-encoded video in scalable adaptive streaming,"Combining the concepts of caching and transmission control protocol (TCP)-friendly streaming of layer-encoded video bears the problem that those videos might not be cached in full quality. Therefore, we focus in this work on the scheduling of retransmissions of missing segments of a cached video in a manner that allows clients to receive the content in an improved quality. In a first step, we conducted subjective assessments of variations in layer-encoded video with the goal to validate existing quality metrics, including our own, which are based on certain assumptions. A statistical analysis of the subjective assessment validates these assumptions. We also show that the frequently used peak signal-to-noise ratio (PSNR) is not an appropriate metric for variations in layer-encoded video. With the insight from the subjective assessment we develop heuristics for retransmission scheduling and prove their applicability by conducting a series of simulations.","Streaming media,
Scalability,
Multimedia communication,
Information technology,
PSNR,
Control systems,
Computer science,
Web and internet services,
Protocols,
Statistical analysis"
Concise and consistent naming [software system identifier naming],"Approximately 70% of the source code of a software system consists of identifiers. Hence, the names chosen as identifiers are of paramount importance for the readability of computer programs and therewith their comprehensibility. However, virtually every programming language allows programmers to use almost arbitrary sequences of characters as identifiers which far too often results in more or less meaningless or even misleading naming. Coding style guides address this problem but are usually limited to general and hard to enforce rules like ""identifiers should be self-describing"". This paper renders adequate identifier naming far more precisely. A formal model, based on bijective mappings between concepts and names, provides a solid foundation for the definition of precise rules for concise and consistent naming. The enforcement of these rules is supported by a tool that incrementally builds and maintains a complete identifier dictionary while the system is being developed. The identifier dictionary explains the language used in the software system, aids in consistent naming, and improves productivity of programmers by proposing suitable names depending on the current context.",
The complexity of online memory checking,"We consider the problem of storing a large file on a remote and unreliable server. To verify that the file has not been corrupted, a user could store a small private (randomized) ""fingerprint"" on his own computer. This is the setting for the well-studied authentication problem in cryptography, and the required fingerprint size is well understood. We study the problem of sub-linear authentication: suppose the user would like to encode and store the file in a way that allows him to verify that it has not been corrupted, but without reading the entire file. If the user only wants to read t bits of the file, how large does the size s of the private fingerprint need to be? We define this problem formally, and show a tight lower bound on the relationship between s and t when the adversary is not computationally bounded, namely: s /spl times/ t = /spl Omega/(n), where n is the file size. This is an easier case of the online memory checking problem, introduced by Blum et al. in 1991, and hence the same (tight) lower bound applies also to that problem. It was previously shown that when the adversary is computationally bounded, under the assumption that one-way functions exist, it is possible to construct much better online memory checkers and sub-linear authentication schemes. We show that the existence of one-way functions is also a necessary condition: even slightly breaking the s /spl times/ t = /spl Omega/(n) lower bound in a computational setting implies the existence of one-way functions.","Authentication,
Fingerprint recognition,
Decoding,
Cryptography,
Encoding,
File servers,
Error correction,
Size measurement,
Information security,
Polynomials"
A comparative study of three moment-based shape descriptors,"Shape is one of the fundamental visual features in the content-based image retrieval (CBIR) paradigm. Numerous shape descriptors have been proposed in the literature. These can be broadly categorized as region-based and contour-based descriptors. Contour-based shape descriptors make use of only the boundary information, ignoring the shape interior content. Therefore, these descriptors cannot represent shapes for which the complete boundary information is not available. On the other hand, region-based descriptors exploit both boundary and internal pixels, and therefore are applicable to generic shapes. Among the region-based descriptors, moments have been very popular since they were first introduced in the 60's. In this paper, we study and compare three moment-based descriptors: Invariant moments, Zernike moments, and radial Chebyshev moments. Experiments on the MPEG-7 shape databases show that radial Chebyshev moments achieve the highest retrieval performance.","Chebyshev approximation,
Image retrieval,
Content based retrieval,
MPEG 7 Standard,
Image databases,
Spatial databases,
Information retrieval,
Shape measurement,
Computer science,
Visual databases"
Secure spread: an integrated architecture for secure group communication,"Group communication systems are high-availability distributed systems providing reliable and ordered message delivery, as well as a membership service, to group-oriented applications. Many such systems are built using a distributed client-server architecture where a relatively small set of servers provide service to numerous clients. In this work, we show how group communication systems can be enhanced with security services without sacrificing robustness and performance. More specifically, we propose several integrated security architectures for distributed client-server group communication systems. In an integrated architecture, security services are implemented in servers, in contrast to a layered architecture, where the same services are implemented in clients. We discuss performance and accompanying trust issues of each proposed architecture and present experimental results that demonstrate the superior scalability of an integrated architecture.","Communication system security,
Computer science,
Data security,
Computer architecture,
Computer Society,
Application software,
Robustness,
Scalability,
Business communication,
National security"
A broker-based framework for QoS-aware Web service composition,"Web services are modular web applications that can be independently deployed and invoked by other software or services on the web. This offers enterprises the capability to integrate in-house business services with external Web services to conduct complex business transactions. The integration efficiency and flexibility are critical for services composition. For Web services providing a similar functionality, Quality of Service (QoS) is the main factor to differentiate them. The overall QoS of a business process must meet a user's requirement. In this paper, we propose a broker-based framework to facilitate dynamic integration and adaptation of QoS-aware Web services with end-to-end QoS constraints. The key functions of a dynamic broker include service collection, selection, composition and adaptation. Our study considers both functional and QoS characteristics of Web services to identify the optimal business process solutions.","Web services,
Quality of service,
Business,
Computer science,
Application software,
System software,
Distributed computing,
Outsourcing,
Systems engineering and theory,
Protocols"
Discovering frequent episodes and learning hidden Markov models: a formal connection,"This paper establishes a formal connection between two common, but previously unconnected methods for analyzing data streams: discovering frequent episodes in a computer science framework and learning generative models in a statistics framework. We introduce a special class of discrete hidden Markov models (HMMs), called episode generating HMMs (EGHs), and associate each episode with a unique EGH. We prove that, given any two episodes, the EGH that is more likely to generate a given data sequence is the one associated with the more frequent episode. To be able to establish such a relationship, we define a new measure of frequency of an episode, based on what we call nonoverlapping occurrences of the episode in the data. An efficient algorithm is proposed for counting the frequencies for a set of episodes. Through extensive simulations, we show that our algorithm is both effective and more efficient than current methods for frequent episode discovery. We also show how the association between frequent episodes and EGHs can be exploited to assess the significance of frequent episodes discovered and illustrate empirically how this idea may be used to improve the efficiency of the frequent episode discovery.","Hidden Markov models,
Data mining,
Data analysis,
Computer science,
Frequency measurement,
Application software,
Statistical analysis,
Pattern analysis,
Time series analysis,
Stochastic processes"
jViz.Rna -a java tool for RNA secondary structure visualization,"Many tools have been developed for visualization of RNA secondary structures using a variety of techniques and output formats. However, each tool is typically limited to one or two of the visualization models discussed in this paper, supports only a single file format, and is tied to a specific platform. In order for structure prediction researchers to better understand the results of their algorithms and to enable life science researchers to interpret RNA structure easily, it is helpful to provide them with a flexible and powerful tool. jViz.Rna is a multiplatform visualization tool capable of displaying RNA secondary structures encoded in a variety of file formats. The same structure can be viewed using any of the models supported, including linked graph, circle graph, dot plot, and classical structure. Also, the output is dynamic and can easily be further manipulated by the user. In addition, any of the drawings produced can be saved in either the EPS or PNG file formats enabling easy usage in publications and presentations.","Java,
RNA,
Visualization,
Manipulator dynamics,
Computational biology,
Nuclear magnetic resonance,
Biology computing,
Libraries,
Biological system modeling,
Molecular biophysics"
Mercer kernels for object recognition with local features,A new class of kernels for object recognition based on local image feature representations are introduced in this paper. These kernels satisfy the Mercer condition and incorporate multiple types of local features and semilocal constraints between them. Experimental results of SVM classifiers coupled with the proposed kernels are reported on recognition tasks with the COIL-100 database and compared with existing methods. The proposed kernels achieved competitive performance and were robust to changes in object configurations and image degradations.,
ISEGEN: generation of high-quality instruction set extensions by iterative improvement,"Customization of processor architectures through instruction set extensions (ISEs) is an effective way to meet the growing performance demands of embedded applications. A high-quality ISE generation approach needs to obtain results close to those achieved by experienced designers, particularly for complex applications that exhibit regularity; expert designers are able to exploit manually such regularity in the data flow graphs to generate high-quality ISEs. We present ISEGEN, an approach that identifies high-quality ISEs by iterative improvement following the basic principles of the well-known Kernighan-Lin (K-L) min-cut heuristic. Experimental results on a number of MediaBench, EEMBC and cryptographic applications show that our approach matches the quality of the optimal solution obtained by exhaustive search. We also show that our ISEGEN technique is on average 20 times faster than a genetic formulation that generates equivalent solutions. Furthermore, the ISEs identified by our technique exhibit 35% more speedup than the genetic solution on a large cryptographic application (AES) by effectively exploiting its regular structure.","Cryptography,
Genetics,
Computer aided instruction,
Embedded computing,
High performance computing,
Application software,
Partitioning algorithms,
Computer science,
Computer architecture,
Flow graphs"
Differential Deserialization for Optimized SOAP Performance,"SOAP, a simple, robust, and extensible protocol for the exchange of messages, is the most widely used communication protocol in the Web services model. SOAPâ€™s XML-based message format hinders its performance, thus making it unsuitable in high-performance scientific applications. The deserialization of SOAP messages, which includes processing of XML data and conversion of strings to in-memory data types, is the major performance bottleneck in a SOAP message exchange. This paper presents and evaluates a new optimization technique for removing this bottleneck. This technique, called differential deserialization (DDS), exploits the similarities between incoming messages to reduce deserialization time. Differential deserialization is fully SOAPcompliant and requires no changes to a SOAP client. A performance study demonstrates that DDS can result in a significant performance improvement for some Web services.","Simple object access protocol,
Web services,
XML,
Grid computing,
Computer networks,
Permission,
Laboratories,
Computer science,
Robustness,
Computer languages"
A unified framework for monitoring data streams in real time,"Online monitoring of data streams poses a challenge in many data-centric applications, such as telecommunications networks, traffic management, trend-related analysis, Web-click streams, intrusion detection, and sensor networks. Mining techniques employed in these applications have to be efficient in terms of space usage and per-item processing time while providing a high quality of answers to (1) aggregate monitoring queries, such as finding surprising levels of a data stream, detecting bursts, and to (2) similarity queries, such as detecting correlations and finding interesting patterns. The most important aspect of these tasks is their need for flexible query lengths, i.e., it is difficult to set the appropriate lengths a priori. For example, bursts of events can occur at variable temporal modalities from hours to days to weeks. Correlated trends can occur at various temporal scales. The system has to discover ""interesting"" behavior online and monitor over flexible window sizes. In this paper, we propose a multi-resolution indexing scheme, which handles variable length queries efficiently. We demonstrate the effectiveness of our framework over existing techniques through an extensive set of experiments.",
A Benchmark Suite for SOAP-based Communication in Grid Web Services,"The convergence of Web services and grid computing has promoted SOAP, a widely used Web services protocol, into a prominent protocol for a wide variety of grid applications. These applications differ widely in the characteristics of their respective SOAP messages, and also in their performance requirements. To make the right decisions, an application developer must thus understand the complex dependencies between the SOAP implementation and the application. We propose a standard benchmark suite for quantifying, comparing, and contrasting the performance of SOAP implementations under a wide range of representative use cases. The benchmarks are defined by a set of WSDL documents. To demonstrate the utility of the benchmarks and to provide a snapshot of the current SOAP implementation landscape, we report the performance of many different SOAP implementations (gSOAP, AxisJava, XSUL and bSOAP) on the benchmarks, and draw conclusions about their current performance characteristics.",
Detection of concentric circles for camera calibration,"The geometry of plane-based calibration methods is well understood, but some user interaction is often needed in practice for feature detection. This paper presents a fully automatic calibration system that uses patterns of pairs of concentric circles. The key observation is to introduce a geometric method that constructs a sequence of points strictly convergent to the image of the circle center from an arbitrary point. The method automatically detects the points of the pattern features by the construction method, and identify them by invariants. It then takes advantage of homological constraints to consistently and optimally estimate the features in the image. The experiments demonstrate the robustness and the accuracy of the new method.","Cameras,
Calibration,
Computer vision,
Robustness,
Image edge detection,
Detection algorithms,
Computer science,
Computational geometry,
Image converters,
Convergence"
A generalized skew information and uncertainty relation,"A generalized skew information is defined and a generalized uncertainty relation is established with the help of a trace inequality which was recently proven by Fujii. In addition, we prove the trace inequality conjectured by Luo and Zhang. Finally, we point out that Theorem 1 in S. Luo and Q. Zhang, IEEE Trans. Inf. Theory, vol. 50, pp. 1778-1782, no. 8, Aug. 2004 is incorrect in general, by giving a simple counter-example.","Uncertainty,
Linear matrix inequalities,
Entropy,
Calculus,
Computer science,
Cities and towns,
Quantum mechanics,
Information theory,
Hilbert space"
Criteria analysis and validation of the reliability of Web services-oriented systems,"As Web services become more prevalent, the need to ensure their quality increases. This paper explores the criteria of reliability of Web services-oriented systems, and discusses how to design and generate test cases to conduct tests over Web services. A prototype system is constructed to test the effectiveness and efficiency of our algorithms. The preliminary results show that our approach facilitates the testing of services-oriented systems.",
ECHOS - enhanced capacity 802.11 hotspots,"The total number of hotspot users around the world is expected to grow from 9.3 million at the end of 2003 to 30 million at the end of 2004 according to researcher Gartner. Given the explosive growth in hotspot wireless usage, enhancing capacity of 802.11-based hot-spot wireless networks is an important problem. In this paper, we make two important contributions. We first present the AP-CST algorithm that dynamically adjusts the carrier sense threshold (CST) in order to allow more flows to coexist in current 802.11 architectures. We then extend the current hotspot engineering paradigm by allowing every cell and AP access to all available channels. These cells are then managed by the RNC-SC algorithm running in a centralized radio network controller. This algorithm assigns mobile stations to appropriate cells/channels and adjusts transmit power values dynamically, thereby exploiting spatial heterogeneity in distribution of users at the hotspots. Through detailed and extensive simulations, we show that the performance of 802.11-based hotspots can be improved by up to 195% per-cell and 70% overall.","Hardware,
Explosives,
Wireless networks,
Computer science,
Educational institutions,
Wireless sensor networks,
Heuristic algorithms,
Power engineering and energy,
Centralized control,
Delay"
A novel scheme for material updating in source distribution optimization of magnetic devices using sensitivity analysis,"A novel material updating scheme, which does not require intermediate states of a material used, is presented for source distribution optimization problems. A mutation factor to determine a degree of topological change in the next design stage on the basis of a current layout accelerates the convergence of an objective function. Easy implementation and fast convergence of the scheme are verified using two MRI design problems where current and permanent magnet distributions have been optimized, respectively.",
An overview of the competitive and adversarial approaches to designing dynamic power management strategies,"Dynamic power management (DPM) refers to the problem of judicious application of various low-power techniques based on runtime conditions in an embedded system to minimize the total energy consumption. To be effective, often such decisions take into account the operating conditions and the system-level design goals. DPM has been a subject of intense research in the past decade driven by the need for low power consumption in modern embedded devices. We present a comprehensive overview of two closely related approaches to designing DPM strategies, namely, competitive analysis approach and model checking approach based on adversarial modeling. Although many other approaches exist for solving the system-level DPM problem, these two approaches are closely related and are based on a common theme. This commonality is in the fact that the underlying model is that of a competition between the system and an adversary. The environment that puts service demands on devices is viewed as an adversary, or to be in competition with the system to make it burn more energy, and the DPM strategy is employed by the system to counter that.","Energy management,
Power system management,
Energy consumption,
Power system modeling,
Algorithm design and analysis,
Stochastic processes,
Embedded computing,
Computer science,
Operating systems,
Runtime"
Pruning training sets for learning of object categories,"Training datasets for learning of object categories are often contaminated or imperfect. We explore an approach to automatically identify examples that are noisy or troublesome for learning and exclude them from the training set. The problem is relevant to learning in semi-supervised or unsupervised setting, as well as to learning when the training data is contaminated with wrongly labeled examples or when correctly labeled, but hard to learn examples, are present. We propose a fully automatic mechanism for noise cleaning, called 'data pruning' and demonstrate its success on learning of human faces. It is not assumed that the data or the noise can be modeled or that additional training examples are available. Our experiments show that data pruning can improve on generalization performance for algorithms with various robustness to noise. It outperforms methods with regularization properties and is superior to commonly applied aggregation methods, such as bagging.","Training data,
Noise robustness,
Computer vision,
Application software,
Cleaning,
Humans,
Labeling,
Data mining,
Computer science,
Face"
The essence of P2P: a reference architecture for overlay networks,"The success of the P2P idea has created a huge diversity of approaches, among which overlay networks, for example, Gnutella, Kazaa, Chord, Pastry, Tapestry, P-Grid, or DKS, have received specific attention from both developers and researchers. A wide variety of algorithms, data structures, and architectures have been proposed. The terminologies and abstractions used, however, have become quite inconsistent since the P2P paradigm has attracted people from many different communities, e.g., networking, databases, distributed systems, graph theory, complexity theory, biology, etc. In this paper we propose a reference model for overlay networks which is capable of modeling different approaches in this domain in a generic manner. It is intended to allow researchers and users to assess the properties of concrete systems, to establish a common vocabulary for scientific discussion, to facilitate the qualitative comparison of the systems, and to serve as the basis for defining a standardized API to make overlay networks interoperable.",
"Algorithmic graph minor theory: Decomposition, approximation, and coloring","At the core of the seminal graph minor theory of Robertson and Seymour is a powerful structural theorem capturing the structure of graphs excluding a fixed minor. This result is used throughout graph theory and graph algorithms, but is existential. We develop a polynomial-time algorithm using topological graph theory to decompose a graph into the structure guaranteed by the theorem: a clique-sum of pieces almost-embeddable into bounded-genus surfaces. This result has many applications. In particular we show applications to developing many approximation algorithms, including a 2-approximation to graph coloring, constant-factor approximations to treewidth and the largest grid minor, combinatorial polylogarithmic approximation to half-integral multicommodity flow, subexponential fixed-parameter algorithms, and PTASs for many minimization and maximization problems, on graphs excluding a fixed minor.",
Graphical passwords: a survey,"The most common computer authentication method is to use alphanumerical usernames and passwords. This method has been shown to have significant drawbacks. For example, users tend to pick passwords that can be easily guessed. On the other hand, if a password is hard to guess, then it is often hard to remember. To address this problem, some researchers have developed authentication methods that use pictures as passwords. In this paper, we conduct a comprehensive survey of the existing graphical password techniques. We classify these techniques into two categories: recognition-based and recall-based approaches. We discuss the strengths and limitations of each method and point out the future research directions in this area. We also try to answer two important questions: ""Are graphical passwords as secure as text-based passwords?""; ""What are the major design and implementation issues for graphical passwords?"" This survey will be useful for information security researchers and practitioners who are interested in finding an alternative to text-based authentication methods",
Multiobjective GA optimization using reduced models,"In this paper, we propose a novel method for solving multiobjective optimization problems using reduced models. Our method, called objective exchange genetic algorithm for design optimization (OEGADO), is intended for solving real-world application problems. For such problems, the number of objective evaluations performed is a critical factor as a single objective evaluation can be quite expensive. The aim of our research is to reduce the number of objective evaluations needed to find a well-distributed sampling of the Pareto-optimal region by applying reduced models to steady-state multiobjective GAs. OEGADO runs several GAs concurrently with each GA optimizing one objective and forming a reduced model of its objective. At regular intervals, each GA exchanges its reduced model with the others. The GAs use these reduced models to bias their search toward compromise solutions. Empirical results in several engineering and benchmark domains comparing OEGADO with two state-of-the-art multiobjective evolutionary algorithms show that OEGADO outperformed them for difficult problems.","Design optimization,
Performance evaluation,
Design engineering,
Genetic algorithms,
Steady-state,
Computer science,
Genetic engineering,
Sampling methods,
Evolutionary computation,
Application software"
Mean-payoff parity games,"Games played on graphs may have qualitative objectives, such as the satisfaction of an /spl omega/-regular property, or quantitative objectives, such as the optimization of a real-valued reward. When games are used to model reactive systems with both fairness assumptions and quantitative (e.g., resource) constraints, then the corresponding objective combines both a qualitative and a quantitative component. In a general case of interest, the qualitative component is a parity condition and the quantitative component is a mean-payoff reward. We study and solve such mean-payoff parity games. We also prove some interesting facts about mean-payoff parity games which distinguish them both from mean-payoff and from parity games. In particular, we show that optimal strategies exist in mean-payoff parity games, but they may require infinite memory.",
On the optimization of heterogeneous MDDs,"This paper proposes minimization algorithms for the memory size and the average path length (APL) of heterogeneous multivalued decision diagrams (MDDs). In a heterogeneous MDD, each multivalued variable can take different domains. To represent a binary logic function using a heterogeneous MDD, we partition the binary variables into groups with different numbers of binary variables and treat the groups as multivalued variables. Since memory size and APL of a heterogeneous MDD depend on the partition of binary variables as well as the ordering of binary variables, the memory size and the APL of a heterogeneous MDD can be minimized by considering both orderings and partitions of binary variables. The experimental results show that heterogeneous MDDs can represent logic functions with smaller memory sizes than free binary decision diagrams (FBDDs) and smaller APLs than reduced ordered BDDs (ROBDDs); the APLs of heterogeneous MDDs can be reduced by half of the ROBDDs without increasing memory size; and heterogeneous MDDs have smaller area-time complexities than MDD(k)s.",
Classification of contour shapes using class segment sets,"Both example-based and model-based approaches for classifying contour shapes can encounter difficulties when dealing with classes that have large nonlinear variability, especially when the variability is structural or due to articulation. This paper proposes a part-based approach to address this problem. Bayesian classification is performed within a three-level framework, which consists of models for contour segments, for classes, and for the entire database of training examples. The class model enables different parts of different exemplars of a class to contribute to the recognition of an input shape. The method is robust to occlusion and is invariant to planar rotation, translation, and scaling. Furthermore, the method is completely automated. It achieves 98% classification accuracy on a large database with many classes.",
Three-dimensional nth derivative of Gaussian separable steerable filters,"This paper details the construction of three-dimensional separable steerable filters. The approach presented is an extension of the construction of two-dimensional separable steerable filters outlined in W.T. Freeman and E.H. Adelson (1991). Additionally, three-dimensional separable steerable filters, both continuous and discrete versions, for the second derivative of the Gaussian and its Hilbert transform are reported. Experimental evaluation demonstrates that the errors in the constructed separable filters are negligible.","Nonlinear filters,
Polynomials,
Discrete transforms,
Spatiotemporal phenomena,
Image analysis,
Pattern analysis,
Jacobian matrices,
Computer science,
Image processing,
Motion detection"
Wearable hand activity recognition for event summarization,"In this paper we develop a first step towards the recognition of hand activity by detecting objects subject to manipulation, and use the results to build a visual summary of events. The motivation is to extract information from hand activity without requiring that the wearer is explicit as in gesture-based interaction. Our method uses simple image measurements within a probabilistic framework and allows real-time implementation.",
RoboCart: toward robot-assisted navigation of grocery stores by the visually impaired,"This paper presents RoboCart, a proof-of-concept prototype of a robotic shopping assistant for the visually impaired. The purpose of RoboCart is to help visually impaired customers navigate a typical grocery store and carry purchased items. The current hardware and software components of the system are presented. For localization, RoboCart relies on RFID tags deployed at various locations in the store. For navigation, RoboCart relies on laser range finding. Experiences with deploying RoboCart in a real grocery store are described. The current status of the system and its limitations are outlined.","Navigation,
Computer science,
Laboratories,
Hardware,
Dogs,
Indoor environments,
Robot sensing systems,
Paper technology,
Software prototyping,
Prototypes"
Lifetime modeling of a sensor network,"We provide a mathematical analysis for the lifetime of a sensor network, when data-generation at individual sensor nodes is a random process. We show that the mathematical results for expected lifetime and its probability distribution closely validate the simulations results, both in linear and planar networks.",
Discovering coherent biclusters from gene expression data using zero-suppressed binary decision diagrams,"The biclustering method can be a very useful analysis tool when some genes have multiple functions and experimental conditions are diverse in gene expression measurement. This is because the biclustering approach, in contrast to the conventional clustering techniques, focuses on finding a subset of the genes and a subset of the experimental conditions that together exhibit coherent behavior. However, the biclustering problem is inherently intractable, and it is often computationally costly to find biclusters with high levels of coherence. In this work, we propose a novel biclustering algorithm that exploits the zero-suppressed binary decision diagrams (ZBDDs) data structure to cope with the computational challenges. Our method can find all biclusters that satisfy specific input conditions, and it is scalable to practical gene expression data. We also present experimental results confirming the effectiveness of our approach.",
The magic volume lens: an interactive focus+context technique for volume rendering,"The size and resolution of volume datasets in science and medicine are increasing at a rate much greater than the resolution of the screens used to view them. This limits the amount of data that can be viewed simultaneously, potentially leading to a loss of overall context of the data when the user views or zooms into a particular area of interest. We propose a focus+context framework that uses various standard and advanced magnification lens rendering techniques to magnify the features of interest, while compressing the remaining volume regions without clipping them away completely. Some of these lenses can be interactively configured by the user to specify the desired magnification patterns, while others are feature-adaptive. All our lenses are accelerated on the GPU. They allow the user to interactively manage the available screen area, dedicating more area to the more resolution-important features.","Lenses,
Focusing,
Computer graphics,
Humans,
Image generation,
Computer displays,
Large screen displays,
Rendering (computer graphics),
Retina,
Navigation"
Sensor networks deployment using flip-based sensors,"In this paper, we study the issue of mobility based sensor networks deployment. The distinguishing feature of our work is that the sensors in our model have limited mobilities. More specifically, the mobility in the sensors we consider is restricted to a flip, where the distance of the flip is bounded. Given an initial deployment of sensors in a field, our problem is to determine a movement plan for the sensors in order to maximize the sensor network coverage, and minimize the number of flips. We propose a minimum-cost maximum-flow based solution to this problem. We prove that our solution optimizes both the coverage and the number of flips. We also study the sensitivity of coverage and the number of flips to flip distance under different initial deployment distributions of sensors. We observe that increased flip distance achieves better coverage, and reduces the number of flips required per unit increase in coverage. However, such improvements are constrained by initial deployment distributions of sensors, due to the limitations on sensor mobility","Intelligent sensors,
Computer science,
Costs,
Load management,
Landmine detection,
Fuel storage,
Sparks,
Propulsion,
Propellers,
Energy consumption"
The Olin curriculum: thinking toward the future,"In 1997, the F. W. Olin Foundation of New York established the Franklin W. Olin College of Engineering, Needham, MA, with the mission of creating an engineering school for the 21st century. Over the last five years, the college has transformed from an idea to a functioning entity that admitted its first freshman class in fall 2002. This paper describes the broad outlines of the Olin curriculum with some emphasis on the electrical and computer engineering degree. The curriculum incorporates the best practices from many other institutions as well as new ideas and approaches in an attempt to address the future of engineering education.","Electrical engineering education,
Computer science education"
Efficient Coding of Time-Relative Structure Using Spikes,"Nonstationary acoustic features provide essential cues for many auditory tasks, including sound localization, auditory stream analysis, and speech recognition. These features can best be characterized relative to a precise point in time, such as the onset of a sound or the beginning of a harmonic periodicity. Extracting these types of features is a difficult problem. Part of the difficulty is that with standard block-based signal analysis methods, the representation is sensitive to the arbitrary alignment of the blocks with respect to the signal. Convolutional techniques such as shift-invariant transformations can reduce this sensitivity, but these do not yield a code that is efficient, that is, one that forms a nonredundant representation of the underlying structure. Here, we develop a non-block-based method for signal representation that is both time relative and efficient. Signals are represented using a linear superposition of time-shiftable kernel functions, each with an associated magnitude and temporal position. Signal decomposition in this method is a non-linear process that consists of optimizing the kernel function scaling coefficients and temporal positions to form an efficient, shift-invariant representation. We demonstrate the properties of this representation for the purpose of characterizing structure in various types of nonstationary acoustic signals. The computational problem investigated here has direct relevance to the neural coding at the auditory nerve and the more general issue of how to encode complex, time-varying signals with a population of spiking neurons.",
A Practical Study of Transitory Master Key Establishment ForWireless Sensor Networks,"Establishing secure links between pairs of directly connected sensor nodes is an important primitive for building secure wireless sensor networks. This paper systematically identifies two important security requirements of pairwise key setup in wireless sensor networks, namely opaqueness and inoculation. Transitory master key schemes, such as the LEAP protocol, can satisfy both requirements if the master key has not been compromised. However, if the master key is compromised, every key in the network is exposed to an adversary. To prevent the master key from becoming a single point failure of the whole system, we propose a new opaque transitory master key (OTMK) scheme for pairwise key setup in sensor networks. In OTMK, even if the master key is compromised, an adversary can only exploit a small number of keys nearby the compromised node, while other keys in the network remain safe. To further investigate key establishment schemes, we experimented with a way to compromise a sensor node, and tested our key establishment time in a real sensor network environment.",
Edge-based rich representation for vehicle classification,"In this paper, we propose an approach to vehicle classification under a mid-field surveillance framework. We develop a repeatable and discriminative feature based on edge points and modified SIFT descriptors, and introduce a rich representation for object classes. Experimental results show the proposed approach is promising for vehicle classification in surveillance videos despite great challenges such as limited image size and quality and large intra-class variations. Comparisons demonstrate the proposed approach outperforms other methods","Vehicles,
Object recognition,
Video surveillance,
Cameras,
Monitoring,
Protection,
Image recognition,
Error analysis,
Computer science,
Artificial intelligence"
Uniform distributed synthesis,"We provide a uniform solution to the problem of synthesizing a finite-state distributed system. An instance of the synthesis problem consists of a system architecture and a temporal specification. The architecture is given as a directed graph, where the nodes represent processes (including the environment as a special process) that communicate synchronously through shared variables attached to the edges. The same variable may occur on multiple outgoing edges of a single node, allowing for the broadcast of data. A solution to the synthesis problem is a collection of finite-state programs for the processes in the architecture, such that the joint behavior of the programs satisfies the specification in an unrestricted environment. We define information forks, a comprehensive criterion that characterizes all architectures with an undecidable synthesis problem. The criterion is effective: for a given architecture with n processes and v variables, it can be determined in O(n/sup 2//spl middot/v) time whether the synthesis problem is decidable. We give a uniform synthesis algorithm for all decidable cases. Our algorithm works for all /spl omega/-regular tree specification languages, including the /spl mu/-calculus. The undecidability proof, on the other hand, uses only LTL or, alternatively, CTL as the specification language. Our results therefore hold for the entire range of specification languages from LTL/CTL to the /spl mu/-calculus.",
Tracking multiple mouse contours (without too many samples),"We present a particle filtering algorithm for robustly tracking the contours of multiple deformable objects through severe occlusions. Our algorithm combines a multiple blob tracker with a contour tracker in a manner that keeps the required number of samples small. This is a natural combination because both algorithms have complementary strengths. The multiple blob tracker uses a natural multi-target model and searches a smaller and simpler space. On the other hand, contour tracking gives more fine-tuned results and relies on cues that are available during severe occlusions. Our choice of combination of these two algorithms accentuates the advantages of each. We demonstrate good performance on challenging video of three identical mice that contains multiple instances of severe occlusion.","Mice,
Computer vision,
Video sequences,
Computer science,
Filtering algorithms,
Robustness,
Particle tracking,
Biomedical monitoring,
Computerized monitoring,
Animals"
A reputation and trust management broker framework for Web applications,"This paper presents a distributed reputation and trust management framework that addresses the challenges of eliciting, evaluating and propagating reputation for Web applications. We propose a broker framework where every service user is associated with a broker who may represent multiple users. A broker collects for its users the distributed reputation ratings about any Web service. In return, a user provides its broker the transaction rating after every transaction with any service in order to build up the reputation database on all services. In addition, brokers form a trust network where they exchange and collect reputation data about services. By delegating trust management to brokers, individual users only need to ask their brokers about the reputation of a service before any transaction with a server. The only overhead for a user is the responsibility to share the reputation feedback with its broker. We present the distributed reputation and trust management framework and show the performance of the system by simulations.","Application software,
Feedback,
Engineering management,
Computer science,
Qualifications,
Web services,
Transaction databases,
Network servers,
Certification,
Cryptography"
On the Possibility of Consensus in Asynchronous Systems with Finite Average Response Times,"It has long been known that the consensus problem cannot be solved deterministically in completely asynchronous distributed systems, i.e., systems (1) without assumptions on communication delays and relative speed of processes and (2) without access to real-time clocks. In this paper, we define a new asynchronous system model. Instead of assuming reliable channels with finite transmission delays, stubborn channels with a finite average response time was assumed (if neither the sender nor the receiver crashes), and it is assumed that there exists some unknown physical bound on how fast an integer can be incremented. Note that there is no limit on how slow a program can be executed or how fast other statements can be executed. Also, there exists no upper or lower bound on the transmission delay of messages or the relative speed of processes. The are no additional assumptions about clocks, failure detectors, etc. that would aid in solving consensus either. It is shown that consensus can nevertheless be solved deterministically in this asynchronous system model",
On constructing k-connected k-dominating set in wireless networks,"An important problem in wireless networks, such as wireless ad hoc and sensor networks, is to select a few nodes to form a virtual backbone that supports routing and other tasks such as area monitoring. Previous work in this area has focused on selecting a small virtual backbone for high efficiency. We propose to construct a k-connected k-dominating set (k-CDS) as a backbone to balance efficiency and fault tolerance. Three localized k-CDS construction protocols are proposed. The first protocol randomly selects virtual backbone nodes with a given probability p/sub k/, where p/sub k/ depends on network condition and the value of k. The second protocol is a deterministic approach. It extends Wu and Dai's coverage condition, which is originally designed for 1-CDS construction, to ensure the formation of a k-CDS. The last protocol is a hybrid of probabilistic and deterministic approaches. It provides a generic framework that can convert many existing CDS algorithms into k-CDS algorithms. These protocols are evaluated via a simulation study.",
Hands-off assistive robotics for post-stroke arm rehabilitation,"This paper describes an autonomous assistive mobile robot that aids stroke patient rehabilitation by providing monitoring, encouragement, and reminders. The robot navigates autonomously, monitors the patient's arm activity, and helps the patient remember to follow a rehabilitation program. Our experiments show that patients post-stroke are positive about this approach and that increasingly active and animated robot behavior is positively received by stroke survivors.",
The statistical strength of nonlocality proofs,"There exist numerous proofs of Bell's theorem, stating that quantum mechanics is incompatible with local realistic theories of nature. Here the strength of such nonlocality proofs is defined in terms of the amount of evidence against local realism provided by the corresponding experiments. Statistical considerations show that the amount of evidence should be measured by the Kullback-Leibler (KL) or relative entropy divergence. The statistical strength of the following proofs is determined: Bell's original proof and Peres' optimized variant of it, and proofs by Clauser, Horne, Shimony, and Holt (CHSH), Hardy, Mermin, and Greenberger, Horne, and Zeilinger (GHZ). The GHZ proof is at least four and a half times stronger than all other proofs, while of the two-party proofs, the one of CHSH is the strongest.","Quantum mechanics,
Computer science,
Information theory,
Statistics,
Entropy,
Physics,
Mathematics,
Probability,
Statistical analysis"
Global software development at Siemens: experience from nine projects,"We report on the experiences of Siemens Corporation in nine globally-distributed software development projects. These projects represent a range of collaboration models, from co-development to outsourcing of components to outsourcing the software for an entire project. We report experience and lessons in issues of project management, division of labor, ongoing coordination of technical work, and communication. We include lessons learned, and conclude the paper with suggestions about important open research issues in this area.","Programming,
Project management,
Outsourcing,
Collaborative software,
Permission,
Computer science,
Collaborative work,
Productivity,
Costs,
Supply chain management"
Defending against Internet worms: a signature-based approach,"With the capability of infecting hundreds of thousands of hosts, worms represent a major threat to the Internet. The defense against Internet worms is largely an open problem. This paper investigates two important problems. Can a localized defense system detect new worms that were not seen before and moreover, capture the attack packets? How to identify polymorphic worms from the normal background traffic? We have two major contributions here. The first contribution is the design of a novel double-honeypot system, which is able to automatically detect new worms and isolate the attack traffic. The second contribution is the proposal of a new type of position-aware distribution signatures (PADS), which fit in the gap between the traditional signatures and the anomaly-based systems. We propose two algorithms based on expectation-maximization (EM) and Gibbs sampling for efficient computation of PADS from polymorphic worm samples. The new signature is capable of handling certain polymorphic worms. Our experiments show that the algorithms accurately separate new variants of the MSBlaster worm from the normal-traffic background.","Internet,
Computer worms,
Intrusion detection,
Atherosclerosis,
Information science,
Proposals,
Sampling methods,
Humans,
IP networks,
Protection"
Swarms for chemical plume tracing,"This paper presents a physics-based framework for managing distributed sensor networks of autonomous vehicles, e.g., robots, which self-organize into structured lattice arrangements using only local information. The vehicles remain in formation during obstacle avoidance and search for a chemical emitter that is actively ejecting a toxic chemical into the air. We discuss a new plume tracing algorithm, based on the principles of fluid physics, that outperforms the leading biomimetic competitors for this task.",
An adaptive FEC code control algorithm for mobile wireless sensor networks,"For better performance over a noisy channel, mobile wireless networks transmit packets with forward error correction (FEC) code to recover corrupt bits without retransmission. The static determination of the FEC code size, however, degrades their performance since the evaluation of the underlying channel state is hardly accurate and even widely varied. Our measurements over a wireless sensor network, for example, show that the average bit error rate (BER) per second or per minute continuously changes from 0 up to 10âˆ’3. Under this environment, wireless networks waste their bandwidth since they can't deterministically select the appropriate size of FEC code matching to the fluctuating channel BER. This paper proposes an adaptive FEC technique called adaptive FEC code control (AFECCC), which dynamically tunes the amount of FEC code per packet based on the arrival of acknowledgement packets without any specific information such as signal to noise ratio (SNR) or BER from receivers. Our simulation experiments indicate that AFECCC performs better than any static FEC algorithm and some conventional dynamic hybrid FEC/ARQ algorithms when wireless channels are modeled with two-state Markov chain, chaotic map, and traces collected from real sensor networks. Finally, AFECCC implemented in sensor motes achieves better performance than any static FEC algorithm.",
What Can a Neuron Learn with Spike-Timing-Dependent Plasticity?,"Spiking neurons are very flexible computational modules, which can implement with different values of their adjustable synaptic parameters an enormous variety of different transformations F from input spike trains to output spike trains. We examine in this letter the question to what extent a spiking neuron with biologically realistic models for dynamic synapses can be taught via spike-timing-dependent plasticity (STDP) to implement a given transformation F. We consider a supervised learning paradigm where during training, the output of the neuron is clamped to the target signal (teacher forcing). The well-known perceptron convergence theorem asserts the convergence of a simple supervised learning algorithm for drastically simplified neuron models (McCulloch-Pitts neurons). We show that in contrast to the perceptron convergence theorem, no theoretical guarantee can be given for the convergence of STDP with teacher forcing that holds for arbitrary input spike patterns. On the other hand, we prove that average case versions of the perceptron convergence theorem hold for STDP in the case of uncorrelated and correlated Poisson input spike trains and simple models for spiking neurons. For a wide class of cross-correlation functions of the input spike trains, the resulting necessary and sufficient condition can be formulated in terms of linear separability, analogously as the well-known condition of learnability by perceptrons. However, the linear separability criterion has to be applied here to the columns of the correlation matrix of the Poisson input. We demonstrate through extensive computer simulations that the theoretically predicted convergence of STDP with teacher forcing also holds for more realistic models for neurons, dynamic synapses, and more general input distributions. In addition, we show through computer simulations that these positive learning results hold not only for the common interpretation of STDP, where STDP changes the weights of synapses, but also for a more realistic interpretation suggested by experimental data where STDP modulates the initial release probability of dynamic synapses.",
Detecting malicious JavaScript code in Mozilla,"The JavaScript language is used to enhance the client-side display of web pages. JavaScript code is downloaded into browsers and executed on-the-fly by an embedded interpreter. Browsers provide sand-boxing mechanisms to prevent JavaScript code from compromising the security of the client's environment, but, unfortunately, a number of attacks exist that can be used to steal users' credentials (e.g., cross-site scripting attacks) and lure users into providing sensitive information to unauthorized parties (e.g., phishing attacks). We propose an approach to solve this problem that is based on monitoring JavaScript code execution and comparing the execution to high-level policies, to detect malicious code behavior. To achieve this goal it is necessary to provide a mechanism to audit the execution of JavaScript code. This is a difficult task, because of the close integration of JavaScript with complex browser applications, such as Mozilla. This paper presents the first existing implementation of an auditing system for JavaScript interpreters and discusses the pitfalls and lessons learned in developing the auditing mechanism.","Java,
Application software,
Information security,
Web server,
Computer science,
Computer displays,
Web pages,
Monitoring,
Mobile computing,
Computer networks"
A power estimation methodology for systemC transaction level models,"Majority of existing works on system level power estimation have focused on the processor, while there are very few that address power consumption of peripherals in a SoC. With the presence of complex cores in current day embedded system-on-chip devices, the problem of complete system level power estimation is gaining significance. Transaction level models for SoCs are gaining increasing attention with emerging architectural modeling standards like SystemC. In this paper we present a methodology for performing system power estimation for different scenarios or applications being executed on these transaction level models. We describe techniques and a setup for transaction level power characterization, and an approach to augment SystemC transaction level models to perform transaction level power estimation. We also present experimental results to validate the accuracy and speed of our approach.","Power system modeling,
Integrated circuit modeling,
Energy consumption,
Frequency,
Bandwidth,
Embedded software,
Computer architecture,
State estimation,
Computer science,
System-on-a-chip"
Peer-to-peer discovery of computational resources for Grid applications,"Grid applications need to discover computational resources quickly, efficiently and scalably, but most importantly in an expressive manner. An expressive query may specify a variety of required metrics for the job, e.g., the number of hosts required, the amount of free CPU required on these hosts, and the minimum amount of RAM required on these hosts, etc. We present a peer-to-peer (P2P) solution to this problem, using structured naming to enable both (1) publishing of information about available computational resources, as well as (2) expressive and efficient querying of such resources. Extensive traces collected from hosts within the Computer Science department at UIUC are used to evaluate our proposed solution. Finally, our solutions are based upon a well known P2P system called Pastry, albeit for Grid applications; this is another step towards the much-needed convergence of Grid and P2P computing.","Peer to peer computing,
Grid computing,
Protocols,
Computer science,
Application software,
Scalability,
Robustness,
Read-write memory,
Publishing,
Scheduling"
A modified sag characterization using voltage tolerance curve for power quality diagnosis,"This paper newly defines a voltage sag duration considering the voltage tolerance characteristics of individual electrical device for power quality (PQ) diagnosis. The conventional sag characterizing method has the possibility to overestimate voltage sag in case of nonrectangular sag. Furthermore, it cannot take the voltage tolerance characteristics of individual device into account. The proposed method utilizes the monitored parameters (V/sub sag/,V/sub avg/,d) at PQ monitor and the minimum voltage (V/sub min/) of voltage tolerance curve. The voltage sag profile is approximated using k/sup th/ order radical root function and finally the sag duration is modified from this function. The proposed method is applied to the nonrectangular sag due to induction motor reacceleration. It effectively evaluates the actual effects of voltage sag on the customer equipments whether the sag is rectangular or not.","Voltage fluctuations,
Power quality,
Moon,
Condition monitoring,
Induction motors,
Circuit faults,
Computer science,
Threshold voltage,
Stochastic processes,
Stochastic systems"
A multiresolution symbolic representation of time series,"Efficiently and accurately searching for similarities among time series and discovering interesting patterns is an important and non-trivial problem. In this paper, we introduce a new representation of time series, the multiresolution vector quantized (MVQ) approximation, along with a new distance function. The novelty of MVQ is that it keeps both local and global information about the original time series in a hierarchical mechanism, processing the original time series at multiple resolutions. Moreover, the proposed representation is symbolic employing key subsequences and potentially allows the application of text-based retrieval techniques into the similarity analysis of time series. The proposed method is fast and scales linearly with the size of database and the dimensionality. Contrary to the vast majority in the literature that uses the Euclidean distance, MVQ uses a multi-resolution/hierarchical distance function. We performed experiments with real and synthetic data. The proposed distance function consistently outperforms all the major competitors (Euclidean, dynamic time warping, piecewise aggregate approximation) achieving up to 20% better precision/recall and clustering accuracy on the tested datasets.","Databases,
Euclidean distance,
Time series analysis,
Computer science,
Multiresolution analysis,
Aggregates,
Testing,
Q measurement,
Time measurement,
Polynomials"
Tomography-based 3-D anisotropic elastography using boundary measurements,"While ultrasound- and magnetic resonance-based elastography techniques have proved to be powerful biomedical imaging tools, most approaches assume isotropic material properties. In this paper, a general framework is developed for tomography-based anisotropic elastography. An anatomically well- motivated piece-wise homogeneous model is proposed to represent a class of biological objects consisting of different regions. With established tomography modality, static displacements are measured on the entire external and internal boundaries, and the force distribution is recorded on part of the external surface. A principle is proposed to identify the anisotropic elastic moduli of the constituent regions with the obtained boundary measurements. The reconstruction procedure is optimization-based with minimizing an objective function that measures the difference between the predicted and observed displacements. Analytic gradients of the objective function with respect to the elastic moduli are calculated using an adjoint method, and are utilized to significantly improve the numerical efficiency. Simulations are performed to identify the elastic moduli in a breast phantom consisting of soft tissue and a hard tumor. For isotropic phantom, one set of the boundary measurements enables unique reconstruction results for the tissue and tumor. For anisotropic phantom, however, multiple sets of the measurements corresponding to different deformation modes become necessary.","Anisotropic magnetoresistance,
Biomedical measurements,
Imaging phantoms,
Displacement measurement,
Force measurement,
Image reconstruction,
Breast neoplasms,
Ultrasonic variables measurement,
Ultrasonic imaging,
Magnetic anisotropy"
Use of Meixner functions in estimation of Volterra kernels of nonlinear systems with delay,"Volterra series representation of nonlinear systems is a mathematical analysis tool that has been successfully applied in many areas of biological sciences, especially in the area of modeling of hemodynamic response. In this study, we explored the possibility of using discrete time Meixner basis functions (MBFs) in estimating Volterra kernels of nonlinear systems. The problem of estimation of Volterra kernels can be formulated as a multiple regression problem and solved using least squares estimation. By expanding system kernels with some suitable basis functions, it is possible to reduce the number of parameters to be estimated and obtain better kernel estimates. Thus far, Laguerre basis functions have been widely used in this framework. However, research in signal processing indicates that when the kernels have a slow initial onset or delay, Meixner functions, which can be made to have a slow start, are more suitable in terms of providing a more accurate approximation to the kernels. We, therefore, compared the performance of Meixner functions, in kernel estimation, to that of Laguerre functions in some test cases that we constructed and in a real experimental case where we studied photoreceptor responses of photoreceptor cells of adult fruitflies (Drosophila melanogaster). Our results indicate that when there is a slow initial onset or delay, MBF expansion provides better kernel estimates.",
Branching of positive discharge streamers in air at varying pressures,"The formation of positive streamers in a 17-mm gap in air is studied at pressures varying in the range from 1010 to 100 mbar. An intensified charge coupled device camera is used to image the discharge. At high pressures, the discharge shows many branches, while at low pressure, fewer branches arise. The structure is not simply determined by the ratio of voltage over pressure.",
A Mimo Mobile-To-Mobile Channel Model: Part II - The Simulation Model,"In this paper, we propose a multiple-input multiple-output (IMIMO) mobile-to-mobile channel simulation model derived from a non-realizable reference model presented in Part I of our paper. The underlying reference model is based on the geometrical two-ring scattering model, where both the transmitter and the receiver are moving. A closed-form solution is provided for the three dimensional (3-D) space-time cross-correlation function (CCF) of the simulation model. An important result is that this CCF is ergodic. Also, our results show that this 3-D function can be expressed as a product of two 2-D space-time CCF, called the transmit and the receive CCF. It is shown that the parameters of the simulation model can be determined for any given space-time CCF describing the reference model. In case of isotropic scattering, we present a closed-form solution of the model parameters and illustrate some numerical results for the transmit and receive CCF. Finally, simulation results show an excellent correspondence between the statistical and time average of the MIMO channel capacity. This supports the hypothesis that the MIMO capacity is ergodic with respect to the mean",
Quantitative evaluation of a novel image segmentation algorithm,"We present a quantitative evaluation of SE-MinCut, a novel segmentation algorithm based on spectral embedding and minimum cut. We use human segmentations from the Berkeley segmentation database as ground truth and propose suitable measures to evaluate segmentation quality. With these measures we generate precision/recall curves for SE-MinCut and three of the leading segmentation algorithms: mean-shift, normalized Cuts, and the local variation algorithm. These curves characterize the performance of each algorithm over a range of input parameters. We compare the precision/recall curves for the four algorithms and show segmented images that support the conclusions obtained from the quantitative evaluation.",
"Analyzing goal semantics for rights, permissions, and obligations","Software requirements, rights, permissions, obligations, and operations of policy enforcing systems are often misaligned. Our goal is to develop tools and techniques that help requirements engineers and policy makers bring policies and system requirements into better alignment. Goals from requirements engineering are useful for distilling natural language policy statements into structured descriptions of these interactions; however, they are limited in that they are not easy to compare with one another despite sharing common semantic features. In this paper, we describe a process called semantic parameterization that we use to derive semantic models from goals mined from privacy policy documents. We present example semantic models that enable comparing policy statements and present a template method for generating natural language policy statements (and ultimately requirements) from unique semantic models. The semantic models are described by a context-free grammar called KTL that has been validated within the context of the most frequently expressed goals in over 100 Internet privacy policy documents. KTL is supported by a policy analysis tool that supports queries and policy statement generation.","Permission,
Privacy,
Natural languages,
Internet,
Standardization,
Data mining,
Computer science,
Context modeling,
Contracts,
Formal specifications"
Radiance caching for efficient global illumination computation,"In this paper, we present a ray tracing-based method for accelerated global illumination computation in scenes with low-frequency glossy BRDFs. The method is based on sparse sampling, caching, and interpolating radiance on glossy surfaces. In particular, we extend the irradiance caching scheme proposed by Ward et al. (1988) to cache and interpolate directional incoming radiance instead of irradiance. The incoming radiance at a point is represented by a vector of coefficients with respect to a hemispherical or spherical basis. The surfaces suitable for interpolation are selected automatically according to the roughness of their BRDF. We also propose a novel method for computing translational radiance gradient at a point.","Lighting,
Interpolation,
Ray tracing,
Acceleration,
Layout,
Monte Carlo methods,
Sampling methods,
Computer science,
Rough surfaces,
Surface roughness"
Hardware-based support vector machine classification in logarithmic number systems,"Support vector machines are emerging as a powerful machine-learning tool. Logarithmic number systems (LNS) utilize the property of logarithmic compression for numerical operations. We present an implementation of a digital support vector machine (SVM) classifier using LNS in which, when compared with other implementations, considerable hardware savings are achieved with no significant loss in classification accuracy.","Support vector machine classification,
Support vector machines,
Kernel,
Machine learning,
Hardware,
Application software,
Neural networks,
Computer science,
Power engineering and energy,
Event detection"
Limiting path exploration in BGP,"Slow convergence in the Internet can be directly attributed to the ""path exploration"" phenomenon, inherent in all path vector protocols. The root cause for path exploration is the dependency among paths propagated through the network. Addressing this problem in BGP is particularly difficult as the AS paths exchanged between BGP routers are highly summarized. In this paper, we describe why path exploration cannot be countered effectively within the existing BGP framework, and propose a simple, novel mechanism - forward edge sequence numbers - to annotate the AS paths with additional ""path dependency"" information. We then develop an enhanced path vector algorithm, EPIC, shown to limit path exploration and lead to faster convergence. In contrast to other solutions, ours is shown to be correct on a very general model of Internet topology and BGP operation. Using theoretical analysis and simulations, we demonstrate that EPIC can achieve a dramatic improvement in routing convergence, compared to BGP and other existing solutions.","Routing protocols,
Internet,
Convergence,
Computer science,
Topology,
Analytical models,
Delay effects,
Streaming media,
Damping"
Visual learning for science and engineering,"This survey looks at visualization techniques used in science and engineering education to enhance student learning and encourage underrepresented students to pursue technical degrees. This article aims to encourage faculty in science, technology, engineering, and math (STEM) disciplines to use visual methods to communicate to their students. Visual learning is an important method for exploiting students' visual senses to enhance learning and engage their interest. This methodology also has the potential to increase the number of students in STEM fields, especially of women and minority students. A visual approach to science and engineering enhances communication. This visualization revolution shows that letting scientists engage the higher cognitive parts of the brain by thinking and communicating visually improved how they performed their research.","Visualization,
Animation,
Computer science,
Education,
Cultural differences,
Multimedia systems,
Best practices,
Polarization,
Glass,
Mars"
PPF Control of a Piezoelectric Tube Scanner,"Piezoelectric tubes are commonly used in Scanning Tunnelling Microscopes and Atomic Force Microscopes to scan material surfaces. In general, scanning using a piezoelectric tube is hampered by the presence of low-frequency mechanical modes that are easily excited to produce unwanted vibration. In this work, a Positive Position Feedback controller is designed to mitigate the undesired mechanical resonance. Experimental results reveal a significant damping of the mechanical dynamics, and consequently, an improvement in tracking performance.","Surface topography,
Atomic force microscopy,
Probes,
Vibration control,
Resonance,
Damping,
Mechanical sensors,
Tunneling,
Adaptive control,
Image resolution"
Computer vision elastography: speckle adaptive motion estimation for elastography using ultrasound sequences,"We present the development and validation of an image based speckle tracking methodology, for determining temporal two-dimensional (2-D) axial and lateral displacement and strain fields from ultrasound video streams. We refine a multiple scale region matching approach incorporating novel solutions to known speckle tracking problems. Key contributions include automatic similarity measure selection to adapt to varying speckle density, quantifying trajectory fields, and spatiotemporal elastograms. Results are validated using tissue mimicking phantoms and in vitro data, before applying them to in vivo musculoskeletal ultrasound sequences. The method presented has the potential to improve clinical knowledge of tendon pathology from carpel tunnel syndrome, inflammation from implants, sport injuries, and many others.","Computer vision,
Speckle,
Motion estimation,
Ultrasonic imaging,
Two dimensional displays,
Capacitive sensors,
Streaming media,
Density measurement,
Ultrasonic variables measurement,
Spatiotemporal phenomena"
Brain Computer Interface Design Using Band Powers Extracted During Mental Tasks,"In this paper, a brain computer interface (BCI) is designed using electroencephalogram (EEG) signals where the subjects have to think of only a single mental task. The method uses spectral power and power difference in 4 bands: delta and theta, beta, alpha and gamma. This could be used as an alternative to the existing BCI designs that require classification of several mental tasks. In addition, an attempt is made to show that different subjects require different mental task for minimising the error in BCI output. In the experimental study, EEG signals were recorded from 4 subjects while they were thinking of 4 different mental tasks. Combinations of resting (baseline) state and another mental task are studied at a time for each subject. Spectral powers in the 4 bands from 6 channels are computed using the energy of the elliptic FIR filter output. The mental tasks are detected by a neural network classifier. The results show that classification accuracy up to 97.5% is possible, provided that the most suitable mental task is used. As an application, the proposed method could be used to move a cursor on the screen. If cursor movement is used with a translation scheme like Morse code, the subjects could use the proposed BCI for constructing letters/words. This would be very useful for paralysed individuals to communicate with their external surroundings",
Beaming of light at broadside through a subwavelength hole: Leaky wave model and open stopband effect,"The optical transmission through a subwavelength hole in a metal film is usually very small, and the beam radiated from its exit aperture is very broad. However, the transmission may be increased by orders of magnitude, and the output beam sharply narrowed, when the tiny hole is surrounded by a properly designed periodic structure, which may take the form of an array of grooves or indentations on the metal surface. We have shown that these dramatic effects are due to the excitation of a leaky surface plasmon mode by the periodic structure on the metal film. Following this understanding, we introduce a simple but effective leaky wave antenna model, which we use to further explain and to quantify these dramatic effects. Particular attention is given to optimizing the structure to achieve maximum radiation at broadside, which offers a significant challenge in view of the open stopband in precisely the broadside direction.","Periodic structures,
Face,
Plasmons,
Optical surface waves,
Metals,
Surface waves,
Apertures"
Weaving aspects into Web service orchestrations,"Web service orchestration engines need to be more open to enable the addition of new features into service-based applications. In this paper, we illustrate how, in a BPEL engine with aspect-weaving capabilities, a process-driven application based on the Google Web service can be dynamically adapted with new features and hot-fixed to meet unforeseen post-deployment requirements. Business processes (the application skeletons) can be enriched with additional features such as debugging, execution monitoring, or an application-specific GUI. Dynamic aspects are also used on the processes themselves to tackle the problem of hot-fixes to long running processes. In this manner, composing a Web service 'on-the-fly' means weaving its choreography interface into the business process.","Weaving,
Web services,
Search engines,
Simple object access protocol,
Prototypes,
Educational institutions,
Computer science,
Application software,
Debugging,
Graphical user interfaces"
Face synthesis and recognition from a single image under arbitrary unknown lighting using a spherical harmonic basis morphable model,"Understanding and modifying the effects of arbitrary illumination on human faces in a realistic manner is a challenging problem both for face synthesis and recognition. Recent research demonstrates that the set of images of a convex Lambertian object obtained under a wide variety of lighting conditions can be approximated accurately by a low-dimensional linear subspace using spherical harmonics representation. Morphable models are statistical ensembles of facial properties such as shape and texture. In this paper, we integrate spherical harmonics into the morphable model framework, by proposing a 3D spherical harmonic basis morphable model (SHBMM) and demonstrate that any face under arbitrary unknown lighting can be simply represented by three low-dimensional vectors: shape parameters, spherical harmonic basis parameters and illumination coefficients. We show that, with our SHBMM, given one single image under arbitrary unknown lighting, we can remove the illumination effects from the image (face ""delighting"") and synthesize new images under different illumination conditions (face ""re-lighting""). Furthermore, we demonstrate that cast shadows can be detected and subsequently removed by using the image error between the input image and the corresponding rendered image. We also propose two illumination invariant face recognition methods based on the recovered SHBMM parameters and the de-lit images respectively. Experimental results show that using only a single image of a face under unknown lighting, we can achieve high recognition rates and generate photorealistic images of the face under a wide range of illumination conditions, including multiple sources of illumination.",
Balancing exploration and exploitation: a new algorithm for active machine learning,"Active machine learning algorithms are used when large numbers of unlabeled examples are available and getting labels for them is costly (e.g. requiring consulting a human expert). Many conventional active learning algorithms focus on refining the decision boundary, at the expense of exploring new regions that the current hypothesis misclassifies. We propose a new active learning algorithm that balances such exploration with refining of the decision boundary by dynamically adjusting the probability to explore at each step. Our experimental results demonstrate improved performance on data sets that require extensive exploration while remaining competitive on data sets that do not. Our algorithm also shows significant tolerance of noise.",
Robust particle systems for curvature dependent sampling of implicit surfaces,"Recent research on point-based surface representations suggests that point sets may be a viable alternative to parametric surface representations in applications where the topological constraints of a parameterization are unwieldy or inefficient. Particle systems offer a mechanism for controlling point samples and distributing them according to needs of the application. Furthermore, particle systems can serve as a surface representation in their own right, or to augment implicit functions, allowing for both efficient rendering and control of implicit function parameters. The state of the art in surface sampling particle systems, however, presents some shortcomings. First, most of these systems have many parameters that interact with some complexity, making it difficult for users to tune the system to meet specific requirements. Furthermore, these systems do not lend themselves to spatially adaptive sampling schemes, which are essential for efficient, accurate representations of complex surfaces. In this paper we present a new class of energy functions for distributing particles on implicit surfaces and a corresponding set of numerical techniques. These techniques provide stable, scalable, efficient, and controllable mechanisms for distributing particles that sample implicit surfaces within a locally adaptive framework.",
Speech-driven facial animation with realistic dynamics,"This work presents an integral system capable of generating animations with realistic dynamics, including the individualized nuances, of three-dimensional (3-D) human faces driven by speech acoustics. The system is capable of capturing short phenomena in the orofacial dynamics of a given speaker by tracking the 3-D location of various MPEG-4 facial points through stereovision. A perceptual transformation of the speech spectral envelope and prosodic cues are combined into an acoustic feature vector to predict 3-D orofacial dynamics by means of a nearest-neighbor algorithm. The Karhunen-Loe/spl acute/ve transformation is used to identify the principal components of orofacial motion, decoupling perceptually natural components from experimental noise. We also present a highly optimized MPEG-4 compliant player capable of generating audio-synchronized animations at 60 frames/s. The player is based on a pseudo-muscle model augmented with a nonpenetrable ellipsoidal structure to approximate the skull and the jaw. This structure adds a sense of volume that provides more realistic dynamics than existing simplified pseudo-muscle-based approaches, yet it is simple enough to work at the desired frame rate. Experimental results on an audiovisual database of compact TIMIT sentences are presented to illustrate the performance of the complete system.","Facial animation,
Speech,
Humans,
Face,
MPEG 4 Standard,
Computer science,
Hidden Markov models,
Natural languages,
Working environment noise,
Acoustics"
Adaptive Rigid Multi-region Selection for Handling Expression Variation in 3D Face Recognition,"We present a new algorithm for 3D face recognition, and compare its performance to that of previous approaches. We focus especially on the case of facial expression change between gallery and probe images. We first establish performance comparisons using a PCA (""eigenface"") algorithm and an ICP (iterative closest point) algorithm similar to ones reported in the literature. Experimental results show that the performance of either approach degrades substantially in the case Then we introduce a new algorithm, Adaptive Rigid Multi-region Selection, is introduced to independently matches multiple facial regions and creates a fused result. This algorithm is fully automated and used no manually selected landmark points. Experimental results show that our new algorithm substantially improves performance in the case of varying facial expression. Our experimental results are based on the largest 3D face dataset to date, with 449 persons, over 4,000 3D images, and substantial lapse between gallery and probe images.","Face recognition,
Shape,
Iterative algorithms,
Probes,
Degradation,
Biosensors,
Layout,
Databases,
Computer science,
Principal component analysis"
Adaptive fast block-matching algorithm by switching search patterns for sequences with wide-range motion content,"Content with rapid, moderate, and slow motion is frequently mixed together in real video sequences. Until now, no fast block-matching algorithm (FBMA), including the well-known three-step search (TSS), the block-based gradient descent search (BBGDS), and the diamond search (DS), can efficiently remove the temporal redundancy of sequences with wide range motion content. This paper proposes an adaptive FBMA, called A-TDB, to solve this problem. Based on the characteristics of a proposed predicted profit list, the A-TDB can adaptively switch search patterns among the TSS, DS, and BBGDS, according to the motion content. Experimental results reveal that the A-TDB successfully adopts the search patterns to remove the temporal redundancy of sequences with slow, moderate and rapid motion content.","Video sequences,
Motion analysis,
Switches,
Motion compensation,
Frequency selective surfaces,
Computer science,
Motion estimation,
Transform coding,
Encoding,
Bit rate"
Efficient force calculations based on continuum sensitivity analysis,"Using continuum design sensitivity analysis (CDSA), in conjunction with the virtual work principle, equations have been derived for calculating forces without the need to solve the adjoint system. The resultant expressions are similar to the Maxwell stress tensor, but have the important advantage of the integration taking place on the surface of material rather than in the air outside. Implementation of the scheme leads to efficient calculations and improved accuracy.","Sensitivity analysis,
Magnetic materials,
Computer science,
Tensile stress,
Magnetic fields,
Maxwell equations,
Accuracy,
Finite element methods,
Frequency,
Permanent magnets"
Discovering frequent arrangements of temporal intervals,"In this paper we study a new problem in temporal pattern mining: discovering frequent arrangements of temporal intervals. We assume that the database consists of sequences of events, where an event occurs during a time-interval. The goal is to mine arrangements of event intervals that appear frequently in the database. There are many applications where these type of patterns can be useful, including data network, scientific, and financial applications. Efficient methods to find frequent arrangements of temporal intervals using both breadth first and depth first search techniques are described. The performance of the proposed algorithms is evaluated and compared with other approaches on real datasets (American sign language streams and network data) and large synthetic datasets.","Transaction databases,
Computer science,
Handicapped aids,
Monitoring,
Event detection,
Intrusion detection,
Data mining"
Static output feedback stabilization: necessary conditions for multiple delay controllers,"This note focuses on the static output feedback stabilization problem for a class of single-input-single-output systems when the control law includes multiple (distinct) delays. We are interested in giving necessary conditions for the existence of such stabilizing controllers. Illustrative examples (second-order system, chain of integrators, or chain of oscillators) are presented, and discussed.","Output feedback,
Control systems,
Stability,
Delay effects,
Oscillators,
Cities and towns,
Automatic control,
Laboratories,
Computer science,
Orbits"
Effects of technology scaling on the SET sensitivity of RF CMOS Voltage-controlled oscillators,"We analyze single-event transient (SET) effects in a high-speed voltage-controlled oscillator (VCO) design that is applicable to mixed-signal RF operations. Our study shows that the sensitivity of this type of circuit topologies exhibits a strong correlation to: the minimum feature size of the technology in use, the range of oscillating frequencies that the circuit is designed to achieve and the frequency at which the VCO is operating. We also determine that current-starved VCOs have an optimal functional range for improved radiation tolerance, located at the upper segment of their voltage-to-frequency transfer characteristic, where exposition to low LET ions become of no consequence on the operation of the circuit. More importantly, we show that derating the frequency of such VCO designed in a state-of-the-art technology does not compensate for the increased radiation vulnerability at that node. If not properly accounted for, the performance overhead that the newer technology provides is shown to severely penalize the radiation hardness of the circuit.","CMOS technology,
Radio frequency,
Voltage-controlled oscillators,
Computer simulation,
Computational modeling,
Circuit simulation,
Radiofrequency identification,
Isolation technology,
Transient analysis,
Circuit topology"
Group-theoretic algorithms for matrix multiplication,"We further develop the group-theoretic approach to fast matrix multiplication introduced by Cohn and Umans, and for the first time use it to derive algorithms asymptotically faster than the standard algorithm. We describe several families of wreath product groups that achieve matrix multiplication exponent less than 3, the asymptotically fastest of which achieves exponent 2.41. We present two conjectures regarding specific improvements, one combinatorial and the other algebraic. Either one would imply that the exponent of matrix multiplication is 2.",
Accurate anchor-free node localization in wireless sensor networks,"There has been a growing interest in the applications of wireless sensor networks in unattended environments. In such applications, sensor nodes are usually deployed randomly in an area of interest. Knowledge of accurate node location is essential in order to correlate the gathered data to the origin of the sensed phenomena and assure the relevance of the reported information. In this paper, we present an efficient anchor-free protocol for localization in wireless sensor networks. Each node discovers its neighbors that are within its transmission range and estimates their ranges. Our algorithm fuses local range measurements in order to form a network wide unified coordinate systems while minimizing the overhead incurred at the deployed sensors. Scalability is achieved through grouping sensors into clusters. Simulation results show that the proposed protocol achieves precise localization of sensors and maintains consistent error margins.","Intelligent networks,
Wireless sensor networks,
Protocols,
Computer science,
Educational institutions,
Application software,
Clustering algorithms,
Sensor systems,
Network topology,
Partitioning algorithms"
Hello protocols for ad-hoc networks: overhead and accuracy tradeoffs,"The hello protocol (Moy, J., 1994) is a fundamental protocol for wired and wireless networks. In mobile ad-hoc networks, a hello protocol helps nodes to establish a neighbor table for link detection. If nodes exchange position information in hello packets, then it also helps them in packet forwarding decisions. In ad-hoc networks, due to node mobility, neighbor relationships change frequently. To cope with mobility, and to have an up-to-date neighbor table, nodes advertise hello packets periodically. These hello packets create congestion, which may cause control and data packets to be dropped in the network. We study the impact of hello protocols on ad-hoc networks. We present three new hello protocols which reduce network congestion. The main idea behind all three protocols is to beacon as little as possible without compromising the accuracy of the neighbor table. To evaluate the performance of our protocols and their impact on ad-hoc networks, we simulated them in GloMoSim 2.03. Our results show that the proposed hello protocols incur much lower overhead and increase the network performance compared to a periodic hello protocol, while maintaining identical neighbor table accuracy.","Ad hoc networks,
Routing protocols,
Media Access Protocol,
Wireless application protocol,
Laboratories,
Computer science,
Wireless networks,
Art,
Wireless communication,
Joining processes"
Information granulation and granular relationships,"As an emerging research method to deal with information and knowledge processing, various topics of granular computing have recently received more attention by researchers. The foundations of granular computing involves general principles of many disciplines that are explored for many years such as divide and conquer, interval computing, fuzzy sets, rough sets and so on. Granules, granulations and relationships are some of the key issues in the study of granular computing. This paper aims to understand mainly granular computing theory from the perspective of information granulation and granular relationships.",
Tools and applications for large-scale display walls,"Increased processor and storage capacities have supported the computational sciences, but have simultaneously unleashed a data avalanche on the scientific community. As a result, scientific research is limited by data analysis and visualization capabilities. These new bottlenecks have been the driving motivation behind the Princeton scalable display wall project. To create a scalable and easy-to-use large-format display system for collaborative visualization, the authors have developed various techniques, software tools, and applications.",
SHARP: a new approach to relative localization in wireless sensor networks,"For wireless sensor networks, localization is crucial due to the dynamic nature of deployment. In relative localization, nodes use the distance measurements to estimate their positions relative to some coordinate system. In absolute localization, a few nodes (called anchors) need to know their absolute positions, and all the other nodes are absolutely localized in the coordinate system of the anchors. Relative and absolute localization methods differ in both the performance and the cost. We present a new approach to relative localization that we refer to as: simple hybrid absolute-relative positioning (SHARP). In SHARP, a relative localization method (M1) is used to relatively localize N/sub r/ reference nodes. Then, an absolute localization method (M2) uses these N/sub r/ nodes as anchors to localize the rest of the nodes. Choosing N/sub r/, M1, and M2 gives a wide range of performance-cost tuning. We have done extensive simulation using the multidimensional scaling (MDS) method as M1 and the ad-hoc positioning system (APS) method as M2. While previous research shows that MDS gives better localization results than APS, our simulation shows that SHARP outperforms MDS if both the localization error and the cost are considered.",
Nonlinear Performance Limits for High Energy Density Piezoelectric Bending Actuators,"To keep pace with recent advances in micro robotic structures demands actuator technologies which can deliver high power and precise motion. For electroactive material based actuators, high power typically implies either high field or high current drives which may lead to greater nonlinearities such as saturation, softening, and increased loss. Physical modeling of actuators is normally taken to be linear since the range of displacements, applied loads, and applied fields is typically small. If extrapolated to high drive conditions, these linear models significantly over predict the power which can be delivered. For actuators driving dynamic systems, a complete nonlinear model of the system will improve controllability and give more accurate estimations of power delivery capabilities. Here static nonlinearities and dynamic linear and nonlinear parameters are derived for high performance piezoelectric bending actuators.",
The Erlangen slot machine: increasing flexibility in FPGA-based reconfigurable platforms,"We present a new concept as well as the implementation of an FPGA based reconfigurable platform, the Erlangen slot machine (ESM). One main advantage of this platform is the possibility for each module to access its periphery independent from its location through a programmable crossbar, allowing an unrestricted relocation of modules on the device. Furthermore, we propose different intermodule communication structures.","Field programmable gate arrays,
Pins,
Runtime,
Time sharing computer systems,
Signal processing algorithms,
Computer science,
IEEE news,
Production,
Logic devices,
Reconfigurable logic"
Name disambiguation in author citations using a K-way spectral clustering method,"An author may have multiple names and multiple authors may share the same name simply due to name abbreviations, identical names, or name misspellings in publications or bibliographies (citations). This can produce name ambiguity which can affect the performance of document retrieval, web search, and database integration, and may cause improper attribution of credit. Proposed here is an unsupervised learning approach using K-way spectral clustering that disambiguates authors in citations. The approach utilizes three types of citation attributes: co-author names, paper titles, and publication venue titles. The approach is illustrated with 16 name datasets with citations collected from the DBLP database bibliography and author home pages and shows that name disambiguation can be achieved using these citation attributes","Clustering methods,
Bibliographies,
Information retrieval,
Computer science,
Web search,
Unsupervised learning,
Permission,
Statistics,
Supervised learning,
Databases"
Haptic Teleoperation of a Mobile Robot: A User Study,"The problem of teleoperating a mobile robot using shared autonomy is addressed: An onboard controller performs close-range obstacle avoidance while the operator uses the manipulandum of a haptic probe to designate the desired speed and rate of turn. Sensors on the robot are used to measure obstacle-range information. A strategy to convert such range information into forces is described, which are reflected to the operator's hand via the haptic probe. This haptic information provides feedback to the operator in addition to imagery from a front-facing camera mounted on the mobile robot. Extensive experiments with a user population both in virtual and in real environments show that this added haptic feedback significantly improves operator performance, as well as presence, in several ways (reduced collisions, increased minimum distance between the robot and obstacles, etc.) without a significant increase in navigation time.",
Rule Extraction from Recurrent Neural Networks: ATaxonomy and Review,"Rule extraction (RE) from recurrent neural networks (RNNs) refers to finding models of the underlying RNN, typically in the form of finite state machines, that mimic the network to a satisfactory degree while having the advantage of being more transparent. RE from RNNs can be argued to allow a deeper and more profound form of analysis of RNNs than other, more or less ad hoc methods. RE may give us understanding of RNNs in the intermediate levels between quite abstract theoretical knowledge of RNNs as a class of computing devices and quantitative performance evaluations of RNN instantiations. The development of techniques for extraction of rules from RNNs has been an active field since the early 1990s. This article reviews the progress of this development and analyzes it in detail. In order to structure the survey and evaluate the techniques, a taxonomy specifically designed for this purpose has been developed. Moreover, important open research issues are identified that, if addressed properly, possibly can give the field a significant push forward.",
Segmented hash,"Hash tables provide efficient table implementations, achieving O(1), query, insert and delete operations at low loads. However, at moderate or high loads collisions are quite frequent, resulting in decreased performance. In this paper, we propose the segmented hash table architecture, which ensures constant time hash operations at high loads with high probability. To achieve this, the hash memory is divided into N logical segments so that each incoming key has N potential storage locations; the destination segment is chosen so as to minimize collisions. In this way, collisions, and the associated probe sequences, are dramatically reduced. In order to keep memory utilization minimized, probabilistic filters are kept on-chip to allow the N segments to be accessed without increasing the number of off-chip memory operations. These filters are kept small and accurate with the help of a novel algorithm, called selective filter insertion, which keeps the segments balanced while minimizing false positive rates (i.e., incorrect filter predictions). The performance of our scheme is quantified via analytical modeling and software simulations. Moreover, we discuss efficient implementations that are easily realizable in modern device technologies. The performance benefits are significant: average search cost is reduced by 40% or more, while the likelihood of requiring more than one memory operation per search is reduced by several orders of magnitude.",
A survey of peer-to-peer storage techniques for distributed file systems,"The popularity of distributed file systems continues to grow. Reasons they are preferred over traditional centralized file systems include fault tolerance, availability, scalability and performance. In addition, peer-to-peer (P2P) system concepts and scalable functions are being incorporated into the domain of file systems. This survey paper explores the design paradigms and important issues that relate to such systems and discusses the various research activities in the field of distributed peerto-peer file systems.","Peer to peer computing,
File systems,
Fault tolerant systems,
Availability,
Computer architecture,
Buildings,
Application software,
Computer science,
Scalability,
Large-scale systems"
A new approach for pulse processing in positron emission tomography,"In this paper, we propose a new technique for positron emission tomography (PET) pulse processing that has the potential to overcome design limitations in PET arising from the need to use fast, high-cost analog-to-digital converters (ADCs). In this technique, we consider the voltage pulse derived from the charge pulse generated by a scintillator/photomultiplier tube (PMT) detector upon a detection event and model the voltage pulse by a fast, linearly rising edge followed by a slower exponential decay. Based on this model, we show that the basic parameters of the voltage pulse that are relevant for PET event detection can be determined from a trigger time and four time intervals measured on the pulse. We also discuss an electronic implementation in which the required measurements can be obtained by using clocks, comparators, and registers, thereby eliminating the use of ADCs and constant fraction discriminators in PET front-end electronics. We conduct computer simulation studies to evaluate the performance of the proposed technique and our results are promising. When used with Cerium Doped Lutetium Oxyorthosilicate (LSO)/PMT, the proposed technique can generate accurate results for the photon energy, the event time, and the decay-time constant. In addition, it can support an energy resolution of about 30% and a coincidence window of about 10 ns. On the other hand, in its present form the proposed technique is not suitable for use with BGO/PMT due to its low light yield.","Positron emission tomography,
Voltage,
Event detection,
Pulse measurements,
Analog-digital conversion,
Pulse generation,
Photomultipliers,
Detectors,
Time measurement,
Clocks"
Performance evaluation of non-ideal iris based recognition system implementing global ICA encoding,"We describe and analyze the performance of a non-ideal iris recognition system. The system is designed to process non-ideal iris images in two steps: (i) estimation of the gaze direction and (ii) processing and encoding of the rotated iris image. We use two objective functions to estimate the gaze direction: Hamming distance and Daugman's integro-differential operator and determine an estimated angle by picking the value that optimizes the selected objective function. After the angle is estimated, the off-angle iris image undergoes geometric transformations involving the estimated angle and is further processed as if it were a frontal view image. The encoding technique developed in this work is based on application of the global independent component analysis (ICA) to masked iris images. We use two datasets: CASIA dataset and a special dataset of off-angle iris images collected at WVU to verify the performance of the encoding technique and angle estimator, respectively. A series of receiver operating characteristics (ROCs) demonstrates various effects on the performance of the non-ideal iris based recognition system implementing the global ICA encoding.","Independent component analysis,
Encoding,
Image coding,
Biometrics,
Performance analysis,
Iris recognition,
Hamming distance,
Feature extraction,
Computer science,
Process design"
Social computing and weighting to identify member roles in online communities,"As more and more people join online communities, the ability to better understand members' roles becomes critical to preserving and improving the health of those communities. We propose a novel approach to identifying key members and their roles by discovering implicit knowledge from online communities. Viewing an online community as a social network connected by poster-poster relationships, the approach takes advantage of the strengths of social network analysis and weighting schemes from information retrieval in identifying key members. Experimental studies were carried out to empirically evaluate the proposed approach with real-world data collected from a Usenet bulletin board over a one year period. The results showed that the proposed approach can not only identify prominent members whose behaviors are community supportive but also filter chatters whose behaviors are superficial to the online community. The findings have broad implications for online communities by allowing moderators to better support their members and by enabling members to better understand the conversation space.","Social network services,
Information analysis,
Information retrieval,
Communities,
Filters,
Visualization,
Frequency conversion,
Computer networks"
A feature-oriented approach to modeling requirements dependencies,"There are many researches on requirements dependencies. However, most of them limit their views to the requirements phase of software development, few focus on the roles of requirements dependencies in the solution space of software. This paper presents a feature-oriented approach to modeling requirements dependencies. A feature is a set of tight-related requirements from user/customer-views. The feature-orientation provides a modular way to organize requirements and a proper granularity to analyze requirements dependencies. In this approach, we care about not only static feature dependencies (i. e. refinements and constraints), but also feature dependencies at the specification level (namely influences) and, furthermore, dynamic feature dependencies (namely interactions). Moreover, we also explore the underlying connections between these four kinds of feature dependency. By this way, this approach gives a more complete view of how requirements dependencies influence the whole process of software development.",
Speaker localization using excitation source information in speech,"This paper presents the results of simulation and real room studies for localization of a moving speaker using information about the excitation source of speech production. The first step in localization is the estimation of time-delay from speech collected by a pair of microphones. Methods for time-delay estimation generally use spectral features that correspond mostly to the shape of vocal tract during speech production. Spectral features are affected by degradations due to noise and reverberation. This paper proposes a method for localizing a speaker using features that arise from the excitation source during speech production. Experiments were conducted by simulating different noise and reverberation conditions to compare the performance of the time-delay estimation and source localization using the proposed method with the results obtained using the spectrum-based generalized cross correlation (GCC) methods. The results show that the proposed method shows lower number of discrepancies in the estimated time-delays. The bias, variance and the root mean square error (RMSE) of the proposed method is consistently equal or less than the GCC methods. The location of a moving speaker estimated using the time-delays obtained by the proposed method are closer to the actual values, than those obtained by the GCC method.",
Predictability of Web-server traffic congestion,"Large swings in the demand for content are commonplace within the Internet. When a traffic hotspot happens, however, there is a delay before measures such as heavy replication of content can be applied. This paper investigates the potential for predicting hotspots sufficiently far, albeit shortly, in advance, so that preventive action can be taken before the hotpot takes place. Performing accurate load predictions appears to be a daunting challenge at first glance, but this paper shows that, when applied to Web-server page-request traffic, even elementary prediction techniques can have a surprising forecasting power. We first argue this predictability from principles, and then confirm it by the analysis of empirical data, which reveals that large server overloads can often be seen well in advance. This allows steps to be taken to reduce substantially the degradation of service quality.",
"BitMAC: a deterministic, collision-free, and robust MAC protocol for sensor networks","Collisions are a source of inefficiency in contention-based MAC protocols that should be reduced to a minimum. We show that concurrent multiple access to a communication channel will, however, not necessarily lead to a collision with undesirable effects. Rather, we demonstrate that it is possible for a receiver to hear the bitwise ""or"" of the transmissions of multiple synchronized senders within communication range. This unconventional communication model allows the efficient implementation of a number of basic operations that serve as a foundation for BitMAC: a deterministic, collision-free, and robust MAC protocol that is tailored to dense sensor networks, where nodes report sensory data over multiple hops to a sink.","Robustness,
Media Access Protocol,
Access protocols,
Sensor phenomena and characterization,
Communication channels,
Bandwidth,
Computer science,
Energy consumption,
Energy resolution,
Delay"
On the decidability of metric temporal logic,"Metric temporal logic (MTL) is a prominent specification formalism for real-time systems. In this paper, we show that the satisfiability problem for MTL over finite timed words is decidable, with non-primitive recursive complexity. We also consider the model-checking problem for MTL: whether all words accepted by a given Alur-Dill timed automaton satisfy a given MTL formula. We show that this problem is decidable over finite words. Over infinite words, we show that model checking the safety fragment of MTL-which includes invariance and time-bounded response properties-is also decidable. These results are quite surprising in that they contradict various claims to the contrary that have appeared in the literature. The question of the decidability of MTL over infinite words remains open.",
Ordinal palmprint represention for personal identification [represention read representation],"Palmprint-based personal identification, as a new member in the biometrics family, has become an active research topic in recent years. Although great progress has been made, how to represent palmprint for effective classification is still an open problem. In this paper, we present a novel palmprint representation - ordinal measure, which unifies several major existing palmprint algorithms into a general framework. In this framework, a novel palmprint representation method, namely orthogonal line ordinal features, is proposed. The basic idea of this method is to qualitatively compare two elongated, line-like image regions, which are orthogonal in orientation and generate one bit feature code. A palmprint pattern is represented by thousands of ordinal feature codes. In contrast to the state-of-the-art algorithm reported in the literature, our method achieves higher accuracy, with the equal error rate reduced by 42% for a difficult set, while the complexity of feature extraction is halved.","Biometrics,
Fingerprint recognition,
Pattern recognition,
Feature extraction,
Face recognition,
Computer vision,
National security,
Laboratories,
Automation,
Computer science"
Using Hierarchical EM to Extract Planes from 3D Range Scans,"Recently, the acquisition of three-dimensional maps has become more and more popular. This is motivated by the fact that robots act in the three-dimensional world and several tasks such as path planning or localizing objects can be carried out more reliable using three-dimensional representations. In this paper we consider the problem of extracting planes from three-dimensional range data. In contrast to previous approaches our algorithm uses a hierarchical variant of the popular Expectation Maximization (EM) algorithm [1] to simultaneously learn the main directions of the planar structures. These main directions are then used to correct the position and orientation of planes. In practical experiments carried out with real data and in simulations we demonstrate that our algorithm can accurately extract planes and their orientation from range data.","Data mining,
Path planning,
Mobile robots,
Buildings,
Computer science,
Educational institutions,
Drives,
Robotics and automation,
Computer architecture,
Visualization"
Capturing accurate snapshots of the Gnutella network,"A common approach for measurement-based characterization of peer-to-peer (P2P) systems is to capture overlay snapshots using a crawler. The accuracy of captured snapshots by P2P crawlers directly depends on both the crawling speed and the fraction of unreachable peers. This in turn affects the accuracy of the conducted characterization based on these captured snapshots. Prior studies frequently rely on crawling the network over an hour or more, during which time the overlay may change substantially. Moreover, none of the previous measurement-based studies on P2P systems have examined the accuracy of their captured snapshots or the impact on conducted characterization.",
Fuzzy PSO: a generalization of particle swarm optimization,"In standard particle swarm optimization (PSO), the best particle in each neighborhood exerts its influence over other particles in the neighborhood. In this paper, we propose fuzzy PSO, a generalization which differs from standard PSO in the following respect: charisma is defined to be a fuzzy variable, and more than one particle in each neighborhood can have a non-zero degree of charisma, and, consequently, is allowed to influence others to a degree that depends on its charisma. We evaluate our model on the weighted maximum satisfiability (maxsat) problem, comparing performance to standard PSO and to Walk-Sat.","Particle swarm optimization,
Space exploration,
Computer science,
Hypercubes,
Topology,
Computational intelligence,
Insects,
Educational institutions,
Marine animals,
Fuzzy cognitive maps"
Reliability analysis for various communication schemes in wireless CORBA,"For the purpose of designing more reliable networks, we extend the traditional reliability analysis from wired networks to wireless networks with imperfect components. Wireless network systems, such as wireless CORBA, inherit the unique handoff characteristic which leads to different communication structures with various components & links. Therefore, the traditional definition of two-terminal reliability is not applicable any more. We propose a new term, end-to-end expected instantaneous reliability, to integrate those different communication structures into one metric, which includes not only failure parameters but also service parameters. Nevertheless, it is still a monotonously decreasing function of time. The end-to-end expected instantaneous reliability, and its corresponding MTTF, are evaluated quantitatively in different wireless communication schemes. To observe the gain in overall reliability improvement, the reliability importance of imperfect components are also evaluated. The results show that the failure parameters of different components take different effects on MTTF & reliability importance. With different expected working time of a system, the focus of reliability improvement should change from one component to another in order to receive the highest reliability gain. Furthermore, the number of engaged components during a communication state is more critical than the number of system states. For simplicity, we assume that the wired & wireless communication links are perfect, and omit them in the reliability analysis. If these two are engaged into the proposed end-to-end expected instantaneous reliability, it can give a more detailed & complete reliability assessment of a wireless network system. Our quantitative measurements are conducted as an example with the assumption that the failure & service rate are constant; however, in practice, failure & service processes may follow other distributions. After all, our investigation provides an initial yet overall approach to measure the reliability of wireless networks. Although our analysis is conducted on wireless CORBA platforms, it is easily extensible to generic wireless network systems.","Wireless communication,
Telecommunication network reliability,
Wireless networks,
Intelligent networks,
Access protocols,
Mobile computing,
Bridges,
Internet,
User-generated content,
Computer science"
A novel framework for energy - conserving data gathering in wireless sensor networks,"Achievable energy conservation rate in wireless sensor networks can further be enhanced depending on application-specific requirements such as data reporting latency and sensing coverage area. In this paper, we propose a novel framework for energy-conserving data gathering which exploits a trade-off between coverage and data reporting latency. The ultimate goal is to extend the network lifetime by offering only the application/user-specific quality of service (e.g., sensing coverage) in each data reporting round using (approximately) k sensors. The selection of these k sensors is based on a geometric probability theory and a randomization technique with constant computational complexity. The selected k sensors form a data gathering tree (DGT) rooted at the data gathering point. To improve on energy-savings, only the sensors on the DGT are scheduled to remain active (with transceiver on) during a reporting round so that the DGT is used as a backbone to reach the data gathering point. We present a probabilistic model for estimating the connectivity of the selected k sensors and also a recursive algorithm which derives the number of additional sensors required to probabilistically guarantee the connectivity. The immediate data reporting capability of sensors is also analyzed since the group of active sensors forming a DGT (backbone) in each round may not be able to serve as a dominating set. Simulation results show that the proposed framework leads to a significant conservation of energy with a small trade-off.",
Oscillator Models and Collective Motion: Splay State Stabilization of Self-Propelled Particles,"This paper presents a Lyapunov design for the stabilization of collective motion in a planar kinematic model of N particles moving at constant speed. We derive a control law that achieves asymptotic stability of the splay state formation, characterized by uniform rotation of N evenly spaced particles on a circle. In designing the control law, the particle headings are treated as a system of coupled phase oscillators. The coupling function which exponentially stabilizes the splay state of particle phases is combined with a decentralized beacon control law that stabilizes circular motion of the particles.",
A fully automated calibration method for an optical see-through head-mounted operating microscope with variable zoom and focus,"Ever since the development of the first applications in image-guided therapy (IGT), the use of head-mounted displays (HMDs) was considered an important extension of existing IGT technologies. Several approaches to utilizing HMDs and modified medical devices for augmented reality (AR) visualization were implemented. These approaches include video-see through systems, semitransparent mirrors, modified endoscopes, and modified operating microscopes. Common to all these devices is the fact that a precise calibration between the display and three-dimensional coordinates in the patient's frame of reference is compulsory. In optical see-through devices based on complex optical systems such as operating microscopes or operating binoculars-as in the case of the system presented in this paper-this procedure can become increasingly difficult since precise camera calibration for every focus and zoom position is required. We present a method for fully automatic calibration of the operating binocular Varioscope/spl trade/ M5 AR for the full range of zoom and focus settings available. Our method uses a special calibration pattern, a linear guide driven by a stepping motor, and special calibration software. The overlay error in the calibration plane was found to be 0.14-0.91 mm, which is less than 1% of the field of view. Using the motorized calibration rig as presented in the paper, we were also able to assess the dynamic latency when viewing augmentation graphics on a mobile target; spatial displacement due to latency was found to be in the range of 1.1-2.8 mm maximum, the disparity between the true object and its computed overlay represented latency of 0.1 s. We conclude that the automatic calibration method presented in this paper is sufficient in terms of accuracy and time requirements for standard uses of optical see-through systems in a clinical environment.",
A registration framework for the comparison of mammogram sequences,"In this paper, we present a two-stage algorithm for mammogram registration, the geometrical alignment of mammogram sequences. The rationale behind this paper stems from the intrinsic difficulties in comparing mammogram sequences. Mammogram comparison is a valuable tool in national breast screening programs as well as in frequent monitoring and hormone replacement therapy (HRT). The method presented in this paper aims to improve mammogram comparison by estimating the underlying geometric transformation for any mammogram sequence. It takes into consideration the various temporal changes that may occur between successive scans of the same woman and is designed to overcome the inconsistencies of mammogram image formation.","Breast,
Biomedical imaging,
Image registration,
Medical diagnostic imaging,
Biomedical engineering,
Cancer,
Monitoring,
Biochemistry,
Medical treatment,
Image processing"
Process algebras for quantitative analysis,"In the 1980s process algebras became widely accepted formalisms for describing and analysing concurrency. Extensions of the formalisms, incorporating some aspects of systems which had previously been abstracted, were developed for a number of different purposes. In the area of performance analysis models must quantify both timing and probability. Addressing this domain led to the formulation of stochastic process algebras. In this paper we give a brief overview of stochastic process algebras and the problems which motivated them, before focussing on their relationship with the underlying mathematical stochastic process. This is presented in the context of the PEPA formalism.","Algebra,
Performance analysis,
Stochastic processes,
Sparse matrices,
Power system modeling,
Timing,
Markov processes,
Informatics,
Concurrent computing,
Context"
Structural metadata research in the EARS program,"Both human and automatic processing of speech require recognition of more than just words. In this paper we provide a brief overview of research on structural metadata extraction in the DARPA EARS rich transcription program. Tasks include detection of sentence boundaries, filler words, and disfluencies. Modeling approaches combine lexical, prosodic, and syntactic information, using various modeling techniques for knowledge source integration. The performance of these methods is evaluated by task, by data source (broadcast news versus spontaneous telephone conversations) and by whether transcriptions come from humans or from an (errorful) automatic speech recognizer. A representative sample of results shows that combining multiple knowledge sources (words, prosody, syntactic information) is helpful, that prosody is more helpful for news speech than for conversational speech, that word errors significantly impact performance, and that discriminative models generally provide benefit over maximum likelihood models. Important remaining issues, both technical and programmatic, are also discussed.",
A cognitive trust-based approach for Web service discovery and selection,"Provision of services within a virtual framework for resource sharing across institutional boundaries has become an active research area. Many such services encode access to computational and data resources, comprising single machines to computational clusters. Such services can also be informational, and integrate different resources within an institution. Consequently, we envision a service rich environment in the future, where service consumers are represented by intelligent agents. If interaction between agents is automated, it is necessary for these agents to be able to automatically discover services and choose between a set of equivalent (or similar) services. In such a scenario trust serves as a benchmark to differentiate between services. In this paper we introduce a novel framework for automated service discovery and selection of Web Services based on a user's trust policy. The framework is validated by a case study of data mining Web Services and is evaluated by an empirical experiment.","Web services,
Data mining,
Intelligent agent,
Computer science,
Resource management,
Collaborative work,
Online Communities/Technical Collaboration,
Web and internet services"
Learner-centered web-based instruction in software engineering,"During the past several years, there has been a growing demand for skilled software engineers, a demand that, unfortunately, has not been satisfied, resulting in a variety of problems for the discipline. This work presents a new approach called learner-centered Web-based instruction in software engineering that can be used to educate skilled engineers. The approach is based on three ideas. First, software engineering education must become more realistic. Second, software engineering education has to move closer to the learner. Finally, it must take advantage of the Web since this technology has the power of being a unique tool for implementing change in education. This work reports and discusses the results from the evaluation of the approach.","Computer aided instruction,
Software engineering,
Object oriented methods,
Internet,
Computer science education"
Learning and identifying haptic icons under workload,"This work addresses the use of vibrotactile haptic feedback to transmit background information with variable intrusiveness, when recipients are engrossed in a primary visual and/or auditory task. We describe two studies designed to (a) perceptually optimize a set of vibrotactile ""icons"" and (b) evaluate users' ability to identify them in the presence of varying degrees of workload. Seven icons learned in approximately 3 minutes were each typically identified within 2.5 s and at 95% accuracy in the absence of workload.","Haptic interfaces,
Feedback,
Protocols,
Auditory displays,
Collaborative work,
Manufacturing,
Vehicles,
Context awareness,
Testing,
Computer science"
Towards complete generic camera calibration,"We consider the problem of calibrating a highly generic imaging model, that consists of a non-parametric association of a projection ray in 3D to every pixel in an image. Previous calibration approaches for this model do not seem to be directly applicable for cameras with large fields of view and non-central cameras. In this paper, we describe a complete calibration approach that should in principle be able to handle any camera that can be described by the generic imaging model. Initial calibration is performed using multiple-images of overlapping calibration grids simultaneously. This is then improved using pose estimation and bundle adjustment-type algorithms. The approach has been applied on a wide variety of central and non-central cameras including fisheye lens, catadioptric cameras with spherical and hyperbolic mirrors, and multi-camera setups. We also consider the question if non-central models are more appropriate for certain cameras than central models.",
Small parity-check erasure codes - exploration and observations,"Erasure codes have profound uses in wide- and medium-area storage applications. While infinite-size codes have been developed with optimal properties, there remains a need to develop small codes with optimal properties. In this paper, we provide a framework for exploring very small codes, and we use this framework to derive optimal and near-optimal ones for discrete numbers of data bits and coding bits. These codes have heretofore been unknown and unpublished, and should be useful in practice. We also use our exploration to make observations about upper bounds for these codes, in order to gain a better understanding of them and to spur future derivations of larger, optimal and near-optimal codes.",
A controlled experiment assessing test case prioritization techniques via mutation faults,"Regression testing is an important part of software maintenance, but it can also be very expensive. To reduce this expense, software testers may prioritize their test cases so that those that are more important are run earlier in the regression testing process. Previous work has shown that prioritization can improve a test suite's rate of fault detection, but the assessment of prioritization techniques has been limited to hand-seeded faults, primarily due to the belief that such faults are more realistic than automatically generated (mutation) faults. A recent empirical study, however, suggests that mutation faults can be representative of real faults. We have therefore designed and performed a controlled experiment to assess the ability of prioritization techniques to improve the rate of fault detection techniques, measured relative to mutation faults. Our results show that prioritization can be effective relative to the faults considered, and they expose ways in which that effectiveness can vary with characteristics of faults and test suites. We also compare our results to those collected earlier with respect to the relationship between hand-seeded faults and mutation faults, and the implications this has for researchers performing empirical studies of prioritization.","Computer aided software engineering,
Genetic mutations,
Software testing,
Fault detection,
Software maintenance,
System testing,
Performance evaluation,
Computer science,
Automatic testing,
Automatic control"
A novel method for early software quality prediction based on support vector machine,"The software development process imposes major impacts on the quality of software at every development stage; therefore, a common goal of each software development phase concerns how to improve software quality. Software quality prediction thus aims to evaluate software quality level periodically and to indicate software quality problems early. In this paper, we propose a novel technique to predict software quality by adopting support vector machine (SVM) in the classification of software modules based on complexity metrics. Because only limited information of software complexity metrics is available in early software life cycle, ordinary software quality models cannot make good predictions generally. It is well known that SVM generalizes well even in high dimensional spaces under small training sample conditions. We consequently propose a SVM-based software classification model, whose characteristic is appropriate for early software quality predictions when only a small number of sample data are available. Experimental results with a medical imaging system software metrics data show that our SVM prediction model achieves better software quality prediction than some commonly used software quality prediction models","Software quality,
Support vector machines,
Predictive models,
Support vector machine classification,
Software metrics,
Computer science,
Programming,
Biomedical imaging,
Software systems,
Testing"
"High-performance computing: clusters, constellations, MPPs, and future directions","In a recent paper, Gordon Bell and Jim Gray (2002) put forth a view of the past, present, and future of high-performance computing (HPC) that is both insightful and thought provoking. Identifying key trends with a grace and candor rarely encountered in a single work, the authors describe an evolutionary past drawn from their vast experience and project an enticing and compelling vision of HPC's future. Yet, the underlying assumptions implicit in their treatment, particularly those related to terminology and dominant trends, conflict with our own experience, common practices, and shared view of HPCs future directions. Taken from our vantage points of the Top500 list,"" the Lawrence Berkeley National Laboratory NERSC computer center, Beowulf-class computing, and research in petaflops-scale computing architectures, we offer an alternate perspective on several key issues in the form of a constructive counterpoint. One objective of this article is to restore the strength and value of the term ""cluster"" by degeneralizing its applicability to a restricted subset of parallel computers. We'll further consider this class in conjunction with its complementing terms constellation, Beowulf class, and massively parallel processing systems (MPPs), based on the classification used by the Top500 list, which has tracked the HPC field for more than a decade.","Computer architecture,
Parallel processing,
Management training,
Supercomputers,
Concurrent computing,
Workstations,
Microprocessors,
Lifting equipment,
Data engineering,
Power engineering and energy"
On finding energy-minimizing paths on terrains,"We discuss the problem of computing optimal paths on terrains for a mobile robot, where the cost of a path is defined to be the energy expended due to both friction and gravity. The physical model used by this problem allows for ranges of impermissible traversal directions caused by overturn danger or power limitations. The model is interesting and challenging, as it incorporates constraints found in realistic situations, and these constraints affect the computation of optimal paths. We give some upper- and lower-bound results on the combinatorial size of optimal paths on terrains under this model. With some additional assumptions, we present an efficient approximation algorithm that computes for two given points a path whose cost is within a user-defined relative error ratio. Compared with previous results using the same approach, this algorithm improves the time complexity by using 1) a discretization with reduced size, and 2) an improved discrete algorithm for finding optimal paths in the discretization. We present some experimental results to demonstrate the efficiency of our algorithm. We also provide a similar discretization for a more difficult variant of the problem due to less restricted assumptions.","Mobile robots,
Path planning,
Sun,
Friction,
Gravity,
Costs,
Rivers,
Military computing,
Computer science,
Energy loss"
Handling asymmetry in gain in directional antenna equipped ad hoc networks,"The deployment of traditional higher layer protocols (especially the IEEE 802.11 MAC protocol at the MAC layer) with directional antennae could lead to problems from an increased number of collisions; this effect is primarily seen due to three specific effects: (i) an increase in the number of hidden terminals; (ii) the problem of deafness and, (iii) a difficulty in determining the locations of neighbors. In this work we propose a new MAC protocol that incorporates circular RTS and CTS transmissions. We show that the circular transmission of the control messages helps avoid collisions of both DATA and ACK packets from hidden terminals. Our protocol intelligently determines the directions in which the control messages ought to be transmitted so as to eliminate redundant transmissions in any given direction. We perform extensive simulations and analyze the obtained results in order to compare our scheme with previously proposed protocols that have been proposed for use in directional antenna equipped ad hoc networks. Our simulation results clearly demonstrate the benefits of incorporating both circular RTS and CTS messages in terms of the achieved aggregate throughput","Intelligent networks,
Directional antennas,
Ad hoc networks,
Media Access Protocol,
Directive antennas,
Deafness,
Throughput,
Computer science,
Analytical models,
Performance analysis"
First results from the high-resolution mouseSPECT annular scintillation camera,"High-resolution single-photon emission computed tomography (SPECT) imaging in small animals tends to use long imaging times and large injected doses due to the poor sensitivity of single pinhole gamma cameras. To increase sensitivity while maintaining spatial resolution, we designed and constructed a multi-pinhole collimator array to replace the parallel hole collimators of a Ceraspect human SPECT brain scanner. The Ceraspect scanner is composed of an annular NaI(Tl) crystal within which the eight pinhole collimators (1-mm-diameter holes) rotate while projecting nonoverlapping images of the object onto the stationary annular crystal. In this manner, only one-eighth of a collimator rotation is required to acquire a full circle orbit tomographic data set. The imaging field of view (FOV) has a diameter of 25.6 mm in the transverse direction, which is sufficient to encompass a mouse in the transverse direction. The axial FOV is 25.6 mm at the center of the FOV and 13.9 mm at the edge of the transverse FOV. Data are currently acquired in step-and-shoot mode; however, the system is capable of list mode acquisition with the collimator continuously rotating. Images are reconstructed using a cone-beam ordered subsets expectation maximization method. The reconstructed spatial resolution of the system is 1.7 mm and the sensitivity at the center of the FOV is 13.8 cps/microCi. A whole-body bone scan of a mouse injected with [Tc-99 m]MDP clearly revealed skeletal structures such as the ribs and vertebral bodies. These preliminary results suggest that this approach is a good tradeoff between resolution and sensitivity and, with further refinement, may permit dynamic imaging in living animals.",
Orthogonal neighborhood preserving projections,"Orthogonal neighborhood preserving projections (ONPP) is a linear dimensionality reduction technique which attempts to preserve both the intrinsic neighborhood geometry of the data samples and the global geometry. The proposed technique constructs a weighted data graph where the weights are constructed in a data-driven fashion, similarly to locally linear embedding (LLE). A major difference with the standard LLE where the mapping between the input and the reduced spaces is implicit, is that ONPP employs an explicit linear mapping between the two. As a result, and in contrast with LLE, handling new data samples becomes straightforward, as this amounts to a simple linear transformation. ONPP shares some of the properties of locality preserving projections (LPP). Both ONPP and LPP rely on a k-nearest neighbor graph in order to capture the data topology. However, our algorithm inherits the characteristics of LLE in preserving the structure of local neighborhoods, while LPP aims at preserving only locality without specifically aiming at preserving the geometric structure. This feature makes ONPP an effective method for data visualization. We provide ample experimental evidence to demonstrate the advantageous characteristics of ONPP, using well known synthetic test cases as well as real life data from computational biology and computer vision.","Principal component analysis,
Topology,
Computational geometry,
Data visualization,
Computer vision,
Data mining,
Computer science,
Data engineering,
Life testing,
Computational biology"
SIOX: simple interactive object extraction in still images,"The following article presents an approach for interactive foreground extraction in still images that is currently being integrated into the GIMP. The presented approach has been derived from color signatures, a technique originating from image retrieval. The article explains the algorithm and presents some benchmark results to show the improvements in speed and accuracy compared to state of the art solutions. The article also describes how the algorithm can easily be adapted for video segmentation tasks.",
Speculative Markov blanket discovery for optimal feature selection,"In this paper we address the problem of learning the Markov blanket of a quantity from data in an efficient manner Markov blanket discovery can be used in the feature selection problem to find an optimal set of features for classification tasks, and is a frequently-used preprocessing phase in data mining, especially for high-dimensional domains. Our contribution is a novel algorithm for the induction of Markov blankets from data, called Fast-IAMB, that employs a heuristic to quickly recover the Markov blanket. Empirical results show that Fast-IAMB performs in many cases faster and more reliably than existing algorithms without adversely affecting the accuracy of the recovered Markov blankets.","Bayesian methods,
Computer science,
Data mining,
Cancer,
Reliability engineering,
Equations,
Calcium,
Lung neoplasms,
Graphical models"
TEEVE: the next generation architecture for tele-immersive environments,"Tele-immersive 3D multi-camera room environments are starting to emerge and with them new challenging research questions. One important question is how to organize the large amount of visual data, being captured, processed, transmitted and displayed, and their corresponding resources, over current COTS computing and networking infrastructures so that ""everybody"" would be able to install and use tele-immersive environments for conferencing and other activities. In this paper, we propose a novel cross-layer control and streaming framework over general purpose delivery infrastructure, called TEEVE (tele-immersive environments for everybody). TEEVE aims for effective and adaptive coordination, synchronization, and soft QoS-enabled delivery of tele-immersive visual streams to remote room(s). The TEEVE experiments between two tele-immersive rooms residing in different institutions more than 2000 miles apart show that we can sustain communication of up to 12 3D video streams with 4/spl sim/5 3D frames per second for each stream, yielding 4/spl sim/5 tele-immersive video rate.",
Resource-aware conference key establishment for heterogeneous networks,"The Diffie-Hellman problem is often the basis for establishing conference keys. In heterogeneous networks, many conferences have participants of varying resources, yet most conference keying schemes do not address this concern and place the same burden upon less powerful clients as more powerful ones. The establishment of conference keys should minimize the burden placed on resource-limited users while ensuring that the entire group can establish the key. In this paper, we present a hierarchical conference keying scheme that forms subgroup keys for successively larger subgroups en route to establishing the group key. A tree, called the conference tree, governs the order in which subgroup keys are formed. Key establishment schemes that consider users with varying costs or budgets are built by appropriately designing the conference tree. We then examine the scenario where users have both varying costs and budget constraints. A greedy algorithm is presented that achieves near-optimal performance, and requires significantly less computational effort than finding the optimal solution. We provide a comparison of the total cost of tree-based conference keying schemes against several existing schemes, and introduce a new performance criterion, the probability of establishing the session key (PESKY), to study the likelihood that a conference key can be established in the presence of budget constraints. Simulations show that the likelihood of forming a group key using a tree-based conference keying scheme is higher than the GDH schemes of Steiner et al.. Finally, we study the effect that greedy users have upon the Huffman-based conference keying scheme, and present a method to mitigate the detrimental effects of the greedy users upon the total cost.",
Teaching digital design to computing science students in a single academic term,"How should digital design be taught to computing science students in a single one-semester course? This work advocates the use of state-of-the-art design tools and programmable devices and presents a series of laboratory exercises to help students learn digital logic. Each exercise introduces new concepts and produces the complete design of a stand-alone apparatus that is fun and interesting to use. These exercises lead to the most challenging capstone designs for a single-semester course of which the authors are aware. Fast progress is made possible by providing students with predesigned input/output modules. Student feedback demonstrates that the students approve of this methodology. An extensive set of slides, supporting teaching material, and laboratory exercises are freely available for downloading.",
Use of web-based materials to teach electric circuit theory,"Beginning fall 2001, the Electrical Engineering Department at Texas A&M University, College Station, significantly altered the instructional philosophy of the ELEN 214 Electric Circuit Theory course by introducing more engineering design into the curriculum and adopting the WebCT-based interactive homework submission system. This paper will discuss the use of the Quiz tool within WebCT for the construction of question banks and their publication to a WebCT server. An example, deriving mathematical expressions, which describes electric circuit behavior and helps customize the homework problems to each individual student, are discussed. In other words, in a class of 200 students, each student is presented with an individual homework assignment with a unique set of problems not repeated to anyone else via WebCT. A help desk staffed by senior undergraduates assists the course students in completing the WebCT-based homework on time. WebCT is an essential ingredient in the delivery of the course. The approach presented in this paper can be adapted to any other course in engineering/science that involves mathematical calculations. So far, the course evaluations suggest that the students are more motivated and excited about electrical and computer engineering as a career.","Internet,
Computer aided instruction,
Electrical engineering education,
Circuits"
An approach to constructing feature models based on requirements clustering,"Feature models have been widely adopted in software reuse to organize the requirements of a set of similar applications in a software domain/product line. However, in most feature-oriented methods, the construction of feature models heavily depends on the domain analysts' personal understanding, and the work of constructing feature models from the original requirements of sample applications is often tedious and ineffective. This paper proposes a semiautomatic approach to constructing feature models based on requirements clustering, which automates the activities of feature identification, organization and variability modeling to a great extent. The underlying idea of this approach is to analyze the relationships between individual requirements and cluster tight-related requirements into features. With the automatic support of this approach, good quality feature models can be constructed in a more effective way. A case study is also provided to show the feasibility of this approach.","Application software,
Software quality,
Context modeling,
Computer science,
Productivity,
Unified modeling language,
Packaging"
Empowerment: a universal agent-centric measure of control,"The classical approach to using utility functions suffers from the drawback of having to design and tweak the functions on a case by case basis. Inspired by examples from the animal kingdom, social sciences and games we propose empowerment, a rather universal function, defined as the information-theoretic capacity of an agent's actuation channel. The concept applies to any sensorimotor apparatus. Empowerment as a measure reflects the properties of the apparatus as long as they are observable due to the coupling of sensors and actuators via the environment. Using two simple experiments we also demonstrate how empowerment influences sensor-actuator evolution",
Motivation for Variable Length Intervals and Hierarchical Phase Behavior,"Most programs are repetitive, where similar behavior can be seen at different execution times. Proposed algorithms automatically group similar portions of a program's execution into phases, where the intervals in each phase have homogeneous behavior and similar resource requirements. These prior techniques focus on fixed length intervals (such as a hundred million instructions) to find phase behavior. Fixed length intervals can make a program's periodic phase behavior difficult to find, because the fixed interval length can be out of sync with the period of the program's actual phase behavior. In addition, a fixed interval length can only express one level of phase behavior. In this paper, we graphically show that there exists a hierarchy of phase behavior in programs and motivate the need for variable length intervals. We describe the changes applied to SimPoint to support variable length intervals. We finally conclude by providing an initial study into using variable length intervals to guide SimPoint",
A probabilistic approach to coordinated multi-robot indoor surveillance,In this paper we discuss the problem of monitoring and searching an indoor environment for an intruder with a group of mobile robots. We present a graph-based algorithm to coordinate a group of robots which takes the limitations and uncertainties of sensors into account and is able to find good coordination plans efficiently even for large environments. We analyze and compare the approach against other coordination strategies based on a new probabilistic framework that allows to evaluate the performance of any coordination strategy based on a probabilistic sensor model and a worst case behavior model for intruders. Using this framework we demonstrate the capabilities of the planning algorithm in several simulation experiments.,
Chirp imaging vibro-acoustography for removing the ultrasound standing wave artifact,"Vibro-acoustography (VA) is an imaging technique that uses the dynamic (oscillatory) radiation force of two continuous-wave (CW) ultrasound to image objects at low frequency (within the kHz range). In this technique, the dynamic radiation force is created by means of a confocused transducer emitting two ultrasound beams at slightly-shifted frequencies f/sub 1/ and f/sub 2/=f/sub 1/+/spl Delta/f. It has been demonstrated previously that high-resolution images of various types of inclusions and tissues can be obtained using this technique. However, if the targeted object reflects ultrasound directly back to the transducer, standing waves are produced that result in an artifact in the VA image. The goal of this study is to remove the standing wave artifact and improve VA images by means of a new process called chirp imaging. The procedure consists of sweeping the frequencies of the primary ultrasound beams in a selected bandwidth while keeping /spl Delta/f constant during the sweep. The chirp image is produced by averaging the amplitude of the acoustic emission produced during the sweep. Vibro-acoustography chirp imaging experiments are performed on a stainless-steel sphere attached to a latex sheet in a tank of degassed water. The resulting chirp images demonstrate remarkable reduction of the standing wave artifact compared to the ""fixed frequency"" VA images.",
Linear-time encodable/decodable codes with near-optimal rate,"We present an explicit construction of linear-time encodable and decodable codes of rate r which can correct a fraction (1-r-/spl epsiv/)/2 of errors over an alphabet of constant size depending only on /spl epsiv/, for every 00. The error-correction performance of these codes is optimal as seen by the Singleton bound (these are ""near-MDS"" codes). Such near-MDS linear-time codes were known for the decoding from erasures; our construction generalizes this to handle errors as well. Concatenating these codes with good, constant-sized binary codes gives a construction of linear-time binary codes which meet the Zyablov bound, and also the more general Blokh-Zyablov bound (by resorting to multilevel concatenation). Our work also yields linear-time encodable/decodable codes which match Forney's error exponent for concatenated codes for communication over the binary symmetric channel. The encoding/decoding complexity was quadratic in Forney's result, and Forney's bound has remained the best constructive error exponent for almost 40 years now. In summary, our results match the performance of the previously known explicit constructions of codes that had polynomial time encoding and decoding, but in addition have linear-time encoding and decoding algorithms.",
Evolutionary policy iteration for solving Markov decision processes,"We propose a novel algorithm called evolutionary policy iteration (EPI) for solving infinite horizon discounted reward Markov decision processes. EPI inherits the spirit of policy iteration but eliminates the need to maximize over the entire action space in the policy improvement step, so it should be most effective for problems with very large action spaces. EPI iteratively generates a ""population"" or a set of policies such that the performance of the ""elite policy"" for a population monotonically improves with respect to a defined fitness function. EPI converges with probability one to a population whose elite policy is an optimal policy. EPI is naturally parallelizable and along this discussion, a distributed variant of PI is also studied.",
A novel approach to proactive human-robot cooperation,"This paper introduces the concept of proactive execution of robot tasks in the context of human-robot cooperation with uncertain knowledge of the human's intentions. We present a system architecture that defines the necessary modules of the robot and their interactions with each other. The two key modules are the intention recognition that determines the human user's intentions and the planner that executes the appropriate tasks based on those intentions. We show how planning conflicts due to the uncertainty of the intention information are resolved by proactive execution of the corresponding task that optimally reduces the system's uncertainly. Finally, we present an algorithm for selecting this task and suggest a benchmark scenario.",
Post-verification debugging of hierarchical designs,"As VLSI designs grow in complexity and size, errors become more frequent and difficult to track. Recent developments have automated most of the verification tasks but debugging still remains a resource-intensive, manually conducted procedure. This paper bridges this gap as it develops robust automated debugging methodologies that complement verification processes. Unlike prior debugging techniques, the proposed one exploits the hierarchical nature of modern designs to improve the performance and quality of debugging. It also formulates the problem in terms of Quantified Boolean Formula Satisfiability to obtain dramatic reduction in memory requirements, which allows for debugging of large designs. Extensive experiments conducted on industrial and benchmark designs confirm the efficiency and practicality of the proposed approach.","Debugging,
Hardware design languages,
Very large scale integration,
Microprocessors,
Circuits,
Computer errors,
Computer science,
Engines,
Bridges,
Robustness"
Research challenges of autonomic computing,"Autonomic computing is a grand-challenge vision in which computing systems manage themselves in accordance with high-level objectives specified by humans. The IT industry recognizes that meeting this challenge is imperative; otherwise, IT systems will soon become virtually impossible to administer. But meeting this challenge is also extremely difficult, and requires a worldwide collaboration among the best minds of academia and industry. In the hope of motivating researchers in relevant areas to apply their expertise to this vitally important problem, the author outlines some of the main scientific and engineering challenges that collectively make up the grand challenge of autonomic computing, and provide pointers to initial efforts to address these challenges.","Computer vision,
Management information systems,
Application software,
Computer industry,
Humans,
Distributed computing,
Artificial intelligence,
Computer science,
Collaboration,
Computer applications"
Algorithms for fast concurrent reconfiguration of hexagonal metamorphic robots,"The problem addressed is the distributed reconfiguration of a system of hexagonal metamorphic robots (modules) from an initial straight chain to a goal configuration that satisfies a simple admissibility condition. Our reconfiguration strategy depends on finding a contiguous path of cells that spans the goal configuration and over which modules can move concurrently without collision or deadlock, called an admissible substrate path. A subset of modules first occupy the admissible substrate path, which is then traversed by other modules to fill in the remainder of the goal. We present a two-phase reconfiguration strategy, beginning with a centralized preprocessing phase that finds and heuristically ranks all admissible substrate paths in the goal configuration, according to which path is likely to result in fast parallel reconfiguration. We prove the correctness of our path-finding algorithm and demonstrate its effectiveness through simulation. The second phase of reconfiguration is accomplished by a deterministic, distributed algorithm that uses little or no intermodule message passing.",
Link-layer jamming attacks on S-MAC,"We argue that among denial-of-service (DoS) attacks, link-layer jamming is a more attractive option to attackers than radio jamming is. By exploiting the semantics of the link-layer protocol (aka MAC protocol), an attacker can achieve better efficiency than blindly jamming the radio signals alone. In this paper, we investigate some jamming attacks on S-MAC, the level of effectiveness and efficiency the attacks can potentially achieve and a countermeasure that can be implemented against one of these attacks.",
Enhancing Multiprocessor Architecture Simulation Speed Using Matched-Pair Comparison,"While cycle-level, full-system architecture simulation tools are capable of estimating performance at arbitrary accuracy, the time to simulate an entire application is usually prohibitive. Moreover, simulating multi-threaded applications further exacerbates this problem as most simulation tools are single-threaded. Recently, statistical sampling techniques, such as SMARTS, have managed to bring down the simulation time significantly by making it possible to only simulate about 1% of the code with sufficient accuracy. However, thousands of simulation points throughout the benchmark must still be simulated. First of all, we propose to use the well-established statistical method matched-pair comparison and motivate why this will bring down the number of simulation points needed to achieve a given accuracy. We apply it to single-processor as well as multiprocessor simulation and show that it is capable of reducing the number of needed simulation points by one order of magnitude. Secondly, since we apply the technique to single- as well as multiprocessors, we study for the first time the efficiency of statistical sampling techniques in multiprocessor systems to establish a baseline to compare with. We show theoretically and confirm experimentally, that while the instruction throughput vary significantly on each individual processor, the variability of instruction throughput across processors in a multiprocessor system decreases as we increase the number of processors for some important workloads. Thus, a factor of P fewer simulation points, where P is the number of processors, are needed to begin with when sampling is applied to multiprocessors",
Piecewise linear branch prediction,"Improved branch prediction accuracy is essential to sustaining instruction throughput with today's deep pipelines. We introduce piecewise linear branch prediction, an idealized branch predictor that develops a set of linear functions, one for each program path to the branch to be predicted, that separate predicted taken from predicted not taken branches. Taken together, all of these linear functions form a piecewise linear decision surface. We present a limit study of this predictor showing its potential to greatly improve predictor accuracy. We then introduce a practical implementable branch predictor based on piecewise linear branch prediction. In making our predictor practical, we show how a parameterized version of it unifies the previously distinct concepts of perceptron prediction and path-based neural prediction. Our new branch predictor has implementation costs comparable to current prominent predictors in the literature while significantly improving accuracy. For a deeply pipelined simulated microarchitecture our predictor with a 256 KB hardware budget improves the harmonic mean normalized instructions-per-cycle rate by 8% over both the original path-based neural predictor and 2Bc-gskew. The average misprediction rate is decreased by 16% over the path-based neural predictor and by 22% over 2Bc-gskew.",
The effect of virtual channel organization on the performance of interconnection networks,"Most of previous studies have assessed the performance issues for regular buffer and virtual channel organizations and have not considered overall buffer size constraint. In this paper, the performance of mesh-based interconnection networks (mesh, torus and hypercube networks) under different traffic patterns (uniform, hotspot, and matrix-transpose) is studied. We investigate the effect of the number of virtual channels and their buffer lengths, on the performance of these topologies when the total buffer size associated to each physical channel (and thus router buffer size) is fixed. The results show that the optimal number of virtual channels and buffer length highly depends on the traffic pattern assumed.",
Defending against Sybil attacks in sensor networks,"Sybil attack is a harmful threat to sensor networks, in which a malicious node illegally forges an unbounded number of identities to defeat redundancy mechanisms. Digital certificates are a way to prove identities. However, they are not viable in sensor networks. In this paper, we propose a light-weight identity certificate method to defeat Sybil attacks. Our proposed method uses one-way key chains and Merkle hash trees. The method thereby avoids the need for public key cryptography. In addition, the method provides a means for authentication of all data messages. A variant of this method is presented that has lower computational requirements under certain conditions. The security of each method is analyzed, and is as good or better than previously-proposed approaches, with fewer assumptions. The overhead (computation, storage, and messages) is also shown to be acceptable for use in sensor networks.","Intelligent networks,
Peer to peer computing,
Redundancy,
Authentication,
Military computing,
Monitoring,
Computer networks,
Identity-based encryption,
Laboratories,
Computer science"
Scheduling processor voltage and frequency in server and cluster systems,"Modern server farm and cluster sites consume large quantities of energy both to power and cool the machines in the site. At the same time, less power supply redundancy is offered and power companies and government officials are requesting power consumption be reduced during certain time periods. These trends lead to the requirement of responding to rapid reductions in the maximum power the site may consume. Each possible solution must respond to the new power budget before a cascading failure occurs. Available techniques include powering down some nodes or slowing all nodes in a system uniformly. This work instead examines the feasibility of slowing nodes non-uniformly in response to their performance demands. This approach provides an opportunity to reduce the performance loss caused by a reduction in the power budget. This paper uses the execution characteristics of the work currently running on each processor of the system or cluster to predict the performance of the work at the available frequency settings. The scheduling mechanism selects the lowest frequency for the processor that provides essentially all of the available performance of the work. It ensures that the frequency fits within the available global power budget and, if not, reduces it so that it does. The paper demonstrates the approach using a simple, synthetic benchmark and then validates it using additional, real-world applications.","Processor scheduling,
Energy consumption,
Portable computers,
Cooling,
Dynamic voltage scaling,
Frequency diversity,
Computer science,
Laboratories,
Power supplies,
Government"
Background Subtraction Using Markov Thresholds,"Many video surveillance and identification applications need to find moving objects in the field of view of a stationary camera. A popular method for obtaining these silhouettes is through the process of background subtraction. We present a novel method for comparing image frames to the model of the stationary background that exploits the spatial and temporal dependencies that objects in motion impose on their images. We achieve this through the development and use of Markov random fields of binary segmentation variates. We show that the MRF approach produces more accurate and visually appealing silhouettes that are less prone to noise and background camouflaging effects than traditional per-pixel based methods. Results include visual examination of silhouettes, comparisons against hand-segmented data, and an analysis of the effects of various silhouette extraction techniques on gait recognition performance.","Hidden Markov models,
Image segmentation,
Video surveillance,
Markov random fields,
Computer science,
Artificial intelligence,
Laboratories,
Application software,
Smart cameras,
Background noise"
On the interaction between overlay routing and underlay routing,"In this paper, we study the interaction between overlay routing and traffic engineering (TE) in a single autonomous system (AS). We formulate this interaction as a two-player non-cooperative non-zero sum game, where the overlay tries to minimize the delay of its traffic and the TE's objective is to minimize network cost. We study a Nash routing game with best-reply dynamics, in which the overlay and TE have equal status, and take turns to compute their optimal strategies based on the response of the other player in the previous round. We prove the existence, uniqueness and global stability of Nash equilibrium point (NEP) for a simple network. For general networks, we show that the selfish behavior of an overlay can cause huge cost increases and oscillations to the whole network. Even worse, we have identified cases, both analytically and experimentally, where the overlay's cost increases as the Nash routing game proceeds even though the overlay plays optimally based on TE's routing at each round. Experiments are performed to verify our analysis.","Routing,
Tellurium,
Telecommunication traffic,
Costs,
Application software,
Physics computing,
Multiprotocol label switching,
Computer science,
Stability,
Nash equilibrium"
A functional quantum programming language,"We introduce the language QML, a functional language for quantum computations on finite types. Its design is guided by its categorical semantics: QML programs are interpreted by morphisms in the category FQC of finite quantum computations, which provides a constructive semantics of irreversible quantum computations realisable as quantum gates. QML integrates reversible and irreversible quantum computations in one language, using first order strict linear logic to make weakenings explicit. Strict programs are free from decoherence and hence preserve superpositions and entanglement -which is essential for quantum parallelism.","Computer languages,
Quantum computing,
Quantum entanglement,
Circuits,
Parallel processing,
Computer science,
Logic programming,
Parallel programming,
Functional programming,
Program processors"
On service provisioning under a scheduled traffic model in reconfigurable WDM optical networks,"In this paper, we propose a general scheduled traffic model, sliding scheduled traffic model. In this model, the setup time t/sub s/ of a demand whose holding time is T time units is not known in advance. Rather t/sub s/ is allowed to begin in a pre-specified time window [l,T] subject to the constraint that l/spl les/t/sub s//spl les/r-T. We then consider two problems: (1) how to properly place a demand within its associated time window to reduce overlapping in time among a set of demands; and (2) route and assign wavelengths (RWA) to a set of demands under the proposed sliding scheduled traffic model in mesh reconfigurable WDM optical networks without wavelength conversion. In addition, we consider how to rearrange a demand by negotiating a new setup time that minimizes the demand schedule change in case that the demand is blocked. To maximize temporal resource reuse, we propose a demand time conflict reduction algorithm to solve the first problem. Two algorithms, window based RWA algorithm and traffic matrix based RWA algorithm, are then proposed for the second problem. We compare the proposed RWA algorithms against a customized tabu search scheme. Simulation results show that the proposed demand time conflict reduction algorithm can resolve well over 50% of time conflicts and the space-time RWA algorithms are effective in satisfying demand requirements and minimizing total network resources used, d.",
Design and Evaluation of an Autonomic Workflow Engine,"In this paper we present the design and evaluate the performance of an autonomic workflow execution engine. Although there exist many distributed workflow engines, in practice, it remains a difficult problem to deploy such systems in an optimal configuration. Furthermore, when facing an unpredictable workload with high variability, manual reconfiguration is not an option. Thanks to its autonomic controller, the engine features self-configuration, self-tuning and self-healing properties. The engine runs on a cluster of computers using a tuple space to coordinate its various components. Its autonomic controller monitors its performance and responds to workload variations by altering the configuration. In case failures occur, the controller can recover the workflow execution state from persistent storage and migrate it to a different node of the cluster. Such interventions are carried out without any human supervision. As part of the results of our performance evaluation, we compare different autonomic control strategies and discuss how they can automatically tune the system","Engines,
Scalability,
Automatic control,
Distributed computing,
Computer science,
Computer displays,
Humans,
Control systems,
Workflow management software,
Automation"
A heuristic approach to schedule periodic real-time tasks on reconfigurable hardware,"This paper deals with scheduling periodic real-time tasks on reconfigurable hardware devices, such as FPGAs. Reconfigurable hardware devices are increasingly used in embedded systems. To utilize these devices also for systems with real-time constraints, predictable task scheduling is required. We formalize the periodic task scheduling problem and propose two preemptive scheduling algorithms. The first is an adaption of the well-known earliest deadline first (EDF) technique to the FPGA execution model. Although the algorithm reveals good scheduling performance, it lacks an efficient schedulability test and requires a high number of FPGA configurations. The second algorithm uses the concept of servers that reserve area and execution time for other tasks. Tasks are successively merged into servers, which are then scheduled sequentially. While this method is inferior to the EDF-based technique regarding schedulability, it comes with a fast schedulability test and greatly reduces the number of required FPGA configurations.","Hardware,
Field programmable gate arrays,
Computer science,
Time factors,
Testing,
Telecommunication computing,
Logic programming,
Processor scheduling,
Control systems,
Telecommunication control"
A generic attack on checksumming-based software tamper resistance,"Self-checking software tamper resistance mechanisms employing checksums, including advanced systems as recently proposed by Chang and Atallah (2002) and Horne et al. (2002) have been promoted as an alternative to other software integrity verification techniques. Appealing aspects include the promise of being able to verify the integrity of software independent of the external support environment, as well as the ability to automatically integrate checksumming code during program compilation or linking. In this paper we show that the rich functionality of many modern processors, including UltraSparc and x86-compatible processors, facilitates automated attacks which defeat such checksumming by self-checking programs.",
Zonal rumor routing for wireless sensor networks,"Routing algorithms for wireless sensor networks are constrained by power, memory and computational resources. Rumor routing algorithm for sensor-networks spreads the information of an event to other nodes in the network, thus enabling queries to discover paths to the events. Zonal rumor routing (ZRR) is an extension to the rumor routing algorithm. ZRR algorithm enables the rumors to spread to a larger part of the network with high energy efficiency by partitioning the network into zones. New algorithm improves the percentage query delivery and requires fewer transmissions, thus reducing the total energy consumption in a sensor network. Rumor routing can be considered as a special case of ZRR when only one node belongs to one zone. The performance of ZRR, the effect of zone size, and the scalability of the algorithm are evaluated using simulations.","Routing,
Wireless sensor networks,
Partitioning algorithms,
Energy consumption,
Energy efficiency,
Monitoring,
Event detection,
Computer science,
Relays,
Computer networks"
Developing and assuring trustworthy Web services,"Web services are emerging technologies that are changing the way we develop and use computer systems and software. Current Web services testing techniques are unable to assure the desired level of trustworthiness, which presents a barrier to WS applications in mission and business critical environments. This paper presents a framework that assures the trustworthiness of Web services. New assurance techniques are developed within the framework, including specification verification via completeness and consistency checking, specification refinement, distributed Web services development, test case generation, and automated Web services testing. Traditional test case generation methods only generate positive test cases that verify the functionality of software. The Swiss cheese test case generation method proposed in this paper is designed to perform both positive and negative testing that also reveal the vulnerability of Web services. This integrated development process is implemented in a case study. The experimental evaluation demonstrates the effectiveness of this approach. It also reveals that the Swiss cheese negative testing detects even more faults than positive testing and thus significantly reduces the vulnerability of Web services.",
Noise stability of functions with low influences: Invariance and optimality,"In this paper, we study functions with low influences on product probability spaces. The analysis of Boolean functions f {-1, 1}/sup n/ /spl rarr/ {-1, 1} with low influences has become a central problem in discrete Fourier analysis. It is motivated by fundamental questions arising from the construction of probabilistically checkable proofs in theoretical computer science and from problems in the theory of social choice in economics. We prove an invariance principle for multilinear polynomials with low influences and bounded degree; it shows that under mild conditions the distribution of such polynomials is essentially invariant for all product spaces. Ours is one of the very few known non-linear invariance principles. It has the advantage that its proof is simple and that the error bounds are explicit. We also show that the assumption of bounded degree can be eliminated if the polynomials are slightly ""smoothed""; this extension is essential for our applications to ""noise stability ""-type problems. In particular; as applications of the invariance principle we prove two conjectures: the ""Majority Is Stablest"" conjecture [29] from theoretical computer science, which was the original motivation for this work, and the ""It Ain't Over Till It's Over"" conjecture [27] from social choice theory. The ""Majority Is Stablest"" conjecture and its generalizations proven here, in conjunction with the ""Unique Games Conjecture"" and its variants, imply a number of (optimal) inapproximability results for graph problems.",
Store vulnerability window (SVW): re-execution filtering for enhanced load optimization,"The load-store unit is a performance critical component of a dynamically-scheduled processor. It is also a complex and non-scalable component. Several recently proposed techniques use some form of speculation to simplify the load-store unit and check this speculation by re-executing some of the loads prior to commit. We call such techniques load optimizations. One recent load optimization improves load queue (LQ) scalability by using re-execution rather than associative search to check speculative intra- and inter- thread memory ordering. A second technique improves store queue (SQ) scalability by speculatively filtering some load accesses and some store entries from it and re-executing loads to check that speculation. A third technique speculatively removes redundant loads from the execution engine; re-execution detects false eliminations. Unfortunately, the benefits of a load optimization are often mitigated by re-execution itself Re-execution contends for cache bandwidth with store commit, and serializes load re-execution with subsequent store com-mit. If a given load optimization requires a sufficient number of load re-executions, the aggregate re-execution cost may overwhelm the benefits of the technique entirely and even cause drastic slowdowns. Store vulnerability window (SVW) is a new mechanism that significantly reduces the re-execution requirements of a given load optimization. SVW is based on monotonic store sequence numbering and an adaptation of Bloom filtering. The cost of a typical SVW implementation is a 1KB buffer and a 16-bit field per LQ entry. Across the three optimizations we study, SVW reduces re-executions by an average of 85%. This reduction relieves cache port contention and removes many of the dynamic serialization events that contribute the bulk of re-execution's cost, allows these load optimizations to perform up to their full potential. For the speculative SQ, this means the chance to perform at all, as without SVW it posts significant slowdowns.",
WS-attestation: efficient and fine-grained remote attestation on Web services,"This paper proposes WS-attestation, attestation architecture on Web services framework. We aim at providing software oriented, dynamic and fine-grained attestation mechanism that leverages TCG technologies to increase trust and confidence in integrity reporting. In addition, the architecture allows efficient binding of attestation with application context, privacy protection, as well as infrastructural support for attestation validation.",
Learning Sensor Network Topology through Monte Carlo Expectation Maximization,"We consider the problem of inferring sensor positions and a topological (i.e. qualitative) map of an environment given a set of cameras with non-overlapping fields of view. In this way, without prior knowledge of the environment nor the exact position of sensors within the environment, one can infer the topology of the environment, and common traffic patterns within it. In particular, we consider sensors stationed at the junctions of the hallways of a large building. We infer the sensor connectivity graph and the travel times between sensors (and hence the hallway topology) from the sequence of events caused by unlabeled agents (i.e. people) passing within view of the different sensors. We do this based on a first-order semi-Markov model of the agent's behavior. The paper describes a problem formulation and proposes a stochastic algorithm for its solution. The result of the algorithm is a probabilistic model of the sensor network connectivity graph and the underlying traffic patterns. We conclude with results from numerical simulations","Network topology,
Monte Carlo methods,
Intelligent sensors,
Layout,
Computer science,
Educational institutions,
Smart cameras,
Stochastic processes,
Numerical simulation,
Signal detection"
MobiREAL simulator-evaluating MANET applications in real environments,"In this paper, we propose a probabilistic rule-based model to describe behavior of mobile nodes for accurately evaluating performance of MANET applications. The proposed model allows us to describe how mobile nodes change their destinations, routes and speeds/directions based on their positions, surroundings (e.g. neighboring nodes), information obtained from applications, and so on. We have designed and developed a network simulator called MobiREAL based on the proposed methodology. Through experiments, we show importance to simulate MANET applications in realistic environments.",
On the Survivability of Routing Protocols in Ad Hoc Wireless Networks,"Survivable routing protocols are able to provide service in the presence of attacks and failures. The strongest attacks that protocols can experience are attacks where adversaries have full control of a number of authenticated nodes that behave arbitrarily to disrupt the network, also referred to as Byzantine attacks. This work examines the survivability of ad hoc wireless routing protocols in the presence of several Byzantine attacks: black holes, flood rushing, wormholes and overlay network wormholes. Traditional secure routing protocols that assume authenticated nodes can always be trusted, fail to defend against such attacks. Our protocol, ODSBR, is an on-demand wireless routing protocol able to provide correct service in the presence of failures and Byzantine attacks. We demonstrate through simulation its effectiveness in mitigating such attacks. Our analysis of the impact of these attacks versus the adversaryâ€™s effort gives insights into their relative strengths, their interaction and their importance when designing wireless routing protocols.","Routing protocols,
Intelligent networks,
Wireless networks,
Communication system security,
Computer science,
Portable computers,
Authentication,
Floods,
Wireless application protocol,
Computational modeling"
The challenge of volunteer computing with lengthy climate model simulations,"This paper describes the issues confronted by the climateprediction.net project in creating a volunteer computing project using a large legacy climate model application. This application typically takes from weeks to months to complete one simulation, and has large memory and disk usage requirements. We describe issues in porting the climate model to a single-processor PC platform, checkpointing, computer resources and workunit size for a simulation, and the volunteer computing infrastructures used (a project-specific system and BOINC). We also describe the methods used to obtain and retain users, and examine the retention/attrition rate of users running the lengthy modelling simulations","Computational modeling,
Object oriented modeling,
Atmospheric modeling,
Application software,
Computer simulation,
Grid computing,
Computer applications,
Operating systems,
Physics,
Laboratories"
"Teaching reconfigurable systems: methods, tools, tutorials, and projects","This paper presents an approach that has been used for teaching disciplines on reconfigurable computing and advanced digital systems, which are intended to cover such topics as architectures and capabilities of field-programmable logic devices; languages for the specification, modeling, and synthesis of digital systems; design methods; computer-aided design tools; reconfiguration techniques; and practical applications. To assist the educational process, the following units have been developed and employed in the pedagogical practice: animated tutorials, miniprojects, hardware templates, and course-oriented library of digital circuits. To stimulate the student's activity, an optional project-based evaluation technique has been applied. All the materials that are required for students are available on the university website (WebCT) and can easily be used for studying inside the university, for obtaining additional information during practical classes and for distance learning.","Reconfigurable architectures,
Computer science education,
Design automation,
Field programmable gate arrays,
Specification languages"
Distributed Stream Management using Utility-Driven Self-Adaptive Middleware,"We consider pervasive computing applications that process and aggregate data-streams emanating from highly distributed data sources to produce a stream of updates that have an implicit business-value. Middleware that enables such aggregation of data-streams must support scalable and efficient self-management to deal with changes in the operating conditions and should have an embedded business-sense. In this paper, we present a novel self-adaptation algorithm that has been designed to scale efficiently for thousands of streams and aims to maximize the overall business utility attained from running middleware-based applications. The outcome is that the middleware not only deals with changing network conditions or resource requirements, but also responds appropriately to changes in business policies. An important feature of the algorithm is a hierarchical node-partitioning scheme that decentralizes reconfiguration and suitably localizes its impact. Extensive simulation experiments and benchmarks attained with actual enterprise operational data corroborate this paper's claims","Middleware,
Management information systems,
Aggregates,
Humans,
Bandwidth,
Delay,
Availability,
Resource management,
Computer science,
Pervasive computing"
A model-driven approach for specifying semantic Web services,"The semantic Web promises automated invocation, discovery, and composition of Web services by enhancing services with semantic descriptions. One such language used for creating semantic descriptions is the Web ontology language or OWL. An upper ontology for Web services called OWL-S has been created to provide a mechanism for describing service semantics in a standard, well-defined manner. Unfortunately, the learning curve for semantic-rich description languages such as OWL-S can be steep, especially with given the current state of tool support for the language. This paper describes an automated software tool that uses model-driven architecture (MDA) techniques to generate an OWL-S description of a Web service from a UML model. This allows' the developer to focus on creating a model of the Web service in a standard UML tool, leveraging existing knowledge.","Semantic Web,
Web services,
Unified modeling language,
Ontologies,
XML,
OWL,
Computer architecture,
Standards development,
Computer science,
Software tools"
rDCF: a relay-enabled medium access control protocol for wireless ad hoc networks,"It is well known that IEEE 802.11 provides a physical layer multi-rate capability, and hence MAC layer mechanisms are needed to exploit this capability. Several solutions have been proposed to achieve this goal. However, these solutions only consider how to exploit good channel quality for the direct link between the sender and the receiver. Since IEEE 802.11 supports multiple transmission rates in response to different channel conditions, data packets may be delivered faster through a relay node than through the direct link if the direct link has low quality and low rate. In this paper, we propose a novel MAC layer relay-enabled distributed coordination function (DCF) protocol, called rDCF, to further exploit the physical layer multi-rate capability. We design a protocol to assist the sender, the relay node and the receiver to reach an agreement on which data rate to use and whether to transmit the data through a relay node. Considering various issues such as bandwidth utilization and channel errors, we propose techniques to further improve the performance of rDCF. Simulation results show that rDCF can significantly improve the system performance when the channel quality of the direct link is poor.",
Robust face alignment based on local texture classifiers,"We propose a robust face alignment algorithm with a novel discriminative local texture model. Different from the conventional descriptive PCA local texture model in ASM, classifiers using LUT-type Haar-like features are trained from a large data set as local texture model. The strong discriminative power of the classifier greatly improves the accuracy and robustness of local searching on faces with expression variation and ambiguous contours. A Bayesian framework is configured for shape parameter optimization and the algorithm is implemented in a hierarchical structure for efficiency. Extensive experiments are reported to show its accuracy and robustness.","Robustness,
Shape,
Principal component analysis,
Active appearance model,
Bayesian methods,
Face detection,
Artificial intelligence,
Computer science,
Laboratories,
Electronic mail"
Sufficient Conditions for the Existence of Zeno Behavior,"In this paper, sufficient conditions for the existence of Zeno behavior in a class of hybrid systems are given; these are the first sufficient conditions on Zeno of which the authors are aware for hybrid systems with nontrivial dynamics. This is achieved by considering a class of hybrid systems termed diagonal first quadrant (DFQ) hybrid systems. When the underlying graph of a DFQ hybrid system has a cycle, we can construct an in finite execution for this system when the vector fields on each domain satisfy certain assumptions. To this execution, we can associate a single discrete time dynamical system that describes its continuous evolution. Therefore, we reduce the study of executions of DFQ hybrid systems to the study of a single discrete time dynamical system. We obtain sufficient conditions for the existence of Zeno by determining when this discrete time dynamical system is exponentially stable.",
Improving connectivity of wireless ad hoc networks,"A fully connected network topology is critical to many fundamental operations in wireless ad hoc networks. We study the problem of deploying additional wireless nodes to improve the connectivity of an existing wireless network. Given a disconnected network, we consider the connectivity improvement (CI) problem, i.e., how to deploy as few additional nodes as possible so that the augmented network is connected. We first prove the NP-completeness of CI, and then present a Delaunay triangulation-based algorithm, connectivity improvement using Delaunay triangulation (CIDT). We discuss several variations of CIDT, and propose two additional optimization techniques to further improve the performance. Finally, we demonstrate the effectiveness of CIDT, and compare the performance of its variations via J-Sim simulation.","Ad hoc networks,
Wireless sensor networks,
Mobile ad hoc networks,
Wireless networks,
Mobile communication,
Network topology,
Vehicle dynamics,
Unmanned aerial vehicles,
Computer science,
Interference"
The globus extensible input/output system (XIO): a protocol independent IO system for the grid,"In distributed heterogeneous grid environments the protocols used to exchange bits are crucial. As researchers work hard to discover the best new protocol for the grid, application developers struggle with ways to use these new protocols. A stable, consistent, and intuitive framework is needed to aid in the implementation and use of these protocols. While the application must not be burdened with the protocol details some of it may need to be exposed to take advantage of potential optimizations. In this paper we examine how the Globus XIO API provides this framework. We explore the performance implications of using this abstraction layer and the benefits gained in application as well as protocol development.",
Noise-shaping techniques applied to switched-capacitor voltage regulators,A delta-sigma control loop for a buck-boost dc-dc converter with fractional gains is presented. This technique reduces the tones caused by the traditional pulse-frequency modulation regulation. The prototype regulator was fabricated in a 0.72-/spl mu/m CMOS process and clocked at 1 MHz. It achieved suppression of tones up to 55 dB in the 0-500-kHz range. The input voltage range was 3-5 V. The output voltage ranged from 1.8 to 4 V for load currents up to 150 mA.,
Motion Layer Based Object Removal in Videos,"This paper proposes a novel method to generate plausible video sequences after removing relatively large objects from the original videos. In order to maintain temporal coherence among the frames, a motion layer segmentation method is applied. Then, a set of synthesized layers are generated by applying motion compensation and region completion algorithm. Finally, a new video, in which the selected object is removed, is plausibly rendered given the synthesized layers and the motion parameters. A number of example videos are shown in the results to demonstrate the effectiveness of our method",
Wavelet coding of volumetric medical images for high throughput and operability,"This paper presents a new three-dimensional (3-D) wavelet-based scalable lossless coding scheme for compression of volumetric medical images. Aiming to improve the productivity of radiologists and the cost-effectiveness of the system, we strive to achieve high decoder throughput, random access to coded data volume, progressive transmission, and high compression ratio in a balanced design approach. These desirable functionalities are realized by a modified 3-D dyadic wavelet transform tailored to volumetric medical images and an optimized Rice code of very low complexity.",
Globally optimal estimates for geometric reconstruction problems,"We introduce a framework for computing statistically optimal estimates of geometric reconstruction problems. While traditional algorithms often suffer from either local minima or nonoptimality - or a combination of both - we pursue the goal of achieving global solutions of the statistically optimal cost-function. Our approach is based on a hierarchy of convex relaxations to solve nonconvex optimization problems with polynomials. These convex relaxations generate a monotone sequence of lower bounds and we show how one can detect whether the global optimum is attained at a given relaxation. The technique is applied to a number of classical vision problems: triangulation, camera pose, homography estimation and last, but not least, epipolar geometry estimation. Experimental validation on both synthetic and real data is provided. In practice, only a few relaxations are needed for attaining the global optimum",
Self-organizing maps for learning the edit costs in graph matching,"Although graph matching and graph edit distance computation have become areas of intensive research recently, the automatic inference of the cost of edit operations has remained an open problem. In the present paper, we address the issue of learning graph edit distance cost functions for numerically labeled graphs from a corpus of sample graphs. We propose a system of self-organizing maps (SOMs) that represent the distance measuring spaces of node and edge labels. Our learning process is based on the concept of self-organization. It adapts the edit costs in such a way that the similarity of graphs from the same class is increased, whereas the similarity of graphs from different classes decreases. The learning procedure is demonstrated on two different applications involving line drawing graphs and graphs representing diatoms, respectively.",
Managing Model Quality in UML-Based Software Development,"With the advent of UML and MDA, models play an increasingly important role in software development. Hence, the management of the quality of models is of key importance for completing projects succesfully. However, existing approaches towards software quality focus on the implementation and execution of systems. These existing quality models cannot be straightforwardly mapped to the domain of UML models as source code and models differ in several essential ways (level of abstraction, precision, completeness and consistency). In this paper we present a quality model for managing UML-based software development. This model enables identifying the need for actions for quality improvement already in early stages of the life-cycle. Early actions for quality improvement are less resource intensive and, hence, less cost intensive than later actions. We discuss our experiences in applying the quality model to several industrial case studies. Finally we present a tool that visualizes our quality model. This tool helps in relating management level quality data to detailed data about specific quality subcharacteristics","Quality management,
Software development management,
Programming,
Unified modeling language,
Software quality,
Costs,
Testing,
Mathematics,
Computer science,
Project management"
Automatic Identification of Home Pages on the Web,"The research reported in this paper is the first phase of a larger project on the automatic classification of Web pages by their genres. The long term goal is the incorporation of web page genre into the search process to improve the quality of the search results. In this phase, a neural net classifier was trained to distinguish home pages from non-home pages and to classify those home pages as personal home page, corporate home page or organization home page. Results indicate that the classifier is able to distinguish home pages from non-home pages and within the home page genre it is able to distinguish personal from corporate home pages. Organization home pages, however, were more difficult to distinguish from personal and corporate home pages.","Web pages,
Neural networks,
Web sites,
Society home pages,
Computer science,
Search engines,
Machine learning"
dPAM: a distributed prefetching protocol for scalable asynchronous multicast in P2P systems,"We leverage the buffering capabilities of end-systems to achieve scalable, asynchronous delivery of streams in a peer-to-peer environment. Unlike existing cache-and-relay schemes, we propose a distributed prefetching protocol where peers prefetch and store portions of the streaming media ahead of their playout time, thus not only turning themselves to possible sources for other peers but their prefetched data can allow them to overcome the departure of their source-peer. This stands in sharp contrast to existing cache-and-relay schemes where the departure of the source-peer forces its peer children to go the original server, thus disrupting their service and increasing server and network load. Through mathematical analysis and simulations, we show the effectiveness of maintaining such asynchronous multicasts from several source-peers to other children peers, and the efficacy of prefetching in the face of peer departures. We confirm the scalability of our dPAM protocol as it is shown to significantly reduce server load.","Multicast protocols,
Prefetching,
Peer to peer computing,
Feeds,
Network servers,
Computer science,
Distributed computing,
Time sharing computer systems,
Data engineering,
Buffer storage"
Asymmetric subsethood-product fuzzy neural inference system (ASuPFuNIS),"This work presents an asymmetric subsethood-product fuzzy neural inference system (ASuPFuNIS) that directly extends the SuPFuNIS model by permitting signal and weight fuzzy sets to be modeled by asymmetric Gaussian membership functions. The asymmetric subsethood-product network admits both numeric as well as linguistic inputs. Input nodes, which act as tunable feature fuzzifiers, fuzzify numeric inputs with asymmetric Gaussian fuzzy sets; and linguistic inputs are presented as is. The antecedent and consequent labels of standard fuzzy if-then rules are represented as asymmetric Gaussian fuzzy connection weights of the network. The model uses mutual subsethood based activation spread and a product aggregation operator that works in conjunction with volume defuzzification in a gradient descent learning framework. Despite the increase in the number of free parameters, the proposed model performs better than SuPFuNIS, on various benchmarking problems, both in terms of the performance accuracy and architectural economy and compares excellently with other various existing models with a performance better than most of them.","Fuzzy systems,
Fuzzy sets,
Fuzzy neural networks,
Government,
Inference algorithms,
Multilayer perceptrons,
Councils,
Physics,
Computer science,
Computer networks"
Accelerating Multiprocessor Simulation with a Memory Timestamp Record,"We introduce a fast and accurate technique for initializing the directory and cache state of a multiprocessor system based on a novel software structure called the memory timestamp record (MTR). The MTR is a versatile, compressed snapshot of memory reference patterns which can be rapidly updated during fast-forwarded simulation, or stored as part of a checkpoint. We evaluate MTR using a full-system simulation of a directory-based cache-coherent multiprocessor running a range of multithreaded workloads. Both MTR and a multiprocessor version of functional fast-forwarding (FFW) make similar performance estimates, usually within 15% of our detailed model. In addition to other benefits, we show that MTR has up to a 1.45x speedup over FFW, and a 7.7x speedup over our detailed baseline",
Adaptive algorithms for finding replacement services in autonomic distributed business processes,"Web service may be used to construct autonomic business processes, where several Web services interact with each other to carry out complex transactions or workflows. During the execution of an autonomic process, if one component service fails or becomes overloaded, a mechanism is needed to ensure that the running process is not interrupted and the failed service is quickly and efficiently replaced. In this paper, we present two algorithms to solve the problem. The first algorithm uses the backup path approach so that the predecessor of a failed service may quickly switch to a predefined backup path. The second algorithm uses the replacement path approach to re-construct a new process by skipping a failed service. All these dynamic adaptations can be done by business process itself or a QoS broker which is part of an autonomic system. The simulation result shows that, when producing the information needed for dynamic adaptation, the running time of business process composition increases only by a constant factor regardless of the system size.",
Correction of bias field in MR images using singularity function analysis,"A new approach for correcting bias field in magnetic resonance (MR) images is proposed using the mathematical model of singularity function analysis (SFA), which represents a discrete signal or its spectrum as a weighted sum of singularity functions. Through this model, an MR image's low spatial frequency components corrupted by a smoothly varying bias field are first removed, and then reconstructed from its higher spatial frequency components not polluted by bias field. The thus reconstructed image is then used to estimate bias field for final image correction. The approach does not rely on the assumption that anatomical information in MR images occurs at higher spatial frequencies than bias field. The performance of this approach is evaluated using both simulated and real clinical MR images.","Image analysis,
Magnetic analysis,
Signal analysis,
Magnetic resonance,
Image reconstruction,
Image segmentation,
Radio frequency,
Iterative methods,
Spline,
Polynomials"
Compiler-directed instruction duplication for soft error detection,"We experiment with compiler-directed instruction duplication to detect soft errors in VLIW datapaths. In the proposed approach, the compiler determines the instruction schedule by balancing the permissible performance degradation with the required degree of duplication. Our experimental results show that our algorithms allow the designer to perform tradeoff analysis between performance and reliability.",
Audio/visual mapping with cross-modal hidden Markov models,"The audio/visual mapping problem of speech-driven facial animation has intrigued researchers for years. Recent research efforts have demonstrated that hidden Markov model (HMM) techniques, which have been applied successfully to the problem of speech recognition, could achieve a similar level of success in audio/visual mapping problems. A number of HMM-based methods have been proposed and shown to be effective by the respective designers, but it is yet unclear how these techniques compare to each other on a common test bed. In this paper, we quantitatively compare three recently proposed cross-modal HMM methods, namely the remapping HMM (R-HMM), the least-mean-squared HMM (LMS-HMM), and HMM inversion (HMMI). The objective of our comparison is not only to highlight the merits and demerits of different mapping designs, but also to study the optimality of the acoustic representation and HMM structure for the purpose of speech-driven facial animation. This paper presents a brief overview of these models, followed by an analysis of their mapping capabilities on a synthetic dataset. An empirical comparison on an experimental audio-visual dataset consisting of 75 TIMIT sentences is finally presented. Our results show that HMMI provides the best performance, both on synthetic and experimental audio-visual data.","Hidden Markov models,
Facial animation,
Streaming media,
Computer science,
Natural languages,
Speech recognition,
Testing,
Speech processing,
Man machine systems"
Personal and contextual requirements engineering,"A framework for requirements analysis is proposed that accounts for individual and personal goals and the effect of time and context on personal requirements. The implications of the framework on system architecture are considered as three implementation pathways: functional specifications, development of customisable features and automatic adaptation by the system. These pathways imply the need to analyse system architecture requirements. Different implementation pathways have cost-benefit implications for stakeholders, so cost-benefit analysis techniques are proposed to assess tradeoffs between goals and implementation strategies. The use of the framework is illustrated with two case studies in assistive technology domains: e-mail and a personalised navigation system.","Taxonomy,
Context,
Human computer interaction,
Monitoring,
Space technology,
Cultural differences,
Informatics,
Information science,
Educational institutions,
Computer science education"
Hybrid real-coded genetic algorithms with female and male differentiation,"Parent-centric real-parameter crossover operators create the offspring in the neighborhood of one of the parents, the female parent, using a probability distribution. The other parent, the male one, defines the range of this probability distribution. The female and male differentiation process determines the individuals in the population that may become female or/and male parents. An important property of this process is that it makes possible the design of two kinds of real-coded genetic algorithms: ones that promote global search and ones that are effective local searchers. In this paper, we study the performance of a hybridization of these real-coded genetic algorithms when tackling the test problems proposed for the Special Session on Real-Parameter Optimization of the IEEE Congress on Evolutionary Computation 2005","Genetic algorithms,
Probability distribution,
Testing,
Biological cells,
Noise measurement,
Computer science,
Evolutionary computation,
Algorithm design and analysis,
Sampling methods,
Application software"
On constructing the minimum orthogonal convex polygon for the fault-tolerant routing in 2-D faulty meshes,"The rectangular faulty block model is the most commonly used fault model for designing fault-tolerant, and deadlock-free routing algorithms in mesh-connected multicomputers. The convexity of a rectangle facilitates simple, efficient ways to route messages around fault regions using relatively few or no virtual channels to avoid deadlock. However, such a faulty block may include many nonfaulty nodes which are disabled, i.e., they are not involved in the routing process. Therefore, it is important to define a fault region that is convex, and at the same time, to include a minimum number of nonfaulty nodes. In this paper, we propose an optimal solution that can quickly construct a set of minimum faulty polygons, called orthogonal convex polygons, from a given set of faulty blocks in a 2-D mesh (or 2-D torus). The formation of orthogonal convex polygons is implemented using either a centralized, or distributed solution. Both solutions are based on the formation of faulty components, each of which consists of adjacent faulty nodes only, followed by the addition of a minimum number of nonfaulty nodes to make each component a convex polygon. Extensive simulation has been done to determine the number of nonfaulty nodes included in the polygon, and the result obtained is compared with the best existing known result. Results show that the proposed approach can not only find a set of minimum faulty polygons, but also does so quickly in terms of the number of rounds in the distributed solution.",
Neural network adaptive control for nonlinear nonnegative dynamical systems,"Nonnegative and compartmental dynamical system models are derived from mass and energy balance considerations that involve dynamic states whose values are nonnegative. These models are widespread in engineering and life sciences and typically involve the exchange of nonnegative quantities between subsystems or compartments wherein each compartment is assumed to be kinetically homogeneous. In this paper, we develop a full-state feedback neural adaptive control framework for adaptive set-point regulation of nonlinear uncertain nonnegative and compartmental systems. The proposed framework is Lyapunov-based and guarantees ultimate boundedness of the error signals corresponding to the physical system states and the neural network weighting gains. In addition, the neural adaptive controller guarantees that the physical system states remain in the nonnegative orthant of the state-space for nonnegative initial conditions.","Neural networks,
Adaptive control,
Control systems,
Aerospace engineering,
Programmable control,
Marine technology,
Biomedical engineering,
Biological materials,
Power engineering and energy,
State feedback"
A statistical model for writer verification,"A statistical model for determining whether a pair of documents, a known and a questioned, were written by the same individual is proposed. The model has the following four components: (i) discriminating elements, e.g., global features and characters, are extracted from each document; (ii) differences between corresponding elements from each document are computed; (iii) using conditional probability estimates of each difference, the log-likelihood ratio (LLR) is computed for the hypotheses that the documents were written by the same or different writers; the conditional probability estimates themselves are determined from labeled samples using either Gaussian or gamma estimates for the differences assuming their statistical independence; and (iv) distributions of the LLRs for same and different writer LLRs are analyzed to calibrate the strength of evidence into a standard nine-point scale used by questioned document examiners. The model is illustrated with experimental results for a specific set of discriminating elements.","Distributed computing,
Probability,
Writing,
Principal component analysis,
Parameter estimation,
Computer science,
Computational modeling,
Gray-scale,
Entropy,
Text analysis"
Understanding motivations for Internet use in distance education,"Uses and Gratifications (U & G) is a communications theory paradigm developed to understand media-use motivations. This research paradigm has recently been applied to understand motivations for Internet use. Internet U & G typically orient to distinct process-based, content-based, and socially based motivations for use of the network. This study applies U & G to examine the Internet usage motivations of technology students enrolled in an Internet-enabled distance education course and finds that digital content is highly sought after by students in Internet-supported distance education classes. Distance education students are also motivated to use Internet communication resources to offset the lack of social interaction found in normal classrooms. Students' Internet usage process motivations actually diverge into two distinct areas, related generally to searching versus browsing in the support of learning objectives.",
Classifiability-based omnivariate decision trees,"Top-down induction of decision trees is a simple and powerful method of pattern classification. In a decision tree, each node partitions the available patterns into two or more sets. New nodes are created to handle each of the resulting partitions and the process continues. A node is considered terminal if it satisfies some stopping criteria (for example, purity, i.e., all patterns at the node are from a single class). Decision trees may be univariate, linear multivariate, or nonlinear multivariate depending on whether a single attribute, a linear function of all the attributes, or a nonlinear function of all the attributes is used for the partitioning at each node of the decision tree. Though nonlinear multivariate decision trees are the most powerful, they are more susceptible to the risks of overfitting. In this paper, we propose to perform model selection at each decision node to build omnivariate decision trees. The model selection is done using a novel classifiability measure that captures the possible sources of misclassification with relative ease and is able to accurately reflect the complexity of the subproblem at each node. The proposed approach is fast and does not suffer from as high a computational burden as that incurred by typical model selection algorithms. Empirical results over 26 data sets indicate that our approach is faster and achieves better classification accuracy compared to statistical model select algorithms.","Classification tree analysis,
Decision trees,
Pattern classification,
Partitioning algorithms,
Machine vision,
Pattern recognition,
Laboratories,
Computer science,
Neural networks,
Power generation"
Leave-One-Out Bounds for Support Vector Regression Model Selection,"Minimizing bounds of leave-one-out errors is an important and efficient approach for support vector machine (SVM) model selection. Past research focuses on their use for classification but not regression. In this letter, we derive various leave-one-out bounds for support vector regression (SVR) and discuss the difference from those for classification. Experiments demonstrate that the proposed bounds are competitive with Bayesian SVR for parameter selection. We also discuss the differentiability of leave-one-out bounds.",
Separating transparent layers of repetitive dynamic behaviors,"In this paper, we present an approach for separating two transparent layers of complex nonrigid scene dynamics. The dynamics in one of the layers is assumed to be repetitive, while the other can have any arbitrary dynamics. Such repetitive dynamics includes, among other, human actions in video (e.g., a walking person), or a repetitive musical tune in audio signals. We use a global to local space time alignment approach to detect and align the repetitive behavior. Once aligned, a median operator applied to space time derivatives is used to recover the intrinsic repeating behavior, and separate it from the other transparent layer. We show results on synthetic and real video sequences. In addition, we show the applicability of our approach to separating mixed audio signals (from a single source).","Legged locomotion,
Video sequences,
Layout,
Humans,
Motion estimation,
Computer science,
Face detection,
Face recognition,
Image segmentation,
Feature extraction"
Hierarchical data dissemination scheme for large scale sensor networks,"In this paper, we propose energy efficient schemes for data dissemination in sensor networks. Our schemes are based on a hierarchical data dissemination model. The hierarchical data dissemination scheme (HDDS) leverages the fact that a nonuniform grid can be constructed to disseminate data based on sink density without any prior information about the position of the sinks. This approach seems to be promising as preliminary experimental results show that an average energy savings of 10-30% can be achieved using our schemes. Even with large number of sources and sinks, the schemes scale well yielding high throughput.. Experimental results show FDDD is suitable for applications which require higher throughput and minimum latency; fully distinct data dissemination for networks with limited energy consumption but can withstand some delay and loss of data, and limited sharing data dissemination is suitable for applications with large number of sources and sinks and average energy consumption and delay.",
Rapid generation of thermal-safe test schedules,"Overheating has been acknowledged as a major issue in testing complex SoC. Several power constrained system-level DFT solutions (power constrained test scheduling) have recently been proposed to tackle this problem. However as is shown in this paper imposing a chip-level maximum power constraint does not necessarily avoid local overheating due to the nonuniform distribution of power across the chip. This paper proposes a new approach for dealing with overheating during test, by embedding thermal awareness into test scheduling. The proposed approach facilitates rapid generation of thermal-safe test schedules without requiring time-consuming thermal simulations. This is achieved by employing a low-complexity test session thermal model used to guide the test schedule generation algorithm. This approach reduces the chances of a design re-spin due to potential overheating during test.","Job shop scheduling,
System testing,
Scheduling algorithm,
Power distribution,
Processor scheduling,
Power engineering and energy,
Electronic equipment testing,
Power dissipation,
Concurrent computing,
Computer science"
Group key distribution via local collaboration in wireless sensor networks,,"Collaboration,
Intelligent networks,
Wireless sensor networks,
Broadcasting,
Cryptography,
Fellows,
Sensor phenomena and characterization,
Information security,
Computer science,
Computer network management"
Graphical-model-based morphometric analysis,"We propose a novel method for voxel-based morphometry (VBM), which we call graphical-model-based morphometric analysis (GAMMA), to identify morphological abnormalities automatically, and to find complex probabilistic associations among voxels in magnetic-resonance images and clinical variables. GAMMA is a fully automatic, nonparametric morphometric-analysis algorithm, with high sensitivity and specificity. It uses a Bayesian network to represent the associations among voxels and the function variable, and uses a contextual-clustering method based on a Markov random field to find clusters in which all voxels have similar associations with the function variable. We use loopy belief propagation to infer the unobserved label field and belief map. As opposed to voxel-based morphometric methods based on general linear models, GAMMA is capable of identifying nonlinear associations among the function variable and voxels. Compared with our previous approach, a Bayesian morphometry algorithm, GAMMA has greater sensitivity, specificity, and computational efficiency.","Bayesian methods,
Markov random fields,
Belief propagation,
Analysis of variance,
Atrophy,
Magnetic analysis,
Image analysis,
Sensitivity and specificity,
Parametric statistics,
Change detection algorithms"
Increasing Web Service Dependability Through Consensus Voting,"This paper demonstrates our Web Service based N-Version model, WS-FTM (Web Service-Fault Tolerance Mechanism), which applies this well proven technique to the domain of Web Services to increase system dependability. WS-FTM achieves transparent usage of replicated Web Services by use of a modified stub. The stub is created using tools included in WS-FTM. Our initial implementation includes a simple consensus voter that allows generic result comparison. Finally we show, through the use of a non-trivial example, that WS-FTM can be used to increase the reliability of a Web Service system.","Web services,
Voting,
Redundancy,
Hardware,
Protection,
Computer science,
Fault tolerant systems,
Computer applications,
Grid computing,
Service oriented architecture"
An automated method for analysis of flow characteristics of circulating particles from in vivo video microscopy,"The behavior of white and red blood cells, platelets, and circulating injected particles is one of the most studied areas of physiology. Most methods used to analyze the circulatory patterns of cells are time consuming. We describe a system named CellTrack, designed for fully automated tracking of circulating cells and micro-particles and retrieval of their behavioral characteristics. The task of automated blood cell tracking in vessels from in vivo video is particularly challenging because of the blood cells' nonrigid shapes, the instability inherent in in vivo videos, the abundance of moving objects and their frequent superposition. To tackle this, the CellTrack system operates on two levels: first, a global processing module extracts vessel borders and center lines based on color and temporal patterns. This enables the computation of the approximate direction of the blood flow in each vessel. Second, a local processing module extracts the locations and velocities of circulating cells. This is performed by artificial neural network classifiers that are designed to detect specific types of blood cells and micro-particles. The motion correspondence problem is then resolved by a novel algorithm that incorporates both the local and the global information. The system has been tested on a series of in vivo color video recordings of rat mesentery. Our results show that the synergy between the global and local information enables CellTrack to overcome many of the difficulties inherent in tracking methods that rely solely on local information. A comparison was made between manual measurements and the automatically extracted measurements of leukocytes and fluorescent microspheres circulatory velocities. This comparison revealed an accuracy of 97%. CellTrack also enabled a much larger volume of sampling in a fraction of time compared to the manual measurements. All these results suggest that our method can in fact constitute a reliable replacement for manual extraction of blood flow characteristics from in vivo videos.",
Model checking C programs using F-Soft,"With the success of formal verification techniques like equivalence checking and model checking for hardware designs, there has been growing interest in applying such techniques for formal analysis and automatic verification of software programs. This paper provides a brief tutorial on model checking of C programs. The essential approach is to model the semantics of C programs in the form of finite state systems by using suitable abstractions. The use of abstractions is key, both for modeling programs as finite state systems and for reducing the model sizes in order to manage verification complexity. We provide illustrative details of a verification platform called F-Soft, which provides a range of abstractions for modeling software, and uses customized SAT-based and BDD-based model checking techniques targeted for software.","Object oriented modeling,
Hardware,
Circuit testing,
Protocols,
Concrete,
Chaotic communication,
National electric code,
Laboratories,
Computer science,
Formal verification"
Self-stabilizing structured ring topology P2P systems,"We propose a self-stabilizing and modeless peer-to-peer (P2P) network construction and maintenance protocol, called the Ring Network (RN) protocol. The RN protocol, when started on a network of peers that are in an arbitrary state, will cause the network to converge to a structured P2P system with a directed ring topology, where peers are ordered according to their identifiers. Furthermore, the RN protocol maintains this structure in the face of peer joins and departures. The RN protocol is a distributed and asynchronous message-passing protocol, which fits well the autonomous behavior of peers in a P2P system. The RN protocol requires only the existence of a bootstrapping system which is weakly connected. Peers do not need to be informed of any global network state, nor do they need to assist in repairing the network topology when they leave. We provide a proof of the self-stabilizing nature of the protocol, and experimentally measure the average cost (in time and number of messages) to achieve convergence.","Protocols,
Network topology,
Computer science,
Time measurement,
Costs,
Convergence,
Fault tolerant systems,
Information retrieval,
Merging,
Monitoring"
Genetic algorithm based scheduler for computational grids,"In the context of highly scalable distributed resource management architectures for grid computing, we present a genetic algorithm based scheduler. A scheduler must use the available resources efficiently, while satisfying competing and mutually conflicting goals. The grid workload may consist of multiple jobs, with quality-of-service constraints. A directed acyclic graph (DAG) represents each job, taking into account arbitrary precedence constraints and arbitrary processing time. The scheduler has been designed to be compatible with other tools being developed by our grid research group. We present the design, implementation and test results for such a scheduler in which we minimize make-span, idle time of the available computational resources, turn-around time and the specified deadlines provided by users. The architecture is hierarchical and the scheduler is usable at either the lowest or the higher tiers. It can also be used in both the intra-grid of a large organization and in a research grid consisting of large clusters, connected through a high bandwidth dedicated network.","Genetic algorithms,
Processor scheduling,
Grid computing,
Resource management,
Testing,
Computer architecture,
Throughput,
Algorithm design and analysis,
Computer science,
Quality of service"
From static distributed systems to dynamic systems,"A noteworthy advance in distributed computing is due to the recent development of peer-to-peer systems. These systems are essentially dynamic in the sense that no process can get a global knowledge on the system structure. They mainly allow processes to look up for data that can be dynamically added/suppressed in a permanently evolving set of nodes. Although protocols have been developed for such dynamic systems, to our knowledge, up to date no computation model for dynamic systems has been proposed. Nevertheless, there is a strong demand for the definition of such models as soon as one wants to develop provably correct protocols suited to dynamic systems. This paper proposes a model for (a class of) dynamic systems. That dynamic model is defined by (1) a parameter (an integer denoted a) and (2) two basic communication abstractions (query-response and persistent reliable broadcast). The new parameter is a threshold value introduced to capture the liveness part of the system (it is the counterpart of the minimal number of processes that do not crash in a static system). To show the relevance of the model, the paper adapts an eventual leader protocol designed for the static model, and proves that the resulting protocol is correct within the proposed dynamic model. In that sense, the paper has also a methodological flavor, as it shows that simple modifications to existing protocols can allow them to work in dynamic systems.","Distributed computing,
Peer to peer computing,
Protocols,
Computational modeling,
Grid computing,
Broadcasting,
Power system modeling,
Computer science,
Computer crashes,
Stability"
Recording and using provenance in a protein compressibility experiment,"Very large scale computations are now becoming routinely used as a methodology to undertake scientific research. In this context, 'provenance systems' are regarded as the equivalent of the scientist's logbook for in silico experimentation: provenance captures the documentation of the process that led to some result. Using a protein compressibility analysis application, we derive a set of generic use cases for a provenance system. In order to support these, we address the following fundamental questions: what is provenance? How to record it? What is the performance impact for grid execution? What is the performance of reasoning? In doing so, we define a technology-independent notion of provenance that captures interactions between components, internal component information and grouping of interactions, so as to allow us to analyze and reason about the execution of scientific processes. In order to support persistent provenance in heterogeneous applications, we introduce a separate provenance store, in which provenance documentation can be stored, archived and queried independently of the technology used to run the application. Through a series of practical tests, we evaluate the performance impact of such a provenance system. In summary, we demonstrate that provenance recording overhead of our prototype system remains under 10% of execution time, and we show that the recorded information successfully supports our use cases in a performant manner.","Proteins,
Documentation,
Large-scale systems,
Computer science,
Independent component analysis,
Information analysis,
System testing,
Magnetic heads,
Physics computing,
Bioinformatics"
Conceptual and Implementation Models for the Grid,"The Grid is rapidly emerging as the dominant paradigm for wide area distributed application systems. As a result, there is a need for modeling and analyzing the characteristics and requirements of Grid systems and programming models. This work adopts the well-established body of models for distributed computing systems, which are based upon carefully stated assumptions or axioms, as a basis for defining and characterizing Grids and their programming models and systems. The requirements of programming Grid applications and the resulting requirements on the underlying virtual organizations and virtual machines are investigated. The assumptions underlying some of the programming models and systems currently used for Grid applications are identified and their validity in Grid environments is discussed. A more in-depth analysis of two programming systems, the Imperial College E-Science Networked Infrastructure (ICENI) and Accord, using the proposed definitions' structure is presented.","Standards organizations,
Application software,
Crisis management,
Middleware,
Environmental management,
Distributed computing,
Virtual machining,
Mesh generation,
Government,
Computer science"
Energy aware non-preemptive scheduling for hard real-time systems,"Slowdown based on dynamic voltage scaling (DVS) provides the ability to perform an energy-delay tradeoff in the system. Nonpreemptive scheduling becomes an integral part of systems where resource characteristics makes preemption undesirable or impossible. We address the problem of energy efficient scheduling of nonpreemptive tasks based on the earliest deadline first (EDF) scheduling policy. We present the stack based slowdown algorithm that builds upon the optimal feasibility test for nonpreemptive systems. We also propose a dynamic stack reclamation policy to further enhance energy savings. Simulation results show on an average 15% energy savings using static slowdown factors and 20% savings with dynamic slowdown, over known slowdown techniques.","Real time systems,
Processor scheduling,
Frequency,
Dynamic voltage scaling,
Energy efficiency,
Energy consumption,
Delay,
Scheduling algorithm,
Embedded computing,
Computer science"
Task placement for heterogeneous reconfigurable architectures,"The concept of partial reconfiguration offers the possibility to dynamically place and remove hardware tasks on reconfigurable architectures, like FPGAs. Common placement algorithms, e.g. Best Fit, are designed for homogeneous architectures, since they do not consider any placement constraints of the hardware tasks. Due to the integration of, e.g., dedicated memory, current FPGAs are heterogeneous reconfigurable architectures. In this paper we introduce two heterogeneous placement algorithms, which are able to deal with the constraints of the hardware tasks. Both algorithms are compared to the Best Fit algorithm by using a simulation framework for partially configurable architectures. We propose concepts of an efficient hardware realization of our placement approach with Xilinx Virtex-II FPGAs. Moreover, we present a task placement mechanism to change the position of a hardware task on the FPGA by manipulating the configuration data of the task","Reconfigurable architectures,
Hardware,
Field programmable gate arrays,
Resource management,
Computer architecture,
Circuits,
Computer science,
Software engineering,
Australia,
Computer vision"
Guide wire reconstruction and visualization in 3DRA using monoplane fluoroscopic imaging,"A method has been developed that, based on the guide wire position in monoplane fluoroscopic images, visualizes the approximate guide wire position in the three-dimensional (3-D) vasculature, that is obtained prior to the intervention with 3-D rotational X-ray angiography (3DRA). The method assumes the position of the guide wire in the fluoroscopic images is known. A two-dimensional feature image is determined from the 3DRA data. In this feature image, the guide wire position is determined in a two-step approach: a mincost algorithm is used to determine a suitable position for the guide wire, and subsequently a snake optimization technique is applied to move the guide wire to a better position. The resulting guide wire can then be visualized in 3-D in combination with the 3DRA dataset. The reconstruction accuracy of the method has been evaluated using a 3DRA image of a vascular phantom filled with contrast, and monoplane fluoroscopic images of the same phantom without contrast and with a guide wire inserted. The evaluation has been performed for different projection angles, and with different parameters for the method. The final result does not appear to be very sensitive to the parameters of the method. The average mean error of the estimated 3-D guide wire position is 1.5 mm, and the average tip distance is 2.3 mm. The effect of inaccurate C-arm geometry information is also investigated. Small errors in geometry information (up to 1/spl deg/) will slightly decrease the 3-D reconstruction accuracies, with an error of at most 1 mm. The feasibility of this approach on clinical data is demonstrated.","Wire,
Image reconstruction,
Imaging phantoms,
Information geometry,
Optical imaging,
Data visualization,
X-ray imaging,
Angiography,
Performance evaluation,
Three dimensional displays"
Large-Scale First-Principles Molecular Dynamics simulations on the BlueGene/L Platform using the Qbox code,"We demonstrate that the Qbox code supports unprecedented large-scale First-Principles Molecular Dynamics (FPMD) applications on the BlueGene/L supercomputer. Qbox is an FPMD implementation specifically designed for large-scale parallel platforms such as BlueGene/L. Strong scaling tests for a Materials Science application show an 86% scaling efficiency between 1024 and 32,768 CPUs. Measurements of performance by means of hardware counters show that 36% of the peak FPU performance can be attained.","Large-scale systems,
Computational modeling,
Scientific computing,
Laboratories,
Computer simulation,
Government,
Equations,
Analytical models,
Materials science and technology,
Concurrent computing"
Efficient online spherical k-means clustering,"The spherical k-means algorithm, i.e., the k-means algorithm with cosine similarity, is a popular method for clustering high-dimensional text data. In this algorithm, each document as well as each cluster mean is represented as a high-dimensional unit-length vector. However, it has been mainly used in hatch mode. Thus is, each cluster mean vector is updated, only after all document vectors being assigned, as the (normalized) average of all the document vectors assigned to that cluster. This paper investigates an online version of the spherical k-means algorithm based on the well-known winner-take-all competitive learning. In this online algorithm, each cluster centroid is incrementally updated given a document. We demonstrate that the online spherical k-means algorithm can achieve significantly better clustering results than the batch version, especially when an annealing-type learning rate schedule is used. We also present heuristics to improve the speed, yet almost without loss of clustering quality.","Clustering algorithms,
Frequency,
Annealing,
Computer science,
Data engineering,
Scheduling algorithm,
Data mining,
Information retrieval,
Information filtering,
Information filters"
Optimal torque per amp for brushless doubly fed reluctance machines,"This paper documents the finite element (FE) analysis of a range of rotors for a brushless doubly fed reluctance machine (BDFRM). The aim to this work is to increase the understanding of the BDFRM and determine an optimal rotor design for torque generation and machine power factor. The results of the analysis show a significant contrast between the rotors for a synchronous reluctance machine and BDFRM. The optimal stacking factor for the BDFRM is 0.86, whereas the synchronous reluctance optimum is 0.5. Even though the two rotors can be physically identical, the resemblance is superficial.","Torque,
Reluctance machines,
Stacking,
Rotors,
Lamination,
Computer science,
Frequency,
Turbines,
Robustness,
Manufacturing"
Design and operation of a sequentially-fired pulse forming network for non-linear loads,"While the construction of a linear pulse forming network (PFN) for a constant load impedance is relatively easy, the process is more difficult for a nonlinear or time-varying load. A passive PFN can certainly be synthesized for nonlinear loads, but is usually large and lacks the flexibility to be truly useful in most practical and research applications. This investigation describes the design and construction of a sequentially-fired pulse forming network (SFPFN) that maintains constant voltage and current for a nonlinear load. Operation of the SFPFN consists of charging multiple capacitor banks (or modules) to various levels and sequentially firing these banks into the load at appropriate times. The load and its characteristics determine the module charge voltage. An added benefit with the SFPFN is real-time computer monitoring and control allowing the PFN modules to be charged from a single prime power source via a group of switching relays. The nonlinear load used in this investigation is a helical coil electromagnetic launcher (HCEL). The SFPFN is also tested with a linear load consisting of a pulsed field coil. The nonlinearity of the HCEL is well-known with a factor of 2 change in winding resistance due to joule heating and a large variation in terminal voltage due to changes in the armature back-voltage. Experimental measurements show the SFPFN can deliver a relatively constant current pulse on the order of 5-15 kA into the HCEL load for a pulse length up to 8 ms. The maximum SFPFN operating voltage is 900 V with a total stored energy of 125 kJ. Scaling the SFPFN to larger or smaller pulse amplitudes or lengths is possible.","Voltage,
Pulse measurements,
Coils,
Impedance,
Network synthesis,
Capacitors,
Computerized monitoring,
Relays,
Electromagnetic launching,
Testing"
Near real-time reliable stereo matching using programmable graphics hardware,"A near-real-time stereo matching technique is presented in this paper, which is based on the reliability-based dynamic programming algorithm we proposed earlier. The new algorithm can generate semi-dense disparity maps using only two dynamic programming passes, while our previous approach requires 20-30 passes. We also implement the algorithm on programmable graphics hardware, which further improves the processing speed. The experiments on the four Middlebury stereo datasets show that the new algorithm can produce dense (>85% of the pixels) and reliable (error rate <0.3%) matches in near real-time (0.05-0.1 sec). If needed, it can also be used to generate dense disparity maps. Based on the evaluation conducted by the Middlebury Stereo Vision Research Website, the new algorithm is ranked between the variable window and the graph cuts approaches and currently is the most accurate dynamic programming based technique. When more than one reference images are available, the accuracy can be further improved with little extra computation time.","Hardware,
Dynamic programming,
Iterative algorithms,
Stereo vision,
Costs,
Computational efficiency,
Computer graphics,
Computer science,
Heuristic algorithms,
Error analysis"
On survivable routing of mesh topologies in IP-over-WDM networks,"Failure restoration at the IP layer in IP-over-WDM networks requires to map the IP topology on the WDM topology in such a way that a failure at the WDM layer leaves the IP topology connected. Such a mapping is called survivable. Finding a survivable mapping is known to be NP-complete E. Modiano et al., (2002), making it impossible in practice to assess the existence or absence of such a mapping for large networks, (i) We first introduce a new concept of piecewise survivability, which makes the problem much easier, and allows us to formally prove that a given survivable mapping does or does not exist, (ii) Secondly, we show how to trace the vulnerable areas in the topology, and how to strengthen them to enable a survivable mapping, (iii) Thirdly, we give an efficient and scalable algorithm that finds a survivable mapping. In contrast to the heuristics proposed in the literature to date, our algorithm exhibits a number of provable properties that are crucial for (i) and (ii). We consider both link and node failures at the physical layer.","Routing,
Network topology,
Intelligent networks,
Wavelength division multiplexing,
Protection,
Physical layer,
Computer science,
Optical switches"
The challenges of hardware synthesis from C-like languages,"Many techniques for synthesizing digital hardware from C-like languages have been proposed, but none have emerged as successful as Verilog or VHDL for register-transfer-level design. Familiarity is the main reason C-like languages have been proposed for hardware synthesis. Synthesize hardware from C, proponents claim, and a C programmer can be turned into a hardware designer. Another common motivation is hardware/software codesign: today's systems usually contain a mix of hardware and software, and it is often unclear initially which portions to implement in hardware. Here, using a single language should simplify the migration task. The paper surveys several C-like hardware synthesis languages and looks at two of the fundamental challenges, concurrency and timing control.",
Bounds on the performance of vector-quantizers under channel errors,"Vector quantization (VQ) is an effective and widely known method for low-bit-rate communication of speech and image signals. A common assumption in the design of VQ-based communication systems is that the compressed digital information is transmitted through a perfect channel. Under this assumption, quantization distortion is the only factor in output signal fidelity. Moreover, the assignment of channel symbols to the VQ reconstruction vectors is of no importance. However, under physical channels, errors may be present, causing degradation in overall system performance. In such a case, the effect of channel errors on the coding system performance depends on the index assignment of the reconstruction vectors. The index assignment problem is a special case of the Quadratic Assignment Problem (QAP) and is known to be NP-complete. For a VQ with N reconstruction vectors there are N! possible assignments, meaning that an exhaustive search over all possible assignments is practically impossible. To help the VQ designer, we present in this correspondence lower and upper bounds on the performance of VQ systems under channel errors, over all possible assignments. The bounds coincide with a general bound for the QAP. Nevertheless, the proposed derivation allows us to compare the bounds with published results on VQ index assignment. A related expression for the average performance is also given and discussed. Special cases and numerical examples are given in which the bounds and average performance are compared with index assignments obtained by known algorithms.",
Use of classroom presenter in engineering courses,"Instructors of computer science, engineering, and technology courses often teach using prepared slides displayed with a computer and data projector. While this approach has advantages, disadvantages can be severe - including limited flexibility in delivery of material, which hinders instructors' spontaneous adaptation to students. This paper describes Classroom Presenter, a Tablet PC-based presentation system that (1) allows instructors to regain flexibility in presentation using Tablet PC-supported digital inking in an application specifically designed for lecturing use, (2) supports role-specific views and interaction mechanisms in class, and (3) introduces novel affordances including the use of wireless technology and student devices to support active learning in the classroom. As of spring 2005, Classroom Presenter has been used in over 200 courses at more than 13 institutions. Here we take advantage of our large use-base to discuss Classroom Presenter's impact on student learning and summarize best practices for lecturing with this technology","Computer science,
Data engineering,
Computer displays,
Springs,
Ink,
Switches,
Mobile computing,
Materials science and technology,
Best practices,
Computational modeling"
Improving the forward solver for the complete electrode model in EIT using algebraic multigrid,"Image reconstruction in electrical impedance tomography is an ill-posed nonlinear inverse problem. Linearization techniques are widely used and require the repeated solution of a linear forward problem. To account correctly for the presence of electrodes and contact impedances, the so-called complete electrode model is applied. Implementing a standard finite element method for this particular forward problem yields a linear system that is symmetric and positive definite and solvable via the conjugate gradient method. However, preconditioners are essential for efficient convergence. Preconditioners based on incomplete factorization methods are commonly used but their performance depends on user-tuned parameters. To avoid this deficiency, we apply black-box algebraic multigrid, using standard commercial and freely available software. The suggested solution scheme dramatically reduces the time cost of solving the forward problem. Numerical results are presented using an anatomically detailed model of the human head.","Electrodes,
Tomography,
Impedance,
Image reconstruction,
Inverse problems,
Linearization techniques,
Contacts,
Finite element methods,
Linear systems,
Gradient methods"
Building a classification cascade for visual identification from one example,"Object identification (OID) is specialized recognition where the category is known (e.g. cars) and the algorithm recognizes an object's exact identity (e.g. Bob's BMW). Two special challenges characterize OID. (1) Interclass variation is often small (many cars look alike) and may be dwarfed by illumination or pose changes. (2) There may be many classes but few or just one positive ""training"" examples per class. Due to (1), a solution must locate possibly subtle object-specific salient features (a door handle) while avoiding distracting ones (a specular highlight). However, (2) rules out direct techniques of feature selection. We describe an online algorithm that takes one model image from a known category and builds an efficient ""same"" vs. ""different"" classification cascade by predicting the most discriminative feature set for that object. Our method not only estimates the saliency and scoring function for each candidate feature, but also models the dependency between features, building an ordered feature sequence unique to a specific model image, maximizing cumulative information content. Learned stopping thresholds make the classifier very efficient. To make this possible, category-specific characteristics are learned automatically in an off-line training procedure from labeled image pairs of the category, without prior knowledge about the category. Our method, using the same algorithm for both cars and faces, outperforms a wide variety of other methods.","Computer science,
Lighting,
Object recognition,
Face detection,
Cameras,
Hair,
Face recognition,
Vehicles,
Eyebrows,
Humans"
Novel mechatronics design for a robotic fish,"This paper presents a novel mechatronics design for a 3D swimming robotic fish, namely MT1 (Mechanical Tail) robotic fish. It has a novel tail structure which uses only one motor to generate fish-like swimming motion using C-bends tail shapes. This design enables MT1 to become the first small size robotic fish (<0.5m in length) and be able to dive over 3 meters deep in water. An effective control method with only 5 parameters is proposed to control its 3D swimming behaviours. Experimental results are presented to show the feasibility and good performance of the proposed control algorithms.","Mechatronics,
Marine animals,
Tail,
Service robots,
Propulsion,
Educational robots,
Turning,
Computer science,
Shape,
Acceleration"
Correlation based video processing in video sensor networks,"While conventional sensor networks have revealed their vast potentials, simple and special purposed data doubtlessly limits their expansion into information hungry domains. Video sensor networks present an unprecedented opportunity to annex rich information with distributed sensor networks. However, directional sensing range and high data rate unique to video sensors also demands novel solutions, especially for reducing data amount and individual sensor's workload. Toward this end, this paper proposes a method for cooperative video processing in video sensor networks bused on sensor correlations. By pairing up highly correlated nodes and divide sensing tasks among them, each sensor only needs to cover a fraction of the targeted area. Compared to the non-cooperative approach, video sensor's image processing and transmission workload is significantly reduced. We design efficient algorithms to calculate the correlations, partition sensing areas, and fuse partial images. A set of experiments are also performed to prove the concept.",
"An interoperable, standards-based grid resource broker and job submission service","We present the architecture and implementation of a grid resource broker and job submission service, designed to be as independent as possible of the grid middleware used on the resources. The overall architecture comprises seven general components and a few conversion and integration points where all middleware-specific issues are handled. The implementation is based on state-of-the-art grid and Web services technology as well as existing and emerging standards (WSRF, JSDL, GLUE, WS-Agreement). Features provided by the service include advance reservations and a resource selection process based on a priori estimations of the total time to delivery for the application, including a benchmark-based prediction of the execution time. The general service implementation is based on the Globus Toolkit 4. For test and evaluation, plugins and format converters are provided for use with the NorduGrid ARC middleware","Middleware,
Grid computing,
Computer architecture,
Web services,
Resource management,
Job design,
Benchmark testing,
Information systems,
Software tools,
Performance evaluation"
The effects of wireless transmission range on path lifetime in vehicle-formed mobile ad hoc networks on highways,"An information network built on top of vehicles using inter-vehicle communication (IVC) can be viewed as a type of mobile ad hoc network (MANET). The lifetime of the unicast routing paths existing in an IVC network can be affected by several factors such as vehicle moving speed, wireless radio transmission range, traffic density, etc. The paper uses more realistic vehicle mobility traces to study the effects of wireless transmission range on path lifetime in an IVC network. The results presented can help IVC researchers choose an appropriate wireless transmission range for IVC networks.",
Three faces of human-computer interaction,"Human-computer interaction is considered a core element of computer science. Yet it has not coalesced; many researchers who identify their focus as human-computer interaction reside in other fields. The author examines the origins and evolution of three HCI research foci: computer operation, information systems management, and discretionary use. The author describes efforts to find common ground and forces that have kept them apart.","Face,
Electron tubes,
Computer displays,
Switches,
Human computer interaction,
Technological innovation,
Military computing,
Human factors,
Teleprinting,
Cathode ray tubes"
A counterexample for the open problem on the minimal delays of orthogonal designs with maximal rates,"X. Liang systematically investigated orthogonal designs with maximal rates, gave the maximal rates of complex orthogonal designs and a concrete construction procedure for complex orthogonal designs with the maximal rates. He also posed an open problem on the minimal decoding delays of complex orthogonal designs with maximal rates, and proved that the problem is correct for less than or equal to six transmit antennas. In this correspondence, we give a counterexample for the open problem for n=8 and prove that the minimal delay for complex orthogonal designs with eight columns is 56. Hence, we give a negative answer for the open problem.","Delay,
Maximum likelihood decoding,
Transmitting antennas,
Concrete,
Upper bound,
Information science,
Block codes,
Wireless communication,
Receiving antennas,
Computer science"
A framework for R.F. spectrum measurements and analysis,"Dynamic spectrum access networks and spectrum policy depend on accurate spectrum utilization statistics. This paper presents the architecture of a collaborative framework to measure, characterize and model the utilization of the spectrum. It provides statistical methods to classify measurements as either signal or noise. Finally, it introduces algorithms for estimating signal thresholds and spectrum occupancy, and presents a performance evaluation to test the accuracy of the algorithms","Antenna measurements,
Testing,
Software architecture,
Statistical analysis,
Computer science,
Frequency,
Collaboration,
Databases,
Software measurement,
Bandwidth"
Bottom-up/top-down image parsing by attribute graph grammar,"In this paper, we present an attribute graph grammar for image parsing on scenes with man-made objects, such as buildings, hallways, kitchens, and living moms. We choose one class of primitives - 3D planar rectangles projected on images and six graph grammar production rules. Each production rule not only expands a node into its components, but also includes a number of equations that constrain the attributes of a parent node and those of its children. Thus our graph grammar is context sensitive. The grammar rules are used recursively to produce a large number of objects and patterns in images and thus the whole graph grammar is a type of generative model. The inference algorithm integrates bottom-up rectangle detection which activates top-down prediction using the grammar rules. The final results are validated in a Bayesian framework. The output of the inference is a hierarchical parsing graph with objects, surfaces, rectangles, and their spatial relations. In the inference, the acceptance of a grammar rule means recognition of an object, and actions are taken to pass the attributes between a node and its parent through the constraint equations associated with this production rule. When an attribute is passed from a child node to a parent node, it is called bottom-up, and the opposite is called top-down","Layout,
Production,
Equations,
Inference algorithms,
Tree graphs,
Computer vision,
Computer science,
Statistics,
Bayesian methods,
Vocabulary"
Geometric spanners for routing in mobile networks,"We propose a new routing graph, the restricted Delaunay graph (RDG), for mobile ad hoc networks. Combined with a node clustering algorithm, the RDG can be used as an underlying graph for geographic routing protocols. This graph has the following attractive properties: 1) it is planar; 2) between any two graph nodes there exists a path whose length, whether measured in terms of topological or Euclidean distance, is only a constant times the minimum length possible; and 3) the graph can be maintained efficiently in a distributed manner when the nodes move around. Furthermore, each node only needs constant time to make routing decisions. We show by simulation that the RDG outperforms previously proposed routing graphs in the context of the Greedy perimeter stateless routing (GPSR) protocol. Finally, we investigate theoretical bounds on the quality of paths discovered using GPSR.","Intelligent networks,
Routing protocols,
Mobile ad hoc networks,
Ad hoc networks,
Network topology,
Computer science,
Laboratories,
Switches,
Clustering algorithms,
Length measurement"
Context-free slicing of UML class models,"The concept of model slicing is introduced as a means to support maintenance through the understanding, querying, and analysis of large UML models. The specific models being examined are class models as defined in the Unified Modeling Language (UML). Model slicing is analogous to classical program slicing. Since UML class models do not explicitly embody any behavioral aspect by themselves, models slices are computed in a context-free manner. The paper defines and formalizes the concept of context-free model slicing. A concrete application of model slicing in software maintenance is presented to support the usefulness and validity of the method.","Unified modeling language,
Context modeling,
Concrete,
Software maintenance,
Computer science,
Application software,
Software systems,
Reverse engineering"
Sampling-Based Motion Planning Using Predictive Models,"Robotic motion planning requires configuration space exploration. In high-dimensional configuration spaces, a complete exploration is computationally intractable. Practical motion planning algorithms for such high-dimensional spaces must expend computational resources in proportion to the local complexity of configuration space regions. We propose a novel motion planning approach that addresses this problem by building an incremental, approximate model of configuration space. The information contained in this model is used to direct computational resources to difficult regions, effectively addressing the narrow passage problem by adapting the sampling density to the complexity of that region. In addition, the expressiveness of the model permits predictive edge validations, which are performed based on the information contained in the model rather then by invoking a collision checker. Experimental results show that the exploitation of the information obtained through sampling and represented in a predictive model results in a significant decrease in the computational cost of motion planning.","Predictive models,
Motion planning,
Sampling methods,
Space exploration,
Orbital robotics,
Computational efficiency,
Computational complexity,
Laboratories,
Computer science,
Robot motion"
Features for recognition: viewpoint invariance for non-planar scenes,"Most current local feature detectors/descriptors implicitly assume that the scene is (locally) planar, an assumption that is violated at surface discontinuities. We show that this restriction is, at least in theory, unnecessary, as one can construct local features that are viewpoint-invariant for generic non-planar scenes. However, we show that any such feature necessarily sacrifices shape information, in the sense of being non shape-discriminative. Finally, we show that if viewpoint is factored out as part of the matching process, rather than explicitly in the representation, then shape is discriminative indeed. We illustrate our theoretical results empirically by showing that, even for simple scenes, current affine descriptors fail where even a naive 3-D viewpoint invariant succeeds in matching","Layout,
Shape,
Lighting,
Computer vision,
Reflectivity,
Computer science,
Detectors,
Image recognition,
Object recognition,
Image reconstruction"
Beyond event handlers: programming wireless sensors with attributed state machines,"Event-driven programming is a popular paradigm for programming sensor nodes. It is based on the specification of actions (also known as event handlers) which are triggered by the occurrence of events. While this approach is both simple and efficient, it suffers from two important limitations. Firstly, the association of events to actions is static-there is no explicit support for adopting this association depending on the program state. Secondly, a program is split up into many distinct actions without explicit support for sharing information among these. These limitations often lead to issues with code modularity, complexity, and correctness. To tackle these issues we propose OSM, a programming model and language for sensor nodes based on finite state machines. OSM extends the event paradigm with states and transitions, such that the invocation of actions becomes a function of both the event and the program state. For removing the second limitation, OSM introduces state attributes that allow sharing of information among actions. They can be considered local variables of a state with support for automatic memory management. OSM specifications can be compiled into sequential C code that requires only minimal runtime support, resulting in efficient and compact systems.","Wireless sensor networks,
Sensor phenomena and characterization,
Data structures,
Yarn,
Automata,
Programming profession,
Embedded system,
Dynamic programming,
Computer science,
Memory management"
Homogeneous redundancy: a technique to ensure integrity of molecular simulation results using public computing,"Distributed computing using PCs volunteered by the public can provide high computing capacity at low cost. However, computational results from volunteered PCs have a non-negligible error rate, so result validation is needed to ensure overall correctness. A generally applicable technique is ""redundant computing"", in which each computation is done on several separate computers, and results are accepted only if there is a consensus. Variations in numerical processing between computers (due to a variety of hardware and software factors) can lead to different results for the same task. In some cases, this can be addressed by doing a ""fuzzy comparison"" of results, so that two results are considered equivalent if they agree within given tolerances. However, this approach is not applicable to applications that are ""divergent"", that is, for which small numerical differences can produce large differences in the results. In this paper we examine the problem of validating results of divergent applications. We present a novel approach called homogeneous redundancy (HR), in which the redundant instances of a computation are dispatched to numerically identical computers, allowing strict equality comparison of the results. HR has been deployed in Predictor@home, a world-wide community effort to predict protein structure from sequence.","Computational modeling,
Personal communication networks,
Distributed computing,
Computer errors,
Hardware,
Application software,
Redundancy,
Biological system modeling,
Biology computing,
Computer science"
Making logistic regression a core data mining tool with TR-IRLS,"Binary classification is a core data mining task. For large datasets or real-time applications, desirable classifiers are accurate, fast, and need no parameter tuning. We present a simple implementation of logistic regression that meets these requirements. A combination of regularization, truncated Newton methods, and iteratively re-weighted least squares make it faster and more accurate than modern SVM implementations, and relatively insensitive to parameters. It is robust to linear dependencies and some scaling problems, making most data preprocessing unnecessary.","Logistics,
Data mining,
Character generation,
Data preprocessing,
Sparse matrices,
Computer science,
Newton method,
Least squares methods,
Support vector machines,
Support vector machine classification"
On the characteristics of spectrum-agile communication networks,"Preliminary studies as well as general observations indicate the presence of a significant amount of ""white space"" in radio spectrum, varying on frequency, time, and geographic locations. Thus, it is likely that spectrum access, instead of true spectrum scarcity, is the limiting factor of potential growth of wireless services. Enabled by regulatory changes and radio technologies advances, opportunistic usage of the white space has the potential to significantly mitigate the spectrum scarcity. In this paper, we study the characteristics of opportunistic spectrum availability and its exploration. We present two new metrics to capture unique characteristics in networks with spectrum agility, namely equivalent non-opportunistic bandwidth and space-bandwidth product. Equivalent non-opportunistic bandwidth is a metric to quality the utilization of white space that is opportunistically available to secondary users. Space-bandwidth product qualifies the reuse in both spatial and frequency domain. Heterogeneity in channel availability and footprints are shown to be beneficial. We also study the spatial and temporal properties of opportunistic spectrum availability","Communication networks,
White spaces,
Space technology,
Bandwidth,
Availability,
Wireless communication,
FCC,
Computer science,
Frequency domain analysis,
US Government"
Script identification using steerable Gabor filters,"Multi-channel Gabor filtering has been widely used in texture classification. In this paper, Gabor filters have been applied to the problem of script identification in printed documents. Our work is divided into two stages. Firstly, a Gabor filter bank is appropriately designed so that extracted rotation-invariant features can handle scripts that are similar in shape and even share many characters. Secondly, the steerability property of Gabor filters is exploited to reduce the high computation cost resulted from the frequent image filtering, which is a common problem encountered in Gabor filter related applications. Results from preliminary experiments are quite promising, where Chinese, Japanese, Korean and English are considered. Over 98.5 % language identification rate can be achieved while image filtering operations have been reduced by 40%.","Gabor filters,
Natural languages,
Filter bank,
Feature extraction,
Filtering,
Shape,
Computational efficiency,
Pattern recognition,
Machine intelligence,
Computer science"
Three-dimensional cache design exploration using 3DCacti,"As technology scales, interconnects dominate the performance and power behavior of deep submicron designs. Three-dimensional integrated circuits (3D ICs) have been proposed as a way to mitigate the interconnect challenges. In this paper, we explore the architectural design of cache memories using 3D circuits. We present a delay and energy model, 3DCacti, to explore different 3D design options of partitioning a cache. The tool allows partitioning of the cache across different device layers at various levels of granularity. The tool has been validated by comparing its results with those obtained from circuit simulation of custom 3D layouts. We also explore the effects of various cache partitioning parameters and 3D technology parameters on delay and energy to demonstrate the utility of the tool.",
Model structure simplification of Nonlinear Systems via immersion,"This paper concerns the simplification of model structures of nonlinear systems while preserving their input-output maps. The basic technique is an immersion of a system, which is a mapping of the initial state from the original system to another system of an identical input-output map. The necessary and sufficient conditions are presented for the immersibility of a given system into a state-space representation of such particular structures as rational functions or polynomials with respect to the state. The conditions are so mild that many types of nonlinear systems can be represented by rational or polynomial model structures. Moreover, it is also shown that a polynomial state-space representation can always be constructed to be at most quadratic with respect to the state.","Nonlinear systems,
Polynomials,
Linear systems,
State-space methods,
Space technology,
State feedback,
Sufficient conditions,
Mathematical model,
Computer science education,
Educational technology"
A middleware for replicated Web services,This paper presents a middleware that supports reliable Web services built on active replication. The middleware is responsible for maintaining the consistency of the replicas' states. A Java package for handling the interactions with the middleware and the failures of the Web services is provided for programmers to use when writing client applications. The package reduces the complexity in developing client applications.,"Middleware,
Web services,
Packaging,
Multicast protocols,
Maintenance,
Computer science,
Java,
Programming profession,
Writing,
Web and internet services"
The impact of agile methods on software project management,"As more and more software projects engage agile methods, there are emerging patterns of success and failure. With growing adoption of agile methods, project managers increasingly need to understand the applicability to their projects and factors that drive key project performance characteristics. While some organizations affirm that agile methods solve all their problems, few have shown consistent success over a range of typical software projects. Agile methods have advantages, especially in accommodating change due to volatile requirements. However, they also present concomitant risks with managing the many dependent pieces of work distributed across a large project. Use of agile methods therefore presents a set of tradeoffs. This paper examines the impact of agile methods on the people involved in a project, the process under which a project is developed, and on the project itself in an attempt to allow project managers to evaluate the applicability using an agile method.","Project management,
Risk management,
Costs,
Disaster management,
Dynamic programming,
Computer science,
Software engineering,
Steel,
Integrated circuit measurements,
Circuit testing"
The Analysis of an Efficient Algorithm for Robot Coverage and Exploration based on Sensor Network Deployment,"In this paper we present the design and theoretical analysis of a novel algorithm (LRV) that efficiently solves the problems of coverage, exploration and sensor network deployment at the same time. The basic premise behind the algorithm is that the robot carries network nodes as a payload, and in the process of moving around, emplaces the nodes into the environment based on certain local criteria. In turn, the nodes emit navigation directions for the robot as it goes by. Nodes recommend directions least recently visited by the robot, hence the name LRV. We formally establish the following two properties: 1. LRV is complete on graphs, and 2. LRV is optimal on trees. We present some experimental conjectures for LRV on regular square lattice graphs and compare its performance empirically to other graph exploration algorithms.","Algorithm design and analysis,
Robot sensing systems,
Sensor systems,
Embedded system,
Payloads,
Navigation,
Laboratories,
Computer science,
Tree graphs,
Lattices"
Rational secure computation and ideal mechanism design,"Secure computation essentially guarantees that whatever computation n players can do with the help of a trusted party, they can also do by themselves. Fundamentally, however, this notion depends on the honesty of at least some players. We put forward and implement a stronger notion, rational secure computation, that does not depend on player honesty, but solely on player rationality. The key to our implementation is showing that the ballot-box - the venerable device used throughout the world to tally secret votes securely - can actually be used to securely compute any function. Our work bridges the fields of game theory and cryptography, and has broad implications for mechanism design.","Game theory,
Privacy,
Protocols,
Security,
Voting,
Bridges,
Cryptography,
Buildings,
Communication channels,
Computer science"
Experiences with GRIA &#8212; Industrial Applications on a Web Services Grid,"The GRIA project set out to make the grid usable by industry. The GRIA middleware is based on Web services, and designed to meet the needs of industry for security and business-to-business (B2B) service procurement and operation. It provides well-defined B2B models for accounting and QoS agreement, and proxy-free delegation to support account management and service federation. The GRIA v3 software is now being used by industry. By taking a business-oriented approach independent of the evolving Open Grid Services Architecture proposals from the Global Grid Forum, GRIA has demonstrated the need for a wider understanding of virtual organizations (VOs). Traditional academic VOs are persistent, resourceful and have logically centralized, membership-oriented management structures. In contrast, the GRIA experience has been that business VOs are likely to be project-focused and have distributed process-oriented management structures","Web services,
Business,
Grid computing,
Middleware,
Computer industry,
Computer architecture,
Resource management,
Application software,
Technological innovation,
Industrial electronics"
Scene-adapted structured light,"In order to overcome several limitations of structured light 3D acquisition methods, the colors, intensities, and shapes of the projected patterns are adapted to the scene. Based on a crude estimate of the scene geometry and reflectance characteristics, the local intensity ranges in the projected patterns are adapted, in order to avoid over- and under-exposure in the image. This avoids the infamous specularity problems and generally increases accuracy. The estimated geometry also helps to limit the effect of aliasing caused by the sampling of foreshortened patterns. Furthermore, the approach also accounts for the adverse effects that small motions during scanning would normally have. Moreover, the approach yields a confidence measure at every pixel of the range image. Last but not least, the scanner consists of consumer products only, and therefore is cheap.","Layout,
Geometry,
Cameras,
Costs,
Reflectivity,
Robustness,
Computer science,
Shape,
Image sampling,
Pixel"
Testing Web services,"Summary form only given. Web services present a promising software technology, which provides application-to-application interaction. They are based on communication protocols, service descriptions, and service discovery and are built on top of existing Web protocols and based on open XML standards. Web services are described using Web Services Description Language (WSDL), and the universal description, discovery, and integration directory provide a registry of Web services descriptions. Testing Web services is important for both the Web service provider and the Web service user. This paper proposes a technique for testing Web services using mutation analysis. The technique is based on applying mutation operators to the WSDL document in order to generate mutated Web service interfaces that are used to test the Web service. For this purpose, we define mutant operators that are specific to WSDL documents. Our empirical results have shown the usefulness of this technique.","Testing,
Web services,
Genetic mutations,
Protocols,
XML,
Software systems,
Web and internet services,
Computer science,
Mathematics,
Communication standards"
Finding glass,"This paper addresses the problem of finding glass objects in images. Visual cues obtained by combining the systematic distortions in background texture occurring at the boundaries of transparent objects with the strong highlights typical of glass surfaces are used to train a hierarchy of classifiers, identify glass edges, and find consistent support regions for these edges. Qualitative and quantitative experiments involving a number of different classifiers and real images are presented.","Glass,
Surface texture,
Image edge detection,
Shape,
Computed tomography,
Switches,
Computer science,
Object detection,
Detectors,
Displays"
Distributed vector Processing of a new local MultiScale Fourier transform for medical imaging applications,"The recently developed S-transform (ST) combines features of the Fourier and Wavelet transforms; it reveals frequency variation over both space and time. It is a potentially powerful tool that can be applied to medical image processing including texture analysis and noise filtering. However, calculation of the ST is computationally intensive, making conventional implementations too slow for many medical applications. This problem was addressed by combining parallel and vector computations to provide a 25-fold reduction in computation time. This approach could help accelerate many medical image processing algorithms.","Fourier transforms,
Biomedical imaging,
Biomedical image processing,
Concurrent computing,
Wavelet transforms,
Frequency,
Image analysis,
Image texture analysis,
Filtering,
Medical services"
CryptoScan: A Secured Scan Chain Architecture,Scan based testing is a powerful and popular test technique. However the scan chain can be used by an attacker to decipher the cryptogram. The present paper shows such a side-channel attack on LFSR-based stream ciphers using scan chains. The paper subsequently discusses a strategy to build the scan chains in a tree based pattern with a selfchecking compactor. It has been shown that such a structure prevents such scan based attacks but does not compromise on fault coverage.,"Cryptography,
Circuit faults,
Circuit testing,
Boolean functions,
Design for testability,
Hardware,
Clocks,
Computer architecture,
Sequential analysis,
Computer science"
Volume rendering in the presence of partial volume effects,"In tomographic imagery, partial volume effects (PVEs) cause several artifacts in volume renditions. In X-ray computed tomography (CT), for example, soft-tissue-like pseudo structures appear in bone-to-air and bone-to-fat interfaces. Further, skin, which is identical to soft tissue in terms of CT number, obscures the rendition of the latter. The purpose of This work is to demonstrate these phenomena and to provide effective solutions that yield significantly improved renditions. We introduce two methods that detect and classify voxels with PVE in X-ray CT. Further, a method is described to automatically peel skin so that PVE-resolved renditions of bone and soft tissue reveal considerably more detail. In the first method to address PVE, called the fraction measure (FM) method, the fraction of each tissue material in each voxel v is estimated by taking into account the intensities of the voxels neighboring v. The second method, called uncertainty principle (UP) method, is based on the following postulate (Saha and Udupa, 2001): In any acquired image, voxels with the highest uncertainty occur in the vicinity of object boundaries. The removal of skin is achieved by means of mathematical morphology. Volume renditions have been created before and after applying the methods for several patient CT datasets. A mathematical phantom experiment involving different levels of PVE has been conducted by adding different degrees of noise and blurring. A quantitative evaluation is done utilizing the mathematical phantom and clinical CT data wherein an operator carefully masked out voxels with PVE in the segmented images. All results have demonstrated the enhanced quality of display of bone and soft tissue after applying the proposed methods. The quantitative evaluations indicate that more than 98% of the voxels with PVE are removed by the two methods and the second method performs slightly better than the first. Further, skin peeling vividly reveals fine details in the soft tissue structures.","Computed tomography,
Skin,
Biological tissues,
X-ray imaging,
Bones,
Imaging phantoms,
Rendering (computer graphics),
X-ray detection,
X-ray detectors,
Biological materials"
Implementation of a Transcutaneous Charger for Fully Implantable Middle Ear Hearing Device,"A transcutaneous charger for the fully implantable middle ear hearing device (F-IMEHD), which can monitor the charging level of battery, has been designed and implemented. In order to recharge the battery of F-IMEHD, the electromagnetic coupling between primary coil at outer body and secondary coil at inner body has been used. Considering the implant condition of the F-IMEHD, the primary coil and the secondary coil have been designed. Using the resonance of LC tank circuit at each coil, transmission efficiency was increased. Since the primary and the secondary coil are magnetically coupled, the current variation of the primary coil is related with the impedance of internal resonant circuit. Using the principle mentioned above, the implanted module could transmit outward the information about charging state of battery or coupling between two coils by the changing internal impedance. As in the demonstrated results of experiment, the implemented charger has supplied the sufficient operating voltage for the implanted battery within about 10 mm distance. And also, it has been confirmed that the implanted module can transmit information outward by control of internal impedance","Ear,
Auditory system,
Coils,
Batteries,
Impedance,
Magnetic resonance,
RLC circuits,
Coupling circuits,
Electromagnetic coupling,
Implants"
Preliminary Results for GAMI: A Genetic Algorithms Approach to Motif Inference,"We have developed GAMI, an approach to motif inference that uses a genetic algorithms search and is designed specifically to work with divergent species and possibly long nucleotide sequences. The system design reduces the size of the search space as compared to typical window-location approaches for motif inference. This paper describes the motivation and system design for GAMI, discusses how we have designed the search space and compares this to the search space of other approaches, and presents initial results with data from the literature and from novel tasks. GAMI is able to find a host of putative conserved patterns; possible approaches for validating the utility of the conserved regions are discussed.","Genetic algorithms,
Sequences,
Genomics,
Bioinformatics,
Humans,
Biology,
Inference algorithms,
Computer science,
Educational institutions,
Laboratories"
Segment-based injection attacks against collaborative filtering recommender systems,"Significant vulnerabilities have recently been identified in collaborative filtering recommender systems. Researchers have shown that attackers can manipulate a system's recommendations by injecting biased profiles into it. In this paper, we examine attacks that concentrate on a targeted set of users with similar tastes, biasing the system's responses to these users. We show that such attacks are both pragmatically reasonable and also highly effective against both user-based and item-based algorithms. As a result, an attacker can mount such a ""segmented"" attack with little knowledge of the specific system being targeted and with strong likelihood of success.",
Packet routing in dynamically changing networks on chip,"On-line routing strategies for communication in a dynamic network on chip (DyNoC) environment are presented. The DyNoC has been presented as a medium supporting communication among modules which are dynamically placed on a reconfigurable device at run-time. Using simulation, we compare the performance of an adaptive Q-routing algorithm to the well known XY-routing strategy. Both algorithms are adapted to support communication on the DyNoC which is equivalent to routing on meshes with obstacles. In our experiments, Q-routing proves its performance under varying network load while using only local information for its routing decisions.","Routing,
Intelligent networks,
Network-on-a-chip,
Runtime,
Logic devices,
Reconfigurable logic,
Network topology,
Tiles,
Computer science,
Costs"
Mobile learning: current trend and future challenges,"Due to the thrive of mobile network and portable device, distance learning is evolved from desktop computer to mobile device. Mobile learning is the use of mobile or wireless devices for learning while the learner is on the move. In this study, the strengths of mobile learning supported by industrial and academic projects are elaborated. These also mark the current status of mobile learning. On the basis of the characteristic and scenario of current mobile learning, challenging issues from the perspective of cognitive learning are addressed to reflect the fundamental needs for effective mobile learning. These addressed issues serve as the initiation for the needs of academic evaluation and solid theoretic framework for the implementation of mobile learning from the view of cognitive science, instead of technological evolution.","Personal digital assistants,
Mobile computing,
Wireless networks,
Humans,
Personal communication networks,
Computer aided manufacturing,
Companies,
Productivity,
Computer aided instruction,
Computer networks"
Examination of a simple pulse-blanking technique for radio frequency interference mitigation,"Radiometry at L band can be adversely impacted by radio frequency interference (RFI) due to the presence of numerous sources, especially pulsed RFI from radars operating below 1400 MHz. RFI mitigation is very important to deal with this problem. A simple strategy for reducing pulsed RFI, termed â€œasynchronous pulse blankingâ€ (APB), has been implemented in a digital receiver developed at Ohio State University. This paper presents results from a simulation of the APB algorithm. Several aspects of algorithm use and performance are reported, including means for choosing the algorithm's parameters and the robustness of the method in a realistic RFI environment. Effects of the blanking process on the final output are also examined.","Blanking,
Radiometers,
Field programmable gate arrays,
Radar,
Hardware,
Interference,
L-band"
A new metric for robustness with application to job scheduling,"Scheduling strategies for parallel and distributed computing have mostly been oriented toward performance, while striving to achieve some notion of fairness. With the increase in size, complexity, and heterogeneity of today's computing environments, we argue that, in addition to performance metrics, scheduling algorithms should be designed for robustness. That is, they should have the ability to maintain performance under a wide variety of operating conditions. Although robustness is easy to define, there are no widely used metrics for this property. To this end, we present a methodology for characterizing and measuring the robustness of a system to a specific disturbance. The methodology is easily applied to many types of computing systems and it does not require sophisticated mathematical models. To illustrate its use, we show three applications of our technique to job scheduling; one supporting a previous result with respect to backfilling, one examining overload control in a streaming video server, and one comparing two different scheduling strategies for a distributed network service. The last example also demonstrates how consideration of robustness leads to better system design as we were able to devise a new and effective scheduling heuristic.","Robustness,
Processor scheduling,
Uncertainty,
Distributed computing,
Application software,
Computer science,
Cities and towns,
Concurrent computing,
Measurement,
Mathematical model"
A time-series biclustering algorithm for revealing co-regulated genes,"Although existing bicluster algorithms claimed to be able to discover co-regulated genes under a subset of given experiment conditions, they ignore the inherent sequential relationship between crucial time points and thus are not applicable to analyze time-series gene expression data. A simple and effective deletion-based algorithm, using the mean squared residue score as a measure, was developed to bicluster time-series gene expression data. While enforcing a threshold value for the score, the algorithm alternately eliminates genes and time points according to their correlation to the bicluster. To ensure the time locality, only the starting and ending points in the time interval are eligible for deletion. As a result, the number of genes and the length of time interval are simultaneously maximized. Our experimental results shown that the proposed method is capable of identifying co-regulated genes characterized by partial time-course data that previous methods failed to discover.","Gene expression,
Regulators,
Fungi,
Hidden Markov models,
Time series analysis,
Chaos,
Computer science,
Data engineering,
Information analysis,
Algorithm design and analysis"
ECG Feature Extraction and Classification Using Wavelet Transform and Support Vector Machines,"This paper presents a new approach to the feature extraction for reliable heart rhythm recognition. This system of classification is comprised of three components including data preprocessing, feature extraction and classification of ECG signals. Two different feature extraction methods are applied together to obtain the feature vector of ECG data. The wavelet transform is used to extract the coefficients of the transform as the features of each ECG segment. Simultaneously, autoregressive modelling (AR) is also applied to obtain the temporal structures of ECG waveforms. Then the support vector machine (SVM) with Gaussian kernel is used to classify different ECG heart rhythm. Computer simulations are provided to verify the performance of the proposed method. From computer simulations, the overall accuracy of classification for recognition of 6 heart rhythm types reaches 99.68%","Electrocardiography,
Feature extraction,
Wavelet transforms,
Support vector machines,
Support vector machine classification,
Heart,
Rhythm,
Computer simulation,
Data preprocessing,
Data mining"
"An infrastructure for development of object-oriented, multi-level configuration management services","In an integrated development environment, the ability to manage the evolution of a software system in terms of logical abstractions, compositions, and their interrelations is crucial to successful software development. This paper presents a novel framework and infrastructure, Molhado, upon which to build object-oriented software configuration management (SCM) services in a SCM-centered integrated development environment. Key contributions of this paper include a product versioning model, an extensible, logical, and object-oriented system model, and a reusable product versioning SCM infrastructure, that allow new types of objects to be implemented as extensions of the system model's basic entities. Versions and configurations of objects are managed at different levels of abstraction and granularity. A new SCM-centered editing environment or development environment for a specific development paradigm can be rapidly realized by re-using Molhado's infrastructure and implementing new object types and their associated tools. This paper also demonstrates our approach in creating prototypes of SCM-centered development environments for different paradigms.","Programming,
Object oriented modeling,
Computer science,
Software development management,
Software systems,
Engineering management,
Environmental management,
Software engineering,
Software maintenance,
Permission"
Feedback-based dynamic voltage and frequency scaling for memory-bound real-time applications,"Dynamic voltage and frequency scaling is increasingly being used to reduce the energy requirements of embedded and real-time applications by exploiting idle CPU resources, while still maintaining all application's real-time characteristics. Accurate predictions of task run-times are key to computing the frequencies and voltages that ensure that all tasks' real-time constraints are met. Past work has used feedback-based approaches, where applications' past CPU utilizations are used to predict future CPU requirements. Mispredictions in these approaches can lead to missed deadlines, suboptimal energy savings, or large overheads due to frequent changes to the chosen frequency or voltage. One shortcoming of previous approaches is that they ignore other 'indicators' of future CPU requirements, such as the frequency of I/O operations, memory accesses, or interrupts. This paper addresses the energy consumptions of memory-bound real-time applications via a feedback loop approach, based on measured task run-times and cache miss rates. Using cache miss rates as indicator for memory access rates introduces a more reliable predictor of future task run-times. Even in modern processor architectures, memory latencies can only be hidden partially, therefore, cache misses can be used to improve the run-time predictions by considering potential memory latencies. The results shown in this paper indicate improvements in both the number of deadlines met and the amount of energy saved.","Dynamic voltage scaling,
Frequency,
Runtime,
Energy consumption,
Processor scheduling,
Application software,
Delay,
Batteries,
Monitoring,
Computer science"
Constraint satisfaction in dynamic Web service composition,"Web services composition is a crucial aspect of Web services technology, which gives us the opportunity for selecting new services and best suits our needs at the moment. Doing this automatically would require us to compute our criteria for composing candidate services. Our research represent all criteria guiding the selection of the services as constraints then choose an optimal set of services to satisfy the customers requirements. This approach reduces the dynamic composition of Web services to a constraint satisfaction problem. This work support in future to semantic Web process.","Web services,
Computer science,
Mathematics,
Web and internet services,
Runtime,
Educational institutions,
Semantic Web,
Distributed computing,
Information resources,
Search engines"
Detection of Anchor Points for 3D Face Veri.cation,This paper outlines methods to detect key anchor points in 3D face scanner data. These anchor points can be used to estimate the pose and then match the test image to a 3D face model. We present two algorithms for detecting face anchor points in the context of face verification; One for frontal images and one for arbitrary pose. We achieve 99% success in finding anchor points in frontal images and 86% success in scans with large variations in pose and changes in expression. These results demonstrate the challenges in 3D face recognition under arbitrary pose and expression. We are currently working on robust ?tting algorithms to localize more precisely the anchor points for arbitrary pose images.,"Face detection,
Shape,
Face recognition,
Iterative closest point algorithm,
Testing,
Pattern recognition,
Nose,
Mouth,
Computer science,
Data engineering"
Supporting predictive change impact analysis: a control call graph based technique,"Change impact analysis plays an important role in software maintenance. It allows developers assessing the possible effects of a change. We present, in this paper, a new static technique supporting software change impact analysis. The technique uses a new model based on control call graphs. It captures the control related to components calls and generates the different control flow paths in a program. The generated paths, in a compacted form, are used to identify the potential set of components that may be affected by a given change. Furthermore, the tool developed can be used to perform predictive impact analysis. It can also be used to support regression testing. We performed an experimental study on several Java programs. The reported results show that the proposed technique can predict impact sets that are more accurate than those obtained using traditional approaches based on call graphs.","Software engineering,
Software maintenance,
Information analysis,
Laboratories,
Mathematics,
Computer science,
Performance analysis,
Testing,
Java,
Software systems"
Soft tissue differentiation using multiband signatures of high resolution ultrasonic transmission tomography,"In this paper, we are interested in soft tissue differentiation by multiband images obtained from the High-Resolution Ultrasonic Transmission Tomography (HUTT) system using a spectral target detection method based on constrained energy minimization (CEM). We have developed a new tissue differentiation method (called ""CEM filter bank"") consisting of multiple CEM filters specially designed for detecting multiple types of tissues. Statistical inference on the output of the CEM filter bank is used to make a decision based on the maximum statistical significance rather than the magnitude of each CEM filter output. We test and validate this method through three-dimensional interphantom/intraphantom soft tissue classification where target profiles obtained from an arbitrary single slice are used for differentiation over multiple other tomographic slices. The performance of the proposed classifier is assessed using receiver operating characteristic analysis. We also apply our method to classify tiny structures inside a bovine kidney and sheep kidneys. Using the proposed method we can detect physical objects and biological tissues such as styrofoam balls, chicken tissue, calyces, and vessel-duct successfully.","Biological tissues,
Tomography,
Ultrasonic imaging,
Filter bank,
Acoustic imaging,
Object detection,
Biomedical engineering,
Acoustic pulses,
Attenuation,
Frequency"
A new framework for approximate labeling via graph cuts,"A new framework is presented that uses tools from duality theory of linear programming to derive graph-cut based combinatorial algorithms for approximating NP-hard classification problems. The derived algorithms include alpha-expansion graph cut techniques merely as a special case, have guaranteed optimality properties even in cases where alpha-expansion techniques fail to do so and can provide very tight per-instance suboptimality bounds in all occasions",
Measurement of highly active prefixes in BGP,"We conduct a systematic study on the pervasiveness and persistency of one specific phenomenon in the global routing system: a small set of highly active prefixes accounts for a large number of routing updates. Our data analysis shows that this phenomenon is commonly observed from monitors in many different ISPs, and exists throughout our 3-year study period. The analysis further shows that the majority of these prefixes are highly active for only one or a few days, while a small number of them are persistently active over long period of time. Case studies demonstrate that the causes of these high routing activity include topological failures, BGP path exploration, protocol defects, and the failure of turning on protection mechanisms.",
Collimator optimization for detection and quantitation tasks: application to gallium-67 imaging,"We describe a new approach to the problem of collimator optimization in nuclear medicine; our methodology is illustrated for the challenging case of gallium-67 imaging. Collimator-design methods based on empirical rules, such as specification of an allowable level of single-septal penetration (SSP) at a fixed energy, are especially inappropriate for radionuclides characterized by an abundance of high-energy contaminant photons that scatter in the patient, collimator, and/or detector before detection within one of a few photopeak energy windows. Lead X-rays produced in the collimator are an additional source of contamination. We designed optimal collimation for /sup 67/Ga based on relevant clinical imaging tasks and a realistic simulation of photon transport in a phantom, collimator, and detector. Collimator designs were compared on the basis of performance in lesion detection, as predicted by a three-channel Hotelling observer (CHO), as well as in tumor and background activity estimation (EST), quantified by task-specific signal-to-noise ratios (SNRs). The optimal values of collimator lead content were 22.0 and 23.8 g/cm/sup 2/, respectively, for CHO and EST, while the optimal geometric resolution values were 1.8 and 1.6 cm full-width at half-maximum (FWHM), respectively, at a distance of 23.5 cm. The resolution of a commercially available medium-energy low-penetration collimator (MELP) is 1.9 cm FWHM at this distance. The optimal values for SSP at 300 keV were 7.3% and 5.8% based on CHO and EST, respectively, compared to 5.2% for the MELP collimator. Compared with the commercial MELP collimator, the /sup 67/Ga collimator optimized for tumor detection or activity estimation tasks provided improved geometric spatial resolution with reduced geometric efficiency and, surprisingly, allowed an increased level of single-septal penetration.","Optical collimators,
Detectors,
Optimization methods,
Nuclear medicine,
Electromagnetic scattering,
Particle scattering,
X-ray scattering,
Contamination,
Optical imaging,
Imaging phantoms"
Multi-objective techniques in genetic programming for evolving classifiers,"The application of multi-objective evolutionary computation techniques to the genetic programming of classifiers has the potential to both improve the accuracy and decrease the training time of the classifiers. The performance of two such algorithms is investigated on the even 6-parity problem and the Wisconsin breast cancer, Iris and Wine data sets from the UCI repository. The first method explores the addition of an explicit size objective as a parsimony enforcement technique. The second represents a program's classification accuracy on each class as a separate objective. Both techniques give a lower error rate with less computational cost than was achieved using a standard GP with the same parameters.","Genetic programming,
Computational efficiency,
Evolutionary computation,
Error analysis,
Diversity reception,
Computer science,
Information technology,
Australia,
Application software,
Breast cancer"
Fault Diagnosis and Fault Tolerant Control for Wheeled Mobile Robots under Unknown Environments: A Survey,"Fault detection and diagnosis (FDD) and fault tolerant control (FTC) are increasingly important for wheeled mobile robots (WMRs), especially those in unknown environments such as planetary exploration. Due to the importance of reliability and safe operation of WMRs, this paper presents a survey of state-of-the-art in FDD & FTC of WMRs under unknown environments. Firstly, we briefly introduce main components, typical kinematics models and fault models of WMRs and error models of inertial navigation sensors. Secondly, we discuss main approaches for FDD/FTC of WMRs, including multiple model based approach, particle filter based approach, sensor fusion based approach, layered fault tolerant architecture and so on. At last, the main challenges, difficulties and some future trends for the field are offered.","Fault diagnosis,
Fault tolerance,
Mobile robots,
Fault detection,
Sensor fusion,
Robot sensing systems,
Computer aided instruction,
Educational institutions,
Information science,
Particle filters"
Optimal ISP subscription for Internet multihoming: algorithm design and implication analysis,"Multihoming is a popular method used by large enterprises and stub ISPs to connect to the Internet to reduce cost and improve performance. Recently researchers have studied the potential benefits of multihoming and proposed protocols and algorithms to realize these benefits. They focus on how to dynamically select which ISPs to use for forwarding and receiving packets, and assume that the set of subscribed ISPs is given a priori. In practice, a user often has the freedom to choose which subset of ISPs among all available ISPs to subscribe to. We call the problem of how to choose the optimal set of ISPs the ISP subscription problem. In this paper, We design a dynamic programming algorithm to solve the ISP subscription problem optimally. We also design a more efficient algorithm for a large class of common pricing functions. Using real traffic traces and realistic pricing data, we show that our algorithm reduces users' cost. Next we study how ISPs respond to users' optimal ISP subscription by adjusting their pricing strategies. We call this problem the ISP pricing problem. Using a realistic charging model, we formulate the problem as a non-cooperative game. We first prove that if cost is the only criterion used by a user to determine which subset of ISPs to subscribe to, at any equilibrium all ISPs receive zero revenue. We then study a more practical formulation in which different ISPs provide different levels of reliability and users choose ISPs to both improve reliability and reduce cost. We analyze this problem and show that at any equilibrium an ISP's revenue is positive and determined by its reliability.","Algorithm design and analysis,
Subscriptions,
Internet,
Pricing,
Routing,
Dynamic programming,
Heuristic algorithms,
Cost function,
Computer science,
Protocols"
Floating-point fused multiply-add: reduced latency for floating-point addition,"In this paper we propose an architecture for the computation of the double-precision floating-point multiply-add fused (MAF) operation A+(B/spl times/C) that permits to compute the floating-point addition with lower latency than floating-point multiplication and MAF. While previous MAF architectures compute the three operations with the same latency, the proposed architecture permits to skip the first pipeline stages, those related with the multiplication B/spl times/C, in case of an addition. For instance, for a MAF unit pipelined into three or five stages, the latency of the floating-point addition is reduced to two or three cycles, respectively. To achieve the latency reduction for floating-point addition, the alignment shifter, which in previous organizations is in parallel with the multiplication, is moved so that the multiplication can be bypassed. To avoid that this modification increases the critical path, a double-datapath organization is used, in which the alignment and normalization are in separate paths. Moreover, we use the techniques developed previously of combining the addition and the rounding and of performing the normalization before the addition.","Delay,
Computer architecture,
Computer science,
Pipelines,
Contracts,
Hardware,
Concurrent computing,
Digital arithmetic"
A safe regression test selection technique for database-driven applications,"Regression testing is a widely-used method for checking whether modifications to software systems have adversely affected the overall functionality. This is potentially an expensive process, since test suites can be large and time-consuming to execute. The overall costs can be reduced if tests that cannot possibly be affected by the modifications are ignored. Various techniques for selecting subsets of tests for re-execution have been proposed, as well as methods for proving that particular test selection criteria do not omit relevant tests. However, current selection techniques are focused on identifying the impact of modifications on program state. They assume that the only factor that can change the result of a test case is the set of input values given for it, while all other influences on the behavior of the program (such as external interrupts or hardware faults) will be constant for each re-execution of the test. This assumption is impractical in the case of an important class of software system, i.e. systems which make use of an external persistent state, such as a database management system, to share information between application invocations. If applied naively to such systems, existing regression test selection algorithms will omit certain test cases which could in fact be affected by the modifications to the code. In this paper, we show why this is the case, and propose a new definition of safety for regression test selection that takes into account the interactions of the program with a database state. We also present an algorithm and associated tool that safely performs test selection for database-driven applications, and (since efficiency is an important concern for test selection algorithms) we propose a variant that defines safety in terms of database state alone. This latter form of safety allows more efficient regression testing to be performed for applications in which program state is used only as a temporary holding space for data from the database. The claims of increased efficiency of both forms of safety are supported by the results of an empirical comparison with existing techniques.","Databases,
System testing,
Safety,
Application software,
Software systems,
Costs,
Performance evaluation,
Computer science,
Software testing,
Hardware"
Nonbinary quantum Reed-Muller codes,"We construct nonbinary quantum codes from classical generalized Reed-Muller codes and derive the conditions under which these quantum codes can be punctured. We provide a partial answer to a question raised by Grassl, Beth and Rotteler on the existence of q-ary quantum MDS codes of length n with q les n les q2 - 1","Quantum computing,
Quantum mechanics,
Computer science,
Error correction codes,
Computer errors,
Fault tolerance"
Japanese Computational Grid Research Project: NAREGI,"The National Research Grid Initiative (NAREGI) is one of the major Japanese national IT projects currently being conducted. NAREGI will cover the period 2003-2007, and collaboration among industry, academia, and the government will play a key role in its success. The Center for Grid Research and Development has been established as a center for R&D of high-performance, scalable grid middleware technologies, which are aimed at enabling major computing centers to host grids over high-speed networks to provide a future computational infrastructure for scientific and engineering research in the 21st century. As an example of utilizing such grid computing technologies, the Center for Application Research and Development is conducting research on leading-edge, grid-enabled nanoscience and nanotechnology simulation applications, which will lead to the discovery and development of new materials and next-generation nanodevices. These two centers are collaborating to establish daily research use of a multiteraflop grid testbed infrastructure, which will be built to demonstrate the advantages of grid technologies for future applications in all areas of science and engineering.","Grid computing,
Research and development,
Collaboration,
Computer networks,
Government,
Middleware,
High-speed networks,
Nanotechnology,
Computational modeling,
Conducting materials"
Short PCPs verifiable in polylogarithmic time,"We show that every language in NP has a probabilistically checkable proof of proximity (i.e., proofs asserting that an instance is ""close"" to a member of the language), where the verifier's running time is polylogarithmic in the input size and the length of the probabilistically checkable proof is only polylogarithmically larger that the length of the classical proof. (Such a verifier can only query polylogarithmically many bits of the input instance and the proof. Thus it needs oracle access to the input as well as the proof, and cannot guarantee that the input is in the language - only that it is close to some string in the language.) If the verifier is restricted further in its query complexity and only allowed q queries, then the proof size blows up by a factor of 2/sup (log n)c/q/ where the constant c depends only on the language (and is independent of q). Our results thus give efficient (in the sense of running time) versions of the shortest known PCPs, due to Ben-Sasson et al. (STOC '04) and Ben-Sasson and Sudan (STOC '05), respectively. The time complexity of the verifier and the size of the proof were the original emphases in the definition of holographic proofs, due to Babai et al. (STOC '91), and our work is the first to return to these emphases since their work. Of technical interest in our proof is a new complete problem for NEXP based on constraint satisfaction problems with very low complexity constraints, and techniques to arithmetize such constraints over fields of small characteristic.","Computer science,
Holography,
Artificial intelligence,
Laboratories,
Computational complexity"
Discriminative training for object recognition using image patches,"We present a method for automatically learning discriminative image patches for the recognition of given object classes. The approach applies discriminative training of log-linear models to image patch histograms. We show that it works well on three tasks and performs significantly better than other methods using the same features. For example, the method decides that patches containing an eye are most important for distinguishing face from background images. The recognition performance is very competitive with error rates presented in other publications. In particular, a new best error rate for the Caltech motorbikes data of 1.5% is achieved.","Object recognition,
Data mining,
Image recognition,
Detectors,
Error analysis,
Layout,
Humans,
Feature extraction,
Object detection,
Computer science"
A reference architecture for Web browsers,"A reference architecture for a domain captures the fundamental subsystems common to systems of that domain as well as the relationships between these subsystems. Having a reference architecture available can aid both during maintenance and at design time: it can improve understanding of a given system, it can aid in analyzing tradeoffs between different design options, and it can serve as a template for designing new systems and re-engineering existing ones. In this paper, we examine the history of the Web browser domain and identify several underlying phenomena that have contributed to its evolution. We develop a reference architecture for Web browsers based on two well known open source implementations, and we validate it against two additional implementations. Finally, we discuss our observations about this domain and its evolutionary history; in particular, we note that the significant reuse of open source components among different browsers and the emergence of extensive Web standards have caused the browsers to exhibit ""convergent evolution"".","Service oriented architecture,
Computer architecture,
History,
World Wide Web,
Internet,
Protocols,
HTML,
Cascading style sheets,
Java,
Computer science"
Fast configurable-cache tuning with a unified second-level cache,"Tuning a configurable cache subsystem to an application can greatly reduce memory hierarchy energy consumption. Previous tuning methods use a level one configurable cache only, or a second level with separate instruction and data configurable caches. The authors instead used a commercially-common unified second level, a seemingly minor difference that actually expands the configuration space from 500 to about 20,000. Additive way tuning for tuning a cache subsystem was developed with this large space, yielding 62% energy savings and 35% performance improvements over a non-configurable cache, greatly outperforming an extension of a previous method.",
Bilinear Sparse Coding for Invariant Vision,"Recent algorithms for sparse coding and independent component analysis (ICA) have demonstrated how localized features can be learned from natural images. However, these approaches do not take image transformations into account. We describe an unsupervised algorithm for learning both localized features and their transformations directly from images using a sparse bilinear generative model. We show that from an arbitrary set of natural images, the algorithm produces oriented basis filters that can simultaneously represent features in an image and their transformations. The learned generative model can be used to translate features to different locations, thereby reducing the need to learn the same feature at multiple locations, a limitation of previous approaches to sparse coding and ICA. Our results suggest that by explicitly modeling the interaction between local image features and their transformations, the sparse bilinear approach can provide a basis for achieving transformation-invariant vision.",
Distributed Sampling-Based Roadmap of Trees for Large-Scale Motion Planning,High-dimensional problems arising from complex robotic systems test the limits of current motion planners and require the development of efficient distributed motion planners that take full advantage of all the available resources. This paper shows how to effectively distribute the computation of the Sampling-based Roadmap of Trees (SRT) algorithm using a decentralized master-client scheme. The distributed SRT algorithm allows us to solve very high-dimensional problems that cannot be efficiently addressed with existing planners. Our experiments show nearly linear speedups with eighty processors and indicate that similar speedups can be obtained with several hundred processors.,"Large-scale systems,
Motion planning,
Robot kinematics,
Distributed computing,
Orbital robotics,
Parallel algorithms,
Space exploration,
Concurrent computing,
Hypercubes,
Computer science"
Placement with symmetry constraints for analog layout design using TCG-S,"In order to handle device matching for analog circuits, some pairs of modules need to be placed symmetrically with respect to a common axis. In this paper, we deal with the module placement with symmetry constraints for analog design using the transitive closure graph-sequence (TCG-S) representation. Since the geometric relationships of modules are transparent to TCG-S and its induced operations, TCG-S has better flexibility than previous works in dealing with symmetry constraints. We first propose the necessary and sufficient conditions of TCG-S for symmetry modules. Then, we propose a polynomial-time packing algorithm for a TCG-S with symmetry constraints. Experimental results show that the TCG-S based algorithm results in the best area utilization.","Analog circuits,
Computer industry,
Electronics industry,
Industrial relations,
Information management,
Industrial electronics,
Analog computers,
Information science,
Sufficient conditions,
Polynomials"
Multi-robot forest coverage,"One of the main applications of mobile robots is terrain coverage: visiting each location in known terrain. Terrain coverage is crucial for lawn mowing, cleaning, harvesting, search-and-rescue, intrusion detection and mine clearing. Naturally, coverage can be sped up with multiple robots. In this paper, we describe multi-robot forest coverage, a new multi-robot coverage algorithm based on an algorithm by Even et al. (2004) for finding a tree cover with trees of balanced weights. The cover time of multi-robot forest coverage is at most eight times larger than optimal, and our experiments show it to perform significantly better than existing multi-robot coverage algorithms.","Tree graphs,
Polynomials,
Mobile robots,
Cleaning,
Intrusion detection,
Costs,
Computer science,
Application software,
Electrical capacitance tomography"
Routing path optimization in optical burst switched networks,,"Routing,
Intelligent networks,
Optical fiber networks,
Optical buffering,
Optical packet switching,
Switches,
Telecommunication traffic,
Delay,
Switching circuits,
Computer science"
Improved phonetic speaker recognition using lattice decoding,"The current ""state-of-the-art"" in phonetic speaker recognition uses relative frequencies of phone n-grams as features for training speaker models and for scoring test-target pairs. Typically, these relative frequencies are computed from a simple 1-best phone decoding of the input speech. We present results on the Switchboard-2 corpus, where we compare 1-best phone decodings versus lattice phone decodings for the purposes of performing phonetic speaker recognition. The phone decodings are used to compute relative frequencies of phone bigrams, which are then used as inputs for two standard phonetic speaker recognition systems, a system based on log-likelihood ratios (LLRs) (Andrews, W.D. et al., Proc. Eurospeech, p.149-53, 2001; Proc. ICASSP, vol.I, p.149-53, 2002), and a system based on support vector machines (SVMs) (Campbell, W.M. et al., Advances in Neural Information Processing Systems, vol.16, 2004). In each experiment, the lattice phone decodings achieve relative reductions in equal-error rate (EER) of between 31% and 66% below the EERs of the 1-best phone decodings. Our best phonetic system achieves an EER of 2.0% on 8-conversation training and 1.4% when combined with a GMM-based system.","Speaker recognition,
Lattices,
Decoding,
Speech,
Frequency estimation,
Testing,
Support vector machines,
Computer science,
Sampling methods,
NIST"
Improved tool support for the investigation of duplication in software,"Code duplication is a well documented problem in software systems. There has been considerable research into techniques for detecting duplication in software, and there are several effective tools to perform this task. However, a common problem with such tools is that the result set returned can be too large to handle without complementary tool support. The goal of this paper is to describe the criteria for a complete tool that is designed to aid in the comprehension of cloning within a software system. Furthermore, we present a prototype of such a tool and demonstrate the value of its features through a case study on the Apache httpd Web server. For example, in our study we found that a single subsystem comprising only 17% of the system code contained 38.8% of the clones.","Software tools,
Cloning,
Software systems,
Software prototyping,
Filtering,
Navigation,
Software architecture,
Computer science,
Software performance,
Prototypes"
Performance Effective Task Scheduling Algorithm for Heterogeneous Computing System,"Finding an optimal solution to the problem of scheduling an application modeled by a directed acyclic graph (DAG) onto a distributed system is known to be NP-complete. The complexity of the problem increases when task scheduling is to be done in a heterogeneous computing system, where the processors in the network may not be identical and take different amounts of time to execute the same task. This paper introduces a performance effective task scheduling (PETS) algorithm for network of heterogeneous system, with complexity O(v+e) (p+log v), which provides optimal results for applications represented by DAGs. The performance of the algorithm is illustrated by comparing the schedule length, speedup, efficiency and the scheduling time with existing algorithms such as, heterogeneous earliest finish time (HEFT) and critical-path on a processor (CPOP) and levelized min time (LMT) reported in this paper. The comparison study based on both randomly generated graphs and graphs of some real applications shows that PETS algorithm substantially outperforms existing algorithms","Scheduling algorithm,
Processor scheduling,
Computer networks,
Positron emission tomography,
Partitioning algorithms,
Distributed computing,
Computer applications,
Computer science,
Information technology,
Educational institutions"
EnPiT: filtered back-projection algorithm for helical CT using an n-Pi acquisition,"In this paper, we formulate a reconstruction algorithm for an n-Pi acquisition, where n can be any positive odd integer. The algorithm is a generalization of the method presented in (Bontus et al. 2003). It is based on the results obtained by Katsevich (2004). For the algorithm, different sets of filter-lines have to be defined. We describe the variation of these lines along the detector in some detail, before we discuss, how the method gives all Radon-plane contributions the correct weighting. The different sets of filter-lines are all contained within the n-Pi window, such that a practical realization is possible. Reconstruction results, which we present in the final section, show convincing image quality.","Image reconstruction,
Band pass filters,
Detectors,
Filtering,
Digital filters,
Computed tomography,
Reconstruction algorithms,
Algorithm design and analysis,
Sampling methods,
Equations"
Multi-objective optimisation using S-metric selection: application to three-dimensional solution spaces,"The S-metric or hypervolume measure is a distinguished quality measure for solution sets in Pareto optimisation. Once the aim to reach a high S-metric value is appointed, it seems to be promising to directly incorporate it in the optimisation algorithm. This idea has been implemented in the SMS-EMOA, an evolutionary multi-objective optimisation algorithm (EMOA) using the hypervolume measure within its selection operator. Solutions are rated according to their contribution to the dominated hypervolume of the current population. Up to now, the SMS-EMOA has only been applied to functions with two objectives. The work at hand extends these studies, by surveying the behaviour of the algorithm on three-objective problems. Additionally, a new efficient algorithm for the computation of the contributions to the dominated hypervolume in three-dimensional solution spaces is presented. Different variants of selection operators are proposed. Among these, a new one is presented that rates a solution concerning the number of solutions dominating it. So, solutions in less explored regions are preferred. This rating is an efficient alternative to the S-metric criterion whenever a selection among dominated solutions has to be made. Comparative studies on standard benchmark problems show that the SMS-EMOA clearly outperforms other well established EMOA. First results on a challenging real-world problem have been obtained, namely the multipoint design of an airfoil involving three objectives and nonlinear constraints. Not only a clear improvement of the baseline design but a good coverage of the Pareto front with a small limited number of points has been achieved.","Pareto optimization,
Automotive components,
Design optimization,
Evolutionary computation,
Extraterrestrial measurements,
Pareto analysis,
Computer science,
Electrostatic precipitators,
Genetic mutations,
Testing"
Efficient aggregation of delay-constrained data in wireless sensor networks,,"Intelligent networks,
Wireless sensor networks,
Delay,
Traffic control,
Relays,
Telecommunication traffic,
Computer science,
Educational institutions,
Application software,
Temperature sensors"
An estimation/correction algorithm for detecting bone edges in CT images,"The normal direction of the bone contour in computed tomography (CT) images provides important anatomical information and can guide segmentation algorithms. Since various bones in CT images have different sizes, and the intensity values of bone pixels are generally nonuniform and noisy, estimation of the normal direction using a single scale is not reliable. We propose a multiscale approach to estimate the normal direction of bone edges. The reliability of the estimation is calculated from the estimated results and, after re-scaling, the reliability is used to further correct the normal direction. The optimal scale at each point is obtained while estimating the normal direction; this scale is then used in a simple edge detector. Our experimental results have shown that use of this estimated/corrected normal direction improves the segmentation quality by decreasing the number of unexpected edges and discontinuities (gaps) of real contours. The corrected normal direction could also be used in postprocessing to delete false edges. Our segmentation algorithm is automatic, and its performance is evaluated on CT images of the human pelvis, leg, and wrist.","Image edge detection,
Bones,
Computed tomography,
Image segmentation,
Pixel,
Detectors,
Humans,
Pelvis,
Leg,
Wrist"
Transparent Checkpoint-Restart of Distributed Applications on Commodity Clusters,"We have created ZapC, a novel system for transparent coordinated checkpoint-restart of distributed network applications on commodity clusters. ZapC provides a thin visualization layer on top of the operating system that decouples a distributed application from dependencies on the cluster nodes on which it is executing. This decoupling enables ZapC to checkpoint an entire distributed application across all nodes in a coordinated manner such that it can he restarted from the checkpoint on a different set of cluster nodes at a later time. ZapC checkpoint-restart operations execute in parallel across different cluster nodes, providing faster checkpoint-restart performance. ZapC uniquely supports network state in a transport protocol independent manner, including correctly saving and restoring socket and protocol state for both TCP and UDP connections. We have implemented a ZapC Linux prototype and demonstrate that it provides low visualization overhead and fast checkpoint-restart times for distributed network applications without any application, library, kernel, or network protocol modifications","Operating systems,
Sockets,
Application virtualization,
Checkpointing,
Application software,
Transport protocols,
Linux,
Libraries,
Kernel,
Computer science"
Topology-based simplification for feature extraction from 3D scalar fields,"In this paper, we present a topological approach for simplifying continuous functions defined on volumetric domains. We introduce two atomic operations that remove pairs of critical points of the function and design a combinatorial algorithm that simplifies the Morse-Smale complex by repeated application of these operations. The Morse-Smale complex is a topological data structure that provides a compact representation of gradient flow between critical points of a function. Critical points paired by the Morse-Smale complex identify topological features and their importance. The simplification procedure leaves important critical points untouched, and is therefore useful for extracting desirable features. We also present a visualization of the simplified topology.","Feature extraction,
Data visualization,
Topology,
Computer science,
Data analysis,
Data structures,
Computer graphics,
Solid modeling,
Tree graphs,
Probability distribution"
Mining frequent spatio-temporal sequential patterns,"Many applications track the movement of mobile objects, which can be represented as sequences of timestamped locations. Given such a spatiotemporal series, we study the problem of discovering sequential patterns, which are routes frequently followed by the object. Sequential pattern mining algorithms for transaction data are not directly applicable for this setting. The challenges to address are: (i) the fuzziness of locations in patterns, and (ii) the identification of non-explicit pattern instances. In this paper, we define pattern elements as spatial regions around frequent line segments. Our method first transforms the original sequence into a list of sequence segments, and detects frequent regions in a heuristic way. Then, we propose algorithms to find patterns by employing a newly proposed substring tree structure and improving a priori technique. A performance evaluation demonstrates the effectiveness and efficiency of our approach.","Transaction databases,
Global Positioning System,
Frequency,
Computer science,
Application software,
Tracking,
Mobile computing,
Tree data structures,
History,
Pattern analysis"
Secure comparison of encrypted data in wireless sensor networks,"End-to-end encryption schemes that support operations over ciphertext are of utmost importance for commercial private party wireless sensor network implementations to become meaningful and profitable. For wireless sensor networks, we demonstrated in our previous work that privacy homomorphisms, when used for this purpose, offer two striking advantages apart from end-to-end concealment of data and ability to operate on ciphertexts: flexibility by keyless aggregation and conservation and balancing of aggregator backbone energy. We offered proof of concept by applying a certain privacy homomorphism for sensor network applications that rely on the addition operation. But a large class of aggregator functions like median computation or finding maximum/minimum rely exclusively on comparison operations. Unfortunately, as shown by Rivest, et al., any privacy homomorphism is insecure even against ciphertext that only attacks if they support comparison operations. In this paper we show that a particular order preserving encryption scheme achieves the above mentioned energy benefits and flexibility when used to support comparison operations over encrypted texts for wireless sensor networks, while also managing to hide the plaintext distribution and being secure against ciphertext only attacks. The scheme is shown to have reasonable memory and computation overhead when applied for wireless sensor networks.","Cryptography,
Intelligent networks,
Wireless sensor networks,
National electric code,
Europe,
Laboratories,
Data privacy,
Computer science,
Spine,
Computer network management"
Feature Combination and Relevance Feedback for 3D Model Retrieval,"Retrieval of 3D models have attracted much research interest, and many types of shape features have been proposed. In this paper, we describe a novel approach of combining the feature types for 3D model retrieval and relevance feedback processing.Our approach performs query processing using pre-computed pairwise distances between objects measured according to various feature types. Experimental tests show that this approach performs better than retrieval by individual feature type.","Feedback,
Shape,
Query processing,
Image retrieval,
Performance evaluation,
Testing,
Computer science,
Drives,
Feature extraction,
Histograms"
Swarm robotics for a dynamic cleaning problem,"Several recent works considered multi agents robotics in static environments. In this work we examine ways of operating in dynamic environments, in which changes may take place regardless of the agents' activity. The work focuses on a dynamic variant of the known Cooperative Cleaners problem (described and analyzed in [I.A. Wagner et al., (1997)]). This problem assumes a grid, part of which is ""dirty"", when the ""dirty"" part is a connected region of the grid. On this dirty region several agents move, each having the ability to ""clean"" the place it is located in. The dynamic variant of the problem involves a deterministic evolution of the environment, simulating a spreading contamination, or fire. A cleaning protocol for the problem is presented, as well as several analytic bounds for it. In addition, the work contains simulative results for the proposed protocol.","Robots,
Cleaning,
Protocols,
Fires,
Multiagent systems,
Contamination,
Computer science"
Scheduling messages with deadlines in multi-hop real-time sensor networks,"Consider a team of robots equipped with sensors that collaborate with one another to achieve a common goal. Sensors on robots produce periodic updates that must be transmitted to other robots and processed in real-time to enable such collaboration. Since the robots communicate with one another over an ad-hoc wireless network, we consider the problem of providing timeliness guarantees for multihop message transmissions in such a network. We derive the effective deadline and the latest start time for per-hop message transmissions from the validity intervals of the sensor data and the constraints imposed by the consuming task at the destination. Our technique schedules messages by carefully exploiting spatial channel reuse for each per-hop transmission to avoid MAC layer collisions, so that deadline misses are minimized. Extensive simulations show the effectiveness of our channel reuse-based SLF (smallest latest-start-time first) technique when compared to a simple per-hop SLF technique, especially at moderate to high channel utilization or when the probability of collisions is high.","Intelligent networks,
Spread spectrum communication,
Robot sensing systems,
Wireless sensor networks,
Collaboration,
Computer science,
Monitoring,
Fires,
Temperature sensors,
Processor scheduling"
"Structuring labeled trees for optimal succinctness, and beyond","Consider an ordered, static tree /spl Tscr/ on t nodes where each node has a label from alphabet set /spl Sigma/. Tree /spl Tscr/ may be of arbitrary degree and of arbitrary shape. Say, we wish to support basic navigational operations such as find the parent of node u, the ith child of u, and any child of it with label /spl alpha/. In a seminal work over fifteen years ago, Jacobson (1989) observed that pointer-based tree representations are wasteful in space and introduced the notion of succinct data structures. He studied the special case of unlabeled trees and presented a succinct data structure of 2t + o(t) bits supporting navigational operations in O(1) time. The space used is asymptotically optimal with the information-theoretic lower bound averaged over all trees. This led to a slew of results on succinct data structures for arrays, trees, strings and multisets. Still, for the fundamental problem of structuring labeled trees succinctly, few results, if any, exist even though labeled trees arise frequently in practice, e.g. in the data as in markup text (XML) or in augmented data structures. We present a novel approach to the problem of succinct manipulation of labeled trees by designing what we call the xbw transform of the tree, in the spirit of the well-known Burrows-Wheeler transform for strings. The xbw transform uses path-sorting and grouping to linearize the labeled tree /spl Tscr/ into two coordinated arrays, one capturing the structure and the other the labels. Using the properties of the xbw transform, we (i) derive the first-known (near-)optimal results for succinct representation of labeled trees with O(1) time for navigation operations, (ii) optimally support the powerful subpath search operation for the first time, and (iii) introduce a notion of tree entropy and present linear time algorithms for compressing a given labeled tree up to its entropy beyond the information-theoretic lower bound averaged over all tree inputs. Our xbw transform is simple and likely to spur new results in the theory of tree compression and indexing, and may have some practical impact in XML data processing.","Tree data structures,
Navigation,
Shape,
Jacobian matrices,
XML,
Entropy,
Indexing,
Data processing,
Binary trees"
Synthetic Aperture Focusing using a Shear-Warp Factorization of the Viewing Transform,"Synthetic aperture focusing consists of warping and adding together the images in a 4D light field so that objects lying on a specified surface are aligned and thus in focus, while objects lying of this surface are misaligned and hence blurred. This provides the ability to see through partial occluders such as foliage and crowds, making it a potentially powerful tool for surveillance. If the cameras lie on a plane, it has been previously shown that after an initial homography, one can move the focus through a family of planes that are parallel to the camera plane by merely shifting and adding the images. In this paper, we analyze the warps required for tilted focal planes and arbitrary camera configurations. We characterize the warps using a new rank- 1 constraint that lets us focus on any plane, without having to perform a metric calibration of the cameras. We also show that there are camera configurations and families of tilted focal planes for which the warps can be factorized into an initial homography followed by shifts. This shear-warp factorization permits these tilted focal planes to be synthesized as efficiently as frontoparallel planes. Being able to vary the focus by simply shifting and adding images is relatively simple to implement in hardware and facilitates a real-time implementation. We demonstrate this using an array of 30 videoresolution cameras; initial homographies and shifts are performed on per-camera FPGAs, and additions and a final warp are performed on 3 PCs.","Focusing,
Cameras,
Surveillance,
Layout,
Computer vision,
Computer science,
Calibration,
Hardware,
Field programmable gate arrays,
Personal communication networks"
On the time complexity of computer viruses,"Computer viruses can disable computer systems not only by destroying data or modifying a system's configuration, but also by consuming most of the computing resources such as CPU time and storage. The latter effects are related to the computational complexity of computer viruses. In this correspondence, we investigate some issues concerning the time complexity of computer viruses, and prove some known experimental results mathematically. We prove that there exist computer viruses with arbitrarily long running time, not only in the infecting procedure but in the executing procedure. Moreover, we prove that there are computer viruses with arbitrarily large time complexity in the detecting procedure, and there are undecidable computer viruses that have no ""minimal"" detecting procedure.","Computer viruses,
Computational complexity,
Turing machines,
Set theory,
Injuries,
Computer science,
Cryptography"
Improving automatic query classification via semi-supervised learning,"Accurate topical classification of user queries allows for increased effectiveness and efficiency in general-purpose Web search systems. Such classification becomes critical if the system is to return results not just from a general Web collection but from topic-specific back-end databases as well. Maintaining sufficient classification recall is very difficult as Web queries are typically short, yielding few features per query. This feature sparseness coupled with the high query volumes typical for a large-scale search service makes manual and supervised learning approaches alone insufficient. We use an application of computational linguistics to develop an approach for mining the vast amount of unlabeled data in Web query logs to improve automatic topical Web query classification. We show that our approach in combination with manual matching and supervised learning allows us to classify a substantially larger proportion of queries than any single technique. We examine the performance of each approach on a real Web query stream and show that our combined method accurately classifies 46% of queries, outperforming the recall of best single approach by nearly 20%, with a 7% improvement in overall effectiveness.","Semisupervised learning,
Web search,
Supervised learning,
Search engines,
Databases,
Large-scale systems,
Manuals,
Information retrieval,
Laboratories,
Computer science"
Active timing-based correlation of perturbed traffic flows with chaff packets,"Network intruders usually launch their attacks through a chain of intermediate stepping stone hosts in order to hide their identities. Detecting such stepping stone attacks is difficult because packet encryption, timing perturbations, and meaningless chaff packets can all be utilized by attackers to evade from detection. In this paper, we propose a method based on packet matching and timing-based active watermarking that can successfully correlate interactive stepping stone connections even if there are chaff packets and limited timing perturbations. We provide several algorithms that have different trade-offs among detection rate, false positive rate and computation cost. Our experimental evaluation with both real world and synthetic data indicates that by integrating packet matching and active watermarking, our approach has overall better performance than existing schemes.","Timing,
Watermarking,
Cryptography,
Computational efficiency,
Protocols,
Decoding,
Telecommunication traffic,
Computer science,
Software engineering,
Correlation"
Fatigue detection based on the distance of eyelid,"A vision-based real-time driver fatigue detection method is proposed in this paper. Firstly, The face is located using the characteristics of skin colors, then the eyes are detected by projections and finding connected components and is used as the dynamic templates for eye tracking in the following frame. Finally, it can decide whether the driver is fatigue by detecting the distance change of the eyelid. The average correct rate for eye location and tracking can reach 98%, the correct rate for fatigue detection is 100%, the average precision rate is 90%.","Fatigue,
Eyelids,
Face detection,
Eyes,
Accidents,
Pixel,
Skin,
Phase detection,
Humans,
Information science"
On the effectiveness of DDoS attacks on statistical filtering,"Distributed denial of service (DDoS) attacks pose a serious threat to service availability of the victim network by severely degrading its performance. Recently, there has been significant interest in the use of statistical-based filtering to defend against and mitigate the effect of DDoS attacks. Under this approach, packet statistics are monitored to classify normal and abnormal behaviour. Under attack, packets that are classified as abnormal are dropped by the filter that guards the victim network. We study the effectiveness of DDoS attacks on such statistical-based filtering in a general context where the attackers are ""smart"". We first give an optimal policy for the filter when the statistical behaviours of both the attackers and the filter are static. We next consider cases where both the attacker and the filter can dynamically change their behaviour, possibly depending on the perceived behaviour of the other party. We observe that while an adaptive filter can effectively defend against a static attacker, the filter can perform much worse if the attacker is more dynamic than perceived.","Computer crime,
Adaptive filters,
Telecommunication traffic,
Filtering,
Computer science,
Degradation,
Context-aware services,
Humans,
Availability,
Statistical distributions"
Eliciting information from people with a gendered humanoid robot,"A conversational robot can take on different personas that have more or less common ground with users. With more common ground, communication is more efficient. We studied this process experimentally. A ""male"" or ""female"" robot queried users about romantic dating norms. We expected users to assume a female robot knows more about dating norms than a male robot. If so, users should describe dating norms efficiently to a female robot but elaborate on these norms to a male robot. Users, especially women discussing norms for women, used more words explaining dating norms to the male robot than to a female robot. We suggest that through simple changes in a robot's persona, we can elicit different levels of information from users-less if the robot's goal is efficient speech, more, if the robot's goal is redundancy, description, explanation, and elaboration.","Humanoid robots,
Human robot interaction,
Cognitive science,
Psychology,
Speech,
Communication channels,
Displays,
Uncertainty"
Development brings scalability to hardware evolution,"The scalability problem is a major impediment to the use of hardware evolution for real-world circuit design problems. A potential solution is to model the map between genotype and phenotype on biological development. Although development has been shown to improve scalability for a few toy problems, it has not been demonstrated for any circuit design problems. This paper presents such a demonstration for two problems, the n-bit adder with carry and even n-bit parity problems, and shows that development imposes, and benefits from, fewer constraints on evolutionary innovation than other approaches to scalability.","Scalability,
Hardware,
Evolution (biology),
Adders,
Circuit synthesis,
Biological system modeling,
Impedance,
Technological innovation,
Proteins,
Computer science"
A novel approach for fitting probability distributions to real trace data with the EM algorithm,"The representation of general distributions or measured data by phase-type distributions is an important and non-trivial task in analytical modeling. Although a large number of different methods for fitting parameters of phase-type distributions to data traces exist, many approaches lack efficiency and numerical stability. In this paper, a novel approach is presented that fits a restricted class of phase-type distributions, namely mixtures of Erlang distributions, to trace data. For the parameter fitting an algorithm of the expectation maximization type is developed. The paper shows that these choices result in a very efficient and numerically stable approach which yields phase-type approximations for a wide range of data traces that are as good or better than approximations computed with other less efficient and less stable fitting methods. To illustrate the effectiveness of the proposed fitting algorithm, we present comparative results for our approach and two other methods using six benchmark traces and two real traffic traces.","Phase measurement,
Traffic control,
Parameter estimation,
Computer science,
Analytical models,
Numerical stability,
Performance analysis,
Distributed computing,
Phase estimation,
Density functional theory"
Distributed proving in access-control systems,"We present a distributed algorithm for assembling a proof that a request satisfies an access-control policy expressed in a formal logic, in the tradition of Lampson et al. (1992). We show analytically that our distributed proof-generation algorithm succeeds in assembling a proof whenever a centralized prover utilizing remote certificate retrieval would do so. In addition, we show empirically that our algorithm outperforms centralized approaches in various measures of performance and usability notably the number of remote requests and the number of user interruptions. We show that when combined with additional optimizations including caching and automatic tactic generation, which we introduce here, our algorithm retains its advantage, while achieving practical performance. Finally, we briefly describe the utilization of these algorithms as the basis for an access-control framework being deployed for use at our institution.","Logic,
Assembly,
Monitoring,
Distributed algorithms,
Algorithm design and analysis,
Usability,
Access control,
Proposals,
Military computing,
Computer science"
New data-background sequences and their industrial evaluation for word-oriented random-access memories,"This paper improves upon the state of the art in the testing of intraword coupling faults (CFs) in word-oriented memories. It first presents a complete set of fault models for intraword CFs. Then, it establishes the data background sequence and tests for each intraword CF, as well as a test with complete fault coverage of the targeted faults. All introduced tests will be evaluated industrially, together with the most well-known memory tests. The tests will be applied to big arrays with an interleaved bit-organization as well as to small arrays with an adjacent bit-organization in order to investigate the influence of the memory organization on the intraword CFs. The test results show that the intraword CFs are also significantly important for interleaved memories, even when the cells within a single cell are not physically adjacent. This is due to coupling between the adjacent bit lines and word lines running across the memory array. The paper concludes that intraword CFs should be considered for any serious test purpose or leave substantial defects undetected, especially when considering a high-volume production and a very low defect-per-million (DPM) level.","Bills of materials,
Circuit faults,
System testing,
Circuit testing,
Fault detection,
Production,
Flexible manufacturing systems,
Laboratories,
Mathematics,
Computer science"
Optimal two-dimension common centroid layout generation for MOS transistors unit-circuit,"A general algorithm for fitting arbitrary channel width transistors in a two-dimensional common centroid MOS transistor matrix is presented. The proposed algorithm guarantees the layout of the transistor unit-circuit not only to be complete common centroid, but also optimal in all the common centroid structures. A novel channel routing algorithm to implement common centroid routing is also proposed. Feasibility of the algorithm is demonstrated by practical analog transistor unit-circuits.","MOSFETs,
Routing,
Fingers,
Analog circuits,
MOS capacitors,
Computer science,
Degradation,
Transistors,
Circuit optimization,
Linear approximation"
Maximal lifetime scheduling in sensor surveillance networks,"This paper addresses the maximal lifetime scheduling problem in sensor surveillance networks. Given a set of sensors and targets in a Euclidean plane, a sensor can watch only one target at a time, our task is to schedule sensors to watch targets, such that the lifetime of the surveillance system is maximized, where the lifetime is the duration that all targets are watched. We propose an optimal solution to find the target watching schedule for sensors that achieves the maximal lifetime. Our solution consists of three steps: 1) computing the maximal lifetime of the surveillance system and a workload matrix by using linear programming techniques; 2) decomposing the workload matrix into a sequence of schedule matrices that can achieve the maximal lifetime; 3) obtaining a target watching timetable for each sensor based on the schedule matrices. Simulations have been conducted to study the complexity of our proposed method and to compare with the performance of a greedy method.","Intelligent networks,
Surveillance,
Sensor systems,
Watches,
Wireless sensor networks,
Sensor phenomena and characterization,
Processor scheduling,
Matrix decomposition,
Monitoring,
Computer science"
Real-time specification patterns,"Embedded systems are pervasive and frequently used for critical systems with time-dependent functionality. Dwyer et al. (1999) have developed qualitative specification patterns to facilitate the specification of critical properties, such as those that must be satisfied by embedded systems. Thus far, no analogous repository has been compiled for realtime specification patterns. This paper makes two main contributions: First, based on an analysis of timing-based requirements of several industrial embedded system applications, we created real-time specification patterns in terms of three commonly used real-time temporal logics. Second, as a means to further facilitate the understanding of the meaning of a specification, we offer a structured English grammar that includes support for real-time properties. We illustrate the use of the real-time specification patterns in the context of property specifications of a real-world automotive embedded system.","Embedded system,
Logic,
Real time systems,
Automotive engineering,
Embedded software,
Formal specifications,
Permission,
Software engineering,
Laboratories,
Computer science"
EasiMed: A remote health care solution,"An embedded remote health care system based on wireless sensor network technology was established. Firstly, a new system architecture was proposed which introduced a scalable wireless personal medical sensor network around human's body. Then the designs of several sensor node and the care base-station were presented. The wireless communication between the sensor nodes and the care base-station used IEEE 802.15.4/Zigbee standard whilst the care base-station and the remote central server was connected in one of the following ways, including computing network, GSM short messages and telephone modem. The system can be used for remote health care at home or in the hospitals","Medical services,
Wireless sensor networks,
Sensor systems,
Wireless communication,
ZigBee,
Communication standards,
Network servers,
Computer networks,
GSM,
Telephony"
A fast DFT based gene prediction algorithm for identification of protein coding regions,"The paper provides theoretical justification for the ""3-periodicity property"" observed in protein coding regions within genomic DNA sequences. We propose a new classification criteria improving upon traditional frequency based approaches for identification of coding regions. Experimental studies indicate superior performance compared with other algorithms that use the 3-periodicity property.","Prediction algorithms,
Discrete Fourier transforms,
Protein engineering,
Genomics,
Bioinformatics,
Frequency,
DNA computing,
Binary sequences,
Computer science,
Algorithm design and analysis"
Comparison of X-ray and neutron tomography investigations of geological materials,"This paper describes X-ray and neutron computer tomography (CT) experiments made on selected samples from geology. It gives a preliminary assessment of the utility of neutron tomography as a nondestructive means of examining geological materials, particularly as it complements X-ray tomography. Both methods show some advantages compared to the traditional two-dimensional analysis based on thin-sections of geologic samples. It is possible to obtain complete three-dimensional information with a resolution magnitude of tenths of a millimeter on the size and distribution of individual crystals within rock samples several centimeters in dimension. Due to basic differences of neutron and X-ray interaction with material the contrast variation between both imaging modalities highlights different structural information about the object.","Neutrons,
Geology,
Minerals,
Attenuation,
Crystalline materials,
X-ray imaging,
Spatial resolution,
Computed tomography,
X-ray tomography,
X-ray diffraction"
A New Allocation Scheme for Parallel Applications with Deadline and Security Constraints on Clusters,"Parallel applications with deadline and security constraints are emerging in various areas like education, information technology, and business. However, conventional job schedulers for clusters generally do not take security requirements of realtime parallel applications into account when making allocation decisions. In this paper, we address the issue of allocating tasks of parallel applications on clusters subject to timing and security constraints in addition to precedence relationships. A task allocation scheme, or TAPADS (task allocation for parallel applications with deadline and security constraints), is developed to find an optimal allocation that maximizes quality of security and the probability of meeting deadlines for parallel applications. In addition, we proposed mathematical models to describe a system framework, parallel applications with deadline and security constraints, and security overheads. Experimental results show that TAPADS significantly improves the performance of clusters in terms of quality of security and schedulability over three existing allocation schemes","Real time systems,
Application software,
Timing,
Information security,
Dynamic scheduling,
Computer security,
Power system security,
Processor scheduling,
Computer networks,
Concurrent computing"
Artistic collaboration in designing VR visualizations,"This article describes one of the recent major collaborative efforts, a class on designing VR scientific visualizations that was co-taught with professors and students from Brown University's computer science department and the Rhode Island School of Design's (RISD's) illustration department. We discuss here the experiences that led us to this conclusion; along with some of the tools we have developed to facilitate working with artists. Many of the experiences and conclusions relayed here are the results of this class. We then discuss three important themes that we derived from our experiences, which are all motivated by a desire to better facilitate artistic collaborations.","Collaboration,
Virtual reality,
Art,
Data visualization,
Fluid flow,
Capacitive sensors,
Collaborative work,
Computer science,
Feedback,
Petroleum"
Automatic processing of audio lectures for information retrieval: vocabulary selection and language modeling,"This paper describes our initial progress towards developing a system for automatically transcribing and indexing audio-visual academic lectures for audio information retrieval. We investigate the problem of how to combine generic spoken data sources with subject-specific text sources for processing lecture speech. In addition to word recognition experiments, we perform audio information retrieval simulations to characterize retrieval performance when using errorful automatic transcriptions. Given an appropriately selected vocabulary, we observe that good retrieval performance can be obtained even with high recognition error rates. For language model training, we observe that the addition of spontaneous speech data to subject-specific written material results in more accurate transcriptions, but has a marginal effect on retrieval performance.",
A Feasibility Analysis of Power Awareness in Commodity-Based High-Performance Clusters,"We present a feasibility study of a power-reduction scheme that reduces the thermal power of processors by lowering frequency and voltage in the context of high-performance computing. The study revolves around a 16-processor Opteron-based Beowulf cluster, configured as four nodes of quad-processors, and shows that one can easily reduce a significant amount of CPU and system power dissipation and its associated energy costs while still maintaining high performance. Specifically, our study shows that a 5% performance slowdown can be traded off for an average of 19% system energy savings and 24% system power reduction. These preliminary empirical results, via real measurements, are encouraging because hardware failures often occur when the cluster is running hot, i.e, when the workload is heavy, and the new power-reduction scheme can effectively reduce a cluster's power demands during these busy periods","Laboratories,
Costs,
Cooling,
Energy consumption,
Dynamic voltage scaling,
Frequency,
Power demand,
Supercomputers,
Temperature,
Software performance"
Experiences in using immersive virtual characters to educate medical communication skills,"This paper presents a system which allows medical students to experience the interaction between a patient and a medical doctor using natural methods of interaction with a high level of immersion. We also present our experiences with a pilot group of medical and physician assistant students at various levels of training. They interacted with projector-based life-sized virtual characters using gestures and speech. We believe that natural interaction and a high level of immersion facilitates the education of communication skills. We present the system details as well as the participants' performance and opinions. The study confirmed that the level of immersion contributed significantly to the experience, and participants reported that the system is a powerful tool for teaching and training. Applying the system to formal communication skills evaluation and further scenario development will be the focus of future research and refinement.",
Similarity-based Web service matchmaking,"With the increasing growth in popularity of Web services, matchmaking of relevant Web services becomes a significant challenge. Commonly, Web service is described by WSDL and published on UDDI registers. UDDI provides limited search facilities allowing only a keyword-based search of businesses, services, and the so called tModels based on names and identifiers. This category-based keyword-browsing method is clearly insufficient. Semantic Web service uses DAML-S instead of WSDL to represent capabilities of Web services. This improvement enables software agents or search engines to automatically find appropriate Web services via ontologies and reasoning algorithm enriched methods. However, the high cost of formally defining to the heavy and complicated services makes this improvement widespread adoption unlikely. To cope with these limitations, we have developed a suite of methods which assesses the similarity of Web services to achieve matchmaking. In particular, we present a conceptual model which classifies properties of Web services into four categories. For each category, a similarity assessment method has been given. In Web service matchmaking process, these similarity assessment methods can be used together or individually. Experiments highlight complementary contributions that our work makes to facilitate Web service matchmaking.","Web services,
Microstrip,
Simple object access protocol,
Educational institutions,
Computer science,
Software agents,
Ontologies,
Costs,
Application software,
Web and internet services"
Error estimation in multicanonical Monte Carlo Simulations with applications to polarization-mode-dispersion emulators,"This paper shows how to estimate errors in multicanonical Monte Carlo (MMC) simulations using a transition-matrix method. MMC is a biasing Monte Carlo technique that allows one to compute the probability of rare events, such as the outage probability in optical-fiber communication systems. Since MMC is a Monte Carlo technique, it is subject to statistical errors, and it is essential to determine their magnitude. Since MMC is a highly nonlinear iterative method, linearized error-propagation techniques and standard error analyses do not work, and a more sophisticated method is needed. The proposed method is based on bootstrap techniques. This method was applied to efficiently estimate the error in the probability density function (pdf) of the differential group delay (DGD) of polarization-mode-dispersion (PMD) emulators that has been calculated using MMC. The method was validated by comparison to the results obtained using a large ensemble of MMC simulations.","Error analysis,
Polarization,
Monte Carlo methods,
Computational modeling,
Optical fiber communication,
Nonlinear optics,
Iterative methods,
Delay estimation,
Probability distribution,
Computer science"
Real-time fusion of endoscopic views with dynamic 3-D cardiac images: a phantom study,"Minimally invasive robotically assisted cardiac surgical systems currently do not routinely employ 3-D image guidance. However, preoperative magnetic resonance and computed tomography (CT) images have the potential to be used in this role, if appropriately registered with the patient anatomy and animated synchronously with the motion of the actual heart. This paper discusses the fusion of optical images of a beating heart phantom obtained from an optically tracked endoscope, with volumetric images of the phantom created from a dynamic CT dataset. High quality preoperative dynamic CT images are created by first extracting the motion parameters of the heart from the series of temporal frames, and then applying this information to animate a high-quality heart image acquired at end systole. Temporal synchronization of the endoscopic and CT model is achieved by selecting the appropriate CT image from the dynamic set, based on an electrocardiographic trigger signal. The spatial error between the optical and virtual images is 1.4/spl plusmn/1.1 mm, while the time discrepancy is typically 50-100 ms.","Imaging phantoms,
Computed tomography,
Heart,
Minimally invasive surgery,
Animation,
Robots,
Magnetic resonance,
Anatomy,
Endoscopes,
Data mining"
Initial study of quasi-monochromatic X-ray beam performance for X-ray computed mammotomography,"We evaluate the feasibility, benefits, and operating parameters of a quasimonochromatic beam for a newly developed x-ray cone beam computed mammotomography application. The value of a near monochromatic x-ray source for fully 3D dedicated mammotomography is the expected improved ability to separate tissues with very small differences in attenuation coefficients while maintaining dose levels at or below that of existing dual view mammography. In previous studies, simulations for a range of tungsten tube potentials, K-edge filter materials, filter thicknesses, and a 12 cm uncompressed breast, with a digital flat-panel CsI(Tl) detector model, indicated that thick, rare earth filter materials may provide optimized image quality. Figures of merit computed included: lesion contrast under different filtering conditions; ratio of measured lesion contrast with and without filtering; and exposure efficiency (SNR/sup 2//exposure). Initial experiments are performed with a custom built x-ray mammotomography system, cerium foil filters, and plastic breast and lesion tissue-equivalent slabs. Simulation results showed that tube potentials of 50-70 kVp with filters of Z=57-63 yielded quasimonochromatic x-ray spectra with improved FOMs. Initial experimental measurements corroborate simulation results in that, relative trends and rank order of contrast ratios and exposure efficiency were in agreement. These studies show that this approach can be implemented practically with simple hardware and yield improved exposure efficiency versus the unfiltered or minimally filtered case.","Digital filters,
Lesions,
Computational modeling,
Breast,
Filtering,
Computer applications,
Attenuation,
Mammography,
Tungsten,
Biological materials"
Voltage-scaling scheduling for periodic real-time tasks in reward maximization,"This paper is interested in reward maximization of periodic real-time tasks under a given energy constraint, where the reward received depends on how much computation a task runs before its deadline. When voltage scaling could be done at any time, and tasks share the same power consumption function, we propose a greedy algorithm which derives a solution with at least a half of the optimal reward for any input instance. A fully polynomial-time approximation scheme is also proposed by applying the dynamic programming approach so that the ratio of the reward of the derived solution to that of an optimal solution is at least 1 - epsiv under polynomial-time complexity in 1/epsiv for any 0 < epsiv < 1, where epsiv denotes a user-specified tolerable error to the derived solutions. When voltage scaling could be done only when a task instance arrives or terminates, or tasks might have different power consumption functions, we develop an approximation algorithm based on linear programming, which guarantees to derive a solution with at least 1/3 optimal reward for any input instance. A series of experiments was conducted to show the capability of the proposed algorithms in reward maximization","Processor scheduling,
Energy efficiency,
Energy consumption,
Scheduling algorithm,
Dynamic voltage scaling,
Polynomials,
Timing,
Heuristic algorithms,
Intelligent networks,
Computer science"
ExPlanTech: multiagent support for manufacturing decision making,"ExPlanTech's multiagent approach offers a unified framework for decision-making support and provides a proven alternative to known mathematical and system science-modeling technologies for simulating the manufacturing process. ExPlanTech provides technological support for various manufacturing problems and comprises different components, which you can assemble to develop a customized system that supports a user's decision making in different aspects of production planning. The system should help human users size resources and time requirements for a particular order, creating production plans, optimizing material resources manipulation, managing and optimizing supply chain relationships, visualizing and analyzing medium- and long-term manufacturing processes, and accessing external data.","Decision making,
Manufacturing processes,
Optimized production technology,
Assembly systems,
Production planning,
Production systems,
Human resource management,
Supply chain management,
Supply chains,
Data visualization"
Query incentive networks,"The concurrent growth of on-line communities exhibiting large-scale social structure, and of large decentralized peer-to-peer file-sharing systems, has stimulated new interest in understanding networks of interacting agents as economic systems. Here we formulate a model for query incentive networks, motivated by such systems: users seeking information or services can pose queries, together with incentives for answering them, that are propagated along paths in a network. This type of information-seeking process can be formulated as a game among the nodes in the network, and this game has a natural Nash equilibrium. In such systems, it is a fundamental question to understand how much incentive is needed in order for a node to achieve a reasonable probability of obtaining an answer to a query from the network. We study the size of query incentives as a function both of the rarity of the answer and the structure of the underlying network. This leads to natural questions related to strategic behavior in branching processes. Whereas the classically studied criticality of branching processes is centered around the region where the branching parameter is 1, we show in contrast that strategic interaction in incentive propagation exhibits critical behavior when the branching parameter is 2.","Peer to peer computing,
Information systems,
Joining processes,
Computer science,
Social network services,
Large-scale systems,
Nash equilibrium,
Routing,
Internet,
Power system modeling"
An undergraduate system-on-chip (SoC) course for computer engineering students,"The authors have developed a senior-level undergraduate system-on-chip (SoC) course at San Jose State University, San Jose, CA, that emphasizes SoC design methods and hardware-software codesign techniques. The course uses a ""real world"" design project as the teaching vehicle and implements an SoC platform to control a five-axis robotic arm using Altera's state-of-the-art Excalibur chip. The Excalibur chip contains both ARM Corporation's embedded processor and a programmable logic device (PLD) array. The course goes through a complete hardware-software codesign flow from implementing custom hardware devices on a PLD to developing an embedded algorithm in a state-of-the-art design environment for a complete SoC solution. Students learn the Quartus II design environment by examining the sample design files in Altera's EXPA1 development kit and following the step-by-step instructions toward creating a simple embedded application. After this familiarization process, students define the architectural specifications of a memory-mapped servo controller, implement it in the Excalibur's PLD array, and interface this device with the ARM processor's internal bus to control each robotic arm servo. Functional regression tests and post-synthesis timing verification steps are applied to the servo controller following the implementation phase. Subsequently, students integrate the servo controller with the rest of the system and perform board-level functional verification tests to observe whether the robotic arm can move an object from a source to a destination point accurately. Students also develop an embedded algorithm, which translates user inputs in Cartesian coordinates into robotic arm movements in spherical coordinates during laboratory sessions.","Computer science education,
Programmable logic arrays"
Modeling joint constraints for an articulated 3D human body model with artificial correspondences in ICP,"This paper describes a new approach for modeling joints in an articulated 3D body model for tracking of the configuration of a human body. The used model consists of a set of rigid generalized cylinders. The joints between the cylinders are modeled as artificial point correspondences within the ICP (iterative closest point) tracking algorithm, which results in a set of forces and torques maintaining the model constraints. It is shown that different joint types with different degrees of freedom can be modeled with this approach. Experiments show the functionality and robustness of the presented model","Biological system modeling,
Joints,
Tracking,
Iterative closest point algorithm,
Human robot interaction,
Magnetic sensors,
Engine cylinders,
Iterative algorithms,
Computer science,
Robustness"
User Motivation and Persuasion Strategy for Peer-to-Peer Communities,"In recent years, peer-to-peer systems have become more and more popular, especially with some successful applications like Napster and KaZaA. However, how to motivate user participation in peer-to-peer systems remains an open question for researchers. If few users are willing to participate in the community or make contributions to it, the peer-to-peer system will never become successful. To address the problem, this paper proposes a motivation strategy based on persuasion theories of social psychology. The main idea is to introduce a set of hierarchical memberships into p2p communities and reward active users with better quality of services. We have applied this strategy to a p2p system called Comtella and launched a study to test its effectiveness. The results of the study show that our motivation strategy is capable of stimulating the users to participate more actively and make more contributions to the community.","Peer to peer computing,
Psychology,
Computer science,
Application software,
Feedback loop,
Radio access networks,
Quality of service,
System testing,
Power system reliability,
Protocols"
FreeLoader: Scavenging Desktop Storage Resources for Scientific Data,"High-end computing is suffering a data deluge from experiments, simulations, and apparatus that creates overwhelming application dataset sizes. End-user workstations-despite more processing power than ever before-are ill-equipped to cope with such data demands due to insufficient secondary storage space and I/O rates. Meanwhile, a large portion of desktop storage is unused. We present the FreeLoader framework, which aggregates unused desktop storage space and I/O bandwidth into a shared cache/scratch space, for hosting large, immutable datasets and exploiting data access locality. Our experiments show that FreeLoader is an appealing low-cost solution to storing massive datasets, by delivering higher data access rates than traditional storage facilities. In particular, we present novel data striping techniques that allow FreeLoader to efficiently aggregate a workstationâ€™s network communication bandwidth and local I/O bandwidth. In addition, the performance impact on the native workload of donor machines is small and can be effectively controlled.","Workstations,
Cache storage,
Government,
Bandwidth,
Computational modeling,
Aggregates,
Data visualization,
Application software,
Computer science,
Storage area networks"
"Granular computing: examples, intuitions and modeling","The notion of granular computing is examined. Obvious examples, such as fuzzy numbers, infinitesimal number and access control model, (pre-) topological spaces are examined. A general models are proposed; localized multi-level granulation can be modeled by generalized topological spaces, called neighborhood systems. For most general granulation are modeled by Tarski type relational structures.","Fuzzy sets,
Mathematics,
Databases,
Fuzzy control,
Biological system modeling,
Humans,
Books,
Computer science,
Radio frequency,
Access control"
Context aware session management for services in ad hoc networks,"The increasing ubiquity of wireless mobile devices is promoting unprecedented levels of electronic collaboration among devices interoperating to achieve a common goal. Issues related to host interoperability are addressed partially by the service-oriented computing paradigm. However, certain technical concerns relating to reliable interactions among hosts in ad hoc networks have not yet received much attention. We introduce follow-me sessions, where interactions occur between a client and a service, rather than a specific provider or server. We allow the client to switch service providers, if needed. We exploit strategies involving the use of contextual information, strong process migration, context-sensitive binding, and location-agnostic communication protocols. We show how follow-me sessions mitigate issues related to proxy-based service-oriented architectures in ad hoc networks.","Context awareness,
Intelligent networks,
Ad hoc networks,
Mobile computing,
Computer network management,
Switches,
Engineering management,
Computer science,
Drives,
Collaboration"
Anisotropic nonlinear filtering of cellular structures in cryoelectron tomography,Cryoelectron tomography attempts to allow the visualization of complex biological specimens' molecular architecture. This technique yields 3D maps with an extremely low signal-to-noise ratio. A new approach based on anisotropic nonlinear diffusion offers a strategy to reduce the noise and enhance local structures.,"Anisotropic magnetoresistance,
Filtering,
Tomography,
Signal to noise ratio,
Noise reduction,
Tensile stress,
Eigenvalues and eigenfunctions,
Visualization,
Multidimensional systems,
Image analysis"
Task assignment for a physical agent team via a dynamic forward/reverse auction mechanism,"In the dynamic distributed task assignment (DDTA) problem, a team of agents is required to accomplish a set of tasks while maximizing the overall team utility. An effective solution to this problem needs to address two closely related questions: first, how to find a near-optimal assignment from agents to tasks under resource constraints, and second, how to efficiently maintain the optimality of the assignment over time. We address the first problem by extending an existing forward/reverse auction algorithm which was designed for bipartite maximal matching to find an initial near-optimal assignment. A difficulty with such an assignment is that the dynamicity of the environment compromises the optimality of the initial solution. We address the dynamicity problem by using swapping to locally move agents between tasks. By linking these local swaps, the current assignment is morphed into one which is closer to what would have been obtained if we had re-executed the computationally more expensive auction algorithm. In this paper, we detail the application of this dynamic auctioning scheme in the context of a UAV (unmanned aerial vehicle) search and rescue mission and present early experimentations using physical agents to show the feasibility of the proposed approach.",
Particle swarm optimization and fitness sharing to solve multi-objective optimization problems,"The particle swarm optimization algorithm has been shown to be a competitive heuristic to solve multi-objective optimization problems. Also, fitness sharing concepts have shown to be significant when used by multi-objective optimization methods. In this paper we introduce an algorithm that makes use of these two main concepts, particle swarm optimization and fitness sharing to tackle multi-objective optimization problems.","Particle swarm optimization,
Evolutionary computation,
Computer science,
Pareto optimization,
Birds,
Space exploration,
Optimization methods,
Computational efficiency,
Performance evaluation,
Testing"
Building Grid Portal Applications From a Web Service Component Architecture,"This work describes an approach to building Grid applications based on the premise that users who wish to access and run these applications prefer to do so without becoming experts on Grid technology. We describe an application architecture based on wrapping user applications and application workflows as Web services and Web service resources. These services are visible to the users and to resource providers through a family of Grid portal components that can be used to configure, launch, and monitor complex applications in the scientific language of the end user. The applications in this model are instantiated by an application factory service. The layered design of the architecture makes it possible for an expert to configure an application factory service with a custom user interface client that may be dynamically loaded into the portal.","Buildings,
Portals,
Web services,
Component architectures,
Computer science,
Supercomputers,
Production facilities,
Data analysis,
Weather forecasting,
Instruments"
Rapid embedded hardware/software system generation,"This paper presents an RTL generation scheme for a SimpleScalar/PISA instruction set architecture with system calls to implement C programs. The scheme utilizes ASIPmeister, a processor generation tool. The RTL generated is available for download. The second part of the paper shows a method of reducing the PISA instruction set and generating a processor for a given application. This reduction and generation can be performed within an hour, making this one of the fastest methods of generating an application specific processor. For five benchmark applications, we show that on average, processor size can be reduced by 30%, energy consumed reduced by 24%, and performance improved by 24%.","Hardware,
Software systems,
Application specific processors,
Embedded system,
Australia,
Energy consumption,
Computer science,
Communications technology,
Computer architecture,
Humans"
Concurrent non-malleable commitments,"We present a non-malleable commitment scheme that retains its security properties even when concurrently executed a polynomial number of times. That is, a man-in-the-middle adversary who is simultaneously participating in multiple concurrent commitment phases of our scheme, both as a sender and as a receiver cannot make the values he commits to depend on the values he receives commitments to. Our result is achieved without assuming an a-priori bound on the number of executions and without relying on any set-up assumptions. Our construction relies on the existence of standard collision resistant hash functions and only requires a constant number of communication rounds.","Polynomials,
Circuits,
Communication standards,
Cryptographic protocols,
Contracts,
Usability,
Computer science,
Computer security"
Social-mobile applications,"Mobile communications devices and applications are primarily designed to increase efficiency and productivity for professionals on the go. However, users invariably appropriate such technology to meet their social needs as well. For many people, particularly younger users, BlackBerry devices, Hiptops, and other handhelds primarily have a social function. A few small companies are beginning to exploit the growing demand for social-mobile applications, also known as mobile social-software services. One of the most popular MoSoSos applications is dodgeball, a New York-based social-mobile network. Dodgeball users also can broadcast messages, or ""shout,"" to those in their network.","World Wide Web,
GSM,
Mobile handsets,
Privacy,
Cities and towns,
Broadcasting,
Portable computers,
Cellular phones,
Personal digital assistants,
Legged locomotion"
Incentives to promote availability in peer-to-peer anonymity systems,"Peer-to-peer (P2P) anonymous communication systems are vulnerable to free-riders, peers that use the system while providing little or no service to others and whose presence limits the strength of anonymity as well as the efficiency of the system. Free-riding can be addressed by building explicit incentive mechanisms into system protocols to promote two distinct aspects of cooperation among peers-compliance with the protocol specification and the availability of peers to serve others. In this paper we study the use of payments to implement an incentive mechanism that attaches a real monetary cost to low availability. Through a game theoretic analysis, we evaluate the effectiveness of such an incentive, finding that peer availability can be significantly increased through the introduction of payments under many conditions. We also demonstrate how a payment-based incentive that preserves anonymity can be implemented and integrated with a popular class of P2P anonymity systems.","Peer to peer computing,
Protocols,
Availability,
Computer science,
Costs,
Communication systems,
Game theory,
Scalability,
Subscriptions,
Degradation"
Weighted expectation maximization reconstruction algorithms for thermoacoustic tomography,"Thermoacoustic tomography (TAT) is an emerging imaging technique with potential for a wide range of biomedical imaging applications. In this correspondence, we propose an infinite family of weighted expectation maximization (EM) algorithms for reconstruction of images from temporally truncated TAT measurement data. The weighted EM algorithms are equivalent mathematically to the conventional EM algorithm, but are shown to propagate data inconsistencies in different ways. Using simulated and experimental TAT measurement data, we demonstrate that suitable choices of weighted EM algorithms can effectively mitigate image artifacts that are attributable to temporal truncation of the TAT data function.",
Beyond VCG: frugality of truthful mechanisms,"We study truthful mechanisms for auctions in which the auctioneer is trying to hire a team of agents to perform a complex task, and paying them for their work. As common in the field of mechanism design, we assume that the agents are selfish and will act in such a way as to maximize their profit, which in particular may include misrepresenting their true incurred cost. Our first contribution is a new and natural definition of the frugality ratio of a mechanism, measuring the amount by which a mechanism ""overpays "", and extending previous definitions to all monopoly-free set systems. After reexamining several known results in light of this new definition, we proceed to study in detail shortest path auctions and 'r-out-of-k sets"" auctions. We show that when individual set systems (e.g., graphs) are considered instead of worst cases over all instances, these problems exhibit a rich structure, and the performance of mechanisms may be vastly different. In particular, we show that the well-known VCG mechanism may be far from optimal in these settings, and we propose and analyze a mechanism that is always within a constant factor of optimal.","Costs,
Protocols,
Computer science,
Tree graphs,
Resource management,
Electronic commerce,
Game theory,
History,
Mechanical factors,
Performance analysis"
3D model-assisted face recognition in video,"Face recognition in video has gained wide attention as a covert method for surveillance to enhance security in a variety of application domains (e.g., airports). A video contains temporal information as well as multiple instances of a face, so it is expected to lead to better face recognition performance compared to still face images. However, faces appearing in a video have substantial variations in pose and lighting. These pose and lighting variations can be effectively modeled using 3D face models. Combining the advantages of 2D video and 3D face models, we propose a face recognition system that identifies faces in a video. The system utilizes the rich information in a video and overcomes the pose and lighting variations using 3D face model. The description of the proposed method and preliminary results are provided.","Face recognition,
Probes,
Image reconstruction,
Facial features,
Computer science,
Surveillance,
Computer security,
Information security,
Application software,
Airports"
Class-based access control for distributed video-on-demand systems,"The focus of this paper is the analysis of threshold-based admission control policies for distributed video-on-demand (VoD) systems. Traditionally, admission control methods control access to a resource based on the resource capacity. We have extended that concept to include the significance of an arriving request to the VoD system by enforcing additional threshold restrictions in the admission control process on request classes deemed less significant. We present an analytical model for computing blocking performance of the VoD system under threshold-based admission control. Extending the same methodology to a distributed VoD architecture we show through simulation that the threshold performance conforms to the analytical model. We also show that threshold-based analysis can work in conjunction with other request handling policies and are useful for manipulating the VoD performance since we are able to distinguish between different request classes based on their merit. Enforcing threshold restrictions with the option of downgrading blocked requests in a multirate service environment results in improved performance at the same time providing different levels of quality of service (QoS). In fact, we show that the downgrade option combined with threshold restrictions is a powerful tool for manipulating an incoming request mix over which we have no control into a workload that the VoD system can handle.","Access control,
Admission control,
Analytical models,
Computer architecture,
Computational modeling,
Quality of service,
Multimedia systems,
Video on demand,
Computer science,
Performance analysis"
Automated selection of results in hierarchical segmentations of remotely sensed hyperspectral images,,"Image segmentation,
Hyperspectral sensors,
Hyperspectral imaging,
Space technology,
Clustering algorithms,
Constraint optimization,
Spectroscopy,
Earth,
Surface morphology,
Computer science"
On routing asymmetry in the Internet,"Routing asymmetry in the Internet can significantly affect the manner in which we model and simulate its behavior. In this paper, we study routing asymmetry in the Internet and present quantitative evaluations on the extent of such asymmetry today. Our quantitative evaluations provide a measure of the difference between the forward and reverse paths between two end points. Routing asymmetry has not been studied extensively before; this is primarily due to the lack of a systematic approach for quantifying asymmetry except for simply computing the difference between the forward and reverse path lengths. By applying our framework for representing asymmetry, we quantify routing asymmetry for both US higher education academic networks and general commercial networks at two different levels: the autonomous system (AS) level and the router (or link) level. We take into consideration, not only the difference in the forward and reverse path lengths, but also the AS and link identities and the sequence in which these entities appear on the paths. We measure the AS level routing asymmetry, and provide upper lower bounds on link level routing asymmetry. Our studies show that academic networks appear to be more symmetric than general commercially deployed networks. Furthermore, our studies demonstrate that routing asymmetry exhibits a skewed distribution i.e., a few end-points seem to display a higher extent of participation on asymmetric routes.","Routing,
Internet,
Telecommunication traffic,
Delay effects,
Delay estimation,
Helium,
Computer science,
Computational modeling,
Computer simulation,
Displays"
A dynamic routing mechanism for network on chip,"With an increase in the number of transistors on-chip, the complexity of the system also increases. In order to cope with the growing interconnect infrastructure, the ""network on chip (NoC)"" concept was introduced. With network methodologies coming on-chip, various characteristics of traditional networks come into play. So far, failures that are common in regular networks were hardly considered on-chip; this paper introduces ideas of dynamic routing in the context of NoCs and explains how they could be applied to cope with adverse physical effects of deep submicron technology.","Routing,
Network-on-a-chip,
Scalability,
Internet,
Integrated circuit interconnections,
Computer science,
System-on-a-chip,
Silicon,
Transistors,
Counting circuits"
"An equation-free, multiscale approach to uncertainty quantification",The authors' equation- and Galerkin-free computational approach to uncertainty quantification for dynamical systems conducts UQ computations using short bursts of appropriately initialized ensembles of simulations. Their basic procedure estimates the quantities arising in stochastic Galerkin computations.,"Uncertainty,
Nonlinear equations,
Sampling methods,
Stochastic processes,
Computational modeling,
Microscopy,
Partial differential equations,
Performance analysis,
Analytical models,
Context modeling"
The Erlangen Slot Machine: a highly flexible FPGA-based reconfigurable platform,"We present a new concept as well as the implementation of an FPGA-based reconfigurable platform, the Erlangen Slot Machine (ESM). The main advantages of this platform are: first, the possibility for each module to access its peripheries independent from its location through a programmable crossbar, and distributed SRAMs among slices. This allows an unrestricted relocation of modules on the device. Second, the intermodule structure allows an unlimited communication among running modules.","Field programmable gate arrays,
Pins,
Time sharing computer systems,
Runtime,
Signal processing algorithms,
Random access memory,
Computer science,
Logic devices,
Reconfigurable logic,
Video signal processing"
Efficiency-fairness tradeoff in telecommunications networks,"Introducing the concept of /spl alpha/-fairness, which allows for a bounded fairness compromise, so that no source is allocated less than a fraction /spl alpha/ of its fair share, this letter studies tradeoffs between efficiency (utilization, throughput or revenue) and fairness in a general telecommunications network with relation to any fairness criterion. We formulate a linear program that finds the optimal bandwidth allocation by maximizing efficiency subject to /spl alpha/-fairness constraints. This leads to what we call an efficiency-fairness curve, which shows the benefit in efficiency as a function of the extent to which fairness is compromised.","Intelligent networks,
Throughput,
Channel allocation,
Australia Council,
Telecommunication network topology,
Metropolitan area networks,
Local area networks,
Wide area networks,
Electronic mail,
Computer science"
RT-Component Object Model in RT-Middleware&#8212;Distributed Component Middleware for RT (Robot Technology),"This paper proposes RT-component object model in RT-middleware for robot system integration. ""RT"" means ""robot technology"", which is applied not only to industrial field but also to nonindustrial field such as human daily life support systems. RT-middleware is a software infrastructure for RT systems. We have studied modularization of RT elements at software level. For that reason, RT-middleware, which promotes application of RT in various field, have been developed. Robotic system development methodology and our RT-middleware concepts was discussed. RT-component, which is a basic software unit of RT-middleware based system integration, is derived from that discussion. Next, the object model and the interface definition of RT-component architecture was discussed. Finally conclusion and future work are described","Middleware,
Service robots,
Intelligent robots,
Robotics and automation,
Application software,
Paper technology,
Computer architecture,
Humans,
Software libraries,
Robot control"
A digital signal processing teaching methodology using concept-mapping techniques,"The main goal of this study is to develop a scientific method for designing a teaching methodology used in a basic digital signal processing (DSP) course. The proposed method is based on concept-mapping techniques, which applies multivariate statistic analysis to summarize the experience and knowledge of teachers involved in basic DSP teaching. As a result, a set of teaching methodologies is obtained. This result, as well as other information obtained related to the relative importance of the concepts to be covered, has been used to program the course. Moreover, different teaching tools have been developed to implement the proposed teaching methodology. Finally, the reliability of the method has been compared with similar studies to validate the proposed methodology.","Signal processing,
Statistics,
Computer science education,
Electrical engineering education"
A tree-based reliable multicast scheme exploiting the temporal locality of transmission errors,"Tree-based reliable multicast protocols provide scalability by distributing error-recovery tasks among several repair nodes. These repair nodes keep in their buffers all packets that are likely to be requested by any of its receiver nodes. We address the issue of deciding how long these packets should be retained and present a buffer management scheme taking into account the fact that most packet losses happen during short error bursts. Under our scheme, receiver nodes do not normally acknowledge correctly received packets and repair nodes routinely discard packets after a reasonable time interval. Whenever a receiver node detects a transmission error, its send a negative acknowledgement to its repair node and start acknowledging up to k correctly received packets. Whenever a repair node receives a retransmission request, it stops discarding packets that have not been properly acknowledged until it has received k consecutive acknowledgements from each node that had requested a packet retransmission.","Buffer storage,
Computer errors,
Scalability,
Proposals,
Computer science,
Multicast protocols,
Error correction,
Transport protocols,
Resource management,
Waste management"
Simulation and the semantic Web,"One of the missions of the semantic Web is to put more knowledge on the Web in an organized fashion and link it to other information and data sources. Three successively more capable languages are (or will soon be) provided for this: RDF, OWL, and SWRL. This paper makes a case for using all three for the domain of modeling and simulation. Based on experience developing the discrete-event modeling ontology (DeMO) some observations on the issues and challenges involved in creating such ontologies are presented. An approach for decomposing models into behavioral and observable parts, a la hidden Markov models, which can make ontologies smaller and easier to understand, is also discussed",
Shared Features for Scalable Appearance-Based Object Recognition,"We present a framework for learning object representations for fast recognition of a large number of different objects. Rather than learning and storing feature representations separately for each object, we create a finite set of representative features and share these features within and between different object models. In contrast to traditional recognition methods that scale linearly with the number of objects, the shared features can be exploited by bottom-up search algorithms which require a constant number of feature comparisons for any number of objects. We demonstrate the feasibility of this approach on a novel database of 50 everyday objects in cluttered real-world scenes. Using Gabor wavelet-response features extracted only at corner points, our system achieves good recognition results despite substantial occlusion and background clutter.","Object recognition,
Layout,
Feature extraction,
Object detection,
Boosting,
Computer vision,
Spatial databases,
Cognitive science,
Quantization,
Runtime"
Ontology-based software analysis and reengineering tool integration: the OASIS service-sharing methodology,"A common and difficult maintenance activity is the integration of existing software components or tools into a consistent and interoperable whole. One area in which this has proven particularly difficult is in the domain of software analysis and reengineering tools, which have a very poor record of interoperability. This paper outlines our experience in facilitating tool integration using a service-sharing methodology that employs a domain ontology and specially constructed, external tool adapters. A proof of concept implementation among three tools allowed us to explore service-sharing as a viable means for facilitating interoperability among these tools.","Ontologies,
Software tools,
Software maintenance,
Computer architecture,
Computer science,
Adaptive systems,
Standards development,
Couplings,
Visualization"
Ambient networks: bridging heterogeneous network domains,"Providing end-to-end communication in heterogeneous internetworking environments is a challenge. Two fundamental problems are bridging between different internetworking technologies and hiding of network complexity and differences from both applications and application developers. This paper presents abstraction and naming mechanisms that address these challenges in the Ambient Networks project. Connectivity abstractions hide the differences of heterogeneous internetworking technologies and enable applications to operate across them. A common naming framework enables end-to-end communication across otherwise independent internetworks and supports advanced networking capabilities, such as indirection or delegation, through dynamic bindings between named entities","Ambient networks,
Internetworking,
Subscriptions,
Distributed control,
IP networks,
Communication industry,
Data security,
Information security,
Internet,
Computer science"
A relation between the Characteristic Generators of a linear code and its dual,"It was conjectured by Koetter and Vardy that if the k characteristic generators of a linear code C are linearly independent, then the corresponding n-k characteristic generators of the dual code C/sup /spl perp// are also linearly independent. In this correspondence, we prove that the conjecture is true for self-dual codes and cyclic codes.","Character generation,
Linear code,
Block codes,
Information science,
Decoding,
Parity check codes,
Computer science"
A symmetric patch-based correspondence model for occlusion handling,"Occlusion is one of the challenging problems in stereo. In this paper, we solve the problem in a segment-based style. Both images are segmented, and we propose a novel patch-based stereo algorithm that cuts the segments of one image using the segments of the other, and handles occlusion areas in a proper way. A symmetric graph-cuts optimization framework is used to find correspondence and occlusions simultaneously. The experimental results show superior performance of the proposed algorithm, especially on occlusions, untextured areas and discontinuities","Image segmentation,
Layout,
Pervasive computing,
Computer science,
Asia,
Pixel,
Image sampling,
Robustness,
Image analysis,
Information analysis"
Efficient hardware architectures for modular multiplication on FPGAs,"The computational fundament of most public-key cryptosystems is the modular multiplication. Improving the efficiency of the modular multiplication is directly associated with the efficiency of the whole cryptosystem. This paper presents an implementation and comparison of three recently proposed, highly efficient architectures for modular multiplication on FPGAs: interleaved modular multiplication and two variants of the Montgomery modular multiplication. This (first) hardware implementation of these designs shows their relative performance regarding area and speed. One of the main findings is that the interleaved multiplication has the least area time product of all investigated architectures. As a typical cryptographic application, we show that a 1024-bit RSA exponentiation can be performed in less than 6.1ms at a clock rate of 69MHz on a Xilinx Virtex FPGA.","Hardware,
Field programmable gate arrays,
Elliptic curve cryptography,
Signal processing algorithms,
Computer architecture,
Public key,
Digital signatures,
Table lookup,
Computer security,
Computer science"
OVSF code channel assignment with dynamic code set and buffering adjustment for UMTS,"The Universal Mobile Telecommunications System (UMTS) provides users variable data rate services, which adopts wide-band code-division multiple-access (WCDMA) as the radio access technology. In WCDMA, orthogonal variable spreading factor (OVSF) codes are assigned to different users to preserve the orthogonality between users' physical channels. The data rate supported by an OVSF code depends on its spreading factor (SF). An OVSF code with smaller SF can support higher data rate services than that with larger SFs. Randomly assigning the OVSF code with a large SF to a user may preclude a larger number of OVSF codes with small SFs, which may cause lots of high data rate call requests to be blocked. Therefore the OVSF code assignment affects the performance of the UMTS network significantly. In this paper, we propose two OVSF code assignment schemes CADPB1 and CADPB2 for UMTS. Both schemes are simple and with low system overhead. The simulation experiments are conducted to evaluate the performances for our schemes. Our study indicates that our proposed schemes outperform previously proposed schemes in terms of the weighted blocking probability and fairness index. Our schemes improve the call acceptance rate by slightly introducing call waiting time.","3G mobile communication,
Multiaccess communication,
Wideband,
Gallium nitride,
Quality of service,
Performance evaluation,
Radio access networks,
Councils,
Computer science,
Binary trees"
Human perspective on affective robotic behavior: a longitudinal study,"Humans are inherently social creatures, and affect plays no small role in their social nature. We use our emotional expressions to communicate our internal state, our moods assist or hinder our interactions on a daily basis, we constantly form lasting attitudes towards others, and our personalities make us uniquely predisposed to perform certain tasks. In this paper, we present a framework under development that combines these four areas of affect to influence robotic behavior, and describe initial results of a longitudinal human-robot interaction study. The study was designed to inform the development of the framework in order to increase ease and pleasantness of human-robot interaction.","Mood,
Human robot interaction,
Orbital robotics,
Mobile robots,
Laboratories,
Educational institutions,
Animation,
Computational modeling,
Animals,
Anthropomorphism"
Target tracking with distributed robotic macrosensors,"We have developed a novel control mechanism that deploys a large number of inexpensive robots as a distributed remote sensing array, called a distributed robotic macrosensor (DRM). This DRM has the capability to track targets of both a discrete (e.g., a vehicle) and diffuse (e.g., a chemical plume) nature. Attack resistance is an inherent property of the DRM as well. A relatively simple virtual spring mesh abstraction is used to provide fully distributed control that is both flexible and fault-tolerant. We describe the algorithms for spring mesh formation and control, discrete target tracking, and diffuse target tracking. We also present simulation results demonstrating the efficacy and robustness of DRMs","Target tracking,
Robot kinematics,
Robot sensing systems,
Springs,
Remote sensing,
Chemicals,
Distributed control,
Fault tolerance,
Computer science,
Vehicles"
Modeling and taming parallel TCP on the wide area network,"Parallel TCP flows are broadly used in the high performance distributed computing community to enhance network throughput, particularly for large data transfers. Previous research has studied the mechanism by which parallel TCP improves aggregate throughput, but there doesn't exist any practical mechanism to predict its throughput and its impact on the background traffic. In this work, we address how to predict parallel TCP throughput as a function of the number of flows, as well as how to predict the corresponding impact on cross traffic. To the best of our knowledge, we are the first to answer the following question on behalf of a user: what number of parallel flows will give the highest throughput with less than a p% impact on cross traffic? We term this the maximum nondisruptive throughput. We begin by studying the behavior of parallel TCP in simulation to help derive a model for predicting parallel TCP throughput and its impact on cross traffic. Combining this model with some previous findings we derive a simple, yet effective, online advisor. We evaluate our advisor through extensive simulations and wide-area experimentation.","Wide area networks,
Throughput,
Aggregates,
Telecommunication traffic,
Traffic control,
Computational modeling,
Predictive models,
Large Hadron Collider,
Computer science,
Distributed computing"
Photometric stereo under perspective projection,"Photometric stereo is a fundamental approach in computer vision. At its core lies a set of image irradiance equations each taken with a different illumination. The vast majority of studies in this field have assumed orthography as the projection model. This paper re-examines the basic set of equations of photometric stereo, under an assumption of perspective projection. We show that the resulting system is linear (as is the case under the orthographic model; Nevertheless, the unknowns are different in the perspective case). We then suggest a simple reconstruction algorithm based on the perspective formulae, and compare it to its orthographic counterpart on synthetic as well as real images. This algorithm obtained lower error rates than the orthographic one in all of the error measures. These findings strengthen the hypothesis that a more realistic set of assumptions, the perspective one, improves reconstruction significantly.","Photometry,
Stereo vision,
Equations,
Lighting,
Computer vision,
Reconstruction algorithms,
Layout,
Reflectivity,
Light sources,
Computer science"
An audit logic for accountability,"We describe a policy language and implement its associated proof checking system. In our system, agents can distribute data along with usage policies in a decentralized architecture. Our language supports the specification of conditions and obligations, and also the possibility to refine policies. In our framework, the compliance with usage policies is not actively enforced. However, agents are accountable for their actions, and may be audited by an authority requiring justifications.","Logic,
Access control,
Protection,
Computer science,
Computer security,
Data security,
Computer architecture,
Data privacy,
Permission,
Control systems"
On uniqueness Theorems for Tsallis entropy and Tsallis relative entropy,"The uniqueness theorem for Tsallis entropy was presented in H. Suyari, IEEE Trans. Inf. Theory, vol. 50, pp. 1783-1787, Aug. 2004 by introducing the generalized Shannon-Khinchin axiom. In the present correspondence, this result is generalized and simplified as follows: Generalization : The uniqueness theorem for Tsallis relative entropy is shown by means of the generalized Hobson's axiom. Simplification: The uniqueness theorem for Tsallis entropy is shown by means of the generalized Faddeev's axiom","Entropy,
Random variables,
Computer science education,
Computer science,
Cities and towns,
Physics,
Fractals"
Real-time intrusion detection for ad hoc networks,"A mobile ad hoc network is a collection of nodes that are connected through a wireless medium and form rapidly changing topologies. The widely accepted existing routing protocols designed to accommodate the needs of such self-organised networks do not address possible threats aimed at the disruption of the protocol itself. The assumption of a trusted environment is not one that can be realistically expected; hence several efforts have been made towards the design of a secure routing protocol for ad hoc networks. The main problems with this approach are that it requires changes to the underlying routing protocol and that manual configuration of the initial security associations cannot be completely avoided. We propose RIDAN, a novel architecture that uses knowledge-based intrusion detection techniques to detect, in real-time, attacks that an adversary can perform against the routing fabric of a mobile ad hoc network. Our system is designed to take countermeasures minimising the effectiveness of an attack and maintaining the performance of the network within acceptable limits. RIDAN does not introduce any changes to the underlying routing protocol since it operates as an intermediate component between the network traffic and the utilised protocol with minimum processing overhead. We have developed a prototype that was evaluated in AODV-enabled networks using the ns-2 network simulator.","Intrusion detection,
Ad hoc networks,
Routing protocols,
Telecommunication traffic,
Mobile ad hoc networks,
Data security,
Network topology,
Traffic control,
Computer science,
Educational institutions"
On improving genetic programming for symbolic regression,"This paper reports an improvement to genetic programming (GP) search for the symbolic regression domain, based on an analysis of dissimilarity and mating. GP search is generally difficult to characterise for this domain, preventing well motivated algorithmic improvements. We first examine the ability of various solutions to contribute to the search process. Further analysis highlights the numerous solutions produced during search with no change to solution quality. A simple algorithmic enhancement is made that reduces these events and produces a statistically significant improvement in solution quality. We conclude by verifying the generalisability of these results on several other regression instances","Genetic programming,
Diversity methods,
Problem-solving,
Computer science,
Evolutionary computation,
Concrete"
Prime DHCP: a prime numbering address allocation mechanism for MANETs,"Most address allocation schemes rely on broadcasting for address solicitation and/or duplicate address detection. In this paper, we propose a Prime DHCP scheme that can allocate addresses to the hosts of a MANET without broadcasting over the whole MANET. Prime DHCP makes each host a DHCP proxy of the MANET and run a prime numbering address allocation algorithm individually to compute unique addresses for address allocation. The concept of DHCP proxies and the prime numbering address allocation algorithm together eliminate the needs for broadcasting in the MANET. Performance results show that Prime DHCP can significantly reduce the signal overhead and the latency for hosts to acquire addresses.","Mobile ad hoc networks,
Broadcasting,
Delay,
Protocols,
Storms,
Communication networks,
Base stations,
Councils,
Computer science"
Real-time natural hand gestures,"In human-computer interaction (HCI) applications, traditional devices, such as keyboard and mouse, can become cumbersome and unsuitable. Researchers consider the human hand to be one of the most promising natural HCI media; specifically, using hand gestures to input computer commands. Vision-based HCI hand-gesture analysis and recognition studies require large numbers of a variety of gestures as input and a virtual hand as output to display the results. Creating a virtual hand with natural gestures would improve human hand HCI research by providing data for hand analysis systems and generating visual output for hand simulation systems.",
Information Technology for Assisted Living at Home: building a wireless infrastructure for assisted living,"A heterogeneous wireless network to support a home health system is presented. This system integrates a set of smart sensors which are designed to provide health and security to the elder citizen living at home. The system facilitates privacy by performing local computation, it supports heterogeneous devices and it provides a platform and initial architecture for exploring the use of sensors with elderly people in the Information Technology for Assisted Living and Home project. The goal of this project is to provide alerts to care givers in the event of an accident or acute illness, and enable remote monitoring by authorized and authenticated care givers","Information technology,
Biomedical monitoring,
Intelligent sensors,
Buildings,
Wireless sensor networks,
Communication system security,
Information security,
Privacy,
Home computing,
Computer architecture"
A binary XML for scientific applications,"XML provides flexible, extensible data models and type systems for structured data, and has found wide-acceptance in many domains. XML processing can be slow, however, especially for scientific data, thus leading to the conventional wisdom that XML is not appropriate for such data. Instead, data is stored in specialized binary formats, and is transmitted via work-arounds such as attachments and base64 encoding. Though these work-arounds can be useful, they nonetheless relegate scientific data to second-class status within the Web services framework; and they generally require yet another API, data model, and type system. An alternative solution is to use more efficient encodings of XML, often known as ""binary XML"". Using XML uniformly throughout an application simplifies and unifies design and development. In this paper we present a binary XML format and implementation for scientific data called Binary XML for Scientific Applications (BXSA). We show that performance is comparable to that of commonly used scientific data formats such as netCDF. These results challenge the prevailing practice of handling control and data separately in scientific applications, with Web services for control and specialized binary formats for data","XML,
Web services,
Costs,
Computer science,
Data models,
Encoding,
Application software,
Large-scale systems,
Electrical equipment industry,
Wire"
SCOPE: scalable consistency maintenance in structured P2P systems,"While current peer-to-peer (P2P) systems facilitate static file sharing, newly developed applications demand that P2P systems be able to manage dynamically changing files. Maintaining consistency between frequently updated files and their replicas is a fundamental reliability requirement for a P2P system. In this paper, we present SCOPE, a structured P2P system supporting consistency among a large number of replicas. By building a replica-partition-tree (RPT) for each key, SCOPE keeps track of the locations of replicas and then propagates update notifications. Our theoretical analyses and experimental results demonstrate that SCOPE can effectively maintain replica consistency while preventing hot spot and node-failure problems. Its efficiency in maintenance and failure-recovery is particularly attractive to the deployment of large-scale P2P systems.","Computer science,
Educational institutions,
Peer to peer computing,
Maintenance,
Buildings,
Broadcasting,
Privacy,
Application software,
Large-scale systems,
Publishing"
Efficient feasibility analysis for real-time systems with EDF scheduling,"This paper presents new fast exact feasibility tests for uniprocessor real-time systems using preemptive EDF (earliest deadline first) scheduling. Task sets which are accepted by previously described sufficient tests are evaluated in nearly the same time as with the old tests by the new algorithms. Many task sets are not accepted by the earlier tests despite them being feasible. These task sets are evaluated by the new algorithms a lot faster than with known exact feasibility tests. Therefore, it is possible to use them for many applications for which only sufficient test are suitable. Additionally this paper shows that the best previous known sufficient test, the best known feasibility bound and the best known approximation algorithm can be derived from these new tests. As a result, this leads to an integrated schedulability theory for EDF.","Real time systems,
Processor scheduling,
Computer science,
System testing,
Approximation algorithms,
Design automation,
Process design,
Formal verification,
Phase measurement,
Time measurement"
Robust Object Detection at Regions of Interest with an Application in Ball Recognition,"In this paper, we present a new combination of a biologically inspired attention system (VOCUS â€“ Visual Object detection with a CompUtational attention System) with a robust object detection method. As an application, we built a reliable system for ball recognition in the RoboCup context. Firstly, VOCUS finds regions of interest generating a hypothesis for possible locations of the ball. Secondly, a fast classifier verifies the hypothesis by detecting balls at regions of interest. The combination of both approaches makes the system highly robust and eliminates false detections. Furthermore, the system is quickly adaptable to balls in different scenarios: The complex classifier is universally applicable to balls in every context and the attention system improves the performance by learning scenario-specific features quickly from only a few training examples.","Robustness,
Object detection,
Application software,
Phase detection,
Classification tree analysis,
Intelligent systems,
Computational intelligence,
Computer science,
Knowledge based systems,
Biology"
Kernelized Non-Euclidean Relational Fuzzy c-Means Algorithm,Successes with kernel-based classification methods have spawned recent efforts to kernelize clustering algorithms for object data. Here we extend the kernelization to relational data clustering by proposing a kernelized form of the nonEuclidean relational fuzzy c-means algorithm. A numerical test result is included,"Kernel,
Clustering algorithms,
Support vector machines,
Computer science,
Fuzzy logic,
Testing,
Pattern recognition,
Computational modeling,
Linearity,
Performance evaluation"
Almost orthogonal linear codes are locally testable,"A code is said to be locally testable if an algorithm can distinguish between a codeword and a vector being essentially far from the code using a number of queries that is independent of the code's length. The question of characterizing codes that are locally testable is highly complex. In this work we provide a sufficient condition for linear codes to be locally testable. Our condition is based on the weight distribution (spectrum) of the code and of its dual. Codes of (large) length n and minimum distance n/2 - /spl Theta/(/spl radic/n) have size which is at most polynomial in n. We call such codes almost-orthogonal. We use our condition to show that almost-orthogonal codes are locally testable, and, moreover, their dual codes can be spanned by words of constant weights (weight of a codeword refers to the number of its non-zero coordinates). Dual-BCH(n, t) codes are generalizations of the well studied Hadamard codes (t = 1 is Hadamard). Alon et al. (2003) raised the question whether Dual-BCH(n, t) codes are locally testable for constant t. As these codes are known to be almost-orthogonal, we solve this question. We further show that BCH(n, t) code is spanned by its almost shortest words, that is by codewords of weight at most 2t + 2, while the minimum weight is 2t + 1. Our results can be straightforwardly extended to Goppa codes and trace subcodes of algebraic-geometric codes.","Testing,
Linear code,
Error correction,
Error correction codes,
Performance evaluation,
Computer science,
Vectors,
Sufficient conditions,
Galois fields,
Probes"
An overview of through the wall surveillance for homeland security,"The Air Force Research Laboratory Information Directorate (AFRL/IF), under sponsorship of the Department of Justice's (DOJ), National Institute of Justice (NIJ) Office of Science and Technology (OS&T), is currently developing and evaluating advanced through the wall surveillance (TWS) technologies. These technologies are partitioned into two categories: inexpensive, handheld systems for locating an individual(s) behind a wall or door; and portable, personal computer (PC) based standoff systems to enable the determination of events during critical incident situations. The technologies utilized are primarily focused on active radars operating in the UHF, L, S (ultra wideband (UWB)), X, and Ku bands. The data displayed by these systems is indicative of range (1 dimension), or range and azimuth (2 dimensions) to the moving individuals). This paper highlights the technologies employed in five (5) prototype TWS systems delivered to NIJ and AFRL/IF for test and evaluation",
Distributed algorithms for secure multipath routing,"To proactively defend against intruders from readily jeopardizing single-path data sessions, we propose a distributed secure multipath solution to route data across multiple paths so that intruders require much more resources to mount successful attacks. Our work exhibits several crucial properties that differentiate itself from previous approaches. They include (1) distributed routing decisions: routing decisions are made without the centralized information of the entire network topology, (2) bandwidth-constraint adaptation: the worst-case link attack is mitigated for any feasible session throughput subject to the link-bandwidth constraints, and (3) lexicographic protection: severe link attacks are suppressed based on lexicographic optimization. We devise two algorithms for the solution, termed the bound-control algorithm and the lex-control algorithm, and prove their convergence to the respective optimal solutions. Experiments show that the bound-control algorithm is more effective to prevent the worst-case single-link attack when compared to the single-path approach, and that the lex-control algorithm further enhances the bound-control algorithm by countering severe single-link attacks and various models of multi-link attacks. Moreover, the lex-control algorithm offers prominent protection after only a few execution rounds. Thus, system designers can sacrifice minimal routing security for significantly improved algorithm performance when deploying the distributed secure multipath solution.","Distributed algorithms,
Data security,
Routing protocols,
Computer crime,
Computer science,
Network topology,
Throughput,
Bandwidth,
Constraint optimization,
Optimal control"
Multiprocessor energy-efficient scheduling for real-time tasks with different power characteristics,"In the past decades, a number of research results have been reported for energy-efficient scheduling over uniprocessor and multiprocessor environments. Different from many of the past results on the assumption for task power characteristics, we consider real-time scheduling of tasks with different power characteristics. The objective is to minimize the energy consumption of task executions under the given deadline constraint. When tasks have a common deadline and are ready at time 0, we propose an optimal real-time task scheduling algorithm for multiprocessor environments with the allowance of task migration. When no task migration is allowed, a 1.412-approximation algorithm for task scheduling is proposed for different settings of power characteristics. The performance of the approximation algorithm was evaluated by an extensive set of experiments, where excellent results were reported.","Energy efficiency,
Energy consumption,
Processor scheduling,
Scheduling algorithm,
Approximation algorithms,
Voltage,
Timing,
Hardware,
Computer science,
Power engineering and energy"
Static analysis tools as early indicators of pre-release defect density,"During software development it is helpful to obtain early estimates of the defect density of software components. Such estimates identify fault-prone areas of code requiring further testing. We present an empirical approach for the early prediction of pre-release defect density based on the defects found using static analysis tools. The defects identified by two different static analysis tools are used to fit and predict the actual pre-release defect density for Windows Server 2003. We show that there exists a strong positive correlation between the static analysis defect density and the pre-release defect density determined by testing. Further, the predicted pre-release defect density and the actual pre-release defect density are strongly correlated at a high degree of statistical significance. Discriminant analysis shows that the results of static analysis tools can be used to separate high and low quality components with an overall classification rate of 82.91%.","Reliability,
Programming,
Software tools,
Software engineering,
Software testing,
Delay estimation,
Computer science,
State estimation,
Fault diagnosis,
Debugging"
Speedup OS-EM image reconstruction by PC graphics card technologies for quantitative SPECT with varying focal-length fan-beam collimation,"In this paper, we present a new hardware acceleration method to speedup the ordered-subsets expectation-maximization (OS-EM) algorithm for quantitative single photon emission computed tomography (SPECT) image reconstruction with varying focal-length fan-beam (VFF) collimation. By utilizing the geometrical symmetry of VFF point-spread function (PSF), compensation for object-specific attenuation and system-specific PSF are accelerated using currently available PC video/graphics card technologies. A tenfold acceleration of quantitative SPECT reconstruction is achieved.","Image reconstruction,
Acceleration,
Hardware,
Geometry,
Optical collimators,
Single photon emission computed tomography,
Attenuation,
Radiology,
Computer graphics,
Computed tomography"
Real-time packet loss prediction based on end-to-end delay variation,"The effect of packet loss on the quality of real-time audio is significant. Nevertheless, Internet measurement experiments continue to show a considerable variation of packet loss, which makes audio error recovery and concealment challenging. We propose a novel framework to predict packet loss and congestion, based on measurements of end-to-end delay variation and trend, enabling proactive error recovery and congestion avoidance. Our preliminary simulation and experimentation results with various sites on the Internet show the effectiveness and the accuracy of the Loss Predictor technique.",
Practical Local Planning in the Contact Space,"Proximity query is an integral part of any motion planning algorithm and takes up the majority of planning time. Due to performance issues, most existing planners perform queries at fixed sampled configurations, sometimes resulting in missed collisions. Moreover, randomly determining collision-free configurations makes it difficult to obtain samples close to, or on, the surface of C-obstacles in the configuration space. In this paper, we present an efficient and practical local planning method in contact space which uses â€œcontinuous collision detectionâ€ (CCD). We show how, using the precise contact information provided by a CCD algorithm, a randomized planner can be enhanced by efficiently sampling the contact space, as well as by constraining the sampling when the roadmap is expanded. We have included our contact-space planning methods in a freely available state-of-the-art planning library - the Stanford MPK library. We have been able to observe that in complex scenarios involving cluttered and narrow passages, which are typically difficult for randomized planners, the enhanced planner offers up to 70 times performance improvement when our contact-space sampling and constrained sampling methods are enabled.",
A realistic network/application model for scheduling divisible loads on large-scale platforms,"Divisible load applications consist of an amount of data and associated computation that can be divided arbitrarily into any number of independent pieces. This model is a good approximation of many real-world scientific applications, lends itself to a natural master-worker implementation, and has thus received a lot of attention. The issue of divisible load scheduling has been studied extensively. However, only a few authors have explored the simultaneous scheduling of multiple such applications on a distributed computing platform. We focus on this increasingly relevant scenario and make the following contributions. We use a novel and more realistic platform model that captures some of the fundamental network properties of grid platforms. We formulate the steady-state multi-application scheduling problem as a linear program that expresses a notion of fairness between applications. This scheduling problem is NP-complete and we propose several heuristics that we evaluate and compare via extensive simulation experiments. Our main finding is that some of our heuristics can achieve performance close to optimal and we quantify the trade-offs between achieved performance and heuristic complexity.",
A methodological approach relating the classification of gesture to identification of human intent in the context of human-robot interaction,"In order to infer intent from gesture, a broad classification of types of gestures into five main classes is introduced. The classification is intended as a generally applicable basis for incorporating the understanding of gesture into human-robot interaction (HRI). Examples from human-robot interaction show the need to take into account not only the kinematics of gesture, but also the interactional context. Requirements for the operational classification of gesture by a robot interacting with humans are suggested and initial steps in its deployment are discussed.","Human robot interaction,
Intelligent robots,
Cognitive robotics,
Manufacturing automation,
Robotics and automation,
Human computer interaction,
Adaptive systems,
Computer science,
Educational institutions,
Computer aided manufacturing"
Contextual encoding in uniform and adaptive mesh-based lossless compression of MR images,"We propose and evaluate a number of novel improvements to the mesh-based coding scheme for 3-D brain magnetic resonance images. This includes: 1) elimination of the clinically irrelevant background leading to meshing of only the brain part of the image; 2) content-based (adaptive) mesh generation using spatial edges and optical flow between two consecutive slices; 3) a simple solution for the aperture problem at the edges, where an accurate estimation of motion vectors is not possible; and 4) context-based entropy coding of the residues after motion compensation using affine transformations. We address only lossless coding of the images, and compare the performance of uniform and adaptive mesh-based schemes. The bit rates achieved (about 2 bits per voxel) by these schemes are comparable to those of the state-of-the-art three-dimensional (3-D) wavelet-based schemes. The mesh-based schemes have been shown to be effective for the compression of 3-D brain computed tomography data also. Adaptive mesh-based schemes perform marginally better than the uniform mesh-based methods, at the expense of increased complexity.","Image coding,
Magnetic resonance,
Mesh generation,
Adaptive optics,
Image motion analysis,
Apertures,
Motion estimation,
Entropy coding,
Motion compensation,
Optical losses"
Policy segmentation for intelligent firewall testing,"Firewall development and implementation are constantly being improved to accommodate higher security and performance standards. Using reliable yet practical techniques for testing new packet filtering algorithms and firewall design implementations from a functionality point of view becomes necessary to assure the required security. In this paper, an efficient paradigm for automated testing of firewalls with respect to their internal implementation and security policies is proposed. We propose a novel firewall testing technique using policy-based segmentation of the traffic address space, which can intelligently adapt the test traffic generation to target potential erroneous regions in the firewall input space. We also show that our automated approach of test case generation, analyzing firewall logs and creating testing reports not only makes the problem solvable but also offers a significantly higher degree of confidence than random testing.",
Variable-size multipacket segments in buffered crossbar (CICQ) architectures,"Buffered crossbars can directly switch variable size packets, but require large cross point buffers to do so, especially when jumbo frames are to he supported. When this is not feasible, segmentation and reassembly (SAR) must be used. We propose a novel SAR scheme for buffered crossbars that uses variable-size segments while merging multiple packets (or fragments thereof) into each segment. This scheme eliminates padding overhead, reduces header overhead, reduces crosspoint buffer size and is suitable for use with external, modern DRAM buffer memory in the ingress line cards. We evaluate the new scheme using simulation, and show that it outperforms existing segmentation schemes in buffered as well as unbuffered crossbars. We also study how the size of the maximum segment affects system performance.",
Detecting Seizure Onset in the Ambulatory Setting: Demonstrating Feasibility,"Ambulatory EEG recorders are commercially available. The majority of these recorders are only capable of capturing and storing EEG for later review by clinicians. A few models are equipped with real-time seizure event detectors, but these detectors make no guarantees on when during a seizure a detection is made. This renders current ambulatory EEG recorders unsuitable for activating alarms or initiating therapies to acutely impact seizure progression in the ambulatory setting. Integrating seizure onset detectors into existing ambulatory recorders will make these applications feasible. Successful integration requires that these detectors be executable on the resource-limited digital signal processors found within ambulatory recorders. In this paper we describe the integration of a patient-specific seizure onset detector with a commercially available ambulatory EEG recorder, and demonstrate how such integration could enable the detection of seizure onset in the ambulatory setting","Electroencephalography,
Detectors,
Event detection,
Digital signal processors,
Hospitals,
Nervous system,
Pediatrics,
Brain modeling,
Medical treatment,
Epilepsy"
Object class recognition using multiple layer boosting with heterogeneous features,"We combine local texture features (PCA-SIFT), global features (shape context), and spatial features within a single multi-layer AdaBoost model of object class recognition. The first layer selects PCA-SIFT and shape context features and combines the two feature types to form a strong classifier. Although previous approaches have used either feature type to train an AdaBoost model, our approach is the first to combine these complementary sources of information into a single feature pool and to use Adaboost to select those features most important for class recognition. The second layer adds to these local and global descriptions information about the spatial relationships between features. Through comparisons to the training sample, we first find the most prominent local features in Layer I, then capture the spatial relationships between these features in Layer 2. Rather than discarding this spatial information, we therefore use it to improve the strength of our classifier. We compared our method to (R. Fergus et al., 2003, A. Opelt et al., 2004, J. Thureson et al., 2004) and in all cases our approach outperformed these previous methods using a popular benchmark for object class recognition (R. Fergus et al., 2003). ROC equal error rates approached 99%. We also tested our method using a dataset of images that better equates the complexity between object and non-object images, and again found that our approach outperforms previous methods.","Boosting,
Object recognition,
Shape,
Benchmark testing,
Target recognition,
Object detection,
Face detection,
Computer science,
Psychology,
Context modeling"
Assessing Cognitive Load with Physiological Sensors,"Assessing the cognitive load of a subject performing a computer task using task performance data is normally available at the end of the task. For assessing cognitive load, physiological data has the advantage of being available in real-time and the potential of assessing the affective components of cognitive load. Described are two new methods of assessing cognitive load from eye tracking and the pressures a subject applies to a computer mouse when subjects perform a math task that involves moving targets. Physiological measures that significantly discriminated task difficulty included eye movement, skin conductivity and one of the pressure signals from the computer mouse. Also, in some cases, these physiological measures can be more sensitive than task performance measures of cognitive load (i.e., incorrect actions) to detect interaction effects with task difficulty. The suite of physiological sensors is shown to be a viable alternative or supplement to task performance measures.","Mice,
Sensor systems,
Target tracking,
Pressure measurement,
Testing,
Biosensors,
Pulse measurements,
Performance analysis,
Time measurement,
Conductivity measurement"
Stability of feature selection algorithms,"With the proliferation of extremely high-dimensional data, feature selection algorithms have become indispensable components of the learning process. Strangely, despite extensive work on the stability of learning algorithms, the stability of feature selection algorithms has been relatively neglected. This study is an attempt to fill that gap by quantifying the sensitivity of feature selection algorithms to variations in the training set. We assess the stability of feature selection algorithms based on the stability of the feature preferences that they express in the form of weights-scores, ranks, or a selected feature subset. We examine a number of measures to quantify the stability of feature preferences and propose an empirical way to estimate them. We perform a series of experiments with several feature selection algorithms on a set of proteomics datasets. The experiments allow us to explore the merits of each stability measure and create stability profiles of the feature selection algorithms. Finally we show how stability profiles can support the choice of a feature selection algorithm.","Classification algorithms,
Predictive models,
Probability distribution,
Testing,
Sampling methods,
Computer science,
Proteomics,
Error analysis,
Stability analysis,
Data mining"
Copula-like operations on finite settings,"This paper deals with discrete copulas considered as a class of binary aggregation operators on a finite chain. A representation theorem by means of permutation matrices is given. From this characterization, we study the structure of associative discrete copulas and a theorem of decomposition of any discrete copula in terms of associative discrete copulas is obtained. Finally, some aspects concerning their extension to copulas are dealt with.","Matrix decomposition,
Distribution functions,
Fuzzy sets,
Fuzzy logic,
Performance evaluation,
Fuzzy set theory,
Uncertainty,
Concrete,
Mathematics,
Computer science"
A novel overlay multicast protocol in mobile ad hoc networks: design and evaluation,"Despite significant research in mobile ad hoc networks, multicast still remains a research challenge. Recently, overlay multicast protocols for MANET have been proposed to enhance the packet delivery ratio by reducing the number of reconfigurations caused by nongroup members' unexpected migration in tree or mesh structure. However, since data is delivered by using replication at each group member, delivery failure on one group member seriously affects all descendent members' packet delivery ratio. In addition, delivery failure can occur by collision between numbers of unicast packets where group members densely locate. In this paper, we propose a new overlay multicast protocol to enhance packet delivery ratio in two ways. One is to construct a new type of overlay data delivery tree, and the other is to apply a heterogeneous data forwarding scheme depending on the density of group members. While the former aims to minimize influence of delivery failure on one group member, the latter intends to reduce excessive packet collision where group members are densely placed. Our simulation results show distinct scalability improvement of our approach without regard to the number of group members or source nodes.","Multicast protocols,
Intelligent networks,
Mobile ad hoc networks,
Unicast,
Mobile communication,
Distributed decision making,
Ad hoc networks,
Scalability,
Computer science,
Encoding"
Context-aware service selection based on dynamic and static service attributes,"Context-aware applications are able to use context, which refers to information about the surrounding environment, to provide relevant information and/or services to the user. A context-aware application may need to make use of existing services (e.g., a print service). There may be several possible choices of services. The context-aware application should be able to discover and select a service that considers context (e.g., current user location). Existing architectures and protocols for service discovery, however, are not suitable for doing so. Contextual information, by its very nature, is dynamic, reflecting the current state and conditions of the application, its user, or its operating environment. Existing architectures and protocols for service discovery, however, tend to assume the world is static, with attributes describing services offered never changing. If attributes are allowed to change, the approaches do not provide the architectural mechanisms required to update them; dynamic attributes with no means of updating are static for all intents and purposes. To support context-aware service discovery and selection, a better approach is required. This paper discusses one possible approach that is based on existing techniques.","Context-aware services,
Printers,
Protocols,
Hardware,
Computer science,
Monitoring,
Wireless networks,
Frequency,
Computer networks,
Books"
Interest-aware information dissemination in small-world communities,"Information dissemination is a fundamental and frequently occurring problem in large, dynamic, distributed systems. We propose a novel approach to this problem, interest-aware information dissemination, that takes advantage of small-world usage patterns in data-sharing communities. These small-world characteristics suggest that users naturally form groups of common interest. We propose algorithms for identifying these groups dynamically, without a need for explicit classification of topics or declaration of user interests. These algorithms use information about the data consumed by users to identify, via online computation, groups with similar interests. As a proof of concept, we apply this methodology to the problem of locating files in large user communities. Using real-world traces from a scientific community and from a peer-to-peer system, we show that proactive information dissemination within groups of common interest can reduce the search load by up to 70%. In addition, this approach naturally supports the efficient discovery of collections of files, a requirement specific to scientific data analysis tasks. We hypothesize that our algorithms can find numerous other uses in distributed systems, such as reputation management.","Computer science,
Data analysis,
Peer to peer computing,
Resource management,
Monitoring,
Scalability,
Costs,
Delay,
Information geometry,
Bandwidth"
The Jikes Research Virtual Machine project: Building an open-source research community,"This paper describes the evolution of the Jikesâ„¢ Research Virtual Machine project from an IBM internal research project, called JalapeÃ±o, into an open-source project. After summarizing the original goals of the project, we discuss the motivation for releasing it as an open-source project and the activities performed to ensure the success of the project. Throughout, we highlight the unique challenges of developing and maintaining an open-source project designed specifically to support a research community.",
Sentence extraction-based presentation summarization techniques and evaluation metrics,"This paper presents automatic speech summarization techniques and its evaluation metrics, focusing on sentence extraction-based summarization methods for making abstracts from spontaneous presentations. Since humans tend to summarize presentations by extracting important sentences from introduction and conclusion parts, this paper proposes a method using sentence location. Experimental results show that the proposed method significantly improves automatic speech summarization performance for the condition of 10% summarization ratio. Results of correlation analysis between subjective and objective evaluation scores confirm that objective evaluation metrics, including summarization accuracy, sentence F-measure and ROUGE-N, are effective for evaluating summarization techniques.","Speech recognition,
Automatic speech recognition,
Speech analysis,
Compaction,
Humans,
Text recognition,
Data mining,
Speech processing,
Computer science,
Abstracts"
Segmentation and pre-recognition of Arabic handwriting,"We propose a novel algorithm for the segmentation and prerecognition of offline handwritten Arabic text. Our character segmentation method over-segments each word, and then removes extra breakpoints using knowledge of letter shapes. On a test set of 200 images, 92.3% of the segmentation points were detected correctly, with 5.1% instances of over-segmentation. The prerecognition component annotates each detected letter with shape information, to be used for recognition in future work.","Image segmentation,
Shape,
Hidden Markov models,
Image recognition,
Venus,
Computer science,
Testing,
Natural languages,
Handwriting recognition,
Computer vision"
Virtual Computing Infrastructures for Nanoelectronics Simulation,"The operational principles, components, and organization of a Grid-computing infrastructure called In-VIGO (standing for In Virtual Information Grid Organizations)are described. In-VIGO enables computational engineering and science in virtual information Grid organizations. Its distinctive feature is the extensive use of virtualization technologies to provide secure execution environments as needed by tools and users. This paper reviews and motivates the requirements of a cyber infrastructure for computational nanoelectronics. It then explains how such requirements are addressed by the In-VIGO middleware approach, which uses virtualized resources to build computational Grids. The architecture and key design aspects of its first deployed version-In-VIGO 1.0-are presented. It is operational and currently being used to enable the use of computational electronics tools over the Web. Aspects of the design and architecture of the next version of In-VIGO are also presented. It uses Web services standards and components, and lessons learned from In-VIGO 1.0.","Nanoelectronics,
Computational modeling,
Middleware,
Grid computing,
Web services,
Resource management,
Protocols,
Resource virtualization,
Computer architecture,
Distributed computing"
Scenariographer: a tool for reverse engineering class usage scenarios from method invocation sequences,"Typical documentation for object-oriented programs includes descriptions of the parameters and return types of each method in a class, but little or no information on valid method invocation sequences. Knowing the sequence with which methods of a class can be invoked is useful information especially for software engineers (e.g., developers, testers) who are actively involved in the maintenance of large software systems. This paper describes a new approach and a tool for generating class usage scenarios (i.e., how a class is used by other classes) from method invocations, which are collected during the execution of the software. Our approach is algorithmic and employs the notion of canonical sets to categorize method sequences into groups of similar sequences, where each group represents a usage scenario for a given class.","Reverse engineering,
Software tools,
Software maintenance,
Software systems,
Software performance,
Visualization,
Computer science,
Educational institutions,
Documentation,
Software testing"
A case study of model context for simulation composability and reusability,"How much effort will be required to compose or reuse simulations? What factors need to be considered? It is generally known that composability and reusability are daunting challenges for both simulations and more broadly software design as a whole. We have conducted a small case study in order to clarify the role that model context plays in simulation composability and reusability. For a simple problem: compute the position and velocity of a falling body, we found that a reasonable formulation of a solution included a surprising number of implicit constraints. Equally surprising, in a challenge posed to a small group of capable individuals, no one of them was able to identify more than three-quarters of the ultimate set of validation constraints. We document the challenge, interpret its results, and discuss the utility our study will have in future investigations into simulation composition and reuse.","Context modeling,
Computer aided software engineering,
Computational modeling,
Software design,
Humans,
Gain measurement,
Gravity,
Research initiatives,
Computer science,
Best practices"
Exploiting spatial correlation towards an energy efficient clustered aggregation technique (CAG) [wireless sensor network applications],"In wireless sensor networks (WSN), monitoring applications use in-network aggregation to minimize energy overhead by reducing the number of transmissions between the nodes. We note that nearby sensor nodes monitoring an environmental feature (e.g., temperature or brightness) typically register similar values. In this paper, we propose clustered aggregation (CAG), which is a mechanism that reduces the number of transmissions and provides approximate results to aggregate queries by utilizing the spatial correlation of sensor data. The result is guaranteed to be within a user-provided error-tolerance threshold. While a query is disseminated to the network, CAG forms clusters of nodes sensing similar values. Subsequently, only one value per cluster is transmitted up the aggregation tree. We use mathematical models and simulations with synthetic and empirical data to evaluate the efficiency-correctness tradeoff of CAG. Our simulation shows that with highly correlated sensor reading and 10% error threshold, CAG can save the communication overhead by as much as 70.9% over TAG while incurring a modest 1.7% error in result.","Energy efficiency,
Wireless sensor networks,
Query processing,
Routing,
Computerized monitoring,
Temperature sensors,
Aggregates,
Technical Activities Guide -TAG,
Filtering algorithms,
Computer science"
Discriminatively trained Markov model for sequence classification,"In this paper, we propose a discriminative counterpart of the directed Markov Models of order k - 1, or MM(k - 1) for sequence classification. MM(k - 1) models capture dependencies among neighboring elements of a sequence. The parameters of the classifiers are initialized to based on the maximum likelihood estimates for their generative counterparts. We derive gradient based update equations for the parameters of the sequence classifiers in order to maximize the conditional likelihood function. Results of our experiments with data sets drawn from biological sequence classification (specifically protein function and subcellular localization) and text classification applications show that the discriminatively trained sequence classifiers outperform their generative counterparts, confirming the benefits of discriminative training when the primary objective is classification. Our experiments also show that the discriminatively trained MM(k - 1) sequence classifiers are competitive with the computationally much more expensive Support Vector Machines trained using k-gram representations of sequences.","Text categorization,
Proteins,
Artificial intelligence,
Laboratories,
Computational intelligence,
Learning,
Computer science,
Maximum likelihood detection,
Maximum likelihood estimation,
Equations"
The fast Fourier transform for experimentalists. Part I. Concepts,"The discrete Fourier transform (DFT) provides a means for transforming data sampled in the time domain to an expression of this data in the frequency domain. The inverse transform reverses the process, converting frequency data into time-domain data. Such transformations can be applied in a wide variety of fields, from geophysics to astronomy, from the analysis of sound signals to CO/sub 2/ concentrations in the atmosphere. Over the course of three articles, our goal is to provide a convenient summary that the experimental practitioner will find useful. In the first two parts of this article, we'll discuss concepts associated with the fast Fourier transform (FFT), an implementation of the DFT. In the third part, we'll analyze two applications: a bat chirp and atmospheric sea-level pressure differences in the Pacific Ocean.",
Cooperative synchronization and channel estimation in wireless sensor networks,"A critical issue in applications involving networks of wireless sensors is their ability to synchronize, and mitigate the fading propagation channel effects. Especially when distributed â€œslaveâ€ sensors (nodes) reach-back to communicate with the â€œmasterâ€ sensor (gateway), low power cooperative schemes are well motivated. Viewing each node as an antenna element in a multi-input multi-output (MIMO) multi-antenna system, we design pilot patterns to estimate the multiple carrier frequency offsets (CFO), and the multiple channels corresponding to each node-gateway link. Our novel pilot scheme consists of non-zero pilot symbols along with zeros, which separate nodes in a time division multiple access (TDMA) fashion, and lead to low complexity schemes because CFO and channel estimators per node are decoupled. The resulting training algorithm is not only suitable for wireless sensor networks, but also for synchronization and channel estimation of single- and multi-carrier MIMO systems. We investigate the performance of our estimators analytically, and with simulations.",
Grasp analysis using deformable fingers,"The human hand is unrivaled in its ability to grasp and manipulate objects, but we still do not understand all of its complexities. One benefit it has over traditional robot hands is the fact that our fingers conform to a grasped object's shape, giving rise to larger contact areas and the ability to apply larger frictional forces. In this paper, we demonstrate how we have extended our simulation and analysis system with finite element modeling to allow us to evaluate these complex contact types. We also propose a new contact model that better accounts for the deformations and show how grasp quality is affected. This work is part of a larger project to understand the benefits the human hand has in grasping.",
Supporting multi-dimensional range queries in peer-to-peer systems,"Today's peer-to-peer (P2P) systems are unable to cope well with range queries on multi-dimensional data. To extend existing P2P systems and thus support multidimensional range queries, one needs to consider such issues as space partitioning and mapping, efficient query processing, and load balancing. In this paper, the authors describe a scheme called ZNet, which addresses all these issues. Moreover, an extensive performance study which evaluates ZNet against several recent proposals was conducted, and results show that ZNet possesses nearly all desirable properties, while others typically fail in one or another.",
OWL-WS: a workflow ontology for dynamic grid service composition,"Semantic grid is becoming a key enabler for next generation grid and the need of supporting process description and enactment, by means of composition of multiple resources, emerged as one of the fundamental requirements. Within NextGRID project, the idea of adopting business processes (expressed as workflow policies) as architectural components in a next generation grid has been developed with the aim of providing architecture with dynamic behaviour and in teroperability. The need of a semantic workflow representation language then emerged and was developed defining an OWLS extension able to support workflow description. The resulting OWL-WS (OWL for workflow and services) ontology allows us modelling concept like abstract and concrete services and workflows according to a semantic workflow model also enabling specification of higher-order workflows and semantic service grouping. This language is being used for specifying adaptive business processes (policy) that are used as evaluation and binding mechanisms by a workflow enactment engine","Ontologies,
Concrete,
Web services,
Grid computing,
Technological innovation,
OWL,
Engines,
Convergence,
Computer architecture,
Contracts"
Electrochemical determination of reversible redox species at interdigitated array micro/nanoelectrodes using charge injection method,"In this work, the interdigitated array microelectrodes/nanoelectrodes (/spl sim/0.2 mm/sup 2/ surface area) have been fabricated and characterized using the charge injection method for the electrochemical determination of reversible redox species. Using p-aminophenol as the redox species, approximately 4/spl times/10/sup -7/ M and 6/spl times/10/sup -9/ M detection limits on the species concentration are respectively achieved with the microelectrodes and the nanoelectrodes.","Electrodes,
Microelectrodes,
Steady-state,
Current measurement,
Computer science,
Switches,
Capacitors,
Instruments,
Inorganic compounds,
Lithography"
Anomaly-based intrusion detection using mobility profiles of public transportation users,"For the purpose of anomaly-based intrusion detection in mobile networks, the utilization of profiles, based on hardware signatures, calling patterns, service usage, and mobility patterns, have been explored by various research teams and commercial systems, namely the fraud management system by Hewlett-Packard and Compaq. This paper examines the feasibility of using profiles, which are based on the mobility patterns of mobile users, who make use of public transportation, e.g. bus. More specifically, a novel framework, which makes use of an instance based learning technique, for classification purposes, is presented. In addition, an empirical analysis is conducted in order to assess the impact of two key parameters, the sequence length and precision level, on the false alarm and detection rates. Moreover, a strategy for enhancing the characterization of users is also proposed. Based on simulation results, it is feasible to use mobility profiles for anomaly-based intrusion detection in mobile wireless networks.","Intrusion detection,
Wireless networks,
Road transportation,
Hardware,
Machine learning,
Computer science,
Telephony,
Mobile computing,
Pattern recognition,
Filters"
A distributed quadtree index for peer-to-peer settings,"We describe a distributed quadtree index for enabling more powerful access on complex data over P2P networks. It is based on the Chord method. Methods such as Chord have been gaining usage in P2P settings to facilitate exact-match queries. The Chord method maps both the data keys and peer addresses. Our work can be applied to higher dimensions, to various data types, i.e., other than spatial data, and to different types of quadtrees. Finally, we can use other key-based methods than the Chord method as our base P2P routing protocol and index scale well. The index also benefits from the underlying fault-tolerant hashing-based methods by achieving a nice load distribution among many peers. We can seamlessly execute a single query on multiple branches of the index hosted by a dynamic set of peers.","Peer to peer computing,
Computer science,
Software engineering,
Educational institutions,
Application software,
Client-server systems,
Indexing,
Testing,
Query processing"
Formal semantics and verification for feature modeling,"Research on features has received much attention in the domain engineering community. Feature modeling plays an important role in the design and implementation of complex software systems. However, the presentation and analysis of feature models are still largely informal. There is also an increasing need for methods and tools that can support automated feature model analysis. This paper presents a formal engineering approach to the specification and verification of feature models. A formal semantics for the feature modeling language is defined using first-order logic. It provides a precise and rigorous formal interpretation for the graphical notation. In addition, further validation of the semantics using the Z/EVES theorem prover is presented. Finally, we demonstrate that the consistency of a feature model and its configurations can be automatically verified by encoding the semantics into the Alloy Analyzer. A case study of the Key Word in Context (KWIC) index systems feature model is presented to illustrate the verification process.","Computer science,
Sun,
Information technology,
Software systems,
Logic,
Encoding,
Context modeling,
Formal verification,
Application software,
Tree graphs"
A shape matching algorithm for synthesizing humanlike enveloping grasps,"Humanoid robots must be capable of interacting with the world using their hands. A variety of humanlike robot hands have been constructed, but it remains difficult to control these hands in a dexterous way. One challenge is grasp synthesis, where we wish to place the hand and control its shape to successfully grasp a given object. In this paper, we present a datadriven approach to grasp synthesis that treats grasping as a shape matching problem. We begin with a database of grasp examples. Given a model of a new object to be grasped (the query), shape features of the object are compared to shape features of hand poses in these examples in order to identify candidate grasps. For effective retrieval, we develop a novel shape matching algorithm that can accommodate the sparse shape information associated with hand pose and that considers relative placements of contact points and normals, which are important for grasp function. We illustrate our approach with examples using a model of the human hand","Mice,
Grasping,
Shape control,
Spatial databases,
Humanoid robots,
Computer science,
Human robot interaction,
Information retrieval,
Kinematics,
Computational geometry"
Configuring common personal software: a requirements-driven approach,"We investigate the personalization capabilities of common personal software systems. We use a typical e-mail client as an example of such a system, and examine the configuration screens it offers to its users. We discover that each configuration value reflects each of the ways with which the user goals can be satisfied. Thus, we construct a goal model in which alternative ways for satisfying high level goals are matched with alternative system configurations. This way, automatic configuration of the system by reasoning about the overlaying goal model can be achieved. We find that the vast majority of the configuration options that refer to system functionality can be configured using this method, facilitating thereby the personalization tasks for users with no technical background, and ensuring, at the same time, consistency and meaningfulness in the configuration result.","Software systems,
Educational institutions,
Computer science,
Communication system software,
Mobile computing,
Home computing,
Mobile communication,
Electronic mail,
Employment,
Productivity"
Computer-Aided Diagnosis Applied to 3-D US of Solid Breast Nodules by Using Principal Component Analysis and Image Retrieval,"Textural features have been shown to be valuable in tumor diagnosis. This study combines three practical textural features in ultrasound (US) images, i.e. spatial gray-level dependence matrices (SGLDMs), gray-level difference matrix (GLDM) and auto-covariance matrix, to identify breast tumor as benign or malignant. The textural features were extracted from 147 3-D ultrasound cases and each case composes a volume of interest (VOI). Usually, the larger region of interest (ROI) sub-image contains considerable textural information. Thus the feature vector extraction utilizes only the adjacent frames with the largest ROI sub-image. The textural features always perform as a high dimensional vector that is unfavorable to differentiate breast tumors in practice. The principal component analysis (PCA) is used to reduce the dimension of textural feature vector and then the image retrieval technique was utilized to differentiate between benign and malignant tumors. The proposed computer-aided diagnosis (CAD) system differentiates solid breast nodules with a relatively high accuracy in the US imaging and helps inexperienced operators avoid misdiagnosis","Computer aided diagnosis,
Solids,
Principal component analysis,
Image retrieval,
Ultrasonic imaging,
Breast tumors,
Feature extraction,
Data mining,
Breast neoplasms,
Cancer"
A comparative evaluation of transparent scaling techniques for dynamic content servers,"We study several transparent techniques for scaling dynamic content Web sites, and we evaluate their relative impact when used in combination. Full transparency implies strong data consistency as perceived by the user, no modifications to existing dynamic content site tiers and no additional programming effort from the user or site administrator upon deployment. We study strategies for scheduling and load balancing queries on a cluster of replicated database back-ends. We also investigate transparent query caching as a means of enhancing database replication. Our work shows that, on an experimental platform with up to 8 database replicas, the various techniques work in synergy to improve overall scaling for the e-commerce TPC-W benchmark. We rank the techniques necessary for high performance in order of impact as follows. Key among the strategies are scheduling strategies, such as conflict-aware scheduling, that minimize consistency maintenance overheads. The choice of load balancing strategy is less important. Transparent query result caching increases performance significantly at any given cluster size for a mostly-read workload. Its benefits are limited for write-intensive workloads, where content-aware scheduling is the only scaling option.","Databases,
Web server,
Dynamic programming,
Load management,
Dynamic scheduling,
Computer science,
Uniform resource locators,
HTML,
Data engineering,
Internet"
Personal identification utilizing finger surface features,"In this paper we present a novel approach for personal identification, which utilizes finger surface features as a biometric identifier. Using dense range data images of the hand, we calculate the curvature-based surface representation, shape index, for the index, middle, and ring fingers. This representation is used for comparisons to determine subject similarity. Our experiments involve the use of a large data set of range images collected over time. We examine the performance of individual finger surfaces as a biometric identifier as well as the performance when using the three finger surfaces in conjunction. The results of our experiments are presented, which indicate that this approach performs well for a first-of-its-kind biometric technique.","Fingers,
Biometrics,
Shape,
Computer science,
Image sensors,
Application software,
Access control,
Computer security,
Law enforcement,
Gratings"
"A comparative study of multicast protocols: top, bottom, or in the middle?","Multicast solutions have been evolving from ""bottom"" to ""top"", i.e., from IP layer (called IP multicast) to application layer (referred to as application layer multicast). Recently, there are some new proposals (named as overlay multicast) using certain ""infrastructure"" (composed of proxies) in the middle. Although it is well accepted that application layer multicast and overlay multicast are easier to deploy while sacrificing bandwidth efficiency compared with IP multicast, little research has been done to systematically evaluate and compare their performance. In this paper, we conduct a comparative study of different types of multicast routing protocols. We first present a qualitative comparison of three types of protocols, and then we provide a quantitative study of four representative protocols, namely, PIM-SSM, NARADA, NICE, and POM by extensive simulations. Our studies will help to answer some of the most important questions, such as which way to go: top, bottom, or in the middle?.","Multicast protocols,
Computer science,
Application software,
Bandwidth,
Routing protocols,
Proposals,
Pricing,
Guidelines"
Strategy for seeding 3D streamlines,"This paper presents a strategy for seeding streamlines in 3D flow fields. Its main goal is to capture the essential flow patterns and to provide sufficient coverage in the field while reducing clutter. First, critical points of the flow field are extracted to identify regions with important flow patterns that need to be presented. Different seeding templates are then used around the vicinity of the different critical points. Because there is significant variability in the flow pattern even for the same type of critical point, our template can change shape depending on how far the critical point is from transitioning into another type of critical point. To accomplish this, we introduce the /spl alpha/-/spl beta/ map of 3D critical points. Next, we use Poisson seeding to populate the empty regions. Finally, we filter the streamlines based on their geometric and spatial properties. Altogether, this multi-step strategy reduces clutter and yet captures the important 3D flow features.","Streaming media,
Visualization,
Computer science,
Shape,
NASA,
Chromium,
Computer graphics,
Spatial resolution,
Level set,
Pattern matching"
The use and usefulness of the ISO/IEC 9126 quality standard,"This paper reports an evaluation the utility of ISO/IEC 9126. ISO/IEC 9126 is an international standard intended to ensure the quality of all software-intensive products including safety-critical systems where lives are at risk if software components fail. Our evaluation exercise arose from an experiment that required a quality assessment of outputs of the design process. Although ISO/IEC 9126 is intended to support evaluation of intermediate software products, both the experimental subjects (158 final year computer science and engineering student) and experimenters found the standard was ambiguous in meaning, incomplete with respect to quality characteristics and overlapping with respect to measured properties. We conclude that ISO/IEC 9126 is not suitable for measuring design quality of software products. This casts serious doubts as to the validity of the standard as a whole.",
Replica placement in data grid: considering utility and risk,"Grid computing emerges from the need to integrate a collection of distributed computing resources to offer performance unattainable by any single machine. Grid technology facilitates data sharing across many organizations in different geographical locations. Data replication is an excellent technique to move and cache data close to users. Replication reduces access latency and bandwidth consumption. It also facilitates load balancing and improves reliability by creating multiple data copies. However, grid environments introduce significant new challenges such as dynamic resource availability and network performance changes. As users requests vary constantly, the system needs a dynamic replication strategy that adapts to users' dynamic behavior. To address such issues, this paper presents and evaluates the performance of six dynamic replication strategies for two different kinds of access patterns. Our replication strategies are mainly based on utility and risk. Before placing a replica at a site, we calculate an expected utility and risk index for each site by considering current network load and user requests. A replication site is then chosen by optimizing expected utility or risk indexes.",
Achieving weighted fairness between uplink and downlink in IEEE 802.11 DCF-based WLANs,"In this paper, we first propose an analytical model of WLANs (wireless LANs) with an arbitrary backoff distribution and AIFS (arbitration inter-frame space). From the analysis, we show that the achievable bandwidth is determined by the mean of backoff distribution regardless of the shape of the backoff distribution. We compare the effectiveness of four parameters on channel access differentiation, namely, the mean of backoff distribution, CWmin (initial contention window), the number of backoff stages, and AIFS. Numerical results show that the mean of backoff distribution provides weighted fair channel access most accurately. Second, based on the proposed analytic frame work, we develop three schemes for the uplink/downlink bandwidth differentiation in order to achieve weighted fairness between uplink and downlink transmissions. Note that IEEE 802.11 is known to have unfairness between uplink and downlink accesses. Each scheme is characterized according to the corresponding channel access rule. The simulation results show that the proposed schemes achieve high system throughput while accurately differentiating bandwidth allocation","Downlink,
Wireless LAN,
Throughput,
Bandwidth,
Analytical models,
Computer science,
Shape,
Channel allocation,
Media Access Protocol,
Data communication"
Reconstructing phylogenetic networks using maximum parsimony,"Phylogenies-the evolutionary histories of groups of organisms-are one of the most widely used tools throughout the life sciences, as well as objects of research within systematics, evolutionary biology, epidemiology, etc. Almost every tool devised to date to reconstruct phylogenies produces trees; yet it is widely understood and accepted that trees oversimplify the evolutionary histories of many groups of organisms, most prominently bacteria (because of horizontal gene transfer) and plants (because of hybrid speciation). Various methods and criteria have been introduced for phylogenetic tree reconstruction. Parsimony is one of the most widely used and studied criteria, and various accurate and efficient heuristics for reconstructing trees based on parsimony have been devised. Jotun Hein suggested a straightforward extension of the parsimony criterion to phylogenetic networks. In this paper we formalize this concept, and provide the first experimental study of the quality of parsimony as a criterion for constructing and evaluating phylogenetic networks. Our results show that, when extended to phylogenetic networks, the parsimony criterion produces promising results. In a great majority of the cases in our experiments, the parsimony criterion accurately predicts the numbers and placements of non-tree events.","Phylogeny,
History,
Computer networks,
Evolution (biology),
Systematics,
Biology computing,
Computer science,
Microorganisms,
Binary trees,
Biological processes"
A multi-level approach to SCOP fold recognition,"The classification of proteins based on their structure can play an important role in the deduction or discovery of protein function. However, the relatively low number of solved protein structures and the unknown relationship between structure and sequence requires an alternative method of representation for classification to be effective. Furthermore, the large number of potential folds causes problems for many classification strategies, increasing the likelihood that the classifier will reach a local optima while trying to distinguish between all of the possible structural categories. Here we present a hierarchical strategy for structural classification that first partitions proteins based on their SCOP class before attempting to assign a protein fold. Using a well-known dataset derived from the 27 most-populated SCOP folds and several sequence-based descriptor properties as input features, we test a number of classification methods, including Naive Bayes and Boosted C4.5. Our strategy achieves an average fold recognition of 74%, which is significantly higher than the 56-60% previously reported in the literature, indicating the effectiveness of a multi-level approach.","Protein engineering,
Sequences,
Databases,
Bioinformatics,
Computer science,
Laboratories,
Testing,
Biological processes,
Genomics,
Proteomics"
Adaptive Sensing for Instantaneous Gas Release Parameter Estimation,"This paper presents a new approach for estimating in real-time the parameters of the advection-diffusion equation that describes the propagation of an instantaneously released gas. A mobile robot equipped with an appropriate sensing device collects measurements in order to estimate the parameters of this equation. The selection of the set of locations where chemical concentration measurements should be recorded, is performed in real-time with the objective of maximizing the accuracy of the parameter estimates and reducing the time to convergence of this estimation problem. Simulation results are presented that validate the described approach, which has significantly lower computational requirements compared to alternative motion strategies based on exhaustive global search.","Parameter estimation,
Equations,
Chemical hazards,
Terrorism,
Mobile robots,
Explosions,
Robot sensing systems,
Chemical sensors,
Computer science,
Time measurement"
A novel approach to multiagent reinforcement learning: utilizing OLAP mining in the learning process,"Reinforcement learning is considered as a strong method for learning in multiagent systems environments. However, it still has some drawbacks, including modeling other learning agents present in the domain as part of the state of the environment, and some states are much less experienced than others, or some state-action pairs are never visited during the learning phase. Further, before the learning process is completed, an agent cannot exhibit a certain behavior in some states that may be sufficiently experienced. This shows that learning in a partially observable and dynamic multiagent systems environment still constitutes a difficult and major research problem that is worth further investigation. Motivated by this, in this paper, a novel learning approach that integrates online analytical processing (OLAP)-based data mining into the process is proposed. First, a data cube OLAP architecture that facilitates effective storage and processing of the state information reported by agents is described. This way, the action of the other agent, even one not in the visual environment of the agent under consideration, can simply be estimated by extracting online association rules, a well-known data mining technique, from the constructed data cube. Then, a new action selection model that is also based on association rules mining is presented. Finally, states that are not sufficiently experienced by mining multiple-level association rules from the proposed data cube are generalized. Experiments conducted on a well-known pursuit domain show the robustness and effectiveness of the proposed learning approach.",
Efficient hop ID based routing for sparse ad hoc networks,"Routing in mobile ad hoc networks remains as a challenging problem given the limited wireless bandwidth, users' mobility and potentially large scale. Recently, there has been a thrust of research to address these problems, including on-demand routing, geographical routing, virtual coordinates, etc. In this paper, we focus on geographical routing, which was shown to achieve good scalability without flooding, but it usually requires location information and can suffer from the severe dead end problem especially in sparse networks. Specifically, we propose a new hop ID based routing protocol, which does not require any location information, yet achieves comparable performance with the shortest path routing. In addition, we design efficient algorithms for setting up the system and adapt to the node mobility quickly, and can effectively route out of dead ends. The extensive analysis and simulation show that the hop ID based routing achieves efficient routing for mobile ad hoc networks with various density, irregular topologies and obstacles","Ad hoc networks,
Scalability,
Network topology,
Mobile ad hoc networks,
Routing protocols,
Computer science,
Asia,
Bandwidth,
Large-scale systems,
Algorithm design and analysis"
Decentralized Diagnosis of Discrete Event Systems using Unconditional and Conditional Decisions,"The past decade has witnessed the development of a body of theory, with associated applications, for fault diagnosis of dynamic systems that can be modeled in a discrete event systems framework. This paper first discusses the dual problem of diagnosing the absence of faults in centralized and decentralized settings. The paper then develops new definitions of decentralized diagnosis in the context of a general decentralized architecture that allows for the use of ""conditional decisions"" by local diagnosers. The properties of these new definitions of decentralized diagnosability are presented and their relationship with existing work discussed. Corresponding verification algorithms are also described.","Discrete event systems,
Fault diagnosis,
Protocols,
Fault detection,
Inference algorithms,
Event detection,
Sensor systems,
Laboratories,
Distributed control,
Sufficient conditions"
Object class recognition by boosting a part-based model,"We propose a new technique for object class recognition, which learns a generative appearance model in a discriminative manner. The technique is based on the intermediate representation of an image as a set of patches, which are extracted using an interest point detector. The learning problem becomes an instance of supervised learning from sets of unordered features. In order to solve this problem, we designed a classifier based on a simple, part based, generative object model. Only the appearance of each part is modeled. When learning the model parameters, we use a discriminative boosting algorithm which minimizes the loss of the training error directly. The models thus learnt have clear probabilistic semantics, and also maintain good classification performance. The performance of the algorithm has been tested using publicly available benchmark data, and shown to be comparable to other state of the art algorithms for this task; our main advantage in these comparisons is speed (order of magnitudes faster) and scalability.","Boosting,
Detectors,
Computer science,
Supervised learning,
Benchmark testing,
Scalability,
Computer vision,
Image representation,
Object recognition,
Image recognition"
Large barrier trees for studying search,"Barrier trees are a method for representing the landscape structure of high-dimensional discrete spaces such as those that occur in the cost function of combinatorial optimization problems. The leaves of the tree represent local optima and a vertex where subtrees join represents the lowest cost saddle-point between the local optima in the subtrees. This paper introduces an extension to existing Barrier tree methods that make them more useful for studying heuristic optimization algorithms. It is shown that every configuration in the search space can be mapped onto a vertex in the Barrier tree. This provides additional information about the landscape, such as the number of configurations in a local optimum. It also allows the computation of additional statistics such as the correlation between configurations in different parts of the Barrier tree. Furthermore, the mappings allow the dynamic behavior of a heuristic search algorithms to be visualized. This extension is illustrated using an instance of the MAX-3-SAT problem.","Cost function,
Heuristic algorithms,
Visualization,
Optimization methods,
Statistics,
Topology,
Performance gain,
Chemical processes,
Computer science,
Joining processes"
An improved artificial fish-swarm algorithm and its application in feed-forward neural networks,"Artificial fish-swarm algorithm (AFSA) is a novel method to search global optimum, which is typical application of behaviorism in artificial intelligence. In order to improve the algorithm's stability and the ability to search the global optimum, we propose an improved AFSA (IAFSA). When the artificial fish swarm's optimum value is not variant after defined generations, we add leaping behavior and change the artificial fish parameter randomly. By the way, we can increase the probability to obtain the global optimum. A new feed-forward neural networks optimization module based on IAFSA is presented. The comparative result between BP algorithm, AFSA and IAFSA demonstrates that the IAFSA has better global stability and avoids premature convergence effectively.","Intelligent networks,
Feedforward systems,
Artificial neural networks,
Neural networks,
Feedforward neural networks,
Marine animals,
Convergence,
Application software,
Stability,
Computer science"
A direct method for modeling non-rigid motion with thin plate spline,"Thin plate spline (TPS) transformations have been applied to non-rigid shape matching with impressive results. However, existing methods often use a sparse set of point correspondences which are established prior to shape matching. A straightforward approach to finding point correspondences and computing TPS parameters imposes expensive computations, thereby motivating us to develop an efficient solution. In this paper, we present a direct method for recovering non-rigid object motion from its appearance in which the point correspondences are simultaneously established while estimating TPS parameters. The motion parameters are estimated in a stiff-to-flexible approach and the principal appearance deformations are learned that can be utilized for motion analysis and recognition. Numerous experiments demonstrate the efficiency and efficacy of the proposed algorithm in modeling the motion details of non-rigid objects undergoing shape deformation and pose variation.","Spline,
Motion estimation,
Parameter estimation,
Deformable models,
Motion analysis,
Linear systems,
Computer vision,
Active shape model,
Computer science,
Humans"
Non-uniform random membership management in peer-to-peer networks,"Existing random membership management algorithms provide each node with a small, uniformly random subset of global participants. However, many applications would benefit more from non-uniform random member subsets. For instance, non-uniform gossip algorithms can provide distance-based propagation bounds and thus information can reach nearby nodes sooner. In another example, Kleinberg shows that networks with random long-links following distance-based non-uniform distributions exhibit better routing performance than those with uniformly randomized topologies. In this paper, we propose a scalable non-uniform random membership management algorithm, which provides each node with a random membership subset with application-specified probability e.g., with probability inversely proportional to distances. Our algorithm is the first non-uniform random membership management algorithm with proved convergence and bounded convergence time. Moreover, our algorithm does not put specific restrictions on the network topologies and thus has wide applicability.","Intelligent networks,
Peer to peer computing,
Sampling methods,
Network topology,
Computer network management,
Broadcasting,
Load management,
Probability distribution,
Delay,
Computer science"
3D Face Recognition using Mapped Depth Images,"This paper addresses 3D face recognition from facial shape. Firstly, we present an effective method to automatically extract ROI of facial surface, which mainly depends on automatic detection of facial bilateral symmetry plane and localization of nose tip. Then we build a reference plane through the nose tip for calculating the relative depth values. Considering the non-rigid property of facial surface, the ROI is triangulated and parameterized into an isomorphic 2D planar circle, attempting to preserve the intrinsic geometric properties. At the same time the relative depth values are also mapped. Finally we perform eigenface on the mapped relative depth image. The entire scheme is insensitive to pose variance. The experiment using FRGC database v1.0 obtains the rank-1 identification score of 95%, which outperforms the result of the PCA base-line method by 4%, which demonstrates the effectiveness of our algorithm.","Face recognition,
Shape,
Nose,
Face detection,
Humans,
Robustness,
Computer science,
Image databases,
Spatial databases,
Principal component analysis"
Evolving problems to learn about particle swarm and other optimisers,We use evolutionary computation (EC) to automatically find problems which demonstrate the strength and weaknesses of modern search heuristics. In particular we analyse particle swarm optimization (PSO) and differential evolution (DE). Both evolutionary algorithms are contrasted with a robust deterministic gradient based searcher (based on Newton-Raphson). The fitness landscapes made by genetic programming (GP) are used to illustrate difficulties in GAs and PSOs thereby explaining how they work and allowing us to devise better extended particle swarm systems (XPS),"Particle swarm optimization,
Genetic programming,
Evolutionary computation,
Stability,
Optimization methods,
Computer science,
Robustness,
Mathematical analysis,
Genetic mutations,
Testing"
A multiobjective approach to the portfolio optimization problem,"The portfolio optimization problem uses mathematical approaches to model stock exchange investments. Its aim is to find an optimal set of assets to invest on, as well as the optimal investments for each asset. In the present work, the problem is treated as a multi-objective optimization problem. Three well-known optimization techniques greedy search, simulated annealing and ant colony optimization are adapted to this multi-objective context. Pareto fronts for five stock indexes are collected, showing the different behaviors of the algorithms adapted. Finally, the results are discussed.","Portfolios,
Investments,
Ant colony optimization,
Simulated annealing,
Mathematical model,
Stock markets,
Computational modeling,
Computer science,
Artificial intelligence,
Context modeling"
Postgres-R(SI): combining replica control with concurrency control based on snapshot isolation,"Replicating data over a cluster of workstations is a powerful tool to increase performance, and provide fault-tolerance for demanding database applications. The big challenge in such systems is to combine replica control (keeping the copies consistent) with concurrency control. Most of the research so far has focused on providing the traditional correctness criteria serializability. However, more and more database systems, e.g., Oracle and PostgreSQL, use multi-version concurrency control providing the isolation level snapshot isolation. In this paper, we present Postgres-R(SI), an extension of PostgreSQL offering transparent replication. Our replication tool is designed to work smoothly with PostgreSQL's concurrency control providing snapshot isolation for the entire replicated system. We present a detailed description of the replica control algorithm, and how it is combined with PostgreSQL's concurrency control component. Furthermore, we discuss some challenges we encountered when implementing the protocol. Our performance analysis based on the TPC-W benchmark shows that this approach exhibits excellent performance for real-life applications even if they are update intensive.","Concurrency control,
Database systems,
Transaction databases,
Control systems,
Middleware,
Computer science,
Workstations,
Application software,
Protocols,
Performance analysis"
Exploring a national cybersecurity exercise for universities,"In cybersecurity competitions, participants either create new or protect preconfigured information systems and then defend these systems against attack in a real-world setting. Institutions should consider important structural and resource-related issues before establishing such a competition. Critical infrastructures increasingly rely on information systems and on the Internet to provide connectivity between systems. Maintaining and protecting these systems requires an education in information warfare that doesn't merely theorize and describe such concepts. A hands-on, active learning experience lets students apply theoretical concepts in a physical environment. Craig Kaucher and John Saunders found that even for management-oriented graduate courses in information assurance, such an experience enhances the students' understanding of theoretical concepts. Cybersecurity exercises aim to provide this experience in a challenging and competitive environment. Many educational institutions use and implement these exercises as part of their computer science curriculum, and some are organizing competitions with commercial partners as capstone exercises, ad hoc hack-a-thons, and scenario-driven, multiday, defense-only competitions. Participants have exhibited much enthusiasm for these exercises, from the DEFCON capture-the-flag exercise to the US Military Academy's Cyber Defense Exercise (CDX). In February 2004, the US National Science Foundation sponsored the Cyber Security Exercise Workshop aimed at harnessing this enthusiasm and interest. The educators, students, and government and industry representatives attending the workshop discussed the feasibility and desirability of establishing regular cybersecurity exercises for postsecondary-level students. This article summarizes the workshop report.","Computer security,
Educational institutions,
Protection,
Information systems,
Internet,
Computer science,
Organizing,
Military computing,
Government,
Defense industry"
Sensor deployment optimization for detecting maneuvering targets,"Sensor deployment is a fundamental issue in sensor networks. Studies on the deployment problem are concentrated deploying sensors to cover assigned areas or set of points efficiently. In this paper, we discuss the sensor deployment problem in the context of target tracking. Firstly, we propose a sensor deployment optimization strategy based on target involved virtual force algorithm (TIVFA). The TIVFA algorithm can dynamically adjust sensor networks configuration according to terrains, intelligence and those detected maneuvering targets in order to improve coverage and detection probability. Secondly, we present an improved sensor-ranking algorithm as well as a sensor protecting strategy with targets importance sequence in consideration. The TIVFA algorithm, together with the sensor protecting strategy, can produce an efficient and robust sensor network. Experiment results sufficiently demonstrate the effectiveness of the proposed approach.","Intelligent sensors,
Force sensors,
Target tracking,
Protection,
Intelligent networks,
Artificial intelligence,
Computer science,
Sun,
Heuristic algorithms,
Robustness"
"Classification of access network types: Ethernet wireless LAN, ADSL, cable modem or dialup?","Ethernet, wireless LAN, ADSL, cable modem and dialup are common access networks, but have dramatically different characteristics. Fast and accurate classification of access network type can improve protocol or application performance significantly. In this paper, we propose a simple and efficient end-end scheme to classify the type of an access network into three categories: Ethernet, wireless LAN and low-bandwidth connection. Our scheme is based on the intrinsic characteristics of the various access networks and utilizes the median and entropy of packet pair inter-arrival times. Extensive experiments show that our scheme obtains accurate classification results in a very short time (10 to 100 seconds).","Ethernet networks,
Wireless LAN,
Modems,
Bandwidth,
Peer to peer computing,
Computer science,
Access protocols,
Application software,
Joining processes,
Hybrid fiber coaxial cables"
Maze routing with OPC consideration,"As the technology of manufacturing process continues to advance, the process variation becomes more and more serious in nanometer designs. Optical proximity correction (OPC) is employed to correct the process variation of the diffraction effect. To obtain the desired layout as early as possible, routers must have some changes to handle the optical effects to speed up the OPC time and to avoid the routing result that cannot be corrected by the OPC process. In this paper, we propose two practical OPC-aware maze routing problems and present how to enhance an existing maze routing algorithm to get an optimal algorithm for each problem. The experimental results are also given to demonstrate the effectiveness of these two enhanced algorithms.","Routing,
Interference constraints,
Optical diffraction,
Lithography,
Optical sensors,
Manufacturing processes,
Semiconductor device modeling,
Costs,
Computer science,
Process control"
Modeling software reliability growth with genetic programming,"Reliability models are very useful to estimate the probability of the software fail along the time. Several different models have been proposed to estimate the reliability growth, however, none of them has proven to perform well considering different project characteristics. In this work, we explore genetic programming (GP) as an alternative approach to derive these models. GP is a powerful machine learning technique based on the idea of genetic algorithms and has been acknowledged as a very suitable technique for regression problems. The main motivation to choose GP for this task is its capability of learning from historical data, discovering an equation with different variables and operators. In this paper, experiments were conducted to confirm this hypotheses and the results were compared with traditional and neural network models",
Scalable enterprise level workflow and infrastructure management in a grid computing environment,"A grid is described as a distributed network which comprises of heterogeneous and non-dedicated elements. The heterogeneity of a grid is not only defined in terms of computing elements and operating systems but also in terms of implementation of policies, policy decisions and the environment. Currently, most grid solutions are targeted at the high performance community for e-sciences and hence are not adequate for enterprises. In this paper, we present an enterprise level workflow and grid management system called the unified grid management and data architecture (UGanDA) which takes care of most needs of enterprises and can be used in traditional e-sciences high performance community as well. It consists of two components: a grid workflow manager called GridWorM and a grid infrastructure manager called MAGI. The two independently deployable components orchestrate to provide the final goal. Representative results are discussed to prove the effectiveness of the system.",
Rapid synchronization and accurate phase-locking of rhythmic motor primitives,"Rhythmic movement is ubiquitous in human and animal behavior, e.g., as in locomotion, dancing, swimming, chewing, scratching, music playing, etc. A particular feature of rhythmic movement in biology is the rapid synchronization and phase locking with other periodic events in the environment, for instance music or visual stimuli as in ball juggling. In traditional oscillator theories to rhythmic movement generation, synchronization with another signal is relatively slow, and it is not easy to achieve accurate phase locking with a particular feature of the driving stimulus. Using a recently developed framework of dynamic motor primitives, we demonstrate a novel algorithm for very rapid synchronization of a rhythmic movement pattern, which can phase lock any feature of the movement to any particular event in the driving stimulus. As an example application, we demonstrate how an anthropomorphic robot can use imitation learning to acquire a complex drumming pattern and keep it synchronized with an external rhythm generator that changes its frequency over time.","Frequency synchronization,
Nonlinear dynamical systems,
Robot kinematics,
Laboratories,
Humans,
Oscillators,
Music,
Legged locomotion,
Tuning,
Computer science"
IROISE: a new QoS architecture for IEEE 802.16 and IEEE 802.11e interworking,"This article proposes a new architecture, which once implemented, would help in achieving end-to-end quality of service (QoS) requirements of an application which is being served in an interworking system of IEEE 802.16/WiMAX and IEEE 802.11e/WiFi networks. Our approach strives at mapping the QoS requirements of an application originating in IEEE 802.11e network to a serving IEEE 802.16 network and assuring the transfer of data having appropriate QoS back to the application in IEEE 802.11e network. We discuss how an application flow specifies its QoS requirements, either in an IEEE 802.11e or IEEE 802.16 network and the mechanisms that ensure that these requirements are known to the serving network. We identify the necessary parameters, as per advice in the standards, that could stipulate the QoS requirements for an application depending upon traffic type it represents. We propose the mapping of various parameters for different kinds of flows which would ultimately make sure that an application receives the QoS it requested. The resulting architecture would work as a hybrid of two different kinds of networks","Quality of service,
Computer architecture,
WiMAX,
Computer science,
Application software,
Telecommunication traffic,
Bandwidth,
Centralized control"
An integrated neighbor discovery and MAC protocol for ad hoc networks using directional antennas,"Many MAC sub-layer protocols for supporting the use of directional antennas in ad hoc networks have been proposed. However, there remain two open issues that are yet to be resolved completely. First, in order to exploit fully the spatial diversity gains due to the use of directional antennas, it is essential to shift to the exclusive use of directional antennas for the transmission and reception of all the upper layers frames. This facilitates maximal spatial re-use and effaces the phenomenon of asymmetry in gain. Second, in the presence of mobility, the MAC protocol should incorporate mechanisms by which a node can efficiently locate and track its neighbors. We propose a new polling based MAC protocol that addresses both the issues in an integrated way. We perform analysis and extensive simulations to understand the performance of our scheme in terms of its ability to maintain connectivity, the achieved utilization efficiency, and throughput. We find that each node, on average, can achieve a per node utilization of about 80% in static and about 45% in mobile scenarios. Our protocol is seen to outperform both the traditional IEEE 802.11 MAC protocol and previously proposed protocols for use with directional antennas that provide partial solutions to solve the aforementioned problems. Finally, we also study the sensitivity of our protocol to various system parameters.","Media Access Protocol,
Ad hoc networks,
Directional antennas,
Directive antennas,
Transmitting antennas,
Throughput,
Receiving antennas,
Mobile antennas,
Computer science,
Spatial resolution"
Efficient node admission for short-lived mobile ad hoc networks,"Admission control is an essential and fundamental security service in mobile ad hoc networks (MANETs). It is needed to securely cope with dynamic membership and topology and to bootstrap other important security primitives (such as key management) and services (such as secure routing) without the assistance of any centralized trusted authority. An ideal admission protocol must involve minimal interaction among the MANET nodes, since connectivity can be unstable. Also, since MANETs are often composed of weak or resource-limited devices, admission control must be efficient in terms of computation and communication. Most previously proposed admission control protocols are prohibitively expensive and require a lot of interaction among MANET nodes in order to securely reach limited consensus regarding admission and cope with potentially powerful adversaries. While the expense may be justified for long-lived group settings, short-lived MANETs can benefit from much less expensive techniques without sacrificing any security. In this paper, we consider short-lived MANETs and present a secure, efficient and a fully non-interactive admission control protocol for such networks. More specifically, our work is focused on novel applications of non-interactive secret sharing techniques based on bi-variate polynomials, but, unlike other results, the associated costs are very low","Mobile ad hoc networks,
Admission control,
Cryptography,
Protocols,
Information security,
Routing,
Computer science,
Computer security,
Network topology,
Polynomials"
Flexure Design Rules for Carbon Fiber Microrobotic Mechanisms,"Mechanisms utilizing rigid links and relatively small flexural joints are very suitable for fabrication at the meso scale. The Micromechanical Flying Insect (MFI) project at UC Berkeley has developed a simple fabrication method which works very well at the scale where the links are a few millimeters in length and the flexures are few hundred microns in length. Previous analysis has concentrated on the geometry of the composite material links for creating rigid links. Recently, we have found that for useful performance, detailed analysis is required of the flexures also. This paper presents a brief analysis of the issues involved in the design of the flexural components of such mechanisms.","Fabrication,
Micromechanical devices,
Insects,
Kinematics,
Geometry,
Composite materials,
Performance analysis,
Wiring,
Actuators,
Biomimetics"
Computation of three-dimensional rigid-body dynamics with multiple unilateral contacts using time-stepping and Gauss-Seidel methods,"A system of rigid bodies with multiple simultaneous unilateral contacts is considered in this paper. The problem is to predict the velocities of the bodies and the frictional forces acting on the simultaneous multicontacts. This paper presents a numerical method based on an extension of an explicit time-stepping scheme and an application of the differential inclusion process introduced by J. J. Moreau. From a differential kinematic analysis of contacts, we derive a set of transfer equations in the velocity-based time-stepping formulation. In applying the Gauss-Seidel iterative scheme, the transfer equations are combined with the Signorini conditions and Coulomb's friction law. The contact forces are properly resolved in each iteration, without resorting to any linearization of the friction cone. The proposed numerical method is illustrated with examples, and its performance is compared with an acceleration-based scheme using linear complementary techniques. Multibody contact systems are broadly involved in many engineering applications. The motivation of this is to solve for the contact forces and body motion for planning the fixture-inserting operation. However, the results of the paper can be generally used in problems involving multibody contacts, such as robotic manipulation, mobile robots, computer graphics and simulation, etc. The paper presents a numerical method based on an extension of an explicit time-stepping scheme, and an application of the differential inclusion process introduced by J. J. Moreau, and compares the numerical results with an acceleration-based scheme with linear complementary techniques. We first describe the mathematical model of contact kinematics of smooth rigid bodies. Then, we present the Gauss-Seidel iterative method for resolving the multiple simultaneous contacts within the time-stepping framework. Finally, numerical examples are given and compared with the previous results of a different approach, which shows that the simulation results of these two methods agree well, and it is also generally more efficient, as it is an explicit method. This paper focuses on the description of the proposed time-stepping and Gauss-Seidel iterations and their numerical implementation, and several theoretical issues are yet to be resolved, like the convergence and uniqueness of the Gauss-Seidel iteration, and the existence and uniqueness of a positive k in solving frictional forces. However, our limited numerical experience has indicated positive answers to these questions. We have always found a single positive root of k and a convergent solution in the Gauss-Seidel iteration for all of our examples.","Gaussian processes,
Kinematics,
Friction,
Acceleration,
Application software,
Mobile robots,
Differential equations,
Motion planning,
Computer graphics,
Computational modeling"
Towards a formal framework for choreography,"One of the main challenges in the area of service oriented computing, in general, and of Web services technology, in particular, is the definition of languages and models for the description of choreographies. A choreography defines the collaborations between interacting services: more precisely, it specifies a contract containing a ""global"" definition of the common ordering conditions and constraints under which messages are exchanged in a services conversation. In this paper, starting from the analysis of the main aspects of Web services technology, we propose a simple choreography language, equipped with a formal semantics, which is intended as the starting point for the development of a framework for the design and analysis of choreographies in service oriented computing.","Web services,
Collaboration,
Distributed computing,
Protocols,
Proposals,
Computer science,
Contracts,
Application software,
Software design,
Search engines"
Detection performance theory for ultrasound imaging systems,"A rigorous statistical theory for characterizing the performance of medical ultrasound systems for lesion detection tasks is developed. A design strategy for optimizing ultrasound systems should be to adjust parameters for maximum information content, which is obtained by maximizing the ideal observer performance. Then, given the radio-frequency data, image and signal processing algorithms are designed to extract as much diagnostically relevant information as possible. In this paper, closed-form and low-contrast approximations of ideal observer performance are derived for signal known statistically detection tasks. The accuracy of the approximations are tested by comparing with Monte Carlo techniques. A metric borrowed and modified from photon imaging, Generalized Noise Equivalent Quanta, is shown to be a useful and measurable target-independent figure of merit when adapted for ultrasound systems. This theory provides the potential to optimize design tradeoffs for detection tasks. For example it may help us understand how to push the limits of specific features, such as spatial resolution, without significantly compromising overall detection performance.","Ultrasonic imaging,
Design optimization,
Biomedical imaging,
Medical diagnostic imaging,
Lesions,
Radio frequency,
Signal processing algorithms,
Process design,
Signal design,
Algorithm design and analysis"
Load-balancing strategy of multi-gateway for ad hoc Internet connectivity,"Mobile ad hoc networks (MANET) are autonomous, infrastructureless networks that support multi-hop communication through IP routing. MANET and Internet have many differences. These differences are not only the structure and topology of the networks, but also communication patterns of nodes in both networks. Therefore, it is a challenging problem for MANET to access to the Internet due to these differences. There are many kinds of accessing mode such as single fix gateway or multi-gateway, where load-balancing is the most important problem. In this paper, we propose the dynamic gateway concept that acts as an interface between MANET and the Internet. These dynamic gateways can use mobile IP and DSDV protocol when they communicate with the Internet and interact with MANET, respectively. The dynamic gateway is used to solve the load-balancing problem. Several simulation experimental results in the dynamic gateway environment show that the performance of network has been improved.","Internet,
Mobile ad hoc networks,
Peer to peer computing,
Switches,
Mobile computing,
Mobile communication,
Computer science,
Network topology,
Access protocols,
Computer networks"
Voltage Regulation and Overcurrent Protection Issues in Distribution Feeders with Distributed Generation - A Case Study,"The installation of distributed generation (DG) on distribution feeders is known to have an impact on voltage regulation. A DG can provide voltage support in some cases, but can also cause an overvoltage or an undervoltage, depending on the several variables including relative DG size and location, distribution line and load characteristics, and method of voltage regulation. This paper shed some light the impact of DGs on voltage regulation and on overcurrent protection. These impacts are investigated on a feasibility study concerning the installation of a 5 MW grid-tied PV system to a local distribution system.","Voltage control,
Protection,
Distributed control,
Computer aided software engineering,
Costs,
Distributed power generation,
Substations,
Power generation,
Synchronous generators,
Fuel cells"
Two-dimensional cluster-correcting codes,"We consider two-dimensional error-correcting codes capable of correcting a single arbitrary cluster of errors of size b. We provide optimal 2-cluster-correcting codes in several connectivity models, as well as optimal, or nearly optimal, 2-cluster-correcting codes in all dimensions. We also construct 3-cluster-correcting codes and b-straight-cluster-correcting codes. We conclude by improving the Reiger bound for two-dimensional cluster-correcting codes.",
Minimum estimation and combining generalized selection combining (MEC-GSC),"This paper introduces a new adaptive minimum-estimation minimum-combining switched-combining based scheme that reduces considerably the average receiver channel estimation complexity and the power drain from the battery while satisfying the desired performance requirement. More specifically, analysis supported by numerical results show that the newly proposed combining scheme can achieve essentially the same performance as competing schemes such as generalized selection combining (GSC) and minimum selection GSC while offering 50% channel estimation and 75% processing power savings for some system parameters of interest",
Conflict detection and resolution in two-dimensional prefix router tables,"We show that determining the minimum number of resolve filters that need to be added to a set of two-dimensional (2-D) prefix filters so that the filter set can implement a given policy using the first-matching-rule-in-table tie breaker is NP-hard. Additionally, we develop a fast O(nlogn+s) time, where n is the number of filters and s is the number of conflicts, plane-sweep algorithm to detect and report all pairs of conflicting 2-D prefix filters. The space complexity of our algorithm is O(n). On our test set of 15 2-D filter sets, our algorithm runs between 4 and 17 times as fast as the 2-D trie algorithm of A. Hari et al. (2000) and uses between 1/4th and 1/8th the memory used by the algorithm of Hari et al. On the same test set, our algorithm is between 4 and 27 times as fast as the bit-vector algorithm of Baboescu and Varghese (2002) and uses between 1/205 and 1/6 as much memory. We introduce the notion of an essential resolve filter and develop an efficient algorithm to determine the essential resolve filters of a prefix filter set.","Information filtering,
Information filters,
Testing,
Internet,
Two dimensional displays,
Bandwidth,
Protocols,
Matched filters,
Computer science,
Information science"
The Design of an Integrated Control System in Heavy Vehicles Based on an LPV Method,"In this paper an integrated control structure with individual active control mechanisms, i.e. active anti-roll bars, active suspensions, and an active brake mechanism, is proposed. Its purpose is to improve rollover prevention, passenger comfort, road holding and guarantee the suspension working space. In the control design the performance specifications both for rollover and suspension problems, and the model uncertainties are taken into consideration. In the weighting strategy of control design fault information is also taken into consideration. The design of the control system is based on an HâˆžLinear Parameter Varying (LPV) method. To enhance the performance of the controlled system, the control mechanism is extended with a prediction procedure, in which an observer-based prediction algorithm is proposed.","Centralized control,
Suspensions,
Bars,
Control systems,
Fault detection,
Roads,
Control design,
Filters,
Vehicle dynamics,
Uncertainty"
Transitive signatures: new schemes and proofs,"We present novel realizations of the transitive signature primitive introduced by Micali and Rivest, enlarging the set of assumptions on which this primitive can be based, and also providing performance improvements over existing schemes. More specifically, we propose new schemes based on factoring, the hardness of the one-more discrete logarithm problem, and gap Diffie-Hellman (DH) groups. All these schemes are proven transitively unforgeable under adaptive chosen-message attack in the standard (not random-oracle) model. We also provide an answer to an open question raised by Micali and Rivest regarding the security of their Rivest-Shamir-Adleman (RSA)-based scheme, showing that it is transitively unforgeable under adaptive chosen-message attack assuming the security of RSA under one-more inversion. We then present hash-based modifications of the RSA, factoring, and gap Diffie-Hellman based schemes that eliminate the need for ""node certificates"" and thereby yield shorter signatures. These modifications remain provably secure under the same assumptions as the starting scheme, in the random oracle model.","Security,
Public key,
Computer science,
DH-HEMTs,
Digital signatures,
Government,
Contracts,
Public key cryptography,
Forgery,
Privacy"
The common instrument middleware architecture: overview of goals and implementation,"Instruments and sensors and their accompanying actuators are essential to the conduct of scientific research. In many cases they provide observations in electronic format and can be connected to computer networks with varying degrees of remote interactivity. These devices vary in their architectures and type of data they capture and may generate data at various rates. In this paper we present an overview of the design goals and initial implementation of the common instrument middleware architecture (CIMA), a framework for making instruments and sensors network accessible in a standards-based, uniform way, and for interacting remotely with instruments and the data they produce. Some of the issues CIMA addresses include: flexibility in network transport, efficient and high throughput data transport, the availability (or lack of) computational, storage and networking resources at the instrument or sensor platform, evolution of instrument design, and reuse of data acquisition and processing codes","Instruments,
Middleware,
Grid computing,
Computer architecture,
Computer networks,
Computer science,
Web services,
Humans,
Informatics,
Laboratories"
A Spatiotemporal Query Service for Mobile Users in Sensor Networks,"This paper presents MobiQuery, a spatiotemporal query service that allows mobile users to periodically gather information from their surrounding areas through a wireless sensor network. A key advantage of MobiQuery lies in its capability to meet stringent spatiotemporal performance constraints crucial to many applications. These constraints include query latency, data freshness and fidelity, and changing query areas due to user mobility. A novel just-in-time prefetching algorithm enables MobiQuery to maintain robust spatiotemporal guarantees even when nodes operate under extremely low duty cycles. Furthermore, it significantly reduces the storage cost and network contention caused by continuous queries from mobile users. We validate our approach through both theoretical analysis and simulation results under a range of realistic settings","Spatiotemporal phenomena,
Intelligent networks,
Wireless sensor networks,
Fires,
Robot sensing systems,
Delay,
Computer science,
Mobile computing,
Prefetching,
Robustness"
Novel approaches to the measurement of arterial blood flow from dynamic digital X-ray images,"We have developed two new algorithms for the measurement of blood flow from dynamic X-ray angiographic images. Both algorithms aim to improve on existing techniques. First, a model-based (MB) algorithm is used to constrain the concentration-distance curve matching approach. Second, a weighted optical flow algorithm (OP) is used to improve on point-based optical flow methods by averaging velocity estimates along a vessel with weighting based on the magnitude of the spatial derivative. The OP algorithm was validated using a computer simulation of pulsatile blood flow. Both the OP and the MB algorithms were validated using a physiological blood flow circuit. Dynamic biplane digital X-ray images were acquired following injection of iodine contrast medium into a variety of simulated arterial vessels. The image data were analyzed using our integrated angiographic analysis software SARA to give blood flow waveforms using the MB and OP algorithms. These waveforms were compared to flow measured using an electromagnetic flow meter (EMF). In total 4935 instantaneous measurements of flow were made and compared to the EMF recordings. It was found that the new algorithms showed low measurement bias and narrow limits of agreement and also out-performed the concentration-distance curve matching algorithm (ORG) and a modification of this algorithm (PA) in all studies.","Fluid flow measurement,
Blood flow,
X-ray imaging,
Electromagnetic measurements,
Image motion analysis,
Image analysis,
Algorithm design and analysis,
Computer simulation,
Circuits,
Computational modeling"
New cyclic relative difference sets constructed from d-homogeneous functions with difference-balanced property,"For a prime power q, we show that a cyclic relative difference set with parameters (q/sup n/-1/q-1,q-1,q/sup n-1/,q/sup n-2/) can be constructed from a d-homogeneous function from F/sub q//sup n//spl bsol/{0} onto F/sub q/ with difference-balanced property, where F/sub q//sup n/ is the finite field with q/sup n/ elements. This construction method enables us to construct several new cyclic relative difference sets with parameters (p/sup n/-1/p/sup l/-1,p/sup l/-1,p/sup n-l/,p/sup n-2l/) from p-ary sequences of period p/sup n/-1 with ideal autocorrelation property introduced by Helleseth and Gong. Using a lifting idea, other new cyclic relative difference sets can be constructed from the Helleseth-Gong (HG) sequences. Also, the 3-ranks and the trace representation of the characteristic sequences of cyclic relative difference sets from a specific class of ternary HG sequences and ternary Lin sequences are derived.","Binary sequences,
Stability,
Computer science,
Galois fields,
Application software,
Cryptography,
Codes,
Autocorrelation"
Empirical Evaluation of Advanced Ear Biometrics,"We present results of the largest experimental investigation of ear biometrics to date. Approaches considered include a PCA (""eigen-ear"") approach with 2D intensity images, achieving 63.8% rank-one recognition; a PCA approach with range images, achieving 55.3% Hausdorff matching of edge images from range images, achieving 67.5% and ICP matching of the 3D data, achieving 98.7%. ICP based matching not only achieves the best performance, but also shows good scalability with size of dataset. The data set used represents over 300 persons, each with images acquired on at least two different dates. In addition, the ICP-based approach is further applied on an expanded data set of 404 subjects, and achieves 97.5% rank one recognition rate. In order to test the robustness and variability of ear biometrics, ear symmetry is also investigated. In our experiments around 90% of peopleâ€™s right ear and left ear are symmetric.","Ear,
Biometrics,
Principal component analysis,
Image recognition,
Shape,
Iterative closest point algorithm,
Testing,
Bayesian methods,
Computer science,
Scalability"
Segmentation of edge preserving gradient vector flow: an approach toward automatically initializing and splitting of snakes,"Active contours or snakes have been extensively utilized in handling image segmentation and classification problems. In traditional active contour models, snake initialization is performed manually by users, and topological changes, such as splitting of the snake, can not be automatically handled. In this paper, we introduce a new method to solve the snake initialization and splitting problem, based on an area segmentation approach: the external force field is segmented first, and then the snake initialization and splitting can be automatically performed by using the segmented external force field. Such initialization and splitting produces multiple snakes, each of which is within the capture range associated to an object and evolved to the object boundary. The external force used in this paper is a gradient vector flow with an edge-preserving property (EPGVF), which can prevent the snakes from passing over weak boundaries. To segment the external force field, we represent it with a graph, and a graph-theory approach can be taken to determine the membership of each pixel. Experimental results establish the effectiveness of the proposed approach.","Active contours,
Solid modeling,
Topology,
Image segmentation,
Computer science,
Computed tomography,
Spline,
Level set,
Active shape model,
Active noise reduction"
Disk drive roadmap from the thermal perspective: a case for dynamic thermal management,"The importance of pushing the performance envelope of disk drives continues to grow, not just in the server market but also in numerous consumer electronics products. One of the most fundamental factors impacting disk drive design is the heat dissipation and its effect on drive reliability, since high temperatures can cause off-track errors, or even head crashes. Until now, drive manufacturers have continued to meet the 40% annual growth target of the internal data rates (IDR) by increasing RPMs, and shrinking platter sizes, both of which have counter-acting effects on the heat dissipation within a drive. As this paper shows, we are getting to a point where it is becoming very difficult to stay on this roadmap. This paper presents an integrated disk drive model that captures the close relationships between capacity, performance and thermal characteristics over time. Using this model, we quantify the drop off in IDR growth rates over the next decade if we are to adhere to the thermal envelope of drive design. We present two mechanisms for buying back some of this IDR loss with dynamic thermal management (DTM). The first DTM technique exploits any available thermal slack, between what the drive was intended to support and the currently lower operating temperature, to ramp up the RPM. The second DTM technique assumes that the drive is only designed for average case behavior, thus allowing higher RPMs than the thermal envelope, and employs dynamic throttling of disk drive activities to remain within this envelope.","Thermal management,
Disk drives,
Computer aided software engineering,
Temperature,
Computer crashes,
Storage area networks,
Thermal engineering,
Thermal management of electronics,
Engineering management,
Computer science"
The Segmentation Problem in Arabic Character Recognition The State Of The Art,"Arabic characters are used in several languages other than Arabic, despite to this fact; Arabic Character Recognition (ACR) has not received enough interests by researchers. Little researchprogress has been achieved comparing to the one done on the Latin or Chinese and the solutions available in the market are still far from being perfect. However, recent years have shown a considerable increase in the number of research papers. The cursive nature of Arabic writing makes the process of recognition a very challenging one. Several methods to segment the Arabic words into characters have been proposed in the past two decades. This paper seeks to provide a comprehensive review of the methods proposed by researchers to segment. There is a room for research in this area; hence, the speech aims at promoting the research among Muslim researchers to work on ACR by addressing the challenges posed by the nature of the characters. The segmentation methods are categorized in nine different methods based on the techniques used. The advantages and drawback of each one are discussed.","Character recognition,
Writing,
Application software,
Natural languages,
Text recognition,
Computer science,
Speech,
Pattern recognition,
Computational modeling,
Humans"
CLICKS: Mining Subspace Clusters in Categorical Data via K-Partite Maximal Cliques,"We present a novel algorithm called CLICKS, that finds clusters in categorical datasets based on a search for k-partite maximal cliques. Unlike previous methods, CLICKS mines subspace clusters. It uses a selective vertical method to guarantee complete search. CLICKS outperforms previous approaches by over an order of magnitude and scales better than any of the existing method for high-dimensional datasets. We demonstrate this improvement in an excerpt from our comprehensive performance studies.","Clustering algorithms,
Engineering profession,
Computer science,
US Department of Energy"
One-dimensional simulation of an ion beam generated by a current-free double-Layer,A current-free electric double-layer is created in a one-dimensional hybrid (particle ions and fluid electrons) plasma computer code by inserting a loss process along the axis of the simulation to mimic an expanding plasma. The image presented here shows a supersonic ion beam which has been accelerated downstream of the double-layer.,
CQMP: a mesh-based multicast routing protocol with consolidated query packets,"We propose a mesh-based multicast routing protocol for wireless ad hoc networks. The protocol retains all of the advantages of the on-demand multicast routing protocol (ODMRP) such as high packet delivery ratio under high mobility, high throughput. Moreover, the protocol significantly reduces control overhead, one of the main weaknesses of ODMRP, under the presence of multiple sources. This feature is a crucial contributing factor to the scalability of multicast routing for mobile ad hoc networks. The results are experimentally verified. It is shown that in the presence of high number of sources, our protocol reduces the control packet load by up to 30 percent, increases the multicast efficiency by 10 to 20 percent and improves the data delivery ratio of ODMRP.","Multicast protocols,
Routing protocols,
Ad hoc networks,
Mobile ad hoc networks,
Throughput,
Scalability,
Mobile communication,
Computer science,
Communication system control,
Communication networks"
A Privacy Preserving Reputation System for Mobile Information Dissemination Networks,"In a mobile information dissemination network mobile users, equipped with wireless devices, exchange information in a spontaneous manner whenever they come into communication range. Users have to specify what kind of information they are looking for and what kind of information they can offer. A priori there is no relation between users, literally spoken, they donâ€™t know each other and confidence in newly collected information might be low. This work presents two reputation schemes, a simple and an extended version, for mobile information dissemination networks that, based on user ratings, increase a userâ€™s confidence in some information source. As reputation systems collect sensitive personal information and monitor usersâ€™ behavior, privacy is an essential requirement â€” especially in a mobile scenario â€” that is neglected by many existing approaches. Using cryptographic group signatures and the concept of an observer, our extended reputation scheme guarantees high user privacy.","Privacy,
Mobile computing,
Mobile communication,
Ad hoc networks,
Humans,
Peer to peer computing,
Computer science,
Electronic mail,
Monitoring,
Cryptography"
Hardness of the undirected edge-disjoint paths problem with congestion,"In the edge-disjoint paths problem with congestion (EDPwC), we are given a graph with n nodes, a set of terminal pairs and an integer c. The objective is to route as many terminal pairs as possible, subject to the constraint that at most c demands can be routed through any edge in the graph. When c = 1, the problem is simply referred to as the edge-disjoint paths (EDP) problem. In this paper, we study the hardness of EDPwC in undirected graphs. We obtain an improved hardness result for EDP, and also show the first polylogarithmic integrality gaps and hardness of approximation results for EDPwC. Specifically, we prove that EDP is (log/sup 1/2 - /spl epsiv// n)-hard to approximate for any constant /spl epsiv/ > 0, unless NP /spl sube/ ZPTIME(n/sup polylog n/). We also show that for any congestion c = o(log log n/log log log n), there is no (log/sup (1-/spl epsiv/)/(c+1)/ n) approximation algorithm for EDPwC, unless NP /spl sube/ ZPTIME(n/sup polylog n/). For larger congestion, where c /spl les/ /spl eta/ log log n/log log log n for some constant /spl eta/, we obtain superconstant inapproximability ratios. All of our hardness results can be converted into integrality gaps for the multicommodity flow relaxation. We also present a separate elementary direct proof of this integrality gap result. Finally, we note that similar results can be obtained for the all-or-nothing flow (ANF) problem, a relaxation of EDP, in which the flow unit routed between the source-sink pairs does not have follow a single path, so the resulting flow is not necessarily integral. Using standard transformations, our results also extend to the node-disjoint versions of these problems as well as to the directed setting.","Approximation algorithms,
Computational Intelligence Society,
Tree graphs,
Polynomials,
Engineering profession,
Routing,
Resource management,
Very large scale integration,
Graph theory"
"Humancentric Applications of RFID Implants: The Usability Contexts of Control, Convenience and Care","Recent developments in the area of RFID have seen the technology expand from its role in industrial and animal tagging applications, to being implantable in humans. With a gap in literature identified between current technological development and future humancentric possibility, little has been previously known about the nature of contemporary humancentric applications. This paper utilizes usability context analyses, to provide a cohesive study on the current development state of humancentric applications, detached from the emotion and prediction which plagues this particular technology","Radiofrequency identification,
Implants,
Usability,
Humans,
Transponders,
Application software,
National security,
Information technology,
Computer science,
Computer industry"
Special education and rehabilitation: teaching and healing with interactive graphics,"The key thesis of this work stipulates that current human-computer interfaces for education and rehabilitation can be greatly improved by recognizing the long-term need for a novel communication layer that can serve as an intermediary between users and today's systems of ever increasing complexity. A special education and rehabilitation system employs real-time interactive computer graphics and photorealistic virtual humans to implement an emotional modulation technique that increases learning efficiency. Emotions play a vital role in a student's ability to memorize and learn new material. Emotions act as a catalyst in the process of transforming information into knowledge, and thus the effectiveness of computer-based learning can be greatly improved if we incorporate emotions into computer use as a learning tool. Tutors, teachers, and professors achieve this effect - called emotional modulation-on a daily basis, using their charisma and other personal and motivational qualities during the time they invest in their students. Therefore, using virtual humans to mimic the natural face-to-face dialogue that normally takes place between student and tutor in real life forms the foundation of a unique and critically important enabling technology for teaching in the future.",
Identifying Individuality Using Mental Task Based Brain Computer Interface,"In recent years, numerous Brain Computer Interface (BCI) technologies have been developed to assist the disabled. In this paper, mental task based BCI is proposed for a different purpose: to identify the individuality of a person. The idea is based on the classification of electroencephalogram (EEG) signals recorded when a user thinks of either one or two mental tasks. As different individuals have different thought processes, this idea would be appropriate for individual identification. To increase the inter-subject differences, EEG data from six electrodes are used instead of one. Sixth order autoregressive features are computed from EEG signals and classified by Linear Discriminant classifier using a modified 10 fold cross validation procedure, which gave an average error of 0.95% when tested on 400 EEG patterns from four subjects. Though the method would have to undergo further development to obtain repeatable good accuracy; this initial study has shown the huge potential of the method over existing biometric identification systems as it is impossible to be faked.","Brain computer interfaces,
Electroencephalography,
Biometrics,
Electrodes,
Fingerprint recognition,
Brain modeling,
Computer interfaces,
Linear discriminant analysis,
Testing,
Geometry"
A Grid Service Infrastructure for Mobile Devices,"One of the visions of grid computing is to access computational resources automatically on demand to deliver the services required with appropriate quality. Because mobile devices are now increasingly common, an infrastructure is required to allow mobile devices to use grid services, and thus enable the execution of complex resource-intensive applications on the resource-constrained devices. We present a system infrastructure that allows local mobile devices to interact with the grid. Central to this infrastructure is a proxy with the ability of dual connectivity to transfer the request from the mobile device to the grid. This system infrastructure combines the mobility of mobile devices with the processing power of the grid.","Grid computing,
Mobile computing,
Pervasive computing,
Personal digital assistants,
Computer networks,
Computer vision,
Batteries,
Application software,
Network servers,
Computer science"
Cross-site computations on the TeraGrid,"The TeraGrid's collective computing resources can help researchers perform very-large-scale simulations in computational fluid dynamics (CFD) applications, but doing so requires tightly coupled communications among different sites. The authors examine a scaled-down turbulent flow problem, investigating the feasibility and scalability of cross-site simulation paradigms, targeting grand challenges such as blood flow in the entire human arterial tree.","Grid computing,
Humans,
Computational modeling,
Biological system modeling,
Blood flow,
Biology computing,
Computational fluid dynamics,
Computer networks,
Scalability,
Physics computing"
Information assurance in wireless sensor networks,"Networking unattended wireless sensors is expected to have significant impact on the efficiency of a large array of military and non-military applications. The main goal of wireless sensor networks is to obtain globally meaningful information from strictly local gleaned by individual sensor nodes. The network is deployed such that the sensors are embedded, possibly at random, in a target environment. However, a wireless sensor network is only as good as the information it produces. In this respect, the most important concern is information assurance. Indeed, in most application domains sensor networks constitute a mission critical component requiring commensurate security protection. Sensor network communications must prevent disclosure and undetected modification of exchanged messages. Due to the fact that individual sensor nodes are anonymous and that communication among sensors is via wireless links, sensor networks are highly vulnerable to security attacks. If an adversary can thwart the work of the network by perturbing the information produced, stopping production, or pilfering information, then the perceived usefulness of sensor networks would be drastically curtailed. Thus, security is a major issue that must be resolved in order for the potential of wireless sensor networks to be fully exploited.","Intelligent networks,
Wireless sensor networks,
Sensor arrays,
Information security,
Military computing,
Mission critical systems,
Protection,
Production,
Computer science,
Application software"
Improving Web clustering by cluster selection,"Web page clustering is a technology that puts semantically related Web pages into groups and is useful for categorizing, organizing, and refining search results. When clustering using only textual information, suffix tree clustering (STC) outperforms other clustering algorithms by making use of phrases and allowing clusters to overlap. One problem of STC and other similar algorithms is how to select a small set of clusters to display to the user from a very large set of generated clusters. The cluster selection method used in STC is flawed in that it does not handle overlapping clusters appropriately. This paper introduces a new cluster scoring function and a new cluster selection algorithm to overcome the problems with overlapping clusters, which are combined with STC to make a new clustering algorithm ESTC. This paper's experiments show that ESTC significantly outperforms STC and that even with less data ESTC performs similarly to a commercial clustering search engine.","Clustering algorithms,
Search engines,
Filters,
Web pages,
Mathematics,
Statistics,
Computer science,
Organizing,
Displays,
Internet"
Efficient content-based detection of zero-day worms,"Recent cybersecurity incidents suggest that Internet worms can spread so fast that in-time human-mediated reaction is not possible, and therefore initial response to cyberattacks has to be automated. The first step towards combating new unknown worms is to be able to detect and identify them at the first stages of their spread. In this paper, we present a novel method for detecting new worms based on identifying similar packet contents directed to multiple destination hosts. We evaluate our method using real traffic traces that contain real worms. Our results suggest that our approach is able to identify novel worms while at the same time the generated false alarms reach as low as zero percent.",
Dual-homing protection in IP-over-WDM networks,"A fault-tolerant scheme, called dual homing, is generally used in IP-based access networks to increase the availability of the networks. In a dual-homing architecture, a host is connected to two different access routers; therefore, it is unlikely that the host will be denied access to the network as the result of an access line break, a defective power supply in the access router, or congestion of the access router. This dual-homing architecture in the access network imposes the overhead to provide protection in the core network. Scaling the next-generation IP-over-wavelength-division-multiplexing (WDM) Internet, and being able to support a growing number of such dual-homing connections, as well as protection, demands a scalable mechanism to contain this overhead for protection in the WDM networks. This paper studies the coordinated protection design to reduce the protection cost in the WDM core network, given a dual-homing infrastructure in the access network. The protection problem is considered for both static and dynamic traffic. Several solutions are proposed, and the performances of the solutions are compared. We also prove that one of the proposed algorithms gives a solution that, in the worst case, is at most 4/3 times the cost of the optimal solution.",
The supercomputer industry in light of the Top500 data,"The Top500 list, which has been updated semiannually for the past decade (1995-2005), ranks the 500 most powerful computers installed worldwide. Analyzing this data gives an impartial look at the supercomputer industry's current state and development trends, and sheds light on the challenges the industry faces.","Supercomputers,
Computer industry,
Computer aided manufacturing,
Power system modeling,
Application software,
Publishing,
Lifting equipment,
Virtual manufacturing,
Machinery production industries,
Throughput"
DynAlloy: upgrading alloy with actions,"We present DynAlloy, an extension to the Alloy specification language to describe dynamic properties of systems using actions. Actions allow us to appropriately specify dynamic properties, particularly, properties regarding execution traces, in the style of dynamic logic specifications. We extend Alloy's syntax with a notation for partial correctness assertions, whose semantics relies on an adaptation of Dijkstra's weakest liberal precondition. These assertions, defined in terms of actions, allow us to easily express properties regarding executions, favoring the separation of concerns between the static and dynamic aspects of a system specification. We also extend the Alloy tool in such a way that DynAlloy specifications are also automatically analyzable, as standard Alloy specifications. We present the foundations, two case-studies, and empirical results evidencing that the analysis of DynAlloy specifications can be performed efficiently.","Computer science,
Specification languages,
Software engineering,
Permission,
Performance analysis,
Software design,
Logic design,
Reasoning about programs,
Formal specifications,
State-space methods"
An x-axis single-crystalline silicon microgyroscope fabricated by the extended SBM process,"A high-aspect ratio, single-crystal line silicon x-axis microgyroscope is fabricated using the extended sacrificial bulk micromachining (SBM) process. The x-axis microgyroscope in this paper uses vertically offset combs to resonate the proof mass in the vertical plane, and lateral combs to sense the Coriolis force in the horizontal plane. This requires fabricating vertically and horizontally moving structures for actuation and sensing, respectively, which is very difficult to achieve in single-crystalline silicon. However, single-crystalline silicon high-aspect ratio structures are preferred for high performance. The extended SRN/I process is a two-mask process, but all structural parts and combs are defined in one mask level. Thus, there is no misalignment in any structural parts or comb fingers. In this extended SBM process, all vertical dimensions of the structure, including the comb height, vertical comb offset and sacrificial gap, can be defined arbitrarily (up to a few tens of micrometers). For electrical isolation, silicon-on-insulator (SOI) wafers are used, but the inherent footing phenomenon in the SOI deep etching is eliminated and smooth structural shapes are obtained, because the SBM process is used. In the fabricated x-axis microgyroscope, the lower combs used to vibrate the proof mass are vertically offset 12 /spl mu/m from the upper combs. The fabricated x-axis microgyroscope can resolve 0.1 deg/s angular rate, and the measured bandwidth is 100 Hz. The reported work represents the first x-axis single-crystalline silicon microgyroscope fabricated using only one wafer without wafer bonding. We have previously reported several versions of z-axis microgyroscopes and x-, y-, and z-axis accelerometers, using the SBM process. The results or this paper allow integrating x-, y-, and z-axis microgyroscopes as well as x-, y-, and z-axis microaccelerometers in one wafer, using the same mask and the same process.","Gyroscopes,
Micromachining,
Bandwidth,
Computer science,
Assembly,
Fingers,
Silicon on insulator technology,
Etching,
Structural shapes,
Wafer bonding"
WIANI: wireless infrastructure and ad-hoc network integration,"Wireless networks have been widely deployed in recent years to provide high-speed Internet access to mobile users. In traditional IEEE 802.11 wireless LANs, all users directly connect to an access point (AP) and all packets are forwarded by the AP. As a result, the coverage and capacity of the network is limited. If ad hoc mode is adopted in both the AP and mobile nodes, the one hop connections from AP can be extended to multiple hops. Such an architecture, termed WIANI (wireless infrastructure and ad-hoc network integration), is able to extend the network coverage beyond the coverage of APs. Furthermore, users may take advantage of the ad hoc connections to forward local data and hence alleviate the traffic load through the AP and increase the network capacity. We propose a dynamic load-balancing protocol for WIANI in which all APs and nodes operate in ad-hoc mode. Our protocol consists of two parts, a load-balancing zone forming algorithm and a weighted x-hop routing algorithm. Using simulation, we show that our protocol improves system throughput and reduces packet delivery delay.","Ad hoc networks,
Wireless LAN,
IP networks,
Helium,
Wireless networks,
Telecommunication traffic,
Costs,
Computer science,
USA Councils,
Mobile computing"
Failure map functions and accelerated mean time to failure tests: new approaches for improving the reliability estimation in systems exposed to single event upsets,"The application cross section ""/spl sigma//sub AP/"" is a parameter used to characterize the systems single event upset (SEU) vulnerability, but does not give, in a direct way, the system's reliability. Reliability prediction of a system exposed to SEU can be improved with the knowledge of the system's time to failure (TTF) probability distribution function. This work presents two new methods suitable to provide this information. The first one is based on the construction of a function named the failure map function (FMF). FMF contains the information needed to calculate all the TTF statistical properties by means of numerical procedures. For the cases where the FMF function is difficult to obtain, a second method is presented which consist of injecting SEUs at a rate several orders of magnitude higher than the real rate. A histogram of the TTF random variable for the accelerated process is obtained. The system's reliability can then be derived by means of statistical and numerical procedures.","Life estimation,
System testing,
Single event transient,
Single event upset,
Registers,
Aerodynamics,
Probability distribution,
Histograms,
Random variables,
Acceleration"
Efficient intrusion detection using automaton inlining,"Host-based intrusion detection systems attempt to identify attacks by discovering program behaviors that deviate from expected patterns. While the idea of performing behavior validation on-the-fly and terminating errant tasks as soon as a violation is detected is appealing, existing systems exhibit serious shortcomings in terms of accuracy and/or efficiency. To gain acceptance, a number of technical advances are needed. In this paper we focus on automated, conservative, intrusion detection techniques, i.e. techniques which do not require human intervention and do not suffer from false positives. We present a static analysis algorithm for constructing a flow- and context-sensitive model of a program that allows for efficient online validation. Context-sensitivity is essential to reduce the number of impossible control-flow paths accepted by the intrusion detection system because such paths provide opportunities for attackers to evade detection. An important consideration for on-the-fly intrusion detection is to reduce the performance overhead caused by monitoring. Compared to the existing approaches, our inlined automaton model (IAM) presents a good tradeoff between accuracy and performance. On a 32K line program, the monitoring overhead is negligible. While the space requirements of a naive IAM implementation can be quite high, compaction techniques can be employed to substantially reduce that footprint.","Intrusion detection,
Automata,
Monitoring,
Algorithm design and analysis,
Context modeling,
Computer science education,
Educational programs,
Computer security,
Information security,
Humans"
Managing software change tasks: an exploratory study,"Programmers often have to perform change tasks that involve unfamiliar portions of a software system's code base. To help inform the design of software development tools intended to support programmers in this context, we conducted a qualitative study of how programmers manage such change tasks. In the study we observed Java programmers using a state-of-the-practice IDE to work on real change tasks to a medium-sized open source software system. In this paper we present our results, describing eight observations about the programmers' behavior and the impact of the development environment on their behavior. We also highlight several key challenges faced by the programmers and discuss the implications of our results on the design of development tools.","Programming profession,
Java,
Software performance,
Software systems,
Open source software,
Computer science,
Software design,
Quality management,
Software development management,
Software tools"
Fast algorithms for approximate semidefinite programming using the multiplicative weights update method,"Semidefinite programming (SDP) relaxations appear in many recent approximation algorithms but the only general technique for solving such SDP relaxations is via interior point methods. We use a Lagrangian-relaxation based technique (modified from the papers of Plotkin, Shmoys, and Tardos (PST), and Klein and Lu) to derive faster algorithms for approximately solving several families of SDP relaxations. The algorithms are based upon some improvements to the PST ideas - which lead to new results even for their framework - as well as improvements in approximate eigenvalue computations by using random sampling.","Approximation algorithms,
Algorithm design and analysis,
Frequency estimation,
Computer science,
Polynomials,
Lagrangian functions,
Eigenvalues and eigenfunctions,
Sampling methods,
Ellipsoids,
NP-hard problem"
Publisher mobility in distributed publish/subscribe systems,"The decoupling of producers and consumers in the publish/subscribe paradigm lends itself well to the support of mobile users who roam about the environment with intermittent network connectivity. This paper presents the first quantitative evaluation of publisher mobility in a distributed publish/subscribe system. Our results indicate that publisher mobility breaks a fundamental assumption of publish/subscribe systems and has a significant performance impact. We formalize publisher mobility algorithms for a distributed publish/subscribe system, and develop and evaluate optimizations to the mobile publisher algorithms.",
The Hierarchical Fair Competition (HFC) Framework for Sustainable Evolutionary Algorithms,"Many current Evolutionary Algorithms (EAs) suffer from a tendency to converge prematurely or stagnate without progress for complex problems. This may be due to the loss of or failure to discover certain valuable genetic material or the loss of the capability to discover new genetic material before convergence has limited the algorithm's ability to search widely. In this paper, the Hierarchical Fair Competition (HFC) model, including several variants, is proposed as a generic framework for sustainable evolutionary search by transforming the convergent nature of the current EA framework into a non-convergent search process. That is, the structure of HFC does not allow the convergence of the population to the vicinity of any set of optimal or locally optimal solutions. The sustainable search capability of HFC is achieved by ensuring a continuous supply and the incorporation of genetic material in a hierarchical manner, and by culturing and maintaining, but continually renewing, populations of individuals of intermediate fitness levels. HFC employs an assembly-line structure in which subpopulations are hierarchically organized into different fitness levels, reducing the selection pressure within each subpopulation while maintaining the global selection pressure to help ensure the exploitation of the good genetic material found. Three EAs based on the HFC principle are tested - two on the even-10-parity genetic programming benchmark problem and a real-world analog circuit synthesis problem, and another on the HIFF genetic algorithm (GA) benchmark problem. The significant gain in robustness, scalability and efficiency by HFC, with little additional computing effort, and its tolerance of small population sizes, demonstrates its effectiveness on these problems and shows promise of its potential for improving other existing EAs for difficult problems. A paradigm shift from that of most EAs is proposed: rather than trying to escape from local optima or delay convergence at a local optimum, HFC allows the emergence of new optima continually in a bottom-up manner, maintaining low local selection pressure at all fitness levels, while fostering exploitation of high-fitness individuals through promotion to higher levels.","genetic programming,
Sustainable evolutionary algorithms,
building blocks,
premature convergence,
diversity,
fair competition,
hierarchical problem solving"
A Hierarchical Bayesian Model for Learning Nonlinear Statistical Regularities in Nonstationary Natural Signals,"Capturing statistical regularities in complex, high-dimensional data is an important problem in machine learning and signal processing. Models such as principal component analysis (PCA) and independent component analysis (ICA) make few assumptions about the structure in the data and have good scaling properties, but they are limited to representing linear statistical regularities and assume that the distribution of the data is stationary. For many natural, complex signals, the latent variables often exhibit residual dependencies as well as nonstationary statistics. Here we present a hierarchical Bayesian model that is able to capture higher-order nonlinear structure and represent nonstationary data distributions. The model is a generalization of ICA in which the basis function coefficients are no longer assumed to be independent; instead, the dependencies in their magnitudes are captured by a set of density components. Each density component describes a common pattern of deviation from the marginal density of the pattern ensemble; in different combinations, they can describe nonstationary distributions. Adapting the model to image or audio data yields a nonlinear, distributed code for higher-order statistical regularities that reflect more abstract, invariant properties of the signal.",
Application of expectation maximization algorithms for image resolution improvement in a small animal PET system,"Modern positron emission tomography (PET) systems, offering high counting rate capabilities, high sensitivity, and near-submillimeter coordinate resolution, require fast image reconstruction software that can operate on list-mode data and take into account most of finite resolution effects such as photon scattering, positron range in tissue, and detector features. It has already been demonstrated that the expectation maximization (EM) method with extended system matrix modeling looks very attractive for image resolution recovery in PET imaging studies. In this paper, the performance of EM-based algorithms (in particular, their ability to improve the image resolution) is evaluated for a small animal PET imager with several phantoms. The achievement of a substantial decrease in processing time using an EM deblurring procedure is shown, as is an approach to successfully treat what are essentially nonspace-invariant resolution effects within a shift-invariant model.","Image resolution,
Animals,
Positron emission tomography,
Application software,
Image reconstruction,
Single photon emission computed tomography,
Electromagnetic scattering,
Particle scattering,
Detectors,
Computer vision"
Taking Topic Detection From Evaluation to Practice,"The Topic Detection and Tracking (TDT) research community investigates information retrieval methods for organizing a constantly arriving stream of news articles by the events that they discuss. Our best system for the open evaluations of TDT has used an approach that turned out to be problematic when the cluster detection technology was deployed in a real world setting. To avoid generating ""garbage"" clusters, we had to revert to a different approach and to explore engineering solutions that were not motivated by the model. Our experiences also led us to propose extensions to the formal TDT evaluation.","Information retrieval,
Event detection,
Organizing,
Computer science,
NIST,
Cost function,
Clustering algorithms,
Failure analysis,
Error correction,
Performance evaluation"
Stochastic analog-to-digital conversion,"This paper suggests a stochastic approach to data conversion. It is applicable to serial, parallel, two-step as well as delta-sigma ADCs and DACs. In the serial implementation of this scheme, a sample-and-hold circuit, a noise source and a comparator are combined with an accumulate-and-dump digital stage to perform serial multibit A/D conversion. In the parallel ADC, M nominally identical primitive converter cells with the same input signal, x(t), but with different and uncorrelated random dither signals, d/sub i/(t) (i = 1, 2, ..., M), are combined to perform the data conversion. Possible combination of the two methodologies is also outlined.","Stochastic processes,
Analog-digital conversion,
Counting circuits,
Signal to noise ratio,
Noise reduction,
Computer science,
Stochastic resonance,
Data conversion,
Quantization,
Drives"
An analysis of FPGA-based UDP/IP stack parallelism for embedded Ethernet connectivity,"When designing FPGA-based Ethernet connected embedded systems the priority and necessity of requirements such as cost, area, flexibility etc. varies for each system. Simplified for most systems, it can be stated that no extra functionality than required is desired. Hence, when designing a UDP/IP stack in an FPGA a single UDP/IP stack ""template"" design is not suitable to effectively realize the different embedded network system requirements. We present three different UDP/IP stack cores, with different grades of parallelism and suited for various network demands. We show that the UDP/IP core area can be reduced to 1/3 of the original size with an appropriate implementation, accomplished by a trade-off between parallelism/latency and area. Furthermore guidelines are proposed on how to perform the trade-off between parallelism, area (cost), flexibility and functionality when designing an UDP/IP stack for compact embedded network systems.","Ethernet networks,
Parallel processing,
Field programmable gate arrays,
IP networks,
Local area networks,
TCPIP,
Access protocols,
Computer science,
Hardware,
Embedded system"
Monocular 3D tracking of the golf swing,"We propose an approach to incorporating dynamic models into the human body tracking process that yields full 3D reconstructions from monocular sequences. We formulate the tracking problem in terms of minimizing a differentiable criterion whose differential structure is rich enough for successful optimization using a simple hill-climbing approach as opposed to a multihypotheses probabilistic one. In other words, we avoid the computational complexity of multihypotheses algorithms while obtaining excellent results under challenging conditions. To demonstrate this, we focus on monocular tracking of a golf swing from ordinary video. It involves both dealing with potentially very different swing styles, recovering arm motions that are perpendicular to the camera plane and handling strong self-occlusions.","Tracking,
Biological system modeling,
Computer vision,
Humans,
Cameras,
Laboratories,
Principal component analysis,
Computer science,
Image reconstruction,
Computational complexity"
SOA-based integration of IT service management applications,"IT service providers use applications to support their business processes. The need for specialized IT management functionality and information generates a multitude and diversity of management applications that can be recognized in one IT provider's scenario. To run IT and to provide IT services effectively and efficiently, management applications have to be integrated along operational processes. This article introduces an approach to integrate management applications by leveraging a service-oriented architecture (SOA). Therefore, a sufficient understanding of IT service provider's processes and cooperative roles is essential. The presented SOA defines how to integrate management applications loosely coupled in a process-oriented manner, bridging the gap between operational processes and management applications.","Service oriented architecture,
Web services,
Application software,
Information management,
Quality management,
Telematics,
Computer science,
Libraries,
Monitoring,
Control systems"
Modeling context-aware e-learning scenarios,"In the last decade, e-learning has been introduced to a variety of blended learning scenarios, such as life-long learning, university lectures, and game-based learning. In all these scenarios the learner's situation or context is an essential asset in designing the learning process. Recent research suggests aiding the design process through the use of visual modeling approaches. Pervasive computing environments particularly call for extending these approaches in terms of enhanced context-awareness. This paper addresses these needs by introducing a UML-based modeling extension for explicitly including relationships between context and learning activities in the learning design models. The feasibility and applicability of our approach is demonstrated by a laboratory lecture case study, and respectively by a context-aware learning prototype that was developed using RFID technology for sensing of nearby persons and physical resources.","Context modeling,
Electronic learning,
Unified modeling language,
Pervasive computing,
Process design,
Laboratories,
Prototypes,
Education,
Conferences,
Computer science"
Design and evaluation of a decentralized system for grid-wide fairshare scheduling,"This contribution presents a decentralized architecture for a grid-wide fairshare scheduling system and demonstrates its potential in a simulated environment. The system, which preserves local site autonomy, enforces locally and globally scoped share policies, allowing local resource capacity as well as global grid capacity to be logically divided across different groups of users. The policy model is hierarchical and subpolicy definition can be delegated so that, e.g., a VO that has been granted a resource share can partition its share across its projects, which in turn can divide their shares between project members. There is no need for a central coordinator as policies are enforced collectively by the resource schedulers. Each local scheduler adopts a grid-wide view on utilization in order to steer local resource utilization to not only maintain local resource shares but also to contribute to maintaining global shares across the entire set of grid resources. Share enforcement is addressed by an algorithm that calculates simple priority values, thus simplifying integration with local schedulers, which can remain unaware of the hierarchical share policy structure",
Cryptography in the bounded quantum-storage model,"We initiate the study of two-party cryptographic primitives with unconditional security, assuming that the adversary's quantum memory is of bounded size. We show that oblivious transfer and bit commitment can be implemented in this model using protocols where honest parties need no quantum memory, whereas an adversarial player needs quantum memory of size at least n/2 in order to break the protocol, where n is the number of qubits transmitted. This is in sharp contrast to the classical bounded-memory model, where we can only tolerate adversaries with memory of size quadratic in honest players' memory size. Our protocols are efficient, non-interactive and can be implemented using today's technology. On the technical side, a new entropic uncertainty relation involving min-entropy is established.","Cryptography,
Cryptographic protocols,
Information security,
National security,
Computer science,
Uncertainty,
Computer errors,
Computer security,
Councils,
Quantum mechanics"
Weaknesses and drawbacks of a password authentication scheme using neural networks for multiserver architecture,"In 2001, Li et al. proposed a password authentication scheme for the multiserver architecture by using a pattern classification system based on neural networks. Herein, we demonstrate that Li et al.'s scheme is vulnerable to an offline password guessing attack and a privileged insider's attack, and is not reparable. Additionally, we show that Li et al.'s scheme has several drawbacks in practice.","Authentication,
Neural networks,
Network servers,
Forgery,
Pattern classification,
Artificial neural networks,
Smart cards,
Public key,
Councils,
Computer science"
Automatic dialog act segmentation and classification in multiparty meetings,"We explore the two related tasks of dialog act (DA) segmentation and DA classification for speech from the ICSI Meeting Corpus. We employ simple lexical and prosodic knowledge sources, and compare results for human-transcribed versus automatically recognized words. Since there is little previous work on DA segmentation and classification in the meeting domain, our study provides baseline performance rates for both tasks. We introduce a range of metrics for use in evaluation, each of which measures different aspects of interest. Results show that both tasks are difficult, particularly for a fully automatic system. We find that a very simple prosodic model aids performance over lexical information alone, especially for segmentation. Both tasks, but particularly word-based segmentation, are degraded by word recognition errors. Finally, while classification results for meeting data show some similarities to previous results for telephone conversations, findings also suggest a potential difference with respect to the effect of modeling DA context.","Telephony,
Natural languages,
Microphones,
Speech recognition,
Automatic speech recognition,
Computer science,
Degradation,
Computer errors,
Context modeling,
Humans"
Movement Generation with Circuits of Spiking Neurons,"How can complex movements that take hundreds of milliseconds be generated by stereotypical neural microcircuits consisting of spiking neurons with a much faster dynamics? We show that linear readouts from generic neural microcircuit models can be trained to generate basic arm movements. Such movement generation is independent of the arm model used and the type of feedback that the circuit receives. We demonstrate this by considering two different models of a two-jointed arm, a standard model from robotics and a standard model from biology, that each generates different kinds of feedback. Feedback that arrives with biologically realistic delays of 50 to 280 ms turns out to give rise to the best performance. If a feedback with such desirable delay is not available, the neural microcircuit model also achieves good performance if it uses internally generated estimates of such feedback. Existing methods for movement generation in robotics that take the particular dynamics of sensors and actuators into account (embodiment of motor systems) are taken one step further with this approach, which provides methods for also using the embodiment of motion generation circuitry, that is, the inherent dynamics and spatial structure of neural circuits, for the generation of movement.",
Market models and pricing mechanisms in a multihop wireless hotspot network,"Multihop wireless hotspot network [A. Balachandran et al., (2003), F. Fitzek et al., (2003), Y-D. Lin et al., (1999), K-C. Wang et al., (2003)] has been recently proposed to extend the coverage area of a base station. However, with selfish nodes in the network, multihop packet forwarding cannot take place without an incentive mechanism. In this paper, we adopt the ""pay for service"" incentive model, i.e., clients pay the relaying nodes for their packet forwarding service. Our focus in this paper is to determine a ""fair"" pricing for packet forwarding. To this end, we model the system as a market where the pricing for packet forwarding is determined by demand and supply. Depending on the network communication scenario, the market models are different. We classify the network into four different scenarios and propose different pricing mechanisms for them. Our simulation results show that the pricing mechanisms are able to guide the market into an equilibrium state quickly. We also show that maintaining communication among the relaying nodes is important for a stable market pricing.","Pricing,
Intelligent networks,
Spread spectrum communication,
Base stations,
Relays,
Humans,
Computer science,
Airports,
Mobile communication,
Throughput"
Anxiety detection during human-robot interaction,"This paper describes an experiment to determine the feasibility of using physiological signals to determine the human response to robot motions during direct human-robot interaction. A robot manipulator is used to generate common interaction motions, and human subjects are asked to report their response to the motions. The human physiological response is also measured. Motion paths are generated using a classic potential field planner and a safe motion planner, which minimizes the potential collision force along the path. A fuzzy inference engine is developed to estimate the human response based on the physiological measures. Results show that emotional arousal can be detected using physiological signals and the inference engine. Comparison of initial results between the two planners shows that subjects report less anxiety and surprise with the safe planner for high planner speeds.","Biomedical monitoring,
Human robot interaction,
State estimation,
Robot motion,
Manipulators,
Engines,
Safety,
Robot control,
Signal processing,
Heart rate"
RTK-Spec TRON: a simulation model of an ITRON based RTOS kernel in SystemC,The paper presents the methodology and the modeling constructs we have developed to capture the real time aspects of RTOS (real time operating system) simulation models in a system level design language (SLDL) like SystemC. We describe these constructs and show how they are used to build a simulation model of an RTOS kernel targeting the /spl mu/-ITRON (micro industrial TRON - the real time operating system nucleus) OS specification standard.,"Kernel,
Context modeling,
Libraries,
System-level design,
Dynamic programming,
Information science,
Real time systems,
Microprogramming,
Acceleration,
Timing"
Quasi-static voltage scaling for energy minimization with time constraints,"Supply voltage scaling and adaptive body-biasing are important techniques that help to reduce the energy dissipation of embedded systems. This is achieved by dynamically adjusting the voltage and performance settings according to the application needs. In order to take full advantage of slack that arises from variations in the execution time, it is important to recalculate the voltage (performance) settings during run time, i.e., online. However voltage scaling (VS) is computationally expensive, and thus significantly hampers the possible energy savings. To overcome the online complexity, we propose a quasi-static voltage scaling scheme, with a constant online time complexity O(1). This allows us to increase the exploitable slack as well as to avoid the energy dissipated due to online recalculation of the voltage settings. We conduct several experiments that demonstrate the advantages of the proposed technique over the previously published voltage scaling approaches.",
Online mining (recently) maximal frequent itemsets over data streams,"A data stream is a massive, open-ended sequence of data elements continuously generated at a rapid rate. Mining data streams is more difficult than mining static databases because the huge, high-speed and continuous characteristics of streaming data. In this paper, we propose a new one-pass algorithm called DSM-MFI (stands for Data Stream Mining for Maximal Frequent Itemsets), which mines the set of all maximal frequent itemsets in landmark windows over data streams. A new summary data structure called summary frequent itemset forest (abbreviated as SFI-forest) is developed for incremental maintaining the essential information about maximal frequent itemsets embedded in the stream so far. Theoretical analysis and experimental studies show that the proposed algorithm is efficient and scalable for mining the set of all maximal frequent itemsets over the entire history of the data streams.",
On the autocorrelation distributions of Sidel'nikov sequences,"For a prime p and positive integers M and n such that M|p/sup n/-1, Sidel'nikov introduced M-ary sequences (called Sidel'nikov sequences) of period p/sup n/-1, the out-of-phase autocorrelation magnitude of which is upper bounded by 4. In this correspondence, we derived the autocorrelation distributions, i.e., the values and the number of occurrences of each value of the autocorrelation function of Sidel'nikov sequences. The frequency of each autocorrelation value of an M-ary Sidel'nikov sequence is expressed in terms of the cyclotomic numbers of order M. It is also pointed out that the total number of distinct autocorrelation values is dependent not only on M but also on the period of the sequence, but always less than or equal to (M/2)+1.","Autocorrelation,
Frequency,
Data communication,
Modulation coding,
Code standards,
Communication standards,
Error correction codes,
Binary sequences,
Computer science,
Costs"
LHA-SP: secure protocols for hierarchical wireless sensor networks,"Wireless sensor networks (WSNs) are ad hoc networks comprised mainly of small sensor nodes with limited resources, and can be used to monitor areas of interest. In this paper, we propose a solution for securing heterogeneous hierarchical WSNs with an arbitrary number of levels. Our solution relies exclusively on symmetric key schemes, is highly distributed, and takes into account node interaction patterns that are specific to clustered WSNs.","Wireless application protocol,
Wireless sensor networks,
Ad hoc networks,
Computer science,
Computerized monitoring,
Base stations,
Reconnaissance,
Surveillance,
Power system protection,
Mobile ad hoc networks"
The implementation and evaluation of OASIS: a web-based learning and assessment tool for large classes,"This paper describes a Web-based learning and assessment tool developed and implemented over a five-year period. Used predominately with first- and second-year students for skills practice and summative assessment, the tool delivers individualized tasks, marks student responses, supplies students with prompt feedback, and logs student activity. Interviews with instructors indicated that the software had enabled them to manage workloads in spite of rising class sizes and that student learning, based on observation and assessment results, had been enhanced rather than compromised. Student surveys, interviews, focus-group discussions, and informal feedback showed that students found the software easy to use and felt it helped them improve their skills and understanding. Student activity logs provided an insight into student study habits and confirmed the motivating power of assessment.","Computer aided instruction,
Internet,
Electrical engineering education,
Computer science education"
Network security basics,"Writing a basic article on network security is something like writing a brief introduction to flying a commercial airliner. Much must be omitted, and an optimistic goal is to enable the reader to appreciate the skills required. The first question to address is what we mean by ""network security."" Several possible fields of endeavor come to mind within this broad topic, and each is worthy of a lengthy article. To begin, virtually all the security policy issues apply to network as well as general computer security considerations. In fact, viewed from this perspective, network security is a subset of computer security. The art and science of cryptography and its role in providing confidentiality, integrity, and authentication represents another distinct focus even though it's an integral feature of network security policy. The topic also includes design and configuration issues for both network-perimeter and computer system security. The practical networking aspects of security include computer intrusion detection, traffic analysis, and network monitoring. This article focuses on these aspects because they principally entail a networking perspective.","Computer hacking,
Web and internet services,
Computer crime,
Computer security,
Privacy,
Local area networks,
IP networks,
Operating systems,
Protocols,
Data security"
Generalizing Consistency Checking between Software Views,"Inconsistencies between software views are a source of errors for software systems. In this paper we present a general approach that aids in finding inconsistencies between different views. This approach supports both intra phase consistency checking and inter phase consistency checking. The approach is suitable for detecting consistency problems between, for example, multiple diagrams in a UML design as well as between a design and an implementation. The approach is based on verification of rules using relation partition algebra. In this paper, we present two types of rules: obligations and the more commonly used constraints, which can be viewed as lower bounds and upper bounds, respectively. To check consistency between views, rules are derived from one view, the so-called prevailing view, and imposed on another view, the so-called subordinate view. Because our approach does not prescribe which views are prevailing, it can be used in any arbitrary process. Violations to rules can be expressed in terms of either the prevailing view or the subordinate view. Exceptions to rules are easiliy embedded in our general approach to consistency checking.","Unified modeling language,
Software systems,
Computer architecture,
Mathematics,
Computer science,
Computer errors,
Algebra,
Upper bound,
Software standards,
Software design"
Considerations for fault-tolerant network on chips,"According to International Technology Roadmap for Semiconductors (ITRS), before the end of this decade we will be entering the era of a billion transistors on a single chip. However, it has been observed that as the system grows, so does the complexity of integrating various components on a chip. The major threat toward the achievement of a billion transistor chip is poor scalability of current interconnect structure of today's SoCs as presented by Benini and De Michelli (2002). In order to cope with growing interconnect infrastructure, the ""network on chip (NoC)"" concept was introduced. With network methodologies coming on-chip, various characteristics of traditional networks come into play. So far, failures that are common in regular networks were hardly considered on-chip; this paper introduces ideas of dynamic routing and congestion control in the context of NoCs and explains how they could be applied to cope with adverse physical effects of deep sub-micron technology.","Fault tolerance,
Network-on-a-chip,
System-on-a-chip,
Scalability,
Routing,
Computer science,
Silicon,
Transistors,
Design engineering,
Computer networks"
Automated Linguistic Analysis of Deceptive and Truthful Synchronous Computer-Mediated Communication,"The present study investigates changes in both the sender's and the target's linguistic style across truthful and deceptive dyadic communication in a synchronous text-based setting. A computer-based analysis of 242 transcripts revealed that senders produced more words overall, decreased their use of self-oriented pronouns but increased other-oriented pronouns, and used more sense-based descriptions (e.g., seeing, touching) when lying than when telling the truth. In addition, motivated senders avoided causal terms during deception, while unmotivated senders relied more heavily on simple negations. Receivers used more words when being deceived, but they also asked more questions and used shorter sentences when being lied to than when being told the truth, especially when the sender was unmotivated. These findings are discussed in terms of their implications for linguistic style matching and interpersonal deception theory.","Computer mediated communication,
Psychology,
Humans,
Communications technology,
Speech,
Production"
Nonuniform banking for reducing memory energy consumption,"Main memories can consume a large percentage of overall energy in many data-intensive embedded applications. The past research proposed and evaluated memory banking as a possible approach for reducing memory energy consumption. One of the common characteristics/assumptions made by most of the past work on banking is that all the banks are of the same size. While this makes the formulation of the problem easy, it also restricts the potential solution space. Motivated by this observation, this paper investigates the possibility of employing nonuniform bank sizes for reducing memory energy consumption. Specifically, it proposes an integer linear programming (ILP) based approach that returns the optimal nonuniform bank sizes and accompanying data-to-bank mapping. It also studies how data migration can further improve over nonuniform banking. We implemented our approach using an ILP tool and made extensive experiments. The results show that the proposed strategy brings important energy benefits over the uniform banking scheme, and data migration across banks tends to increase these savings.","Banking,
Energy consumption,
Integer linear programming,
Embedded system,
Memory architecture,
Computer science,
Data engineering,
Power engineering and energy,
Application software,
System performance"
Multiple-antenna differential lattice decoding,"From a lattice viewpoint, Clarkson, Sweldens and Zheng significantly reduced the complexity of multiantenna differential decoding. Their approximate decoding algorithm, however, has not unleashed the full potential of lattice decoding. In this paper, we present several improved algorithms, generally referred to as differential lattice decoding (DLD), for multiantenna communication. We first analyze two distinct approximate DLD algorithms, and then develop an algorithm that exactly finds the closest lattice point in the Euclidean space. This exact DLD is subsequently augmented by local search to compensate for the remaining approximation. The small amount of extra complexity of the exact or augmented DLD is rewarded by a clear performance gain. We find that employing basis reduction is very effective to reduce the overall decoding complexity for high lattice dimensions. Moreover, the dimension of the lattice defined in this paper is independent of the number of receive antennas, which results in not only lower complexity, but also better performance for a multiantenna receiver.","Lattices,
Maximum likelihood decoding,
MIMO,
Senior members,
Algorithm design and analysis,
Performance gain,
Receiving antennas,
Wireless communication,
Councils,
Computer science"
Mimicry of Sharp Turning Behaviours in a Robotic Fish,"In nature, fish has astonishing swimming ability after thousands years of evolution. To realise fish-like swimming behaviours by a robotic system poses tremendous challenges, especially for the C-shape turning (CST). This requires fully understanding of fish biomechanics and the way to mimic it. Based on observations of fish swimming, this paper presents a new kinematics model to mimic the CST behaviour in a robotic fish with a 4-DOF (degrees of freedom) tail. The simulated and the real experiments are conducted to show the theoretic feasibility. Both behaviour analyses and hydrodynamics features between the robotic fish and the real fish are presented to show the performance.","Turning,
Marine animals,
Kinematics,
Tail,
Hydrodynamics,
Propulsion,
Computer science,
Educational robots,
Biological system modeling,
Biomechanics"
SRI's 2004 NIST speaker recognition evaluation system,"The paper describes our recent efforts in exploring longer-range features and their statistical modeling techniques for speaker recognition. In particular, we describe a system that uses discriminant features from cepstral coefficients, and systems that use discriminant models from word n-grams and syllable-based NERF n-grams. These systems together with a cepstral baseline system are evaluated on the 2004 NIST speaker recognition evaluation dataset. The effect of the development set is measured using two different datasets, one from Switchboard databases and another from the FISHER database. Results show that the difference between the development and evaluation sets affects the performance of the systems only when more training data is available. Results also show that systems using longer-range features combined with the baseline result in about a 31% improvement with 1-side training over the baseline system and about a 61% improvement with 8-side training over the baseline system.","NIST,
Speaker recognition,
Testing,
Cepstral analysis,
Spatial databases,
Telephone sets,
Error analysis,
Telephony,
Automatic speech recognition,
Computer science"
FRR: a proportional and worst-case fair round robin scheduler,"In this paper, we propose an O(1) complexity round robin scheduler, called fair round robin (FRR), that provides good fairness and delay properties. Unlike existing O(1) complexity round robin schedulers that can only achieve long term fairness, FRR not only provides proportional fairness, but also maintains a constant normalized worst-case fair index as defined in Bennett and Zhang's work. This means that FRR guarantees both short term and long term fairness among all backlogged flows.","Round robin,
Scheduling algorithm,
Delay,
Processor scheduling,
Quality of service,
Bandwidth,
High-speed networks,
Computer science,
Calculus"
Virtual patrol: a new power conservation design for surveillance using sensor networks,"Surveillance has been a typical application of wireless sensor networks. To conduct surveillance of a given area in real life, one can use stationary watch towers, or can also use patrolling sentinels. Comparing them to solutions in sensor network surveillance, all current coverage based methods fall into the first category. In this paper, we propose and study patrol-based surveillance operations in sensor networks. Two patrol models are presented: the coverage-oriented patrol and the on-demand patrol. They achieve one of the following goals, respectively, i) to achieve surveillance of the entire field with low power drain but still bounded delay of detection; ii) to use an on-demand manner to achieve user initiated surveillance only to interested places. We propose the ""SENSTROL"" protocol to fulfill the patrol setup procedure for both models. With the implementation in the GloMoSim simulator, it is shown that patrol on arbitrary path can be set up in a network where each node follows a 98%-time-sleep-2%-time-wake power schedule.","Surveillance,
Protocols,
Event detection,
Wireless sensor networks,
Delay,
Target tracking,
Object detection,
Monitoring,
Chaos,
Computer science"
Fast component interaction for real-time systems,"Open real-time systems provide for co-hosting hard-, soft- and non-real-time applications. Microkernel-based designs in addition allow for these applications to be mutually protected. Thus, trusted servers can coexist next to untrusted applications. These systems place a heavy burden on the performance of the message-passing mechanism, especially when based on microkernel-like inter-process communication. In this paper we introduce capacity-reserve donation (in short Credo), a mechanism for the fast interaction of interdependent components, which is applicable to common real-time resource-access models. We implemented Credo by extending L4's message-passing mechanism to provide proper resource accounting and time-donation control, thereby preserving desired real-time properties. We were able to achieve priority inheritance and stack-based priority-ceiling resource sharing with virtually no overhead added to L4's message-passing implementation. By providing a. mechanism that does not impose performance penalties, while still guaranteeing correct real-time behaviour, Credo allows for the usage of microkernels in general-purpose but also in specialized systems.","Real time systems,
Protection,
Open systems,
Application software,
Communication system control,
Control systems,
Switches,
Computer science,
Mechanical factors,
Resource management"
An algebraic framework for merging incomplete and inconsistent views,"View merging, also called view integration, is a key problem in conceptual modeling. Large models are often constructed and accessed by manipulating individual views, but it is important to be able to consolidate a set of views to gain a unified perspective, to understand interactions between views, or to perform various types of end-to-end analysis. View merging is complicated by inconsistency of views. Once views are merged, it is useful to be able to trace the elements of the merged view back to their sources. In this paper, we propose a framework for merging incomplete and inconsistent graph-based views. We introduce a formalism, called annotated graphs, which incorporates a systematic annotation scheme capable of modeling incompleteness and inconsistency as well as providing a built-in mechanism for stakeholder traceability. We show how structure-preserving maps can capture the relationships between disparate views modeled as annotated graphs, and provide a general algorithm for merging views with arbitrary interconnections. We use the i* modeling language (Yu, 1997) as an example to demonstrate how our approach can be applied to existing graph-based modeling languages.","Merging,
Performance analysis,
Performance gain,
Computer science,
LAN interconnection"
Computational engine for development of complex cascaded models of signal and noise in X-ray imaging systems,"The detective quantum efficiency (DQE) is generally accepted as the primary metric of signal-to-noise performance in medical X-ray imaging systems. Simple theoretical models of the Wiener noise power spectrum (NPS) and DQE can be developed using a cascaded-systems approach to assess particular system designs and establish operational benchmarks. However, the cascaded approach is often impractical for the development of comprehensive models due to the complexity and extremely large number of algebraic terms that must be manipulated to describe signal and noise transfer. We have developed a computational engine that overcomes this limitation. Using a predefined library of elementary physical processes, complex models are assembled and input-output relationships established using a graphical interface. A novel recursive algorithm is described that allows the signal and noise analyses of models with arbitrary complexity including the use of multiple parallel cascades. Symbolic mathematics is used to develop analytic expressions for the NPS and DQE. The algorithm is validated by manual calculation for simple models and by Monte Carlo calculation for complex models. We believe our approach enables the use of complex cascaded models to design better detectors with improved image quality.","Engines,
X-ray imaging,
Power system modeling,
Medical signal detection,
X-ray detection,
X-ray detectors,
Biomedical imaging,
Libraries,
Assembly,
Signal analysis"
Implementing MPI-IO atomic mode without file system support,"The ROMIO implementation of the MPI-IO standard provides a portable infrastructure for use on top of any number of different underlying storage targets. These different targets vary widely in their capabilities, and in some cases, additional effort is needed within ROMIO to support the complete MPI-IO semantics. One aspect of the interface that can be problematic to implement is the MPI-IO atomic mode. This mode requires enforcing strict consistency semantics. For some file systems, native locks may be used to enforce these semantics, but not all file systems have lock support. In this work, we describe two algorithms for implementing efficient mutex locks using MPI-1 and MPI-2 capabilities. We then show how these algorithms may be used to implement a portable MPI-IO atomic mode for ROMIO. We evaluate the performance of these algorithms and show that they impose little additional overhead on the system. Because of the low-overhead nature of these algorithms, they are likely useful in a variety of situations where distributed locks are needed in the MPI-2 environment.","File systems,
Mathematics,
Computer science,
Laboratories,
Availability,
Packaging"
The effect of DNS delays on worm propagation in an IPv6 Internet,"It is a commonly held belief that IPv6 provides greater security against random-scanning worms by virtue of a very sparse address space. We show that an intelligent worm can exploit the directory and naming services necessary for the functioning of any network, and we model the behavior of such a worm in this paper. We explore via analysis and simulation the spread of such worms in an IPv6 Internet. Our results indicate that such a worm can exhibit propagation speeds comparable to an IPv4 random-scanning worm. We develop a detailed analytical model that reveals the relationship between network parameters and the spreading rate of the worm in an IPv6 world. We also develop a simulator based on our analytical model. Simulation results based on parameters chosen from real measurements and the current Internet indicate that an intelligent worm can spread surprising fast in an IPv6 world by using simple strategies. The performance of the worm depends heavily on these strategies, which in turn depend on how secure the directory and naming services of a network are. As a result, additional work is needed in developing detection and defense mechanisms against future worms, and our work identifies directory and naming services as the natural place to do it.","Delay effects,
Propagation delay,
Internet,
Computer worms,
Analytical models,
Computer science,
Cities and towns,
Computer security,
Intelligent networks,
Current measurement"
TARP: ticket-based address resolution protocol,"IP networks fundamentally rely on the address resolution protocol (ARP) for proper operation. Unfortunately, vulnerabilities in the ARP protocol enable a raft of IP-based impersonation, man-in-the-middle, or DoS attacks. Proposed countermeasures to these vulnerabilities have yet to simultaneously address backward compatibility and cost requirements. This paper introduces the ticket-based address resolution protocol (TARP). TARP implements security by distributing centrally issued secure MAC/IP address mapping attestations through existing ARP messages. We detail the TARP protocol and its implementation within the Linux operating system. Our experimental analysis shows that TARP improves the costs of implementing ARP security by as much as two orders of magnitude over existing protocols. We conclude by exploring a range of operational issues associated with deploying and administering ARP security","Protocols,
Costs,
Computer security,
IP networks,
Linux,
Operating systems,
Data security,
Space technology,
Laboratories,
Computer science"
A New Approach to Coin Recognition using Neural Pattern Analysis,"In business transactions, to enable computers to recognize coins and other different forms of currency has become an essential process. If the computers are able to perform such recognitions, monetary transactions becomes much easier in all forms of trade. Keeping all the necessary factors in mind we have created a system that could easily identify the numeral in the coins. To limit the scope of this problem, our research focuses on recognizing the exact numeral in a 1-rupee, 2-rupee and 5-rupee Indian coin. The proposed system focuses only on the numerals rather than the use of other images presented in the front and rear side of the coin. In the proposed approach coin images are acquired and numeral in the coins are extracted. Unlike other edge detection process, the coin edges are not sharp and gradually become dull by years of usage. Moreover, numeral edges are same as the background pixel value, which increases the complexity of edge detection process. Hence statistical color threshold method is suggested and implemented in the coin recognition process. After finding the Cartesian co-ordinates of numeral in the coins, the sub image of the numeral is extracted from the given coin image. This sub image is used for character recognition process. In this phase, rotation-invariant character recognition is carried out by multi channel Gabor filter and back propagation network methods. The overall collection contains 72 images in which skewed images are acquired in various angles of rotation varying from 30 degrees onwards.","Pattern recognition,
Pattern analysis,
Image edge detection,
Gabor filters,
Character recognition,
Educational institutions,
Color,
Telephony,
System testing,
Computer science"
Data redistribution and remote method invocation in parallel component architectures,"With the increasing availability of high-performance massively parallel computer systems, the prevalence of sophisticated scientific simulation has grown rapidly. The complexity of the scientific models being simulated has also evolved, leading to a variety of coupled multi-physics simulation codes. Such cooperating parallel programs require fundamentally new interaction capabilities, to efficiently exchange parallel data structures and collectively invoke methods across programs. So-called ""M/spl times/N"" research, as part of the Common Component Architecture (CCA) effort, addresses these special and challenging needs, to provide generalized interfaces and tools that support flexible parallel data redistribution and parallel remote method invocation. Using this technology, distinct simulation codes with disparate distributed data decompositions can work together to achieve greater scientific discoveries.","Component architectures,
Biological system modeling,
Computational modeling,
Computer science,
Mathematics,
Concurrent computing,
Object oriented modeling,
Laboratories,
Scientific computing,
Mathematical model"
A study on the use of 8-directional features for online handwritten Chinese character recognition,"This paper presents a study of using 8-directional features for online handwritten Chinese character recognition. Given an online handwritten character sample, a series of processing steps, including linear size normalization, adding imaginary strokes, nonlinear shape normalization, equidistance resampling, and smoothing, are performed to derive a 64/spl times/64 normalized online character sample. Then, 8-directional features are extracted from each online trajectory point, and 8 directional pattern images are generated accordingly, from which blurred directional features are extracted at 8/spl times/8 uniformly sampled locations using a filter derived from the Gaussian envelope of a Gabor filter. Finally, a 512-dimensional vector of raw features is formed. Extensive experiments on the task of recognizing 3755 level-1 Chinese characters in GB2312-80 standard are performed to compare and discern the best setting for several algorithmic choices and control parameters. The effectiveness of the studied approach is confirmed.","Character recognition,
Feature extraction,
Shape,
Gabor filters,
Pattern recognition,
Flowcharts,
Computer science,
Smoothing methods,
Image generation,
Text analysis"
"Avoiding the ""streetlight effect"": tracking by exploring likelihood modes","Classic methods for Bayesian inference effectively constrain search to lie within regions of significant probability of the temporal prior. This is efficient with an accurate dynamics model, but otherwise is prone to ignore significant peaks in the true posterior. A more accurate posterior estimate can be obtained by explicitly finding modes of the likelihood function and combining them with a weak temporal prior. In our approach, modes are found using efficient example-based matching followed by local refinement to find peaks and estimate peak bandwidth. By reweighting these peaks according to the temporal prior we obtain an estimate of the full posterior model. We show comparative results on real and synthetic images in a high degree of freedom articulated tracking task.","Humans,
Bayesian methods,
Bandwidth,
Search methods,
State-space methods,
Shape,
Computer science,
Artificial intelligence,
Laboratories,
Tracking"
Same-destination-intermediate grouping vs. end-to-end grouping for waveband switching in WDM mesh networks,"We investigate waveband switching (WBS) with different grouping strategies in wavelength-division multiplexing (WDM) mesh networks. End-to-end waveband switching (ETE-WBS) and same-destination-intermediate waveband switching (SD-IT-WBS) are analyzed and compared in terms of blocking probability and cost savings. First, an analytical model for ETE-WBS is proposed to determine the network blocking probability in a mesh network. For SD-IT-WBS, a simple waveband switching algorithm is presented. An analytical model to determine the network blocking probability is proposed for SD-IT-WBS based on the algorithm. The analytical results are validated by comparing with simulation results. Both results match well and show that ETE-WBS slightly outperforms SD-IT-WBS in terms of blocking probability. On the other hand, simulation results show that SD-IT-WBS outperforms ETE-WBS in terms of cost savings.","Intelligent networks,
Wavelength division multiplexing,
WDM networks,
Mesh networks,
Analytical models,
Costs,
Bandwidth,
Wavelength routing,
Computer science,
Spine"
Common gateway architecture for mobile ad-hoc networks,"Ad-hoc networks are by definition created on demand, without any infrastructure. On the other hand, they are often considered as an extension of the range of Internet access points, providing multihop wireless access to them. This paper tries to examine the situation where there exist several Internet access points in a single ad-hoc network. We present a common gateway architecture which allows to use multiple access points and send traffic using the closest one.","Ad hoc networks,
Routing protocols,
Internet,
IP networks,
Space technology,
Mobile communication,
Broadcasting,
Computer architecture,
Computer science,
Mathematics"
Architecture-adaptive routability-driven placement for FPGAs,"Current FPGA placement algorithms estimate the routability of a placement using architecture-specific metrics. The shortcoming of using architecture-specific routability estimates is limited adaptability. A placement algorithm that is targeted to a class of architecturally similar FPGAs may not be easily adapted to other architectures. The subject of this paper is the development of a routability-driven architecture adaptive FPGA placement algorithm called Independence. The core of the Independence algorithm is a simultaneous place-and-route approach that tightly couples a simulated annealing placement algorithm with an architecture adaptive FPGA router (Pathfinder). The results of our experiments demonstrate Independence's adaptability to island-style FPGAs, a hierarchical FPGA architecture (HSRA), and a coarse-grained reconfigurable architecture (RaPiD). The quality of the placements produced by Independence is within 1.2% of the quality of VPRs placements. 17% better than the placements produced by HSRA's placer, and within 0.7% of RaPiD's placer. Further, our results show that Independence produces clearly superior placements on routing-poor island-style FPGA architectures.","Field programmable gate arrays,
Routing,
Logic,
Simulated annealing,
Wires,
Computer science,
Reconfigurable architectures,
Hardware,
Impedance,
Costs"
Visual Exploration of Combined Architectural and Metric Information,"We present MetricView, a software visualization and exploration tool that combines traditional UML diagram visualization with metric visualization in an effective way. MetricView is very easy and natural to use for software architects and developers yet offers a powerful set of mechanisms that allow fine customization of the visualizations for getting specific insights. We discuss several visual and architectural design choices which turned out to be important in the construction of MetricView, and illustrate our approach with several results using real-life datasets","Unified modeling language,
Data visualization,
Software metrics,
Computer architecture,
Software tools,
Computer science,
Software design,
Instruments,
Guidelines,
Software systems"
Smoothly Blending Vector Fields for Global Robot Navigation,"We introduce a new algorithm for constructing smooth vector fields for global robot navigation. Given a ddimensional cell complex with each cell a convex polygon, our algorithm defines a number of local vector fields: one for each cell, and one for each face connecting two cells together. We smoothly blend these component vector fields together using bump functions; the precomputation of the component vector field and all queries can be done in linear time. The integral curves of the resulting globally-defined vector field are guaranteed to arrive at a neighborhood of the goal state in finite time. Except for a set of measure zero, the vector field is smooth. The resulting vector field can be used directly to control kinematic systems or can be used to develop dynamic control policies. We prove convergence for the integral curves of the vector fields produced by our algorithm and give examples illustrating the practical advantages of our technique.",
Practical Vision-Based Monte Carlo Localization on a Legged Robot,"Mobile robot localization, the ability of a robot to determine its global position and orientation, continues to be a major research focus in robotics. In most past cases, such localization has been studied on wheeled robots with range finding sensors such as sonar or lasers. In this paper, we consider the more challenging scenario of a legged robot localizing with a limited field-of-view camera as its primary sensory input. We begin with a baseline implementation adapted from the literature that provides a reasonable level of competence, but that exhibits some weaknesses in real-world tests. We propose a series of practical enhancements designed to improve the robotâ€™s sensory and actuator models that enable our robots to achieve a 50% improvement in localization accuracy over the baseline implementation. We go on to demonstrate how the accuracy improvement is even more dramatic when the robot is subjected to large unmodeled movements. These enhancements are each individually straightforward, but together they provide a roadmap for avoiding potential pitfalls when implementing Monte Carlo Localization on vision-based and/or legged robots.","Monte Carlo methods,
Legged locomotion,
Robot vision systems,
Robot sensing systems,
Mobile robots,
Sonar,
Filtering,
Robustness,
Motion estimation,
Computer vision"
GridMiner: a fundamental infrastructure for building intelligent grid systems,"The grid is considered as a crucial technology for the future knowledge-based economy and science. The Wisdom Grid project (a joint research effort of the University of Vienna and the Vienna University of Technology) aims, as the first research effort, to cover all aspects of the knowledge life cycle on the grid - from discovery in grid data repositories, to processing, sharing and finally reusing of knowledge as input for a new discovery. This paper first outlines the architecture of the Wisdom Grid infrastructure and then focuses on the kernel architecture component called Grid-Miner, which realizes the knowledge discovery, based on data mining and on-line analytical processing (OLAP) in grid repositories. A running GridMiner prototype is already available to the scientific community as an open service system.","Intelligent systems,
Intelligent structures,
Computer architecture,
Data mining,
Internet,
Grid computing,
Medical services,
Medical treatment,
Scientific computing,
Interactive systems"
Minimum-color path problems for reliability in mesh networks,"In this work, we consider the problem of maximizing the reliability of connections in mesh networks against failure scenarios in which multiple links may fail simultaneously. We consider the single-path connection problem as well as multiple-path (protected) connection problems. The problems are formulated as minimum-color path problems, where each link is associated with one or more colors, and each color corresponds to a given failure event Thus, when a certain color fails, all links which include that color will fail. In a single-path problem, by minimizing the number of colors on the path, the failure probability of the path can be minimized if all colors have the same probability of causing failures. In the case of two paths, where one path is a protection path, if all colors have the same probability of causing failures, the problem becomes that of finding two link-disjoint paths which either have a minimum total number of colors, or which have a minimum number of overlapping colors. By minimizing the total number of colors, the probability that a failure will occur on either of the paths is minimized. On the other hand, by minimizing the number of overlapping colors, the probability that a single failure event will cause both paths to fail simultaneously is minimized. The problems are proved to be NP-complete, and ILP formulations are developed. Heuristic algorithms are proposed for larger instances of the problems, and the heuristics are evaluated through simulation.","Intelligent networks,
Mesh networks,
Protection,
Optical fiber networks,
Computer network reliability,
Colon,
WDM networks,
Telecommunication network reliability,
Computer science,
Heuristic algorithms"
The Perception of Walking Speed in a Virtual Environment,"Studies of locomotion in virtual environments assume that correct geometric principles define the relationship between walking speed and environmental flow. However, we have observed that geometrically correct optic flow appears to be too slow during simulated locomotion on a treadmill. Experiment 1 documents the effect in a head-mounted display. Experiment 2 shows that the effect is eliminated when the gaze is directed downward or to the side, or when the walking speed is slow. Experiment 3 shows that the effect is unchanged by stride length. Experiment 4 verifies that the effect is not attributable to image jitter. The change in perceived speed from straight ahead to side or down gaze coincides with a shift from expanding optic flow to lamellar flow. Therefore, we hypothesize that lamellar flow is necessary for accurate speed perception, and that a limited field of view eliminates this cue during straight-ahead gaze.",
Modeling user search behavior,Web usage mining is a main research area in Web mining focused on learning about Web users and their interactions with Web sites. Main challenges in Web usage mining are the application of data mining techniques to Web data in an efficient way and the discovery of non trivial user behaviour patterns. In this paper we focus the attention on search engines analyzing query log data and showing several models about how users search and how users use search engine results.,"Search engines,
Computer science,
Application software,
Data mining,
Data analysis,
TV,
Navigation,
Feedback,
Buildings,
Information analysis"
Dynamic verification of sequential consistency,"In this paper, we develop the first feasibly implemental scheme for end-to-end dynamic verification of multithreaded memory systems. For multithreaded (including multiprocessor) memory systems, end-to-end correctness is defined by its memory consistency model. One such consistency model is sequential consistency (SC), which specifies that all loads and stores appear to execute in a total order that respects program order for each thread. Our design, DVSC-Indirect, performs dynamic verification of SC (DVSC) by dynamically verifying a set of sub-invariants that, when taken together, have been proven equivalent to SC. We evaluate DVSC-Indirect with full-system simulation and commercial workloads. Our results for multiprocessor systems with both directory and snooping cache coherence show that DVSC-Indirect detects all injected errors that affect system correctness (i.e., SC). We show that it uses only a small amount more bandwidth (less than 25%) than an unprotected system and thus can achieve comparable performance when provided with only modest additional link bandwidth.","Multiprocessor interconnection networks,
Bandwidth,
Coherence,
Bit error rate,
Hardware,
Control systems,
Computer science,
Computer architecture,
Protocols,
System recovery"
A multiple view approach to support common ground in distributed and synchronous geo-collaboration,"In this paper we investigate strategies to support knowledge sharing in distributed, synchronous collaboration. Our goal is to propose, justify, and assess a multiple view approach to support common ground in geo-collaboration within multi-role teams. We argue that a collaborative workspace, which includes multiple role-specific views coordinated with a team view, affords a clear separation between role-specific and shared data, enables the team to filter out role-specific details and share strategic knowledge, and allows serendipitous learning about knowledge and expertise within the team. We discuss some key issues that need to be addressed when designing multiple views as a collaborative visualization. We illustrate the design features of a geo-collaborative prototype that address these issues in the context of two collaborative scenarios. We finally describe a laboratory method for investigating how multi-role teams establish common ground while the amount of prior shared knowledge and the type of visualization are experimentally manipulated.","Ground support,
Collaborative work,
Collaboration,
Costs,
Knowledge management,
Computer science,
Filters,
Data visualization,
Prototypes,
Laboratories"
On the computation of the linear complexity and the k-error linear complexity of binary sequences with period a power of two,"The linear Games-Chan algorithm for computing the linear complexity c(s) of a binary sequence s of period lscr=2n requires the knowledge of the full sequence, while the quadratic Berlekamp-Massey algorithm requires knowledge of only 2c(s) terms. We show that we can modify the Games-Chan algorithm so that it computes the complexity in linear time knowing only 2c(s) terms. The algorithms of Stamp-Martin and Lauder-Paterson can also be modified, without loss of efficiency, to compute analogs of the k-error linear complexity for finite binary sequences viewed as initial segments of infinite sequences with period a power of two. We also develop an algorithm which, given a constant c and an infinite binary sequence s with period lscr=2n , computes the minimum number k of errors (and an associated error sequence) needed over a period of s for bringing the linear complexity of s below c. The algorithm has a time and space bit complexity of O(lscr). We apply our algorithm to decoding and encoding binary repeated-root cyclic codes of length lscr in linear, O(lscr), time and space. A previous decoding algorithm proposed by Lauder and Paterson has O(lscr(loglscr)2) complexity","Binary sequences,
Decoding,
Application software,
Cryptography,
Encoding,
Linear feedback shift registers,
Computer science,
Computer applications,
Game theory"
Random subspaces and subsampling for 2-D face recognition,"Random subspaces are a popular ensemble construction technique that improves the accuracy of weak classifiers. It has been shown, in different domains, that random subspaces combined with weak classifiers such as decision trees and nearest neighbor classifiers can provide an improvement in accuracy. In this paper, we apply the random subspace methodology to the 2-D face recognition task. The main goal of the paper is to see if the random subspace methodology can do as well, if not better, than the single classifier constructed on the tuned face space. We also propose the use of a validation set for tuning the face space, to avoid bias in the accuracy estimation. In addition, we also compare the random subspace methodology to an ensemble of subsamples of image data. This work shows that a random subspaces ensemble can outperform a well-tuned single classifier for a typical 2-D face recognition problem. The random subspaces approach has the added advantage of requiring less careful tweaking.","Face recognition,
Nearest neighbor searches,
Principal component analysis,
Filtering,
Testing,
Classification tree analysis,
Computer science,
Decision trees,
Pixel,
Pattern recognition"
Video assisted speech source separation,"We investigate the problem of integrating the complementary audio and visual modalities for speech separation. Rather than using independence criteria suggested in most blind source separation (BSS) systems, we use visual features from a video signal as additional information to optimize the unmixing matrix. We achieve this by using a statistical model characterizing the nonlinear coherence between audio and visual features as a separation criterion for both instantaneous and convolutive mixtures. We acquire the model by applying the Bayesian framework to the fused feature observations based on a training corpus. We point out several key existing challenges to the success of the system. Experimental results verify the proposed approach, which outperforms the audio only separation system in a noisy environment, and also provides a solution to the permutation problem.","Source separation,
Signal processing algorithms,
Humans,
Speech enhancement,
Working environment noise,
Computer science,
Blind source separation,
Coherence,
Bayesian methods,
Frequency domain analysis"
Using perceptual models to improve fidelity and provide invariance to valumetric scaling for quantization index modulation watermarking,"Quantization index modulation (QIM) is a computationally efficient method of watermarking with side information. This paper proposes two improvements to the original algorithm. First, the fixed quantization step size is replaced with an adaptive step size that is determined using Watson's perceptual model. Experimental results on a database of 1000 images illustrate significant improvements in both fidelity and robustness to additive white Gaussian noise. Second, modifying the Watson model such that it scales linearly with valumetric (amplitude) scaling, results in a QIM algorithm that is invariant to valumetric scaling. Experimental results compare this algorithm with both the original QIM and an adaptive QIM and demonstrate superior performance.","Quantization,
Watermarking,
Transmitters,
Computer science,
Educational institutions,
Image databases,
Noise robustness,
Additive white noise,
Channel capacity,
Communication channels"
Effort estimation of use cases for incremental large-scale software development,"This paper describes an industrial study of an effort estimation method based on use cases, the use case points method. The original method was adapted to incremental development and evaluated on a large industrial system with modification of software from the previous release. We modified the following elements of the original method: a) complexity assessment of actors and use cases, and b) the handling of non-functional requirements and team factors that may affect effort. For incremental development, we added two elements to the method: c) counting both all and the modified actors and transactions of use cases, and d) effort estimation for secondary changes of software not reflected in use cases. We finally extended the method to: e) cover all development effort in a very large project. The method was calibrated using data from one release and it produced an estimate for the successive release that was only 17% lower than the actual effort. The study identified factors affecting effort on large projects with incremental development. It also showed how these factors can be calibrated for a specific context and produce relatively accurate estimates.",
A new rectangular partitioning based lossless binary image compression scheme,"In this paper, we propose a lossless binary image compression scheme that can achieve high compression ratio via partitioning the black regions (one's) of the input image into rectangles. After partitioning, the top-left and the bottom-right vertices of each rectangle are identified and the coordinates of which are efficiently coded. Three different routines are used in this research. The proposed scheme is targeting images, which contain graphs and tables with solid gridlines in the background on the one hand. While on the other hand it is suitable for text images of languages where many characters have dots ""nuqta "" on them such as Urdu, Persian, and Arabic with big fonts. The proposed scheme has outperformed CCITT run length coding, modified READ, and REC significantly. Also it is faster and simpler to implement than the method reported in A. Quddus et al (1999)","Image coding,
Image reconstruction,
Shape,
Image processing,
Computer graphics,
Computer science,
Decoding,
Solids,
Data compression,
Insurance"
Electromagnetic Tracking for Image-Guided Abdominal Procedures: Overall System and Technical Issues,"This paper summarizes our work over the past several years in developing an image-guided system based on electromagnetic tracking for abdominal interventions. The paper begins with a review of computer-aided surgery and electromagnetic tracking. We next describe our image-guided system along with phantom and animal studies. We then present some technical issues in improving accuracy including pivot calibration, dynamic referencing, and registration using two 5 degree-of-freedom sensors. Electromagnetic tracking has great potential for assisting physicians in precision placement of instruments during minimally invasive interventions. However, the accuracy of these systems needs to be validated in the clinical environment and issues such as respiratory motion and organ deformation need to be addressed","Abdomen,
Biomedical optical imaging,
Optical sensors,
Instruments,
Computer displays,
Computer vision,
Target tracking,
Minimally invasive surgery,
Biomedical imaging,
Physics computing"
Scientific grid computing: the first generation,"The scientific user's computing demands are becoming increasingly complex and can benefit from distributed resources, but effectively marshalling these distributed systems often introduces new challenges. The authors describe how researchers can exploit existing distributed grid infrastructure to get meaningful scientific results.",
Deterministic extractors for affine sources over large fields,"An (n, k)-affine source over a finite field F is a random variable X = (X/sub 1/, ..., X/sub n/) /spl epsi/ F/sub n/, which is uniformly distributed over an (unknown) k-dimensional affine subspace of F/sub n/. We show how to (deterministically) extract practically all the randomness from affine sources, for any field of size larger than n/sup c/ (where c is a large enough constant). Our main results are as follows: 1. (For arbitrary k): For any n, k and any F of size larger than n/sub 20/, we give an explicit construction for a function D : F/sub n/ /spl rarr/ F/sub k-1/, such that for any (n, k)-affine source X over F, the distribution of D(X) is /spl epsiv/-close to uniform, where /spl epsiv/ is polynomially small in |F|. 2. (For k = 1): For any n and any F of size larger than n/sup c/, we give an explicit construction for a function D : F/sup n/ /spl rarr/ {0,1}/sup (1-/spl sigma/)log//sub 2/|F|, such that for any (n, 1)-affine source X over F, the distribution of D(X) is /spl epsiv/-close to uniform, where /spl epsiv/ is polynomially small in |F|. Here, /spl delta/ > 0 is an arbitrary small constant, and c is a constant depending on /spl delta/.","Galois fields,
Polynomials,
Radio access networks,
Random variables,
Computer science"
A tool-supported approach to testing UML design models,"For model driven development approaches to succeed, there is a need for model validation techniques. This paper presents an approach to testing designs described by UML class diagrams, interaction diagrams, and activity diagrams. A UML design model under test is transformed into an executable form. Test infrastructure is added to the executable form to carry out tests. During testing, object configurations are created, modified and observed. In this paper, we identify the structural and behavioral characteristics that need to be observed during testing. We describe a prototype tool that (1) transforms UML design models into executable forms with test infrastructure, (2) executes tests, and (3) reports failures.","Unified modeling language,
Object oriented modeling,
Prototypes,
Software testing,
Computer science,
Software systems,
Software prototyping,
Inspection,
Manuals,
Computer architecture"
Kernel relevant component analysis for distance metric learning,"Defining a good distance measure between patterns is of crucial importance in many classification and clustering algorithms. Recently, relevant component analysis (RCA) is proposed which offers a simple yet powerful method to learn this distance metric. However, it is confined to linear transforms in the input space. In this paper, we show that RCA can also be kernelized, which then results in significant improvements when nonlinearities are needed. Moreover, it becomes applicable to distance metric learning for structured objects that have no natural vectorial representation. Besides, it can be used in an incremental setting. Performance of this kernel method is evaluated on both toy and real-world data sets with encouraging results.","Kernel,
Classification algorithms,
Clustering algorithms,
Euclidean distance,
Surveillance,
Principal component analysis,
Pattern analysis,
Algorithm design and analysis,
Computer science,
Nearest neighbor searches"
A criticality analysis of clustering in superscalar processors,"Clustered machines partition hardware resources to circumvent the cycle time penalties incurred by large, monolithic structures. This partitioning introduces a long inter-cluster forwarding latency and the potential for load imbalance, both of which degrade IPC and thus counter the cycle time benefits of clustering. We show that program dataflow can be mapped to clustered machines so as to achieve an IPC rivaling that of an equivalent monolithic machine. That is, the IPC penalties observed by extant schemes are largely an artifact of instruction steering and scheduling policies. Using critical path analysis, we investigate and uncover the main causes for this performance loss. By way of code samples, we illustrate those causes and propose three policies for mitigating them. First, we introduce a new metric, likelihood of criticality, and show how it can halve the performance lost to contention-induced stalls. Second, we develop a stall-over-steer policy that addresses performance lost to inter-cluster forwarding delay. Finally, we show that a proactive load-balancing policy is necessary to improve the distribution of ready instructions among the clusters. Together, these three policies yield performance on 2-, 4- and 8-cluster implementations of an 8-wide machine that is within 2, 4, and 6%, respectively, of the monolithic equivalent","Delay,
Out of order,
Degradation,
Parallel processing,
Clocks,
Aggregates,
Processor scheduling,
Computer science,
Hardware,
Counting circuits"
Resolution-effective diameters for asymmetric-knife-edge pinhole collimators,"The effects of penetration are included in the formulas for the prediction of the resolution of pinhole collimators through the use of effective diameters. Expressions of the resolution-effective diameter for pinholes with a double-knife-edge (DKE) profile are available in the literature. In this paper the expressions applicable to asymmetric-knife-edge (AKE) profiles, which include the important case of the single-knife-edge (SKE), are presented. Results indicate that the simplest methods that are still accurate in the calculation of DKE effective diameters do not produce in general formulas with similar accuracy for AKE profiles, due to increased susceptibility to penetration. Especially at high energy (365 keV), for the SKE case more advanced formulas are necessary and were, therefore, derived.","Single photon emission computed tomography,
Apertures,
Slabs,
Radiology,
Image resolution,
High-resolution imaging,
Optical collimators,
Animal structures,
Humans,
Geometry"
On demand multicast routing with unidirectional links,"In wireless ad-hoc networks, unidirectional links occur for several reasons: nonuniform transmit power, nonuniform background noise, and external interference. Several researchers have addressed unidirectional links and the associated unidirectional routing problem. The main focus has been so far on ""unicast"" routing; the consensus is that unidirectional links should be detected and avoided. In this paper, we consider the multicast case and derive a different conclusion: namely, it pays to exploit unidirectional links rather then avoid them. To prove the point, we select a popular ad hoc multicast protocol, on-demand multicast routing protocol (ODMRP) and introduce a slightly modified version, ODMRP-ASYM, that can handle unidirectional links. Specifically, ODMRP-ASYM reroutes the join reply packet when a unidirectional link is detected on the join query path. The option is invoked only when a unidirectional link is detected. The main advantages are: control overhead comparable with ODMRP even in highly asymmetric topologies; virtually no performance degradation in the presence of unidirectional links (while ODMRP typically suffers up to 15% drop in delivery performance); 2-connectivity maintenance even if no bidirectional path exists between sender and receiver (in this case, unidirectional link avoidance strategies fail). Extensive simulation experiments demonstrate ODMRP-ASYM robustness to unidirectional links and superiority over conventional ODMRP.","Ad hoc networks,
Routing protocols,
Background noise,
Multicast protocols,
Unicast,
Interference,
Network topology,
Wireless sensor networks,
Computer science,
Degradation"
Leakage minimization of nano-scale circuits in the presence of systematic and random variations,This paper presents a novel gate sizing methodology to minimize the leakage power in the presence of process variations. The leakage and delay are modeled as posynomials functions to formulate a geometric programming problem. The existing statistical leakage model of Rao et al. (2004) is extended to include the variations in gate sizes as well as systematic variations. We propose techniques to efficiently evaluate constraints on the /spl alpha/-percentile of the path delays without enumerating the paths in the circuit. The complexity of evaluating the objective function is O(|N|/sup 2/) and that of evaluating the delay constraints is O(|N| + |E|) for a circuit with |N| gates and |E| wires. The optimization problem is then solved using a convex optimization algorithm that gives an exact solution.,"Minimization,
Delay,
Computer science,
Power engineering and energy,
Power system modeling,
Functional programming,
Algorithm design and analysis,
Circuit optimization,
Lenses,
Solid modeling"
IP traceback based on packet marking and logging,"Two main kinds of IP traceback techniques have been proposed in two dimensions: packet marking and packet logging. IP traceback based on packet marking is often referred to as probabilistic packet marking (PPM) approach where packets are probabilistically marked with partial path information as they are forwarded by routers. This approach incurs little overhead at routers. But due to its probabilistic nature, it can only determine the source of the traffic composed of a number of packets. IP traceback based on packet logging is often referred to as hash-based approach where routers compute and store digest for each forwarded packet. This approach can trace an individual packet to its source. However, the storage space requirement for packet digests and the access time requirement for recording packets commensurate with their arriving rate are prohibitive at routers with high speed links. We propose an IP traceback approach based on both packet marking and packet logging. Compared with the PPM approach, our approach is able to track individual packets. Compared with the hash-based approach, our approach incurs less storage overhead and less access time overhead at routers. Specifically, the storage overhead is reduced to roughly one half, and the access time requirement is decreased by a factor of the number of neighbor routers.","Computer science,
Telecommunication traffic,
Chaos,
Computer crime,
Filtering,
Data structures,
Filters,
Floods"
Routing layer support for service discovery in mobile ad hoc networks,"Service discovery in mobile ad hoc networks is an essential process in order for these networks to be self-configurable. In this paper we argue that service discovery can be greatly enhanced in terms of efficiency (regarding service discoverability and energy consumption), by piggybacking service information in routing layer messages. Thus, a node requesting a service in addition to discovering that service, it is simultaneously informed of the route to the service provider. We extend the zone routing protocol in order to encapsulate service information in its routing messages and through extensive simulations we prove the superiority of our routing layer-enhanced service discovery scheme against an application layer-based flooding scheme.","Intelligent networks,
Mobile ad hoc networks,
Mobile computing,
Routing protocols,
Availability,
Computer architecture,
Batteries,
Conferences,
Computer science education,
Educational programs"
Performance evaluation of view-oriented parallel programming,This paper evaluates the performance of a novel view-oriented parallel programming style for parallel programming on cluster computers. View-oriented parallel programming is based on distributed shared memory which is friendly and easy for programmers to use. It requires the programmer to divide shared data into views according to the memory access pattern of the parallel algorithm. One of the advantages of this programming style is that it offers the performance potential for the underlying distributed shared memory system to optimize consistency maintenance. Also it allows the programmer to participate in performance optimization of a program through wise partitioning of the shared data into views. Experimental results demonstrate a significant performance gain of the programs based on the view-oriented parallel programming style.,"Parallel programming,
Programming profession,
Message passing,
Concurrent computing,
Parallel algorithms,
Computer science,
Information science,
Optimization,
Performance gain,
Distributed computing"
An efficient multi-server password authenticated key agreement scheme using smart cards with access control,"Due to the rapid development of science and techniques, people can remotely access computers over the networks. Thus, user authentication and key agreement become more and more important to ensure the legality of the user and the security of later communications, respectively. Because the number of servers providing the facilities for the user is usually more than one, the concept of multi-server protocols is introduced. On the Internet, each server usually provides various services, and each service provided by the server may not be accessed by the user. Hence, access control is required in the multi-service environment. In 2004, Juang proposed a multi-server authentication scheme with key agreement. However, access control is not taken into account in Juang's proposed scheme, so we propose an efficient multi-server password authenticated key agreement scheme with access control in this article.","Smart cards,
Access control,
Authentication,
Information security,
Web server,
Network servers,
Access protocols,
Data security,
Computer science,
Computer networks"
"High Performance Circular TE01, Mode Converter","Summary form only given. The development and experimental test of a Ka-band TE01 mode converter are presented. A wave in TE10 rectangular waveguide mode is efficiently converted into the TE01 circular waveguide mode. This converter is composed of a power-dividing section and a mode-converting section. The field pattern and the working principle of each section are analyzed and discussed. A prototype has been built and tested. Back-to-back transmission measurements show excellent agreement with computer simulations. The measured optimum transmissions are 97% with 1-dB bandwidth of 5.8 GHz and 3-dB bandwidth of 7 GHz. High mode purity is predicted in theory and demonstrated in experiment. The field pattern of the circular TE01 mode is directly displayed on a temperature sensitive liquid crystal sheet, where the electric field strength can be discerned from the color spectrum. In addition to three just mentioned advantages, high converting efficiency, high mode purity, and broad bandwidth, this converter also features easy constructions and compact size",
Improving power save protocols using carrier sensing for dynamic advertisement windows,"Energy efficient protocols are important in ad hoc networks since battery life for wireless devices is limited. The IEEE 802.11 protocol specifies a simple power save mechanism (PSM) to conserve energy. Packets are advertised for a fixed length of time, known as an advertisement window, at epochs known as beacon intervals. However, the protocol needlessly wastes energy when traffic is relatively light in a network. In this paper, we address this problem by proposing the use of carrier sensing to dynamically adjust the size of the advertisement windows. The adjustment is based on the amount of traffic that needs to be advertised in the current window as opposed to the static window size used by 802.11 PSM. Carrier sensing is used for two different aspects of our protocol. First, carrier sensing is used as an energy efficient method to provide a binary signal which lets neighbors know if a node intends to advertise any packets in the upcoming window. Second, carrier sensing is used as a mechanism for nodes to keep track of whether their neighbors have already stopped listening for advertisements and possibly returned to sleep. Using the ns-2 simulator we show that our techniques can significantly reduce the energy consumption of 802.11 PSM while only slightly increasing latency","Energy consumption,
Wireless sensor networks,
Batteries,
Delay,
Energy efficiency,
Ad hoc networks,
Telecommunication traffic,
Computer science,
Wireless application protocol,
Portable computers"
Neural networks approach to clustering of activity in fMRI data,"Clusters of correlated activity in functional magnetic resonance imaging data can identify regions of interest and indicate interacting brain areas. Because the extraction of clusters is computationally complex, we apply an approximative method which is based on artificial neural networks. It allows one to find clusters of various degrees of connectivity ranging between the two extreme cases of cliques and connectivity components. We propose a criterion which allows to evaluate the relevance of such structures based on the robustness with respect to parameter variations. Exploiting the intracluster correlations, we can show that regions of substantial correlation with an external stimulus can be unambiguously separated from other activity.","Neural networks,
Intelligent networks,
Data mining,
Magnetic resonance imaging,
Brain,
Robustness,
Signal processing,
Computer networks,
Artificial neural networks,
Hopfield neural networks"
Histogram equalization using neighborhood metrics,"We present a refinement of histogram equalization which uses both global and local information to remap the image grey levels. Local image properties, which we generally call neighborhood metrics, are used to subdivide histogram bins that would be otherwise indivisible using classical histogram equalization (HE). Choice of the metric influences how the bins are subdivided, affording the opportunity for additional contrast enhancement. We present experimental results for two specific neighborhood metrics and compare the results to classical histogram equalization and local histogram equalization (LHE). We find that our methods can provide an improvement in contrast enhancement versus HE, while avoiding undesirable over-enhancement that can occur with LHE and other methods. Moreover, the improvement over HE is achieved with only a small increase in computation time.","Histograms,
Helium,
Pixel,
Computer science,
Noise reduction,
Image quality,
Computer vision,
Robot vision systems,
Level set,
Windows"
ECG analysis based on PCA and Support Vector Machines,"Cardiovascular diseases is one of the main courses of death around the world. Electrocardiogram (ECG) supervising is the most important and efficient way of preventing heart attacks. Machine monitoring and analysis of ECG is becoming a major topic of the modern medical research. In this paper, we propose a system to detect cardiac arrhythmia using the ECG data form MIT-BIH database as a reference. The purpose of this paper is to develop an algorithm for recognizing and classifying normal beat, left bundle branch block beat, right bundle branch block beat and premature ventricular contraction (PVC). In order to do so, we extract more than 6000 signals from the original database, each signal representing a single and complete heart beat. We extract the principal characteristics of the signal by means of the principal component analysis (PCA) technique. Support vector machine (SVM) has a major predominance over other classification methods in complicated problems. SVM method is applied to classify the ECG data into the 4 categories of heart diseases. Base on this idea, we achieved better results in comparison with other pattern classification method from our computer simulations","Electrocardiography,
Principal component analysis,
Support vector machines,
Support vector machine classification,
Databases,
Data mining,
Cardiovascular diseases,
Cardiac arrest,
Biomedical monitoring,
Condition monitoring"
Exploiting dynamic querying like flooding techniques in unstructured peer-to-peer networks,"In unstructured peer-to-peer networks, controlled flooding aims at locating an item at the minimum message cost. Dynamic querying is a new controlled flooding technique. While it is implemented in some peer-to-peer networks, little is known about its undesirable behavior and little is known about its general usefulness in unstructured peer-to-peer networks. This paper describes the first evaluation and analysis of such techniques, and proposes novel techniques to improve them. We make three contributions. First, we find the current dynamic querying design is flawed. Although it is advantageous over the expanding ring algorithm in terms of search cost, it is much less attractive in terms of peer perceived latency, and its strict constraints on network connectivity prevent it from being widely adopted. Second, we propose an enhanced flooding technique which requires the search cost close to the minimum, reduces the search latency by more than four times, and loosens the constraints on the network connectivity. Thus, we make such techniques useful for the general unstructured peer-to-peer networks. Third, we show that our proposal requires only minor modifications to the existing search mechanisms and can be incrementally deployed in peer-to-peer networks.","Intelligent networks,
Peer to peer computing,
Floods,
Costs,
Delay,
Telecommunication traffic,
Iterative algorithms,
Computer science,
Proposals,
IP networks"
Robust Contrast Invariant Stereo Correspondence,"A stereo pair of cameras attached to a robot will inevitably yield images with different contrast. Even if we assume that the camera hardware is identical, due to slightly different points of view, the amount of light entering the two cameras is also different, causing dynamically adjusted internal parameters such as aperture, exposure and gain to be different. Due to the difficulty of obtaining and maintaining precise intensity or color calibration between the two cameras, contrast invariance becomes an extremely desirable property of stereo correspondence algorithms. The problem of achieving point correspondence between a stereo pair of images is often addressed by using the intensity or color differences as a local matching metric, which is sensitive to contrast changes. We present an algorithm for contrast invariant stereo matching which relies on multiple spatial frequency channels for local matching. A fast global framework uses the local matching to compute the correspondences and find the occlusions. We demonstrate that the use of multiple frequency channels allows the algorithm to yield good results even in the presence of significant amounts of noise.","Robustness,
Cameras,
Robot vision systems,
Frequency,
Robotics and automation,
Educational institutions,
Apertures,
Calibration,
Computer science,
Hardware"
Burst cloning: a proactive scheme to reduce data loss in optical burst-switched networks,"In this paper, we propose a novel proactive scheme, burst cloning, to reduce data loss due to burst contention in optical burst-switched (OBS) networks. The idea is to replicate a burst and send duplicated copies of the burst through the network simultaneously. If the original burst is lost, the cloned burst way still be able to reach the destination. Primary design issues in burst cloning are to select the optimal nodes at which to do cloning and to prevent cloned bursts from contending for resources with original bursts. An analytical model is developed to evaluate the proposed scheme. The model is verified through extensive simulations. We observe that burst cloning could significantly reduce data loss in OBS networks.","Cloning,
Optical losses,
Intelligent networks,
Optical fiber networks,
Analytical models,
Optical buffering,
Optical wavelength conversion,
Computer science,
Computer networks,
Optical computing"
New denoising scheme for magnetic resonance spectroscopy signals,"A new scheme for denoising magnetic resonance spectroscopy (MRS) signals is presented. This scheme is based on projecting noisy MRS signals in different domains, consecutively, and performing noise filtering operations in these domains. The domains are chosen such that the noise portion, which is inseparable from the desired signal in one domain, is separable in the other. A set of stable, linear, time-frequency (SLTF) transforms with different resolutions was selected for these projections as an example. Scheme evaluation was performed using extensive MRS signals with various noise levels. Compared with one domain denoising, it was observed that the proposed scheme gives superior results that compensate for the excess computational requirements. The proposed scheme supersedes also the wavelet packet denoising schemes.","Noise reduction,
Magnetic resonance,
Spectroscopy,
Magnetic noise,
Magnetic separation,
Filtering,
Time frequency analysis,
Signal resolution,
Performance evaluation,
Noise level"
Call stack coverage for test suite reduction,"Test suite reduction is an important test maintenance activity that attempts to reduce the size of a test suite with respect to some criteria. Emerging trends in software development such as component reuse, multi-language implementations, and stringent performance requirements present new challenges for existing reduction techniques that may limit their applicability. A test suite reduction technique that is not affected by these challenges is presented; it is based on dynamically generated language-independent information that can be collected with little run-time overhead. Specifically, test cases from the suite being reduced are executed on the application under test and the call stacks produced during execution are recorded. These call stacks are then used as a coverage requirement in a test suite reduction algorithm. Results of experiments on test suites for the space antenna-steering application show significant reduction in test suite size at the cost of a moderate loss in fault detection effectiveness.","Instruments,
Software testing,
Programming,
Runtime,
Fault detection,
Quality of service,
System testing,
Computer science,
Educational institutions,
Costs"
Ad-Hoc Routing Protocol Avoiding Route Breaks Based on AODV,"Many routing protocols in mobile ad-hoc networks have been developed by many researchers. One of ad-hoc routing protocol types is the on-demand routing that establishes a route to a destination node only when required. But it is necessary to re-establish a new route when its route breaks down. However, most of on-demand routing protocols re-establish a new route after a route break. In this paper, we propose a new route maintenance algorithm to avoid route breaks because each intermediate node on an active route detects a danger of a link break to an upstream node and re-establishes a new route before a route break. We propose this algorithm based on AODV (Ad-hoc On-demand Distance Vector routing protocol) that is one of the routing protocols under study by the IETF MANET Working Group. Then, we evaluate the performance of our proposal by computer simulations.","Routing protocols,
Ad hoc networks,
Proposals,
Wireless networks,
Mobile ad hoc networks,
Computer simulation,
Wireless LAN,
Network topology,
Delay,
Batteries"
A three-parameter mechanical property reconstruction method for MR-based elastic property imaging,"A reconstruction process featuring full parameterization of the three dimensional, time-harmonic equations of linear elasticity is developed and reconstructed property images are presented from simulation-based investigation. While interesting in its own right through the potential for increased adaptability of these reconstructive elastic imaging techniques, this study also presents a set of analysis tools used to study the poor convergence behavior found in the case of tissue like conditions (i.e. nearly incompressible materials). The choice of elastic properties for imaging in elastography research remains an open question at this point; the use of the stability and sensitivity-based analytical methods described here will help to predict and understand the value and reliability of different parameterizations of elasticity imaging. Additionally, though results indicate significant work needs to be done to achieve effective multiparameter reconstructive imaging, the methods detailed here offer the promise of increased flexibility and sophistication in elastographic imaging techniques.","Mechanical factors,
Reconstruction algorithms,
Elasticity,
Image reconstruction,
Equations,
Image analysis,
Cancer,
Iterative algorithms,
Convergence,
Biological materials"
Dynamic power management using on demand paging for networked embedded systems,"The power consumption of the network interface plays a major role in determining the total operating lifetime of wireless networked embedded systems. In case of on-demand paging, a low power secondary radio is used to wake up the higher power radio, allowing the latter to sleep for longer periods of time. In this paper we present use of Bluetooth radios to serve as a paging channel for the 802.11b wireless LAN. We have implemented an on-demand paging scheme on an infrastructure based WLAN consisting of iPAQ PDAs equipped with Bluetooth radios and Cisco Aironet wireless networking cards. Our results show power saving ranging from 23% to 48% over the present 802.11b standard operating modes with negligible impact on performance.","Energy management,
Power system management,
Embedded system,
Energy consumption,
Wireless LAN,
Computer science,
Bluetooth,
Personal digital assistants,
Protocols,
Receivers"
Improved min-sum decoding algorithms for irregular LDPC codes,"In this paper, we apply two improved min-sum algorithms, the normalized and offset min-sum algorithms, to the decoding of irregular LDPC codes. We show that the behavior of the two algorithms in decoding irregular LDPC codes is different from that in decoding regular LDPC codes, due to the existence of bit nodes of degree two. We analyze and give explanations to the difference, and propose approaches to improve the performance of the two algorithms","Decoding,
Parity check codes,
Inference algorithms,
Degradation,
Performance analysis,
Algorithm design and analysis,
Signal to noise ratio,
AWGN channels,
Quantization,
Computer science"
Polyglot synthesis using a mixture of monolingual corpora,"The paper proposes a new approach to multilingual synthesis based on an HMM synthesis technique. The idea consists of combining data from different monolingual speakers in different languages to create a single polyglot average voice. This average voice is then transformed into any real speaker's voice of one of these languages. The speech synthesized in this way has the same intelligibility and retains the same individuality for all the languages mixed to create the average voice, regardless of the target speaker's own language.","Speech synthesis,
Natural languages,
Hidden Markov models,
Synthesizers,
Computer science,
Business communication,
Globalization,
Costs,
Disk recording,
Speech recognition"
An ant algorithm hyperheuristic for the project presentation scheduling problem,"Ant algorithms have generated significant research interest within the search/optimization community in recent years. Hyperheuristic research is concerned with the development of ""heuristics to choose heuristics"" in an attempt to raise the level of generality at which optimization systems can operate. In this paper the two are brought together. An investigation of the ant algorithm as a hyperheuristic is presented and discussed. The results are evaluated against other hyperheuristic methods, when applied to a real world scheduling problem.","Scheduling algorithm,
Genetic algorithms,
Processor scheduling,
Space exploration,
Biological cells,
Technology planning,
Computer science,
Information technology,
Optimization methods,
Decision support systems"
Study on network traffic prediction techniques,"We briefly describe a number of traffic predictors (such as ARIMA, FARIMA, ANN and wavelet-based predictors) and analyze their computational complexity. We compare their performance with MSE, NMSE and computational complexity by simulating the predictors on four wireless network traffic traces and decide the most suitable network traffic predictor based on acceptable performance and accuracy.","Telecommunication traffic,
Traffic control,
Communication system traffic control,
Predictive models,
Artificial neural networks,
Measurement,
Computational complexity,
Channel allocation,
Neural networks,
Computer science"
On failure recoverability of client-server applications in mobile wireless environments,"Analytical results for the Cdf of the failure recovery time for client-server applications in mobile wireless environments characterized by logging, and mobility handoff strategies for facilitating failure recovery are reported in the paper. The results can be applied to determine if a mobile application can satisfy its recoverability requirement upon a mobile host failure when operating under a set of parameter values characterizing the mobile application, the underlying client-server environment, and the logging & mobility handoff strategies adopted by the mobile application. Model parameters which affect the shape of the failure recovery time Cdf for two mobility handoff strategies, namely, Eager and Lazy, are identified, and their effects are analyzed, with numerical data and result interpretations given. A tradeoff analysis between the cost invested by these two mobility handoff strategies for maintaining the logging and checkpoint information before failure versus the return of investment in terms of improved failure recoverability is given, and the best checkpoint interval period that would yield the best return of investment for the eager mobility handoff strategy over the lazy strategy is identified.","Wireless networks,
Protocols,
Failure analysis,
Investments,
Computer science,
Shape,
Data analysis,
Information analysis,
Costs,
Distribution functions"
An ECA-P policy-based framework for managing ubiquitous computing environments,"Ubiquitous computing environments feature massively distributed systems containing a large number of devices, services and applications that help end-users perform various kinds of tasks. One way by which administrators and end-users can manage these environments is through the use of policies. In particular, obligation policies are used to specify what actions must or must not be performed by the environment on the occurrence of certain events. Obligation policies are often specified as event-condition-action (ECA) rules. However an important problem in ubiquitous computing systems is that different users and administrators may have conflicting policies for managing the system. Hence, a key challenge in policy-based management is detecting and resolving conflicts between multiple policy rules that get activated by a single event. Existing approaches are limited in power and scope mainly because they do not have semantic information about the effects of policy actions; hence, they cannot infer that two actions may conflict unless they are explicitly stated to be conflicting. In this paper, we propose an extended model of ECA called event-condition-action-post-condition (ECA-P), where developers and administrators can annotate actions with their effects. The ECA-P model allows inferring that actions may conflict based on conflicting post-conditions. Detected conflicts are resolved using meta-rules that specify preferred system states. The ECA-P framework also detects failures in policy execution by using postconditions to verify successful completion of policy actions. We present the details of the framework.","Environmental management,
Ubiquitous computing,
Event detection,
Power system management,
Computer science,
Application software,
Power system modeling,
Mobile computing,
Computer network management,
Content management"
Citron: a context information acquisition framework for personal devices,This paper describes a context information acquisition framework for a personal device that equips a variety of sensors. The framework captures context information about a user and his/her surrounding environment; and the information is used to adapt the behavior of applications running on the personal device. Our framework adopts the blackboard architecture to execute multiple analysis modules that analyze signals from respective sensors. Respective modules implement different algorithms to complement each other's results to retrieve more accurate and higher abstract context information.,"Sensor phenomena and characterization,
Algorithm design and analysis,
Signal analysis,
Context,
Intelligent sensors,
Prototypes,
Temperature sensors,
Computer science,
Computer architecture,
Information retrieval"
"Who, what, and how: a survey of informal and professional Web developers","We describe a survey of Web developers in which we collected over 300 responses from individuals with widely varying levels of experience and training. This survey continues our studies of informal Web developers, loosely defined as those who develop Web sites but have not been trained as programmers. They are a growing segment of end user programmers, but very little is known about them, and this survey was aimed at helping to characterize this population. In this paper we report on survey questions probing Web development projects, tool use, development process, reuse, and learning and collaboration. Throughout the discussion we compare the responses of developers who self-identify as programmers with those who do not. We use these comparisons as a basis for discussion of tools that might assist nonprogrammers in Web development.","Programming profession,
Collaborative tools,
Information systems,
Web sites,
Application software,
Computer science,
Recruitment,
IEEE activities,
Probes"
Benchmarking Wireless LAN Location Systems Wireless LAN Location Systems,"Wireless LANs not only provide an affective means of communication, but also allow to extract information about the location of mobile stations. Numerous wireless LAN location systems have been proposed in the past, yet it is difficult to compare the performance of different systems, since the conditions under which these systems are evaluated differ considerably. Hence, the accuracy information presented in literature varies widely, even for conceptually identical systems. This paper proposes to evaluate the performance of location systems in standardised test environments. To this end, existing location systems are discussed to assess their information requirements and their reported accuracies. Subsequently, the requirements for benchmarks in the context of Wireless LAN location systems are established. To conclude, a publicly available benchmark is presented against which different Wireless LAN location systems can be compared","Wireless LAN,
Benchmark testing,
Computer science,
System testing,
Business,
Wireless communication,
Context-aware services,
Context,
Global Positioning System,
Mobile communication"
Attacking control overhead to improve synthesised asynchronous circuit performance,"The development of robust synthesis techniques and tools is important if asynchronous design is to gain more widespread acceptance. Handshake circuits are a method of constructing asynchronous circuits from a set of modular components connected by handshake channels. They offer a level of abstraction above a particular target technology or implementation style. The Balsa system employs the handshake circuit approach and has demonstrated that it can be used to rapidly generate large, robust circuits. This speed and flexibility is currently achieved at the cost of performance. This paper examines the problem of control overhead in handshake circuits and proposes new handshake component specifications and implementations that significantly reduce this overhead. These changes are incorporated into the Balsa synthesis system and are shown to produce a doubling of the performance of a 32-bit processor without making any changes to the original description.","Circuit synthesis,
Asynchronous circuits,
Robustness,
Control system synthesis,
Smart cards,
Computer science,
Performance gain,
Costs,
Prototypes,
Network synthesis"
A framework for integrating existing and novel intelligent transportation systems,"Efficient use and re-use of traffic data depends on an ITS architecture that enables information sharing across a wide variety of intelligent transportation systems and applications. Existing ITS architectures, such as KAREN or the national ITS architecture, can be used to develop systems within a given framework thereby facilitating such inter-system integration. However, these architectures typically include assumptions regarding the overall organization of system functionality that prohibit integration of previously deployed systems without major reengineering. This paper presents a framework for an ITS architecture that has been designed for integrating novel as well as existing intelligent transportation systems and applications. The iTransIT framework supports a number of possible systems interaction paradigms and proposes a layered data model to facilitate data exchange between systems with diverse service requirements and functional organizations. These data layers are defined within a common context model, may be distributed across multiple systems, and exploit the overlapping temporal and spatial aspects of information generated and used by both legacy and future systems.",
Learning sequential constraints of tasks from user demonstrations,"Learning from human demonstration is likely to be one of the key features for service robots in household domains if they are to be accepted by humans. To be of most benefit possible to its user, the robot should go beyond simply imitating a user's demonstration but try to build task knowledge that is as general and flexible as possible. One way to achieve this is equipping the robot with the ability to reason about the task knowledge he has already acquired in order to refine, generalize and complete it. A system to record and interpret manipulation task demonstrations is presented in this paper. As a representation for the sequential constraints a valid task execution must obey, task precedence graphs (TPG's) are introduced. Means of reasoning on a task's underlying TPG are proposed and evaluated within two tasks from the household domain","Humanoid robots,
Humans,
Service robots,
Robot programming,
Cognitive robotics,
Computer science,
Machine learning,
Collaborative work,
Contracts,
Monitoring"
Hand tracking with Flocks of Features,"Tracking hands in live video is a challenging task: the hand appearance can change too rapidly for appearance-based trackers to work, and color-based trackers (that do not rely on geometry) have to make limiting assumptions about the background color. This article shows the results of hand tracking with ""Flocks of Features"", a tracking method that combines motion cues and a learned foreground color distribution to achieve fast and robust 2D tracking of highly articulated objects. Many independent image artifacts are tracked from one frame to the next, adhering only to local constraints. This concept is borrowed from nature since these tracks mimic the flight of flocking birds - exhibiting local individualism and variability while maintaining a clustered entirety. Hand tracking has important applications for interaction with wearable computers, for intuitive manipulation of virtual objects, for detection of activity signatures, and much more. Tracking with Flocks of Features is not limited to hands - any articulated or appearance-changing object can benefit from this multi-cue tracking method.","Target tracking,
Computer science,
Computational geometry,
Birds,
Colored noise,
Color,
Noise robustness,
Background noise,
Cameras,
Testing"
An effective real-time mosaicing algorithm apt to detect motion through background subtraction using a PTZ camera,"Nowadays, many visual surveillance systems exploit pan/tilt/zoom (PTZ) cameras to increase the field of view of a surveyed area. The background subtraction technique is widespread to detect moving objects with a high accuracy using one stationary camera. Extending such algorithms to work with moving cameras requires to have a background mosaic at one's disposal. Many solutions using mosaic background subtraction have been proposed, which offer real time capabilities or high quality of the detected objects. However, most of them rely on prior assumptions which limit the camera motion or the algorithm to work with a depth field of view only. In this work we propose some innovative solutions to achieve a real time mosaic background apt to work with existing background subtraction algorithms to yield excellent foreground object masks. Extensive experiments accomplished on challenging indoor and outdoor scenes permit to assess the quality of the mosaic as well as of the detected moving masks.","Motion detection,
Cameras,
Layout,
Surveillance,
Subtraction techniques,
Object detection,
Solid modeling,
Geometry,
Computer science,
Motion segmentation"
Improved class statistics estimation for sparse data problems in offline signature verification,"Sparse data problems are prominent in applications of offline signature verification. By using a small number of training samples, the class statistics estimation errors may be significant, resulting in worsened verification performance. In this paper, we propose two methods to improve the statistics estimation. The first approach employs an elastic distortion model to artificially generate additional training samples for pairs of genuine signatures. These additional samples, together with original genuine samples, are used to compute statistic parameters for a Mahalanobis distance threshold classifier. The other approach is to adopt regularization techniques to overcome the problem of inverting an ill-conditioned sample covariance matrix due to insufficient training samples. A ridge-like estimator is modeled to add some constant values for diagonal elements of the sample covariance matrix. Experimental results showed that both methods were able to improve verification accuracy when they were incorporated with a set of peripheral features. Effectiveness of the methods was validated by quantity analysis.","Statistics,
Handwriting recognition,
Covariance matrix,
Statistical distributions,
Error analysis,
Estimation error,
Computer science,
Nearest neighbor searches,
Training data,
Pattern recognition"
An adaptive bilateral negotiation model for e-commerce settings,"This paper studies adaptive bilateral negotiation between software agents in e-commerce environments. Specifically, we assume that the agents are self-interested, the environment is dynamic, and both agents have deadlines. Such dynamism means that the agents' negotiation parameters (such as deadlines and reservation prices) are functions of both the state of the encounter and the environment. Given this, we develop an algorithm that the negotiating agents can use to adapt their strategies to changes in the environment in order to reach an agreement within their specific deadlines and before the resources available for negotiation are exhausted. In more detail, we formally define an adaptive negotiation model and cast it as a Markov decision process. Using a value iteration algorithm, we then indicate a novel solution technique for determining optimal policies for the negotiation problem without explicit knowledge of the dynamics of the system. We also solve a representative negotiation decision problem using this technique and show that it is a promising approach for analyzing negotiations in dynamic settings. Finally, through empirical evaluation, we show that the agents using our algorithm learn a negotiation strategy that adapts to the environment and enables them to reach agreements in a timely manner.","Supply chains,
Intelligent agent,
Computer science,
Software agents,
Mechanical factors,
Bandwidth,
Multiagent systems"
"Constrained, globally optimal, multi-frame motion estimation","We address the problem of estimating the relative motion between the frames of a video sequence. In comparison with the commonly applied pairwise image registration methods, we consider global consistency conditions for the overall multi-frame motion estimation problem, which is more accurate. We review the recent work on this subject and propose an optimal framework, which can apply the consistency conditions as both hard constraints in the estimation problem, or as soft constraints in the form of stochastic (Bayesian) priors. The framework is applicable to virtually any motion model and enables us to develop a robust approach, which is resilient against the effects of outliers and noise. The effectiveness of the proposed approach is confirmed by a super-resolution application on synthetic and real data sets","Motion estimation,
Cameras,
Image registration,
Application software,
Image resolution,
Image sequences,
Computer science,
Video sequences,
Stochastic resonance,
Bayesian methods"
Link buffer sizing: a new look at the old problem,"We revisit the question of how much buffer an IP router should allocate for its droptail FIFO link. For a long time, setting the buffer size to the bitrate-delay product has been regarded as reasonable. Recent studies of interaction between queueing at IP routers and TCP congestion control offered alternative guidelines. First, we explore and reconcile contradictions between the existing rules. Then, we argue that the problem of link buffer sizing needs a new formulation: design a buffer sizing algorithm that accommodates needs of all Internet applications without engaging IP routers in any additional signaling. Our solution keeps network queues short: set the buffer size to 2L datagrams, where L is the number of input links. We also explain how end systems can utilize the network effectively despite such small buffering at routers.","Guidelines,
Internet,
IP networks,
Protocols,
TCPIP,
Thumb,
Laboratories,
Computer science,
Signal design,
Algorithm design and analysis"
A cooperative approach to interdomain traffic engineering,"For performance or cost reasons, autonomous systems (AS) often need to control the flow of their incoming interdomain traffic. Controlling its incoming traffic is a difficult task since it often implies influencing ASes on the path. The current BGP-based techniques that an AS can use for this purpose are primitive. Moreover, their effect is often difficult to predict. In this paper, we propose to solve this problem by using Virtual Peerings. A Virtual Peering is an IP tunnel between a border router of a source AS and a border router of a destination AS. This tunnel is established upon request from the destination AS. These tunnels can be negotiated by using backward compatible modifications to the border gateway protocol (BGP). By using Virtual Peerings, the source and destination ASes can achieve various traffic engineering objectives such as traffic-balancing or reducing the latency. A key advantage of our solution is that it does not require cooperation of the intermediate ASes and that it can be incrementally deployed in today's Internet. We then show by simulations that in a load-balancing scenario, a multi-homed AS only needs to request a few dozens of Virtual Peerings to balance its incoming traffic.","Communication system traffic control,
Traffic control,
Telecommunication traffic,
Protocols,
Delay,
Internet,
IP networks,
Cost function,
Computer science,
Control systems"
A systematic review of Web engineering research,"This paper uses a systematic literature review as means of investigating the rigor of claims arising from Web engineering research. Rigor is measured using criteria combined from software engineering research. We reviewed 173 papers and results have shown that only 5% would be considered rigorous methodologically. In addition to presenting our results, we also provide suggestions for improvement of Web engineering research based on lessons learnt by the software engineering community.","Software engineering,
Maintenance engineering,
Computer science,
Software measurement,
Quality management,
Engineering management,
Testing,
Reliability engineering,
Physics,
Manufacturing processes"
A new task model for streaming applications and its schedulability analysis,"In this paper, we introduce a new task model that is specifically targeted towards representing stream processing applications. Examples of such applications are those involved in network packet processing (such as a software-based router) and multimedia processing (such as an MPEG decoder application). Our task model is made up of two parts: (i) a new task structure to accurately model the software structures of stream processing applications such as conditional branches and different end-to-end deadlines for different types of input data items, and (ii) a new event model to represent the arrival pattern of the data items to be processed, which triggers the task structure. This event model is more expressive than classical models such as purely periodic, periodic with jitter or sporadic event models. We then present algorithms for the schedulability analysis of this task model. The basic scheme underlying our algorithms is a generalization of the techniques used for the schedulability analysis of the recently proposed generalized multiframe and the recurring real-time task models.","Application software,
Streaming media,
Algorithm design and analysis,
Processor scheduling,
Computer networks,
Scheduling algorithm,
Real time systems,
Computer science,
Laboratories,
Decoding"
A stream-weight optimization method for multi-stream HMMs based on likelihood value normalization,"In the field of audio-visual speech recognition, multi-stream HMM are widely used, thus how to automatically and properly determine stream weight factors using a small data set becomes an important research issue. This paper proposes a new stream-weight optimization method based on an output likelihood normalization criterion. In this method, the stream weights are adjusted to equalize the mean values of log likelihood for all HMM based on likelihood-ratio maximization which achieved significant improvement by using a large optimization data set. The new method is evaluated using Japanese connected digit speech recorded in real-world environments. Using 10 seconds speech data for stream-weight optimization, a 10% absolute accuracy improvement is achieved compared to the result before optimization. By additionally applying the MLLR (maximum likelihood linear regression) adaptation, a 23% improvement is obtained over the audio-only scheme.","Optimization methods,
Hidden Markov models,
Streaming media,
Automatic speech recognition,
Speech recognition,
Maximum likelihood linear regression,
Acoustic noise,
Maximum likelihood estimation,
Computer science,
Speech analysis"
Biologically inspired embodied evolution of survival,"Embodied evolution is a methodology for evolutionary robotics that mimics the distributed, asynchronous and autonomous properties of biological evolution. The evaluation, selection and reproduction are carried out by and between the robots, without any need for human intervention. In this paper, we propose a biologically inspired embodied evolution framework, which fully integrates self-preservation, recharging from external batteries in the environment, and self-reproduction, pair-wise exchange of genetic material, into a survival system. The individuals are explicitly evaluated for the performance of the battery capturing task, but also implicitly for the mating task by the fact that an individual that mates frequently has larger probability to spread its gene in the population. We have evaluated our method in simulation experiments and the simulation results show that the solutions obtained by our embodied evolution method were able to optimize the two survival tasks, battery capturing and mating, simultaneously. We have also performed preliminary experiments in hardware, with promising results.","Evolution (biology),
Batteries,
Genetics,
Erbium,
Biological materials,
Robot vision systems,
Humans,
Robotics and automation,
Numerical analysis,
Computer science"
CyberCIEGE: gaming for information assurance,"CyberCIEGE is a high-end, commercial-quality video game developed jointly by Rivermind and the Naval Postgraduate School's Center for Information Systems Security Studies and Research. This dynamic, extensible game adheres to information assurance principles to help teach key concepts and practices. CyberCIEGE is a resource management simulation in which the player assumes the role of a decision maker for an IT dependent organization. The objective is to keep the organization's virtual users happy and productive while providing the necessary security measures to protect valuable information assets.","Engines,
Information security,
Games,
Computer security,
Libraries,
Privacy,
Costs,
Authorization,
Concrete,
Computer science education"
Making driver modeling attractive,"Automobile industry and academic researchers expend considerable effort to develop driver assistance systems. DASs are aware of certain driving situations and support drivers through information, warnings, or even intervention. Many DAS applications are safety oriented, such as lane departure warning systems. Some are comfort oriented, such as automated parking assistants. Moreover, DAS human-machine interfaces must support careful communication with potentially taxed drivers. Driver models support DAS design in several ways. Our work focuses on modeling the tactical level of driving decisions, such as when to brake and whether to accelerate and pass another vehicle. Such decisions are based on local, instantaneously available environmental information about the road and other cars in the same or an adjacent lane.","Traffic control,
Acceleration,
Permission,
Road vehicles,
Vehicle driving,
Intelligent vehicles,
Vehicle dynamics,
Intelligent systems,
Evolutionary computation,
Safety"
Data dissemination in autonomic wireless sensor networks,"In this paper, a new data dissemination algorithm for wireless sensor networks is presented. The key idea of the proposed solution is to combine concepts presented in trajectory-based forwarding with the information provided by the energy map of the network to determine routes in a dynamic fashion, according to the energy level of the sensor nodes. This is an important feature of an autonomic system, which must have the capacity of adapting its behavior according to its available resources. Simulation results revealed that the energy spent with the data dissemination activity can be concentrated on nodes with high-energy reserves, whereas low-energy nodes can use their energy only to perform sensing activity or to receive information addressed to them. In this manner, partitions of the network due to nodes that ran out of energy can be significantly delayed and the network lifetime extended.","Intelligent networks,
Wireless sensor networks,
Monitoring,
Trajectory,
Data communication,
Distributed computing,
Computer science,
Energy states,
Radio access networks,
Communication system control"
Polychaete-like Undulatory Robotic Locomotion,"Polychaete annelid worms provide a biological paradigm of versatile locomotion and effective motion control, adaptable to a large variety of unstructured environmental conditions (water, sand, mud, sediment, etc.). The undulatory locomotion of their segmented body is characterized by the combination of a unique form of tail-to-head body undulations, with the rowing-like action of numerous lateral appendages distributed along their body. Computational models of polychaete-like crawling and swimming have been developed, based on the Lagrangian dynamics of the system and on resistive models of its interaction with the environment, and used for simulation studies demonstrating the generation of undulatory gaits. Several lightweight robotic prototypes have been developed, whose undulatory actuation achieves propulsion on sand. Extensive experiments demonstrate that the propulsion of these robots is characterized by essential features of polychaete locomotion, in agreement with the corresponding simulations.","Motion control,
Prototypes,
Propulsion,
Sea surface,
Surface morphology,
Computational modeling,
Humans,
Mobile robots,
Computer science,
Computer worms"
Modeling reward functions for incomplete state representations via echo state networks,"This paper investigates an echo state network (ESN) (Jaeger, 2001 and Maass and Markram, 2002) architecture as the approximation of the Q-function for temporally dependent rewards embedded in a linear dynamical system, the mass-spring-damper (MSD). This problem has been solved utilizing feed-forward neural networks (FNN) when all state information necessary to specify the dynamics is provided as input (Kretchmar, 2000). Time-delayed neural networks (TDNN) solve this problem with finite-size windows of incomplete state information. Our research demonstrates that the ESN architecture represents the Q-function of the MSD system given incomplete state information as well as current feed forward neural networks given either perfect state or a temporally-windowed, incomplete state vector. The remainder of this paper is organized as follows. We introduce basic concepts of reinforcement learning and the echo state network architecture. The MSD system simulation is defined in section IV. Experimental results for learning state quality given incomplete state information are presented in section V. Results for learning estimates of all future state qualities for incomplete state information is presented in section VI. Section VII discusses the potential of the ESN for use in reinforcement learning and provides current and future directions of research.","Learning,
Recurrent neural networks,
Neural networks,
Computer science,
Feedforward systems,
Control systems,
Feedforward neural networks,
Electronic mail,
Computer architecture,
Fuzzy control"
Co-evolutionary modular neural networks for automatic problem decomposition,"Decomposing a complex computational problem into sub-problems, which are computationally simpler to solve individually and which can be combined to produce a solution to the full problem, can efficiently lead to compact and general solutions. Modular neural networks represent one of the ways in which this divide-and-conquer strategy can be implemented. Here we present a co-evolutionary model which is used to design and optimize modular neural networks with task-specific modules. The model consists of two populations. The first population consists of a pool of modules and the second population synthesizes complete systems by drawing elements from the pool of modules. Modules represent a part of the solution, which co-operates with others in the module population to form a complete solution. With the help of two artificial supervised learning tasks created by mixing two sub-tasks we demonstrate that if a particular task decomposition is better in terms of performance on the overall task, it can be evolved using this co-evolutionary model.","Neural networks,
Design optimization,
System testing,
Computer science,
Network synthesis,
Supervised learning,
Humans,
Europe,
Design methodology,
Artificial neural networks"
When functions change their names: automatic detection of origin relationships,"It is a common understanding that identifying the same entity such as module, file, and function between revisions is important for software evolution related analysis. Most software evolution researchers use entity names, such as file names and function names, as entity identifiers based on the assumption that each entity is uniquely identifiable by its name. Unfortunately names change over time. In this paper, we propose an automated algorithm that identifies entity mapping at the function level across revisions even when an entity's name changes in the new revision. This algorithm is based on computing function similarities. We introduce eight similarity factors to determine if a function is renamed from a function. To find out which similarity factors are dominant, a significance analysis is performed on each factor. To validate our algorithm and for factor significance analysis, ten human judges manually identified renamed entities across revisions for two open source projects: Subversion and Apache2. Using the manually identified result set we trained weights for each similarity factor and measured the accuracy of our algorithm. We computed the accuracies among human judges. We found our algorithm's accuracy is better than the average accuracy among human judges. We also show that trained weights for similarity factors from one period in one project are reusable for other periods and/or other projects. Finally we combined all possible factor combinations and computed the accuracy of each combination. We found that adding more factors does not necessarily improve the accuracy of origin detection.",
Reducing the Calibration Effort for Location Estimation Using Unlabeled Samples,"WLAN location estimation based on 802.11 signal strength is becoming increasingly prevalent in today's pervasive computing applications. As an alternative to the well-established deterministic approaches, probabilistic location determination techniques show good performance and thus become increasingly popular. For these techniques to achieve a high level of accuracy, however, adequate training samples should be collected offline for calibration. As a result, a great amount of manual effort is incurred. In this paper, we aim to solve the problem by reducing both the sampling time and the number of locations sampled in constructing the radio map. A learning algorithm is proposed to build location estimation systems based on a small fraction of the calibration data that traditional techniques require and a collection of user traces that can be cheaply obtained. Our experiments show that unlabeled user traces can be used to compensate for the effects of reducing calibration effort and can even improve the system performance. Consequently, manual effort can be significantly reduced while a high level of accuracy is still achieved","Calibration,
Wireless LAN,
Pervasive computing,
Phase estimation,
Computer science,
Application software,
Sampling methods,
System performance,
Mobile computing,
Radio frequency"
NavTracks: supporting navigation in software,"Any modification to a software system involves investigating the source code with the goal of understanding it. In turn, this investigation usually requires significant navigation effort. However, relatively little research has been conducted on software developer navigation patterns and tool support for improving navigation. Our research addresses this issue. We present NavTracks, a tool that supports browsing through software. NavTracks keeps track of the navigation history of software developers, forming associations between related files. These associations are then used as the basis for recommending potentially related files as a developer browses the software system.","Navigation,
Software systems,
Cognitive science,
Computer science,
Software tools,
History,
Documentation,
Human computer interaction,
Space exploration,
Hierarchical systems"
Kernel-based Bayesian filtering for object tracking,"Particle filtering provides a general framework for propagating probability density functions in nonlinear and non-Gaussian systems. However, the algorithm is based on a Monte Carlo approach and sampling is a problematic issue, especially for high dimensional problems. This paper presents a new kernel-based Bayesian filtering framework, which adopts an analytic approach to better approximate and propagate density functions. In this framework, the techniques of density interpolation and density approximation are introduced to represent the likelihood and the posterior densities by Gaussian mixtures, where all parameters such as the number of mixands, their weight, mean, and covariance are automatically determined. The proposed analytic approach is shown to perform sampling more efficiently in high dimensional space. We apply our algorithm to real-time tracking problems, and demonstrate its performance on real video sequences as well as synthetic examples.","Bayesian methods,
Density functional theory,
Sampling methods,
Interpolation,
Monte Carlo methods,
Filtering algorithms,
Kernel,
Computer vision,
Density measurement,
Computer science"
Error propagation profiling of operating systems,"An operating system (OS) constitutes a fundamental software (SW) component of a computing system. The robustness of its operations, or lack thereof, strongly influences the robustness of the entire system. Targeting enhancement of robustness at the OS level via use of add-on SW wrappers, this paper presents an error propagation profiling framework that assists in a) systematic identification and location of design and operational vulnerabilities, and b) quantification of their potential impact. Focusing on data (value) errors occurring in OS drivers, a set of measures is presented that aids a designer to locate such vulnerabilities, either on an OS service (system call) basis or a per driver basis. A case study and associated experimental process, using Windows CE .Net, is presented outlining the utility of our proposed approach.",
Leveraging 1-hop neighborhood knowledge for efficient flooding in wireless ad hoc networks,"Flooding is a fundamental and frequently invoked operation in wireless ad hoc networks. Existing flooding techniques either are unreliable, generate overwhelming unnecessary retransmission, or incur excessive network control overhead. In this paper, we address these crucial problems and propose a new flooding technique called edge forwarding. The new scheme minimizes the flooding traffic by leveraging location information to limit broadcast retransmission to only hosts near the perimeter of each broadcast coverage. Unlike most existing techniques, edge forwarding requires each host to track only neighboring nodes within its one-hop distance. Therefore, it is more adaptive to host mobility and incurs less network control overhead. In particular, it can be easily incorporated into many existing routing protocols without any additional control overhead. Our performance studies indicate that with our strategy, a substantial portion of the unnecessary broadcast retransmission can be eliminated.","Intelligent networks,
Ad hoc networks,
Floods,
Broadcasting,
Mobile ad hoc networks,
Computer science,
Telecommunication traffic,
Costs,
Routing,
Relays"
A Flexible Infrastructure for the Development of a Robot Companion with Extensible HRI-Capabilities,"The development of robot companions with nat ural human-robot interaction (HRI) capabilities is a challeng ing task as it requires incorporating various functionalities. Consequently, a flexible infrastructure for controlling module operation and data exchange between modules is proposed, taking into account insights from software system integration. This is achieved by combining a three-layer control architec ture containing a flexible control component with a powerful communication framework. The use of XML throughout the whole infrastructure facilitates ongoing evolutionary develop ment of the robot companion's capabilities.","Robot kinematics,
Human robot interaction,
Communication system control,
Mobile robots,
Computer architecture,
XML,
Robot control,
Computer science,
Control systems,
Software systems"
Automatic 3D to 2D registration for the photorealistic rendering of urban scenes,"This paper presents a novel and efficient algorithm for the 3D range to 2D image registration problem in urban scene settings. Our input is a set of unregistered 3D range scans and a set of unregistered and uncalibrated 2D images of the scene. The 3D range scans and 2D images capture real scenes in extremely high detail. A new automated algorithm calibrates each 2D image and computes an optimized transformation between the 2D images and 3D range scans. This transformation is based on a match of 3D with 2D features that maximizes an overlap criterion. Our algorithm attacks the hard 3D range to 2D image registration problem in a systematic, efficient, and automatic way. Images captured by a high-resolution 2D camera, that moves and adjusts freely, are mapped on a centimeter-accurate 3D model of the scene providing photorealistic renderings of high quality. We present results from experiments in three different urban settings.","Layout,
Cameras,
Image registration,
Solid modeling,
Rendering (computer graphics),
Image sensors,
Sensor systems,
Computer science,
Educational institutions,
Calibration"
Convergent piecewise affine systems: analysis and design Part I: continuous case,"In this paper convergence properties for piecewise affine (PWA) systems are studied. The notions of exponential, uniform and input-to-state convergence are introduced and studied. For PWA systems with continuous right-hand sides it is shown that the existence of a common quadratic Lyapunov function for the linear parts of the system dynamics in every mode is sufficient for the exponential and input-to-state convergence of the system. For a class of PWA control systems we design (output) feedback controllers that make the closed-loop system input-to-state convergent. The conditions for such controller design are formulated in terms of LMIs. The obtained results can be used for designing observers and (output-feedback) tracking controllers for PWA systems.",
Operator dependence of 3-D ultrasound-based computational fluid dynamics for the carotid bifurcation,"The association between vascular wall shear stress (WSS) and the local development of atherosclerotic plaque makes estimation of in vivo WSS of considerable interest. Three-dimensional ultrasound (3DUS) combined with computational fluid dynamics (CFD) provides a potentially valuable tool for acquiring subject-specific WSS, but the interoperator and intraoperator variability associated with WSS calculations using this method is not known. Here, the accuracy, reproducibility and operator dependence of 3DUS-based computational fluid dynamics were examined through a phantom and in vivo studies. A carotid phantom was scanned and reconstructed by two operators. In the in vivo study, four operators scanned a healthy subject a total of 11 times, and their scan data were processed by three individuals. The study showed that with some basic training, operators could acquire accurate carotid geometry for flow reconstructions. The variability of measured cross-sectional area and predicted shear stress was 8.17% and 0.193 N/m/sup 2/ respectively for the in vivo study. It was shown that the variability of the examined parameters was more dependent on the scan operators than the image processing operator. The range of variability of geometrical and flow parameters reported here can be used as a reference for future in vivo studies using the 3DUS-based CFD approach.","Ultrasonic imaging,
Computational fluid dynamics,
Bifurcation,
In vivo,
Stress,
Imaging phantoms,
Image reconstruction,
Reproducibility of results,
Geometry,
Area measurement"
On the Usability Evaluation of E-Learning Applications,"Despite the advances of the electronic technologies in e-learning, a consolidated evaluation methodology for e-learning applications does not yet exist. The goal of e-learning is to offer the users the possibility to become skillful and acquire knowledge on a new domain. The evaluation of educational software must consider its pedagogic effectiveness as well as its usability. The design of its interface should take into account the way students learn and also provide good usability so that student's interactions with the software are as natural and intuitive as possible. In this paper, we present the results obtained from a first phase of observation and analysis of the interactions of people with e-learning applications. The aim is to provide a methodology for evaluating such applications.","Usability,
Electronic learning,
Application software,
User centered design,
Human computer interaction,
Software tools,
Design methodology,
Web design,
Interactive systems"
A Generic Framework for Semantic Sports Video Analysis Using Dynamic Bayesian Networks,"Automatic detection of semantic events in sport videos is a challenging task. In this paper, we propose a multimodal multilayer statistical inference framework for semantic sports video analysis using Dynamic Bayesian Networks (DBNs). Based on this framework, three instances including factorial hierarchical hidden Markov model (FHHMM), coupled hierarchical hidden Markov model (CHHMM), and product hierarchical hidden Markov model (PHHMM), are constructed and compared. Play-break detection in soccer videos is used as a testbed with hierarchical hidden Markov model (HHMM) as a baseline. Experimental results indicate the superior capability of the PHHMM, because it not only effectively models dynamic interactions between different modalities, but also sufficiently utilizes context constraints in multilayer structures.","Bayesian methods,
Hidden Markov models,
Event detection,
Nonhomogeneous media,
Games,
Videoconference,
Information analysis,
Computers,
Asia,
Testing"
Waveband switching networks with limited wavelength conversion,We study reconfigurable multi-granular optical cross-connects (MG-OXCs) in waveband switching networks with limited wavelength conversion and propose a heuristic algorithm to minimize the number of used wavelength converters while reducing the blocking probability.,"Optical wavelength conversion,
Optical switches,
Optical fiber networks,
Computer science,
Costs,
Telecommunication traffic,
Computer architecture,
Heuristic algorithms,
Wavelength division multiplexing,
Wavelength routing"
Dynamosaics: video mosaics with non-chronological time,"With the limited field of view of human vision, our perception of most scenes is built over time while our eyes are scanning the scene. In the case of static scenes, this process can be modeled by panoramic mosaicing: stitching together images into a panoramic view. Can a dynamic scene, scanned by a video camera, be represented with a dynamic panoramic video even though different regions were visible at different times? In this paper, we explore time flow manipulation in video, such as the creation of new videos in which events that occurred at different times are displayed simultaneously. More general changes in the time flow are also possible, which enable re-scheduling the order of dynamic events in the video, for example. We generate dynamic mosaics by sweeping the aligned space-time volume of the input video by a time front surface and generating a sequence of time slices in the process. Various sweeping strategies and different time front evolutions manipulate the time flow in the video, enabling many unexplored and powerful effects, such as panoramic movies.","Layout,
Motion pictures,
Cameras,
Videoconference,
Humans,
Eyes,
Computer science,
Contracts,
Stacking,
Strips"
Mixed block placement via fractional cut recursive bisection,"Recursive bisection is a popular approach for large scale circuit placement problems, combining a high degree of scalability with good results. In this paper, we present a bisection-based approach for both standard cell and mixed block placement; in contrast to prior work, our horizontal cut lines are not restricted to row boundaries. This technique, which we refer to as a fractional cut, simplifies mixed block placement and also avoids a narrow region problem encountered in standard cell placement. Our implementation of these techniques in the placement tool Feng Shui 2.6 retains the speed and simplicity for which bisection is known, while making it competitive with leading methods on standard cell designs. On mixed block placement problems, we obtain substantial improvements over recently published work. Half perimeter wire lengths are reduced by 29% on average, compared to a flow based on Capo and Parquet; compared to mPG-ms, wire lengths are reduced by 26% on average.","Circuits,
Wire,
Design automation,
Scalability,
Algorithm design and analysis,
Computer science,
Large-scale systems,
Fabrication,
Explosions,
Logic gates"
Towards constant bandwidth overhead integrity checking of untrusted data,"We present an adaptive tree-log scheme to improve the performance of checking the integrity of arbitrarily large untrusted data, when using only a small fixed-sized trusted state. Currently, hash trees are used to check the data. In many systems that use hash trees, programs perform many data operations before performing a critical operation that exports a result outside of the program's execution environment. The adaptive tree-log scheme we present uses this observation to harness the power of the constant runtime bandwidth overhead of a log-based scheme. For all programs, the adaptive tree-log scheme's bandwidth overhead is guaranteed to never be worse than a parameterizable worst case bound. Furthermore, for all programs, as the average number of times the program accesses data between critical operations increases, the adaptive tree-log scheme's bandwidth overhead moves from a logarithmic to a constant bandwidth overhead.","Bandwidth,
Runtime,
Data security,
Computer science,
Artificial intelligence,
Laboratories,
Licenses,
Application software,
Protection,
Displays"
Signaling performance of SIP based VoIP: a measurement-based approach,"VoIP must offer the same level of performance and reliability as PSTN, if it were to replace the traditional PSTN. An important indicator of the performance of VoIP is the signaling or call set up delay. Most of the prior research efforts concerning the signaling performance consider long distance calls routed over public wide area networks. Due to the many outstanding issues associated with using VoIP over public networks, however, VoIP is considered to be a viable option primarily in the case of corporate intranets and tunnels. When VoIP is used over corporate intranets and tunnels, the influence of factors such as workload of end user devices, and the number of proxies along the routing path may be prominent and this influence needs to be determined. Also, corporate VoIP service is likely to be offered across heterogeneous network infrastructures using solutions from multiple vendors. As a result, it is necessary to be able to measure the signaling performance of VoIP independent of the underlying networking technology and the vendor-specific implementation. In this paper we demonstrate the feasibility of open, standard JAIN (Java APIs for integrated networks) SIP APIs, which are originally developed to foster rapid service creation across heterogeneous networks, for the purpose of measuring signaling performance of VoIP. Since the performance measurement capability is based on open, standard APIs it can be used uniformly for VoIP implementations from multiple vendors operating on heterogeneous network infrastructures. We quantify the impact of workload on end-user devices on the signaling performance of VoIP via extensive experimentation. Our results can be used to estimate the signaling performance for a given level of load prior to deployment of VoIP","Internet telephony,
Wide area networks,
IP networks,
Propagation delay,
Measurement standards,
Quality of service,
Bandwidth,
Computer science,
Reliability engineering,
Computer network reliability"
An EM based training algorithm for cross-language text categorization,"Due to the globalization on the Web, many companies and institutions need to efficiently organize and search repositories containing multilingual documents. The management of these heterogeneous text collections increases the costs significantly because experts of different languages are required to organize these collections. Cross-language text categorization can provide techniques to extend existing automatic classification systems in one language to new languages without requiring additional intervention of human experts. In this paper, we propose a learning algorithm based on the EM scheme which can be used to train text classifiers in a multilingual environment. In particular, in the proposed approach, we assume that a predefined category set and a collection of labeled training data is available for a given language L/sub 1/. A classifier for a different language L/sub 2/ is trained by translating the available labeled training set for L/sub 1/ to L/sub 2/ and by using an additional set of unlabeled documents from L/sub 2/. This technique allows us to extract correct statistical properties of the language L/sub 2/ which are not completely available in automatically translated examples, because of the different characteristics of language L/sub 1/ and of the approximation of the translation process. Our experimental results show that the performance of the proposed method is very promising when applied on a test document set extracted from newsgroups in English and Italian.","Text categorization,
Humans,
Costs,
Machine learning algorithms,
Machine learning,
Labeling,
Computer science,
Globalization,
Training data,
Natural languages"
Exploiting Hierarchical Identity-Based Encryption for Access Control to Pervasive Computing Information,"Access control to confidential information in pervasive computing environments is challenging for multiple reasons: First, a client requesting access might not know which access rights are necessary in order to be granted access to the requested information. Second, access control must support flexible access rights that include context-sensitive constraints. Third, pervasive computing environments consist of a multitude of information services, which makes simple management of access rights essential. We discuss the shortcomings of existing access-control schemes that rely on either clients presenting a proof of access to a service or services encrypting information before handing the information over to a client. We propose a proofbased access-control architecture that employs hierarchical identity-based encryption in order to enable services to inform clients of the required proof of access in a covert way, without leaking information. Furthermore, we introduce an encryption-based access-control architecture that exploits hierarchical identity-based encryption in order to deal with multiple, hierarchical constraints on access rights. We present an example implementation of our proposed architectures and discuss the performance of this implementation.","Identity-based encryption,
Access control,
Pervasive computing,
Permission,
Cryptography,
Calendars,
Computer science,
Environmental management,
Computer architecture,
File systems"
Unsupervised border detection of skin lesion images,"As a result of the advances in skin imaging technology and the development of suitable image processing/computer vision algorithms, during the last decade, there has been a significant increase of interest in the computer-aided diagnosis of skin cancer. Automated border extraction is one of the most important steps in this procedure since the accuracy of the subsequent steps crucially depends on the accuracy of this very first step. In this paper, we present an unsupervised approach to border detection in skin lesion (tumor) images based on a modified version of the JSEG algorithm [Y. Deng et al., (2001)]. The segmentation results are visually examined by an expert dermatologist and are found to be highly accurate.","Lesions,
Skin cancer,
Image segmentation,
Computer science,
Biomedical engineering,
Biomedical imaging,
Medical diagnostic imaging,
Image processing,
Computer vision,
Computer aided diagnosis"
Shedding new light on Zadeh's criticism of Dempster's rule of combination,"Shortly after proposing Dempster's rule of combination as a general aggregation rule for evidence from independent sources, Zadeh formulated an annoying example for which Dempster's rule seemed to produce counter-intuitive results. Since then, many authors have used Zadeh's example either to criticize Demster-Shafer theory as a whole, or as a motivation for constructing alternative combination rules. This paper shows that the counter-intuition of Zadeh's example is not a problem of Dempster's rule, but a problem of Zadeh's model that does not correspond to reality. Two different ways to fix the problem will be discussed, showing that Dempster's rule of combination perfectly behaves, as one would expect.","Computer science,
Mathematics,
Measurement uncertainty,
Bayesian methods"
Preventing DoS attack in sensor networks: a game theoretic approach,"In order to isolate malicious nodes in wireless sensor and actor networks and provide a secure routing, we formulate the attack-defense problem as a two-player, nonzero-sum, non-cooperative game between an attacker and a wireless sensor and actor network. We show that this game achieves Nash equilibrium and thus leads to a defense strategy for the network. We propose two novel schemes for preventing denial of service attack. The first approach is called utility based dynamic source routing (UDSR). It incorporates the total utility of each route in data packets, where utility is a value that we are trying to maximize in the game theoretic approach. The second approach is based on a watch-list, where each node earns a rating from its neighbors, based on its previous cooperation in the network. Results show that the proposed game frameworks significantly increase the chance of success in defense strategy for the wireless sensor and actor networks.","Computer crime,
Intelligent networks,
Game theory,
Wireless sensor networks,
Routing,
Communication system security,
Energy consumption,
Computer science,
Batteries,
Data security"
Connected k-hop clustering in ad hoc networks,"In wireless ad hoc networks, clustering is one of the most important approaches for many applications. A connected k-hop clustering network is formed by electing clusterheads in k-hop neighborhoods and finding gateway nodes to connect clusterheads. Therefore, the number of nodes to be flooded in broadcast related applications could be reduced. In this paper, we study the localized solution for the connectivity issue of clusterheads with less gateway nodes. We develop the adjacency-based neighbor clusterhead selection rule (A-NCR) by extending the ""2.5"" hops coverage theorem and generalizing it to k-hop clustering. We then design the local minimum spanning tree based gateway algorithm (LMSTGA), which could be applied on the adjacent clusterheads selected by A-NCR to further reduce gateway nodes. In the simulation, we study the performance of the proposed approaches, using different values for parameter k. The results show that the proposed approaches generate a connected k-hop clustering network, and reduce the number of gateway nodes effectively.","Intelligent networks,
Ad hoc networks,
Clustering algorithms,
Mobile ad hoc networks,
Scalability,
Computer science,
Computer networks,
Application software,
Broadcasting,
Algorithm design and analysis"
Automatic management of laboratory work in mass computer engineering courses,"Teaching computer engineering calls for an important practical component, usually covered by setting several laboratory exercises for each course. These exercises are specified as assignments by the teachers and have to be completed by the students. At the Computer Science School of the Technical University of Madrid (UPM), Madrid, Spain, some of these laboratory exercises have to be set for up to 400 students. High-quality laboratory work requires the use of technology to help in student management, interaction, and assessment. Over the last ten years, this department at UPM has been a site for new tool development. Students use these tools to submit their laboratory work over the network. These tools allow the students' work to be checked by a battery of tests proposed by the instructor. In addition, a specific tool has been built to detect plagiarism by students. This work describes the services provided by this environment and the experience gathered during the use of this laboratory.","Computer science education,
Computer aided instruction"
Optimal Positioning Strategies for Shape Changes in Robot Teams,"In this paper, we consider the task of repositioning a formation of robots to a new shape while minimizing either the maximum distance that any robot travels, or the total distance traveled by the formation. We show that optimal solutions in SE (2) can be achieved for either metric through second-order cone programming (SOCP) techniques. For the case where the orientation of the new formation shape is fixed, we obtain optimal solutions in both R2and R3. The latter also allows for complete regulation of the formation size via constraints on the shape scale. We expect that these results will prove useful for extending the mission lives of robot formations and mobile ad-hoc networks (MANETs).","Robot kinematics,
Robot sensing systems,
Orbital robotics,
Mobile robots,
Ad hoc networks,
Biosensors,
Robot control,
Shape control,
Computer science,
Robot programming"
Soup or Art? The role of evidential force in empirical software engineering,"Software project managers' decisions should be based on solid evidence, not on common wisdom or vendor hype. What distinguishes the science from the art is the way in which we as managers and practitioners make decisions, by forming rational arguments from the evidence we have - evidence that comes both from our experience and from related research. That is, we move from the part to the whole, examining the body of evidence to determine what we know about the best ways to build good software. This view isn't particular to software engineering or even to mathematical sciences; it's what characterizes good science in general. This article examines the ways in which careful understanding of argumentation and evidence can lead to more effective empirical software engineering - and ultimately to better decision making and higher-quality software products and processes.","Subspace constraints,
Software engineering,
Uncertainty,
Solids,
Testing,
Layout,
Painting,
Art,
Project management,
Engineering management"
Efficient Byzantine broadcast in wireless ad-hoc networks,"This paper presents an overlay based Byzantine tolerant broadcast protocol for wireless ad-hoc networks. The use of an overlay results in a significant reduction in the number of messages. The protocol overcomes Byzantine failures by combining digital signatures, gossiping of message signatures, and failure detectors. These ensure that messages dropped or modified by Byzantine nodes will be detected and retransmitted and that the overlay will eventually consist of enough correct processes to enable message dissemination. An appealing property of the protocol is that it only requires the existence of one correct node in each one-hop neighborhood. The paper also includes a detailed performance evaluation by simulation.","Broadcasting,
Intelligent networks,
Ad hoc networks,
Routing,
Robustness,
Multicast protocols,
Computer science,
Broadcast technology,
Wireless application protocol,
Digital signatures"
Educational Applets for active learning in properties of electronic materials,"The traditional lecturer-driven classroom is giving way to a new more active environment, where students have access to a variety of multimedia course materials. The authors created several Java applets (http://www.collage.soe.ucsc.edu) that present concepts related to properties of materials in both active and passive styles. The authors evaluated the use of the applets in a classroom setting, considering student learning preferences, background profiles, and applet preferences.",
Mutually unbiased bases are complex projective 2-designs,"Mutually unbiased bases (MUBs) are a primitive used in quantum information processing to capture the principle of complementarity. While constructions of maximal sets of d+1 such bases are known for system of prime power dimension d, it is unknown whether this bound can be achieved for any non-prime power dimension. In this paper we demonstrate that maximal sets of MUBs come with a rich combinatorial structure by showing that they actually are the same objects as the complex projective 2-designs with angle set {0, 1/d}. We also give a new and simple proof that symmetric informationally complete POVMs are complex projective 2-designs with angle set {1/(d+1)}","Information processing,
Quantum mechanics,
Eigenvalues and eigenfunctions,
Density measurement,
Matrix decomposition,
Tomography,
Computer science,
National electric code,
Mechanical variables measurement,
Protocols"
Comparative study of name disambiguation problem using a scalable blocking-based framework,"In this paper, we consider the problem of ambiguous author names in bibliographic citations, and comparatively study alternative approaches to identify and correct such name variants (e.g., ""Vannevar Bush"" and ""V. Vush""). Our study is based on a scalable two-step framework, where step 1 is to substantially reduce the number of candidates via blocking, and step 2 is to measure the distance of two names via coauthor information. Combining four blocking methods and seven distance measures on four data sets, we present extensive experimental results, and identify combinations that are scalable and effective to disambiguate author names in citations","Partitioning algorithms,
Computer science,
Large-scale systems,
Software libraries,
Portals,
Permission,
Books,
Error correction,
Information systems,
Information retrieval"
Locally testable cyclic codes,"Cyclic linear codes of block length n over a finite field F/sub q/ are linear subspaces of F/sub q//sup n/ that are invariant under a cyclic shift of their coordinates. A family of codes is good if all the codes in the family have constant rate and constant normalized distance (distance divided by block length). It is a long-standing open problem whether there exists a good family of cyclic linear codes. A code C is r-testable if there exists a randomized algorithm which, given a word x/spl isin//sub q//sup n/, adaptively selects r positions, checks the entries of x in the selected positions, and makes a decision (accept or reject x) based on the positions selected and the numbers found, such that 1) if x/spl isin/C then x is surely accepted; ii) if dist(x,C) /spl ges/ /spl epsi/n then x is probably rejected. (""dist"" refers to Hamming distance.) A family of codes is locally testable if all members of the family are r-testable for some constant r. This concept arose from holographic proofs/PCP's. Recently it was asked whether there exist good, locally testable families of codes. In this paper the intersection of the two questions stated is addressed. Theorem. There are no good, locally testable families of cyclic codes over any (fixed) finite field. In fact the result is stronger in that it replaces condition ii) of local testability by the condition ii') if dist (x,C) /spl ges/ /spl epsi/n then x has a positive chance of being rejected. The proof involves methods from Galois theory, cyclotomy, and diophantine approximation.","Testing,
Computer science,
Linear code,
Galois fields,
Hamming distance,
Holography,
National security,
Research and development,
Laboratories,
Mathematics"
An event-driven multithreaded dynamic optimization framework,"Dynamic optimization has the potential to adapt the program's behavior at run-time to deliver performance improvements over static optimization. Dynamic optimization systems usually perform their optimization in series with the application's execution. This incurs overhead which reduces the benefit of dynamic optimization, and prevents some aggressive optimizations from being performed. In this paper we propose a new dynamic optimization framework called Trident. Concurrent with the program's execution, the framework uses hardware support to identify optimization opportunities, and uses spare threads on a multithreaded processor to perform dynamic optimizations for these optimization events. We evaluate the benefit of using Trident to guide code layout, basic compiler optimizations, and value specialization. Our results show that using Trident with these optimizations achieves an average 20% speedup, and is complementary with other memory latency tolerant techniques, such as prefetching.","Hardware,
Yarn,
Monitoring,
Runtime,
Optimizing compilers,
Software performance,
Computer science,
Delay,
Prefetching,
Multithreading"
"GLARE: A Grid Activity Registration, Deployment and Provisioning Framework","Resource management is a key concern for implementing effective Grid middleware and shielding application developers from low level details. Existing resource managers concentrate mostly on physical resources. However, some advanced Grid programming environments allow application developers to specify Grid application components at high level of abstraction which then requires an effective mapping between high level application description (activity types) and actual deployed software components (activity deployments). This paper describes GLARE framework that provides dynamic registration, automatic deployment and on-demand provision of application components (activities) that can be used to build Grid applications. GLARE simplifies description and presentation of both activity types and deployments so that they can easily be located in the Grid and thus become available on-demand. GLARE has been implemented based on a super-peer model with support for activity leasing, self management, and fault tolerance. Experiments are shown to reflect the effectiveness of the GLARE.","Application software,
Resource management,
Grid computing,
Programming environments,
Computer networks,
Permission,
Computer science,
Middleware,
Disaster management,
Computer network management"
Complexity measures for software systems : towards multi-agent based software testing,"Bringing together agents and other fields of software engineering might be difficult, as the advantages of agent technology are still not widely recognized. Effectiveness claims of agent-oriented software engineering are based upon the strategies for addressing complex systems. Agent technologies facilitate the automated software testing by virtue of their high-level decomposition, independency and parallel activation. The informal interpretations of qualitative agent theories are not sufficient to distinguish agent-based approaches from other approaches in software testing. In this paper, we do not just described the agent-based approach in software testing, also developed an evaluation framework for agent-oriented approach in software testing and proposed a multi-agent system for software testing. This paper therefore provides a timely summary and enhancement of agent theory in software testing, which motivates recent efforts in adapting concepts and methodologies for agent-oriented software testing (AOST) to complex systems, which has not previously done. The 'multi-modal' approach proposed here is to offer a definition for encompassing to cover the software testing phenomena, based on agents, at the preliminary level, yet sufficiently tight that it can rule out complex systems that are clearly not agent-based.","Software measurement,
Software systems,
Software testing,
Software engineering,
Application software,
Multiagent systems,
Computer science,
System testing,
Software quality,
Design engineering"
Supporting Complex Multi-Dimensional Queries in P2P Systems,"More and more applications require peer-to-peer (P2P) systems to support complex queries over multi-dimensional data. For example, a P2P auction network for real estate frequently needs to answer queries such as ""select five available buildings closest to the airport"". Such queries are not efficiently supported in current P2P systems. Towards an efficient and scalable P2P system capable of processing complex multi-dimensional queries, the authors first proposed a comprehensive framework for sharing, indexing, and querying multi-dimensional data, where (i) peers with more computational power coordinate indexing and query processing, and (ii) other peers participate in part of the computation in order to achieve scalability and load-balance. Based on this framework, Network-R-tree (NR-tree), a P2P adaptation of the dominant spatial index - R*-tree was proposed. NR-tree, indexing spatial data at clustered peers, is capable of processing complex queries such as range queries and k-nearest neighbor queries. The authors proposed query processing algorithms for range and k-nearest neighbor queries and experimentally prove the effectiveness of proposed techniques with real data","Peer to peer computing,
Indexing,
Query processing,
Computer science,
Routing,
Multidimensional systems,
Bandwidth,
Network servers,
Data engineering,
Application software"
A framework for teaching real-time digital signal processing with field-programmable gate arrays,"Many curricula include separate classes in both digital signal processing (DSP) theory and very high-speed integrated circuit hardware description language (VHDL) modeling; however, there are few opportunities given to students to combine these two skills into a working knowledge of DSP hardware design. A pedagogical framework has been developed whereby students can leverage their previous knowledge of DSP theory and VHDL hardware design techniques to design, simulate, synthesize, and test digital signal processing systems. The synthesized hardware is implemented on field-programmable gate arrays (FPGAs), which provide a fast and cost-effective way of prototyping hardware systems in a laboratory environment. This framework allows students to expand their previous knowledge into a more complete understanding of the entire design process from specification and simulation through synthesis and verification. Students are exposed to different aspects of signal processing design, including finite precision, parallel implementation, and implementation cost tradeoffs.","Digital signal processors,
Field programmable gate arrays,
Hardware design languages,
Electrical engineering education,
Computer science education"
Extended ZRP: a routing layer based service discovery protocol for mobile ad hoc networks,"Service discovery in mobile ad hoc networks is an essential process in order for these networks to be self-configurable with zero or minimal administration overhead. In this paper we argue that service discovery can be greatly enhanced in terms of efficiency (regarding service discoverability and energy consumption), by piggybacking service information into routing layer messages. Thus, service discovery does not generate additional messages and a node requesting a service, in addition to discovering that service, it is simultaneously informed of the route to the service provider. We extended the zone routing protocol (ZRP) in order to encapsulate service information in its routing messages. Extensive simulations demonstrate the superiority of this routing layer-based service discovery scheme over that of a similar, but application layer based service discovery scheme.","Routing protocols,
Mobile ad hoc networks,
Availability,
Energy consumption,
Mobile computing,
Computer science,
Power generation economics,
Environmental economics,
Computer science education,
Educational programs"
Characterizing the duration and association patterns of wireless access in a campus,"Our goal is to characterize the access patterns in a IEEE802.11 infrastructure. This can be beneficial in many domains, including coverage planning, resource reservation, supporting location-dependent applications and applications with real-time constraints, and producing models for simulations. We conducted an extensive measurement study of wireless users and their association patterns on a major university campus using the IEEE802.11 wireless infrastructure. We characterized and analyzed the wireless access pattern based on several parameters such as mobility, session and visit durations. We show that the mobility and building type affect the session and visit durations. As the mobility increases, the visit duration tends to decrease stochastically. The opposite happens in the case of the session duration. Moreover, there exist different stochastic orders among visit durations of different building types when conditioning on session mobility. A family of BiPareto distributions can model the visit and session duration.","USA Councils,
Privacy"
Cross domain automatic transcription on the TC-STAR EPPS corpus,"This paper describes the ongoing development of the British English European Parliament Plenary Session corpus. This corpus will be part of the speech-to-speech translation evaluation infrastructure of the European TC-STAR project. Furthermore, we present first recognition results on the English speech recordings. The transcription system has been derived from an older speech recognition system built for the North-American broadcast news task. We report on the measures taken for rapid cross-domain porting and present encouraging results.","Automatic speech recognition,
Speech synthesis,
Speech recognition,
Performance loss,
Humans,
Natural languages,
Satellite broadcasting,
Computer science,
Databases,
Training data"
Learning with constrained and unlabelled data,"Classification problems abundantly arise in many computer vision tasks eing of supervised, semi-supervised or unsupervised nature. Even when class labels are not available, a user still might favor certain grouping solutions over others. This bias can be expressed either by providing a clustering criterion or cost function and, in addition to that, by specifying pairwise constraints on the assignment of objects to classes. In this work, we discuss a unifying formulation for labelled and unlabelled data that can incorporate constrained data for model fitting. Our approach models the constraint information by the maximum entropy principle. This modeling strategy allows us (i) to handle constraint violations and soft constraints, and, at the same time, (ii) to speed up the optimization process. Experimental results on face classification and image segmentation indicates that the proposed algorithm is computationally efficient and generates superior groupings when compared with alternative techniques.","Computer vision,
Cost function,
Image segmentation,
Labeling,
Computer science,
Fitting,
Entropy,
Constraint optimization,
Face detection,
Object recognition"
Design and Evaluation of an Educational Software Process Simulation Environment and Associated Model,"Simulation is an educational tool that is commonly used to teach processes that are infeasible to practice in the real world. Software process education is a domain that has not yet taken full advantage of the benefits of simulation. To address this, we have developed SimSE, an educational, interactive, graphical environment for building and simulating software engineering processes in a game-like setting. We detail the design of SimSE, present an initial simulation model of a waterfall process that we developed, and describe an experiment that we conducted to evaluate the educational potential of SimSE and its initial model","Software engineering,
Computational modeling,
Computer simulation,
Project management,
Software development management,
Engineering management,
Graphical user interfaces,
Computer displays,
Employee rights,
Software tools"
Existing and emerging image quality metrics,"This paper summarizes and evaluates some of the existing methods of measuring and quantifying the quality of a digital image. Unfortunately, no general method has been found. The performance of a quality metric is normally gauged by its prediction accuracy, monotonicity and consistency. It is also expected to mirror the quality scores assigned by independent human observers. Research to this point has generally focused on full-reference (FR) measures that assume that coded and original images are available. Often times, an original is not easily obtainable, or perhaps does not even exist. Therefore, researchers have recently shown a great deal of interest in developing reduced-reference (RR) and no-reference (NR) metrics. This study implements and compares some of the most common IQMs and seeks to determine if there is any difference in their performance. Analysis of the results focuses on determining if any IQM is superior to the others over a general set of test images","Image quality,
Humans,
Accuracy,
Testing,
Computer science,
Drives,
Digital images,
Mirrors,
Image analysis,
Compression algorithms"
Implementing Kilo-Instruction Multiprocessors,"Multiprocessors are coming into wide-spread use in many application areas, yet there are a number of challenges to achieving a good tradeoff between complexity and performance. For example, while implementing memory coherence and consistency is essential for correctness, efficient implementation of critical sections and synchronization points is desirable for performance. The multi-checkpointing mechanisms of Kilo-Instruction Processors can be leveraged to achieve good complexity-effective multiprocessor designs. We describe how to implement a Kilo-Instruction Multiprocessor that transparently, i.e. without any software support, uses transaction-based memory updates. Our model not only simplifies memory coherence and consistency hardware, but at the same time, it provides the potential for implementing high performance speculative mechanisms for commonly occurring synchronization constructs.",
Open Source Enterprise Systems: Towards a Viable Alternative,"Enterprise systems are located within the antinomy of appearing as generic product, while being means of multiple integrations for the user through configuration and customisation. Technological and organisational integrations are defined by architectures and standardised interfaces. Until recently, technological integration of enterprise systems has been supported largely by monolithic architectures that were designed, and maintained by the respective developers. From a technical perspective, this approach had been challenged by the suggestion of component-based enterprise systems that would allow for a more user-focused system through strict modularisation. Lately, the product nature of software as proprietary item has been questioned through the rapid increase of open source programs that are being used in business computing in general, and also within the overall portfolio that makes up enterprise systems. This suggests the potential for altered technological and commercial constellations for the design of enterprise systems, which are presented in different scenarios. The technological and commercial decomposition of enterprise software and systems may also address some concerns emerging from the users' experience of those systems, and which may have arisen from their proprietary or product nature.","Open source software,
Packaging,
Computer architecture,
Information systems,
System software,
Software packages,
Enterprise resource planning,
Business,
Portfolios,
Software systems"
Survey and classification of transport layer mobility management schemes,"Mobility of Internet hosts allows computing nodes to move between subnets. Mobility can be handled at different layers of the protocol stack, with network and transport layer mobility being the most widely studied. Transport layer mobility can overcome many of the limitations of network layer schemes like mobile IP. Various approaches have been proposed to implement mobility in the transport layer. In this paper, we discuss a number of transport layer mobility schemes, classify them according to their approach, and compare them based on a number of evaluation criteria","Mobile radio mobility management,
Peer to peer computing,
IP networks,
Computer science,
USA Councils,
Internet,
Telecommunication computing,
Transport protocols,
Wireless networks,
Disruption tolerant networking"
Blind image restoration with eigen-face subspace,"Performance of conventional image restoration methods is sensitive to signal-to-noise ratios. For heavily blurred and noisy human facial images, information contained in the eigen-face subspace can be used to compensate for the lost details. The blurred image is decomposed into the eigen-face subspace and then restored with a regularized total constrained least square method. With Generalized cross-validation, a cost function is deduced to include two unknown parameters: the regularization factor and one parameter relevant to point spread function. It is shown that, in minimizing the cost function, the cost function dependence of any one unknown parameter can be separated from the other one, which means the cost function can be considered roughly, depending on single variable in an iterative algorithm. With realistic constraints on the regularized factor, a global minimum for the cost function is achieved to determine the unknown parameters. Experiments are presented to demonstrate the effectiveness and robustness of the new method.","Image restoration,
Cost function,
Subspace constraints,
Additive noise,
Degradation,
Computer science,
Educational technology,
Laboratories,
Pervasive computing,
Computer science education"
Partitioning Multi-Threaded Processors with a Large Number of Threads,"Today's general-purpose processors are increasingly using multithreading in order to better leverage the additional on-chip real estate available with each technology generation. Simultaneous multi-threading (SMT) was originally proposed as a large dynamic superscalar processor with monolithic hardware structures shared among all threads. Inters hyper-threaded Pentium 4 processor partitions the queue structures among two threads, demonstrating more balanced performance by reducing the hoarding of structures by a single thread. IBM's Power5 processor is a 2-way chip multiprocessor (CMP) of SMT processors, each supporting 2 threads, which significantly reduces design complexity and can improve power efficiency. This paper examines processor partitioning options for larger numbers of threads on a chip. While growing transistor budgets permit four and eight-thread processors to be designed, design complexity, power dissipation, and wire scaling limitations create significant barriers to their actual realization. We explore the design choices of sharing, or of partitioning and distributing, the front end (instruction cache, instruction fetch, and dispatch), the execution units and associated state, as well as the L1 Dcache banks, in a clustered multi-threaded (CMT) processor. We show that the best performance is obtained by restricting the sharing of the L1 Dcache banks and the execution engines among threads. On the other hand, significant sharing of the front-end resources is the best approach. When compared against large monolithic SMT processors, a CMT processor provides very competitive IPC performance on average, 90-96% of that of partitioned SMT while being more scalable and much more power efficient. In a CMP organization, the gap between SMT and CMT processors shrinks further, making a CMP of CMT processors a highly viable alternative for the future","Yarn,
Surface-mount technology,
Hardware,
Process design,
Power dissipation,
Throughput,
Computer science,
Laboratories,
Multithreading,
System-on-a-chip"
Discriminative training of hidden Markov models for multiple pitch tracking [speech processing examples],"We present a multiple pitch tracking algorithm that is based on direct probabilistic modeling of the spectrogram of the signal. The model is a factorial hidden Markov model whose parameters are learned discriminatively from the Keele pitch database. Our algorithm can track several pitches and determines the number of pitches that are active at any given time. We present simulation results on mixtures of several speech signals and noise, showing the robustness of our approach.","Hidden Markov models,
Spectrogram,
Signal processing algorithms,
Speech,
Inference algorithms,
Graphical models,
Computer science,
Noise robustness,
Multiple signal classification,
Algorithm design and analysis"
Glass Box: An Instrumented Infrastructure for Supporting Human Interaction with Information,"In this paper, we discuss the challenges involved in developing an infrastructure to support a new generation of analytic tools for information analysts. The infrastructure provides data for establishing context about what the analyst is doing with the analytic tools, supports an integration environment to allow suites of tools to work together, and supports evaluation of the analytic tools. We discuss the functionality of the Glass Box, the challenges of evaluating adaptive systems including the capture of data for evaluation metrics, and lessons learned from our experiences to date.","Glass,
Instruments,
Humans,
Information analysis,
Research and development,
Data analysis,
NIST,
Adaptive systems,
Error analysis,
Computer errors"
On discovery of extremely low-dimensional clusters using semi-supervised projected clustering,"Recent studies suggest that projected clusters with extremely low dimensionality exist in many real datasets. A number of projected clustering algorithms have been proposed in the past several years, but few can identify clusters with dimensionality lower than 10% of the total number of dimensions, which are commonly found in some real datasets such as gene expression profiles. In this paper we propose a new algorithm that can accurately identify projected clusters with relevant dimensions as few as 5% of the total number of dimensions. It makes use of a robust objective function that combines object clustering and dimension selection into a single optimization problem. The algorithm can also utilize domain knowledge in the form of labeled objects and labeled dimensions to improve its clustering accuracy. We believe this is the first semi-supervised projected clustering algorithm. Both theoretical analysis and experimental results show that by using a small amount of input knowledge, possibly covering only a portion of the underlying classes, the new algorithm can be further improved to accurately detect clusters with only 1% of the dimensions being relevant. The algorithm is also useful in getting a target set of clusters when there are multiple possible groupings of the objects.","Clustering algorithms,
Gene expression,
Text mining,
Computer science,
Mathematics,
Robustness,
Algorithm design and analysis,
Computer vision,
Euclidean distance,
Object recognition"
A framework of social interaction support for ubiquitous learning,"This paper describes the computer supported ubiquitous learning system and the social interaction between learners. We define the ubiquitous learning with five prime attributes, and present a generalized social interaction support model for the ubiquitous learning. Moreover, in order to support learners with increasing social skill, a solution for constructing social interaction in ubiquitous learning environment is designed, which includes three major functions; encounter, communication and collaboration support functions.","Pervasive computing,
Personal digital assistants,
Computer networks,
Learning systems,
Computer science education,
Ubiquitous computing,
Computer interfaces,
Embedded computing,
Humans,
Collaboration"
Shape Analysis of Breast Masses in Mammograms via the Fractal Dimension,"Masses due to benign breast diseases and tumors due to breast cancer present significantly different shapes on mammograms. In general, malignant tumors appear with rough and complex boundaries or contours, whereas benign masses present smooth, round, or oval contours. Fractal analysis may be used to derive shape features to perform pattern classification of breast masses and tumors. Several procedures have been proposed to compute the fractal dimension of various types of objects or regions of interest in biomedical images, among which the box-counting and ruler methods are popular. In this study, we applied the two methods mentioned above to compute the fractal dimension of both the two-dimensional (2D) contours of breast masses and tumors, as well as their one-dimensional (1D) signatures. A comparative analysis was performed to assess the performance of the two methods of computing the fractal dimension and the two methods of representing the boundaries of masses. It was observed that analysis of the 2D contour representation with the ruler method resulted in the highest classification accuracy of up to 0.946, as indicated by the area under the receiver operating characteristics (ROC) curve. The results indicate that the fractal dimension can serve as a good shape feature for the benign-versus-malignant classification of breast masses in mammograms","Shape,
Fractals,
Breast neoplasms,
Performance analysis,
Biomedical computing,
Diseases,
Breast cancer,
Malignant tumors,
Benign tumors,
Pattern analysis"
Accessing ubiquitous services using smart phones,"The integration of Bluetooth service discovery protocol (SDP), and GPRS Internet connectivity into phones provides a simple yet powerful infrastructure for accessing services in nomadic environments. In this paper, we discuss the design and implementation of SDIPP, a protocol for provisioning services on smart phones. Although several service discovery protocols have been proposed earlier, such as SLP, Jini, UPnP, Salutation, they all have their own infrastructure requirements and target audiences, Bluetooth SDP is an on-the-fly service discovery protocol. However, it is not nearly as powerful as its counterparts. SDIPP works by augmenting Bluetooth SDP with Web access and personalization. Payment of services has been overlooked in the protocols proposed earlier. SDIPP provides a novel protocol for anonymous payment, based on the idea of Millicent scrips. We have implemented a few services to illustrate our protocol. We report on our experiences and experimental results. In particular, we analyze and provide an application level solution to the Bluetooth inquiry clash problem that was discovered in the process.","Smart phones,
Bluetooth,
Access protocols,
Pervasive computing,
Ground penetrating radar,
Power system modeling,
Costs,
Computer science,
Web and internet services,
Cellular phones"
Approximate reduct computation by rough sets based attribute weighting,Rough set theory provides the reduct and the core concepts for knowledge reduction. The cost of reduct set computation is highly influenced by the attribute set size of the dataset where the problem of finding reducts has been proven as an NP-hard problem. This paper proposes an approximate approach for reduct computation. The approach uses the discernibility matrix concept and a weighting mechanism to determine the significance of an attribute to be considered in the reduct. A second supplementary weight is used to break the tie when several attributes have the same significance. The approach is extensively experimented and evaluated on various standard domains.,"Rough sets,
Set theory,
Costs,
NP-hard problem,
Algorithm design and analysis,
Computer science,
Information technology,
Data analysis,
Heuristic algorithms,
Genetic algorithms"
Cyberinfrastructure for Science and Engineering: Promises and Challenges,"It is now clear that information technology (IT) construed broadly has the power to revolutionize all areas of science and engineering (S&E) research and education. If exploited, this transformative power can change S&E forever. The National Science Foundation (NSF) is working toward this end, and, as a result, past efforts in supercomputing and high-performance networking are being subsumed into a broader, integrated vision of a more capable, ubiquitous, and accessible cyberinfrastructure (CI). We cannot attain this overnight, or alone, or without substantial research in computer science and engineering. This paper will outline the potential for revolution, the vision of CI, some of the challenges, and a possible path toward reaching that vision.","Power engineering and energy,
Grid computing,
Information technology,
Power engineering education,
Computer science,
Pervasive computing,
Computational modeling,
Computer simulation,
Lattices,
Entropy"
LOCH: supporting informal language learning outside the classroom with handhelds,"The continuous development of wireless and mobile technologies has allowed the creation of an additional platform for supporting learning, one that can be embedded in the same physical space in which the learning is taking place. This paper describes a computer supported ubiquitous learning environment for language learning, called LOCH (language-learning outside the classroom with handhelds). In the environment, the teacher assigns field activities to the students, who go around the town to fulfil them and share their individual experiences. The main aim of this project, called one day trip with PDA (personal digital assistant), was to integrate the knowledge acquired in the classroom and the real needs of the students in their daily life.","Natural languages,
Space technology,
Pervasive computing,
Personal digital assistants,
Ubiquitous computing,
Vocabulary,
Information science,
Intelligent systems,
Handheld computers,
Cities and towns"
Quasi steady-state models for long-term voltage and frequency dynamics simulation,"The quasi steady-state (QSS) approximation of long-term dynamics relies on time-scale decomposition and consists of replacing faster phenomena by their equilibrium conditions, in order to reduce the complexity of the whole model and increase the computation efficiency of time simulations. This paper describes the extension of a QSS model extensively used in long-term voltage stability studies, to readily incorporate the frequency dynamics that takes place over the same time scale. This extended QSS model relies on a common-frequency assumption. Its advantages, limitations and possible improvements are discussed through simulation results on the hydro-Quebec system, where it has been compared to full time scale simulations. Disturbances with an impact on either frequency or voltages are considered and the coupling between these two aspects of long-term dynamics is briefly discussed.","Steady-state,
Voltage,
Frequency,
Power system dynamics,
Power system modeling,
Computational modeling,
Power system simulation,
Equations,
Turbines,
Analytical models"
An agent-based resource allocation model for grid computing,"The paper employs three types of agents namely resource brokering agents (RBAs), job agents (JAs), and resource monitoring agents (RMAs) for resource allocation in grid computing. RBA acts as a resource scheduler as well as a broker for the users to submit their jobs through JAs. JAs search for the resources. RMAs reside inside the nodes of the local cluster and inform the status of the resources to the local cluster server. The proposed model is evaluated in a simulated environment to test its operational effectiveness in various scenarios.","Resource management,
Grid computing,
Pricing,
Monitoring,
Computer networks,
Buffer storage,
Information science,
Educational institutions,
US Government,
Scheduling"
Mechanism design via machine learning,"We use techniques from sample-complexity in machine learning to reduce problems of incentive-compatible mechanism design to standard algorithmic questions, for a wide variety of revenue-maximizing pricing problems. Our reductions imply that for these problems, given an optimal (or /spl beta/-approximation) algorithm for the standard algorithmic problem, we can convert it into a (1 + /spl epsi/)-approximation (or /spl beta/(1 +/spl epsi/)-approximation) for the incentive-compatible mechanism design problem, so long as the number of bidders is sufficiently large as a function of an appropriate measure of complexity of the comparison class of solutions. We apply these results to the problem of auctioning a digital good, the attribute auction problem, and to the problem of item-pricing in unlimited-supply combinatorial auctions. From a learning perspective, these settings present several challenges: in particular the loss function is discontinuous and asymmetric, and the range of bidders' valuations may be large.","Machine learning,
Pricing,
Computer science,
Algorithm design and analysis,
Machine learning algorithms,
Marketing and sales,
Measurement standards,
Cost accounting,
Writing,
Automobiles"
Application of MPC to an active structure using sampling rates up to 25kHz,"In this paper we demonstrate the implementation of model predictive control (MPC) for vibration suppression of the first five bending modes of an active structure. For adequate performance, this requires a 5kHz sampling rate, which is achieved using a standard active-set optimisation technique running on a 200MHz digital signal processor. Experimental results show that MPC offers improved performance for this application when compared with other standard approaches.","Sampling methods,
Voltage control,
Vibration control,
Structural beams,
Quadratic programming,
Displacement control,
Piezoelectric transducers,
Predictive models,
Predictive control,
Digital signal processors"
A selective neighbor caching scheme for fast handoff in IEEE 802.11 wireless networks,"Mobility support in IEEE 802.11 networks is a challenging issue. Recently, a new scheme, called proactive neighbor caching (PNC), was proposed and adopted as an IEEE standard. The PNC scheme introduces a neighbor graph, which dynamically captures the mobility topology of a wireless network for pre-positioning the context of a mobile host (MH). However, the PNC scheme may result in a significant signaling overhead because the MH's context is propagated to all neighbor access points (APs). We propose a selective neighbor caching (SNC) scheme, which propagates an MH's context to only the selected neighbor APs considering handoff frequencies between APs. When the context transfer is needed, neighbor APs with handoff probabilities equal to or higher than a predefined threshold value are selected. We also derive an optimal threshold value when the target cache hit probability is given. Simulation results reveal that the SNC scheme significantly reduces the signaling overhead while guaranteeing a comparable cache hit probability compared to the PNC scheme.","Intelligent networks,
Wireless networks,
Delay,
Wireless LAN,
Network topology,
Web and internet services,
Quality of service,
Protocols,
Computer science,
Frequency"
Analysis and algorithms for content-based event matching,"Content-based event matching is an important problem in large-scale event-based publish/subscribe systems. However, open questions remain in analysis of its difficulty and evaluation of its solutions. This paper makes a few contributions toward analysis, evaluation and development of matching algorithms. First, based on a simplified yet generic model, we give a formal proof of hardness of matching problem by showing its equivalence to the notoriously hard Partial Match problem. Second, we compare two major existing matching approaches and show that counting-based algorithms are likely to be more computationally expensive than tree-based algorithms in this model. Third, we observe an important, prevalent characteristic of real-world publish/subscribe events, and develop a new matching algorithm called RAPIDMatch to exploit it. Finally, we propose a new metric for evaluation of matching algorithms. We analyze and evaluate RAPIDMatch using both the traditional and new metrics proposed. Results show that RAPIDMatch achieves large performance improvement over the tree-based algorithm under certain publish/subscribe scenarios.","Algorithm design and analysis,
Subscriptions,
Partitioning algorithms,
Computer science,
Large-scale systems,
Real time systems,
Delay,
Throughput,
Guidelines,
Impedance matching"
Evolutionary computation technologies for space systems,"The Evolvable Computation Group, at NASA's Jet Propulsion Laboratory, is tasked with demonstrating the utility of computational engineering and computer optimized design for complex space systems. The group is comprised of researchers over a broad range of disciplines including biology, genetics, robotics, physics, computer science and system design, and employs biologically inspired evolutionary computational techniques to design and optimize complex systems. Over the past two years we have developed tools using genetic algorithms, simulated annealing and other optimizers to improve on human design of space systems. We have further demonstrated that the same tools used for computer-aided design and design evaluation can be used for automated innovation and design. These powerful techniques also serve to reduce redesign costs and schedules",
Tracking Cell Signals in Fluorescent Images,"In this paper we present the techniques for tracking cell signal in GFP (Green Fluorescent Protein) images of growing cell colonies. We use such tracking for both data extraction and dynamic modeling of intracellular processes. The techniques are based on optimization of energy functions, which simultaneously determines cell correspondences, while estimating the mapping functions. In addition to spatial mappings such as affine and Thin-Plate Spline mapping, the cell growth and cell division histories must be estimated as well. Different levels of joint optimization are discussed. The most unusual tracking feature addressed in this paper is the possibility of one-to-two correspondences caused by cell division. A novel extended softassign algorithm for solutions of one-to-many correspondences is detailed in this paper. The techniques are demonstrated on three sets of data: growing bacillus Subtillus and e-coli colonies and a developing plant shoot apical meristem. The techniques are currently used by biologists for data extraction and hypothesis formation.","Fluorescence,
Bioinformatics,
Proteins,
Data mining,
Genomics,
Computer science,
Organisms,
Circuit testing,
Computer vision,
Propulsion"
Multi-objective optimisation in the presence of uncertainty,"There has been only limited discussion on the effect of uncertainty and noise in multi-objective optimization problems and how to deal with it. We address this problem by assessing the probability of dominance and maintaining an archive of solutions which are, with some known probability, mutually non-dominating. We examine methods for estimating the probability of dominance. These depend crucially on estimating the effective noise variance and we introduce a novel method of learning the variance during optimization. Probabilistic domination contours are presented as a method for conveying the confidence that may be placed in objectives that are optimized in the presence of uncertainty","Uncertainty,
Error analysis,
Computer science,
Optimization methods,
Evolutionary computation,
Design optimization,
Computer aided manufacturing,
Measurement errors,
Stochastic systems,
Pattern recognition"
Performance Benefits of Avoiding Head-of-Line Blocking in SCTP,"Mitigating the effects of head-of-line blocking (HoLB) was one of the major reasons the IETF SIGTRAN working group developed SCTP, a new transport protocol for PSTN signaling traffic, in the first place. However, studies of the impact of HoLB blocking on TCP and SCTP have given ambiguous results as to whether HoLB has, in fact, any significantly deteriorating effect on transmission delay. To this end, we have carried out a detailed experimental study on the quantitative effects of HoLB. Our study suggests that although HoLB could indeed incur a substantial delay penalty on a small fraction of the messages in an SCTP session, it has only a marginal impact on the average end-to-end transmission delay. We only observed improvements in the range of O% to 18% in average message transmission delay of using unordered delivery as compared to ordered delivery. Furthermore, there was a large variability in between different test runs, which often made the impact of HoLB statistically insignificant","Transport protocols,
Communication industry,
Telecommunications,
TCPIP,
Computer science,
Delay effects,
Testing,
Costs,
Streaming media,
Manufacturing"
A decentralized fault diagnosis system for wireless sensor networks,"The irregularities of a low cost wireless communication interface, changing environmental conditions, in-situ deployment and scarce resources make management, monitoring and troubleshooting performance of a sensor network a challenging task. In this paper we present the design of a decentralized fault diagnosis system for a wireless sensor network. Our system distinguishes between multiple root causes of degraded performance and provides efficient feedback into the network to troubleshoot the fault","Fault diagnosis,
Sensor systems,
Wireless sensor networks,
Throughput,
Base stations,
Monitoring,
Computer science,
Costs,
Wireless communication,
Computer network management"
Automatic In Situ Identification of Plankton,"Earth's oceans are a soup of living micro-organisms known as plankton. As the foundation of the food chain for marine life, plankton are also an integral component of the global carbon cycle which regulates the planet's temperature. In this paper, we present a technique for automatic identification of plankton using a variety of features and classification methods including ensembles. The images were obtained in situ by an instrument known as the flow cytometer and microscope (FlowCAM), that detects particles from a stream of water siphoned directly from the ocean. The images are of necessity of limited resolution, making their identification a rather difficult challenge. We expect that upon completion, our system will become a useful tool for marine biologists to assess the health of the world's oceans.","Marine vegetation,
Instruments,
Organisms,
Laboratories,
Microscopy,
Streaming media,
Ocean temperature,
Image resolution,
Chemical industry,
Computer science"
User-satisfaction based differentiated services for wireless data networks,"The success of future generation wireless data services depends on the parameterized provisioning of quality of service (QoS) for applications whose demands and nature are highly heterogeneous. Also, user satisfaction plays a key role in the economic viability of wireless service deployments. In this paper, we present a QoS framework based on the paradigm of traffic class and user satisfaction. The proposed class-based QoS framework comprises a radio resource management scheme which considers user satisfaction based on the perceived QoS, and caters to heterogeneous applications such as voice and multiple types of data services, with diverse QoS requirements. The resource management scheme has two components: the admission control algorithm caters to the long term user satisfaction while the session-based rate and bandwidth allocation scheme manipulates the short term user satisfaction. Performance metrics have been specifically defined for each traffic class - blocking for voice communications, average rate jitter for streaming applications, bounded and negotiated delay for interactive data and background traffic. Extensive simulations using four types of traffic and three classes of users reveal that the proposed framework offer improved QoS without compromising the utilization of the system.","Resource management,
Quality of service,
Traffic control,
Delay effects,
Scheduling algorithm,
Computer science,
Data engineering,
Application software,
Admission control,
Channel allocation"
Adaptive watermarking using relationships between wavelet coefficients,"We propose an approach that hides watermarks in relationships between wavelet coefficients. This approach minimizes the perceptual distortion of watermarked images, measured by PSNR or just noticeable distortion (JND). Therefore, the strength of watermarks can be enlarged to increase the robustness of the watermarks, while keeping the quality of watermarked images visually acceptable. Experimental results show that the watermark is still detectable after common image processing operations such as JPEG compression, Gaussian filtering and sharpening.",
Scratchpad sharing strategies for multiprocess embedded systems: a first approach,"Portable embedded systems require diligence in managing their energy consumption. Thus, power efficient processors coupled with onchip memories (e.g. caches, scratchpads) are the base of today's portable devices. Scratchpads are more energy efficient than caches but require software support for their utilization. Portable devices' applications consist of multiple processes for different tasks. However, all the previous scratchpad allocation approaches only consider single process applications. In this paper, we propose a set of optimal strategies to reduce the energy consumption of applications by sharing the scratchpad among multiple processes. The strategies assign both code and data elements to the scratchpad and result in average total energy reductions of 9%-20% against a published single process approach. Furthermore, the strategies generate Pareto-optimal curves for the applications allowing design time exploration of energy/scratchpad size tradeoffs.","Embedded system,
Energy consumption,
Application software,
Convergence,
Mobile handsets,
Computer science,
Energy management,
Power system management,
Energy efficiency,
Bluetooth"
Change impact analysis for requirement evolution using use case maps,"Changing customer needs and computer technology are the driving factors influencing software evolution. There is a need to assess the impact of these changes on existing software systems. Requirement specification is gaining increasingly attention as a critical phase of software systems development process. In particular for larger systems, it quickly becomes difficult to comprehend what impact a requirement change might have on the overall system or parts of the system. Thus, the development of techniques and tools to support the evolution of requirement specifications becomes an important issue. In this paper we present a novel approach to change impact analysis at the requirement level. We apply both slicing and dependency analysis at the use case map specification level to identify the potential impact of requirement changes on the overall system. We illustrate our approach and its applicability with a case study conducted on a simple telephony system.",
A two-level semantic caching scheme for super-peer networks,"Recent measurement studies of file-sharing peer-to-peer networks have demonstrated the presence of semantic proximity between peers and between shared files. In this paper we tackle the problem of exploiting the semantic locality of peer requests to improve the performance of a P2P network by the use of semantic caches. Such caches group together peers with similar interests as well as files with similar request patterns. The resulting two-level caching infrastructure can be integrated in a very natural way with the super-peer concept. The super-peers in our system cache pointers to files recently requested by their client peers. The client peers, on the other hand, keep caches of super-peers that answered most of their requests in the past. As a consequence, peers with similar interests are grouped (clustered) under the same super-peers, and also files with similar request patterns are indexed by the same super-peers. We discuss the design choices and optimizations of the presented model. We also evaluate our system versus the network that uses only one level of semantic caches.","Peer to peer computing,
Computer science,
Mathematics,
Design optimization,
Internet,
Bandwidth,
Protocols,
Information management,
Network servers,
Conferences"
Floorplanning for 3-D VLSI design,"In this paper, we present a floorplanning algorithm for 3D ICs. The problem can be formulated as that of packing a given set of 3D rectangular blocks while minimizing a suitable cost function. Our algorithm is based on a generalization of the classical 2D slicing floorplans to 3D slicing floorplans. A new encoding scheme of slicing floorplans (2D/3D) and its associated set of moves form the basis of the new simulated annealing based algorithm. The best-known algorithm for packing 3D rectangular blocks is based on simulated annealing using sequence-triple floorplan representation. Experimental results show that our algorithm produces packing results on average 3% better than the sequence-triple-based algorithm under the same annealing parameters, and our algorithm runs much faster (17 times for problems containing 100 blocks) than the sequence-triple. Moreover, our algorithm can be extended to consider various types of placement constraints and thermal distribution while the existing sequence-triple-based algorithm does not have such capabilities. Finally, when specializing to 2D problems, our algorithm is a new 2D slicing floorplanning algorithm. We are excited to report the surprising results that our new 2D floorplanner has produced slicing floorplans for the two largest MCNC benchmarks ami33 and ami49 which have the smallest areas (among all slicing/nonslicing floorplanning algorithms) ever reported in the literature.","Very large scale integration,
Integrated circuit interconnections,
Simulated annealing,
CMOS technology,
Integrated circuit technology,
Three-dimensional integrated circuits,
Testing,
Computer science,
Cost function,
Encoding"
Multiple view geometry and the L/sub /spl infin//-norm,"This paper presents a new framework for solving geometric structure and motion problems based on L/sub /spl infin//-norm. Instead of using the common sum-of-squares cost-function, that is, the L/sub /spl infin//-norm, the model-fitting errors are measured using the L/sub /spl infin//-norm. Unlike traditional methods based on L/sub 2/ our framework allows for efficient computation of global estimates. We show that a variety of structure and motion problems, for example, triangulation, camera resectioning and homography estimation can be recast as a quasiconvex optimization problem within this framework. These problems can be efficiently solved using second order cone programming (SOCP) which is a standard technique in convex optimization. The proposed solutions have been validated on real data in different settings with small and large dimensions and with excellent performance.","Geometry,
Cameras,
Image reconstruction,
Computer vision,
Computer science,
Motion estimation,
Layout,
Motion measurement,
Polynomials,
Application software"
Commercial adoption of open source software: an empirical study,"There has been a dramatic increase in commercial interest in the potential of open source software (OSS) over the past few years. However, given the many complex and novel issues that surround the use of OSS, the process of OSS adoption is not well-understood. We investigated this issue using a framework derived from innovation adoption theory which was then validated in an organisation which had embarked on a large-scale of adoption of OSS. The framework comprised four macro-factors - external environment, organisational context, technological context and individual factors. We then investigated these factors in a large-scale survey. Overall, the findings suggest a significant penetration of OSS with general deployment in two industry sectors - consultancy/software house and service/communication - and more limited deployment in government/public sector. However, the existence of a coherent and planned IT infrastructure based on proprietary software served to impede adoption of OSS. Finally, individual-relevant factors such as support for the general OSS ideology and committed personal championship of OSS were found to be significant.","Open source software,
Technological innovation,
Hospitals,
Large-scale systems,
Information systems,
Context,
Government,
Business,
Computer science,
Communication industry"
Protocol design challenges for multi-hop dynamic spectrum access networks,"Driven by the need to improve network capacity, there is a growing interest for dynamically utilizing spectrum over a wide range of frequency bands. The available spectrum is typically divided into multiple channels. Past work on designing protocols for multi-channel wireless networks has assumed that all channels are homogeneous. However, channels that are located in widely separated frequency bands exhibit considerable heterogeneity in transmission ranges, data rates, etc. In this paper, we identify' the impact of channel heterogeneity on network performance, and motivate the need to account for channel heterogeneity while designing higher layer protocols. We present some approaches for managing heterogeneity, and propose hiding most of the channel heterogeneity from higher layers by designing suitable channel abstractions","Access protocols,
Spread spectrum communication,
Frequency,
Proposals,
Cognitive radio,
Wireless networks,
Wireless mesh networks,
Buildings,
Computer science,
Computer networks"
Similarity Searching in Peer-to-Peer Databases,"We consider the problem of handling similarity queries in peer-to-peer databases. We propose an indexing and searching mechanism which, given a query object, returns the set of objects in the database that are semantically related to the query. We propose an indexing scheme which clusters data such that semantically related objects are partitioned into a small set of clusters, allowing for a simple and efficient similarity search strategy. Our indexing scheme also decouples object and node locations. Our adaptive replication and randomized lookup schemes exploit this feature and ensure that the number of copies of an object is proportional to its popularity and all replicas are equally likely to serve a given query, thus achieving perfect load balancing. The techniques developed in this work are oblivious to the underlying DHT topology and can be implemented on a variety of structured overlays such as CAN, CHORD, Pastry, and Tapestry. We also present DHT-independent analytical guarantees for the performance of our algorithms in terms of search accuracy, cost, and load-balance; the experimental results from our simulations confirm the insights derived from these analytical models","Peer to peer computing,
Databases,
Indexing,
Analytical models,
Information retrieval,
Computer science,
Educational institutions,
Load management,
Topology,
Performance analysis"
Leading Computational Methods on Scalar and Vector HEC Platforms,"The last decade has witnessed a rapid proliferation of superscalar cache-based microprocessors to build high-end computing (HEC) platforms, primarily because of their generality, scalability, and cost effectiveness. However, the growing gap between sustained and peak performance for full-scale scientific applications on conventional supercomputers has become a major concern in high performance computing, requiring significantly larger systems and application scalability than implied by peak performance in order to achieve desired performance. The latest generation of custom-built parallel vector systems have the potential to address this issue for numerical algorithms with sufficient regularity in their computational structure. In this work we explore applications drawn from four areas: atmospheric modeling (CAM), magnetic fusion (GTC), plasma physics (LBMHD3D), and material science (PARATEC). We compare performance of the vector-based Cray X1, Earth Simulator, and newly-released NEC SX-8 and Cray X1E, with performance of three leading commodity-based superscalar platforms utilizing the IBM Power3, Intel Itanium2, and AMD Opteron processors. Our work makes several significant contributions: the first reported vector performance results for CAM simulations utilizing a finite-volume dynamical core on a high-resolution atmospheric grid; a new data-decomposition scheme for GTC that (for the first time) enables a breakthrough of the Teraflop barrier; the introduction of a new three-dimensional Lattice Boltzmann magneto-hydrodynamic implementation used to study the onset evolution of plasma turbulence that achieves over 26Tflop/s on 4800 ES promodity-based superscalar platforms utilizing the IBM Power3, Intel Itanium2, and AMD Opteron processors, with modern parallel vector systems: the Cray X1, Earth Simulator (ES), and the NEC SX-8. Additionally, we examine performance of CAM on the recently-released Cray X1E. Our research team was the first international group to conduct a performance evaluation study at the Earth Simulator Center; remote ES access is not available. Our work builds on our previous efforts [16, 17] and makes several significant contributions: the first reported vector performance results for CAM simulations utilizing a finite-volume dynamical core on a high-resolution atmospheric grid; a new datadecomposition scheme for GTC that (for the first time) enables a breakthrough of the Teraflop barrier; the introduction of a new three-dimensional Lattice Boltzmann magneto-hydrodynamic implementation used to study the onset evolution of plasma turbulence that achieves over 26Tflop/s on 4800 ES processors; and the largest PARATEC cell size atomistic simulation to date. Overall, results show that the vector architectures attain unprecedented aggregate performance across our application suite, demonstrating the tremendous potential of modern parallel vector systems.","Plasma simulation,
Atmospheric modeling,
Computer aided manufacturing,
CADCAM,
Earth,
Scalability,
Plasma applications,
National electric code,
Magnetic cores,
Lattice Boltzmann methods"
Comparative study between the internal behavior of GA and PSO through problem-specific distance functions,"The evolutionary approach is a family of probabilistic search algorithms. The genetic algorithm (GA) and particle swarm optimization (PSO) are members of the evolutionary family, where both GA and PSO have been proven to be successful in finding good solutions in a short time for many combinatorial problems. In this paper, we have proposed several metrics, in the form of distance functions (DP), to examine and compare the internal behavior of GA and PSO based on a problem-specific DF rather than an algorithmic DF. Our initial experimental results show that PSO has more smooth and steady distance function values than GA.","Genetic algorithms,
Particle swarm optimization,
Evolution (biology),
Genetic mutations,
Problem-solving,
Algorithm design and analysis,
Application software,
Biology,
Computer science,
Biological cells"
A multidisciplinary approach to biometrics,"Biometrics is an emerging field of technology using unique and measurable physical, biological, or behavioral characteristics that can be processed to identify a person. It is a multidisciplinary subject that integrates engineering, statistics, mathematics, computing, psychology, and policy. The need for biometrics can be found in governments, in the military, and in commercial applications. The Electrical Engineering Department at the U.S. Naval Academy, Annapolis, MD, has introduced a biometric signal processing course for senior-level undergraduate students and has developed a biometrics lab to support this course. In this paper, the authors present the course content, the newly developed biometric signal processing lab, and the interactive learning process of the biometric course. They discuss some of the challenges that were encountered in implementing the course and how they were overcome. They also provide some feedback from the course assessment.","Access control,
Military computing,
Electrical engineering education,
Computer science education"
SQL DOM: compile time checking of dynamic SQL statements,"Most object oriented applications that involve persistent data interact with a relational database. The most common interaction mechanism is a call level interface (CLI) such as ODBC or JDBC. While there are many advantages to using a CLI - expressive power and performance being two of the most key - there are also drawbacks. Applications communicate through a CLI by constructing strings that contain SQL statements. These SQL statements are only checked for correctness at runtime, tend to be fragile and are vulnerable to SQL injection attacks. To solve these and other problems, we present the SQL DOM: a set of classes that are strongly-typed to a database schema. Instead of string manipulation, these classes are used to generate SQL statements. We show how to extract the SQL DOM automatically from an existing database schema, demonstrate its applicability to solve the mentioned problems, and evaluate its performance.",
Generalized performance management of multi-class real-time imprecise data services,"The intricacy of real-time data service management increases mainly due to the emergence of applications operating in open and unpredictable environments, increases in software complexity, and need for performance guarantees. In this paper we propose an approach for managing the quality of service of real-time databases that provide imprecise and differentiated services, and that operate in unpredictable environments. Transactions are classified into service classes according to their level of importance. Transactions within each service class are further classified into subclasses based on their quality of service requirements. In this way transactions are explicitly differentiated according to their importance and quality of service requests. The performance evaluation shows that during overloads the most important transactions are guaranteed to meet their deadlines and that reliable quality of service is provided even in the face of varying load and execution time estimation errors","Quality of service,
Real time systems,
Application software,
Quality management,
Computer science,
Disaster management,
Environmental management,
Engines,
Industrial control,
Databases"
Improved port knocking with strong authentication,"It is sometimes desirable to allow access to open ports on a firewall only to authorized external users and present closed ports to all others. We examine ways to construct an authentication service to achieve this goal, and then examine one such method, ""port knocking"", and its existing implementations, in detail. We improve upon these existing implementations by presenting a novel port knocking architecture that provides strong authentication while addressing the weaknesses of existing port knocking systems",
Improving coverage performance in sensor networks by using mobile sensors,"Sensor networks hold the promise of facilitating large-scale, real-time data processing in complex environments. Most existing researches on sensor networks consider networks where all sensors are static nodes. We propose to improve sensor network performance by deploying a small number of mobile sensors in addition to a large number of static sensors. In this paper, we present a distributed solution that utilizes a small number of mobile sensors to improve coverage performance in sensor networks. The distributed solution includes distributed schemes for: 1) determining the boundary of a coverage hole; 2) determining the number and locations of mobile sensors for covering a hole; and 3) calling mobile sensors. We design simulation experiments to evaluate the performance of three algorithms that determine the number and locations of mobile sensors for covering a hole. Our experiments show that the integer linear programming algorithm achieves the best results with the cost of high computation requirement, while the other two heuristic algorithms achieve good sub-optimal results with small computation requirement.","Intelligent networks,
Wireless sensor networks,
Costs,
Wireless communication,
Large-scale systems,
Mobile computing,
Computer science,
Neodymium,
Data processing,
Algorithm design and analysis"
"The fast Fourier transform for experimentalists, part IV: autoregressive spectral analysis","We consider two additional kinds of spectrum estimates: autoregressive (AR) estimates and the maximum entropy (ME) method. In the first approach, we assume that an AR process generates the time series, which means we can compute the PSD of the time series from estimates of the AR parameters. The second approach is a special case of the first, but it uses a different method for estimating the AR parameters. Specifically, it chooses them to make the PSD's inverse transform compatible with the measured time series, while remaining maximally noncommittal about the data outside the observational window.","Fast Fourier transforms,
Spectral analysis,
Equations,
Winches,
Physics,
Linear systems,
Symmetric matrices,
Frequency estimation,
Computer aided software engineering,
IEEE Spectrum Editorial Board"
A cost-effective main memory organization for future servers,"Today, the amount of main memory in mid-range servers is pushing practical limits with as much as 192 GB memory in a 24 processor system. Further, with the onset of multi-threaded, multi-core processor chips, it is likely that the number of memory chips per processor chip will start to increase, making DRAM cost and size an even larger burden. We investigate in this paper an alternative main memory organization - a two-level non-inclusive memory hierarchy - where the second level is substantially slower than the first level, with the aim of reducing total system cost and spatial requirements of servers of today and the future. We quantitatively investigate how big and how slow the second level can be. Surprisingly, we find that only 30% of the entire memory resources typically needed must be accessed at DRAM speed whereas the rest can be accessed at a speed that is an order of magnitude slower with a negligible (1.2% on average) performance impact. We also present a cost-effective implementation of how to manage such a hierarchy and how it can bring down memory cost by leveraging memory compression and sharing of memory resources among servers.","Computer science,
Delay,
Memory management,
Fault detection,
Distributed processing,
Technology management,
Engineering management,
Associative memory,
Operating systems,
Software algorithms"
Secure Routing with the AODV Protocol,"Ad-hoc networks, due to their improvised nature, are frequently established in insecure environments, which makes them susceptible to attacks. These attacks are launched by participating malicious nodes against different network services. Routing protocols, which act as the binding force in these networks, are a common target of these nodes. Ad-hoc on-demand distance vector (AODV) is one of the widely used routing protocols that is currently undergoing extensive research and development. AODV is based on distance vector routing, but the updates are shared not on a periodic basis but on an as per requirement basis. The control packets contain a hop-count and sequence number field that identifies the freshness of routing updates. As these fields are mutable, it creates a potential vulnerability that is frequently exploited by malicious nodes to advertise better routes. Similarly, transmission of routing updates in clear text also discloses vital information about the network topology, which is again a potential security hazard. In this paper we present a novel and pragmatic scheme for securing the ad-hoc on-demand distance vector routing protocol that protects against a number of attacks carried out against mobile ad-hoc wireless networks",
Energy reduction in multiprocessor systems using transactional memory,"The emphasis in microprocessor design has shifted from high performance, to a combination of high performance and low power. Until recently, this trend was mostly true for uniprocessors. In this work the authors focused on new energy consumption issues unique to multiprocessor systems: synchronization of accesses to shared memory. The authors investigated and compared different means of providing atomic access to shared memory, including locks and lock-free synchronization (i.e., transactional memory), with respect to energy as well as performance. It is shown that transactional memory has an advantage in terms of energy consumption over locks, but that this advantage largely depends on the system architecture, the contention level, and the policy of conflict resolution.","Multiprocessing systems,
Energy consumption,
Microprocessors,
Pressing,
Permission,
Iris,
Design engineering,
Power engineering and energy,
Computer science,
Energy resolution"
An efficient architecture for the AES mix columns operation,"In this paper, a compact architecture for the AES mix columns operation and its inverse is presented. The hardware implementation is compared with previous work done in this area. We show that our design has a lower gate count than other designs that implement both the forward and the inverse mix columns operation.",
Characterisation of optical flow anomalies in pedestrian traffic,"This paper applies a video modelling technique to a surveillance scenario where pedestrians are monitored to detect unusual events. The aim is to investigate the components of an automatic vision system capable of detecting normal and abnormal behaviour. Such a system has application in surveillance scenarios like town centre plazas, stadiums, train stations and shopping malls. Surveillance usually relies on tracking, but in crowded scenarios tracking is not reliable. Thus our framework for representation and analysis is based on optical flow to avoid tracking of individuals. We demonstrate that patterns derived from optical flow and encoded by a Hidden Markov Model are able to capture the dynamic evolution of normal behaviour allowing the classification of abnormal events.","road traffic,
image sequences,
computer vision,
hidden Markov models,
video signal processing,
surveillance,
feature extraction,
behavioural sciences computing,
gesture recognition"
Linear time construction of redundant trees for recovery schemes enhancing QoP and QoS,"Medard, Finn, Barry and Gallager proposed an elegant recovery scheme (known as the MFBG scheme) using redundant trees. Xue, Chen and Thulasiraman extended the MFBG scheme and introduced the concept of quality of protection (QoP) as a metric of multifailure recovery capabilities for single failure recovery schemes. In this paper, we present three linear time algorithms for constructing redundant trees for single link failure recovery in 2-edge connected graphs and for single node failure recovery in 2-connected graphs. Our first algorithm aims at high QoP for single link recovery schemes in 2-edge connected graphs. The previous best algorithm has a running time of O(n/sup 2/(m+n)), where n and m are the number of nodes and links in the network. Our algorithm has a running time of O(m+n), with comparable performance. Our second algorithm aims at high QoS for single link recovery schemes in 2-edge connected graphs. Our algorithm improves the previous best algorithm with O(n/sup 2/(m+n)) time complexity to O(m+n) time complexity with comparable performance. Our third algorithm aims at high QoS for single node recovery schemes in 2-connected graphs. Again, our algorithm improves the previous best algorithm with O(n/sup 2/(m+n)) time complexity to O(m+n) time complexity with comparable performance. Simulation results show that our new algorithms outperform previously known linear time algorithms significantly in terms of QoP or QoS, and outperform other known algorithms in terms of running time, with comparable QoP of QoS performance.","Protection,
Tree graphs,
Quality of service,
Computer science,
SONET,
High-speed networks,
WDM networks,
Wavelength division multiplexing,
Solids"
A quantitative comparison of reputation systems in the grid,"Reputation systems have been a hot topic in the peer-to-peer community for several years. In a services-oriented distributed computing environment like the grid, reputation systems can be utilized by clients to select between competing service providers. In this paper, we selected several existing reputation algorithms and adapted them to the problem of service selection in a grid-like environment. We performed a quantitative comparison of both the accuracy and overhead associated with these techniques under common scenarios. The results indicate that using a reputation system to guide service selection can significantly improve client satisfaction with minimal overhead. In addition, we show that the most appropriate algorithm depends on the kinds of anticipated attacks. A new algorithm we've proposed appears to be the approach of choice if clients can misreport service ratings.",
Design and implementation of message-passing services for the Blue Gene/L supercomputer,"The Blue GeneÂ®/L (BG/L) supercomputer, with 65,536 dual-processor compute nodes, was designed from the ground up to support efficient execution of massively parallel message-passing programs. Part of this support is an optimized implementation of the Message Passing Interface (MPI), which leverages the hardware features of BG/L. MPI for BG/L is implemented on top of a more basic message-passing infrastructure called the message layer. This message layer can be used both to implement other higher-level libraries and directly by applications. MPI and the message layer are used in the two BG/L modes of operation: the coprocessor mode and the virtual node mode. Performance measurements show that our message-passing services deliver performance close to the hardware limits of the machine. They also show that dedicating one of the processors of a node to communication functions (coprocessor mode) greatly improves the message-passing bandwidth, whereas running two processes per compute node (virtual node mode) can have a positive impact on application performance.",
Experiences with the KOALA co-allocating scheduler in multiclusters,"In multicluster systems, and more generally, in grids, jobs may require co-allocation, i.e., the simultaneous allocation of resources such as processors and input files in multiple clusters. While such jobs may have reduced runtimes because they have access to more resources, waiting for processors in multiple clusters and for the input files to become available in the right locations, may introduce inefficiencies. Moreover, as single jobs now have to rely on multiple resource managers, co-allocation introduces reliability problems. In this paper, we present two additions to the original design of our KOALA co-allocating scheduler (different priority levels of jobs and incrementally claiming processors), and we report on our experiences with KOALA in our multicluster testbed while it was unstable.","Resource management,
Processor scheduling,
Testing,
Distributed computing,
Mathematics,
Computer science,
Runtime,
Load management,
Fault tolerance,
Supercomputers"
Human Mental Models of Humanoid Robots,Effective communication between a person and a robot may depend on whether there exists a common ground of understanding between the two. In two experiments modelled after human-human studies we examined how people form a mental model of a robotâ€™s factual knowledge. Participants estimated the robotâ€™s knowledge by extrapolating from their own knowledge and from information about the robotâ€™s origin and language. These results suggest that designers of humanoid robots must attend not only to the social cues that robots emit but also to the information people use to create mental models of a robot.,"Cognitive science,
Humanoid robots,
Human robot interaction,
Psychology,
Interactive systems,
Human computer interaction,
Communication effectiveness,
Animals,
Computer applications,
Robotics and automation"
"Key, Chord, and Rhythm Tracking of Popular Music Recordings",,
A dynamic clustering and scheduling approach to energy saving in data collection from wireless sensor networks,,"Dynamic scheduling,
Intelligent networks,
Wireless sensor networks,
Sampling methods,
Surveillance,
Monitoring,
Temperature sensors,
Batteries,
Processor scheduling,
Computer science"
The Y architecture for on-chip interconnect: analysis and methodology,"The Y architecture for on-chip interconnect is based on pervasive use of 0/spl deg/, 120/spl deg/, and 240/spl deg/ oriented semiglobal and global wiring. Its use of three uniform directions exploits on-chip routing resources more efficiently than traditional Manhattan wiring architecture. This paper gives in-depth analysis of deployment issues associated with the Y architecture. Our contributions are as follows. 1) We analyze communication capability (throughput of meshes) for different interconnect architectures using a multicommodity flow approach and a Rentian communication model. Throughput of the Y architecture is largely improved compared to the Manhattan architecture, and is close to the throughput of the X architecture. 2) We improve existing estimates for the wirelength reduction of various interconnect architectures by taking into account the effect of routing-geometry-aware placement. 3) We propose a symmetrical Y clock tree structure with better total wire length compared to both H and X clock tree structures, and better path length compared to the H tree. 4) We discuss power distribution under the Y architecture, and give analytical and SPICE simulation results showing that the power network in Y architecture can achieve (8.5%) less IR drop than an equally resourced power network in Manhattan architecture. 5) We propose the use of via tunnels and banks of via tunnels as a technique for improving routability for Manhattan and Y architectures.","Clocks,
Throughput,
Computer science,
Wiring,
Routing,
Tree data structures,
Power distribution,
Very large scale integration,
Integrated circuit interconnections,
Wire"
Functional imaging in small animals using X-ray computed Tomography -study of physiologic measurement reproducibility,"X-ray computed tomography (CT) has been traditionally used for morphologic analysis and in the recent past has been used for physiology imaging. This paper seeks to demonstrate functional CT as an effective tool for monitoring changes in tissue physiology associated with disease processes and cellular and molecular level therapeutic processes. We investigated the effect of noise and sampling time on the uncertainty of tissue physiologic parameters. A whole body compartmental model of mouse was formulated to simulate tissue time density curves and study the deviation of tissue physiologic parameters from their true values. These results were then used to determine the appropriate scanning protocols for the experimental studies. Dynamic contrast enhanced CT (DCE-CT) was performed in mice following the injection of hydrophilic iodinated contrast agent (CA) at three different injection rates, namely 0.5 ml/min, 1 ml/min, and 2.0 ml/min. These experiments probed the Nyquist sampling limit for reproducibility of tissue physiologic parameters. Separate experiments were performed with three mice at four different X-ray tube currents corresponding to different image noise values. A two-compartment model (2CM) model was formulated to describe the contrast kinematics in the kidney cortex. Three different 2CMs were implemented namely the 4-parameter (4P), 5-parameter (5P), and the 6-parameter (6P) model. The tissue kinematics is fitted to the models by using the Levenberg-Marquardt algorithm implemented in IDL (RSI Inc.) programming language to minimize the weighted sum of squares. The relevant tissue physiologic parameters extracted from the models are the renal blood flow (RBF), glomerular filtration rate (GFR), fractional plasma volume, fractional tubular volumes and urine formation rates. The experimental results indicate that the deviation of the tissue physiologic parameters is within the limits required for tracking disease physiology in vivo and thus small animal functional X-ray CT would be able to determine changes in tissue physiology in vivo.","X-ray imaging,
Optical imaging,
Animals,
Computed tomography,
Reproducibility of results,
Biomedical monitoring,
Physiology,
Brain modeling,
Mice,
Diseases"
PhotoStudy: vocabulary learning and collaboration on fixed & mobile devices,"PhotoStudy is a system that supports vocabulary study on both wired and wireless devices. It is designed to make it simple to annotate content with multimedia such as images and audio recorded on these devices. This paper presents a prototype system that uses wireless markup languages and Java MIDlets. User evaluations have been conducted, and are being continued in our iterative design approach. We report the results from questionnaire evaluations, observational studies and interviews.","Vocabulary,
Collaboration,
Prototypes,
Image generation,
Collaborative work,
Feedback,
Computer science,
Markup languages,
Java,
Iterative methods"
On static and dynamic partitioning behavior of large-scale networks,"In this paper, we analyze the problem of network disconnection in the context of large-scale P2P networks and understand how both static and dynamic patterns of node failure affect the resilience of such graphs. We start by applying classical results from random graph theory to show that a large variety of deterministic and random P2P graphs almost surely (i.e., with probability 1-o(1)) remain connected under random failure if and only if they have no isolated nodes. This simple, yet powerful, result subsequently allows us to derive in closed-form the probability that a P2P network develops isolated nodes, and therefore partitions, under both types of node failure. We finish the paper by demonstrating that our models match simulations very well and that dynamic P2P systems are extremely resilient under node churn as long as the neighbor replacement delay is much smaller than the average user lifetime","Large-scale systems,
Resilience,
Failure analysis,
Delay,
Performance analysis,
Computer science,
Pattern analysis,
Graph theory,
Power system modeling,
Explosions"
SOLAR: sound object localization and retrieval in complex audio environments,"The ability to identify sounds in complex audio environments is highly useful for multimedia retrieval, security, and many mobile robotic applications, but very little work has been done in this area. We present the SOLAR system, a system capable of finding sound objects, such as dog barks or car horns, in complex audio data extracted from movies. SOLAR avoids the need for segmentation by scanning over the audio data in fixed increments and classifying each short audio window separately. SOLAR employs boosted decision tree classifiers to select suitable features for modeling each sound object and to discriminate between the object of interest and all other sounds. We demonstrate the effectiveness of our approach with experiments on thirteen sound object classes trained using only tens of positive examples and tested on hours of audio data extracted from popular movies.","Object detection,
Gunshot detection systems,
Mobile robots,
Motion pictures,
Data mining,
Music,
Background noise,
Computer science,
Computer security,
Data security"
Ontology meta-model for building a situational picture of catastrophic events,"The overall goal of the research described in this paper is to design a general methodology for situation assessment to support crisis management. The purpose of situation assessment is to produce contextual understanding and interpretation of the relationships between various entities, events and behaviors of interest. One of the main challenges of designing a situation assessment process is to provide a formal structure for ontological analyses of domain-specific types of entities, attributes, situations, and the relationships between them. This paper presents an attempt to confront this challenge by utilizing formal philosophical categories and theories to design a formal ontology of catastrophic events that describe the most basic and relevant structures of objective reality. The ontology is designed from both a top-down philosophical perspective (from abstract level to domain-specific level) and a bottom-up application-based perspective (from domain-specific level to abstract level). Situations are characterized by spatial items of interest (SNAP) at different levels of granularity (objects, aggregates, combination of aggregates), temporal items of interest (SPAN) that characterize the behaviors of SNAP items, and the relations between them.","Ontologies,
Crisis management,
Aggregates,
Buildings,
Cognitive science,
Educational institutions,
Design methodology,
Earthquakes,
OWL,
Computer science"
Enabling DRM-preserving digital content redistribution,"Traditionally, the process of online digital content distribution has involved a limited number of centralised distributors selling protected contents and licenses authorising the use of these contents, to consumers. In this paper, we extend this model by introducing a security scheme that enables DRM preserving digital content redistribution. Essentially consumers can not only buy the rights to use digital content but also the rights to redistribute it to other consumers in a DRM controlled fashion. We examine the threats associated with such a redistribution model and explain how our scheme addresses them.","Protection,
Peer to peer computing,
Licenses,
Manufacturing,
Security,
Consumer electronics,
Power system modeling,
Computer science,
Communication system control,
Content management"
Handling Uncertainty in Controllers Using Type-2 Fuzzy Logic,"Uncertainty is an inherent part in controllers used for real-world applications. The use of new methods for handling incomplete information is of fundamental importance in engineering applications. This paper deals with the design of controllers using type-2 fuzzy logic for minimizing the effects of uncertainty produced by the instrumentation elements. We simulated type-1 and type-2 fuzzy logic controllers to perform a comparative analysis of the systems' response, in the presence of uncertainty","Uncertainty,
Fuzzy logic,
Fuzzy sets,
Control systems,
Instruments,
Computer science,
Computer errors,
Application software,
Analytical models,
Performance analysis"
Assessing the Effectiveness of Remote Networking Laboratories,"The Interdisciplinary Telecommunications Program at the University of Colorado has developed an Internet based remote laboratory environment for master's level graduate students; our suite of telecommunications experiments substantially extends prior work focused on networking equipment, by (1) providing a systems focus, (2) enabling multiple reinforcing methods of accessing the educational material, (3) providing a configuration matrix to support realtime network reconfigurations (of real network elements), and (4) undertaking a careful assessment of the learning environment. The goal was to create an environment that reproduced (not just emulated) the lab experience. We recently completed the final phase of this project focusing on assessment of this learning tool; such assessment is still rare in the literature on remote laboratories. We describe the project from three perspectives; students' exam results, students' lab reports, and students' satisfaction with the distance experience (based on interviews). We conclude that our remote laboratories provide similar learning outcomes to their in class analogues, but that there are important differences in student perceptions of the experience, including perceived difficulty and pace","Remote laboratories,
Internet,
Telecommunication control,
IP networks,
Costs,
Engineering education,
Instruments,
Software tools,
Programming,
Computer science"
Extracting sequence diagram from execution trace of Java program,"A software system is changed many times. When we try to change software, we must understand how the software is implemented, especially the functions to be modified. However, such repeated changes may cause situations in which there is no document which reflects the changes and represents the behavior of the software correctly. So, it is important to develop a technique to extract useful information to understand the behavior of the software. We propose a method to extract compact sequence diagrams from dynamic information of object-oriented programs. Our method generates sequence diagrams by compacting a repetition included in the execution trace. This paper presents four compaction rules. The experiment illustrates how our rules effectively compact the execution trace and generate compact sequence diagrams.","Java,
Compaction,
Data mining,
Software maintenance,
Information science,
Software systems,
Software debugging,
Computer bugs,
Conferences"
Detecting and visualizing refactorings from software archives,"We perform knowledge discovery in software archives in order to detect refactorings on the level of classes and methods. Our REFVIS prototype finds these refactorings in CVS repositories and relates them to transactions and configurations. Additionally, REFVIS relates movements of methods to the class inheritance hierarchy of the analyzed project. Furthermore, we present our visualization technique that illustrates these refactorings. REFVIS provides both a class hierarchy layout and a package layout and uses color coding to distinguish different kinds of refactorings. Details on each can be displayed on demand using mouse-over tooltips. Finally, we demonstrate by case studies on two open source projects how REFVIS facilitates understanding of refactorings applied to a software project.","Visualization,
Open source software,
Programming profession,
Data mining,
Computer science,
Packaging,
Software performance,
Software prototyping,
Prototypes,
Software systems"
Maximal causality analysis,"Perfectly synchronous systems immediately react to the inputs of their environment, which may lead to so-called causality cycles between actions and their trigger conditions. Algorithms to analyze the consistency of such cycles usually extend data types by an additional value to explicitly indicate unknown values. In particular, Boolean functions are thereby extended to ternary functions. However, a Boolean function usually has several ternary extensions, and the result of the causality analysis depends on the chosen ternary extension. In this paper, we show that there always is a maximal ternary extension that allows one to solve as many causality problems as possible. Moreover, we elaborate the relationship to hazard elimination in hardware circuits, and finally show how the maximal ternary extension of a Boolean function can be efficiently computed by means of binary decision diagrams.","Boolean functions,
Equations,
Hardware,
Algorithm design and analysis,
Multivalued logic,
Logic circuits,
Delay,
Computer science,
Hazards,
Data structures"
A robust testing framework for verifying Web services by completeness and consistency analysis,This paper presents a specification based robust testing framework for Web services. Web services testing is done by 1) extracting condition and event combinations from the Web service specification; 2) ensuring that these combinations are consistent with each other; 3) performing completeness analysis to identify all the missing condition and event combinations; 4) identifying the locations where updates are needed to maintain the completeness and consistency; and 5) emphasizing on robustness testing by generating positive as well as negative test cases. An efficient algorithm called Covering Scenario Generation is proposed to identify the locations where incompleteness and inconsistency exist. The algorithm is based on the min-terms of Boolean expressions that combine multiple conditions into a single checkable item. The proposed algorithm has been experimented with several large industrial applications and the results indicated that the proposed algorithm is robust and scalable to large applications. A case study is designed to illustrate the design and testing process.,"Robustness,
Web services,
Simple object access protocol,
System testing,
Computer science,
Performance evaluation,
Performance analysis,
Process design,
Telephony,
Books"
How well do professional developers test with code coverage visualizations? An empirical study,"Despite years of availability of testing tools, professional software developers still seem to need better support to determine the effectiveness of their tests. Without improvements in this area, inadequate testing of software seems likely to remain a major problem. To address this problem, industry and researchers have proposed systems that visualize ""testedness"" for end-user and professional developers. Empirical studies of such systems for end-user programmers have begun to show success at helping end users write more effective tests. Encouraged by this research, we examined the effect that code coverage visualizations have on the effectiveness of test cases that professional software developers write. This paper presents the results of an empirical study conducted using code coverage visualizations found in a commercially available programming environment. Our results reveal how this kind of code coverage visualization impacts test effectiveness, and provide insights into the strategies developers use to test code.","Visualization,
Software testing,
Computer science,
Software tools,
Programming profession,
System testing,
Programming environments,
Software engineering,
NIST,
Costs"
A Global Observer for Observable Autonomous Systems with Bounded Solution Trajectories,"The problem of global observer design for autonomous systems is investigated in this paper. A constructive approach is presented for the explicit design of global observers for completely observable systems whose solution trajectories are bounded from any initial condition. Since the bound of a solution trajectory depends on the initial condition and is therefore not known a priori, the idea of universal control is employed to tune the observer gains on-line, achieving global asymptotic convergence of the proposed high-gain observer.","Observers,
Nonlinear systems,
Linear systems,
State estimation,
Observability,
Laboratories,
Design methodology,
Linearization techniques,
Nonlinear equations"
Secure address auto-configuration for mobile ad hoc networks,"Auto-configuration provides convenience in implementing a mobile ad hoc network. However, it may sacrifice network security if there is not a reliable authentication mechanism, e.g., an attacker may spoof other nodes and hijack their traffic. In this paper, we propose a secure address auto-configuration scheme for MANET. It binds each IP address with a public key, allows a node to self-authenticate itself and thus thwarts address spoofing and other attacks associated with auto-configuration.",
A grid portal for an undergraduate parallel programming course,"This paper describes an experience of designing and implementing a portal to support transparent remote access to supercomputing facilities to students enrolled in an undergraduate parallel programming course. As these facilities are heterogeneous, are located at different sites, and belong to different institutions, grid computing technologies have been used to overcome these issues. The result is a grid portal based on a modular and easily extensible software architecture that provides a uniform and user-friendly interface for students to work on their programming laboratory assignments.","Computer science education,
User interfaces"
Biclustering of Gene Expression Data Using Genetic Algorithm,The biclustering problem of gene expression data deals with finding a subset of genes which exhibit similar expression patterns along a subset of conditions. Most of the current algorithms use a statistically predefined threshold as an input parameter for biclustering. This threshold defines the maximum allowable dissimilarity between the cells of a bicluster and is very hard to determine beforehand. Hence we propose two genetic algorithms that embed greedy algorithm as local search procedure and find the best biclusters independent of this threshold score. We also establish that the HScore of a bicluster under the additive model approximately follows chi-square distribution. We found that these genetic algorithms outperformed other greedy algorithms on yeast and lymphoma datasets.,"Gene expression,
Genetic algorithms,
Interference,
Iterative algorithms,
Greedy algorithms,
Fungi,
Clustering algorithms,
Evolutionary computation,
DNA,
Data analysis"
Improving soft real-time performance through better slack reclaiming,"Modern operating systems frequently support applications with a variety of timing constraints including hard real-time, soft real-time, and best-effort. To guarantee performance, critical applications typically over-reserve resources based on worst-case resource usage estimates, while others may reserve based on average-case or other estimates. When resources are fully subscribed, the performance of soft- and non-real-time applications depends upon the effective distribution of dynamic slack - reserved, but unused resources - from other tasks. Motivated by several representative examples, we derive four general principles for the effective management of slack. We have implemented these principles in four progressively better slack schedulers that demonstrate their effectiveness. BACKSLASH, which employs all four principles, misses fewer soft realtime deadlines than all of the other slack schedulers we examined","Real time systems,
Processor scheduling,
Operating systems,
Application software,
Computer science,
Timing,
Computer applications,
Time factors,
High level synthesis,
Power system modeling"
In-home online entertainment: analyzing the impact of the wireless MAC-transport protocols interference,"Online entertainment is now possible at home by means of a plethora of ubiquitous services that can be provided based on wireless technologies such as, for instance, the common Wi-Fi IEEE802.11g network technology. Yet a scarce attention has been devoted to study the impact of wireless MAC layer design choices on the native Internet transport protocols (e.g., TCP, UDP) during the distribution of in-house entertainment contents. We show that the current setting of the maximum number of retransmissions and the queue size in the IEEE802.11 protocol not necessarily corresponds to the optimal choice in this context. Based on the obtained results, we also provide directions in order to find the most efficient MAC layer setting.","Wireless application protocol,
Interference,
Internet,
Transport protocols,
Home automation,
Media Access Protocol,
Wireless LAN,
Communication system security,
TCPIP,
Computer science"
Making use of what you don't see: negative information in Markov localization,"This paper explores how the absence of an expected sensor reading can be used to improve Markov localization. This negative information usually is not being used in localization, because it yields less information than positive information (i.e. sensing a landmark), and a sensor often fails to detect a landmark, even if it falls within its sensing range. We address these difficulties by carefully modeling the sensor to avoid false negatives. This can also be thought of as adding an additional sensor that detects the absence of an expected landmark. We show how such modeling is done and how it is integrated into Markov localization. In real world experiments, we demonstrate that a robot is able to localize in positions where otherwise it could not and quantify our findings using the entropy of the particle distribution. Exploiting negative information leads to a greatly improved localization performance and reactivity.","Robot sensing systems,
Mobile robots,
Monte Carlo methods,
Entropy,
Computer science,
Artificial intelligence,
Laboratories,
Intelligent sensors,
Navigation,
Position measurement"
Streamers and diffuse glow observed in upper atmospheric electrical discharges,"A telescopic imaging system was designed to observe fine structure in high-altitude lightning-related electrical discharges known as ""sprites"". Hundreds of sprites were observed and much of the fine structure can be categorized in terms of streamer and diffuse glow discharges. A streamer/diffuse glow transition occurs at varying altitudes and is dependent on the ambient mesospheric/lower ionospheric conductivity profile. Both stationary and dynamic beading is observed and this feature is currently unexplained by prevailing sprite theories.","Sprites (computer),
Streaming media,
Cameras,
Telescopes,
Charge-coupled image sensors,
Glow discharges,
Conductivity,
Lightning,
Ionosphere,
High-resolution imaging"
Routing with congestion awareness and adaptivity in mobile ad hoc networks,"Mobility, channel error, and congestion are the main causes for packet loss in mobile ad hoc networks. Reducing packet loss typically involves congestion control operating on top of a mobility and failure adaptive routing protocol at the network layer. In current designs, routing is not congestion-adaptive. Routing may let congestion happen, which is detected by congestion control, but dealing with congestion in this reactive manner results in longer delay and unnecessary packet loss and requires significant overhead if a new route is needed. This problem becomes especially more visible in large-scale transmission of heavy traffic such as multimedia data, where congestion is more probable and the negative impact of packet loss on the service quality is more significant. We argue that routing should not only be aware of, but also be adaptive to, network congestion. We propose a routing protocol with such properties.","Intelligent networks,
Mobile ad hoc networks,
Routing protocols,
Delay,
Communication system traffic control,
Programmable control,
Adaptive control,
Collaborative work,
Computer science,
Computer errors"
Interference-aware topology control for wireless sensor networks,,"Interference,
Network topology,
Wireless sensor networks,
Communication system control,
Routing protocols,
Energy consumption,
Media Access Protocol,
Mobile ad hoc networks,
Computer science,
Information geometry"
Automatic detection and rating of dementia of Alzheimer type through lexical analysis of spontaneous speech,"Current methods of assessing dementia of Alzheimer type (DAT) in older adults involve structured interviews that attempt to capture the complex nature of deficits suffered. One of the most significant areas affected by the disease is the capacity for functional communication as linguistic skills break down. These methods often do note capture the true nature of language deficits in spontaneous speech. We address this issue by exploring novel automatic and objective methods for diagnosing patients through analysis of spontaneous speech. We detail several lexical approaches to the problem of detecting and rating DAT. The approaches explored rely on character n-gram-based techniques, shown recently to perform successfully in a different, but related task of automatic authorship attribution. We also explore the correlation of usage frequency of different parts of speech and DAT. We achieve a high 95% accuracy of detecting dementia when compared with a control group, and we achieve 70% accuracy in rating dementia in two classes, and 50% accuracy in rating dementia into four classes. Our results show that purely computational solutions offer a viable alternative to standard approaches to diagnosing the level of impairment in patients. These results are significant step forward toward automatic and objective means to identifying early symptoms of DAT in older adults.","Speech analysis,
Dementia,
Alzheimer's disease,
Machine learning,
Natural language processing,
Text categorization,
Electric breakdown,
Computer science,
Medical diagnostic imaging,
Application specific processors"
Design of a reversible binary coded decimal adder by using reversible 4-bit parallel adder,"In this paper, we have proposed a design technique for the reversible circuit of binary coded decimal (BCD) adder. The proposed circuit has the ability to add two 4-bits binary variables and it transforms the addition into the appropriate BCD number with efficient error correcting modules where the operations are reversible. We also show that the proposed design technique generates the reversible BCD adder circuit with minimum number of gates as well as the minimum number of garbage outputs.","Adders,
Circuit synthesis,
Logic gates,
Logic devices,
Logic circuits,
DH-HEMTs,
Temperature,
Computer science,
Error correction,
Energy dissipation"
TreeP: A Tree Based P2P Network Architecture,In this paper we proposed a hierarchical P2P network based on a dynamic partitioning on a 1-D space. This hierarchy is created and maintained dynamically and provides a grid middleware (like DGET) a P2P basic functionality for resource discovery and load-balancing. This network architecture is called TreeP (Tree based P2P network architecture) and is based on a tessellation of a 1-D space. We show that this topology exploits in an efficient way the heterogeneity feature of the network while limiting the overhead introduced by the overlay maintenance. Experimental results show that this topology is highly resilient to a large number of network failures,
Multiscale Permutation Entropy of Physiological Time Series,"Time series derived from simpler systems are single scale based and thus can be quantified by using traditional measures of entropy. However, times series derived from physical and biological systems are complex and show structures on multiple spatio-temporal scales. Traditional approaches of entropy based complexity measures fail to account for multiple scales inherent in these time series. Recently multi-scale entropy (MSE) method was introduced, which provide a way to measure complexity over a range of scales. MSE method uses sample entropy, a refinement of approximate entropy to quantify the complexity of time series. Nonstationarity, outliers and artifacts affect the sample entropy values because they change time series standard deviation and therefore, the value of similarity criterion. In this paper, we have used permutation entropy for quantifying the complexity, which is useful in the presence of dynamical and observational noise. We called this modified procedure multiscale permutation entropy (MPE). We observed that MPE is robust in presence of artifacts and robustly separates pathological and healthy groups","Entropy,
Time measurement,
Noise robustness,
Legged locomotion,
Biological systems,
Spatiotemporal phenomena,
Pathology,
Humans,
Signal generators,
Degradation"
Context-awareness and personalisation in the Daidalos pervasive environment,"Context-awareness and personalisation are important concepts that are essential cornerstones in future systems providing pervasive services. These two concepts are closely interrelated and dependent on each other for fully functional context-aware personalised services. Daidalos is a European research project in the area of 3G and beyond, which aims to combine heterogeneous networks in a transparent and seamless way, and develop a pervasive environment for applications and end-users on top of this. The first phase of implementation has produced a basic system that is currently being developed further. This paper outlines the basic pervasive service platform architecture and describes the context-awareness and personalisation features that are being implemented in Daidalos.","Pervasive computing,
Context-aware services,
Context awareness,
Computer science,
Computer architecture,
Mobile computing,
Distributed computing,
Mobile communication,
Fabrics,
Computer networks"
Supporting the evolution of a software visualization tool through usability studies,"The paper presents a usability study conducted with graduate and undergraduate computer science students, designed to evaluate the effectiveness of a software visualization tool named sv3D, and to provide necessary user data for the evolution of the system. Sv3D is a software visualization tool for comprehension of large software, capable of displaying source code and associated metrics in three dimensions. The participants in the study answered two types of questions: one set provided objective measurements to support the formulated hypotheses with respect to the accuracy and speed of the users answering questions using sv3D; the second set of questions provided subjective measurements that were used to support the evolution of sv3D. We formulated two null hypotheses with respect to accuracy and time respectively. The collected data supported one hypothesis and rejected the other.","Software tools,
Usability,
Data visualization,
Software performance,
Computer science,
Software systems,
Velocity measurement,
Particle measurements,
Software algorithms,
Animation"
Evolving Evolutionary Algorithms Using Linear Genetic Programming,"A new model for evolving Evolutionary Algorithms is proposed in this paper. The model is based on the Linear Genetic Programming (LGP) technique. Every LGP chromosome encodes an EA which is used for solving a particular problem. Several Evolutionary Algorithms for function optimization, the Traveling Salesman Problem and the Quadratic Assignment Problem are evolved by using the considered model. Numerical experiments show that the evolved Evolutionary Algorithms perform similarly and sometimes even better than standard approaches for several well-known benchmarking problems.","evolving evolutionary algorithms,
Genetic algorithms,
genetic programming,
linear genetic programming"
A characterization of the (natural) graph properties testable with one-sided error,"The problem of characterizing all the testable graph properties is considered by many to be the most important open problem in the area of property-testing. Our main result in this paper is a solution of an important special case of this general problem; Call a property tester oblivious if its decisions are independent of the size of the input graph. We show that a graph property P has an oblivious one-sided error tester, if and only if P is (semi) hereditary. We stress that any ""natural"" property that can be tested (either with one-sided or with two-sided error) can be tested by an oblivious tester In particular, all the testers studied thus far in the literature were oblivious. Our main result can thus be considered as a precise characterization of the ""natural"" graph properties, which are testable with one-sided error. One of the main technical contributions of this paper is in showing that any hereditary graph property can be tested with one-sided error. This general result contains as a special case all the previous results about testing graph properties with one-sided error. These include the results of Goldreich et al., [1998] about testing k-colorability, the characterization of Goldreich and Trevisan [2001] of the graph-partition problems that are testable with 1-sided error, the induced vertex colorability properties of Alon et al., [2000], the induced edge colorability properties of Fischer [2001], a transformation from 2-sided to 1-sided error testing [Goldreich and Trevisan, 2001], as well as a recent result about testing monotone graph properties [Alon and Shapira, 2005]. More importantly, as a special case of our main result, we infer that some of the most well studied graph properties, both in graph theory and computer science, are testable with one-sided error. Some of these properties are the well known graph properties of being perfect, chordal, interval, comparability and more. None of these properties was previously known to be testable.","Testing,
Computer errors,
Graph theory,
Computer science,
Geometry,
Algorithm design and analysis,
Linearity"
Delayed visual and haptic feedback in a reciprocal tapping task,"To make optimal use of distributed virtual environments (DVEs), we must understand and quantify the effects of latency on user performance. The current study investigates whether delaying haptic, and/or visual feedback in a reciprocal tapping task impairs performance or makes the task appear more difficult to the user. Results show haptic latency has a small effect on performance, but is considerably less disruptive than visual lag.",
Common pattern discovery using earth mover's distance and local flow maximization,"In this paper, we present a novel segmentation-insensitive approach for mining common patterns from 2 images. We develop an algorithm using the earth movers distance (EMD) framework, unary and adaptive neighborhood color similarity. We then propose a novel local flow maximization approach to provide the best estimation of location and scale of the common pattern. This is achieved by performing an iterative optimization in search of the most stable flows' centroid. Common pattern discovery is difficult owing to the huge search space and problem domain. We intend to solve this problem by reducing the search space through identifying the location and a reduced spatial space for common pattern discovery. Experimental results justify the effectiveness and the potential of the approach","Earth,
Image segmentation,
Image databases,
Visual databases,
Computer science,
Iterative algorithms,
Digital images,
Spatial databases,
Data mining,
Indexing"
Game Theoretic Control for Robot Teams,"In the real world, noisy sensors and limited communication make it difficult for robot teams to coordinate in tightly coupled tasks. Team members cannot simply apply single-robot solution techniques for partially observable problems in parallel because they do not take into account the recursive effect that reasoning about the beliefs of others has on policy generation. Instead, we must turn to a game theoretic approach to model the problem correctly. Partially observable stochastic games (POSGs) provide a solution model for decentralized robot teams, however, this model quickly becomes intractable. In previous work we presented an algorithm for lookahead search in POSGs. Here we present an extension which reduces computation during lookahead by clustering similar observation histories together. We show that by clustering histories which have similar profiles of predicted reward, we can greatly reduce the computation time required to solve a POSG while maintaining a good approximation to the optimal policy. We demonstrate the power of the clustering algorithm in a real-time robot controller as well as for a simple benchmark problem.","Game theory,
Robot control,
Robot kinematics,
Robot sensing systems,
Orbital robotics,
Mobile communication,
Stochastic processes,
Clustering algorithms,
History,
Bandwidth"
Towards a flow analysis for embedded system C programs,"Reliable program worst-case execution time (WCET) estimates are a key component when designing and verifying real-time systems. One way to derive such estimates is by static WCET analysis methods, relying on mathematical models of the software and hardware involved. This paper describes an approach to static flow analysis for deriving information on the possible execution paths of C programs. This includes upper bounds for loops, execution dependencies between different code parts and safe determination of possible pointer values. The method builds upon abstract interpretation, a classical program analysis technique, which is adopted to calculate flow information and to handle the specific properties of the C programming language.","Embedded system,
Real time systems,
Hardware,
Upper bound,
Timing,
Mathematical model,
Information analysis,
Power system reliability,
Performance analysis,
Computer science"
Cell Planning of 4G Cellular Networks: Algorithmic Techniques and Results,Optimal planning of cellular networks is one of the most fundamental problems in network design. This paper discusses cell planning problems for the fourth generation (4G) cellular networks. We describe important planning aspects derived by the anticipated 4G technologies and model several relevant cell planning problems. We then review several algorithmic techniques for solving these problems.,"Cell Planning Algorithms,
4G Cellular Networks"
LiMIC: support for high-performance MPI intra-node communication on Linux cluster,"High performance intra-node communication support for MPI applications is critical for achieving best performance from clusters of SMP workstations. Present day MPI stacks cannot make use of operating system kernel support for intra-node communication. This is primarily due to the lack of an efficient, portable, stable and MPI friendly interface to access the kernel functions. In this paper we attempt to address design challenges for implementing such a high performance and portable kernel module interface. We implement a kernel module interface called LiMIC and integrate it with MVAPICH, an open source MPI over InfiniBand. Our performance evaluation reveals that the point-to-point latency can be reduced by 71% and the bandwidth improved by 405% for 64 KB message size. In addition, LiMIC can improve HPCC effective bandwidth and NAS IS class B benchmarks by 12% and 8%, respectively, on an 8-node dual SMP InfiniBand cluster.","Linux,
Kernel,
Bandwidth,
Message passing,
Delay,
Operating systems,
Computer science,
Application software,
Workstations,
Local area networks"
Who Joins the Platform? The Case of the RFID Business Ecosystem,"Today, many knowledge-based technology applications form a business ecosystem: a set of complex products and services made by multiple firms in which no firm is dominant. For this paper the emerging radio frequency ID (RFID) ecosystem was built based on firms' alliance announcements, and propositions around the behavior of large, multi-line technology firms in this network were analyzed. The RFID network is used to empirically show that absorptive capacity, and exploration vs. exploitation theories may explain some behavior of large firms. Specifically, a propensity to form alliances in general makes it more likely large firms will join the RFID ecosystem, and more exploratory firms join earlier. Greater availability of slack resources also leads to the formation of more alliances in the network. The ecosystem perspective and these results may influence alliance decisions of firms entering into high cost technological innovations.","Computer aided software engineering,
Radiofrequency identification,
Ecosystems,
Radio frequency,
IEEE news,
RFID tags,
Application software,
Manufacturing,
Space technology,
Management information systems"
Localized generalization error and its application to RBFNN training,"The generalization error bounds for the entire input space found by current error models using the number of effective parameters of a classifier and the number of training samples are usually very loose. But classifiers such as SVM, RBFNN and MLPNN, are really local learning machines used for many application problems, which consider unseen samples close to the training samples more important. In this paper, we propose a localized generalization error model which bounds above the generalization error within a neighborhood of the training samples using stochastic sensitivity measure (expectation of the squared output perturbations). It is then used to develop a model selection technique for a classifier with maximal coverage of unseen samples by specifying a generalization error threshold. Experiments by using eight real world datasets show that, in comparison with cross-validation, sequential learning, and two other ad-hoc methods, our technique consistently yields the best testing classification accuracy with fewer hidden neurons and less training time.","Analytical models,
Machine learning,
Support vector machines,
Support vector machine classification,
Neural networks,
Computer errors,
Pattern classification,
EMP radiation effects,
Mathematics,
Computer science"
A metamorphic approach to integration testing of context-sensitive middleware-based applications,"During the testing of context-sensitive middleware-based software, the middleware identifies the current situation and invokes the appropriate functions of the applications. Since the middleware remains active and the situation may continue to evolve, however, the conclusion of some test cases may not be easily identified. Moreover, failures appearing in one situation may be superseded by subsequent correct outcomes and may, therefore, be hidden. We alleviate the above problems by making use of a special kind of situation, which we call checkpoints, such that the middleware will not activate the functions under test. We propose to generate test cases that start at a checkpoint and end at another. We identify functional relations that associate different execution sequences of a test case. Based on a metamorphic approach, we check the results of the test case to detect any contravention of such relations. We illustrate our technique with an example that shows how re-hidden failures may be detected.","Application software,
Context,
Middleware,
Software testing,
Ubiquitous computing,
Appropriate technology,
Software quality,
Logic,
Councils,
Computer science"
A stochastic model for studying the laminar structure of cortex from MRI,"The human cerebral cortex is a laminar structure about 3 mm thick, and is easily visualized with current magnetic resonance (MR) technology. The thickness of the cortex varies locally by region, and is likely to be influenced by such factors as development, disease and aging. Thus, accurate measurements of local cortical thickness are likely to be of interest to other researchers. We develop a parametric stochastic model relating the laminar structure of local regions of the cerebral cortex to MR image data. Parameters of the model include local thickness, and statistics describing white, gray and cerebrospinal fluid (CSF) image intensity values as a function of the normal distance from the center of a voxel to a local coordinate system anchored at the gray/white matter interface. Our fundamental data object, the intensity-distance histogram (IDH), is a two-dimensional (2-D) generalization of the conventional 1-D image intensity histogram, which indexes voxels not only by their intensity value, but also by their normal distance to the gray/white interface. We model the IDH empirically as a marked Poisson process with marking process a Gaussian random field model of image intensity indexed against normal distance. In this paper, we relate the parameters of the IDH model to the local geometry of the cortex. A maximum-likelihood framework estimates the parameters of the model from the data. Here, we show estimates of these parameters for 10 volumes in the posterior cingulate, and 6 volumes in the anterior and posterior banks of the central sulcus. The accuracy of the estimates is quantified via Cramer-Rao bounds. We believe that this relatively crude model can be extended in a straightforward fashion to other biologically and theoretically interesting problems such as segmentation, surface area estimation, and estimating the thickness distribution in a variety of biologically relevant contexts.","Stochastic processes,
Brain modeling,
Magnetic resonance imaging,
Cerebral cortex,
Histograms,
Parameter estimation,
Humans,
Visualization,
Magnetic resonance,
Diseases"
Securing P2P networks using peer reputations: is there a silver bullet?,"Peer reputations have been used as security tools not only to motivate peers against cheating but also to protect good peers from the chronic cheaters. Although the reputation management techniques are not confined to P2P networks, they present novel challenges that were absent in central server based distributed systems. We enumerate these challenges and survey the solutions proposed by the community to counter them. These challenges include, but are not limited to, peer-identification in decentralized environments, reputation metrics, storage and exchange of reputation data. Finally we survey the applications which use P2P network paradigm and therefore can benefit from the reputation systems.","Silver,
Peer to peer computing,
Ad hoc networks,
Computer science,
Centralized control,
Routing,
Computer security,
Protection,
Computer network management,
Network servers"
Control over a Bandwidth Limited Signal to Noise Ratio constrained Communication Channel,"Stabilisability of a minimum phase unstable continuous plant is studied under the presence of a bandwidth limited and Signal to Noise Ratio constrained communication link. The problem is addressed in two different ways: first through the use of an LTI filter explicitly modelling the bandwidth limitation, and in second place, for the case of one real unstable pole, through the Poisson Integral Formula and design attenuation requirements on the power outside the assigned bandwidth. Results show that when a bandwidth limitation is in existence this increases the minimum value of Signal to Noise Ratio required for stabilisability. An example is used to study both approaches.","Communication system control,
Bandwidth,
Signal to noise ratio,
Communication channels,
Delay effects,
Output feedback,
AWGN channels,
State feedback,
Filters,
Attenuation"
Event count automata: a state-based model for stream processing systems,"Recently there has been a growing interest in models and methods targeted towards the (co)design of stream processing applications; e.g. those for audio/video processing. Streams processed by such applications tend to be highly bursty and exhibit a high data-dependent variability in their processing requirements. As a result, classical event and service models such as periodic, sporadic, etc. can be overly pessimistic when dealing with such applications. In this paper, we present a new model called event count automata (ECA) for capturing the timing properties of such streams. Our model can be used to cleanly formulate properties relevant to stream processing on heterogeneous multiprocessor architectures, such as buffer overflow/underflow constraints. It can also provide the basis for developing analysis methods to compute delay/timing properties of the processed streams under different scheduling policies. Our ECAs, though similar in flavor to timed and hybrid automata, have a different semantics, are more light-weight, and are specifically suited for modeling stream processing applications and architectures. We present the basic aspects of this model and illustrate its modeling potential. We then apply it in a specific stream processing setting and develop an analysis technique based on the formalism of colored Petri nets (CPNs). Finally, we validate our modeling and analysis techniques with the help of preliminary experimental results generated using the CPN simulation tool","Automata,
Streaming media,
Application software,
Computer architecture,
Delay,
Computer science,
Timing,
Processor scheduling,
Decoding,
Electronic mail"
A heuristic scheduling strategy for independent tasks on grid,"Task scheduling is an NP-complete problem and is an integral part of parallel and distributed computing. It is more complicated under the grid computing environment. In this paper, we consider the problem of allocating independent, heterogeneous tasks on grid environment. A heuristic task scheduling strategy which is composed of two algorithms satisfied with resource load balance is presented. The WMTG-min heuristic algorithm schedules tasks by employing the weighted mean execution time, which reflects the performance of overall machines, and it is a high efficient algorithm. The further optimal scheduling result of the above algorithm is gained by the WMTSG-min algorithm which employs the weighted mean execution time and sufferage value as heuristic information. The performance of the proposed algorithms is evaluated via extensive simulation experiments. Experiment results show that the heuristic strategy performs significantly to ensure high throughput. Furthermore, results show that although it may not be polynomial, the WMTSG-min algorithm is efficient","Optimal scheduling,
Scheduling algorithm,
Processor scheduling,
Throughput,
Grid computing,
Heuristic algorithms,
Resource management,
Distributed computing,
Computer science,
Educational institutions"
A comparative study on term weighting schemes for text categorization,"The term weighting scheme, which is used to convert documents into vectors in the term spaces, is a vital step in automatic text categorization. The previous studies showed that term weighting schemes dominate the performance rather than the kernel functions of SVMs for the text categorization task. In this paper, we conducted experiments to compare various term weighting schemes with SVM on two widely-used benchmark data sets. We also presented a new term weighting scheme tf.rf for text categorization. The cross-scheme comparison was performed by using McNemar's tests. The controlled experimental results showed that the newly proposed tf.rf scheme is significantly better than other term weighting schemes. Compared with schemes related with tf factor alone, the idf factor does not improve or even decrease the term's discriminating power for text categorization. The binary and tf.chi representations significantly underperform the other term weighting schemes.","Tellurium,
Text categorization,
Support vector machines,
Kernel,
Support vector machine classification,
Frequency,
Benchmark testing,
Computer science,
Drives,
Performance evaluation"
On using hierarchical motion history for motion estimation in H.264/AVC,"The embedded multireference frames selection with variable block-size motion compensation model drastically increases the computational complexity of the H.264/AVC video coding standard. This paper proposes an adaptive hierarchical motion estimation (ME) algorithm for H.264/AVC with the objective of minimizing the complexity while maximizing the visual quality. The proposed algorithm is based on a framework that exploits the ""history"" of the motion intensity from a video sequence in order to control ME. The complexity and memory requirement for this meta information is low. The algorithm determines the motion intensity of a video sequence at three levels and accordingly employs different ME techniques. The results certify that the history-based hierarchical information can be very effective in improving the efficiency of ME.","History,
Motion estimation,
Automatic voltage control,
Video sequences,
Sun,
Video coding,
Motion compensation,
Computational complexity,
Computer science,
Maintenance"
An RPC design for wireless sensor networks,"Wireless sensor networks (WSNs) will profoundly influence the ubiquitous computing landscape. Their utility derives not from the computational capabilities of any single sensor node, but from the emergent capabilities of many communicating sensor nodes. Consequently, the details of communication within and across single hop neighborhoods is a fundamental component of most WSN applications. But these details are often complex, and popular embedded languages for WSNs do not provide suitable communication abstractions. We propose that the absence of such abstractions contributes to the difficulty of developing large-scale WSN applications. To address this issue, we present the design and implementation of a remote procedure call (RPC) abstraction for nesC and TinyOS, the defacto standard for developing WSN applications. We present the key language extensions, operating system services, and automation tools that enable the proposed abstraction. We illustrate these contributions in the context of a small case study, and draw preliminary conclusions regarding the suitably of our approach to resource-constrained sensor nodes","Wireless sensor networks,
Hardware,
Ubiquitous computing,
Pervasive computing,
Standards development,
Operating systems,
Monitoring,
Wireless communication,
Computer science,
Large-scale systems"
SRDP: securing route discovery in DSR,"Routing is a critical function in multi-hop mobile ad hoc networks (MANETs). A number of MANET-oriented routing protocols have been proposed, of which DSR is widely considered both the simplest and the most effective. At the same time, security in MANETs-especially, routing security-presents a number of new and interesting challenges. Many security techniques geared for MANETs have been developed, among which Ariadne is the flagship protocol for securing DSR. The focus of this work is on securing the route discovery process in DSR. Our goal is to explore a range of suitable cryptographic techniques with varying flavors of security, efficiency and robustness. The Ariadne approach (with TESLA), while very efficient, assumes loose time synchronization among MANET nodes and does not offer non-repudiation. If the fanner is not possible or the latter is desired, an alternative approach is necessary. To this end, we construct a secure route discovery protocol (SRDP) which allows the source to securely discover an authenticated route to the destination using either aggregated message authentication codes (MACs) or multi-signatures. Several concrete techniques are presented and their efficiency and security are compared and evaluated.","Routing protocols,
Mobile ad hoc networks,
Security,
Cryptographic protocols,
Cryptography,
Spread spectrum communication,
Message authentication,
Bandwidth,
Intelligent networks,
Computer science"
The Development of Embodied Cognition: Six Lessons from Babies,"The embodiment hypothesis is the idea that intelligence emerges in the interaction of an agent with an environment and as a result of sensorimotor activity. We offer six lessons for developing embodied intelligent agents suggested by research in developmental psychology. We argue that starting as a baby grounded in a physical, social, and linguistic world is crucial to the development of the flexible and inventive intelligence that characterizes humankind.","motor control,
Development,
cognition,
language,
embodiment"
Electricity in the palms of her hands-the perception of electrical engineering by outstanding female high school pupils,"This paper explores how outstanding female high school pupils perceived the profession of electrical engineering (EE) before and after a one-day conference aimed at exposing them to the profession of EE. The main finding indicates that, if planned properly and thoughtfully, even a single one-day conference can significantly change the perception of what EE is. In addition, the authors explore the pros and cons of studying EE, as expressed by the female high school pupils. They conclude with some recommendations related to the role of women in EE.",Electrical engineering education
Manipulation Planning for Knotting/Unknotting and Tightly Tying of Deformable Linear Objects,"A planning method for knotting/unknotting and tightening manipulation of deformable linear objects is proposed. It is important for linear object manipulation in industrial/medical field to analyze knotting. Modeling of knotting/unknotting process is useful for design of knotting/unknotting system with different mechanism from human arms/hands and manipulation planning suitable for such system. Firstly, knotting/unknotting processes of a linear object is represented as a sequence of finite crossing state transitions. Secondly, grasping points and their moving direction to perform each state transition are defined. Then, possible qualitative manipulation plans can be generated on a computer system once the initial state and the objective state of a linear object are given. Thirdly, a planning method for tightly tying is proposed. Pulling parts for tightening knots can be determined by using this method. Finally, an experiment for tying an overhand knot by our developed system is shown.","Humans,
Arm,
Process planning,
Grasping,
Shape,
Manufacturing,
Service robots,
Medical robotics,
Defense industry,
Wires"
Replication methods for load balancing on distributed storages in P2P networks,"In a peer-to-peer (P2P) network, in order to improve the search performance and to achieve load balancing, replicas of original data are created and distributed over the Internet. However, the replication methods, which have been proposed so far focus only on the improvement of search performance. We examine the load on the storage systems, which is due to writing and reading, and propose two replication methods for balancing the load on the storages distributed over P2P networks while limiting the degradation of the search performance within an acceptable level. Furthermore, we investigate the performance of our proposed replication methods through computer simulations, and show their effectiveness in balancing the load.","Load management,
IP networks,
Writing,
Degradation,
Network servers,
Web server,
Robustness,
Computer science,
Computer simulation,
Peer to peer computing"
Adapting multiple kernel parameters for support vector machines using genetic algorithms,"Kernel parameterization is a key design step in the application of support vector machines (SVM) for supervised learning problems. A grid-search with a cross-validation criteria is often conducted to choose the kernel parameters but it is computationally unfeasible for a large number of them. Here we describe a genetic algorithm (GA) as a method for tuning kernels of multiple parameters for classification tasks, with application to the weighted radial basis function (RBF) kernel. In this type of kernels the number of parameters equals the dimension of the input patterns which is usually high for biological datasets. We show preliminary experimental results where adapted weighted RBF kernels for SVM achieve classification performance over 98% in human serum proteomic profile data. Further improvements to this method may lead to discovery of relevant biomarkers in biomedical applications",
Distributed resource discovery in large scale computing systems,"There has been significant effort to build high throughput delivering computing systems out of distributed workstations. These systems are growing to accommodate larger number of workstations with growing demand. Discovery of available resources in such environments is a challenging problem. We present a completely distributed resource discovery solution, which utilizes P2P design to provide a scalable service. Our design allows jobs to search for desired workstations, as well as, workstations to search for jobs that may run on them.","Large-scale systems,
Distributed computing,
Throughput,
Workstations,
Grid computing,
Internet,
Peer to peer computing,
Checkpointing,
Computer science,
Job design"
Cost-benefit trade-off analysis using BBN for aspect-oriented risk-driven development,"Security critical systems must perform at the required security level, make effective use of available resources, and meet end-users expectations. Balancing these needs, and at the same time fulfilling budget and time-to-market constraints, requires developers to design and evaluate alternative security treatment strategies. In this paper, the authors presented a development framework that utilizes Bayesian belief networks (BBN) and aspect-oriented modeling (AOM) for a cost-benefit trade-off analysis of treatment strategies. AOM allows developers to model pervasive security treatments separately from other system functionality. This eases the trade-off by making it possible to swap treatment strategies in and out when computing return on security investments (RoSI). The trade-off analysis is implemented using BBN, and RoSI is computed by estimating a set of variables describing properties of a treatment strategy. RoSI for each treatment strategy is then used as input to choice of design.","Risk analysis,
Information security,
Topology,
Bayesian methods,
Computer science,
Computer security,
Investments,
Data security,
Unified modeling language,
Software performance"
Gridmedia: A Multi-Sender Based Peer-to-Peer Multicast System for Video Streaming,"We present a novel single source peer-to-peer multicast architecture called GridMedia which mainly consists of 1) multi-sender based overlay multicast protocol (MSOMP) and 2) multi-sender based redundancy-retransmitting algorithm (MSRRA). The MSOMP deploys mesh-based two-layer structure and groups all the peers into clusters with multiple distinct paths from the source root to each peer. To address the problem of long burst packet loss, the MSRRA is proposed at the sender peers to patch the lost packets by using receiver peer loss pattern prediction. Consequently, GridMedia provides a scalable and reliable video streaming system for a large and highly dynamic population of end hosts, and ensures the quality of service in terms of continuous playback, bandwidth demanding and low latency. A real experimental system based on GridMedia architecture has been constructed over CERNET and broadcasting TV programs for seven months. More than 140,000 end users have been attracted with almost 600 simultaneously being online at Aug 2004 during Athens Olympic Games","Peer to peer computing,
Streaming media,
Multicast protocols,
Clustering algorithms,
Multicast algorithms,
Quality of service,
Bandwidth,
Delay,
Multimedia communication,
TV broadcasting"
Flow routing for variable bit rate source nodes in energy-constrained wireless sensor networks,"We consider a two-tier wireless sensor network and focus on the flow routing problem for the upper tier aggregation and forwarding nodes (AFNs). Assuming each AFN is equipped with directional antennas for transmission, we are interested in how to perform flow routing at each node such that the network lifetime is maximized. We present a flow routing algorithm that provably has the following properties: (1) when the average source rate of each AFN is known a priori, the flow routing algorithm is optimal and gives maximum network lifetime performance; (2) when the average source rate of each AFN is unknown but is within a fraction, /spl epsiv/, of an estimated rate value, then the network lifetime given by the proposed flow routing algorithm is no more than 2/spl epsiv//(1-/spl epsiv/) from optimal. As a result, the proposed flow routing algorithm can provide predictable lifetime performance, even when the source bit rate can be time-varying.","Routing,
Bit rate,
Intelligent networks,
Wireless sensor networks,
Directional antennas,
Life estimation,
Lifetime estimation,
Computer science,
Power engineering and energy,
Prediction algorithms"
Privacy Preserving ID3 Algorithm over Horizontally Partitioned Data,"For the problem of decision tree classification with privacy concerns, we propose several efficient secure multi-party computation protocols to construct a privacy preserving ID3 algorithm over horizontally partitioned data among multiple parties. Our algorithm presents the first solution to privacy preserving decision tree classification among more than two parties. We also make a performance comparison with the existing solution, which is only applicable to the twoparty case. The result shows that our solution has a significantly better performance.","Data privacy,
Partitioning algorithms,
Sliding mode control,
Decision trees,
Classification tree analysis,
Protocols,
Data mining,
Distributed computing,
Cryptography,
Computer networks"
Quaternary Reed-Muller codes,The class of quaternary Reed-Muller codes is introduced as a generalization of Reed-Muller codes to /spl Zopf//sub 4/-linear codes. Properties of this class of codes are established including the rank and kernel of the Gray map image of codes in this class. The class includes all /spl Zopf//sub 4/-linear Kerdock-like and Preparata-like codes.,
Synchronization in Generalized Erd&#246;s-R&#233;nyi Networks of Nonlinear Oscillators,"In this paper, we study synchronization of complex random networks of nonlinear oscillators, with specifiable expected degree distribution. We review a sufficient condition for synchronization and a sufficient condition for desynchronization, expressed in terms of the eigenvalue distribution of the Laplacian of the graph and the coupling strength. We then provide a general way to approximate the Laplacian eigenvalue distribution for the case of large random graphs produced by a generalization, [2], of the ErdÃ¶s-RÃ©nyi model. Our approach is based on approximating the moments of the eigenvalue density function. The analysis is illustrated by using a complex network of nonlinear oscillators, with a power-law degree distribution.","Intelligent networks,
Oscillators,
Lattices,
Sufficient conditions,
Couplings,
Chaos,
Nonlinear equations"
Fast Implementation of Lemke&#8217;s Algorithm for Rigid Body Contact Simulation,"We present a fast method for solving rigid body contact problems with friction, based on optimizations incorporated into Lemkeâ€™s algorithm for solving linear complementarity problems. These optimizations improve computation time in general and reduce the expected solution complexity from O(n3) to nearly O(nm + m3), where n and m are the number of contacts and rigid bodies. For a fixed number of bodies the expected complexity is therefore close to O(n) . Our method also improves numerical robustness, and removes the need to explicitly compute the large matrices associated with rigid body contact problems.","Computational modeling,
Friction,
Computer simulation,
Robustness,
Haptic interfaces,
Iterative methods,
Convergence,
Testing,
Computer science,
Optimization methods"
Object naming analysis for reverse-engineered sequence diagrams,"UML sequence diagrams are commonly used to represent object interactions in software systems. This paper considers the problem of extracting UML sequence diagrams from existing code for the purposes of software understanding and testing. A static analysis for such reverse engineering needs to map the interacting objects from the code to sequence diagram objects. We propose an interprocedural dataflow analysis algorithm that determines precisely which objects are the receivers of certain messages, and assigns the appropriate diagram objects to represent them. Our experiments indicate that the majority of message receivers can be determined exactly, resulting in highly-precise object naming for reverse-engineered sequence diagrams.","Reverse engineering,
Unified modeling language,
Software maintenance,
Algorithm design and analysis,
Computer science,
Software testing,
Logic programming,
Permission,
Java,
Runtime"
Progressive surface reconstruction from images using a local prior,"This paper introduces a new method for surface reconstruction from multiple calibrated images. The primary contribution of this work is the notion of local prior to combine the flexibility of the carving approach with the accuracy of graph-cut optimization. A progressive refinement scheme is used to recover the topology and reason the visibility of the object. Within each voxel, a detailed surface patch is optimally reconstructed using a graph-cut method. The advantage of this technique is its ability to handle complex shape similarly to level sets while enjoying a higher precision. Compared to carving techniques, the addressed problem is well-posed, and the produced surface does not suffer from aliasing. In addition, our approach seamlessly handles complete and partial reconstructions: If the scene is only partially visible, the process naturally produces an open surface; otherwise, if the scene is fully visible, it creates a complete shape. These properties are demonstrated on real image sequences","Surface reconstruction,
Image reconstruction,
Shape,
Layout,
Geometry,
Level set,
Computer science,
Topology,
Image sequences,
Head"
Towards a metamorphic testing methodology for service-oriented software applications,"Testing applications in service-oriented architecture (SOA) environments needs to deal with issues like the unknown communication partners until the service discovery, the imprecise black-box information of software components, and the potential existence of non-identical implementations of the same service. In this paper, we exploit the benefits of the SOA environments and metamorphic testing (MT) to alleviate the issues. We propose an MT-oriented testing methodology in this paper. It formulates metamorphic services to encapsulate services as well as the implementations of metamorphic relations. Test cases for the unit test phase is proposed to generate follow-up test cases for the integration test phase. The metamorphic services invoke relevant services to execute test cases and use their metamorphic relations to detect failures. It has potentials to shift the testing effort from the construction of the integration test sets to the development of metamorphic relations.","Software testing,
Application software,
Service oriented architecture,
Automatic testing,
Web services,
Councils,
Computer science,
Software performance,
Software quality"
Computing location from ambient FM radio signals [commercial radio station signals],"We present a method for computing the location of a device down to a radius of several miles within a greater metropolitan area by analyzing the signal strengths observed from commercial FM radio stations. The use of ambient commercial radio signals allows for wide coverage, both indoor and outdoor reception, client-side computing for privacy, and the feasibility of employing inexpensive, low-power measurement hardware. Our technique is based on a model for computing the likelihood of locations using both received signal strengths and information from a simulated signal strength map. Using simulated signal strengths relieves the burden of manually measuring signal strength as a function of location. We account for the inevitable measurement variations among devices by comparing rankings of radio stations by signal strength. Our experiments show we can measure location down to a median error of about 8 kilometers (5 miles) in the greater Seattle area by listening to seven different radio stations.","Watches,
Hardware,
Computational modeling,
Receivers,
Cellular phones,
Filters,
Computer science,
Educational institutions,
Urban areas,
Signal analysis"
Anthill: a scalable run-time environment for data mining applications,"Data mining techniques are becoming increasingly more popular as a reasonable means to collect summaries from the rapidly growing datasets in many areas. However, as the size of the raw data increases, parallel data mining algorithms are becoming a necessity. In this paper, we present a run-time support system that was designed to allow the efficient implementation of data-mining algorithms on heterogeneous distributed environments. We believe that the runtime framework is suitable for a broader class of applications, beyond data mining. We also present a parallelization strategy that is supported by the run-time system. We show scalability results of three different data-mining algorithms that were parallelized using our approach and our run-time support. All applications scale almost linearly up to a large number of nodes.","Runtime environment,
Data mining,
Application software,
Clustering algorithms,
Algorithm design and analysis,
Scalability,
Computer science,
Costs,
Memory,
Data analysis"
Job allocation schemes in computational grids based on cost optimization,"In this paper we propose two price-based job allocation schemes for computational grids. A grid system tries to solve problems submitted by various grid users by allocating the jobs to the computing resources governed by different resource owners. The prices charged by these owners are obtained based on a pricing model using a bargaining game theory framework. These prices are then used for job allocation. We present the grid system model and formulate the two schemes as a constraint minimization problem and as a non-cooperative game respectively. The objective of these schemes is to minimize the cost for the grid users. We present algorithms to compute the optimal load (job) fractions to allocate jobs to the computers. Finally, the two schemes are compared under simulations with various system loads and configurations and conclusions are drawn.","Grid computing,
Cost function,
Computer networks,
Pricing,
Resource management,
Game theory,
Distributed computing,
Power engineering computing,
Load management,
Computer science"
Short-Term Traffic Forecasting in a Campus-Wide Wireless Network,"Our goal is to characterize the traffic load in an IEEE802.11 infrastructure. This can be beneficial in many domains, including coverage planning, resource reservation, network monitoring for anomaly detection, and producing more accurate simulation models. The key issue that drives this study is traffic forecasting at each wireless access point (AP) in an hourly timescale. We conducted an extensive measurement study of wireless users on a major university campus using the IEEE802.11 wireless infrastructure. We propose several traffic models that take into account the periodicity and recent traffic history for each AP and present a time-series forecasting methodology. Finally, we build and evaluate these forecasting algorithms and discuss our findings","Telecommunication traffic,
Intelligent networks,
Wireless networks,
Traffic control,
Predictive models,
Computer science,
History,
Wireless LAN,
Bandwidth,
Delay"
Blending human and robot inputs for sliding scale autonomy,"Most robot systems have discrete autonomy levels, if they possess more than a single autonomy level. A user or the robot may switch between these discrete modes, but the robot can not operate at a level between any two modes. We have developed a sliding scale autonomy system that allows autonomy levels to be created and changed on the fly. This paper discusses the system's architecture and presents the results of experiments with the sliding scale autonomy system.","Human robot interaction,
Humanoid robots,
Robot control,
Computer science,
Switches,
Mobile robots,
Orbital robotics,
Sliding mode control,
NIST,
Anthropometry"
Estimation and Marginalization Using the Kikuchi Approximation Methods,"In this letter, we examine a general method of approximation, known as the Kikuchi approximation method, for finding the marginals of a product distribution, as well as the corresponding partition function. The Kikuchi approximation method defines a certain constrained optimization problem, called the Kikuchi problem, and treats its stationary points as approximations to the desired marginals. We show how to associate a graph to any Kikuchi problem and describe a class of local message-passing algorithms along the edges of any such graph, which attempt to find the solutions to the problem. Implementation of these algorithms on graphs with fewer edges requires fewer operations in each iteration. We therefore characterize minimal graphs for a Kikuchi problem, which are those with the minimum number of edges. We show with empirical results that these simpler algorithms often offer significant savings in computational complexity, without suffering a loss in the convergence rate. We give conditions for the convexity of a given Kikuchi problem and the exactness of the approximations in terms of the loops of the minimal graph. More precisely, we show that if the minimal graph is cycle free, then the Kikuchi approximation method is exact, and the converse is also true generically. Together with the fact that in the cycle-free case, the iterative algorithms are equivalent to the well-known belief propagation algorithm, our results imply that, generically, the Kikuchi approximation method can be exact if and only if traditional junction tree methods could also solve the problem exactly.",
Absolutely positively on time: what would it take? [embedded computing systems],"Despite considerable progress in software and hardware techniques, many recent computing advances do more harm than good when embedded computing systems absolutely must meet tight timing constraints. For example, while synchronous digital logic delivers precise timing determinacy, advances in computer architecture and software have made it difficult or impossible to estimate or predict software's execution time. Moreover, networking techniques introduce variability and stochastic behavior, while operating systems rely on best-effort techniques. Worse, programming language semantics do not handle time well, so developers can only specify timing requirements indirectly. Thus, achieving precise timeliness in a networked embedded system - an absolutely essential goal - requires sweeping changes. For embedded computing to realize its full potential, we must reinvent computer science. Resource limitations have influenced embedded software's evolution. Embedded software differs from other software in more fundamental ways.","Embedded computing,
Embedded software,
Timing,
Hardware,
Logic programming,
Computer architecture,
Stochastic systems,
Operating systems,
Computer languages,
Embedded system"
A model for usage policy-based resource allocation in grids,"Challenging usage policy issues can arise within virtual organizations (VOs) that integrate participants and resources spanning multiple physical institutions. Participants may wish to delegate to one or more VOs the right to use certain resources subject to local policy and service level agreements; each VO then wishes to use those resources subject to VO policy. How are such local and VO policies to be expressed, discovered, interpreted, and enforced? As a first step to addressing these questions, we develop and evaluate policy management solutions within a specialized context, namely scientific data grids within which the resources to be shared are computers and storage. We propose an architecture and recursive policy model, and define roles and functions, for scheduling resources in grid environments while satisfying resource owner and VO policies.","Resource management,
Collaborative work,
Laboratories,
Grid computing,
Computer science,
Computer architecture,
Processor scheduling,
Outsourcing,
Collaboration,
Monitoring"
Dynamically optimal policies for stochastic scheduling subject to preemptive-repeat machine breakdowns,"We consider the problem of finding a dynamically optimal policy to process n jobs on a single machine subject to stochastic breakdowns. We study the preemptive-repeat breakdown model, i.e., if a machine breaks down during the processing of a job, the work done on the job prior to the breakdown is lost and the job will have to be started over again. Our study is built on a general setting, which allows: 1) the uptimes and downtimes of the machine to follow general probability distributions, not necessarily independent of each other; 2) the breakdown process to depend upon the job being processed; and 3) the processing times of the jobs to be random variables following arbitrary distributions. We consider two possible cases for the processing time of a job interrupted by a breakdown: a) it is resampled according to its probability distribution or b) it is the same random variable as that before the breakdown. We introduce the concept of occupying time and find its Laplace and integral transforms. For the problem with resampled processing times, we establish a general optimality equation on the optimal dynamic policy under a unified objective measure. We deduce from the optimality equation the optimal dynamic policies for several problems with well-known criteria, including weighted discounted reward, weighted flowtime, truncated cost, number of tardy jobs under stochastic order, and maximum holding cost. For the problem with same random processing time, we develop the optimal dynamic policy via the theory of bandit process. A set of Gittins indices are derived that give the optimal dynamic policies under the criteria of weighted discounted reward and weighted flowtime. Note to Practitioners-It is common in practice that a machine is subject to breakdowns, which may severely interrupt the job it is processing. In such a situation, there may be limited information on the breakdown patterns of the machine and the processing requirements of the jobs. A great challenge faced by the decision-maker is how to utilize the information available to make a right decision. Stochastic scheduling considering stochastic machine breakdowns aims to determine the optimal policies in these situations. In this paper, we study the problem within the preemptive-repeat breakdown framework, to address the practical situations where a job will have to be re-started again if a machine breakdown occurs when it is being processed. Problems of such can be found in many industrial applications. Examples include refining metal in a refinery factory, running a program on a computer, performing a reliability test on a facility, etc. Generally, if a job must be continuously processed with no interruption until it is totally completed, then the preemptive-repeat breakdown formulation should be used. Our research in this paper focuses on optimal dynamic policies, which aim to utilize real-time information to dynamically adjust/improve a decision. We consider two types of models, depending on whether the processing time of the job interrupted by a breakdown must be resampled or not. For the problem with resampled processing times, we establish a general optimality equation under a unified objective measure. We further deduce the optimal dynamic policies under a number of well-known criteria. For the problem without resampled processing times, we develop the optimal dynamic policies, under the criteria of weighted discounted reward and weighted flowtime. Broadly speaking, our findings can be applied in any situations where it is desirable to derive the best dynamic decisions to tackle the problem with stochastic machine breakdowns and preemptive-repeat jobs.","Stochastic processes,
Dynamic scheduling,
Electric breakdown,
Probability distribution,
Random variables,
Integral equations,
Laplace equations,
Cost function,
Refining,
Single machine scheduling"
Multiagents to separating handwritten connected digits,"This paper addresses an important and vital problem within the general area of character recognition, namely separating connected digits in handwritten numerals. The basic idea is employing multiagents to handle this problem. Our approach is mainly based on detecting the deepest-top valley and the highest-bottom hill, with one agent dedicated to each. The former agent decides on candidate cut-point as the closest feature point to the center of the deepest top-valley, if any. On the other hand, the other agent argues candidate cut-point as the closest feature point to the center of the highest bottom-hill, if any. After each of the two agents applies its own rules and reports a candidate cut-point, the two agents negotiate to agree on the actual cut-point. It may happen that both agents come to the negotiation with the same candidate cut-point. However, in general, each agent may find a different candidate cut-point and they negotiate a compromise on the actual cut-point, which may be neither of the two already proposed cut-points. A degree of confidence in each candidate cut-point influences the negotiation process. Experiments conducted so far are promising and successful. The obtained results are very encouraging with a success factor of 97.8%. Finally, none of the two agents alone achieved a close success rate.","Handwriting recognition,
Character recognition,
Computer science,
Writing,
Image recognition,
Postal services,
Computer vision,
Database systems,
Application software,
Laboratories"
Parallel algorithm for pricing American Asian options with multi-dimensional assets,"In this paper, we develop parallel algorithms for pricing American-style Asian options employing binomial tree method. We describe the algorithm, explain the complexities, and study the performance. We have extended our algorithm to handle Asian options with up to 10 underlying assets and shown that the multi-asset Asian options offer a better problem for parallel computation.","Parallel algorithms,
Pricing,
Contracts,
Security,
Instruments,
Arithmetic,
Computer science,
Concurrent computing,
Mathematical model,
Algorithm design and analysis"
A low-leakage twin-precision multiplier using reconfigurable power gating,"A twin-precision multiplier that uses reconfigurable power gating is presented. Employing power cut-off techniques in independently controlled power-gating regions yields significant static leakage reductions when half-precision multiplications are carried out. In comparison to a conventional 8-bit tree multiplier, the power overhead of a 16-bit twin-precision multiplier operating at 8-bit precision has been reduced by 53% when reconfigurable power gating based on the SCCMOS power cut-off technique was applied.","Switches,
Logic gates,
Embedded system,
Delay,
Power dissipation,
Very large scale integration,
Computer science,
Power engineering and energy,
Embedded computing,
Decoding"
Exploiting barriers to optimize power consumption of CMPs,"Power consumption is an important concern for future billion transistor designs. This paper proposes a novel technique for optimizing the power consumption of chip-multiprocessors (CMPs) using an integrated hardware-software mechanism. By using a high level synchronization construct, called the barrier, our technique tracks the idle times spent by a processor waiting for other processors to get to the same point in the program. Using this knowledge, the frequency of the processors can be modulated to reduce/eliminate these idle times, thus providing power savings without compromising on performance. Using real applications from the SpecOMP suite, and a complete system CMP simulator, we demonstrate that this approach can provide as much as 40% power savings (and 32% on the average across five applications) with little impact on performance.","Energy consumption,
Frequency synchronization,
Hardware,
Yarn,
Application software,
Parallel processing,
Symbiosis,
Dynamic voltage scaling,
Voltage control,
Computer science"
TCP New Vegas: Improving the Performance of TCP Vegas Over High Latency Links,"TCP Vegas provides significant performance gains over most TCP variants, especially when used over networks that utilise error prone links. However, whilst examining the impact of latency on TCP we noticed that the performance of TCP Vegas decreases significantly when the network round trip time exceeds 50ms. This paper details research undertaken to identify the cause of this performance decreasing behaviour in TCP Vegas. Three sender-side modifications are proposed and implemented as TCP New Vegas, before being validated via simulation","Delay,
Jacobian matrices,
Computer networks,
Algorithm design and analysis,
Computer science,
Performance gain,
Computer errors,
Time measurement,
Communication system traffic control,
Throughput"
High-Quality Hardware-Based Ray-Casting Volume Rendering Using Partial Pre-Integration,"In this paper, we address the problem of the interactive volume rendering of unstructured meshes and propose a new hardware-based ray-casting algorithm using partial pre-integration. The proposed algorithm makes use of modern programmable graphics card and achieves rendering rates competitive with full pre-integration approaches (up to 2M tet/sec). This algorithm allows the interactive modification of the transfer function and results in high-quality images, since no artifact due to under-sampling the full numerical pre-integration exists. We also compare our approach with implementations of cell-projection algorithm and demonstrate that ray-casting can perform better than cell projection, because it eliminates the high costs involved in ordering and transferring data.","Rendering (computer graphics),
Computer graphics,
Transfer functions,
Costs,
Hardware,
Data visualization,
Computer science,
Proposals,
Algorithm design and analysis,
Inspection"
Detection of Web-based attacks through Markovian protocol parsing,This paper presents a novel approach based on the monitoring of incoming HTTP requests to detect attacks against Web servers. The detection is accomplished through a Markovian model whose states and transitions between them are determined from the specification of the HTTP protocol while the probabilities of the symbols associated to the Markovian source are obtained during a training stage according to a set of attack-free requests for the target server. The experiments carried out show a high detection capability with low false positive rates at reasonable computation requirements.,"Protocols,
Intrusion detection,
Payloads,
Web server,
Internet,
Telematics,
Computerized monitoring,
Information security,
Information resources,
Computer science"
A sorted RSSI quantization based algorithm for sensor network localization,"Range estimation is essential in many sensor network localization algorithms. Although wireless sensor systems usually have available received signal strength indication (RSSI) readings, this information has not been effectively used in the existing localization algorithms. In this paper, we present a sensor network localization algorithm based on a sorted RSSI quantization scheme that can improve the range estimation accuracy when distance information is not available. The range level used in the quantization process can be decided by each node using an adaptive quantization scheme. The new algorithm can be implemented in a distributed way and has significant improvement over existing range-free algorithms. The performance advantage for various sensor networks is shown with experimental results from our extensive simulation with a more realistic radio model.","Quantization,
Hardware,
Wireless sensor networks,
Computer science,
Sensor systems,
Global Positioning System,
Sensor phenomena and characterization,
Measurement errors,
Collaborative work,
RF signals"
Modeling and learning contact dynamics in human motion,"We propose a simple model of human motion as a switching linear dynamical system where the switches correspond to contact forces with the ground. This significantly improves the modeling performance when compared to simpler linear systems, with only marginal increase in complexity. We introduce a novel closed-form (non-iterative) algorithm to estimate the switches and learn the model parameters in between switches. We validate our model qualitatively by running simulations, and quantitatively by computing prediction errors that show significant improvements over previous approaches using linear models.","Humans,
Biological system modeling,
Predictive models,
Statistics,
Switches,
Computer vision,
Kinematics,
Computer science,
Linear systems,
Computational modeling"
A Multi-Modal Agent Based Mobile Route Advisory System for Public Transport Network,"In a metropolis such as Hong Kong, the transport network is massive, dynamic, and complicated, and therefore route finding is not an easy task, especially when routes comprising several modes of transport vehicles (such as bus, train, and ferry). This problem is even more important for e-tourism tourists and moving workforces, who may need to visit an unfamiliar part of the metropolis. To find a route that is the most cost-effective is even harder and time-consuming. In this paper, we present a conceptual model for a multi-modal public transport network and a multi-agent information system (MAIS) implementation architecture. We propose several intelligent approaches in the search agents to enhance the performance of route finding, in particular a knowledge-basket approach. Backend information agents gather updates periodically from transportation companies. We also consider hired transportation services, such as taxis and vans, which might offer similar costs for group commuters. We further support an agent-based auction sub-system for reservation of these mobile vehicles. We have built a route advisory system prototype with such features. Our system supports a flexible multiple user interface views on different mobile platforms. Our paper also includes some benchmark results to demonstrate the effectiveness our approach.","Information systems,
Cost function,
Rail transportation,
Gold,
Computer science,
Councils,
Vehicle dynamics,
Intelligent agent,
Prototypes,
User interfaces"
A comparative measurement study the workload of wireless access points in campus networks,"Our goal is to perform a system-wide characterization of the workload of wireless access points (APs) in a production 802.11 infrastructure. The key issues of this study are the characterization of the traffic at each access point (AP), its modeling, and a comparison among APs of different wireless campus-wide infrastructures. Unlike most other studies, we compare two networks using similar data acquisition techniques and analysis methods. This makes the results more generally applicable. We analyzed the aggregate traffic load of APs and found that the log normality is prevalent. The distributions of the wireless received and sent traffic load for these infrastructures are similar. Furthermore, we discovered a dichotomy of APs: there are APs with the majority of clients that are uploaders and APs in which the majority of their clients are downloaders. Also, the number of non-unicast wireless packets and the percentage of roaming events are large. Finally, there is a correlation between the number of associations and traffic load in the log-log scale","Intelligent networks,
Telecommunication traffic,
Wireless networks,
Data acquisition,
Traffic control,
Monitoring,
Roaming,
Computer science,
Production systems,
Aggregates"
Evaluation of a Haptic Mixed Reality System for Interactions with a Virtual Control Panel,"We present a haptic feedback technique that combines feedback from a portable force-feedback glove with feedback from direct contact with rigid passive objects. This approach is a haptic analogue of visual mixed reality, since it can be used to haptically combine real and virtual elements in a single display. We discuss device limitations that motivated this combined approach and summarize technological challenges encountered. We present three experiments to evaluate the approach for interactions with buttons and sliders on a virtual control panel. In our first experiment, this approach resulted in better task performance and better subjective ratings than the use of only a force-feedback glove. In our second experiment, visual feedback was degraded and the combined approach resulted in better performance than the glove-only approach and in better ratings of slider interactions than both glove-only and passive-only approaches. A third experiment allowed subjective comparison of approaches and provided additional evidence that the combined approach provides the best experience.",
Towards aspect weaving applications,"Software must be adapted to accommodate new features in the context of changing requirements. In this paper, we illustrate how applications with aspect weaving capabilities can be easily and dynamically adapted with unforeseen features. Aspects were used at three levels: in the context of semantic analysers, within a BPEL engine that orchestrates Web services, and finally within BPEL processes themselves. Each level uses its own tailored domain-specific aspect language that is easier to manipulate than a general-purpose one (close to the programming language) and the pointcuts are independent from the implementation.","Weaving,
Application software,
Engines,
Software performance,
Educational institutions,
Computer science,
Design methodology,
Software design,
Permission,
Business"
Practical divisible load scheduling on grid platforms with APST-DV,"Divisible load applications consist of a load, that is input data and associated computation, that can be divided arbitrarily into independent pieces. Such applications arise in many fields and are ideally suited to a master-worker execution, but they pose several scheduling challenges. While the ""divisible load scheduling"" (DLS) problem has been studied extensively from a theoretical standpoint, in this paper we focus on practical issues: we extend a production grid application execution environment, APST, to support divisible load applications; we implement previously proposed DLS algorithms as part of APST; we evaluate and compare these algorithms on a real-world two-cluster platform; we show in a case study how a user can easily and effectively run a real-world divisible load application; and we uncover several issues that are critical for using DLS theory in practice. To the best of our knowledge the software resulting from this work, APST-DV, is the first usable and generic tool for deploying divisible load applications on distributed computing platforms.","Application software,
Scheduling algorithm,
Processor scheduling,
Load modeling,
Middleware,
Supercomputers,
Computer science,
Data engineering,
Grid computing,
Production"
Measurement-based admission control for a flow-aware network,"To provide statistical service guarantee and achieve high network utilization, measurement-based admission control (MBAC) has been studied for over one decade. Many MBAC algorithms have been proposed in the literature. However, most of them belong to aggregate MBAC algorithms which assume or require that (1) first-in-first-out (FIFO) is used for aggregating flows; (2) statistical service guarantees are provided to the aggregate of admitted flows; (3) each flow requires and experiences the same statistical service guarantees as the aggregate. In this paper, we focus on per-flow MBAC that aims to provide possibly different statistical service guarantees to individual flows in an aggregate. Particularly, we propose a simple per-flow MBAC algorithm in which dynamic priority scheduling (DPS) is adopted to aggregate flows. With this DPS-based per-flow MBAC algorithm, a newly admitted flow is always given a lower priority level than all existing flows, and its priority level is improved if an existing flow leaves the system. Consequently, once a flow is admitted, its received service will not be adversely affected by other flows admitted after it. Because of this, there is no need to re-check or adjust network resources allocated to existing flows due to the admission of a new flow.",
Interference mitigation using a focal plane array,"We consider the use of spatial filtering algorithms for radio frequency interference (RFI) mitigation in conjunction with a focal plane feed array of electrically small elements. Numerical simulations are used to study the performance of 7 and 19 element hexagonal dipole arrays with a 25 m reflector at an operating frequency of 1612 MHz. Using the maximum SNR algorithm to generate array weights, an interfering signal was attenuated by 40 dB or more. The effective sensitivity of the system, including interferer power in the system noise temperature, was comparable to the sensitivity attained in the absence of RFI. Moving the interferer through the reflector pattern sidelobes caused fluctuations in the gain and system sensitivity. This effect was exacerbated by a reflector model with random surface distortions. These results indicate that array feeds are a promising approach for RFI mitigation, but achieving stable radiation patterns in the presence of an interferer may require a trade-off between pattern control and maximum attainable sensitivity.","Feeds,
Sensitivity,
Antenna arrays,
Array signal processing,
Apertures,
Radio astronomy,
Signal to noise ratio"
Directed position estimation: a recursive localization approach for wireless sensor networks,"The establishment of a localization system is an important task in wireless sensor networks. Due to the geographical correlation of the sensed data, location information is commonly used to name the gathered data and address nodes and regions in data dissemination protocols. In general, to estimate its location, a node needs the position information of, at least, three reference points (neighbors that know their positions). In this work, we propose a different scheme in which only two reference points are required to estimate a position. To choose between the two possible solutions of an estimate, we use the known direction of the recursion. This approach leads to a recursive localization system that works with low density networks (increasing in 40% the number of nodes with estimates in some cases), reduces the position error in almost 30%, requires 37% less processor resources to estimate a position, uses less beacon nodes, and also indicates the node position error based on its distance to the recursion origin. No GPS-enabled node is required, as the recursion origin can be used as the relative coordinate system.","Recursive estimation,
Wireless sensor networks,
Sensor systems,
Routing,
Computer science,
Information analysis,
Technological innovation,
Information technology,
Protocols,
Biomedical monitoring"
Robot dribbling using a high-speed multifingered hand and a high-speed vision system,"In order to achieve faster and more dexterous manipulations, we propose a strategy called ""dynamic holding"". In the dynamic holding condition, the object is held in a stable condition while moving at high-speed. In this paper, we first explain the concept of dynamic holding, then we describe an experiment of dribbling by a robot as an example of dynamic holding using a high-speed multifingered hand and a high-speed vision system.","Robot vision systems,
Machine vision,
Manipulator dynamics,
Humans,
Grasping,
Fingers,
End effectors,
Computer vision,
Physics computing,
Information science"
WordNet Ontology Based Model for Web Retrieval,"It is well known that ontologies will become a key piece, as they allow making the semantics of semantic Web content explicit. In spite of the big advantages that the semantic Web promises, there are still several problems to solve. Those concerning ontologies include their availability, development, and evolution. In the area of information retrieval, the dimension of document vectors plays an important role. Firstly, with higher index dimensions the indexing structures suffer from the ""curse of dimensionality"" and their efficiency rapidly decreases. Secondly, we may not use exact words when looking for a document, thus we miss some relevant documents. LSI is a numerical method, which discovers latent semantics in documents by creating concepts from existing terms. In this paper we present a basic method of mapping LSI concepts on given ontology (Word-Net), used both for retrieval recall improvement and dimension reduction. We offer experimental results for this method on a subset of TREC collection, consisting of Los Angeles Times articles","Ontologies,
Indexing,
Large scale integration,
Information retrieval,
Semantic Web,
Q measurement,
Computer science,
Software engineering,
Multidimensional systems,
Costs"
Event-based blackboard architecture for multi-agent systems,"Developing large multi-agent systems is a complex task involving the processes of the requirement, architecture, design and implementation of these systems. In particular, the architectural design is critical to cope with the increasing size and complexity of these systems. The multi-agent system that accesses a central repository is typically based on the blackboard architectural pattern. However, the control strategies of the agents are inherently complex. In this paper, we present an event-based control strategy to manage the access of central repository by multi-agents. In this approach, the blackboard pattern is composed with the implicit invocation pattern to separate the control policies from the control component so that the architecture is ease of evolution, reuses and changes.","Multiagent systems,
Computer architecture,
Centralized control,
Application software,
Open systems,
Broadcasting,
Computer science,
Software architecture,
Software systems,
Intelligent agent"
LAD: localization anomaly detection for wireless sensor networks,"In wireless sensor networks (WSNs), sensors' locations play a critical role in many applications. Having a GPS receiver on every sensor node is costly. In the past, a number of location discovery (localization) schemes have been proposed. Most of these schemes share a common feature: they use some special nodes, called beacon nodes, which are assumed to know their own locations (e.g., through GPS receivers or manual configuration). Other sensors discover their locations based on the reference information provided by these beacon nodes. Most of the beacon-based localization schemes assume a benign environment, where all beacon nodes are supposed to provide correct reference information. However, when the sensor networks are deployed in a hostile environment, where beacon nodes can be compromised, such an assumption does not hold anymore. In this paper, we propose a general scheme to detect localization anomalies that are caused by adversaries. Our scheme is independent from the localization schemes. We formulate the problem as an anomaly intrusion detection problem, and we propose a number of ways to detect localization anomalies. We have conducted simulations to evaluate the performance of our scheme, including the false positive rates, the detection rates, and the resilience to node compromises.","Global Positioning System,
Wireless sensor networks,
Intrusion detection,
Computer science,
Application software,
Resilience,
Personnel,
Routing protocols,
Costs,
Wireless application protocol"
Coevolutionary optimization of fuzzy logic intelligence for strategic decision support,"We present a description and initial results of a computer code that coevolves fuzzy logic rules to play a two-sided zero-sum competitive game. It is based on the TEMPO Military Planning Game that has been used to teach resource allocation to over 20 000 students over the past 40 years. No feasible algorithm for optimal play is known. The coevolved rules, when pitted against human players, usually win the first few competitions. For reasons not yet understood, the evolved rules (found in a symmetrical competition) place little value on information concerning the play of the opponent.",
Case studies on temperature-dependent Characteristics in AC PDPs,"The temperature-dependent characteristics of ac plasma display panels (PDPs) are investigated, based on various case studies using a conventional driving scheme with reset pulses. Though the main factor of the thermal effects is caused by strong sustain discharges, it is not only caused by the panel characteristics, but also by the temperature-dependent characteristics of the driving system. One important thermal effect is a decreased breakdown voltage due to an increase in the panel temperature. Therefore, these results may be helpful in solving image-sticking and temperature-related phenomena.","Computer aided software engineering,
Pulse measurements,
Electrodes,
Testing,
Space vector pulse width modulation,
Current measurement,
Surface discharges,
Temperature,
Plasma displays,
Thermal factors"
Safe concurrency for aggregate objects with invariants,Developing safe multithreaded software systems is difficult due to the potential unwanted interference among concurrent threads. This paper presents a flexible methodology for object-oriented programs that protects object structures against inconsistency due to race conditions. It is based on a recent methodology for single-threaded programs where developers define aggregate object structures using an ownership system and declare invariants over them. The methodology is supported by a set of language elements and by both a sound modular static verification method and run-time checking support. The paper reports on preliminary experience with a prototype implementation.,"Concurrent computing,
Aggregates,
Yarn,
Software systems,
Protection,
Runtime,
Jacobian matrices,
Computer science,
Interference,
Software prototyping"
A generative/discriminative learning algorithm for image classification,"We have developed a two-phase generative/discriminative learning procedure for the recognition of classes of objects and concepts in outdoor scenes. Our method uses both multiple types of object features and context within the image. The generative phase normalizes the description length of images, which can have an arbitrary number of extracted features of each type. In the discriminative phase, a classifier learns which images, as represented by this fixed-length description, contain the target object. We have tested the approach by comparing it to several other approaches in the literature and by experimenting with several different data sets and combinations of features. Our results, using color, texture, and structure features, show a significant improvement over previously published results in image retrieval. Using salient region features, we are competitive with recent results in object recognition","Classification algorithms,
Image classification,
Image segmentation,
Videos,
Image recognition,
Layout,
Image retrieval,
Object recognition,
Computer vision,
Computer science"
Sociological orbit aware location approximation and routing in MANET,"In this paper, we introduce a novel concept of integrating ''macro-mobility"" information obtained from the sociological movement pattern of mobile MANET users into routing. The extraction of this mobility information is based on our observation that the movement of a mobile user exhibits a partially repetitive ""orbital"" pattern involving a set of ""hubs"" in practice. This partially deterministic movement pattern is both practical and useful in locating nodes and routing packets to them without the need for constant tracking or flooding. Leveraging on this hub-based orbital pattern, we propose a sociological orbit aware location approximation and routing (SOLAR) protocol. Through extensive performance analysis we show that SOLAR significantly outperforms conventional routing protocols like dynamic source routing (DSR) and location aided routing (LAR) in terms of higher data throughput, lower control overhead, and lower end-to-end delay.","Mobile ad hoc networks,
Routing protocols,
Disruption tolerant networking,
Computer science,
Mobile computing,
Data mining,
Tracking,
Performance analysis,
Throughput,
Delay"
Markerless real-time 3-D target region tracking by motion backprojection from projection images,"Accurate and fast localization of a predefined target region inside the patient is an important component of many image-guided therapy procedures. This problem is commonly solved by registration of intraoperative 2-D projection images to 3-D preoperative images. If the patient is not fixed during the intervention, the 2-D image acquisition is repeated several times during the procedure, and the registration problem can be cast instead as a 3-D tracking problem. To solve the 3-D problem, we propose in this paper to apply 2-D region tracking to first recover the components of the transformation that are in-plane to the projections. The 2-D motion estimates of all projections are backprojected into 3-D space, where they are then combined into a consistent estimate of the 3-D motion. We compare this method to intensity-based 2-D to 3-D registration and a combination of 2-D motion backprojection followed by a 2-D to 3-D registration stage. Using clinical data with a fiducial marker-based gold-standard transformation, we show that our method is capable of accurately tracking vertebral targets in 3-D from 2-D motion measured in X-ray projection images. Using a standard tracking algorithm (hyperplane tracking), tracking is achieved at video frame rates but fails relatively often (32% of all frames tracked with target registration error (TRE) better than 1.2 mm, 82% of all frames tracked with TRE better than 2.4 mm). With intensity-based 2-D to 2-D image registration using normalized mutual information (NMI) and pattern intensity (PI), accuracy and robustness are substantially improved. NMI tracked 82% of all frames in our data with TRE better than 1.2 mm and 96% of all frames with TRE better than 2.4 mm. This comes at the cost of a reduced frame rate, 1.7 s average processing time per frame and projection device. Results using PI were slightly more accurate, but required on average 5.4 s time per frame. These results are still substantially faster than 2-D to 3-D registration. We conclude that motion backprojection from 2-D motion tracking is an accurate and efficient method for tracking 3-D target motion, but tracking 2-D motion accurately and robustly remains a challenge.","Target tracking,
Medical treatment,
Motion estimation,
Robustness,
Computed tomography,
Motion measurement,
X-ray imaging,
Image registration,
Mutual information,
Costs"
Voltage and frequency control with adaptive reaction time in multiple-clock-domain processors,"Dynamic voltage and frequency scaling (DVFS) is a widely used method for energy-efficient computing. In this paper, we present a new intra-task online DVFS scheme for multiple clock domain (MCD) processors. Most existing online DVFS schemes for MCD processors use a fixed time interval between possible voltage/frequency changes. The downside to this approach is that the interval boundaries are predetermined and independent of workload changes. Thus, they can be late in responding to large, severe activity swings. In this work, we propose an alternative online DVFS scheme in which the reaction time is self-tuned and adaptive to application and work-load changes. In addition to designing such a scheme, we model the proposed DVFS control and use the derived model in a formal stability analysis. The obtained analytical insight is then used to guide and improve the design in terms of stability margin and control effectiveness. We evaluate our DVFS scheme through cycle-accurate simulation over a wide set of MediaBench and SPEC2000 benchmarks. Compared to the best-known prior fixed-interval DVFS schemes for MCD processors, the proposed DVFS scheme has a simpler decision process, which leads to smaller and cheaper hardware. Our scheme has achieved significant energy savings over all studied benchmarks (19% energy savings with 3% performance degradation on average, which is close to the best results from existing fixed-interval DVFS schemes). For a group of applications with fast workload variations, our scheme outperforms existing fixed-interval DVFS schemes significantly due to its adaptive nature. Overall, we feel the proposed adaptive online DVFS scheme is an effective and promising alternative to existing fixed-interval DVFS schemes. Designers may choose the new scheme for processors with limited hardware budget, or if the anticipated work-load behavior is variable. In addition, the modeling and analysis techniques in this work serve as examples of using stability analysis in other aspects of high-performance CPU design and control.","Frequency control,
Programmable control,
Adaptive control,
Dynamic voltage scaling,
Energy efficiency,
Stability analysis,
Statistics,
Clocks,
Hardware,
Computer science"
On feature selection through clustering,We study an algorithm for feature selection that clusters attributes using a special metric and then makes use of the dendrogram of the resulting cluster hierarchy to choose the most relevant attributes. The main interest of our technique resides in the improved understanding of the structure of the analyzed data and of the relative importance of the attributes for the selection process.,"Clustering algorithms,
Bismuth,
Computer science,
Data mining,
Terminology,
Relational databases,
Data analysis,
Performance analysis,
Algorithm design and analysis,
Robustness"
Efficient resource description and high quality selection for virtual grids,"Simple resource specification, resource selection, and effective binding are critical capabilities for grid middleware. We describe the virtual grid, an abstraction for providing these capabilities complex resource environments. Elements of the virtual grid include a novel resource description language (vgDL) and a resource selection and binding component (vgFAB), which accepts a vgDL specification and returns a virtual grid, that is, a set of selected and bound resources. The goals of vgFAB are efficiency, scalability, robustness to high resource contention, and the ability to produce results with quantifiable high quality. We present the design of vgDL, showing how it captures application-level resource abstractions using resource aggregates and connectivity amongst them. We present and evaluate a prototype implementation of vgFAB. Our results show that resource selection and binding for virtual grids of 10,000's of resources can scale up to grids with millions of resources, identifying good matches in less than one second. Further, these matches have quantifiable quality, enabling applications to have high confidence in the results. We demonstrate the effectiveness of our combined selection and binding approach in the presence of resource contention, showing that robust selection and binding can be achieved at moderate cost.","Resource management,
Robustness,
Prototypes,
Computer science,
Middleware,
Scalability,
Aggregates,
Costs,
Computer vision,
Grid computing"
Towards real-world searching with fixed-wing mini-UAVs,"We discuss several techniques that assist in the field deployments of fixed-wing mini-UAVs to assist search teams, focusing on automatic takeoff and landing. We also present a real-time flightpath generation routine that performs a spiral search centered on a target point, and a path planner that creates waypoints for searches up mountain canyons.","Unmanned aerial vehicles,
Humans,
Computer science,
Safety,
Hardware,
Propellers,
Cameras,
Mechanical engineering,
Spirals,
Electric breakdown"
A Multi-Agent Infrastructure for Mobile Workforce Management in a Service Oriented Enterprise,"With recent advances in mobile technologies and infrastructures, there are increasing demands for the support of mobile workforce management (MWM) across multiple platforms. Mobile users can interact with the MWM system through SMS messages or Web browsers from personal computers, PDA devices, or WAP phones. MWM typically involves tight collaboration, negotiation, and sophisticated business domain knowledge. Therefore, the main challenge of MWM for a service oriented enterprise is the integration of disparate business function for its mobile professional workforce and the management with a unified infrastructure, together with the provision of personalized assistance and automation. These requirements can be facilitated with the use of intelligent software agents, but has not been adequately studied before. As mobile devices become more powerful, intelligent software agents can now be deployed on these devices and hence also subject to mobility. Thus, MWM systems have to accommodate these requirements as well as the support of heterogeneous platforms. In this paper, we formulate a scalable, flexible, and intelligent multi-agent information system (MAIS) infrastructure for MWM with agent clusters in the context of a large service oriented enterprise. Each agent cluster comprise several types of agents to achieve the goal of each phase of the workforce management process, namely, task formulation, matchmaking, brokering, commuting, and service. We evaluate our approach against different stakeholders' perspective.","Intelligent agent,
Software agents,
Technology management,
Mobile computing,
Microcomputers,
Personal digital assistants,
Collaborative work,
Automation,
Intelligent systems,
Management information systems"
Integrating Tactile and Force Feedback with Finite Element Models,"Integration of the correct tactile and kinesthetic force feedback response with an accurate computational model of a compliant environment is a formidable challenge. We examine several design issues that arise in the construction of a compliance renderer, specifically the interaction between impedances of tactile displays, impedances of robot arms, and the computational model. We also describe an implementation of a compliance rendering system combining a low-impedance robot arm for large workspace kinesthetic force feedback, a high-impedance shape display for distributed tactile feedback to the finger pad, and a real-time finite element modeler. To determine the efficacy of the integration of tactile and kinesthetic force feedback components, we conducted a study examining the userâ€™s ability to discriminate stiffness. Subjects were able to reliably detect a 20% difference in rendered material stiffness using our compliance rendering system.",
Model checking Markov reward models with impulse rewards,"This paper considers model checking of Markov reward models (MRMs), continuous-time Markov chains with state rewards as well as impulse rewards. The reward extension of the logic CSL (continuous stochastic logic) is interpreted over such MRMs, and two numerical algorithms are provided to check the reachability of a set of goal states under a time and an accumulated reward constraint. This extends existing model-checking techniques for MRMs with just state rewards, and improves the applicability to thousands of states. Our approach is illustrated by using rewards for energy consumption in the setting of dynamic power management.","Logic,
Computer science,
Performance analysis,
Stochastic processes,
Energy management,
Costs,
Energy consumption,
Degradation,
Formal verification,
Distributed computing"
Bypass caching: making scientific databases good network citizens,"Scientific database federations are geographically distributed and network bound. Thus, they could benefit from proxy caching. However, existing caching techniques are not suitable for their workloads, which compare and join large data sets. Existing techniques reduce parallelism by conducting distributed queries in a single cache and lose the data reduction benefits of performing selections at each database. We develop the bypass-yield formulation of caching, which reduces network traffic in wide-area database federations, while preserving parallelism and data reduction. Bypass-yield caching is altruistic; caches minimize the overall network traffic generated by the federation, rather than focusing on local performance. We present an adaptive, workload-driven algorithm for managing a bypass-yield cache. We also develop on-line algorithms that make no assumptions about workload: a k-competitive deterministic algorithm and a randomized algorithm with minimal space complexity. We verify the efficacy of bypass-yield caching by running workload traces collected from the Sloan Digital Sky Survey through a prototype implementation.","Telecommunication traffic,
Parallel processing,
Bandwidth,
Scalability,
Computer science,
Data engineering,
Distributed databases,
Delay,
Concurrent computing,
Intelligent networks"
A dependability-driven system-level design approach for embedded systems,"The paper introduces dependability as an optimization criterion in the system-level design process of embedded systems. Given the pervasiveness of embedded systems, especially in the area of highly dependable and safety-critical systems, it is imperative to consider dependability in the system level design process directly. This naturally leads to a multi-objective optimization problem, as cost and time have to be considered too. The paper proposes a genetic algorithm to solve this multi-objective optimization problem and to determine a set of Pareto optimal design alternatives in a single optimization run. Based on these alternatives, the designer can choose his best solution, finding the desired tradeoff between cost, schedulability, and dependability.",
Impact of process variation on soft error vulnerability for nanometer VLSI circuits,"In this paper, the impact of process variation on-soft error vulnerability for nanometer VLSI circuits is studied. Particle strike is modeled in SPICE as a current source connected to the node of interest. Qcritical of four kinds of circuits at different technology nodes is analyzed. Our simulation result shows that process variation can make Qcritical vary from -33.5% to 81.7% compared to the case without considering process variation. Because of the exponential dependence of SER on critical, the result shows significant impact of process variation on SER. Gate length found to be the most important variability source while the influence of threshold voltage cannot be ignored","Very large scale integration,
Circuits,
Semiconductor device measurement,
Semiconductor device modeling,
Computer errors,
SPICE,
Voltage,
Single event upset,
Leakage current,
Computer science"
Improving geometric accuracy in the presence of susceptibility difference artifacts produced by metallic implants in magnetic resonance imaging,"Geometric and intensity distortions due to the presence of metallic implants in magnetic resonance imaging impede the full exploitation of this advanced imaging modality. The aim of this study is to provide a method for (a) quantifying and (b)reducing the implant distortions in patient images. Initially, a set of reference images (without distortion) was obtained by imaging a custom-designed three-dimensional grid phantom. Corresponding test images (containing the distortion) were acquired with the same imaging parameters, after positioning a specific metallic implant in the grid phantom. After determining: 1) the nonrecoverable; 2) the distorted, but recoverable; and 3) the unaffected areas, a point-based thin-plate spline image registration algorithm was employed to align the reference and test images. The calculated transformation functions utilized to align the image pairs described the implant distortions and could therefore be used to correct any other images containing the same distortions. The results demonstrate successful correction of grid phantom images with a metallic implant. Furthermore, the calculated correction was applied to porcine thigh images bearing the same metallic implant, simulating a patient environment. Qualitative and quantitative assessments of the proposed correction method are included.","Magnetic susceptibility,
Implants,
Magnetic resonance imaging,
Testing,
Imaging phantoms,
Surgery,
Impedance,
Spline,
Image registration,
Thigh"
Distributed intrusion detection based on clustering,"The research on distributed intrusion detection system (DIDS) is a rapidly growing area of interest because the existence of centralized intrusion detection system (IDS) techniques is increasingly unable to protect the global distributed information infrastructure. Distributed analysis employed by agent-based DIDS is an accepted fabulous method. Clustering-based intrusion detection technique overcomes the drawbacks of relying on labeled training data which most current anomaly-based intrusion detection depend on. Clustering-based DIDS technique according to the advantages of two techniques is presented. For effectively choosing the attacks, twice clustering is employed: the first clustering is to choose the candidate anomalies at agent IDS and the second clustering is to choose the true attack at central IDS. At last, through experiment on the KDD CUP 1999 data records of network connections verified that the methods put forward is better.",
A Policy-based Management Framework for Pervasive Systems using Axiomatized Rule-Actions,"Pervasive systems comprise large collections of heterogeneous and mobile devices, services and applications. A management infrastructure is required to govern the system behavior according to policies specified by the system administrator. Policy-based management is a well-established approach where policies are specified as Event-Condition-Action (ECA) rules that determine the management actions to be performed when certain situations occur. The problem with ECA policies is that conflicting actions may get triggered on the same event resulting in policy conflicts. Cycles may result when a set of policy rules trigger each other continuously. Existing approaches to conflict detection are limited in scope and can only detect conflicting actions if they are explicitly stated. In addition, current techniques do not detect cycles in management policies. We propose an extension to the ECA rule framework, called Event-Condition-PreCondition-Action-PostCondition (ECPAP) as a rule framework for management policies. In this framework, actions are annotated with axiomatic specifications that enable powerful reasoning to detect conflicts and cycles in policies. We present the details of this framework","Power system management,
Mobile computing,
Computer science,
Application software,
Pervasive computing,
Resource management,
Detection algorithms,
Computer architecture,
Computer applications,
Computer networks"
Performance of DNS as location manager for wireless systems in IP networks,"Domain name system (DNS) can be deployed in the network as a location manager (LM) for mobility management. The suitability of domain name system (DNS) as an LM can be measured by how successfully it can serve to locate a mobile host. In this paper, we developed an analytical model to measure the performance of DNS as LM for mobility management techniques with IP diversity support based on success rate which takes into account the radius of the subnet, the residence time of MH in that subnet, latency in the network and the overlapping distance of two neighboring subnets. Our analysis shows that for a reasonable overlapping distance, DNS can serve as an LM with very high success rate even under some high network latency.","Intelligent networks,
IP networks,
Mobile radio mobility management,
Computer network management,
Domain Name System,
Delay,
Internet,
Computer science,
USA Councils,
Analytical models"
Fast floorplanning by look-ahead enabled recursive bipartitioning,"A new paradigm is introduced for floorplanning any combination of fixed-shape and variable-shape blocks under tight fixed-outline area constraints and a wirelength objective. Dramatic improvement over traditional floor-planning methods is achieved by explicit construction of strictly legal layouts for every partition block at every level of a cutsize-driven, top-down hierarchy. By scalably incorporating legalization into the hierarchical flow, post-hoc legalization is successfully eliminated. For large floorplanning benchmarks, an implementation, called PATOMA, generates solutions with half the wirelength of state-of-the-art floorplanners in orders of magnitude less run time.","Clustering algorithms,
Law,
Legal factors,
Simulated annealing,
Traffic control,
Computer science,
Art,
Very large scale integration,
Circuits,
Logic design"
Cluster-based congestion control for supporting multiple classes of traffic in sensor networks,"In wireless sensor networks, multiple flows from data collecting sensors to an aggregating sink could traverse paths that are largely interference coupled. These interference effects manifest themselves as congestion, and cause high packet loss and arbitrary packet delays. This is particularly problematic in event-based sensor networks where some flows are of greater importance than others and require fidelity in terms of higher packet delivery and timeliness. In this paper we present COMUT (congestion control for multi-class traffic), a distributed cluster-based mechanism for supporting multiple classes of traffic in sensor networks. COMUT is based on the self-organization of the network into clusters each of which autonomously and proactively monitors congestion within its localized scope. The clusters then exchange appropriate information to facilitate system wide rate control. Our simulation results demonstrate that our techniques are highly effective in dealing with multiple, randomly initiated flows.","Communication system traffic control,
Intelligent networks,
Wireless sensor networks,
Interference,
Temperature sensors,
Sensor phenomena and characterization,
Traffic control,
Delay,
Computer science,
Data engineering"
Identifying semantically equivalent object fragments,"We describe a novel technique for identifying semantically equivalent parts in images belonging to the same object class, (e.g. eyes, license plates, aircraft wings etc.). The visual appearance of such object parts can differ substantially, and therefore, traditional image similarity-based methods are inappropriate for this task. The technique we propose is based on the use of common context. We first retrieve context fragments, which consistently appear together with a given input fragment in a stable geometric relation. We then use the context fragments in new images to infer the most likely position of equivalent parts. Given a set of image examples of objects in a class, the method can automatically learn the part structure of the domain - identify the main parts, and how their appearance changes across objects in the class. Two applications of the proposed algorithm are shown: the detection and identification of object parts and object recognition.","Object recognition,
Object detection,
Licenses,
Face detection,
Nose,
Speech recognition,
Computer science,
Mathematics,
Eyes,
Aircraft"
A study on residual prediction techniques for voice conversion,"Several well-studied voice conversion techniques use line spectral frequencies as features to represent the spectral envelopes of the processed speech frames. In order to return to the time domain, these features are converted to linear predictive coefficients that serve as coefficients of a filter applied to an unknown residual signal. We compare several residual prediction approaches that have already been proposed in the literature dealing with voice conversion. We also present a novel technique that outperforms the others in terms of voice conversion performance and sound quality.",
Evaluation of color spaces for edge classification in outdoor scenes,"Knowing which edges in an image denote shadow edges and which are due to object boundaries or changes in surface reflectance has important applications in both computer vision and mixed reality. We show that the choice of color space has a significant effect on our ability to differentiate shadow edges from reflectance edges, particularly in sunlit scenes. We have evaluated the performance of 11 color spaces on an input data of more than a hundred colors imaged in a variety of illumination conditions. We quantify the performance of these color spaces using receiver operating characteristic curves, and use the z statistic to find if the difference in performances is statistically significant.",
A Case for Cooperative and Incentive-Based Coupling of Distributed Clusters,"Interest in grid computing has grown significantly over the past five years. Management of distributed cluster resources is a key issue in grid computing. Central to management of resources is the effectiveness of resource allocation, as it determines the overall utility of the system. In this paper, we propose a new grid system that consists of grid federation agents which couple together distributed cluster resources to enable a cooperative environment. The agents use a computational economy methodology, that facilitates QoS scheduling, with a cost-time scheduling heuristic based on a scalable, shared federation directory. We show by simulation, while some users that are local to popular resources can experience higher cost and/or longer delays, the overall users' QoS demands across the federation are better met. Also, the federation's average case message passing complexity is seen to be scalable, though some jobs in the system may lead to large numbers of messages before being scheduled","Resource management,
Processor scheduling,
Grid computing,
Delay,
Large-scale systems,
Concurrent computing,
Distributed computing,
Computer networks,
Access protocols,
Computer science"
The LAN-simulation: a refactoring teaching example,"The notion of refactoring - transforming the source-code of an object-oriented program without changing its external behaviour - has been studied intensively within the last decade. This diversity has created a plethora of toy-examples, cases and code snippets, which make it hard to assess the current state-of-the-art. Moreover, due to this diversity, there is currently no accepted way of teaching good refactoring practices, despite the acknowledgment in the software engineering body of knowledge. Therefore, this paper presents a common example - the LAN simulation - which has been used by a number of European Universities for both research and teaching purposes.","Education,
Software engineering,
Software systems,
Prototypes,
Local area networks,
Object oriented modeling,
Educational institutions,
Software maintenance,
Large-scale systems,
Costs"
An alert fusion framework for situation awareness of coordinated multistage attacks,"Recent incidents in the cyber world strongly suggest that coordinated multistage cyber attacks are quite feasible and that effective countermeasures need to be developed. Attack detection by correlation and fusion of intrusion alerts has been an active area of current research. However, most of these research efforts focus on ex post facto analysis of alert data to uncover related attacks. In this paper, we present an approach for dynamically calculating 'scenario credibilities' based on the state of a live intrusion alert stream. We also develop a framework for attack scenario representation that facilitates real-time fusion of intrusion alerts and calculation of the scenario credibility values. Our approach provides a usable mechanism for detecting, predicting and reasoning about multistage goal-oriented attacks in real time. The details of the fusion framework and a description of multistage attack detection using this framework are presented in this paper.","Intrusion detection,
Fusion power generation,
Computer science,
Data analysis,
Subcontracting,
Fuses,
Conferences,
Protection"
Detecting intra-enterprise scanning worms based on address resolution,"Signature-based schemes for detecting Internet worms often fail on zero-day worms, and their ability to rapidly react to new threats is typically limited by the requirement of some form of human involvement to formulate updated attack signatures. We propose an anomaly-based detection technique detailing a method to detect propagation of scanning worms within individual network cells, thus protecting internal networks from infection by internal clients. Our software implementation indicates that this technique is both accurate and rapid enough to enable automatic containment and suppression of worm propagation within a network cell. Our approach relies on an aggregate anomaly score, derived from the correlation of address resolution protocol (ARP) activity from individual network attached devices. Our preliminary analysis and prototype indicate that this technique can be used to rapidly detect zero-day worms within a very small number of scans","Computer worms,
Broadcasting,
Protocols,
Computer science,
Internet,
Humans,
Protection,
Aggregates,
Prototypes,
Proposals"
On the complexity of real functions,"We establish a new connection between the two most common traditions in the theory of real computation, the Blum-Shub-Smale model and the computable analysis approach. We then use the connection to develop a notion of computability and complexity of functions over the reals that can be viewed as an extension of both models. We argue that this notion is very natural when one tries to determine just how difficult a certain function is for a very rich class of functions.","Scientific computing,
Computer science,
Computational modeling,
Turing machines,
Predictive models,
Physics computing,
Logic,
Computational complexity,
Read-write memory,
Scholarships"
A quadratic prediction based fractional-pixel motion estimation algorithm for H.264,"In this study, a quadratic prediction based fractional-pixel motion estimation (ME) algorithm for H.264 is proposed. Here, a ""degenerate"" quadratic function is used to determine the ""best"" quantized predicted fractional-pixel motion vector (MV) for a variable size block. Based on the partial probability distribution of the sum of absolute component differences between the best fractional-pixel MV determined by the full search ME algorithm and the ""best"" quantized predicted fractional-pixel MV determined by the proposed algorithm, a small diamond search pattern (SDSP) is used to determine the final best MV at 1/4-pixel accuracy and the SDSP will be applied at most 3 times. Additionally, if both the best quantized predicted fractional-pixel MV determined by the proposed algorithm and that determined by the center biased fractional-pixel search (CBFPS) algorithm are identically (0,0), (0,0) is directly determined as the final best MV at 1/4-pixel accuracy. Based on the experimental results obtained in this study, the four ME performance measures of the proposed algorithm are better than that of four comparison algorithms, with slight degradations in average PSNR and bit rate.","Motion estimation,
Degradation,
Computational complexity,
Computer science,
Probability distribution,
PSNR,
Bit rate,
Video coding,
Video compression,
Councils"
Generalized likelihood ratio tests for complex fMRI data: a Simulation study,"Statistical tests developed for the analysis of (intrinsically complex valued) functional magnetic resonance time series, are generally applied to the data's magnitude components. However, during the past five years, new tests were developed that incorporate the complex nature of fMRI data. In particular, a generalized likelihood ratio test (GLRT) was proposed based on a constant phase model. In this work, we evaluate the sensitivity of GLRTs for complex data to small misspecifications of the phase model by means of simulation experiments. It is argued that, in practical situations, GLRTs based on magnitude data are likely to perform better compared to GLRTs based on complex data in terms of detection rate and constant false alarm rate properties.","Testing,
Magnetic resonance imaging,
Analytical models,
Magnetic resonance,
Time series analysis,
Magnetic analysis,
Data visualization,
Humans,
Blood flow,
Brain"
Phishing Web page detection,"An approach to detection of phishing Web pages based on visual similarity is proposed, which can be utilized as a part of an enterprise solution to antiphishing. A legitimate Web page owner can use this approach to search the Web for suspicious Web pages which are visually similar to the true Web page. The approach first decomposes the Web pages into salient (visually distinguishable) block regions. The visual similarity between two Web pages is then evaluated in three metrics: block level similarity, layout similarity, and overall style similarity. A Web page is reported as a phishing suspect if any of them (with regards to the true one) is higher than its corresponding preset threshold. Preliminary experiments show that the approach can successfully detect those phishing Web pages with few false alarms at a speed adequate for online application.","Uniform resource locators,
Testing,
Computer science,
Text analysis,
Information analysis,
Information filtering,
Credit cards,
Internet,
Electronic mail,
Size measurement"
From optimal measurement to efficient quantum algorithms for the hidden subgroup problem over semidirect product groups,"We approach the hidden subgroup problem by performing the so-called pretty good measurement on hidden subgroup states. For various groups that can be expressed as the semidirect product of an abelian group and a cyclic group, we show that the pretty good measurement is optimal and that its probability of success and unitary implementation are closely related to an average-case algebraic problem. By solving this problem, we find efficient quantum algorithms for a number of nonabelian hidden subgroup problems, including some for which no efficient algorithm was previously known: certain metacyclic groups as well as all groups of the form /spl Zopf//sub p/ /sup r/ /spl times/ /spl Zopf//sub p/ fixed r (including the Heisenberg group, r = 2). In particular our results show that entangled measurements across multiple copies of hidden subgroup states can be useful for efficiently solving the nonabelian HSP.",
Stable Exploration for Bearings-only SLAM,"Recent work on robotic exploration and active sensing has examined a variety of information-theoretic approaches to efficient and convergent map construction. These involve moving an exploring robot to locations in the world where the anticipated information gain is maximized. In this paper we demonstrate that, for map construction using bearings-only information and the Extended Kalman Filter (EKF), driving exploration so as to maximize expected information gain leads to ill-conditioned filter updates and a high probability of divergence between the inferred map and reality. In particular, we present analytical and numerical results demonstrating the effects of blindly applying an information-theoretic approach to bearings-only exploration. Subsequently, we present experimental results demonstrating that an exploration approach that favours the conditioning of the filter update will lead to more accurate maps.","Simultaneous localization and mapping,
Robot sensing systems,
Robot kinematics,
Information analysis,
Working environment noise,
Computer science,
Information filtering,
Information filters,
Navigation,
Robot localization"
Design and evaluation of an XML-based platform-independent computerized adaptive testing system,"This paper is an attempt to design and to evaluate the platform-independent computerized adaptive testing (CAT) system, which can expand the diversity of CAT-administering platforms. By using extensible markup language (XML) to describe the item bank, one might find the implementation of CAT on a different platform, such as a personal computer (PC), a personal digital assistant (PDA), and other handheld devices, more convenient. An experiment was conducted to examine the effects of a CAT administration platform on precision and efficiency. Fifty senior high school students were selected to take an English vocabulary CAT both on PC and PDA, which enabled them to compare the relevant advantages with the disadvantages of the two different administration platforms firsthand. Both tests used the same-size, well-calibrated item bank, ability estimation algorithm, and item selection strategy. The results indicate that the platforms on which examinees take CAT do not affect the performance of CAT. The responses of the questionnaire on the testing environment also show that most examinees prefer to take the test on PDA. It is concluded that using a PDA to administer CAT is both as precise and effective as a PC and more enjoyable and convenient.","Microcomputers,
Automatic testing,
Computer aided instruction"
Adaptive mesh refinement for multiscale nonequilibrium physics,"In this paper, the authors demonstrate how to use adaptive mesh refinement (AMR) methods for the study of phase transition kinetics. In particular, they apply a block-structured AMR approach to investigate phase ordering in the time-dependent Ginzburg-Landau equations.","Adaptive mesh refinement,
Physics,
Temperature,
Difference equations,
Thermodynamics,
Thermal force,
Metastasis,
Numerical models,
Numerical simulation,
Pressing"
Efficient text classification by weighted proximal SVM,"In this paper, we present an algorithm that can classify large-scale text data with high classification quality and fast training speed. Our method is based on a novel extension of the proximal SVM mode (Fung and Mangasarian, 2001). Previous studies on proximal SVM have focused on classification for low dimensional data and did not consider the unbalanced data cases. Such methods will meet difficulties when classifying unbalanced and high dimensional data sets such as text documents. In this work, we extend the original proximal SVM by learning a weight for each training error. We show that the classification algorithm based on this model is capable of handling high dimensional and unbalanced data. In the experiments, we compare our method with the original proximal SVM (as a special case of our algorithm) and the standard SVM (such as SVM light) on the recently published RCV1-v2 dataset. The results show that our proposed method had comparable classification quality with the standard SVM. At the same time, both the time and memory consumption of our method are less than that of the standard SVM.",
A probabilistic semantic model for image annotation and multimodal image retrieval,"This paper addresses automatic image annotation problem and its application to multi-modal image retrieval. The contribution of our work is three-fold. (1) We propose a probabilistic semantic model in which the visual features and the textual words are connected via a hidden layer which constitutes the semantic concepts to be discovered to explicitly exploit the synergy among the modalities. (2) The association of visual features and textual words is determined in a Bayesian framework such that the confidence of the association can be provided. (3) Extensive evaluation on a large-scale, visually and semantically diverse image collection crawled from Web is reported to evaluate the prototype system based on the model. In the proposed probabilistic model, a hidden concept layer which connects the visual feature and the word layer is discovered by fitting a generative model to the training image and annotation words through an Expectation-Maximization (EM) based iterative learning procedure. The evaluation of the prototype system on 17,000 images and 7,736 automatically extracted annotation words from crawled Web pages for multi-modal image retrieval has indicated that the proposed semantic model and the developed Bayesian framework are superior to a state-of-the-art peer system in the literature.","Image retrieval,
Bayesian methods,
Prototypes,
Information retrieval,
Content based retrieval,
Image databases,
Computer science,
Asia,
Application software,
Large-scale systems"
BDD representation for incompletely specified multiple-output logic functions and its applications to functional decomposition,"A multiple-output function can be represented by a binary decision diagram for characteristic function (BDD/spl I.bar/for/spl I.bar/CF). This paper presents a new method to represent multiple-output incompletely specified functions using BDD/spl I.bar/for/spl I.bar/CF. An algorithm to reduce the widths of BDD/spl I.bar/for/spl I.bar/CFs is presented. This method is useful for decomposition of incompletely specified multiple-output functions. Experimental results for radix converters, adders and a multiplier show that this method is useful for the synthesis of LUT cascades. This data structure is also useful to three-valued logic simulation.",
Resource-aware scientific computation on a heterogeneous cluster,"Although researchers can develop software on small, local clusters and move it later to larger clusters and supercomputers, the software must run efficiently in both environments. Two efforts aim to improve the efficiency of scientific computation on clusters through resource-aware dynamic load balancing. The popularity of cost-effective clusters built from commodity hardware has opened up a new platform for the execution of software originally designed for tightly coupled supercomputers. Because these clusters can be built to include any number of processors ranging from fewer than 10 to thousands, researchers in high-performance scientific computation at smaller institutions or in smaller departments can maintain local parallel computing resources to support software development and testing, then move the software to larger clusters and supercomputers. As promising as this ability is, it has also led to the need for local expertise and resources to set up and maintain these clusters. The software must execute efficiently both on smaller local clusters and on larger ones. These computing environments vary in the number of processors, speed of processing and communication resources, and size and speed of memory throughout the memory hierarchy as well as in the availability of support tools and preferred programming paradigms. Software developed and optimized using a particular computing environment might not be as efficient when it's moved to another one. In this article, we describe a small cluster along with two efforts to improve the efficiency of parallel scientific computation on that cluster. Both approaches modify the dynamic load-balancing step of an adaptive solution procedure to tailor the distribution of data across the cooperating processes. This modification helps account for the heterogeneity and hierarchy in various computing environments.","Supercomputers,
Concurrent computing,
Load management,
Hardware,
Software design,
Parallel processing,
Programming,
Software testing,
Software maintenance,
Availability"
Guaranteed proofs using interval arithmetic,"This paper presents a set of tools for mechanical reasoning of numerical bounds using interval arithmetic. The tools implement two techniques for reducing decorrelation: interval splitting and Taylor's series expansions. Although the tools are designed for the proof assistant system PVS, expertise on PVS is not required. The ultimate goal of the tools is to provide guaranteed proofs of numerical properties with a minimal human-theorem prover interaction.",
ShareMe: running a distributed systems lab for 600 students with three faculty members,"The goal of the distributed systems (DS) laboratory is to provide an attractive environment in which students learn about network programming and apply some fundamental concepts of distributed systems. In the last two years, students had to implement a fully functional peer-to-peer file sharing system called ShareMe. This paper presents the approach the authors used to provide the best possible support and guidance for the students while keeping up with ever-rising participant numbers in the laboratory course (approximately 600 last year), as well as managing budget and personnel constraints. The learning environment is based on Web and Internet technologies and not only offers the description of the laboratory tasks but also covers electronic submission, a discussion forum, automatic grading, and online access to grading and test results. The authors report their experiences of using the automated grading system, the amount of work required to prepare and run the laboratory, and how they deal with students who submit plagiarized solutions. Furthermore, the results of student feedback and evaluation forms are presented, and the overall student course satisfaction is discussed. Detailed information about the DS laboratory is available at http://www.dslab.tuwien.ac.at.",
Semantic Relatedness Measures in Ontologies Using Information Content and Fuzzy Set Theory,The success of the semantic Web is linked with the use of ontologies on the semantic Web. Ontologies help systems understand the meaning of information and can serve as the interface to the inferencing layer of the semantic Web. An increasingly important task is to determine a degree or measure of semantic relatedness between concepts within and across ontologies. This paper presents an overview of such measures using several examples found in the research literature. The relationship between a distance-based network semantic relatedness measure and an information theoretic measure is shown for the first time by using Tversky's set-theoretic measures and a new information content measure. New measures of semantic relatedness between ontological concepts are proposed by viewing each concept as a set of its descendent leaf concepts,
A self-organized grouping (SOG) method for efficient Grid resource discovery,"This paper presents a self-organized grouping (SOG) method that achieves efficient Grid resource discovery by forming and maintaining autonomous resource groups. Each group dynamically aggregates a set of resources that are similar to each other in some pre-specified resource characteristic. The SOG method takes advantage of the strengths of both centralized and decentralized approaches that were previously developed for Grid/P2P resource discovery. The design of the SOG method minimizes the overhead incurred in forming and maintaining groups and maximizes resource discovery performance. The way SOG method handles resource discovery queries is metaphorically similar to searching for a word in an English dictionary by identifying its alphabetical groups at the first place. It is shown from a series of computational experiments that SOG method achieves more stable (i.e., independent of the factors such as resource densities, and Grid sizes) and efficient lookup performance than other existing approaches.","Peer to peer computing,
Grid computing,
Assembly,
Distributed computing,
Computer science education,
Educational technology,
Computer science,
Aggregates,
Dictionaries,
Large-scale systems"
A new path planning algorithm for maximizing visibility in computed tomography colonography,"In virtual colonoscopy, minimizing the blind areas is important for accurate diagnosis of colonic polyps. Although useful for describing the shape of an object, the centerline is not always the optimal camera path for observing the object. Hence, conventional methods in which the centerline is directly used as a path produce considerable blind areas, especially in areas of high curvature. Our proposed algorithm first approximates the surface of the object by estimating the overall shape and cross-sectional thicknesses. View positions and their corresponding view directions are then jointly determined to enable us to maximally observe the approximated surface. Moreover, by adopting bidirectional navigations, we may reduce the blind area blocked by haustral folds. For comfortable navigation, we carefully smoothen the obtained path and minimize the amount of rotation between consecutive rendered images. For the evaluation, we quantified the overall observable area on the basis of the temporal visibility that reflects the minimum interpretation time of a human observer. The experimental results show that our algorithm improves visibility coverage and also significantly reduces the number of blind areas that have a clinically meaningful size. A sequence of rendered images shows that our algorithm can provide a sequence of centered and comfortable views of colonography.","Path planning,
Computed tomography,
Colonography,
Shape,
Navigation,
Rendering (computer graphics),
Virtual colonoscopy,
Colonic polyps,
Cameras,
Humans"
Study of temporal stationarity and spatial consistency of fMRI noise using independent component analysis,"Spatial independent component analysis (ICA) was used to study the temporal stationarity and spatial consistency of structured functional MRI (fMRI) noise. Spatial correlations have been used in the past to generate filters for the removal of structured noise for each time-course in an fMRI dataset. It would be beneficial to produce a multivariate filter based on the same principles. ICA is examined to determine if it has properties that are beneficial for this type of filtering. Six fMRI baseline datasets were decomposed via spatial ICA. The time-courses associated with each component were tested for wide-sense stationarity using the wide sense stationarity quotient (WSS). Each dataset was divided into three subsets and each subset was decomposed. The components of first and third subset were matched by the strength of their correlation. The components produced by ICA were found to have largely nonstationary time-courses. Despite the temporal nonstationarity in the data, ICA was found to produce consistent spatial components. The degree of correlation among components differed depending on the amount of dimension reduction performed on the data. It was found that a relatively small number of dimensions produced components that are potentially useful for generating a spatial fMRI filter.","Independent component analysis,
Low-frequency noise,
Fluctuations,
Filters,
Testing,
Noise level,
Magnetic resonance imaging,
Blood,
Hemodynamics,
Noise generators"
Automatic Determination of the Number of Clusters Using Spectral Algorithms,"We introduce a novel spectral clustering algorithm that allows us to automatically determine the number of clusters in a dataset. The algorithm is based on a theoretical analysis of the spectral properties of block diagonal affinity matrices; in contrast to established methods, we do not normalise the rows of the matrix of eigenvectors, and argue that the non-normalised data contains key information that allows the automatic determination of the number of clusters present. We present several examples of datasets successfully clustered by our algorithm, both artificial and real, obtaining good results even without employing refined feature extraction techniques","Clustering algorithms,
Partitioning algorithms,
Iterative algorithms,
Computer science,
Information analysis,
Spectral analysis,
Algorithm design and analysis,
Feature extraction,
Image segmentation,
Speech recognition"
Complex-valued token Petri nets,"This paper presents a new extension to ordinary Petri nets (PNs) that uses complex-valued tokens. By allowing two kinds of tokens, ""real"" and ""imaginary,"" each place marking contains both quantity and type information. Complex-valued token PNs were designed to integrate seamlessly with other popular Petri net extensions such as timed nets, stochastic nets, and colored nets. This simple and intuitive application of complex numbers and complex arithmetic to PNs provides a unique modeling tool. Some examples show the capabilities of this proposed class of PNs. Note to Practitioners-Discrete-event systems are often man-made systems such as transportation systems, computer communication networks, distributed software, and manufacturing systems. They typically involve the flow of information and physical goods through a network. The flow itself evolves in continuous time but the initiation or completion of the event happens at a discrete point in time. Analyzing the system's performance is key to their successful operation. This paper presents a new approach to performance analysis with application to supply-chain management.","Petri nets,
Performance analysis,
Stochastic processes,
Application software,
Arithmetic,
Transportation,
Computer aided manufacturing,
Computer networks,
Distributed computing,
Communication networks"
Humanoid robot platform suitable for studying embodied interaction,"This paper presents the humanoid robot BARTHOC who has been developed to study human-robot interaction (HRI). The main focus of BARTHOC's design was to realize the expression and behavior of the robot to be as human-like as possible. This allows to apply the platform to manifold research and demonstration areas. With his human-like look and mimic possibilities, he differs from other platforms like ASIMO or QRIO, and enables experiments even close to Mori's 'uncanny valley'. The paper describes details of the mechanical and electrical design of BARTHOC together with its PC control interface. Through its humanoid appearance, it can imitate human behavior with its soft- and hardware. Currently, several components for HRI on a mobile robot platform are being ported to BARTHOC. Starting with these components, the robot's human-like appearance enables us to study embodied interaction and to explore theories of human intelligence.",
Concept identification in object-oriented domain analysis: why some students just don't get it,"Anyone who has taught object-oriented domain analysis or any other software process requiring concept identification has undoubtedly observed that some students just don't get it. Our evaluation of the work of over 740 University of Waterloo students on over 135 software requirements specifications during the last four years supports this same observation. The students' task was to specify a telephone exchange or a voice-over-IP telephone system and the related accounts management subsystem, based on models they developed using object-oriented analysis. A detailed comparative study of three much smaller specifications, all of an elevator system, suggests that object orientation is poorly suited to domain analysis, even of small-sized domains, and that the difficulties we have observed are independent both of the size of the system under specification and of the overall abilities of the students.",
A reversible data hiding scheme with modified side match vector quantization,"Indices are modified so that the secret data can be hidden into the index-based cover image, and thereby the problem of the stego-image quality degradation occurs. If the stego-image quality degradation problem can be solved enabling the receiver to reconstruct the original indices after extracting the hidden secret data from the index-based stego-image, then the compressed cover image can be used repeatedly by different users. To achieve our goal, in this paper, a reversible data-hiding scheme based on a modified side match vector quantization (SMVQ) technique is proposed. Our experimental results confirm the effectiveness and the reversibility of the proposed scheme.","Data encapsulation,
Vector quantization,
Steganography,
Information security,
Payloads,
Computer science,
Degradation,
IP networks,
Data security,
Cryptography"
Approximate server selection algorithms in content distribution networks,"Server selection is an important function in any replication-based infrastructure, aiming at redirecting client requests to the ""best"" server according to some predefined metrics. Previous research work has mainly focused on client-side redirection schemes, where the client is responsible for the server selection process. Furthermore, previous work has shown that client probing techniques perform significantly better in discovering the ""best"" server, compared to hop- or RTT-based schemes. Client probing, however, is not very scalable, since the number of clients and servers in the network will be very large. In this paper, we propose a novel technique to transform the server selection problem into a problem of optimal routing, which enables us to shift the redirection process from the client to the server-side. In particular, we consider the environment of a content distribution network (CDN), and propose a flexible framework that can be used to optimize the server selection process, according to various metrics and/or policies. Using trace-driven simulations, we show that the proposed method can improve significantly the response time of HTTP requests while keeping the control overhead at a very low level.","Network servers,
Intelligent networks,
Web server,
Delay,
Routing,
Computer science,
Explosives,
Web sites,
Availability,
Internet"
Distributed Multi-Robot Exploration and Mapping,,"Robot kinematics,
Robustness,
Computer science,
Mobile computing,
Mobile robots,
Surveillance,
Robot localization,
Humans"
Lifetime optimization of sensor networks under physical attacks,"We address a resource constrained lifetime problem in sensor networks in an operating environment where nodes can be physically destroyed. Specifically, given a limited number of sensors and a hostile environment, our goal is to maximize the network lifetime and derive a node deployment plan. The problem of physical destruction, due to hostile environments and the small size of the sensors, is a viable threat and severely constrains the practical lifetime of sensor networks. The lifetime problem we define is representative, practical, and encompasses other versions of similar problems. We also define a representative physical attack model under which we study and solve the lifetime problem. Our solutions take into account both node constraints and the goal of energy minimization. An important observation based on this work is that network lifetime is greatly affected by the presence of physical attacks, highlighting the importance of our study. Our work has a broad and immediate impact for system designers deploying networks in hostile environments.","Wireless sensor networks,
Throughput,
Computer science,
Telephony,
Design optimization,
Batteries,
Energy resources,
Terrain factors,
Base stations"
MASSIVE: An Emulation Environment for Mobile Ad-Hoc Networks,"Developing and evaluating protocols and applications for mobile ad-hoc networks requires significant organisational effort when real mobile ad-hoc networks with several mobile terminals are considered. For fine tuning and optimizing ad-hoc protocols, network simulators like the OpNet Modeller, NS2, or GloMoSim are most suitable. However, for the comprehension of the mode of operations of adhoc network mechanisms and their effects on the communication flow of applications a real-time experiment environment is more preferable. In this paper, an emulation environment for mobile ad-hoc networks is presented, which operates within existing local area networks utilizing virtual overlay structures reflecting the characteristics of mobile ad-hoc networks. Routing protocols and applications are able to communicate in real-time over emulated mobile infrastuctures using the unmodified IP protocol stack. The emulator provides an extensible interface for specifying mobility patterns in order to define and to evaluate diverse mobile scenarios. Several ad-hoc specific mobility patterns have been integrated and evaluated.","Ad hoc networks,
Mobile communication,
Computational modeling,
Network servers,
Routing protocols,
LAN emulation,
Visualization,
Communication system traffic control,
Computer science,
Application software"
An aspect-oriented development method for embedded control systems with time-triggered and event-triggered processing,"The paper presents a design method for embedded control systems based on the mixed architecture that consists of both time-triggered processing and event-triggered processing. We divide a design process into functional design and behavioral design. We also apply aspect-oriented programming to realize the separated design. In the functional design, we identify objects and define classes to realize functions of the target system. In the behavioral design, we determine triggering methods of objects to meet real-time requirements and define them as aspects. The aspects defined in the behavioral design are woven into the classes defined in the functional design. By using our method, we can design triggering methods independently of the functional design. Our method also improves the reusability of a model and source code of the system. It is possible to reuse classes designed in the functional design for both time-triggered systems and event-triggered systems.","Control systems,
Computer architecture,
Design methodology,
Real time systems,
Object oriented modeling,
Communication system control,
Process design,
Automotive engineering,
Computer science,
Design engineering"
An adaptive load management mechanism for distributed simulation of multi-agent systems,"The paper presents a load management mechanism for distributed simulations of multi-agent systems. The mechanism minimizes the cost of accessing the shared state in the distributed simulation by dynamically redistributing shared state variables according to the access pattern of the simulation model. To evaluate the effectiveness and performance of the mechanism, a series of benchmark experiments were performed using the PDES-MAS framework for distributed simulation of multi-agent systems. Although preliminary, the results indicate that the proposed mechanism significantly reduces the overall access cost of the system.","Load management,
Multiagent systems,
Computational modeling,
Discrete event simulation,
Environmental management,
Computer science,
Costs,
Information technology,
Computer simulation,
Performance evaluation"
A comparative study on unsupervised feature selection methods for text clustering,"Text clustering is one of the central problems in text mining and information retrieval area. For the high dimensionality of feature space and the inherent data sparsity, performance of clustering algorithms will dramatically decline. Two techniques are used to deal with this problem: feature extraction and feature selection. Feature selection methods have been successfully applied to text categorization but seldom applied to text clustering due to the unavailability of class label information. In this paper, four unsupervised feature selection methods, DF, TC, TVQ, and a new proposed method TV are introduced. Experiments are taken to show that feature selection methods can improves efficiency as well as accuracy of text clustering. Three clustering validity criterions are studied and used to evaluate clustering results.","Feature extraction,
Clustering algorithms,
Frequency,
Information retrieval,
Navigation,
Principal component analysis,
Computer science,
Text mining,
Text categorization,
TV"
Attack-resilient time synchronization for wireless sensor networks,"The existing time synchronization schemes in sensor networks were not designed with security in mind, thus leaving them vulnerable to security attacks. In this paper, we first identify various attacks that are effective to several representative time synchronization schemes, and then focus on a specific type of attack called delay attack, which cannot be addressed by cryptographic techniques. Next we propose two approaches to detect and accommodate the delay attack. Our first approach uses the generalized extreme studentized deviate (GESD) algorithm to detect multiple outliers introduced by the compromised nodes; our second approach uses a threshold derived using a time transformation technique to filter out the outliers. Finally we show the effectiveness of these two schemes through extensive simulations","Wireless sensor networks,
Delay effects,
Cryptography,
Object detection,
Computer security,
Protocols,
Computer science,
Design engineering,
Filters,
Time division multiple access"
Fairness among game players in networked haptic environments influence of network latency,"By experiment, this paper examines the influence of network latency on the fairness among players in a networked real-time game where we use haptic interface devices. In the experiment, we subjectively and objectively clarify the influence by changing the difference in network latency between two players. Experimental results show that the difference which leads to unfairness depends on the network latency. We also demonstrate that the differences larger than 30 ms or 40 ms lead to unfairness in the experimental system. Furthermore, we hardly perceive unfairness when the network latency of the two players is less than around 30 ms in the experiment.","Intelligent networks,
Haptic interfaces,
Delay,
Imaging phantoms,
Character generation,
Network servers,
Computer science,
Virtual environment,
Degradation,
Collaborative work"
On learning mixtures of heavy-tailed distributions,"We consider the problem of learning mixtures of arbitrary symmetric distributions. We formulate sufficient separation conditions and present a learning algorithm with provable guarantees for mixtures of distributions that satisfy these separation conditions. Our bounds are independent of the variances of the distributions; to the best of our knowledge, there were no previous algorithms known with provable learning guarantees for distributions having infinite variance and/or expectation. For Gaussians and log-concave distributions, our results match the best known sufficient separation conditions by D. Achlioptas and F. McSherry (2005) and S. Vempala and G. Wang (2004). Our algorithm requires a sample of size O/spl tilde/(dk), where d is the number of dimensions and k is the number of distributions in the mixture. We also show that for isotropic power-laws, exponential, and Gaussian distributions, our separation condition is optimal up to a constant factor.","Computer science,
Gaussian distribution,
Statistical distributions,
Probability distribution,
Polynomials,
Machine learning,
Data mining,
Statistical analysis,
Iterative methods,
Parameter estimation"
A programmable hardware path profiler,"For aggressive path-based program optimizations to be profitable in cost-sensitive environments, accurate path profiles must be available at low overheads. In this paper, we propose a low-overhead, non-intrusive hardware path profiling scheme that can be programmed to detect several types of paths including acyclic, intra-procedural paths, paths for a whole program path and extended paths. The profiler consists of a path stack, which detects paths and generates a sequence of path descriptors using branch information from the processor pipeline, and a hot path table that collects a profile of hot paths for later use by a program optimizer. With assistance from the processor's event detection logic, our profiler can track a host of architectural metrics along paths, enabling context-sensitive performance monitoring and bottleneck analysis. We illustrate the utility of our scheme by associating paths with a power metric that estimates power consumption in the cache hierarchy caused by instructions along the path. Experiments using programs from the SPEC CPU2000 benchmark suite show that our path profiler, occupying 7KB of hardware real-estate, collects accurate path profiles (average overlap of 88% with a perfect profile) at negligible execution time overheads (0.6% on average).",
TrustDavis: a non-exploitable online reputation system,"We present TrustDavis, an online reputation system that provides insurance against trade fraud by leveraging existing relationships between players, such as the ones present in social networks. Using TrustDavis and a simple strategy, an honest player can set an upper bound on the losses caused by any malicious collusion of players. In addition, TrustDavis incents participants to accurately rate each other, resists participants' pseudonym changes, and is inherently distributed.","Feedback,
Resists,
Computer science,
Insurance,
Social network services,
Upper bound,
Aggregates,
Internet,
Engineering profession,
Protection"
Evaluating UML Class Diagram Layout based on Architectural Importance,"The paper presents and assesses a layout scheme for UML class diagrams that takes into account the architectural importance of a class in terms of its stereotype (e.g., boundary, control, entity). The design and running of a user study is described. The results of the study supports the hypothesis that layout based on architectural importance is more helpful in class diagram comprehension compared to layouts focusing primarily on aesthetics and/or abstract graph guidelines","Unified modeling language,
Guidelines,
Navigation,
Organizing,
Sun,
Visualization,
Cognitive science,
Industrial relations"
Verifying physical presence of neighbors against replay-based attacks in wireless ad hoc networks,"Verifying physical presence of a neighbor in wireless ad hoc networks is one of the key components in developing protocols resilient to replay-based attacks. For this, we first consider RTT-based and power-based approaches. We then couple them to design an effective neighbor verification protocol (NVP). In theory, we always see some room for replay-based attacks. However, our proposed protocol significantly limits the effectiveness of replay-based attacks by restricting the range where they might be launched and thus makes them practically impossible.","Intelligent networks,
Ad hoc networks,
Routing protocols,
Mobile ad hoc networks,
Wireless application protocol,
Cryptography,
Costs,
Energy consumption,
Computer science,
Wireless networks"
Spikernels: Predicting Arm Movements by Embedding Population Spike Rate Patterns in Inner-Product Spaces,"Inner-product operators, often referred to as kernels in statistical learning, define a mapping from some input space into a feature space. The focus of this letter is the construction of biologically motivated kernels for cortical activities. The kernels we derive, termed Spikernels, map spike count sequences into an abstract vector space in which we can perform various prediction tasks. We discuss in detail the derivation of Spikernels and describe an efficient algorithm for computing their value on any two sequences of neural population spike counts. We demonstrate the merits of our modeling approach by comparing the Spikernel to various standard kernels in the task of predicting hand movement velocities from cortical recordings. All of the kernels that we tested in our experiments outperform the standard scalar product used in linear regression, with the Spikernel consistently achieving the best performance.",
High-resolution three-dimensional sensing of fast deforming objects,"In applications like motion capture, high speed collision testing and robotic manipulation of deformable objects there is a critical need for capturing the 3D geometry of fast moving and/or deforming objects. Although there exists many 3D sensing techniques, most cannot deal with dynamic scenes (e.g., laser scanning). Others, like stereovision, require that object surfaces be appropriately textured. Few, if any, build high-resolution 3D models of dynamic scenes. This paper presents a technique to compute high-resolution range maps from single images of moving and deforming objects. This method is based on observing the deformation of a projected light pattern that combines a set of parallel colored stripes and a perpendicular set of sinusoidal intensity stripes. While the colored stripes allow the sensor to compute absolute depths at coarse resolution, the sinusoidal intensity stripes give dense relative depths. This twofold pattern makes it possible to extract a high-resolution range map from each image in a video sequence. The sensor has been implemented and tested on several deforming objects.","Layout,
Robot sensing systems,
Computational geometry,
Deformable models,
Vehicle dynamics,
Navigation,
Vehicle crash testing,
Reflectivity,
Computer science,
Application software"
On the appropriateness of negative selection defined over Hamming shape-space as a network intrusion detection system,"Artificial immune systems have become popular in recent years as a new approach for intrusion detection systems. Indeed, the (natural) immune system applies very effective mechanisms to protect the body against foreign intruders. We present empirical and theoretical arguments, that the artificial immune system negative selection principle, which is primarily used for network intrusion detection systems, has been copied to naively and is not appropriate and not applicable for network intrusion detection systems.","Intrusion detection,
Immune system,
Artificial immune systems,
Computer worms,
Protection,
Detectors,
Monitoring,
Databases,
Computer science,
Diseases"
Q-CAD: QoS and context aware discovery framework for mobile systems,"This paper presents Q-CAD, a resource discovery framework that enables pervasive computing applications to discover and select the resource(s) best satisfying the user needs, taking the current execution context and quality-of-service (QoS) requirements into account. The available resources are first screened, so that only those suitable to the current execution context of the application would be considered; the shortlisted resources are then evaluated against the QoS needs of the application, and a binding is established to the best available.","Context awareness,
Protocols,
Context-aware services,
Computer science,
Optical wavelength conversion,
Pervasive computing,
Application software,
Network topology,
Routing,
Ontologies"
Wireless LAN positioning with mobile devices in a library environment,"This paper describes the issues involved in performing location tracking in a library environment using existing wireless LAN infrastructure and personal digital assistants. Limitations on computation power and operating systems support are considered in the experiment design and four mainstream location estimation algorithms: center of gravity, triangulation, smallest M-vertex polygon and fingerprinting, are being compared to facilitate location-aware computing.",
Global e-science collaboration,"Today's e-science, with its extreme-scale scientific applications, marks a turning point for high-end requirements on the compute infrastructure and, in particular, on optical networking resources. Although ongoing research efforts are aimed at exploiting the vast bandwidth of fiber-optic networks to both interconnect resources and enable high-performance applications, challenges continue to arise in the area of the optical control plane. The ultimate goal in this area is to extend the concept of application-driven networking into the optical space, providing unique features that couldn't be achieved otherwise. Many researchers in the e-science community are adopting grid computing to meet their ever-increasing computational and bandwidth needs as well as help them with their globally distributed collaborative efforts. This recent awareness of the network as a prime resource has led to a sharper focus on interactions with the optical control plane, grid middleware, and other applications. This article attempts to explain the rationale for why high-end e-science applications consider optical network resources to be as essential and dynamic as CPU and storage resources in a grid infrastructure and why rethinking the role of the optical control plane is essential for next-generation optical networks.","International collaboration,
Optical fiber networks,
Optical control,
Optical interconnections,
Bandwidth,
Grid computing,
Ultraviolet sources,
Turning,
Computer networks,
Optical computing"
Scheduling algorithms for effective thread pairing on hybrid multiprocessors,"With the latest high-end computing nodes combining shared-memory multiprocessing with hardware multithreading, new scheduling policies are necessary for workloads consisting of multithreaded applications. The use of hybrid multiprocessors presents schedulers with the problem of job pairing, i.e. deciding which specific jobs can share each processor with minimum performance penalty, by running on different execution contexts. Therefore, scheduling policies are expected to decide not only which job mix will execute simultaneously across the processors, but also which jobs can be combined within each processor. This paper addresses the problem by introducing new scheduling policies that use run-time performance information to identify the best mix of threads to run across processors and within each processor. Scheduling of threads across processors is driven by the memory bandwidth utilization of the threads, whereas scheduling of threads within processors is driven by one of three metrics: bus transaction rate per thread, stall cycle rate per thread, or outermost level cache miss rate per thread. We have implemented and experimentally evaluated these policies on a real multiprocessor server with Intel Hyperthreaded processors. The policy using bus transaction rate for thread pairing achieves an average 13.4% and a maximum 28.7% performance improvement over the Linux scheduler. The policy using stall cycle rate for thread pairing achieves an average 9.5% and a maximum 18.8% performance improvement. The average and maximum performance gains of the policy using cache miss rate for thread pairing are 7.2% and 23.6% respectively.","Scheduling algorithm,
Yarn,
Processor scheduling,
Hardware,
Multithreading,
Runtime,
Interference,
Counting circuits,
Computer science,
Educational institutions"
Accurate motion layer segmentation and matting,"Given a video sequence, obtaining accurate layer segmentation and alpha matting is very important for various applications. However, when a non-textured or smooth area is present in the scene, the segmentation based on only single motion cue usually cannot provide satisfactory results. Conversely, the most matting approaches require a smooth assumption on foreground and background to obtain a good result. In this paper, we combine the merits of motion segmentation and alpha matting technique together to simultaneously achieve high-quality layer segmentation and alpha mattes. First, we explore a general occlusion constraint and design a novel graph cuts framework to solve the layer-based motion segmentation problem for the textured regions using multiple frames. Then, an alpha matting technique is further used to refine the segmentation and resolve the non-textured ambiguities by determining proper alpha values for the foreground and background respectively.","Computer vision,
Motion segmentation,
Video sequences,
Pixel,
Layout,
Image segmentation,
Computer science,
Application software,
Image sequences,
Merging"
A new approach for software requirements elicitation,"Requirements elicitation is both the hardest and most critical part of software development, since errors at this beginning stage propagate through the development process and are the hardest to repair later. This paper proposes an improved process for requirements elicitation. The key improvements are: (1) to train the non-technical stakeholders (primarily the users) in the capabilities and limitations of computer hardware, software, and of software developers; (2) identify keywords while interviewing the stakeholders, visually as well as in text form; (3) use keyword mapping to generate candidate system requirements; (4) apply the techniques of quality function deployment (QFD) and the Capability Maturity Model (CMM) during the elicitation process.","Programming,
Computer science,
Software quality,
Quality function deployment,
Electric breakdown,
Information analysis,
Automotive engineering,
Computer errors,
Hardware,
Capability maturity model"
Using objects of measurement to detect spreadsheet errors,"There are many common spreadsheet errors that traditional spreadsheet systems do not help users find. This paper presents a statically-typed spreadsheet language that adds additional information about the objects that the spreadsheet values represent. By annotating values with both units and labels, users denote both the system of measurement in which the values are expressed as well as the properties of the objects to which the values refer. This information is used during computation to detect some invalid computations and allow users to identify properties of the resulting values.","Object detection,
Costs,
Computer errors,
Computer science,
USA Councils,
Inspection,
Computer displays,
Concrete,
Measurement units"
Multi-tag radio frequency identification systems,"We propose and analyze the effects of attaching more than one RFID tag to each object. We define different types of multi-tag systems and examine their benefits, both analytically and empirically. We also analyze how multi-tags affect some existing tag singulation algorithms. We show how multi-tags can serve as security enhancers, and propose several new promising applications of multi-tags, such as preventing illegal deforestation.","Radiofrequency identification,
Electromagnetic coupling,
Computer science,
Algorithm design and analysis,
Security,
Backscatter,
Electromagnetic propagation,
Magnetic fields,
Voltage,
Joining processes"
Controlling Gossip Protocol Infection Pattern Using Adaptive Fanout,"We propose and evaluate a model for controlling infection patterns defined over rounds or real time in a gossip-based protocol using adaptive fanout. We model three versions of gossip-based protocols: the synchronous protocol, the pseudosynchronous protocol and the asynchronous protocol. Our objective is to ensure that the members of a group receive a desired message within a bounded latency with very high probability. We argue that the most important parameter that controls the latency of message delivery is the fanout used during gossiping, i.e., the number of gossip targets chosen in a particular instance of gossip. We formally analyze the three protocols and provide expressions for fanout. We introduce the idea of using variable fanouts in different rounds in the synchronous protocol. We define fanout as a function of time for the asynchronous protocol such that an expected infection pattern is observed with high probability. For a better understanding of the theoretical model, we develop a pseudosynchronous protocol to highlight the modelling done in order to derive time dependent fanout. We show that our protocols generate Theta(n log n) messages, which is optimal for gossip protocols. We aim to use the gossiping mechanism for large-scale group communication with soft real time constraints. This would alleviate the dependence on tree-based deterministic protocols which usually lack scalability",
Signals and Systems using MATLAB: an integrated suite of applications for exploring and teaching media signal processing,"Effectively teaching introductory media signal processing (MSP) to students requires a means of communicating complex concepts without advanced mathematics. At the Media Arts and Technology program at UCSB we are teaching graduate media arts students concepts of MSP. These students, while interested in learning how their digital tools work, have an incredibly tough time working through the material the way traditional engineering students do. To address this we have used MATLAB to build a comprehensive suite of exploratory demonstrations and applications tailored to illustrate sophisticated concepts, and to inspire students that may not possess a strong background in mathematics. Our application, ""Signals and Systems using MATLAB"" (SSUM), is presented here, and its use in a course designed to teach MSP to media arts students is discussed. Its usefulness extends beyond media arts to engineering and computer science curriculum. SSUM can be obtained for free from http://www.mat.ucsb.edu/tildeb.sturm","Signal processing,
MATLAB,
Art,
Application software,
Mathematics,
Digital signal processing,
Engineering students,
Computer science education,
Educational programs,
Signal processing algorithms"
DI-GRUBER: A Distributed Approach to Grid Resource Brokering,"Managing usage service level agreements (USLAs) within environments that integrate participants and resources spanning multiple physical institutions is a challenging problem. Maintaining a single unified USLA management decision point over hundreds to thousands of jobs and sites can become a bottleneck in terms of reliability as well as performance. DIGRUBER, an extension to our GRUBER brokering framework, was developed as a distributed grid USLAbased resource broker that allows multiple decision points to coexist and cooperate in real-time. DIGRUBER addresses issues regarding how USLAs can be stored, retrieved, and disseminated efficiently in a large distributed environment. The key question this paper addresses is the scalability and performance of DI-GRUBER in large Grid environments. We conclude that as little as three to five decision points can be sufficient in an environment with 300 sites and 60 VOs, an environment ten times larger than todayâ€™s Open Science Grid.","Scheduling,
Scalability,
Laboratories,
Permission,
Computer science,
Environmental management,
Resource management,
Maintenance,
Outsourcing,
Collaborative work"
An empirical comparison of test suite reduction techniques for user-session-based testing of Web applications,"Automated cost-effective test strategies are needed to provide reliable, secure, and usable Web applications. As a software maintainer updates an application, test cases must accurately reflect usage to expose faults that users are most likely to encounter. User-session-based testing is an automated approach to enhancing an initial test suite with real user data, enabling additional testing during maintenance as well as adding test data that represents usage as operational profiles evolve. Test suite reduction techniques are critical to the cost effectiveness of user-session-based testing because a key issue is the cost of collecting, analyzing, and replaying the large number of test cases generated from user-session data. We performed an empirical study comparing the test suite size, program coverage, fault detection capability, and costs of three requirements-based reduction techniques and three variations of concept analysis reduction applied to two Web applications. The statistical analysis of our results indicates that concept analysis-based reduction is a cost-effective alternative to requirements-based approaches.","Automatic testing,
Application software,
Software testing,
Costs,
Software maintenance,
Fault detection,
Performance evaluation,
Uniform resource locators,
Computer science,
Performance analysis"
Leveraging relational autocorrelation with latent group models,"The presence of autocorrelation provides a strong motivation for using relational learning and inference techniques. Autocorrelation is a statistical dependence between the values of the same variable on related entities and is a nearly ubiquitous characteristic of relational data sets. Recent research has explored the use of collective inference techniques to exploit this phenomenon. These techniques achieve significant performance gains by modeling observed correlations among class labels of related instances, but the models fail to capture a frequent cause of autocorrelation - the presence of underlying groups that influence the attributes on a set of entities. We propose a latent group model (LGM) for relational data, which discovers and exploits the hidden structures responsible for the observed autocorrelation among class labels. Modeling the latent group structure improves model performance, increases inference efficiency, and enhances our understanding of the datasets. We evaluate performance on three relational classification tasks and show that LGM outperforms models that ignore latent group structure, particularly when there is little information with which to seed inference.","Autocorrelation,
Motion pictures,
Web pages,
Computer science,
Performance gain,
Predictive models,
Graphical models,
Advertising,
Web sites,
Data mining"
Innovative use of mobile learning for occupational stress: evaluation of non functional requirements and architectures,"We investigate an innovative use of mobile learning for long term absents (LTAs) from work for occupational stress. There is a need to represent the educational needs of LTAs, and to reason on them for choosing the most suitable mobile learning contents (MLC) and mobile learning architectures. The main aim of this paper is to tackle the problem of evaluating MLC non functional requirements (NFRs) by developing a scheme for annotating NFRs to architectures.",
Timing driven track routing considering coupling capacitance,"As VLSI technology enters the ultra-deep submicron era, wire coupling capacitance starts to dominate self capacitance and can no longer be neglected in timing driven routing. In this paper, a coupling aware timing driven track routing heuristic is proposed. Given a global routing solution and timing constraint for each net, major trunks of wire segments are assigned to routing tracks such that the minimum timing slack among all nets is maximized. Delay penalties from both coupling capacitance and wire detour are considered in a unified graph model. The core problem is formulated and solved as a sequential ordering problem (SOP). Routing blockages are handled in a post processing procedure. The experimental results on benchmark circuits show that the effect of coupling capacitance on timing is significant and the proposed heuristic results in greater improvement on coupling aware timing compared with other approaches.",
Analysis of humanoid appearances in human-robot interaction,"It is important to identify how much the appearance of a humanoid robot affects human behaviors toward it. We compared participants' impressions of and behaviors toward two real humanoid robots in simple human-robot interaction. These two robots have different appearances but are controlled to perform the same recorded utterances and motions, which are adjusted by using a motion capturing system. We conducted an experiment where 48 human participants participated. In the experiment, participants interacted with the two robots one by one and also with a human as a reference. As a result, we found that the different appearances did not affect the participants' verbal behaviors but did affect their non-verbal behaviors such as distance and delay of response. These differences are explained by two factors, impressions and attributions.","Humanoid robots,
Human robot interaction,
Robot sensing systems,
Mobile robots,
Intelligent robots,
Arm,
Computer interfaces,
Face detection,
Legged locomotion,
Research and development"
Lower bounds for the noisy broadcast problem,"We prove the first nontrivial (superlinear) lower bound in the noisy broadcast model of distributed computation. In this model, there are n + 1 processors P/sub 0/, P/sub 1/, ..., P/sub n/. Each P/sub i/, for i /spl ges/ 1, initially has a private bit x/sub i/ and the goal is for P/sub 0/ to learn f (x/sub l/, ..., x/sub n/) for some specified function f. At each time step, a designated processor broadcasts some function of its private bit and the bits it has heard so far. Each broadcast is received by the other processors but each reception may be corrupted by noise. In this model, Gallager (1988) gave a noise-resistant protocol that allows P/sub 0/ to learn the entire input in O(n log log n) broadcasts. We prove that Gallager's protocol is optimal up to a constant factor. Our lower bound follows from a lower bound in a new model, the generalized noisy decision tree model, which may be of independent interest.","Broadcasting,
Protocols,
Noise cancellation,
Context modeling,
Computational modeling,
Distributed computing,
Quantum computing,
Decision trees,
Circuit noise,
Computer networks"
Host anomalies from network data,"Network administrators need to be able to quickly synthesize a large amount of raw data into comprehensive information and knowledge about a network system in order to determine if there is any unusual activity occurring on that network. This paper presents some initial results of a simplistic baselining method applied to a class B-sized network. These baselines are then used as the basis for an anomaly detection system that examines unusual amounts of activity to any one port on any one host. Thus we provide a system that can detect changes in the activity of any one host, regardless of whether those changes are noticeable when observing overall traffic behavior.",
DNA motif detection using particle swarm optimization and expectation-maximization,"Motif discovery, the process of discovering a meaningful pattern of nucleotides or amino acids that is shared by two or more molecules, is an important part of the study of gene function. In this paper, we propose a hybrid motif discovery approach based upon a combination of particle swarm optimization (PSO) and the expectation-maximization (EM) algorithm. In the proposed algorithm, we use PSO to generate a seed for the EM algorithm.","DNA,
Particle swarm optimization,
Sequences,
Hidden Markov models,
Computer science,
Genomics,
Bioinformatics,
Sampling methods,
Amino acids,
Background noise"
Time-domain noise analysis of linear time-Invariant and linear time-variant systems using MATLAB and HSPICE,"A custom simulation tool that combines HSPICE and MATLAB to enable time-domain noise analysis is reported. The simulation technique is based on computing the statistics of a random process by ensemble averaging and is applicable to both linear time-invariant (LTI) and linear time-variant (LTV) systems. MATLAB is used to generate a set of representative noise signals, which are imported into HSPICE for simulation. Once the simulations are complete the results are read back into MATLAB and ensemble statistics are calculated. The MATLAB-generated noise signals have a user-defined white-noise floor and flicker-noise corner frequency and thus are suitable for modeling a wide variety of electronic components, including CMOS transistors and resistors. Simulation results of the time-dependent output noise of a gated integrator and the timing resolution of a gated integrator/comparator detector are presented to highlight both the utility and the versatility of the tool.","Time domain analysis,
MATLAB,
Computational modeling,
1f noise,
Statistics,
Analytical models,
Random processes,
Signal generators,
Noise generators,
Computer languages"
Optimal routing with multiple traffic matrices tradeoff between average and worst case performance,"In this paper, we consider the problem of finding an ""efficient"" and ""robust"" set of routes in the face of changing/uncertain traffic. The changes/uncertainty in exogenous traffic is characterized by multiple traffic matrices. Our goal is to find a set of routes that result in good average case performance over the set of traffic matrices, while avoiding bad worst case performance for any single traffic matrix. With multiple traffic matrices, previous work aims solely to optimize the average case performance Chun Zhang, et al., (2005), or the worst case performance David Applegate, et al., (2003). For a given set of traffic matrices, different sets of routes offer a different tradeoff between the average case and the worst case performance. In this paper, we quantify the performance of a routing configuration at both network level and link level. We propose a simple metric-a weighted sum of the average case and the worst case performance-to control the tradeoff between these two considerations. Despite of its simple form, this metric is very effective. We prove that optimizing routing using this metric has desirable properties, such as the average case performance being a decreasing, convex and differentiable function to the worst case performance. By extending previous work Chun Zhang, et al., (2005) Bernard Fortz, et al., (2002), we derive methods to find the optimal routes with respect to the proposed metric for two classes of intra-domain routing protocols: MPLS and OSPF/IS-IS. We evaluate our approach with data collected from an operational tier-I ISP. For MPLS, we find that there exists significant tradeoff (e.g., 15%-23% difference) between optimizing solely on the average case performance and solely on the worst case performance. Our approach can identify solutions that can dramatically improve the worst case performance (13%-15%) while only slightly sacrificing the average case performance (2.2%-3%), in comparison to that by optimizing solely on the average case performance. For OSPF/IS-IS, we still find a significant difference between the two optimization objectives, however, a fine-grained tradeoff is difficult to achieve due to the limited control that OSPF/IS-IS provide.","Computer aided software engineering,
Multiprotocol label switching,
Routing protocols,
Cost function,
Intersymbol interference,
Computer science,
Uncertainty,
Telecommunication traffic,
Delay,
Internet"
Efficient learning of relational object class models,"We present an efficient method for learning part-based object class models. The models include location and scale relations between parts, as well as part appearance. Models are learnt from raw object and background images, represented as an unordered set of features extracted using an interest point detector. The object class is generatively modeled using a simple Bayesian network with a central hidden node containing location and scale information, and nodes describing object parts. The model's parameters, however are optimized to reduce a loss function which reflects training error as in discriminative methods. Specifically, the optimization is done using a boosting-like technique with complexity linear in the number of parts and the number of features per image. This efficiency allows our method to learn relational models with many parts and features, and leads to improved results when compared with other methods. Extensive experimental results are described, using some common bench-mark datasets and three sets of newly collected data, showing the relative advantage of our method","Bayesian methods,
Optimization methods,
Object recognition,
Computer science,
Feature extraction,
Data mining,
Object detection,
Detectors,
Humans,
Lighting"
Simple detectors for shaped-offset QPSK using the PAM decomposition,"In this paper we develop a reduced-complexity detection scheme for shaped-offset quadrature phase-shift keying (SOQPSK), a highly bandwidth-efficient constant-envelope modulation. The detector is based on the well-known pulse amplitude modulation (PAM) representation of continuous phase modulation (CPM). Since SOQPSK is a ternary CPM, we show how the binary-based PAM technique is extended to accommodate the ternary case. We demonstrate that a detector based on the first two PAM components requires a simple trellis of only 4 states. We show that near-optimum performance is achieved using this reduced-complexity detector. The potential complexity reduction can be quite large, since one version of SOQPSK requires a trellis of 512 states.","Detectors,
Quadrature phase shift keying,
Phase detection,
Frequency,
Pulse modulation,
Continuous phase modulation,
Bandwidth,
Military standards,
Telemetry,
Computer science"
The Entity Container - An Object-Oriented and Model-Driven Persistency Cache,"Data persistency is a fundamental, but complex aspect of a modern software development process. Therefore, in order to reduce development costs and improve a system's quality, support for data persistency must be provided to common software paradigms, such as object-oriented programming or component based development. In this paper we present a new approach of an object persistency cache - the Entity Container (EC), based on a data model. The EC allows data and metadata management according to a data model independent of any specific persistency mechanism. We present the complete architecture, functionality and implementation of the system and compare our new approach with existing frameworks in order to point out features and major improvements of the EC.","Containers,
Object oriented modeling,
Unified modeling language,
Application software,
Programming,
Costs,
Data models,
Computer architecture,
Memory,
Informatics"
A taxonomy of software component models,"CBSE currently lacks a universally accepted terminology. Existing component models adopt different component definitions and composition operators. We believe that for future research it would be crucial to clarify and unify the CBSE terminology, and that the starting point for this endeavour should be a study of current component models. In this paper, we take this first step and present and discuss a taxonomy of these models. The purpose of this taxonomy is to identify the similarities and differences between them with respect to commonly accepted criteria, with a view to clarification and/or potential unification.","Taxonomy,
Terminology,
Assembly,
Computer science,
Councils,
Java,
Fractals,
Software engineering,
Application software,
Computer languages"
Sequential design and rational metamodelling,"Metamodelling techniques are used in many engineering applications for efficient exploration of the design space of complex deterministic simulation systems, and for optimisation purposes and sensitivity analysis. This paper presents a new sequential design and rational metamodelling technique which combines adaptive modelling and sampling algorithms.","Metamodeling,
Sampling methods,
Design optimization,
Input variables,
Polynomials,
Mathematics,
Computer science,
Design engineering,
Application software,
Computational modeling"
Non-preemptive earliest-deadline-first scheduling policy: a performance study,"This paper introduces an analytical method for approximating the performance of a soft real-time system modeled by a single-server queue. The service discipline in the queue is earliest-deadline-first (EDF), which is an optimal scheduling policy. Real-time jobs with exponentially distributed deadlines arrive according to a Poisson process. All jobs have deadlines until the end of service and are served non-preemptively. Occurrences of transient faults in the server are also taken into account. The important performance measure to calculate is the loss probability due to deadline misses and/or transient faults. The system is approximated by a Markovian model in the long run. A key parameter, namely, the loss rate when there are n jobs in the system is used in the model, which is estimated by partitioning the system into two virtual subsystems. The resulting model can then be solved analytically using standard Markovian solution techniques. Comparing numerical and simulation results, we find that the existing errors are relatively small.","Job shop scheduling,
Optimal scheduling,
Real time systems,
Processor scheduling,
Telecommunication computing,
Timing,
Multimedia systems,
Packet switching,
Hardware,
Computer science"
Automated Vertebra Detection and Segmentation from the Whole Spine MR Images,"Our algorithm contains two major steps: the intervertebral disk localization step, and the vertebra detection and segmentation step. In the first step, we apply a model-based searching method to approximately locate all the intervertebral disk clues between adjacent vertebrae of the whole spine and the best slice selection. A new approach using an intensity profile on a polynomial function for fitting all these disk clues on the best slice is then used to refine the disk search process. Vertebra centers are detected, and initial boundaries are extracted in the second step. The initial test of the algorithm on the five sets of 7 sagittal slices locates all 23 intervertebral disk centers for the best slice of all five sets. For the evaluation of the boundary extraction of 22 vertebrae, our algorithm successfully locates 100%, 96.6%, 93.2%, 95.5%, 87.5% vertebra corners in image set No.1, 2, 3, 4, and 5, respectively","Image segmentation,
Spine,
Image edge detection,
Bone diseases,
Degenerative diseases,
Osteoporosis,
Polynomials,
Testing,
Medical treatment,
Surgery"
The impact of fading correlation on the error performance of MIMO systems over Rayleigh fading channels,This paper analyzes the impact of receive fading correlation on the error performance of a multiple-input multiple-output (MIMO) system that employs a zero-forcing detection scheme over frequency-nonselective Rayleigh fading channels. Error rate expressions as a function of the eigenvalues of the fading correlation matrix and the number of transmit and receive antennas are derived. Numerical results indicate that MIMO systems are resistant to receive fading correlation.,"Rayleigh channels,
MIMO,
Fading,
Receiving antennas,
Performance analysis,
Upper bound,
Frequency,
Eigenvalues and eigenfunctions,
Computer science,
Intersymbol interference"
ScatterType: a legible but hard-to-segment CAPTCHA,"The ScatterType CAPTCHA (completely automated public Turing tests to tell computers and humans apart), designed to resist character-segmentation attacks and shown to be highly legible to human readers, is analyzed for vulnerabilities and is offered for experiments in automatic attack. As introduced in Baird and Riopka (2005), 'ScatterType' challenges are images of machine-print text whose characters are cut into pieces which then drift apart, in an attempt to frustrate segment-then-recognize computer vision attacks. Analysis of experimental human legibility data has shown that better than 95% correct legibility can be achieved through judicious choice of the pseudorandom generating parameters (Baird et al., 2005). That analysis is summarized and discussed here as motivation for a discussion of potential vulnerabilities. An invitation to attack ScatterType is offered.",
The Paradox of Software Visualization,"Software visualization seems like such a logical and helpful concept with obvious benefits and advantages. But after decades of research and work, it has yet to be successful in any mainstream development environment. What is the reason for this paradox? Will software visualization ever be actually widely used? In this paper we argue that most past and current work in the field (our own included) is out of touch with the reality of software development and that new approaches and new ideas are needed","Programming profession,
Unified modeling language,
Data visualization,
Instruments,
Production,
Computer science,
Software design,
Data structures,
Software debugging,
Animation"
Characterization of Low-Temperature Sintered Nanoscale Silver Paste for Attaching Semiconductor Devices,"Attachment of semiconductor devices to a package substrate is essential for providing electrical and structural connections as well as a heat dissipation path. The die-attach materials play a vital role in ensuring the system performance and reliability. As the electronics industry continues to integrate more functions in smaller packages, the electrical, thermal and mechanical properties of the existing die-attach materials such as solders and conductive epoxies fail to meet more demanding requirements for performance and reliability. To address this problem, we developed low-temperature sintered nanosilver as a new die-attaching material. Experimental results of the electrical, thermal, and mechanical properties of the sintered silver die-attachment are presented in this paper. The nanoscale silver paste was made by dispersing 30-nm silver powder under ultrasonic agitation in an organic binder system. The electrical resistivity, obtained from screen printed resistor patterns on an insulate substrate that were sintered at 280degC for around 10 minutes in air, was found to be 2.6 times 10 5 (Omegamiddotcm)-1. The thermal conductivity was obtained by the laser flash method and was found to be 240 W/K-m. Both values are lower than those of bulk silver because the sintered silver had a density around 80%. The coefficient of thermal expansion (CTE) of the sintered silver was measured by dilatometry and was found to be 19 times 10-6/degC, nearly identical to that of bulk silver. The apparent elastic modulus of the sintered silver was found to be 9 GPa while the yield strength was around 43 MPa. Furthermore, die-shear tests on devices bonded by the sintered silver gave strength of around 21 MPa for a gold-metallized substrate and strength of 38 MPa for a silver-metallized substrate. These results demonstrate that the nanoscale silver paste sintered at low temperature is an excellent alternative to solders or epoxies for die attachment","Nanoscale devices,
Silver,
Joining processes,
Semiconductor devices,
Electronic packaging thermal management,
Conducting materials,
Thermal conductivity,
Semiconductor device packaging,
Substrates,
Materials reliability"
Separating Abstractions from Resources in a Tactical Storage System,"Sharing data and storage space in a distributed system remains a difficult task for ordinary users, who are constrained to the fixed abstractions and resources provided by administrators. To remedy this situation, we introduce the concept of a tactical storage system (TSS) that separates storage abstractions from storage resources, leaving users free to create, reconfigure, and destroy abstractions as their needs change. In this paper, we describe how a TSS can provide a variety of filesystem and database abstractions for unmodified applications without requiring special privileges or kernel changes. A TSS provides performance competitive with NFS for single clients and also scales well for multiple servers and multiple clients. A prototype TSS of 120 disks and 6 TB of storage has been deployed at the University of Notre Dame and used for applications in high energy physics and bioinformatics.","Energy storage,
Grid computing,
Hardware,
Permission,
Computer science,
Data engineering,
Power engineering and energy,
Databases,
Kernel,
Prototypes"
On the hardness of approximating MULTICUT and SPARSEST-CUT,"We show that the MULTICUT, SPARSEST-CUT, and MIN-2CNF/spl equiv/DELETION problems are NP-hard to approximate within every constant factor, assuming the unique games conjecture of Khot [STOC, 2002]. A quantitatively stronger version of the conjecture implies inapproximability factor of /spl Omega/(log log n).","Costs,
Computer science,
Mathematics,
Linear programming,
Approximation algorithms"
Compressing the illumination-adjustable images with principal component analysis,"The ability to change illumination is a crucial factor in image-based modeling and rendering. Image-based relighting offers such capability. However, the tradeoff is the enormous increase of storage requirement. In this paper, we propose a compression scheme that effectively reduces the data volume while maintaining the real-time relighting capability. The proposed method is based on principal component analysis (PCA). A block-wise PCA is used to practically process the huge input data. The output of PCA is a set of eigenimages and the corresponding relighting coefficients. By dropping those low-energy eigenimages, the data size is drastically reduced. To further compress the data, eigenimages left are compressed using transform coding and quantization while the relighting coefficients are compressed using uniform quantization. We also suggest the suitable target bit rate for each phase of the compression method in order to preserve the visual quality. Finally, we propose a real-time engine that relights images from the compressed data.","Image coding,
Principal component analysis,
Lighting,
Rendering (computer graphics),
Navigation,
Transform coding,
Quantization,
Layout,
Computer science,
Bit rate"
Enhancing QoS support for vertical handoffs using implicit/explicit handoff notifications,"Vertical handoffs between different wireless technologies usually lead to dramatic changes in the link capacity. A successful QoS solution for vertical handoffs must be able to fast track the capacity changes and agilely adapt the delivery rates and qualities of the ongoing applications. Though traditional AIMD-based source adaptation schemes (as found in TCP, TFRC, etc.) have been well designed for mild, gradual rate adjustments required by load fluctuations and network congestion, their response time is inadequate when the rate must be adjusted to the drastic network capacity changes that are typical in vertical handoff scenarios. To expedite the response to such changes, we propose in this paper two adaptive algorithms, named the fast rate adaptation (FRA) and early rate reduction (ERR), that are launched when the handoff is from low to high capacity (LOW-to-HIGH) or from high to low capacity (HIGH-to-LOW), respectively. We also propose two vertical handoff notification mechanisms to work with FRA and/or ERR, i.e. the implicit handoff notification (IHN) and explicit handoff notification (EHN). We show by simulation that our proposed schemes are able to provide better QoS support than the traditional AIMD based schemes during vertical handoffs",
A dual dielectric approach for performance aware gate tunneling reduction in combinational circuits,"With continued and aggressive scaling, using ultra-low thickness SiO/sub 2/ for the transistor gates, tunneling current has emerged as the major component of leakage in CMOS circuits. In this paper, we propose a new approach called dual dielectrics of dual thicknesses (DKDT) for the reduction of both ON and OFF state gate tunneling currents. We claim that the simultaneous utilization of SiON and SiO/sub 2/ each with multiple thicknesses is a better approach for gate leakage reduction than the conventional one that uses a single gate dielectric, SiO/sub 2/, of multiple thicknesses. We develop an algorithm for the corresponding assignment of dual dielectric and dual thickness cells that minimizes the overall tunneling current for a circuit without compromising its performance. We performed extensive experiments on ISCAS'85 benchmarks using 45 nm technology which demonstrate that our approach can reduce the tunneling current by as much as 98.7% (on average 94.8%), without performance degradation.","Dielectrics,
Tunneling,
Combinational circuits,
CMOS logic circuits,
Leakage current,
Computer science,
Gate leakage,
CMOS technology,
Voltage,
Silicon"
Control to facet problems for affine systems on simplices and polytopes &#8211; With applications to control of hybrid systems,"In this paper, a general control-to-facet problem for affine systems on polytopes is studied: find an affine feedback law such that all trajectories of the closed-loop system leave the state polytope through an a priori specified (possibly empty) set of facets. Solutions are presented in terms of (bi)linear inequalities in the coefficients of the affine feedback. The result is applied to control synthesis for piecewise-affine hybrid systems. Using a backward recursion algorithm, a sufficient condition for reachability of hybrid systems is obtained, and a piecewise-affine controller is computed that realizes the required reachability property.","Control systems,
Automata,
State feedback,
Automatic control,
Control system synthesis,
Systems engineering and theory,
Physics computing,
Sufficient conditions,
Manifolds,
Engines"
Decision-theoretic throttling for optimistic simulations of multi-agent systems,"In this paper we present a throttling mechanism for optimistic simulations of multi-agent systems, which delays read accesses to the shared simulation state that are likely to be rolled back. We develop a decision-theoretic model of rollback and show how this can be used to derive the optimal time to delay a read event so as to minimize the expected overall execution time of the simulation. We briefly describe an implementation of this approach in ASSK, a distributed simulation kernel developed to investigate synchronization mechanisms for MAS simulation, and report the results of preliminary experiments to evaluate the effectiveness of our approach.","Multiagent systems,
Computational modeling,
Discrete event simulation,
Delay effects,
Kernel,
Computer science,
Computer simulation,
Information technology,
Delay systems,
Research and development"
Space-time scene manifolds,"The space of images is known to be a nonlinear sub-space that is difficult to model. This paper derives an algorithm that walks within this space. We seek a manifold through the video volume that is constrained to lie locally in this space. Every local neighborhood within the manifold resembles some image patch. We call this the scene manifold because the solution traces the scene outline. For a broad class of inputs the problem can be posed as finding the shortest path in a graph and can thus be solved efficiently to produce the globally optimal solution. Constraining appearance rather than geometry gives rise to numerous new capabilities. Here we demonstrate the usefulness of this approach by posing the well-studied problem of mosaicing in a new way. Instead of treating it as geometrical alignment, we pose it as an appearance optimization. Since the manifold is constrained to lie in the space of valid image patches, the resulting mosaic is guaranteed to have the least distortions possible. Any small part of it can be seen in some image even though the manifold spans the whole video. Thus it can deal seamlessly with both static and dynamic scenes, with or without 3D parallax. Essentially, the method simultaneously solves two problems that have been solved only separately until now: alignment and mosaicing.",
Teaching a Course on Software Architecture,"Software architecture is a relatively new topic in software engineering. It is quickly becoming a central issue, and leading-edge organizations spend a considerable fraction of their development effort on software architecture. Consequently, software architecture is increasingly often the topic of a dedicated course in software engineering curricula. There are two general flavors as for the contents of such a course. One flavor emphasizes the programming-in-the-large aspects of software architecture and concentrates on design and architectural patterns, architecture description languages and the like. The other emphasizes the communication aspects of software architecture to a variety of stakeholders, thereby acknowledging a broader view of software architecture. In this paper we report our experiences with two master-level courses in software architecture that focus on these communication aspects. We show that, by appropriately focusing the contents of such a course, key aspects of this industrially very relevant field within software engineering can be taught successfully in a university setting","Education,
Software architecture,
Computer architecture,
Software engineering,
Architecture description languages,
Computer industry,
Programming,
Software standards,
Maintenance engineering,
Design engineering"
Integrating vision and speech for conversations with multiple persons,"An essential capability for a robot designed to interact with humans is to show attention to the people in its surroundings. To enable a robot to involve multiple persons into interaction requires the maintenance of an accurate belief about the people in the environment. In this paper, we use a probabilistic technique to update the knowledge of the robot based on sensory input. In this way, the robot is able to reason about the uncertainty in its belief about people in the vicinity and is able to shift its attention between different persons. Even people who are not the primary conversational partners are included into the interaction. In practical experiments with a humanoid robot, we demonstrate the effectiveness of our approach.","Speech,
Robot sensing systems,
Humanoid robots,
Human robot interaction,
Face detection,
Robot vision systems,
Cameras,
Computer science,
Uncertainty,
Visual perception"
Measuring fine-grained change in software: towards modification-aware change metrics,"In this paper we propose the notion of change metrics, those that measure change in a project or its entities. In particular we are interested in measuring fine-grained changes, such as those stored by version control systems (such as CVS). A framework for the classification of change metrics is provided. We discuss the idea of change metrics which are modification aware, that is metrics which evaluate the change itself and not just the change in a measurement of the system before and after the change. We then provide examples of the use of these metrics on two mature projects",
Contract-based mutation for testing components,"Testing plays an important role in the maintenance of component based software development. Test adequacy for component testing is one of the hardest issues for component testing. To tackle this problem, it is a natural idea to apply mutation testing, which is a fault-based testing method used for measuring test adequacy, for component contracts, whose aim is to improve the testability of the component. Though powerful, mutation testing is usually very computation-expensive, as many mutants need to be produced and executed in mutation testing. In this paper, we propose a contract-based mutation technique for testing components. Based on the discordance between contracts and specification, our approach employs a set of high level contract mutation operators. The experimental results show that these operators can greatly reduce the number of mutants compared with traditional mutation operators. At the same time, the contract-based mutation using our contract mutation operators can provide almost the same ability as that of using traditional mutation operators. Moreover, effective test suite can be produced to reduce the maintenance effort.","Genetic mutations,
Contracts,
Software testing,
Software reusability,
Software maintenance,
Electronic equipment testing,
Programming,
Application software,
Computer science,
Automation"
Maintaining correctness in scientific programs,"Combine a high rate of change (which makes correctness hard to maintain) with an increased sensitivity to failure to maintain correctness and you have a big problem. Solving this problem must be the focus of our methodology. In this paper, the author describes the layered approach that he found to be the most successful in maintaining correctness in the face of rapid change.","Testing,
Programming profession,
Environmental management,
Target tracking,
Open source software,
Maintenance engineering,
Robustness,
Best practices,
History"
Distributed optimal contention window control for elastic traffic in wireless LANs,"This paper presents a theoretical study on distributed contention window control algorithms for achieving arbitrary bandwidth allocation policies and efficient channel utilization. By modeling different bandwidth allocation policies as an optimal contention window assignment problem, we design a general and fully distributed contention window control algorithm, called GCA (general contention window adaptation), and prove that it converges to the solution of the contention window assignment problem. By examining the stability of GCA, we identify the optimal stable point that maximizes channel utilization and provide solutions to control the stable point of GCA near the optimal point. Due to the generality of GCA, our work provides a theoretical foundation to analyze existing and design new contention window control algorithms.","Optimal control,
Wireless LAN,
Local area networks,
Bandwidth,
Channel allocation,
Telecommunication traffic,
Algorithm design and analysis,
Computer science,
Application software,
Distributed control"
Is ICA significantly better than PCA for face recognition?,"The standard PCA was always used as baseline algorithm to evaluate ICA-based face recognition systems in the previous research. In this paper, we examine the two architectures of ICA for image representation and find that ICA architecture I involves a PCA process by vertically centering (PCA I), while ICA architecture II involves a whitened PCA process by horizontally centering (PCA II). So, it is reasonable to use these two PCA versions as baseline algorithms to revaluate the ICA-based face recognition systems. The experiments were performed on the FERET face database. The experimental results show there is no significant performance differences between ICA architecture I (II) and PCA I (II), although ICA architecture II significantly outperforms the standard PCA. It can be concluded that the performance of ICA strongly depends on its involved PCA process. The pure ICA projection has little effect on the performance of face recognition.","Independent component analysis,
Principal component analysis,
Face recognition,
Biometrics,
Image representation,
Computer science,
Computer architecture,
Databases,
Information security,
Law enforcement"
Solution space for fixed-priority with preemption threshold,"This paper reaffirms that fixed-priority with preemption threshold (FPPT) is an important form of real-time scheduling algorithm, which fills the gap between fixed-priority preemptive (FPP) and fixed-priority nonpreemptive (FPNP). When a task set is schedulable by FPPT, there may exist multiple valid preemption threshold assignments, which provide useful scheduling options. All valid assignments form a solution space that is delimited by a minimal and maximal assignment. A mechanism is presented to generate part of the valid assignments once the minimal and maximal assignments are known. The known algorithm to compute the minimal assignment starts at FPP, and the known algorithm to compute the maximal assignment starts from any valid assignment. This paper presents algorithms to compute the minimal and maximal assignments starting from FPNP, and the proofs for the correctness of these algorithms are also presented.","Scheduling algorithm,
Processor scheduling,
Delay,
Computer science,
Dynamic scheduling,
Dispatching,
Resource management,
Access protocols,
Timing,
Testing"
Blocking anonymity threats raised by frequent itemset mining,"In this paper we study when the disclosure of data mining results represents, per se, a threat to the anonymity of the individuals recorded in the analyzed database. The novelty of our approach is that we focus on an objective definition of privacy compliance of patterns without any reference to a preconceived knowledge of what is sensitive and what is not, on the basis of the rather intuitive and realistic constraint that the anonymity of individuals should be guaranteed. In particular, the problem addressed here arises from the possibility of inferring from the output of frequent itemset mining (i.e., a set of item-sets with support larger than a threshold a), the existence of patterns with very low support (smaller than an anonymity threshold k)[M. Atzori et. al, 2005]. In the following we develop a simple methodology to block such inference opportunities by introducing distortion on the dangerous patterns.","Itemsets,
Data mining,
Databases,
Association rules,
Laboratories,
Data privacy,
Computer science,
Data analysis,
Protection,
Distortion measurement"
Performance analysis of high-speed MOS transistors with different layout styles,"Several layout schemes for MOS transistors have been investigated and compared in terms of speed and layout area. Among them, the so-called closed, donut or doughnut transistors have been characterized, obtaining an analytical expression for the calculation of the equivalent W/L ratio for a general n-side regular polygonal-shape. The comparisons show that with quasi-minimum dimension transistors and L=0.35 /spl mu/m, reductions of up to 81% on the drain area can be achieved with an increase of only a 10% on the total layout area for given W and L. An application improving the switching speed of an output multiplexer is shown.","Performance analysis,
MOSFETs,
Parasitic capacitance,
Computer science,
Multiplexing,
Signal resolution,
Energy consumption,
Circuit topology,
Power supplies,
Shape"
Patch dynamics for multiscale problems,"The engineering analysis and microscopic simulations required for predicting materials' properties from atomistic descriptions require approaches for predicting macroscopic properties. Patch dynamics bridges the gap between the time and space scales at which the microscopic models operate, helping predict system-level behavior.","Predictive models,
Microscopy,
Fluid dynamics,
Nonlinear equations,
Finite difference methods,
Nonlinear dynamical systems,
Bridges,
Stress,
Kinetic theory,
Deformable models"
Formal verification and its impact on the snooping versus directory protocol debate,"This invited paper argues that to facilitate formal verification, multiprocessor systems should (1) decouple enforcing coherence from enforcing a memory consistency model and (2) decouple the interconnection network from the cache coherence protocol (by not relying on any specific interconnect ordering or synchronicity properties). Of the two dominant classes of cache coherence protocols - directory protocols and snooping protocols - these two desirable properties favor use of directory protocols over snooping protocols. Although the conceptual simplicity of snooping protocols is seductive, aggressive implementations of snooping protocols lack these decoupling properties, making them perhaps more difficult in practice to reason about, verify, and implement correctly. Conversely, directory protocols may seem more complicated, but they are more amenable to these decoupling properties, which simplify protocol design and verification. Finally, this paper describes the recently-proposed token coherence protocol's adherence to these properties and discusses some of its implications for future multiprocessor systems.",
Estimation of extraction fraction (EF) and glomerular filtration rate (GFR) using MRI: considerations derived from a new Gd-chelate biodistribution model Simulation,"Previous reports have described the use of magnetic resonance imaging (MRI) to estimate single-kidney extraction fraction (EF) and glomerular filtration rate (GFR), by measuring the concentration difference of intravenously injected Gd-chelate ([Gd]) in the renal artery and renal vein from measurements of blood T1. Problematic is the fact that [Gd] measurements in the renal artery are often inaccurate due to the small size, tortuousness and motion of the vessel. Consequently, the [Gd] in the inferior vena cava (IVC) below the renal vein ostia (i.e., the infrarenal IVC) has been used instead of the renal artery [Gd], based on the assumption that the [Gd] in the infrarenal IVC is the same as it is in the renal artery. However, this assumption has neither been theoretically nor experimentally investigated. Herein, we describe new difference and differential equation pharmacological models that can predict the biodistribution of Gd-chelate throughout the extracellular space. Assuming known average normal blood flows and GFR, our models predict that the infrarenal IVC [Gd] is 3.2% to 4.7% greater than the renal artery [Gd], and that the EF estimate using this IVC measurement is overestimated by 14.2%-20.0%. To support these predictions, algebraic equations are derived which show that the infrarenal IVC must develop a relatively high [Gd] in order to satisfy Gd flux constraints within the vascular system. These results suggest that the infrarenal IVC [Gd] is not a valid substitute for the renal artery [Gd].","Filtration,
Magnetic resonance imaging,
Arteries,
Motion measurement,
Size measurement,
Veins,
Predictive models,
Motion estimation,
Blood,
Differential equations"
Integrating grid with intrusion detection,"In recent years, distributed denial-of-service (DDoS) and denial-of-service (DoS) are the most dreadful network threats. Single-node IDS often suffers from losing its detection effectiveness and capability when processing enormous network traffic. To solve the drawbacks, we propose grid-based IDS, called grid intrusion detection system (GIDS), which uses grid computing resources to detect intrusion packets. For balancing detection load, score subtraction approach (SSA) and score addition approach (SAA) are deployed. Furthermore, to effectively detect intrusions, a two-phase packet detection process is proposed. The first phase detects logical and momentary attacks. Chronic attacks are detected in the second phase. Experiments are also performed and the results show that GIDS is truly an outstanding system in detecting attacks.","Intrusion detection,
Computer crime,
Phase detection,
Computer crashes,
Floods,
Grid computing,
Resource management,
Chaos,
Computer science,
Telecommunication traffic"
Exploring Web services from a business value perspective,"Emerging Web services technologies provide an open infrastructure for automated business interaction, thereby creating new opportunities for business actors to collaborate within a networked constellation of enterprises via the Internet. The basis for a viable network of Web services (the supporting information system of such a networked constellation of enterprises) is a value model that shows sound value propositions to all actors involved. Requirements engineering techniques can be developed to support: (1) exploring alternative business models, and (2) evaluating alternatives on their economic viability, leading into the design and implementation of technical systems. In this paper, we present a business-oriented approach supporting Web services idea exploration (BASSIE), which exploits the synergy between the agent- and goal-oriented i* framework and the value-based e/sup 3/value framework. The approach iterates between exploration of structural alternatives and qualitative evaluation using i*, and quantitative modeling and evaluation of business value using e/sup 3/value. The approach is illustrated with a real life case study in digital music distribution.","Web services,
Computer science,
Web and internet services,
Acoustical engineering,
Profitability,
Collaboration,
IP networks,
Information systems,
Design engineering,
Application software"
Consistency management among replicas in peer-to-peer mobile ad hoc networks,"Recent advances in wireless communication along with peer-to-peer (P2P) paradigm have led to increasing interest in P2P mobile ad hoc networks. In this paper, we assume an environment where each mobile peer accesses data items held by other peers which are connected by a mobile ad hoc network. Since peers' mobility causes frequent network partitions, replicas of a data item may be inconsistent due to write operations performed by mobile peers. In such an environment, the global consistency of data items is not desirable by many applications. Thus, new consistency maintenance based on local conditions such as location and time need to be investigated. This paper attempts to classify different consistency levels according to requirements from applications and provides protocols to realize them. We report simulation results to investigate the characteristics of these consistency protocols in a P2P wireless ad hoc network environment and their relationship with the quorum sizes.",
Efficient nearest neighbor classification using a cascade of approximate similarity measures,"This paper proposes a method for efficient nearest neighbor classification in non-Euclidean spaces with computationally expensive similarity/distance measures. Efficient approximations of such measures are obtained using the BoostMap algorithm, which produces embeddings into a real vector space. A modification to the BoostMap algorithm is proposed, which uses an optimization cost that is more appropriate when our goal is classification accuracy as opposed to nearest neighbor retrieval accuracy. Using the modified algorithm, multiple approximate nearest neighbor classifiers are obtained, that provide a wide range of trade-offs between accuracy and efficiency. The approximations are automatically combined to form a cascade classifier, which applies the slower and more accurate approximations only to the hardest cases. The proposed method is experimentally evaluated in the domain of handwritten digit recognition using shape context matching. Results on the MNIST database indicate that a speed-up of two to three orders of magnitude is gained over brute force search, with minimal losses in classification accuracy.","Nearest neighbor searches,
Shape measurement,
Image databases,
Testing,
Extraterrestrial measurements,
Cost function,
Handwriting recognition,
Error analysis,
Computer science,
Optical computing"
Service-oriented software engineering (SOSE) framework,"The gap between business decision making and software engineering causes inefficiency and quality problems in software development. Software engineers do not understand organization's value creation objectives and their influence on software production and structure. For this reason software does not fulfill the requirements of business and software quality is inadequate too often. The objective of the authors in the service-oriented software engineering project (SOSE) is to develop methods and tools to improve quality and profitability of software development. In this paper the authors described SOSE framework and clarify with examples its phases, utility, and application in pilot projects. SOSE framework's first activity is to create a well-defined business case. Then, business processes and data concepts are identified, to meet business requirements of the business case, and modelled with informal diagrams like UML and BPML. Finally, the refinement continues with use case maps, system-level services, and business service components. It is proposed that service, process, entity, and utility components are used as design elements of the business service component. In implementation platform independent and platform specific models were utilized. This study has been carried out in cooperation with ICT companies and their customers in electricity domain in Finland.",
An efficient algorithm for incremental mining of association rules,"Incremental algorithms can manipulate the results of earlier mining to derive the final mining output in various businesses. This study proposes a new algorithm, called the New Fast UPdate algorithm (NFUP) for efficiently incrementally mining association rules from a large transaction database. NFUP is a backward method that only requires scanning incremental database. Rather than rescanning the original database for some new generated frequent itemsets in the incremental database, we accumulate the occurrence counts of newly generated frequent itemsets and delete infrequent itemsets obviously. Thus, NFUP need not rescan the original database and to discover newly generated frequent itemsets. NFUP has good scalability in our simulation.",
Introducing Neuromodulation to a Braitenberg Vehicle,"Artificial neural networks are often used as the control systems for mobile robots. However, although these models usually claim inspiration from biology, they often lack an analogue of the biological phenomenon called neuromodulation. In this paper, we describe our initial work exploring a simple model of neuromodulation, used to provide a mobile robot with foraging behaviour.",
An improved categorization of classifier's sensitivity on sample selection bias,"A recent paper categorizes classifier learning algorithms according to their sensitivity to a common type of sample selection bias where the chance of an example being selected into the training sample depends on its feature vector x but not (directly) on its class label y. A classifier learner is categorized as ""local"" if it is insensitive to this type of sample selection bias, otherwise, it is considered ""global"". In that paper, the true model is not clearly distinguished from the model that the algorithm outputs. In their discussion of Bayesian classifiers, logistic regression and hard-margin SVMs, the true model (or the model that generates the true class label for every example) is implicitly assumed to be contained in the model space of the learner, and the true class probabilities and model estimated class probabilities are assumed to asymptotically converge as the training data set size increases. However, in the discussion of naive Bayes, decision trees and soft-margin SVMs, the model space is assumed not to contain the true model, and these three algorithms are instead argued to be ""global learners"". We argue that most classifier learners may or may not be affected by sample selection bias; this depends on the dataset as well as the heuristics or inductive bias implied by the learning algorithm and their appropriateness to the particular dataset.","Bayesian methods,
Logistics,
Decision trees,
Computer science,
Training data,
Data mining,
Testing,
Classification tree analysis,
Regression tree analysis,
Support vector machines"
Fault tolerant connected sensor cover with variable sensing and transmission ranges,,"Fault tolerance,
Sensor phenomena and characterization,
Signal processing,
Hardware,
Network topology,
Costs,
Signal to noise ratio,
Radiofrequency identification,
Radio frequency,
Computer science"
Automatic generation of buffer overflow attack signatures: an approach based on program behavior models,"Buffer overflows have become the most common target for network-based attacks. They are also the primary mechanism used by worms and other forms of automated attacks. Although many techniques have been developed to prevent server compromises due to buffer overflows, these defenses still lead to server crashes. When attacks occur repeatedly, as is common with automated attacks, these protection mechanisms lead to repeated restarts of the victim application, rendering its service unavailable. To overcome this problem, we develop a new approach that can learn the characteristics of a particular attack, and filter out future instances of the same attack or its variants. By doing so, our approach significantly increases the availability of servers subjected to repeated attacks. The approach is fully automatic, does not require source code, and has low runtime overheads. In our experiments, it was effective against most attacks, and did not produce any false positives","Buffer overflow,
Network servers,
Filters,
Computer worms,
Protection,
Computer science,
Ash,
Runtime,
Radio access networks,
Automatic speech recognition"
Understanding concerns in software: insights gained from two case studies,"Much of the complexity of software arises from the interactions between disparate concerns. Even in well-designed software, some concerns can not always be encapsulated in a module. Research on separation of concerns seeks to address this problem, but we lack an understanding of how programmers conceptualize the notion of a concern and then identify that concern in code. In this work, we have conducted two exploratory case studies to better understand these issues. The case studies involved programmers identifying concerns and associated code in existing, unfamiliar software: GNU's sort.c and the game Minesweeper. Based on our experiences with these two case studies, we have identified several types of concerns and have detailed a number of factors that impact programmer identification of concerns. Based on these insights, we have created two sets of guidelines: one to help programmers identify relevant concerns and another to help programmers identify code relating to concerns.","Computer aided software engineering,
Programming profession,
Software systems,
Computer science,
Educational institutions,
Guidelines,
Concrete,
Functional programming,
Terminology"
Nonlinear feature extraction of hyperspectral data based on locally linear embedding (LLE),,"Feature extraction,
Hyperspectral imaging,
Hyperspectral sensors,
Remote sensing,
Data mining,
Principal component analysis,
Scattering,
Covariance matrix,
Computer science,
Forestry"
X-ray: a tool for automatic measurement of hardware parameters,"There is growing interest in self-optimizing computing systems that can optimize their own behavior on different platforms without manual intervention. Examples of successful self-optimizing systems are ATLAS, which generates basic linear algebra subroutine (BLAS) libraries, and FFTW, which generates FFT libraries. Self-optimizing systems need values for hardware parameters such as the number of registers of various types and the capacities of caches at various levels. For example, ATLAS uses the capacity of the LI cache and the number of registers in determining the size of cache tiles and register tiles. In this paper, we describe X-ray, a system for implementing micro-benchmarks to measure such hardware parameters. We also present novel algorithms for measuring some of these parameters. Experimental evaluations of X-ray on traditional workstations, servers and embedded systems show that X-ray produces more accurate and complete results than existing tools.","Hardware,
Registers,
Pollution measurement,
Libraries,
Tiles,
Optimizing compilers,
Performance evaluation,
Computer science,
Linear algebra,
Algorithms"
An organizational grid of federated MOSIX clusters,"MOSIX is a cluster management system that uses process migration to allow a Linux cluster to perform like a parallel computer. Recently it has been extended with new features that could make a grid of Linux clusters run as a cooperative system of federated clusters. On one hand, it supports automatic workload distribution among connected clusters that belong to different owners, while still preserving the autonomy of each owner to disconnect its cluster from the grid at any time, without sacrificing migrated processes from other clusters. Other new features of MOSIX include grid-wide automatic resource discovery; a precedence scheme for local processes and among guest processes (from other clusters); flood control; a secure run-time environment (sandbox) which prevents guest processes from accessing local resources in a hosting system, and support of cluster partitions. The resulting grid management system is suitable to create an intra-organizational high-performance computational grid, e.g., in an enterprise or in a campus. The paper presents enhanced and new features of MOSIX and their performance.",
Storing XML (with XSD) in SQL databases: interplay of logical and physical designs,"Much of business XML data has accompanying XSD specifications. In many scenarios ""shredding"" such XML data into a relational storage is a popular paradigm. Optimizing evaluation of XPath queries overmuch XML data requires paying careful attention to both the logical and physical designs of the relational database where XML data is shredded. None of the existing solutions has taken into account physical design of the generated relational database. In this paper, we study the interplay of logical and physical design and conclude that 1) solving them independently leads to suboptimal performance and 2) there is substantial overlap between logical and physical designs: some well-known logical design transformations generate the same mappings as physical design. Furthermore, existing search algorithms are inefficient to search the extremely large space of logical and physical design combinations. We propose a search algorithm that carefully avoids searching duplicated mappings and utilizes the workload information to further prune the search space. Experimental results confirm the effectiveness of our approach.","XML,
Relational databases,
Design optimization,
Algorithm design and analysis,
Indexes,
Conference proceedings,
Information systems,
Computer science,
Informatics"
Phase-aware remote profiling,"Recent advances in networking and embedded device technology have made the vision of ubiquitous computing a reality; users can access the Internet's vast offerings anytime and anywhere. Moreover, battery-powered devices such as personal digital assistants and Web-enabled mobile phones have successfully emerged as new access points to the world's digital, infrastructure. This ubiquity offers a new opportunity for software developers: users can now participate in the software development, optimization, and evolution process while they use their software. Such participation requires effective techniques for gathering profile information from remote, resource-constrained devices. Further, these techniques must be unobtrusive and transparent to the user; profiles must be gathered using minimal computation, communication, and power. Toward this end, we present a flexible hardware-software scheme for efficient remote profiling. We rely on the extraction of meta information from executing programs in the form of phases, and then use this information to guide intelligent online sampling and to manage the communication of those samples. Our results indicate that phase-based remote profiling can reduce the communication, computation, and energy consumption overheads by 50-75% over random and periodic sampling.","Personal digital assistants,
Programming,
Sampling methods,
Energy consumption,
Optimizing compilers,
Information analysis,
Computer science,
Ubiquitous computing,
IP networks,
Mobile handsets"
SafeSMS - end-to-end encryption for SMS,,"Cryptography,
Mobile handsets,
Java,
Application software,
Authentication,
Security,
Books,
Runtime environment,
Computer science,
Message service"
The effect of animated transitions on user navigation in 3D tree-maps,"This paper describes a user study conducted to evaluate the use of smooth animated transitions between directories in a three-dimensional, tree-map visualization. We looked specifically at the task of returning to a previously visited directory after either an animated or instantaneous return to the root location. The results of the study show that animation is a double-edged sword. Even though users take more shortcuts, they also make more severe navigational errors. It seems as though the promise of a more direct route to the target directory, which animation provides, somehow precludes users who navigate incorrectly from applying a successful recovery strategy.","Animation,
Navigation,
Visualization,
Computer science,
Educational technology,
Computer science education,
History,
User interfaces,
Fading,
DC generators"
A process model and typology for software product updaters,"Product software is constantly evolving through extensions, maintenance, changing requirements, changes in configuration settings, and changing licensing information. Managing evolution of released and deployed product software is a complex and often underestimated problem that has been the cause of many difficulties for both software vendors and customers. This paper presents a process model and typology to characterize techniques that support product software update methods. Also, this paper assesses and surveys a variety of existing techniques against the characterization framework and lists unsolved problems related to software product updaters.","Software maintenance,
Licenses,
Application software,
Software tools,
Mathematics,
Computer science,
Information management,
Software systems,
Enterprise resource planning,
Hardware"
Sustaining interaction dynamics and engagement in dyadic child-robot interaction kinesics: lessons learnt from an exploratory study,"Motivated by questions of interaction design for human-robot interaction (HRI), an exploratory initial study was carried out with children and a robotic ""pet"" in order to improve understanding the design space for interaction with an autonomous robot. Interactions were very unstructured in a relaxing and familiar environment. The scope of the study was quite broad in order to cover a wide range of possibly relevant types of interactions. The study of the resulting interaction dynamics - with rich and with poor contextual cues - identifies key factors for interaction design and suggests some guidelines for initiating, maintaining, and regulating on-going interaction. In particular, non-directed and directional feedback, turn-taking rhythms, and the interactional kinesics of human-robot dyads are discussed dimensions for HRI design. This is hoped to enable future studies to specifically address in more depth the issues raised in this paper.","Cognitive robotics,
Orbital robotics,
Human robot interaction,
Human computer interaction,
Cognition,
Humanoid robots,
Adaptive systems,
Computer science,
Educational institutions,
Positron emission tomography"
Efficient Mining of Contrast Patterns and Their Applications to Classification,"Data mining is one of the most important areas in the 21st century with many wide ranging applications. These include medicine, finance, commerce and engineering. Pattern mining is amongst the most important and challenging techniques employed in data mining. Patterns are collections of items which satisfy certain properties. Emerging patterns are those whose frequencies change significantly from one dataset to another. They represent strong contrast knowledge and have been shown to be very successful for constructing accurate and robust classifiers. In this paper, we examine various kinds of contrast patterns. We also investigate efficient pattern mining techniques and discuss how to exploit patterns to construct effective classifiers","Data mining,
Gene expression,
Finance,
Business,
Frequency,
Supervised learning,
Itemsets,
Robustness,
Statistics,
Machine learning"
Violin: a framework for extensible block-level storage,"In this work we propose Violin, a virtualization framework that allows easy extensions of block-level storage stacks. Violin allows (i) developers to provide new virtualization functions and (ii) storage administrators to combine these functions in storage hierarchies with rich semantics. Violin makes it easy to develop such new functions by providing support for (i) hierarchy awareness and arbitrary mapping of blocks between virtual devices, (ii) explicit control over both the request and completion path of I/O requests, and (iii) persistent metadata management. To demonstrate the effectiveness of our approach we evaluate Violin in three ways: (i) we loosely compare the complexity of providing new virtual modules in Violin with the traditional approach of writing monolithic drivers. In many cases, adding new modules is a matter of recompiling existing user-level code that provides the required functionality. (ii) We show how simple modules in Violin can be combined in more complex hierarchies. We demonstrate hierarchies with advanced virtualization semantics that are difficult to implement with monolithic drivers. (iii) We use various benchmarks to examine the overheads introduced by Violin in the common I/O path. We find that Violin modules perform within 10% of the corresponding monolithic Linux drivers.","Computer science,
Application software,
Technology management,
Writing,
Linux,
Batteries,
Application virtualization,
Quality management,
Cryptography,
Control systems"
Segmentation induced by scale invariance,"Perceptual organization is scale-invariant. In turn, a segmentation that separates features consistently at all scales is the desired one that reveals the underlying structural organization of an image. Addressing cross-scale correspondence with interior pixels, we develop this intuition into a general segmenter that handles texture and illusory contours through edges entirely without any explicit characterization of texture or curvilinearity. Experimental results demonstrate that our method not only performs on par with either texture segmentation or boundary completion methods on their specialized examples, but also works well on a variety of real images.","Image segmentation,
Detectors,
Mathematical model,
Computer science,
Pixel,
Filters,
MATLAB,
Hysteresis,
Computer Society,
Computer vision"
An Empirical Study of Software Process in Practice,"In adopting a software process model, many small software companies are ignoring standard process models and models for process improvement. This study uses an empirical approach to investigate what processes software companies are using on a day-to-day basis and examines why these companies are rejecting ""best practice"" approaches.","Capability maturity model,
Software standards,
Software engineering,
ISO standards,
Programming,
Mathematics,
Electronic mail,
Appraisal,
Computer industry,
Software development management"
"Comparing algorithms, representations and operators for the multi-objective knapsack problem","This paper compares the performance of three evolutionary multi-objective algorithms on the multi-objective knapsack problem. The three algorithms are SPEA2 (strength Pareto evolutionary algorithm, version 2), MOGLS (multi-objective genetic local search) and SEAMO2 (simple evolutionary algorithm for multi-objective optimization, version 2). For each algorithm, we try two representations: bit-string and order-based. Our results suggest that a bit-string representation works best for MOGLS, but that SPEA2 and SEAMO2 perform better with an order-based approach. Although MOGLS outperforms the other algorithms in terms of solution quality, SEAMO2 runs much faster than its competitors and produces results of a similar standard to SPEA2.","Evolutionary computation,
Decoding,
Computer science,
Genetics,
Pareto optimization,
Testing,
Biological cells,
Standards development"
Visualization of graphs with associated timeseries data,"The most common approach to support analysis of graphs with associated time series data include: overlay of data on graph vertices for one timepoint at a time by manipulating a visual property (e.g. color) of the vertex, along with sliders or some such mechanism to animate the graph for other timepoints. Alternatively, data from all the timepoints can be overlaid simultaneously by embedding small charts into graph vertices. These graph visualizations may also be linked to other visualizations (e.g., parallel co-ordinates) using brushing and linking. This paper describes a study performed to evaluate and rank graph+timeseries visualization options based on users' performance time and accuracy of responses on predefined tasks. The results suggest that overlaying data on graph vertices one timepoint at a time may lead to more accurate performance for tasks involving analysis of a graph at a single timepoint, and comparisons between graph vertices for two distinct timepoints. Overlaying data simultaneously for all the timepoints on graph vertices may lead to more accurate and faster performance for tasks involving searching for outlier vertices displaying different behavior than the rest of the graph vertices for all timepoints. Single views have advantage over multiple views on tasks that require topological information. Also, the number of attributes displayed on nodes has a non trivial influence on accuracy of responses, whereas the number of visualizations affect the performance time.","Data visualization,
Multidimensional systems,
Data analysis,
Bioinformatics,
Joining processes,
Computer science,
Time series analysis,
Mechanical factors,
Color,
Animation"
A Role-Based Framework for Business Process Modeling,"Business objects are object-oriented representations of the concepts of interest in an organization, such as activities, resources and actors. Business objects collaborate with one another in order to achieve business goals, showing different behavior and properties according to each specific collaboration context. This means the same business object may be perceived differently depending on the business objects it is collaborating with. However, most approaches to business process modeling do not separate the collaborative aspects of a business object from its internal aspects. To cope with such issues, this paper makes use of role modeling to separate these concerns while increasing the understandability and reusability of business process models. This approach makes use of object-oriented concepts to separate a business process model into a business object model and a role model. The business object models deals with specifying the structure and intrinsic behavior of business objects, while the role model specifies its collaborative aspects.","Object oriented modeling,
Business process re-engineering,
Business communication,
Collaboration,
Humans,
Manufacturing processes,
Logistics,
Software engineering,
Information systems,
Computer science"
MAC sleep mode control considering downlink traffic pattern and mobility,"Energy of a mobile terminal (MT) is an important resource, so recent wireless network protocols like IEEE 802.11 and IEEE 802.16 include energy saving techniques. These protocols, however, have a problem in minimizing energy loss under varying traffic pattern and mobile environment. In this paper, we propose a sleep mode interval control algorithm that considers downlink traffic pattern and terminal mobility to maximize energy-efficiency. Through simulations, we show that our proposed algorithm improves energy saving and delay performance. There is no energy loss because it does not incur unnecessary cell searching in multi-cell environments.","Downlink,
Communication system traffic control,
Energy loss,
Traffic control,
Energy efficiency,
Batteries,
Transmitters,
Media Access Protocol,
Computer science,
Electronic mail"
Interpolation for wireless sensor network coverage,"One of the primary issues for any wireless sensor network (WSN) deployment is that of longevity of the network. The lifespan of the network must be maximized while maintaining the quality of the sensory data received from the network. Most current solutions for achieving this require the definition of a sensing radius for each sensor together with a coverage level k. The sensed area is covered if every point within the area is inside the sensing radius of at least k sensors. When some points are covered by more than k sensors, it may be possible to instruct the surplus sensors to enter a low power sleep mode. This will conserve the energy of the network while maintaining the required coverage level. In. this paper we propose a novel alternative to this approach based on interpolation. If the sensor network is capable of interpolating the sensed medium at a given sensor's location to a specified accuracy or higher, then we propose that this sensor is redundant and can be put into sleep mode. We demonstrate our approach using live sensory data.","Interpolation,
Wireless sensor networks,
Batteries,
Energy management,
Degradation,
Computer science,
Educational institutions,
Electronic mail,
Large-scale systems,
Fault tolerance"
Scheduling in switches with small internal buffers,"Unbuffered crossbars or switching fabrics contain no internal buffers, and function using only input (VOQ) and possibly output queues. Schedulers for such switches are complex, and introduce increased delay at medium loads, because they have to admit at most one cell per input and per output, during each time slot. Buffered crossbars, on the other hand, contain Q sufficient internal buffering (N2 buffers) to allow independent schedulers to concurrently forward packets to the same output from any number of inputs. These architectures represent the two extremes in a range of solutions, which we examine here; although intermediate points in this range are of reduced practical interest for crossbars, they are nevertheless quite interesting for switching fabrics, and they may be of interest for optical switches. We find that tolerating two cells per-output per time-slot, using small buffers inside the switch or fabric, suffices for independent and efficient scheduling. First, we introduce a novel ""request-grant"" credit protocol, enabling N inputs to share a small switch buffer. Then, we apply this protocol to a switch with N such buffers, one per output, and we consider the resulting scheduling problem. Interestingly, this looks like unbuffered crossbar schedulers, but it is much simpler because it comprises independent schedulers that can be pipelined. We show that individual buffer sizes do not need to grow, neither with switch size nor with propagation delay. Through simulations, we study performance as a function of the number of cells allowed per-output per-time-slot. For one cell, the switch performs very close to the iSLIP unbuffered crossbar with one iteration. For more cells, performance improves quickly; for 12 cells, packet delay under (smooth) uniform load is practically as low as ideal output queueing. Under unbalanced load, throughput is superior to buffered crossbars, due to better buffer sharing",
Safety-liveness semantics for UML 2.0 sequence diagrams,"We provide an automata-theoretic solution to one of the main open questions about the UML standard, namely how to assign a formal semantics to a set of sequence diagrams without compromising refinement? Our solution relies on a rather obvious idea, but to our knowledge has not been used before in this context: that bad and good sequence diagrams in the UML standard should be regarded as safety and liveness properties, respectively. Proceeding in this manner, we obtain a semantics that essentially complements the set of behaviors associated with the set of sequence diagrams, thereby allowing us to use the standard notion of refinement as language inclusion. We show that refinement in this setting is compositional with respect to sequential composition, alternative composition, parallel composition, and star+ composition.","Unified modeling language,
Automata,
Safety,
Computer science,
Distributed computing,
Telecommunication computing,
Communication industry,
Computer industry,
Software standards,
Telecommunication standards"
Compact explicit multi-path routing for LEO satellite networks,"The dynamic topology of low Earth orbit (LEO) satellite networks and variable traffic load in different satellite coverage areas pose special challenges to routing algorithm design in the phase of satellite network dimensioning. In this paper, a compact explicit multi-path routing (CEMR) algorithm is proposed. The new algorithm can support traffic load balancing in satellite networks with lower signal overhead compared to traditional multi-path routing algorithm or MPLS through a compact PathID encoding scheme. A PathID validating algorithm is also given to guarantee loop free packet forwarding during transition of time interval. Results from simulations show that CEMR has better performance than other routing schemes in term of end-to-end delay and packet loss probability, and is especially suited to data transferring in case of high traffic load in LEO satellite networks.","Routing,
Low earth orbit satellites,
Telecommunication traffic,
Network topology,
Multiprotocol label switching,
Explosives,
Satellite constellations,
Switches,
Military satellites,
Computer science"
A universal service-semantics description language,"For Web-services to become practical, an infrastructure needs to be supported that allows users and applications to discover, deploy, compose, and synthesize services automatically. This automation can take place only if a formal description of the Web-services is available. In this paper we present an infrastructure using USDL (universal service-semantics description language), a language for formally describing the semantics of Web-services. USDL is based on the Web Ontology Language (OWL) and employs WordNet as a common basis for understanding the meaning of services. USDL can be regarded as formal service documentation that will allow sophisticated conceptual modeling and searching of available Web-services, automated service composition, and other forms of automated service integration. A theory of safe service substitution for USDL is presented and proved sound and complete. The rationale behind the design of USDL along with its formal specification in OWL is presented with examples. We also compare USDL with other approaches like OWL-S and WSML and show that USDL is complementary to these approaches.","Web services,
OWL,
Computer science,
Automation,
Formal specifications,
Internet,
Humans,
Tail,
Semantic Web,
XML"
Robust face detection with multi-class boosting,"With the aim to design a general learning framework for detecting faces of various poses or under different lighting conditions, we are motivated to formulate the task as a classification problem over data of multiple classes. Specifically, our approach focuses on a new multi-class boosting algorithm, called MBHboost, and its integration with a cascade structure for effectively performing face detection. There are three main advantages of using MBHboost: 1) each MBH weak learner is derived by sharing a good projection direction such that each class of data has its own decision boundary; 2) the proposed boosting algorithm is established based on an optimal criterion for multi-class classification; and 3) since MBHboost is flexible with respect to the number of classes, it turns out that it is possible to use only one single boosted cascade for the multi-class detection. All these properties give rise to a robust system to detect faces efficiently and accurately.","Robustness,
Face detection,
Boosting,
Detectors,
Neural networks,
Information science,
Multi-layer neural network,
Multilayer perceptrons,
Testing,
Computer vision"
Face recognition by stepwise nonparametric margin maximum criterion,"Linear discriminant analysis (LDA) is a popular feature extraction technique in face recognition. However, it often suffers from the small sample size problem when dealing with the high dimensional data. Moreover, while LDA is guaranteed to find the best directions when each class has a Gaussian density with a common covariance matrix, it can fail if the class densities are more general. In this paper; a new nonparametric linear feature extraction method, stepwise nonparametric margin maximum criterion (SNMMC), is proposed to find the most discriminant directions, which does not assume that the class densities belong to any particular parametric family and does not depend on the non- singularity of the within-class scatter matrix neither. On three datasets from ATT and FERET face databases, our experimental results demonstrate that SNMMC outperforms other methods and is robust to variations of pose, illumination and expression",
Randomized pseudo-random function tree walking algorithm for secure radio-frequency identification,"Privacy and security are two main concerns in radio frequency identification (RFID) systems. We first extend the analysis of the randomized tree walking algorithm for RFID tag collision avoidance, which is secure against passive adversaries. Then, we devise a new randomized pseudo-random function (PRF) tree walking algorithm, which is secure against active eavesdroppers and allows for the efficient interrogation of many tags. Our algorithm accommodates the addition and removal of tags from the system, and dynamically adapts to security and privacy policy changes.","Legged locomotion,
Radio frequency,
Radiofrequency identification,
Privacy,
Algorithm design and analysis,
Authentication,
Security,
Computer science,
RFID tags,
Communication channels"
Detection of creak clicks of sperm whales in low SNR conditions,A function of a nonlinear operator referred to as Teager-Kaiser Energy operator is described to detect creak clicks of sperm whales in adverse conditions. Clicks are considered as a sum of transient signals mixed with interference signals and background Gaussian noise of unknown variance. The method has been applied to synthetic as well as to real recordings of creak clicks. On synthetic data the detection rate gives excellent results. For low SNR a detection rate close to 100% is achieved while it works very well in detecting clicks in real creak sounds.,"Whales,
Signal to noise ratio,
Interference,
Acoustic noise,
Estimation error,
Speech synthesis,
Frequency estimation,
Computer science,
Gaussian noise,
Acoustic signal detection"
Aspects of Traditional versus Virtual Laboratory for Education in Instrumentation and Measurement,"Nowadays the multimedia tools have an important role in both the management of the lectures and the organization of the course program on instrumentation and measurement. In this new scenario, the virtual laboratory (VL) represents the environment in which the learning activities are performed. Starting from this observation, in the paper an overview of the state of the art of the VL-based education on instrumentation and measurement is given. The fundamental aspects concerning both the software and the hardware to design the VL are examined. Attention it is also devoted to innovative requirements and interesting open questions arising from the large diffusion of the VLs","Laboratories,
Instrumentation and measurement"
An efficient numerical spectral method for solving the Schrodinger equation,"Spectral expansions of a smooth function converge rapidly and are suited for accurate solutions of differential or integral equations. Based on such expansions, the authors have developed a numerical algorithm for solving the Schrodinger equation.","Integral equations,
Schrodinger equation,
Chebyshev approximation,
Polynomials,
Differential equations,
Difference equations,
Wave functions,
Convergence,
Finite difference methods"
Characterizing and Predicting TCP Throughput on the Wide Area Network,"DualPats exploits the strong correlation between TCP throughput and flow size, and the statistical stability of Internet path characteristics to accurately predict the TCP throughput of large transfers using active probing. We propose additional mechanisms to explain the correlation, and then analyze why traditional TCP benchmarking fails to predict the throughput of large transfers well. We characterize stability and develop a dynamic sampling rate adjustment algorithm so that we probe a path based on its stability. Our analysis, design, and evaluation is based on a large-scale measurement study",
Efficient monitoring of end-to-end network properties,"It is often desirable to monitor end-to-end properties, such as loss rates or packet delays, across an entire network. However, active end-to-end measurement in such settings does not scale well, and so complete network-wide measurement quickly becomes infeasible. More efficient measurement strategies are therefore needed. Previous work, examining this problem from a linear algebraic perspective, has shown that for exact recovery of complete end-to-end network properties, the number of paths that need to be monitored can be reduced to approximately the number of links in the network. In this paper we ask whether measurement strategies of even greater efficiency are possible. We recast the problem as one of statistical prediction and show that end-to-end network properties may be accurately predicted in many cases using a significantly smaller set of carefully chosen paths than needed for exact recovery. We formulate a general framework for the prediction problem, propose a simple class of predictors for standard quantities of interest (e.g., averages, totals, differences), and show that linear algebraic methods of subset selection may be used to make effective choice of which paths to measure. We explore the accuracy of the resulting methods both analytically and numerically, in the context of real network topologies of varying size. The feasibility of our methods derives from the low effective rank of routing matrices as encountered in practice, which appears to be a new observation of interest in its own right. The resulting framework, which is quite general, appears to hold promise for studying and improving the efficiency of monitoring of end-to-end-network properties.","Routing,
Computerized monitoring,
Mathematics,
Statistics,
Delay,
Condition monitoring,
Computer science,
Measurement standards,
Network topology,
Aggregates"
Comparative evaluation of Web image search engines for multimedia applications,"While text-oriented document searching are relatively mature on the Internet, image searching, which requires much more than text matching, significantly lags behind. The use of image search engines significantly enlarges the scope of images to users accessibility. This paper provides an understanding of current technologies in image searching on the Internet, and points to future areas of improvement for multimedia applications. We develop a systematic set of image queries to assess the competence and performance of the major image search engines. We find that current technology is only able to deliver an average precision of around 42% and an average recall of around 12%, while the best performers are capable of producing over 70% for precision and around 27% for recall. The reasons for such differences, and mechanisms for search improvement, are also indicated.",
Empirical mode decomposition for dimensionality reduction of hyperspectral data,,
A provenance-aware weighted fault tolerance scheme for service-based applications,"Service-orientation has been proposed as a way of facilitating the development and integration of increasingly complex and heterogeneous system components. However, there are many new challenges to the dependability community in this new paradigm, such as how individual channels within fault-tolerant systems may invoke common services as part of their workflow, thus increasing the potential for common-mode failure. We propose a scheme that - for the first time - links the technique of provenance with that of multi-version fault tolerance. We implement a large test system and perform experiments with a single-version system, a traditional MVD system, and a provenance-aware MVD system, and compare their results. We show that for this experiment, our provenance-aware scheme results in a much more dependable system than either of the other systems tested, whilst imposing a negligible timing overhead.","Fault tolerance,
Service oriented architecture,
Fault tolerant systems,
Application software,
System testing,
Costs,
Quality of service,
Computer science,
Performance evaluation,
Timing"
Utilization based duty cycle tuning MAC protocol for wireless sensor networks,"In this paper, we propose U-MAC, a medium access control protocol designed for wireless sensor networks. Nowadays, wireless sensor network are formed by a great quantity of sensor nodes, which are generally battery-powered and may not recharge easily. Consequently, how to prolong the lifetime of the nodes is an important issue while designing a MAC protocol. However, lowering the energy consumption may result in higher latency. Addressing on such tradeoff, U-MAC balances the tradeoff by utilization based tuning of duty cycle and selective sleeping after transmission. The experiment results show that our proposed U-MAC saves energy about 43% and reduce latency by 65% from S-MAC in a chain topology. In the cross topology, U-MAC also achieves 32% energy saving and 45% latency reduction from S-MAC","Media Access Protocol,
Wireless application protocol,
Wireless sensor networks,
Delay,
Sleep,
Sensor phenomena and characterization,
Energy consumption,
Topology,
Computer science,
Access protocols"
Restrictions on the three-class ideal observer's decision boundary lines,"We are attempting to develop expressions for the coordinates of points on the three-class ideal observer's receiver operating characteristic (ROC) hypersurface as functions of the set of decision criteria used by the ideal observer. This is considerably more difficult than in the two-class classification task, because the conditional probabilities in question are not simply related to the cumulative distribution functions of the decision variables, and because the slopes and intercepts of the decision boundary lines are not independent; given the locations of two of the lines, the location of the third will be constrained depending on the other two. In this paper, we attempt to characterize those constraining relationships among the three-class ideal observer's decision boundary lines. As a result, we show that the relationship between the decision criteria and the misclassification probabilities is not one-to-one, as it is for the two-class ideal observer.",
Enhancing Web services availability,Little work has been reported on highly available Web services which are essential for mission critical applications. In this paper we propose architecture for highly available Web services for mission critical applications. The central idea is the enhancement of Web services by the introduction of a central hub to increase the availability of Web services,
Anonymity vs. Information Leakage in Anonymity Systems,"Measures for anonymity in systems must be on one hand simple and concise, and on the other hand reflect the realities of real systems. Such systems are heterogeneous, as are the ways they are used, the deployed anonymity measures, and finally the possible attack methods. Implementation quality and topologies of the anonymity measures must be considered as well. We therefore propose a new measure for the anonymity degree, which takes into account possible heterogeneity. We model the effectiveness of single mixes or of mix networks in terms of information leakage and measure it in terms of covert channel capacity. The relationship between the anonymity degree and information leakage is described, and an example is shown","Channel capacity,
Telecommunication network topology,
Computer science,
Network topology,
Information retrieval,
Routing,
Information technology,
Entropy"
Non-minimal routing strategy for application-specific networks-on-chips,"We propose a deterministic routing strategy called flee which introduces non-minimal paths in order to distribute traffic with a high degree of communication locality in networks-on-chips. In the recent design methodology, target system and its application of the systems-on-a-chip are designed in system level description language like system-C, and simulated in the early stage of design. The task distribution is statically decided in this stage, and the amount of traffic between nodes can be analyzed. According to the analysis, a path that transfers a large amount of total data is firstly assigned with a relaxed limitation, thus it is mostly minimal. On the other hand, paths for small amount of total data, are secondly established so as not to disturb previously established paths, thus they are sometimes non-minimal. Simulation results show that the flee routing strategy improves up to 28.6% of throughput against the dimension-order routing on typical stream processing application programs.","Routing,
Network-on-a-chip,
Traffic control,
Bandwidth,
Telecommunication traffic,
Design methodology,
System-on-a-chip,
Concurrent computing,
Buffer storage,
Computer science"
Parallel Parameter Tuning for Applications with Performance Variability,"In this paper, we present parallel on-line optimization algorithms for parameter tuning of parallel programs. We employ direct search algorithms that update parameters based on real-time performance measurements. We discuss the impact of performance variability on the accuracy and efficiency of the optimization algorithms and proposed modified versions of the direct search algorithms to cope with it. The modified version uses multiple samples instead of single sample to estimate the performance more accurately. We present preliminary results that the performance variability of applications on clusters is heavy tailed. Finally, we studay and demonstrate the performance of the proposed algorithms for real scientific application.",
Power breakdown analysis for a heterogeneous NoC platform running a video application,"Users expect future handheld devices to provide extended multimedia functionality and have long battery life. This type of application imposes heavy constraints on performance and power consumption and forces designers to optimize all parts of their platform. Evaluating the overall platform power breakdown is therefore critical to determine where to spend the efforts on power optimization. Surprisingly, few studies exist on that topic and decisions generally rely on common belief. We have realized a complete power breakdown for a realistic platform to identify the major power bottlenecks. This paper presents this power assessment of a realistic heterogeneous network on chip platform including processors, network and data/instruction memory hierarchy, running a video processing chain from camera to display. Our power breakdown identifies the main bottlenecks in the memory hierarchy and the foreground memory, and shows that global interconnect is not that critical for a well-optimized application mapping.",
Noise characterization of block-iterative reconstruction algorithms: II. Monte Carlo simulations,"In Soares et al. (2000), the ensemble statistical properties of the rescaled block-iterative expectation-maximization (RBI-EM) reconstruction algorithm and rescaled block-iterative simultaneous multiplicative algebraic reconstruction technique (RBI-SMART) were derived. Included in this analysis were the special cases of RBI-EM, maximum-likelihood EM (ML-EM) and ordered-subset EM (OS-EM), and the special case of RBI-SMART, SMART. Explicit expressions were found for the ensemble mean, covariance matrix, and probability density function of RBI reconstructed images, as a function of iteration number. The theoretical formulations relied on one approximation, namely that the noise in the reconstructed image was small compared to the mean image. We evaluate the predictions of the theory by using Monte Carlo methods to calculate the sample statistical properties of each algorithm and then compare the results with the theoretical formulations. In addition, the validity of the approximation will be justified.","Reconstruction algorithms,
Image reconstruction,
Covariance matrix,
Probability density function,
Cancer,
Heart,
Lungs,
Blood,
Mathematics,
Computer science"
Undergraduate student perceptions of pair programming and agile software methodologies: verifying a model of social interaction,"One of the reasons that undergraduate students, particularly women and minorities, can become disenchanted with computer science education is because software development is wrongly characterized as a solitary activity. We conducted a collective case study in a software engineering course at North Carolina State University to ascertain the effects of a collaborative pedagogy intervention on student perceptions. The pedagogy intervention was based upon the practices of agile software development with a focus on pair programming. Six representative students in the course participated in the study. Their perspectives helped validate a social interaction model of student views. The findings suggest that pair programming and agile software methodologies contribute to more effective learning opportunities for computer science students and that students understand and appreciate these benefits.","Software engineering,
Collaborative work,
Programming profession,
Computer science,
Collaborative software,
Computer science education,
Iterative methods,
Mathematical model,
Mathematics,
Educational programs"
Enhancing coordination in global cooperative software design,"Global cooperative software design is a formidable task because of the communication and coordination issues. While rescheduling of the tasks is necessary to reduce the impact of communication delay, the dependence of tasks must be well handled. Multiple component status transition graph (MCSTG) describes task dependence within and between components efficiently and support the handle of task dependence. A computer-aided notification mechanism was developed on the basis of MCSTG. The automatic mechanism avoids the task conflicts in coordination while minimizing the cost in project tracking.","Software design,
Delay,
Computer science,
Costs,
Frequency,
Lattices,
Educational institutions,
Cultural differences,
Global communication,
Virtual groups"
Reliability Improvement and Models in Autonomic Computing,"The rapidly increasing complexity of computing systems is driving the movement towards autonomic systems that are capable of managing themselves without the need for human intervention. Without autonomic technologies, many conventional systems suffer reliability degradation due to the accumulation of errors. The autonomic management techniques break the traditional reliability degradation trend. This paper comprehensively describes the roles and functions of various autonomic components, and systematically reviews past and current technologies that have been/are being developed to address the specific areas of the autonomic computing environment. The effort to identify those ideas can lead to the design of more advanced autonomic computing that support highly reliable systems, as briefly proposed in the conclusion","Humans,
Power system reliability,
Degradation,
Costs,
Biology computing,
Environmental management,
Technology management,
Information science,
System performance,
Power system economics"
Unsupervised Modeling of Signs Embedded in Continuous Sentences,"The common practice in sign language recognition is to ?rst construct individual sign models, in terms of discrete state transitions, mostly represented using Hidden Markov Models, from manually isolated sign samples and then to use it to recognize signs in continuous sentences. In this paper we (i) propose a continuous state space model, where the states are based on purely image-based features, without the use of special gloves, and (ii) present an unsupervised approach to both extract and learn models for continuous basic units of signs, which we term as signemes, from continuous sentences. Given a set of sentences with a common sign, we can automatically learn the model for part of the sign, or signeme, that is least affected by coarticulation effects. While there are coarticulation effects in speech recognition, these effects are even stronger in sign language. The model itself is in term of traces in a space of Relational Distributions. Each point in this space represents a Relational Distribution, capturing the spatial relationships between low-level features, such as edge points. We perform speed normalization and then incrementally extract the common sign between sentences, or signemes, with a dynamic programming framework at the core to compute warped distance between two subsentences. We test our idea using the publicly available Boston SignStream Dataset by building signeme models of 18 signs. We test the quality of the models by considering how well we can localize the sign in a new sentence. We also present preliminary results for the ability to generalize across signers.",
Small substructures and decidability issues for first-order logic with two variables,"We study first-order logic with two variables FO/sup 2/ and establish a small substructure property. Similar to the small model property for FO/sup 2/ we obtain an exponential size bound on embedded substructures, relative to a fixed surrounding structure that may be infinite. We apply this technique to analyse the satisfiability problem for FO/sup 2/ under constraints that require several binary relations to be interpreted as equivalence relations. With a single equivalence relation, FO/sup 2/ has the finite model property and is complete for non-deterministic exponential time, just as for plain FO/sup 2/. With two equivalence relations, FO/sup 2/ does not have the finite model property, but is shown to be decidable via a construction of regular models that admit finite descriptions even though they may necessarily be infinite. For three or more equivalence relations, FO/sup 2/ is undecidable.",
Asynchronous games 4: a fully complete model of propositional linear logic,We construct a denotational model of propositional linear logic based on asynchronous games and winning uniform innocent strategies. Every formula A is interpreted as an asynchronous game [A] and every proof /spl pi/ of A is interpreted as a winning uniform innocent strategy [/spl pi/] of the game [A]. We show that the resulting model is fully complete: every winning uniform innocent strategy /spl sigma/ of the asynchronous game [A] is the denotation [/spl pi/] of a proof /spl pi/ of the formula A.,"Logic,
Computer languages,
Tensile stress,
Dynamic programming,
Refining,
Polarization,
Computer science"
Ultracompact vertically coupled microring resonator with buried vacuum cladding structure,"We developed a novel fabrication process for the stacked dense integration of a microring resonator (MRR) buried in a SiO/sub 2/ cladding with vacuum cladding in the lateral direction. Owing to the vertical coupling and the lateral vacuum cladding, the radiation loss can be greatly reduced without affecting the coupling strength between the busline waveguide and the MRR. The reduction of radiation loss by ten orders of magnitude compared with the SiO/sub 2/ buried cladding was theoretically and experimentally demonstrated. Using this technique, an ultracompact vertically coupled MRR with the ring radius of 5 /spl mu/m was realized, and the free spectral range of 29 nm was successfully obtained.","Optical waveguides,
Optical resonators,
Sputtering,
Fabrication,
Optical filters,
Substrates,
Radio frequency,
Optical waveguide theory,
Optical ring resonators,
Computer science education"
An Effective IQ Imbalance Compensation Scheme for MIMO-OFDM Communication System,"MIMO communication systems can provide high channel capacity and is considered as a candidate for the next generation wireless systems. However, there is insufficient investigation about effects of the imperfection caused by the RF hardware on the MIMO systems. In this paper, we focus on the influence of the IQ imbalance on a MIMO-OFDM communication system. As a result, it is found that the IQ imbalance causes both inter-subcarrier and inter-stream interferences in the MIMO-OFDM system, and severely degrades the performance of the system. To combat this problem, a new extended channel matrix including the influence of IQ imbalance is introduced. Then the method of IQ imbalance compensation by using the extended channel matrix is proposed in this paper. The validity of the proposed IQ imbalance compensation scheme on the MIMO-OFDM system is investigated by computer simulation",
Automatic synthesis of cache-coherence protocol processors using Bluespec,"There are few published examples of the proof of correctness of a cache-coherence protocol expressed in an HDL. A designer generally shows the correctness of a protocol where many implementation details have been abstracted away. Abstract protocols are often expressed as a table of rules or state transition diagrams with an (implicit) model of atomic actions. There is enough of a semantic gap between these high-level abstract descriptions and HDLs that the task of showing the correctness of an implementation of a verified abstract protocol is as daunting as proving the abstract protocol's correctness in the first place. The main contribution of this paper is to show that this problem can be largely avoided by expressing the verified abstract protocol in Bluespec SystemVerilog (BSV), which is based on guarded atomic actions and is synthesizable into efficient hardware. Consequently, once a protocol has been verified at the rules-level, little verification effort is needed to verify the implementation. We illustrate our approach by synthesizing a non-blocking MSI cache-coherence protocol for distributed memory systems and discuss the performance of the resulting implementation.","Access protocols,
Hardware design languages,
Formal verification,
Computer science,
Artificial intelligence,
Network synthesis,
Delay,
Parallel programming,
Buildings,
Refining"
Global exponential convergence of multitime-scale neural networks,"In this paper, we investigate the convergence and stability of a neural network model with different time scales, which models the activity of cortical cognitive maps. We provide a theoretic condition for global exponential convergence of the solutions of the network, which is proved weaker than some existing results in the literature. We also introduce time-varying delays with less constraints into the neural network model and derive a general stability condition for the delay network.",
Internet security games as a pedagogic tool for teaching network security,"This research investigates the suitability of online security games as a pedagogic tool, for teaching network security in an educational framework. Based on the advanced challenges they provide, we have selected security games offered by Next Generation Security. Our research is based on network security principles, a core module in MSc Network Security at Anglia Polytechnic University. We compare two cohorts; both were given lectures and laboratory sessions. Only for the second group laboratory sessions were conducted by means of security games. We consider the game usage and views expressed by lecturers and students, to assess whether this method can be usefully incorporated in teaching specific sections of information security. Online and offline course feedback and interviews are used to assess the student experience. Quantitative and qualitative data gathered from this empirical study is analysed to derive conclusions. Advantages, discontents and educational concerns of this method are discussed. Deviation from current learning paradigms is addressed, in relation to the use of pure Internet based tools","IP networks,
Education,
Laboratories,
Information security,
Data security,
Feedback,
Internet,
Computer security,
Testing,
Switches"
Quantifying software architectures: an analysis of change propagation probabilities,"Summary form only given. Software architectures are an emerging discipline in software engineering as they play a central role in many modern software development paradigms. Quantifying software architectures is an important research agenda, as it allows software architects to subjectively assess quality attributes and rationalize architecture-related decisions. In this paper, we discuss the attribute of change propagation probability, which reflects the likelihood that a change that arises in one component of the architecture propagates (i.e. mandates changes) to other components.",
People's assumptions about robots: investigation of their relationships with attitudes and emotions toward robots,"People's assumptions about robots are an important factor affecting their construction and change of attitudes and emotions toward robots. The assumptions should be measured not only to investigate psychological factors determining reactions toward robots but also to study crosscultural attitudes toward robots. As far as measurement of people's attitudes and emotions can contribute to the design of human-robot interaction, questionnaire items measuring assumptions of robots should be prepared. To develop these items measuring individuals' assumptions about robots, a pilot research was administered in Japan. The results implied the possibility that Japanese people assume ""humanoids"" as a representative robots, though this assumption still remains unconnected to realistic assumptions about situations where and tasks that these robots perform; the classical views of robots that physically act for humans remains. These implications present some problems for the administration of the survey. This paper reports detailed results of the pilot survey, discusses implications of the results and problems with them, and suggests possible future works based on the results.",
Verifiable Web services with hierarchical interfaces,We propose an hierarchical state machine (HSM) model for specifying behavioral interfaces of peers participating in a composite Web service. We integrate the HSM model to a design pattern which is supported by a modular verification technique that can 1) statically analyze the properties about global interactions of a composite Web service and 2) check the conformance of the Java implementations of the participant peers to their interfaces. We extend the synchronizability analysis to HSMs to efficiently identify composite Web services whose global interactions can be analyzed with respect to unbounded queues using finite state model checkers. We also discuss automated translation of behavioral interfaces specified as HSMs to BPEL specifications to be published and used by other services.,"Web services,
Automata,
Contracts,
Asynchronous communication,
Computer science,
Pattern analysis,
Java,
Queueing analysis,
Web and internet services,
Collaborative work"
Relative entropy and exponential deviation bounds for general Markov chains,"We develop explicit, general bounds for the probability that the normalized partial sums of a function of a Markov chain on a general alphabet would exceed the steady-state mean of that function by a given amount. Our bounds combine simple information-theoretic ideas together with techniques from optimization and some fairly elementary tools from analysis. In one direction, we obtain a general bound for the important class of Doeblin chains; this bound is optimal, in the sense that in the special case of independent and identically distributed random variables it essentially reduces to the classical Hoeffding bound. In another direction, motivated by important problems in simulation, we develop a series of bounds in a form which is particularly suited to these problems, and which apply to the more general class of ""geometrically ergodic"" Markov chains",
Selective stimulation to superficial mechanoreceptors by temporal control of suction pressure,"In this paper we propose a new set of primitives to realize a large-area covering realistic tactile display. They stimulate the skin surface with suction pressure (SPS method) as our former paper proposed. The difference from the former device is that a single suction hole provides a pair of primitives. Since the identical hole provides the multiple primitives, we can expect multi-primitive tactile stimulation is realized more stably, and the physical structure is simpler than the former method. The method uses the frequency characteristics of the mechanoreceptor sensitivity and a feature of SPS that suction pressure through a hole does not reach the deep receptors, Pacinian corpuscles. We show the basic theory and results of fundamental experiments. In the experiments, we show the spatial feature of the virtual object (edged or round) can be controlled by the temporal profile of the primitives. We explain the reason of the phenomena based on a tactile perception model called simple bundle model.",
Guidelines for designing and developing contents for mobile learning,"The learning benefits deriving from the use of mobile technologies in in-field research and professional training have been demonstrated by a number of initiatives worldwide. Because learning can occur in highly variable places and conditions, the choice of mobile device and an accurate analysis of the target users are essential during the design phase of mobile learning applications, to identify the user scenarios and various possible experiences deriving from use of the system. The present work reconstructs previous experiences in the field of designing contents for mobile learning, with the aim of gleaning clear guidelines for designing and implementing highly efficacious, usable contents and courses for mobile devices. These guidelines were applied to create a course on the work of Caravaggio, to be studied using a handheld device.","Guidelines,
Handheld computers,
Education,
Educational technology,
Educational institutions,
Cellular phones,
Computer science,
Mobile computing,
Australia,
Portable computers"
"Comments on ""A generalized LMI-based approach to the global asymptotic stability of delayed cellular neural networks""","In this letter, we point out that the linear matrix inequality (LMI)-based criterion obtained in the above paper (Singh, IEEE Trans. Neural Netw., vol. 15, no. 1, p. 223-5, 2004) for the global exponential stability of the delayed neural networks can be simplified to a simpler but equivalent form and, thus, show that it is not necessary to have such complex form of condition in the above paper. As a result, we also answer the question raised by the author of the above paper.",
Formal transition in agent organizations,"Multiagent systems (MAS) can be employed to solve a great many problems. When combined with an organization model, a MAS becomes an even more useful structure capable of taking on problems that require self-organization, adaptation, and recovery. To capture and understand the concept of organization and reorganization, we must define organization transition. Specifically, we describe a formal process of organization transition.","Humans,
Companies,
Computer science,
Robots,
Information science,
Educational institutions,
Software agents"
Fast 802.11 link adaptation for real-time video streaming by cross-layer signaling,"The quality of real-time video streaming over wireless links is significantly reduced due to packet losses and delays caused by rapid changes of the link conditions. We resolve this problem by two solutions. First, we have developed a responsive MAC adaptation method using SNR and packet loss statistics. The second solution depends on the first one; it allows the MAC to communicate information about changes of link conditions to the video codec. Together, these solutions prevent packet losses and skipped frames, both of which have an adverse effect on the visual quality of the decoded video. We present the results of a streaming video experiment using an 802.11a link between a fixed and a mobile station. We show that, with the cross-layer signaling between the MAC-layer and the video coder, a 4 dB increase of visual quality is achievable.",
Generalized greedy broadcasting for efficient media-on-demand transmissions,"To reduce the bandwidth of media-on-demand transmissions, different broadcasting techniques have been developed over the last years. For highly demanded media, proactive schemes have a lower bandwidth requirement compared to reactive ones. While some of the proactive schemes try to obtain an efficiency near to the theoretical limit, others have been developed to lower the bandwidth by partial preloading, additional playout delay, the use of breaks within the transmission or by exploiting bandwidth variations of variable-bit-rate media. In this article we describe how one of the most efficient proactive transmission schemes can further be improved and generalized so it supports all these additional enhancements and outperforms its predecessor and the specialized schemes.","Protocols,
Streaming media,
Bandwidth,
TV broadcasting,
Multimedia communication,
Added delay,
Bit rate,
Frequency,
Mathematics,
Computer science"
Effect of localized optimal clustering for reader anti-collision in RFID networks: fairness aspects to the readers,"This paper proposes an adaptive and dynamic localized scheme unique to hierarchical clustering in RFID networks, while reducing the overlapping areas of clusters and consequently reducing collisions among RFID readers. Drew on our LLC scheme that adjusts cluster coverage to minimize energy consumption, low-energy localized clustering for RFID networks (LLCR) addresses RFID reader anti-collision problem in this paper. LLCR is a RFID reader anti-collision algorithm that minimizes collisions by minimizing overlapping areas of clusters that each RFID reader covers. LLCR takes into account each RFID reader's energy state as well as RFID reader collisions. For the energy state factor, we distinguish homogeneous RFID networks from heterogeneous ones according to computing power of each RFID reader. Therefore, we have designed efficient homo-LLCR and hetero-LLCR schemes for each case. Our simulation-based performance evaluation shows that LLCR minimizes energy consumption and overlapping areas of clusters of RFID readers.","Intelligent networks,
Radiofrequency identification,
Clustering algorithms,
RFID tags,
Energy consumption,
Energy states,
Computer networks,
Communication system control,
Computer science,
Educational institutions"
Analysis of access point selection strategy in wireless lan LAN,,"Wireless LAN,
Local area networks,
Throughput,
Computer science,
Computer architecture,
Robustness,
Telecommunication traffic,
Degradation,
Information analysis,
Information science"
User navigational behavior in e-learning virtual environments,"In this paper, we describe the navigational behavior of the students of an e-learning virtual environment, in order to determine whether such navigational patterns are related to the academic performance achieved by the students or not, and which behaviors can be identified as more successful. As an example, a subset of students taking a degree in computer science in a completely virtual online university is selected as the matter of study. Three levels of analysis are described: a session level, where students perform a few actions in a single session logged to the virtual campus; a course level, where all single sessions are joined to form a course navigational pattern; and a lifelong learning level, where students enroll in several subjects each academic semester. A simple experiment is outlined for the course level to demonstrate the possibilities of such analysis in a virtual e-learning environment. This experiment shows that the information collected in this level is useful for understanding user behavior and the relationship with his or her academic achievements, and that some intuitive ideas about the relevance of specific user actions or particularities can be also better explained.",
An adaptive interest management scheme for distributed virtual environments,"Traditionally interest management (IM) in distributed environments has been performed through a 'top-down' expression of interest patterns in the model. For many models the interest patterns are complicated or highly dynamic and are not suitable for expression in this way. Top-down expression also forces the model to contain data about the network communication which should strictly be encapsulated by the infrastructure. We propose a system, based on a distributed shared variable abstraction, capable of automatically deriving interest expressions at the infrastructure level in a 'bottom-up' manner. We demonstrate through a simulated testbed that for certain access patterns this dramatically improves the precision of the IM system without sacrificing transparency from a modeller perspective.",
Checking array bound violation using segmentation hardware,"The ability to check memory references against their associated array/buffer bounds helps programmers to detect programming errors involving address overruns early on and thus avoid many difficult bugs down the line. This paper proposes a novel approach called Cash to the array bound checking problem that exploits the segmentation feature in the virtual memory hardware of the X86 architecture. The Cash approach allocates a separate segment to each static array or dynamically allocated buffer, and generates the instructions for array references in such a way that the segment limit check in X86's virtual memory protection mechanism performs the necessary array bound checking for free. In those cases that hardware bound checking is not possible, it falls back to software bound checking. As a result, Cash does not need to pay per-reference software checking overhead in most cases. However, the Cash approach incurs a fixed set-up overhead for each use of an array, which may involve multiple array references. The existence of this overhead requires compiler writers to judiciously apply the proposed technique to minimize the performance cost of array bound checking. This paper presents the detailed design and implementation of the Cash compiler, and a comprehensive evaluation of various performance tradeoffs associated with the proposed array bound checking technique. For the set of complicated network applications we tested, including Apache, Sendmail, Bind, etc., the latency penalty of Cash's bound checking mechanism is between 2.5% to 9.8% when compared with the baseline case that does not perform any bound checking.","Hardware,
Data structures,
Computer errors,
Computer bugs,
Application software,
Buffer overflow,
Computer science,
Programming profession,
Protection,
Costs"
Teaching (with) robots in secondary schools: some new and not-so-new pedagogical problems,"Research on the teaching of programming has shown that novice programmers often come up against significant difficulties in understanding programming concepts as well as in finding solutions to even elementary programming problems. One of the basic strategies which has been adopted in order to confront this problem, were programming languages and environments created with the specific aim to make both the teaching and learning processes of programming easier. A typical category of these types of environments consists of systems where the novice programmer manipulates real entities such as robots and automatic mechanisms. In this research we present a series of pilot lessons for an introduction to programming with the help of Lego Mindstorms and the visual programming environment ROBOLAB. The research was carried out on Greek secondary school students in the 9th and 10th grades. We present the initial results, which clearly show the advantages of this environment over the standard ones conventionally used, as well as some problems.","Education,
Educational robots,
Programming profession,
Robot programming,
Computer languages,
Robotics and automation,
Programming environments,
Educational institutions,
Toy manufacturing industry,
Informatics"
An optimal tone reproduction curve operator for the display of high dynamic range images,"We present a new tone mapping method for the display of high dynamic range images in low dynamic range devices. We formulate high dynamic range image tone mapping as an optimisation problem. We introduce a two-term cost function, the first term favours linear scaling mapping, the second term favours histogram equalisation mapping, and jointly optimising the two terms optimally maps a high dynamic range image to a low dynamic range image. We control the mapping results by adjusting the relative weightings of the two terms in the objective function. We also present a fast and simple implementation for solving the optimisation problem. We present results to demonstrate that our method works very effectively.","Dynamic range,
Computer displays,
Humans,
Layout,
Visual system,
Image coding,
Brightness,
Computer science,
Cost function,
Histograms"
Lossless compression of DNA microarray images,"Microarray experiments are characterized by a massive amount of data, usually in the form of an image. Based on the nature of microarray images, we consider the microarray in terms of its structure and statistics. Based on the microarray image model, we propose a context-based method for lossless compression of microarray images using prediction by partial approximate matching (PPAM). In synchronization experiments, the raw data consists of two channel microarray images. The correlation between these two channel microarray images is explored in order to improve the compression performance. Our results show that, the proposed approach produces a better compression result when compared with results from the best-known microarray compression algorithm.","Image coding,
DNA,
Bioinformatics,
Pixel,
Statistics,
Genomics,
Computer science,
Context modeling,
Predictive models,
Compression algorithms"
BOXes: efficient maintenance of order-based labeling for dynamic XML data,"Order-based element labeling for tree-structured XML data is an important technique in XML processing. It lies at the core of many fundamental XML operations such as containment join and twig matching. While labeling for static XML documents is well understood, less is known about how to maintain accurate labeling for dynamic XML documents, when elements and subtrees are inserted and deleted. Most existing approaches do not work well for arbitrary update patterns; they either produce unacceptably long labels or incur enormous relabeling costs. We present two novel I/O-efficient data structures, W-BOX and B-BOX that efficiently maintain labeling for large, dynamic XML documents. We show analytically and experimentally that both, despite consuming minimal amounts of storage, gracefully handle arbitrary update patterns without sacrificing lookup efficiency. The two structures together provide a nice tradeoff between update and lookup costs: W-BOX has logarithmic amortized update cost and constant worst-case lookup cost, while B-BOX has constant amortized update cost and logarithmic worst-case lookup cost. We further propose techniques to eliminate the lookup cost for read-heavy workloads.","Labeling,
XML,
Costs,
Query processing,
Helium,
Computer science,
Data structures,
Pattern analysis,
Internet,
Engineering profession"
Detecting and filtering instant messaging spam - a global and personalized approach,"While instant message (IM) is gaining its popularity it is exposed to increasingly severe security threats. A serious problem is IM spam (spim) that is unsolicited commercial messages sent via IM messengers. Unlike e-mail spam (unsolicited bulk e-mails), which has been a serious security issue for a long time and a number of techniques have been proposed to cope with, spim has not received adequate attention from the research community yet, and traditional spam filtering techniques are not directly applicable to spim due to its presence information and real time nature. In this paper, we present a new architecture for detecting and filtering spim. With the unique infrastructure of IM systems spim detection and filtering can be achieved not only at the client (receiver) side - for a personalized filtering - but also at the server side and various IM gateways - for a global filtering. Our technique integrates a number of mature spam defending techniques with modifications for IM applications, such as Black/White List, collaborative feedback based filtering, content-based technique, and challenge-response based filtering. We also design and implement new techniques for efficient spim detection and filtering, including filtering methods based on IM sending rate, content based spim defending techniques, fingerprint vector based filtering, text comparison filtering, and Bayesian filtering. We provide an analysis of their performances based on experimental results.","Unsolicited electronic mail,
Electronic mail,
Protocols,
Information security,
Computer science,
Computer security,
Information filtering,
Information filters,
Collaboration,
Feedback"
Numerical computation of incomplete Lipschitz-Hankel integrals of the Hankel type for complex-valued arguments,"Incomplete Lipschitz-Hankel integrals (ILHIs) form an important class of special functions since they appear in numerous applications in engineering and physics. While ILHIs of the Hankel type can be expressed as linear combinations of ILHIs of the Bessel and Neumann types, this procedure can lead to computational inaccuracies. As shown in this paper, these inaccuracies can be avoided by directly computing ILHIs of the Hankel types (both the first and second kinds). If desired, these results can then be combined to obtain accurate values for the ILHIs of the Bessel and Neumann types. Series representations for complementary incomplete Lipschitz-Hankel integrals (CILHIs) are also derived in this paper. CILHIs are often needed to avoid numerical inaccuracies caused by finite precision arithmetic. In order to better understand this group of special functions, the characteristics of ILHIs and CILHIs of the Hankel type are also discussed.","Computers,
Propagation losses,
Analytical models,
Electrical engineering,
Integrated circuits,
Transient analysis"
Collaborative two-level task scheduling for wireless sensor nodes with multiple sensing units,,"Collaboration,
Wireless sensor networks,
Scheduling algorithm,
Energy consumption,
Data communication,
Delay,
Switches,
Processor scheduling,
Computer science,
Data engineering"
Identifying Top Java Errors for Novice Programmers,"All freshmen at the United States Military Academy take an introductory programming course. We use a custom-built integrated development environment to help teach Java. During previous work, we implemented an integrated semantic and syntax error pre-processing system to help novice programmers decipher the otherwise cryptic compiler error messages in order for them to focus more on design issues than implementation issues. The syntactic errors that we checked were gathered by an informal survey of the current and former faculty members teaching the course. We noticed over the course of the year that there were discrepancies between the errors that the instructors had identified and the errors that the students were encountering. In response, we developed a real-time, automated error collection system that logged 100% of the Java errors in a central database that all users, students and faculty alike, encountered while using the integrated development environment over the course of a semester. This paper discusses the implementation and results of our system as well as the implications for novice programmers","Java,
Programming profession,
Computer errors,
Real time systems,
Lab-on-a-chip,
Program processors,
Education,
Information technology,
Databases,
Educational institutions"
Convex grouping combining boundary and region information,"Convexity is an important geometric property of many natural and man-made structures. Prior research has shown that it is imperative to many perceptual-organization and image-understanding tasks. This paper presents a new grouping method for detecting convex structures from noisy images in a globally optimal fashion. Particularly, this method combines both region and boundary information: the detected structural boundary is closed and well aligned with detected edges while the enclosed region has good intensity homogeneity. We introduce a ratio-form cost function for measuring the structural desirability, which avoids a possible bias to detect small structures. A new fragment-pruning algorithm is developed to achieve the structural convexity. The proposed method can also be extended to detect open boundaries, which correspond to the structures that are partially cropped by the image perimeter and incorporate a human-computer interaction for detecting a convex boundary around a specified point. We test the proposed method on a set of real images and compare it with the Jacobs'convex-grouping method","Image edge detection,
Jacobian matrices,
Psychology,
Face detection,
Image segmentation,
Computer science,
Cost function,
Testing,
Computer vision,
Application software"
Dot plots for time series analysis,"Since their introduction in the seventies by Gibbs and McIntyre, dot plots have proved to be a powerful and intuitive technique for visual sequence analysis and mining. Their main domain of application is the field of bioinformatics where they are frequently used by researchers in order to elucidate genomic sequence similarities and alignment. However, this useful technique has remained comparatively constrained to domains where the data has an inherent discrete structure (i.e., text). In this paper we demonstrate how dot plots can be used for the analysis and mining of real-valued time series. We design a tool that creates highly descriptive dot plots which allow one to easily detect similarities, anomalies, reverse similarities, and periodicities well as changes in the frequencies of repetitions. As the underlying algorithm scales we with the input size, we also show the feasibility of the plots for on-line data monitoring","US Department of Transportation,
Time series analysis,
Bioinformatics,
Computer science,
Application software,
Genomics,
Frequency,
Monitoring,
Amino acids,
Pattern matching"
Straight line routing for wireless sensor networks,"Sensor networks are large-scale distributed sensing networks comprised of many small sensing devices equipped with memory, processors, and short-range wireless communication radio. Instead of broadcast-based routing protocols, in this paper we propose a novel energy-efficient routing protocol, which is called straight line routing algorithm (SLR), for wireless sensor networks. To achieve the routing task without broadcasting, the source host constructs the event path and the sink host constructs the query path respectively. That is, the routing path is found as the query path and the event path first intersect. Moreover, the SLR is able to build both the query path and the event path without any help of the geographic information. We evaluate the performance of straight line routing and rumor routing protocols through extensive simulations. The simulation results indicate that compared with rumor routing, the SLR can save more energy consumption, provide better path quality, and improve the successful ratio of routing as well.","Wireless sensor networks,
Routing protocols,
Monitoring,
Temperature sensors,
Wireless communication,
Radio broadcasting,
Sensor phenomena and characterization,
Spirals,
Chaotic communication,
Computer science"
Implementing ERP systems in higher education institutions,,"Enterprise resource planning,
Business,
Computer science education,
Companies,
Packaging,
Market opportunities,
Educational institutions,
Informatics,
Pervasive computing,
Communications technology"
Noise in algorithm refinement methods,"Hybrid or algorithm refinement (AR) schemes have focused mainly on the mean behavior of system states. However, variances in these behaviors, such as spontaneous fluctuations, are important for modeling certain phenomena. This paper discusses the effects of statistical fluctuations on hybrid computational methods that combine a particle algorithm with a partial differential equation solver.","Fluctuations,
Temperature,
Stochastic resonance,
Noise generators,
Partial differential equations,
Algorithm design and analysis,
System testing,
Computational modeling,
Microscopy,
Metastasis"
Inference-based Ambiguity Management in Decentralized Decision-Making: Decentralized Control of Discrete Event Systems,"Decentralized decision-making requires the interaction of various local decision-makers in order to arrive at a global decision. Limited sensing capabilities at each local site can create ambiguities in a decision-making process at each local site. We argue that such ambiguities are of differing gradations. We propose a framework for decentralized decisionmaking (applied to decentralized control in particular) which allows computation of such ambiguity-gradations, and utilizes their knowledge in arriving at a global decision. Each local decision is tagged with a certain grade or level of ambiguity; zero being the minimum ambiguity level. A global decision is taken to be the same as a ""winning"" local decision, i.e., one having the minimum level of ambiguity. The computation of an ambiguity level for a local decision requires an assessment of the self-ambiguities as well as the ambiguities of the others, and an inference based up on such knowledge. In order to characterize the class of closed-loop behaviors achievable under the control of such an inference-based decentralized control, we introduce the notion of N-inference-observability, where N is an index representing the maximum ambiguity level of any winning local decision. We show that the C&PVD&Acoobservability is the same as the zero-inference-observability, whereas the conditional C&PVD&A-coobservability is the same as the unity-inference-observability. We also present examples of higher order inference-observable languages.",
Reconfiguration Planning Among Obstacles for Heterogeneous Self-Reconfiguring Robots,"Most reconfiguration planners for self-reconfiguring robots do not consider the placement of specific modules within the configuration. Recently, we have begun to investigate heterogeneous reconfiguration planning in lattice-based systems, in which there are various classes of modules. The start and goal configurations specify the class of each module, in addition to placement. Our previous work presents solutions for this problem with unrestricted free space available to the robot during reconfiguration, and also free space limited to a thin connected region over the entire surface of the configuration. In this paper, we further this restriction and define free space by an arbitrarily-shaped bounding region. This addresses the important problem of reconfiguration among obstacles, and reconfiguration over a rigid surface. Our algorithm plans module trajectories through the volume of the structure, and is divided into two phases: shape-forming, and sorting the goal configuration to correctly position modules by class. The worst-case running time for the first phase is O(n2) with O(n2) moves for an n-module robot, and a loose upper bound for the second phase is O(n4) time and moves. However, we show this bound to be Î˜ (n2)time and moves in common instances.","Orbital robotics,
Robot sensing systems,
Strontium,
Computer science,
Power system planning,
Educational institutions,
Sorting,
Upper bound,
Context,
Communication system control"
Multi-path admission control for mobile ad hoc networks,"As wireless networks become more prevalent, users will demand the same applications that are currently available in wired networks. Further, they will expect to receive a quality of service similar to that obtained in a wired network. Included in these applications are real-time applications such as voice over IP and multimedia streams. To enable the support of applications that require real-time communication in ad hoc networks, congestion must be prevented so that the needed quality of service can be provided. An admission control mechanism is an essential component of the quality of service solution. Unfortunately, current admission control solutions encounter problems during mobility, often resulting in unacceptable disruptions in communication. To solve this problem, we apply multi-path routing mechanisms that maintain alternate paths to the destination and propose a new admission control protocol. We show through simulation that our solution is able to prevent communication disruptions and meet the QoS needs of applications better than previous solutions.",
Virtual camerawork for generating lecture video from high resolution images,"We propose a method for generating a dynamic lecture video from the high resolution images recorded by a HDV camcorder. The lecture images are cropped to track the region of the instructor. Our approach uses bilateral filtering to avoid jittery motion caused by temporal differencing, and pseudo camera motion (panning) based on shooting technique of broadcast cameraman is generated. We evaluated our method, and showed that the effectiveness of our algorithm was verified through subjective experiments.","Image resolution,
Cameras,
Broadcasting,
Multimedia communication,
Motion detection,
Video equipment,
Filtering,
Broadcast technology,
Timing,
Computer science"
Early evaluation for performance enhancement in phased logic,"Data-dependent completion time is a well-known advantage of self-timed circuits, one that allows them to operate at average rather than worst-case execution rates. A technique called early evaluation (EE) that extends this advantage by allowing self-timed modules to produce results before all of their inputs have arrived is described here. The technique can be applied to any combinational function and is integrated into the phased logic (PL) design methodology that accepts synchronous design entry and produces delay-insensitive self-timed circuits. We describe an algorithm that ensures that the resulting delay-insensitive circuits are safe, and develop a generalized method for inserting EE gates into any PL netlist. We give performance results for several benchmark circuits, including a five-stage pipelined CPU and a microprogrammed floating-point unit. Comparisons are made among clocked circuits, PL circuits, and PL circuits with EE. Simulation results show a clear performance benefit for PL circuits that use EE.","Logic circuits,
Clocks,
Delay,
Logic design,
Signal design,
Circuit synthesis,
Computer science,
Safety,
Feedback,
Combinational circuits"
Cooperative caching with keep-me and evict-me,"Cooperative caching seeks to improve memory system performance by using compiler locality hints to assist hardware cache decisions. In this paper, the compiler suggests cache lines to keep or evict in set-associative caches. A compiler analysis predicts data that will be and will not be reused, and annotates the corresponding memory operations with a keep-me or evict-me hint. The architecture maintains these hints on a cache line and only acts on them on a cache miss. Evict-me caching prefers to evict lines marked evict-me. Keep-me caching retains keep-me lines if possible. Otherwise, the default replacement algorithm evicts the least-recently-used (LRU) line in the set. This paper introduces the keep-me hint, the associated compiler analysis, and architectural support. The keep-me architecture includes very modest ISA support, replacement algorithms, and decay mechanisms that avoid retaining keep-me lines indefinitely. Our results are mixed for our implementation of keep-me, but show it has potential. We combine keep-me and evict-me from previous work, but find few additive benefits due to limitations in our compiler algorithm, which only applies each independently rather than performing a combined analysis.",
A universal random test generator for functional verification of microprocessors and system-on-chip,"This paper presents a universal random test generator template for the design verification of microprocessors and system-on-chips (SOCs). The tool enables verification of the product in one continuous, integrated environment, from C model to behavioral RTL and gate to system-level integration, all in one self-contained chassis. Due to complexity of large designs, it has been a common practice to rely on the power of randomization, to bless us with the humanly not-conceivable corner cases that can arise in reality. There are lots of common features shared by random tools used for testing products with diverse functionalities. This paper proposes a template which captures the commonalities among the different random testing tools and enables the user to quickly design a random test generator by adding product-specific details and using most of the methods available in the template. This leads to high degree of code reuse, less debugging of the random tool and huge reduction in design-cycle time. In addition the template provides enough flexibility and interfaces to enable the execution of the generated tests on targets which may be a C model, RTL or the final chip. By this, one may test a software component, say a bootup code for the system-on-chip or microprocessor at all stages of its design, namely, the software prototype, the RTL at the pre-silicon level and finally the chip, at a post-silicon level. This satisfies the expectations out of a verification platform for a hardware-software codesign environment. The random test generator template was employed for testing a x86-compatible microprocessor both at RTL and post-silicon stage and a software model of a 802.11 MAC. The results are presented in the paper.","System testing,
Microprocessors,
System-on-a-chip,
Hardware,
Software testing,
Computer science,
Design engineering,
Automatic testing,
Debugging,
Software prototyping"
The new intrusion prevention and detection approaches for clustering-based sensor networks [wireless sensor networks],"In this paper, we propose two approaches to improve the security of clustering-based sensor networks: authentication-based intrusion prevention and energy-saving intrusion detection. In the first approach, different authentication mechanisms are adopted for two common packet categories in generic sensor networks to save the energy of each node. In the second approach, different monitoring mechanisms are also needed to monitor cluster-heads (CHs) and member nodes according to their importance. When monitoring CHs, member nodes of a CH take turns to monitor this CH. This mechanism reduces the monitor time, and therefore saves the energy of the member nodes. When monitoring member nodes, CHs have the authority to detect and revoke the malicious member nodes. This also saves the node energy because of using CHs to monitor member nodes instead of using all the member nodes to monitor each other. Finally, simulations are performed and compared with LEACH, based on an ns2 LEACH CAD tool. The simulation result shows that the proposed approaches obviously extend the network lifetime when the clustering-based sensor network is under attack.","Intrusion detection,
Wireless sensor networks,
Monitoring,
Data security,
Computer science,
Power engineering and energy,
Military computing,
Routing protocols,
Computer security,
Information security"
Monocular 3D tracking of the golf swing,"This paper showcases our tracking algorithm described in (R. Urtasun et al., 2005). The proposed approach incorporates dynamic motion models into the human body tracking process yielding full 3D reconstruction from monocular sequences. The tracking is formulated in terms of minimizing a differentiable criterion whose differential structure is rich enough for successful optimization using a single hypothesis. In other words we avoid the computational complexity of multihypotheses algorithms while obtaining excellent results under challenging conditions.","Computer vision,
Laboratories,
Tracking,
Cameras,
Skeleton,
Computer science,
Biological system modeling,
Humans,
Image reconstruction,
Computational complexity"
Learning by Kernel Polarization,"Kernels are key components of pattern recognition mechanisms. We propose a universal kernel optimality criterion, which is independent of the classifier to be used. Defining data polarization as a process by which points of different classes are driven to geometrically opposite locations in a confined domain, we propose selecting the kernel parameter values that polarize the data in the associated feature space. Conversely, the kernel is said to be polarized by the data. Kernel polarization gives rise to an unconstrained optimization problem. We show that complete kernel polarization yields consistent classification by kernel-sum classifiers. Tested on real-life data, polarized kernels demonstrate a clear advantage over the Euclidean distance in proximity classifiers. Embedded in a support vectors classifier, kernel polarization is found to yield about the same performance as exhaustive parameter search.",
Semi-supervised cross feature learning for semantic concept detection in videos,"For large scale automatic semantic video characterization, it is necessary to learn and model a large number of semantic concepts. But a major obstacle to this is the insufficiency of labeled training samples. Multi-view semi-supervised learning algorithms such as co-training may help by incorporating a large amount of unlabeled data. However, one of their assumptions requiring that each view be sufficient for learning is usually violated in semantic concept detection. In this paper, we propose a novel multi-view semi-supervised learning algorithm called semi-supervised cross feature learning (SCFL). The proposed algorithm has two advantages over co-training. First, SCFL can theoretically guarantee its performance not being significantly degraded even when the assumption of view sufficiency fails. Also, SCFL can also handle additional views of unlabeled data even when these views are absent from the training data. As demonstrated in the TRECVID '03 semantic concept extraction task, the proposed SCFL algorithm not only significantly outperforms the conventional co-training algorithms, but also comes close to achieving the performance when the unlabeled set were to be manually annotated and used for training along with the labeled data set.","Videos,
Semisupervised learning,
Data mining,
Speech,
Gunshot detection systems,
Computer science,
Drives,
Large-scale systems,
Degradation,
Training data"
A method for determination of the timing stability of PET scanners,"We report on the timing resolution and stability of the MicroPET R4 PET scanner. Its detectors have energy resolutions in the range of 25% and previously reported timing resolutions 3.2 ns. Our preliminary evaluation of this instrument showed that artefact-free normalization sinograms could only be obtained with a timing window of 10 ns or more in spite of a timing resolution of 3.2 ns. This instrument uses high-speed electronics albeit with 2-ns timing clock. We performed sham transmission scans with nothing in the field of view, and a range of timing windows from 2 to 14 ns and used a 14-ns timing blank scan to generate effective attenuation sinograms as a function of timing window. These showed trues count-rates which fit well to a ERF (/spl tau/) function. However, the effective attenuation value, which should be 1.0, changes from block to block and becomes very high in some blocks (>3.5 at 6 ns) suggesting the need for timing alignment. A method was devised to measure the timing stability to a precision better than the timing bin width of 2 ns.","Timing,
Stability,
Positron emission tomography,
Probes,
Energy resolution,
Detectors,
Instruments,
Crystals,
Attenuation,
High-speed electronics"
Analysis on the central-stage buffered Clos-network for packet switching,"Our work is motivated by the desire to build a scalable packet switch with extremely large number of ports. We consider building a multi-stage packet switch from many mid-size packet switches with distributed memories in the central stage. This new architecture resembles the famous Clos-network used in circuit switching systems except that it has buffers in the central stage. We call it Central-stage buffered Clos-network (CBC). In particular, we denote the symmetric Clos-network as (n, m, k) which means k input modules with n input ports each and m central modules. Each module is a non-blocking switch. Ideally, this CBC architecture would have similar benefits as those of an output-queued switch, i.e., the delay of individual packets could be precisely controlled, allowing the provision of guaranteed qualities of service. The main result of this paper is that, if m is approximately 4 times that of n, it is theoretically possible for a CBC to emulate an FCFS output-queued packet switch with all components running at the line rate, i.e., with no speedup. Particularly, we need to double the traditional strictly non-blocking Clos-network in the number of central modules. We show that the need to double the central modules is due to resolving the input port conflicts. But it is still much more cost effective compared with scaling one stage switches which usually have a complexity proportional to the square of the number of ports. If we slightly modify the CBC structure, we can further show that CBC can emulate any QoS queuing discipline if m is approximately 4 times of n. But packets may experience some delay which is bounded within a constant time.","Packet switching,
Switches,
Delay,
Switching systems,
Traffic control,
Buildings,
Circuits,
Costs,
Computer science,
Quality of service"
Exploring Usability Discussions in Open Source Development,The public nature of discussion in open source projects provides a valuable resource for understanding the mechanisms of open source software development. In this paper we explore how open source projects address issues of usability. We examine bug reports of several projects to characterise how developers address and resolve issues concerning user interfaces and interaction design. We discuss how bug reporting and discussion systems can be improved to better support bug reporters and open source developers.,"Usability,
Computer bugs,
Open source software,
User interfaces,
Databases,
Software libraries,
Information science,
Computer science,
Robustness,
Graphical user interfaces"
High resolution tracking of non-rigid 3D motion of densely sampled data using harmonic maps,"We present a novel fully automatic method for high resolution, nonrigid dense 3D point tracking. High quality dense point clouds of nonrigid geometry moving at video speeds are acquired using a phase-shifting structured light ranging technique. To use such data for the temporal study of subtle motions such as those seen in facial expressions, an efficient nonrigid 3D motion tracking algorithm is needed to establish inter-frame correspondences. The novelty of this paper is the development of an algorithmic framework for 3D tracking that unifies tracking of intensity and geometric features, using harmonic maps with added feature correspondence constraints. While the previous uses of harmonic maps provided only global alignment, the proposed introduction of interior feature constraints guarantees that nonrigid deformations are accurately tracked as well. The harmonic map between two topological disks is a diffeomorphism with minimal stretching energy and bounded angle distortion. The map is stable, insensitive to resolution changes and is robust to noise. Due to the strong implicit and explicit smoothness constraints imposed by the algorithm and the high-resolution data, the resulting registration/deformation field is smooth, continuous and gives dense one-to-one inter-frame correspondences. Our method is validated through a series of experiments demonstrating its accuracy and efficiency.","Tracking,
Geometry,
Computer vision,
Face detection,
Shape,
Computer science,
Mechanical engineering,
Clouds,
Harmonic distortion,
Energy resolution"
Message ferrying for constrained scenarios,"Message ferrying (MF) (Wenrui Zhao and Ammar, M.H., Proc. IEEE Workshop on Future Trends in Distrib. Computing Syst., 2003), a viable solution for routing in highly partitioned ad-hoc networks, exploits message ferries to transfer packets between disconnected nodes. The paper studies the delivery quality of service (QoS) for certain urgent messages in the constrained and the relaxed constrained MF systems. Efficient algorithms to compute near-optimal ferry routes are proposed, delay analysis is conducted and the results are compared to the non-constrained scenario.","Mobile communication,
Routing,
Ad hoc networks,
Quality of service,
Computer networks,
Computer science,
Partitioning algorithms,
Delay,
Algorithm design and analysis,
Marketing and sales"
Timing-Based Model of Body Schema Adaptation and its Role in Perception and Tool Use: A Robot Case Study,"The multisensory representation of our body (body schema), and its conscious and manipulable counterpart (body image) play a pivotal role in the development and expression of many higher level cognitive functions, such as tool use, imitation, spatial perception, and self-awareness. This paper addresses the issue of how the body schema changes as a result of tool use-dependent experience. Although it is plausible to assume that such an alteration is inevitable, the mechanisms underlying such plasticity have yet to be clarified. To tackle the problem, we propose a novel model of body schema adaptation which we instantiate in a tool using robot. Our experimental results confirm the validity of our model. They also show that timing is a particularly important feature of our model because it supports the integration of visual, tactile, and proprioceptive sensory information. We hope that the approach exposed in this study allows gaining further insights into the development of tool use skills and its relationship to body schema plasticity",
Linear randomized voting algorithm for fault tolerant sensor fusion and the corresponding reliability model,"Sensor failures in process control programs can be tolerated through application of well known modular redundancy schemes. The reliability of a specific modular redundancy scheme depends on the predefined number of sensors that may fail, f, out of the total number of sensors available, n. Some recent sensor fusion algorithms offer the benefit of tolerating a more significant number of sensor failures than modular redundancy techniques at the expense of degrading the precision of sensor readings. In this paper, we present a novel sensor fusion algorithm based on randomized voting, having linear - O(n) expected execution time. The precision (the length) of the resulting interval is dependent on the number of faulty sensors - parameter f. A novel reliability model applicable to general sensor fusion schemes is proposed. Our modeling technique assumes the coexistence of two major types of sensor failures, permanent and transient. The model offers system designers the ability to analyze and define application specific balances between the expected system reliability and the desired interval estimate precision. Under the assumptions of failure independence and exponentially distributed failure occurrences, we use Markov models to compute system reliability. The model is then validated empirically and examples of reliability prediction are provided for networks with fairly large number of sensors (n>100).",
Contrast enhancement of multi-displays using human contrast sensitivity,"Study of contrast sensitivity of the human eye shows that we are more sensitive to brightness differences at low intensity levels than at high intensity levels. We apply this fact effectively to achieve brightness seamless-ness in multi-projector displays. Multi-displays, popularly made of a rectangular array of partially overlapping projectors, show severe spatial variation in brightness. Existing methods achieve brightness uniformity across the display by matching the brightness response of every pixel to the pixel with the most limited contrast leading to severe compression in the contrast of the display. In this paper, we propose a method that allows a constrained variation in brightness guided by the human contrast sensitivity function such that it is imperceptible to the human eye. This achieves a seamless multi-display with a dramatic improvement in the contrast making the display practically usable.","Humans,
Brightness,
Liquid crystal displays,
Optical attenuators,
Optical filters,
Optical sensors,
Computer science,
Computer displays,
Large-scale systems,
Visualization"
Non-Cooperation in Competitive P2P Networks,"Large-scale competitive P2P networks are threatened by the non-cooperation problem, where peers do not forward queries to potential competitors. Non-cooperation will be a growing problem in such applications as pay-per-transaction file-sharing, P2P auctions, and P2P service discovery networks, where peers are in competition with each other to provide services. Here, the authors showed how non-cooperation causes unacceptable degradation in quality of results, and present an economic protocol to address this problem. This protocol, called the RTR protocol, is based on the buying and selling of the right-to-respond (RTR) to each query in the network. Through simulations it is shown how the RTR protocol not only overcomes non-cooperation by providing proper incentives to peers, but also results in a network that is even more effective and efficient through intelligent, incentive-compatible routing of messages","Intelligent networks,
Peer to peer computing,
Computer science,
Large-scale systems,
Application software,
Degradation,
Routing protocols,
Costs,
Usability,
Information retrieval"
Digital electronics learning system based on FPGA applications,"FPGAs (field programmable gate arrays) constitute the base of many complex electronic systems with different applications ranging from the automotive sector to the multimedia market. Due to integrated circuits fabrication progress, current FPGAs includes very complex embedded digital blocks as serial transceivers or memory blocks. Therefore, FPGA fundamentals and characteristics are not easy to explain considering that there are also a lot of devices and families from different manufacturers. By this reason, FPGA learning is usually based on describing a family and doesn't give a general view of the wide range of commercial devices. Due to that it is essential for the electronic learning community to dispose of tools facilitating the learning process of the FPGA fundamentals and the design of systems based on them. The learning system combines a tutorial with hardware and software tools to achieve a friendly interface with a computer intended to facilitate FPGA distance learning for students with a basic knowledge of digital electronics and VHDL","Electronic learning,
Field programmable gate arrays,
Automotive engineering,
Multimedia systems,
Digital integrated circuits,
Fabrication,
Transceivers,
Manufacturing,
Learning systems,
Hardware"
Coding with side information techniques for LSF reconstruction in voice over IP,"In applications like VoIP, speech codecs have to deal with excessive packet losses, caused by network errors and/or delays. In this paper, a new method for the reconstruction of lost speech spectral envelopes is presented, which is based on a statistical estimation function. We suggest the usage of a minimal ""corrective"" bitstream and propose coding with side information (CSI) techniques for an efficient forward error correction (FEC) strategy. The proposed methods are tested on multiple scenarios of missing frames. Objective results indicate that with only 4 bits per lost frame, a spectral distortion reduction of 0.77-1.14 dB is achieved, compared to results obtained by current state-of-the-art estimation methods. Compared to ""predictive"" estimation methods, the use of the jitter buffer as side information and 4 bits per lost frame provide a 42% reduction of spectral distortion for single packet losses, and a 32% reduction for double packet losses. Subjective results indicate that the corrected speech has fewer artifacts.",
Sampling-based approximation algorithms for multi-stage stochastic optimization,"Stochastic optimization problems provide a means to model uncertainty in the input data where the uncertainty is modeled by a probability distribution over the possible realizations of the actual data. We consider a broad class of these problems in which the realized input is revealed through a series of stages, and hence are called multi-stage stochastic programming problems. Our main result is to give the first fully polynomial approximation scheme for a broad class of multi-stage stochastic linear programming problems with any constant number of stages. The algorithm analyzed, known as the sample average approximation (SAA) method, is quite simple, and is the one most commonly used in practice. The algorithm accesses the input by means of a ""black box"" that can generate, given a series of outcomes for the initial stages, a sample of the input according to the conditional probability distribution (given those outcomes). We use this to obtain the first polynomial-time approximation algorithms for a variety of k-stage generalizations of basic combinatorial optimization problems.","Approximation algorithms,
Stochastic processes,
Uncertainty,
Probability distribution,
Linear programming,
Polynomials,
Costs,
Water resources,
Reservoirs,
Investments"
Decoupled dynamic ternary content addressable memories,"The content addressable memory (CAM) is a memory in which data can be accessed on the basis of contents rather than by specifying physical address. In the paper, five novel dynamic ternary CAM cells with decoupled match lines are presented. A ternary CAM cell is capable of storing and matching three values: zero (0), one (1), and don't care (X). The proposed dynamic CAM (DCAM) cells range in the number of transistors from 6 n-type transistors up to 10.5 n- and p-type transistors (one transistor is shared between two cells). The cells are capable of fast match and read operations enhancing the performance of the memory system. Using a 0.25-/spl mu/m CMOS technology, simulations of the proposed CAM cells were performed to compare their performance. With this technology, the shortest match delay is 89.7 ps for the 7.5 DCAM cell. A complete characterization of the five cells is provided in this paper. These results show that the novel CAM cells outperform existing cells. The compact size and low power dissipation of these ternary CAM cells make them suitable for many applications such as routers, database, and associative cache memories.","Associative memory,
Computer aided manufacturing,
CADCAM,
Vehicle dynamics,
Routing,
CMOS technology,
Circuits,
Delay,
Computer science,
Logic"
Text region extraction and text segmentation on camera-captured document style images,"In this paper, we propose a text extraction method from camera-captured document style images and propose a text segmentation method based on a color clustering method. The proposed extraction method detects text regions from the images using two low-level image features and verifies the regions through a high-level text stroke feature. The two level features are combined hierarchically. The low-level features are intensity variation and color variance. And, we use text strokes as a high-level feature using multi-resolution wavelet transforms on local image areas. The stroke feature vector is an input to a SVM (support vector machine) for verification, when needed. The proposed text segmentation method uses color clustering to the extracted text regions. We improved K-means clustering method and it selects K and initial seed values automatically. We tested the proposed methods with various document style images captured by three different cameras. We confirmed that the extraction rates are good enough to be used in real-life applications.",
Voice over IP - considerations for a next generation architecture,"Voice over IP is on its way to becoming an alternative to the classical telephony system because more and more Voice over IP providers offer their solutions on the Internet and try to increase their clientele. But current voice over IP technology is still in its commercial and technological infancy and has not yet proven to be worldwide accessible, usable and scalable as the classical telephony system has proven in the past 100 years. In this paper, architecture for a next generation Voice over IP model will be outlined and discussed. The main focus lies on interoperability between different Voice over IP providers as well as dependability and robustness.","Internet telephony,
Web and internet services,
Hardware,
Computer architecture,
Costs,
Software maintenance,
Computer science,
Robustness,
Protocols,
TV"
An Iterative and Agile Process Model for Teaching Software Engineering,This paper describes the use of an iterative and agile process model in a software engineering undergraduate course. The proposed model serves both as an educational technique (for teachers) and as a subject of learning (for students). Details are given of the procedures and methods used and related management aspects are also addressed. The results obtained are discussed and compared with a previous experience using a weighted process model. This particular approach has been found a valuable and high satisfactory experience for both students and teachers,"Education,
Software engineering,
Computer science,
Project management,
Educational products,
Software design,
Application software,
Artificial intelligence,
Learning,
Software tools"
State feedback stabilization and majorizing achievement of min-max-plus systems,"The state feedback stabilization of min-max-plus systems is investigated in this note. The concepts of digraph description, (r,s)-reachability, max-plus projection direct product, etc., are introduced to deal with the stability. The sufficient condition and the necessary condition for the state feedback stabilization are presented respectively and the desired eigenvalue area is determined. Moreover, the state feedback is constructed for majorizing achievement of the stabilization and the corresponding algorithm is derived.","State feedback,
Stability,
Eigenvalues and eigenfunctions,
Control systems,
Discrete event systems,
Sufficient conditions,
Computer science,
Operations research,
Computer aided manufacturing,
Virtual manufacturing"
GIMI: generic infrastructure for medical informatics,"Breakthroughs in medical informatics have yielded a wealth of data across all aspects of patient care. One of the fundamental goals of e-Science should be facilitate the appropriate use of such data to improve patient care: both in the short-term and the long-term. Developments in Grid technology have brought about the promise of such data being used to, for example, support research into evidence-based patient centred care and facilitate on-demand decision support for practitioners. For such health grid dreams to become reality, however, it will first be necessary to tackle key technical challenges, such as those of interoperability and security. The GIMI (Generic Infrastructure for Medical Informatics) proposes to tackle exactly these generic problems within the context of healthcare in the United Kingdom.",
The use of e-learning towards new learning paradigm: case study student centered e-learning environment at Faculty of Computer Science - University of Indonesia,"Faculty of Computer Science (Fasilkom), University of Indonesia (UI) develops Student Centered E-Learning Environment (SCELE) for Graduate Program in Information Technology as part of distance learning system, which is developed using enterprise resources planning approach. SCELE is also well known as learning management system (LMS). Beside SCELE, Fasilkom UI also develops contents that are conformant with SCELE, and also Digital Library and Online Academic Registration System to support learning process. This paper elaborates the study of SCELE, content development, and the result of system evaluation.","Electronic learning,
Computer aided software engineering,
Distance learning,
Computer science,
Computer aided instruction,
Educational institutions,
Educational technology,
Isolation technology,
Internet,
Information technology"
Authorization and account management in the Open Science Grid,"An attribute-based authorization infrastructure developed for the Open Science Grid is presented. The infrastructure integrates existing identity-mapping and group-membership service using concepts prototyped in the PRIMA system. Authorization scenarios for requests to compute and data resources are detailed. A new SAML obligated authorization decision statement is introduced that attaches an XACML obligation to the authorization decision. The use of obligations enables site-centralized, service-independent policy management. Authorization decisions are enforced via a Workspace Service that creates constrained execution environments configured in accordance with the obligations and other attribute-based information. Finally, an experimental PRIMA authorization service that extends and simplifies the infrastructure is described.","Authorization,
Resource management,
Computer science,
Prototypes,
Grid computing,
Large-scale systems,
Collaborative work,
Information security,
National security"
Reliability prediction and assessment of fielded software based on multiple change-point models,"In this paper, we investigate some techniques for reliability prediction and assessment of fielded software. We first review how several existing software reliability growth models based on non-homogeneous Poisson processes (NHPPs) can be readily derived based on a unified theory for NHPP models. Furthermore, based on the unified theory, we can incorporate the concept of multiple change-points into software reliability modeling. Some models are proposed and discussed under both ideal and imperfect debugging conditions. A numerical example by using real software failure data is presented in detail and the result shows that the proposed models can provide fairly good capability to predict software operational reliability.","Predictive models,
Software reliability,
Testing,
Arithmetic,
Debugging,
Solid modeling,
Software measurement,
Software quality,
Fault detection,
Computer science"
An efficient and secure peer-to-peer overlay network,"Most of structured P2P overlays exploit distributed Hash table (DHT) to achieve an administration-free, fault tolerant overlay network and guarantee to deliver a message to the destination within O(log N) hops. While elegant from a theoretical perspective, those systems face difficulties under an open environment. Not only frequently joining and leaving of nodes generate enormous maintenance overhead, but also various behaviors and resources of peers compromise overlay performance and security. Instead of building P2P overlay from the theoretical view, this paper builds the overlay network from the perspective of the physical network. By combining different network topology-aware techniques, a distinctive overlay network that closely matches the Internet topology is constructed. The P2P system based on this structure is not only highly efficient for routing, but also keeps maintenance overhead very low even under highly dynamic environment. Moreover, the system provides a new aspect to solve many security issues","Peer to peer computing,
Routing,
Intrusion detection,
Computer networks,
Delay,
Telecommunication traffic,
Distributed computing,
Computer science,
Fault tolerance,
Buildings"
Coverage and energy tradeoff in density control on sensor networks,"In dense wireless sensor networks, density control is an important technique for prolonging network's lifetime while providing sufficient sensing coverage. In this paper, we develop three new density control protocols by considering the tradeoff between energy usage and coverage. The first one, non-overlapping density control, aims at maximizing coverage while avoiding the overlap of sensing areas of active sensors. Under the ideal case, a set of optimality conditions are derived to select sensors such that the sensing space is covered systematically to maximize the usage of each sensors and the coverage gap is minimized. Based on the optimality conditions, we develop a distributed protocol that can be efficiently implemented in large sensor networks. Next, we present a protocol called non-overlapping density control based on distances that does not require location information of the nodes. This protocol is more flexible and easier to implement than existing location-based methods. Finally, we present a new range-adjustable protocol called non-overlapping density control for adjustable sensing ranges. It allows heterogenous sensing ranges for different sensors to save energy consumption. Extensive simulation shows promising results of the new protocols.",
Online resource matching for heterogeneous grid environments,"In this paper, we first present a linear programming based approach for modeling and solving the resource matching problem in grid environments with heterogeneous resources. The resource matching problem described takes into account resource sharing, job priorities, dependencies on multiple resource types, and resource specific policies. We then propose Web service style architecture for online matching of independent jobs with resources in a grid environment and describe a prototype implementation. Our preliminary performance results indicate that the linear programming based approach for resource matching is efficient in speed and accuracy and can keep up with high job arrival rates of an important criterion for online resource matching systems. Also, the Web service style architecture makes the system scalable and extendable. It can also be integrated with other existing grid services in a straightforward manner.","Resource management,
Linear programming,
Web services,
Service oriented architecture,
Prototypes,
Environmental management,
Load management,
Throughput,
Computer science,
Centralized control"
Odd-order probe correction technique for spherical near-field antenna measurements,"In this paper, an odd-order probe for spherical near-field antenna measurements is defined. A probe correction technique for odd-order probes is then formulated and tested by computer simulations. The probe correction for odd-order probes is important, since a wide range of realistic antennas belongs to this class. To the authors' knowledge, the proposed technique is the first practical high-order probe correction technique that has been formulated in detail, has been tested, and has been shown to work.","Probes,
Antenna measurements,
Geophysical measurements,
Rectangular waveguides,
Time measurement,
Frequency measurement,
Wave functions"
Software modernization decision criteria: an empirical study,"Decisions regarding software evolution strategies such as modernizations are economically important. We present results of our empirical study of the views of decision makers. We have asked their views of the relative importance of 49 software modernization decision criteria. We have gathered data from Finnish software industry. There were 26 experts from 8 organizations involved. They were mainly upper or middle level managers. Our study shows that there is a large set of criteria which should be taken into account, and that those studied by us provide a good coverage of the relevant ones. We list the top-20 criteria. We also performed a cluster analysis which produced two groups of subjects. Views of the decision makers in software user and software supplier organizations were different. We suggest that decision maker's work process could be enhanced by taking into account the received lessons.","Software maintenance,
Decision making,
Environmental economics,
Information technology,
Computer science,
Computer industry,
Costs,
Investments,
Logic,
Information systems"
A Byzantine resilient multi-path key establishment scheme and its robustness analysis for sensor networks,"Sensor networks are composed of a large number of low power sensor devices. For secure communication among sensors, secret keys must be established between them. Random key predistribution and pairwise key establishment schemes have been proposed for key management in large-scale sensor networks. In these schemes, after being deployed, sensors set up pairwise keys via preinstalled keys. The key establishment schemes are vulnerable to Byzantine attacks, i.e., packet dropping or altering. To counter these attacks, we propose a Byzantine resilient multi-path key establishment scheme that uses the Reed-Solomon error-correct coding scheme to improve resilience to Byzantine attacks. Our proposed scheme can tolerate at most t faulty key paths, where t - (n - k)/2 when (n,k) Reed-Solomon error-correct coding scheme is used. In addition, by using the Reed-Solomon coding scheme, sensors can identify the faulty paths with minimal communication overhead.","Robustness,
Resilience,
Reed-Solomon codes,
Protocols,
Cities and towns,
Large-scale systems,
Computer science,
Counting circuits,
Fault diagnosis,
Proposals"
"Magellan: performance-based, cooperative multicast","Among the proposed overlay multicast protocols, tree-based systems have proven to be highly scalable and efficient in terms of physical link stress and end-to-end latency. Conventional tree-based protocols, however, distribute the forwarding load unevenly among the participating peers. An effective approach for addressing this problem is to stripe the multicast content across a forest of disjoint trees, evenly sharing the forwarding responsibility among participants. DHTs seem to be naturally well suited for the task, as they are able to leverage the inherent properties of their routing model in building such a forest. In heterogeneous environments, though, DHT-based schemes for tree (and forest) construction may yield deep, unbalanced structures with potentially large delivery latencies. This paper introduces Magellan, a new overlay multicast protocol we have built to explore the tradeoff between fairness and performance in these environments. Magellan builds a data-distribution forest out of multiple performance-centric, balanced trees. It assigns every peer in the system a primary tree with priority over the peer's resources. The peers' spare resources are then made available to secondary trees. In this manner, Magellan achieves fairness, ensuring that every participating peer contributes resources to the system. By employing a balanced distribution tree with O(lg N)-bounded, end-to-end hop-distance, Magellan also provides high delivery ratio with comparable low latency. Preliminary simulation results show the advantage of this approach.",
Design and performance of configurable endsystem scheduling mechanisms,"This paper describes a scheduling abstraction, called group scheduling, that emphasizes fine grain configurability of system scheduling semantics. The group scheduling approach described and evaluated in this paper provides an extremely flexible framework within which a wide range of scheduling semantics can be expressed, including familiar priority and deadline based algorithms. The paper describes both OS and middleware based implementations of the framework, and shows through evaluation that they can produce the same behavior for a nontrivial set of application computations. We also show that the framework can support application-specific scheduling constraints such as progress, to improve performance of applications whose scheduling semantics do not match those of traditional scheduling algorithms.",
Topological structures of 3D tensor fields,"Tensor topology is useful in providing a simplified and yet detailed representation of a tensor field. Recently the field of 3D tensor topology is advanced by the discovery that degenerate tensors usually form lines in their most basic configurations. These lines form the backbone for further topological analysis. A number of ways for extracting and tracing the degenerate tensor lines have also been proposed. In this paper, we complete the previous work by studying the behavior and extracting the separating surfaces emanating from these degenerate lines. First, we show that analysis of eigenvectors around a 3D degenerate tensor can be reduced to 2D. That is, in most instances, the 3D separating surfaces are just the trajectory of the individual 2D separatrices which includes trisectors and wedges. But the proof is by no means trivial since it is closely related to perturbation theory around a pair of singular slate. Such analysis naturally breaks down at the tangential points where the degenerate lines pass through the plane spanned by the eigenvectors associated with the repeated eigenvalues. Second, we show that the separatrices along a degenerate line may switch types (e.g. trisectors to wedges) exactly at the points where the eigenplane is tangential to the degenerate curve. This property leads to interesting and yet complicated configuration of surfaces around such transition points. Finally, we apply the technique to several common data sets to verify its correctness.",
A Breast Cancer Diagnosis System: A Combined Approach Using Rough Sets and Probabilistic Neural Networks,"In this paper, we present a medical decision support system based on a hybrid approach utilizing rough sets and a probabilistic neural network. We utilized the ability of rough sets to perform dimensionality reduction to eliminate redundant attributes from a biomedical dataset. We then utilized a probabilistic neural network to perform supervised classification. Our results indicate that rough sets were able to reduce the number of attributes in the dataset by 67% without sacrificing classification accuracy. Our classification accuracy results yielded results on the order of 93%","Breast cancer,
Rough sets,
Neural networks,
Computer science,
Medical diagnostic imaging,
Decision support systems,
Mathematics,
IEEE members,
Testing,
Europe"
Face recognition in the presence of multiple illumination sources,"Most existing face recognition algorithms work well for controlled images but are quite susceptible to changes in illumination and pose. This has led to the rise of analysis-by-synthesis approaches due to their inherent potential to handle these external factors. Though these approaches work quite well, most of them assume that the face is illuminated by a single light source which is usually not true in realistic conditions. In this paper, we propose an algorithm to recognize faces illuminated by arbitrarily placed, multiple light sources. The algorithm does not need to know the number of light sources and works extremely well even while recognizing faces illuminated by different number of light sources. Results using this algorithm are reported on multiple-illumination datasets generated from PIE by T. Sim, et al. (2003) and Yale Face Database B by W. Zhao, et al. (2003). We also highlight the importance of the hard non-linearity in the Lambert's law which is often ignored, probably to linearize the estimation process","Face recognition,
Lighting,
Shape,
Light sources,
Educational institutions,
Reflectivity,
Computer science,
Image databases,
Focusing,
Image processing"
Statistical models for assessing the individuality of fingerprints,"The problem of fingerprint individuality is as follows: Given a sample fingerprint, what is the probability of finding a sufficiently similar fingerprint in a target population? In this paper, we develop a family of finite mixture models to represent the distribution of minutiae locations and directions in fingerprint images, including clustering tendencies and dependencies in different regions of the fingerprint domain. These models are shown to be a better fit to the observed distribution of minutiae features and give better assessments of fingerprint individuality compared to previous models. Estimates of fingerprint individuality are obtained using the probability of a random correspondence (PRC). For the ""12-point match"" criteria, a PRC of 9.2/spl times/10/sup -5/ was obtained for the FVC2002 DB1 database when the number of query and template minutiae features both equal 26. The corresponding PRC based on the MSU VERIDICOM database for the same matching criteria is 6.6/spl times/10/sup -4/.","Fingerprint recognition,
Spatial databases,
Probability,
Partial response channels,
Testing,
Image matching,
Image databases,
Statistics,
Computer science,
Current measurement"
Online time-constrained scheduling in linear networks,"We consider the problem of scheduling a sequence of packets over a linear network, where every packet has a source and a target, as well as a release time and a deadline by which it must arrive at its target. The model we consider is bufferless, where packets are not allowed to be buffered in nodes along their paths other than at their source. This model applies to optical networks where opto-electronic conversion is costly, and packets mostly travel through bufferless hops. The offline version of this problem was previously studied in M. Adler et al. (2002). In this paper we study the online version of the problem, where we are required to schedule the packets without knowledge of future packet arrivals. We use competitive analysis to evaluate the performance of our algorithms. We present the first deterministic online algorithms for several versions of the problem. For the problem of throughput maximization, where all packets have uniform weights, we give an algorithm with a logarithmic competitive ratio, and present some lower bounds. For other weight functions, we show algorithms that achieve optimal competitive ratios. We complete our study with several experimental results.",
Comparative Studies on Feature Extraction Methods for Multispectral Remote Sensing Image Classification,"Feature extraction of multispectral remote sensing image is an important task before classifying the image. When land areas are clustered into groups of similar land cover, one of the most important things is to extract the key features of a given image. Usually multispectral remote sensing images have many bands, and there may have been much redundancy information and it becomes difficult to extract the key features of the image. Therefore, it is necessary to study methods regarding how to extract the main features of the image effectively. In this paper, five methods are comparatively studied to reduce the multi-bands into lower dimensions in order to extract the most available features. These methods include the Euclid distance measurement (EDM), the discrete measurement criteria function (DMCF), the minimum differentiated entropy (MDE), the probability distance criterion (PDC), and the principle component analysis (PCA) method. The advantage and disadvantage of each method are evaluated by the classification results","Feature extraction,
Remote sensing,
Image classification,
Data mining,
Image analysis,
Computer science,
Distance measurement,
Entropy,
Model driven engineering,
Principal component analysis"
BMI: a network abstraction layer for parallel I/O,"As high-performance computing increases in popularity and performance, the demand for similarly capable input and output systems rises. Parallel I/O takes advantage of many data server machines to provide linearly scaling performance to parallel applications that access storage over the system area network. The demands placed on the network by a parallel storage system are considerably different than those imposed by message-passing algorithms or data-center operations; and, there are many popular and varied networks in use in modern parallel machines. These considerations lead us to develop a network abstraction layer for parallel I/O which is efficient and thread-safe, provides operations specifically required for I/O processing, and supports multiple networks. The buffered message interface (BMI) has low processor overhead, minimal impact on latency, and can improve throughput for parallel file system workloads by as much as 40% compared to other more generic network abstractions.",
Iteration aware prefetching for remote data access,"Although processing speed, storage capacity and network bandwidth are steadily increasing, network latency remains a bottleneck for scientists accessing large remote data sets. This problem is most acute with n-dimensional data. Grid researchers have only recently begun to develop tools for efficient remote access to n-dimensional data sets. Within the context of the Granite Scientific Database system, we show that latency penalties can be dramatically reduced using explicit knowledge of a user's access pattern represented as an iterator. The iterator not only performs an n-dimensional iteration for the user, but also communicates the access pattern to Granite so that a prefetching cache can be constructed that is tuned to the user's access pattern. We experimentally evaluate a scenario for incorporating Granite's prefetching mechanism into the grid, demonstrating extraordinary performance gains. In light of these results, we describe planned additions to existing grid services to allow selection of datasets according to the user access pattern","Prefetching,
Delay,
Performance gain,
Costs,
Computer networks,
Information science,
Bandwidth,
Context,
Database systems,
High performance computing"
"Does the ""Refactor to Understand"" reverse engineering pattern improve program comprehension?","Program comprehension is a fundamental requirement for all but the most trivial maintenance activities. Previous research has demonstrated key principles for improving comprehension. Among others, these consist of the introduction of beacons as indexes into knowledge, and the chunking of low-level structures into higher-level abstractions. These principles are naturally reflected in the reverse engineering pattern Refactor to Understand, which uses incremental renaming and extracting of program elements as the means to decipher cryptic code. In this paper, we discuss a controlled experiment to explore differences in program comprehension between the application of Refactor to Understand and the traditional Read to Understand pattern. Our results support added value of Refactor to Understand regarding specific aspects of program comprehension and specific types of source code. These findings illustrate the need for further experiments to provide clear guidelines on the application of refactorings for improving program comprehension.","Reverse engineering,
Management information systems,
Guidelines,
Pattern analysis,
Computer science,
Time factors,
Delay,
Software maintenance,
Software testing,
System testing"
Strong mobile authentication,"Our main contribution is a protocol that provides strong mobile authentication with non-repudiation using SMS messages. For ensuring these properties, governmentally controlled PKI, and SIM cards with electronic identity application are used. Moreover, our protocol provides confidentiality and integrity of transferred content. An application that implements this protocol was developed and tested in a partly simulated environment. Furthermore, we developed a protocol for mobile payment for vending machines. In comparison to current systems, this protocol contains several enhancements in security, usability and cost both from the client as well as from the service provider point of view.","Authentication,
Protocols,
Mobile handsets,
Cryptography,
Business,
Public key,
Computer science,
Mobile computing,
Application software,
Testing"
A new weighted text filtering method,"In order to describe the user's profile exactly and provide what they need in information filtering, a new method to weight the user's characterized demand and filter for him is put forward. In the paper, at first we define the terms' weight of the user's profile according to the order of the proposed sequence, then calculate the similarity between the user's profile and the documents using the semantic dictionaries of Synonym Dictionary and HowNet, and at last submit the results to the user sorted in descending order of the similarity. The experiments show that it can improve the precision of information filtering.","Information filtering,
Information filters,
Dictionaries,
Educational technology,
Modems,
Information processing,
Computer science,
Computer science education,
Communications technology,
Animation"
Mining Multi-Level Associations with Fuzzy Hierarchies,"In this paper we investigate application of fuzzy concept hierarchies to mining multi-level knowledge from large datasets via a well-known attribute-oriented induction approach (Han and Kamber, 2000). We analyze in detail the original process of fuzzy hierarchical induction and extend it with two new characteristics which improve applicability of the original approach to scientific data mining. These are a consistency of our fuzzy induction model, and an approximate drilling-down technique allowing a user to retrieve estimated explanations of the generated abstract concept. An application to discovery of multi-level association rules from environmental data stored in a toxic release inventory is presented","Data mining,
Data analysis,
Transaction databases,
Information analysis,
Computer science,
Application software,
Induction generators,
Association rules,
Global Positioning System,
NASA"
A model-based evolutionary algorithm for bi-objective optimization,"The Pareto optimal solutions to a multi-objective optimization problem often distribute very regularly in both the decision space and the objective space. Most existing evolutionary algorithms do not explicitly take advantage of such a regularity. This paper proposed a model-based evolutionary algorithm (M-MOEA) for bi-objective optimization problems. Inspired by the ideas from estimation of distribution algorithms, M-MOEA uses a probability model to capture the regularity of the distribution of the Pareto optimal solutions. The local principal component analysis (local PCA) and the least-squares method are employed for building the model. New solutions are sampled from the model thus built. At alternate generations, M-MOEA uses crossover and mutation to produce new solutions. The selection in M-MOEA is the same as in non-dominated sorting genetic algorithm-II (NSGA-II). Therefore, MOEA can be regarded as a combination of EDA and NSGA-II. The preliminary experimental results show that M-MOEA performs better than NSGA-II.","Evolutionary computation,
Electronic design automation and methodology,
Pareto optimization,
Genetic mutations,
Approximation algorithms,
Data mining,
Probability,
Computer science,
Europe,
Research and development"
The CSI multimedia architecture,"An instruction set extension designed to accelerate multimedia applications is presented and evaluated. In the proposed complex streamed instruction (CSI) set, a single instruction can process vector data streams of arbitrary length and stride and combines complex memory accesses (with implicit prefetching), program control for vector sectioning, and complex computations on multiple data in a single operation. In this way, CSI eliminates overhead instructions (such as instructions for data sectioning, alignment, reorganization, and packing/unpacking) often needed in applications utilizing MMX-like extensions and accelerates key multimedia kernels. Simulation results demonstrate that a superscalar processor extended with CSI outperforms the same processor enhanced with Sun's VIS extension by a factor of up to 7.77 on key multimedia kernels and by up to 35% on full applications.","Streaming media,
Kernel,
Parallel processing,
Acceleration,
Laboratories,
Computer science,
Instruction sets,
Application software,
Mathematics,
Prefetching"
Separable and Anonymous Identity-Based Key Issuing,"In identity-based (ID-based) cryptosystems, a local registration authority (LRA) is responsible for authentication of users while the key generation center (KGC) is responsible for computing and sending the private keys to users and therefore, a secure channel is required. For privacy-oriented applications, it is important to keep in secret whether the private key corresponding to a certain identity has been requested. All of the existing ID-based key issuing schemes have not addressed this anonymity issue. Besides, the separation of duties of LRA and KGC has not been discussed as well. We propose a novel separable and anonymous ID-based key issuing scheme without secure channel. Our protocol supports the separation of duties between LRA and KGC. The private key computed by the KGC can be sent to the user in an encrypted form such that only the legitimate key requester authenticated by LRA can decrypt it. and any eavesdropper cannot know the identity corresponding to the secret key",
Music genre classification with taxonomy,"Automatic music genre classification is a fundamental component of music information retrieval systems and has been gaining importance and enjoying a growing amount of attention with the emergence of digital music on the Internet. Although considerable research has been conducted in automatic music genre classification, little has been done on hierarchical classification with taxonomies. The underlying hierarchical taxonomy identifies the relationships of dependence between different genres and provides valuable sources of information for genre classification. This paper investigates the use of taxonomy for music genre classification. Our empirical experiments on two datasets show that using taxonomy improves the classification performance. We also propose an approach for automatically generating genre taxonomies based on the confusion matrix via linear discriminant projection. Our work also provides some insights for future research.","Multiple signal classification,
Taxonomy,
Music information retrieval,
Computer science,
Internet,
Feature extraction,
Data mining,
Information resources,
Information processing,
Signal analysis"
Peer-to-peer system-based active worm attacks: modeling and analysis,"Recent active worm propagation events show that active worms can spread in an automated fashion and flood the Internet in a very short period of time. Due to the recent surge of peer-to-peer (P2P) systems with large numbers of users, P2P systems can be a potential vehicle for the active worms to achieve fast worm propagation in the Internet. In this paper, we address the issue of the impacts of active worm propagation on top of P2P systems. In particular: (1) we define a P2P system based active worm attack model and study two attack strategies (an off-line and on-line strategy) under the defined model; (2) we develop an analytical approach to analyze the propagation of active worms under the defined attack models and conduct an extensive study to the impacts of P2P system parameters, such as size, topology degree, and the structured/unstructured properties on active worm propagation. Based on numerical results, we observe that a P2P-based attack can significantly worsen attack effects (improve attack performance), and we observe that the speed of worm propagation is very sensitive to P2P system parameters. We believe that our work can provide important guidelines in design and control of P2P systems as well as overall active worm defense.","Peer to peer computing,
Computer worms,
Internet,
Surges,
Vehicles,
Routing,
Computer science,
Control systems,
Computer crime,
IP networks"
Repeated Measures GLMM Estimation of Subject-Related and False Positive Threshold Effects on Human Face Verification Performance,"Subject covariate data were collected on 1, 072 pairs of FERET images for analysis in a human face verification experiment. The subject data included information about facial hair, bangs, eyes, gender, and age. The verification experiment was replicated at seven different false alarm rates ranging from 1/10, 000 to 1/100. A generalized linear mixed model (GLMM) was fit to the binary outcomes indicating correct verification. Statistically significant main effects for bangs, eyes, gender, and age were found. The effect of the log false positive rate on verification success was found to interact significantly with bangs, gender, and age. These results have important implications for future evaluation of biometrics, and the GLMM methodology used here is shown to be effective and informative for this sort of data.","Humans,
Face recognition,
Pattern recognition,
Performance analysis,
Eyes,
Statistics,
Virtual reality,
Statistical analysis,
Computer science,
NIST"
Experiences with a requirements-based programming approach to the development of a NASA autonomous ground control system,"Requirements-to-design-to-code (R2D2C) is an approach to the engineering of computer-based systems that embodies the idea of requirements-based programming in system development. It goes further, however, in that the approach offers not only an underlying formalism, but full formal development from requirements capture through to the automatic generation of provably-correct code. As such, the approach has direct application to the development of systems requiring autonomic properties. We describe a prototype tool to support the method, and illustrate its applicability to the development of LOGOS, a NASA autonomous ground control system, which exhibits autonomic behavior. Finally, we briefly discuss other areas where the approach and prototype tool are being considered for application.",
ADDIE instruction design and cognitive apprenticeship for project-based software engineering education in MIS,"The completion of SWEBOK and SE2004 marks a key milestone for the software engineering education community. It not only initiates issues of what to do next in the long run, but introduces some pragmatic issues such as customization and transition issues in the short term. Meanwhile, there is a lack of theoretical foundation with respect to instruction design and learning theory, which is also a shortage from an interdisciplinary perspective. Based upon experiences and lessons learned, we plan a course of introduction to software engineering based on the ADDIE instruction design model and cognitive apprenticeship enhanced with a situated business case and peer apprenticeship, along within a disciplined software process. We believe it can help combat both customization and transition barriers in the department of management information systems, not only by well-founded pedagogical design and implementation, but by being extensible to capstone projects.","Software engineering,
Management information systems,
Psychology,
Systems engineering education,
Taxonomy,
Computer science education,
Educational institutions,
Computer science,
Programming,
Cultural differences"
Cooperating services for data-driven computational experimentation,"The Linked Environments for Atmospheric Discovery (LEAD) project seeks to provide on-demand weather forecasting. A triad of cooperating services provides the core functionality needed to execute experiments and manage the data. In this article, we focus on three MyLEAD services - the metadata catalog service, notification service, and workflow service - that together form the core services for managing complex experimental meteorological investigations and managing the data products used in and generated during the computational experimentation. We show how the services work together on the user's behalf, easing the technological burden on the scientists and freeing them to focus on more of the science that compels them.","Weather forecasting,
Predictive models,
Atmospheric modeling,
Meteorology,
Humans,
Demand forecasting,
Technology management,
Portals,
Concurrent computing,
Distributed computing"
Mapping structures for flash memories: techniques and open problems,"Flash memory is a type of electrically erasable programmable read-only memory (EEPROM). Because flash memories are nonvolatile and relatively dense, they are now used to store files and other persistent objects in handheld computers, mobile phones, digital cameras, portable music players, and many other computer systems in which magnetic disks are inappropriate. Flash, like earlier EEPROM devices, suffers from two limitations. First, bits can only be cleared by erasing a large block of memory. Second, each block can only sustain a limited number of erasures, after which it can no longer reliably store data. Due to these limitations, sophisticated data structures and algorithms are required to effectively use flash memories. These algorithms and data structures support efficient not-in-place updates of data, reduce the number of erasures, and level the wear of the blocks in the device. This survey presents these algorithms and data structures as well as open theoretical problems that arise in this area.","Flash memory,
Data structures,
EPROM,
Handheld computers,
Random access memory,
Computer science,
PROM,
Nonvolatile memory,
Mobile handsets,
Digital cameras"
Tracking human breath in infrared imaging,"In this paper, we propose a novel tracker to capture the human breathing signal through an infrared imaging method. Human facial physiology information is used to select salient thermal features on the human face as good features to track. The major component of the tracker is mean shift localization (MSL)-based particle filtering. A special measurement model is designed for particle filtering so that the tracker can handle significant head movement and object occlusion. The breathing signal is achieved based on tracking results. The experiments show that the tracker is robust and stable and the recovered breathing signal is clear enough for breathing functionality computation.","Humans,
Infrared imaging,
Temperature,
Nose,
Face,
Particle tracking,
Filtering,
Facial features,
Computer science,
Particle measurements"
On the Use of Specification-Based Assertions as Test Oracles,"The ""oracle problem' is a well-known challenge for software testing. Without some means of automatically computing the correct answer for test cases, testers must instead compute the results by hand, or use a previous version of the software. In this paper, we investigate the feasibility of revealing software faults by augmenting the code with complete, specification-based assertions. Our evaluation method is to (1) develop a formal specification, (2) translate this specification into assertions, (3) inject or identify existing faults, and (4) for each version of the assertion-enhanced system containing a fault, execute it using a set of test inputs and check for assertion violations. Our goal is to determine whether specification-based assertions are a viable method of revealing faults, and to begin to assess the extent to which their cost-effectiveness can be improved. Our evaluation is based on two case studies involving real-world software systems. Our results indicate that specification-based assertions can effectively reveal faults, as long as they adversely affect the program state. We describe techniques that we used for translating high-level specifications into code-level assertions. We also discuss the costs associated with the approach, and potential techniques for reducing these costs","Automatic testing,
Software testing,
Costs,
Fault diagnosis,
System testing,
Computer science,
Educational institutions,
Formal specifications,
Software systems,
Impedance matching"
Data dependence based testability transformation in automated test generation,"Source-code based test data generation is a process of finding program input on which a selected element, e.g., a target statement, is executed. There exist many test generation methods that automatically find a solution to the test generation problem. The existing methods work well for many programs. However, they may fail or are inefficient for programs with complex logic and intricate relationships between program elements. In this paper we present a testability transformation that transforms programs so that the chances of finding a solution are increased when the existing methods fail using only the original program. In our approach data dependence analysis is used to identify statements in the program that affect computation of the fitness function associated with the target statement. The transformed program contains only these statements, and it is used to explore different ways the fitness may be computed. These explorations are inexpensive when using the transformed program as compared to explorations using the original program. As a result, executions in the transformed program that lead to the evaluation of the fitness function to the target value are identified. The identified executions are then used to guide the search in the original program to find an input on which the target statement is executed. In this paper, the approach is evaluated using a case study which demonstrates the potential for this testability transformation to improve the efficacy of the test generation","Automatic testing,
Flow graphs,
Computer science,
Logic,
Data analysis,
Process control,
Educational institutions,
Genetics,
Evolutionary computation,
Information resources"
Accelerated penalized weighted least-squares and maximum likelihood algorithms for reconstructing transmission images from PET transmission data,"We present penalized weighted least-squares (PWLS) and penalized maximum-likelihood (PML) methods for reconstructing transmission images from positron emission tomography transmission data. First, we view the problem of minimizing the weighted least-squares (WLS) and maximum likelihood objective functions as a sequence of nonnegative least-squares minimization problems. This viewpoint follows from using certain quadratic functions as surrogate functions for the WLS and maximum likelihood objective functions. Second, we construct surrogate functions for a class of penalty functions that yield closed form expressions for the iterates of the PWLS and PML algorithms. Due to the slow convergence of the PWLS and PML algorithms, accelerated versions of them are developed that are theoretically guaranteed to monotonically decrease their respective objective functions. In experiments using real phantom data, the PML images produced the most accurate attenuation correction factors. On the other hand, the PWLS images produced images with the highest levels of contrast for low-count data.","Acceleration,
Image reconstruction,
Positron emission tomography,
Attenuation,
Detectors,
Random variables,
Delay estimation,
Convergence,
Imaging phantoms,
Protocols"
A Universal Service Description Language,"To fully utilize Web-services, users and applications should be able to discover, deploy, compose and synthesize services automatically. This automation can take place only if a formal semantic description of the Web-services is available. In this paper we present a markup language called USDL (Universal Service Description Language), for formally describing the semantics of Web-services.","Web services,
OWL,
Automation,
Ontologies,
Computer science,
Application software,
Markup languages,
Internet,
Automatic control,
Communication system control"
A comparative study on model selection and multiple model fusion,"There exist quite a few criteria for penalty-based model selection. Although they have various justifications for large sample problems, their performance under small or moderate sample size is unclear which hinders the development of model combination methods using the appropriate penalty term. In this paper, we assess the performance of seven model selection criteria based on linear regression models with unknown noise variance. We set the true data generation mechanism to be within the model set as well as outside the model set. In the latter case, soft model selection through multiple model fusion is proposed and its difference from Bayesian model averaging is highlighted. The penalty term used in each model selection criterion provides a natural link to estimate the model probability without assuming any prior knowledge of the unknown parameter. An important question is whether the estimated model probabilities are consistent when multiple models are fused for prediction or interpolation. We argue that strong consistency only holds under large sample regime while soft model selection can still be better than choosing a single model with small sample size. Our numerical results using different model selection criteria for polynomial fitting indicate that the conditional model estimator (CME) has the best performance in selecting the correct model order and fusing multiple models for prediction and interpolation. The minimum description length (MDL) based criteria are next to CME and outperform Bayesian information criterion (BIC) and Akaike information criterion (AIC) significantly.","Predictive models,
Linear regression,
Bayesian methods,
Interpolation,
State estimation,
Computer science,
Fusion power generation,
Polynomials,
Time series analysis,
Parameter estimation"
Supporting quality of privacy (QoP) in pervasive computing,"Privacy might be the greatest barrier to the long-term success of pervasive or ubiquitous computing (ubicomp). The invisibility of embedded computing devices has made it easier to collect and use information about individuals without their knowledge. Sensitive and private information might be stored for long periods of time and appear anywhere at anytime. Thus, a cost, in the form of privacy, might need to be paid to benefit from ubicomp. In this paper, we introduce the concept of quality of privacy (QoP) which allows balancing the trade-off between the amount of privacy a user is willing to concede and the value of the services that can be provided by a ubicomp application, in a similar way as that of quality of service (QoS). We propose an agent-based architecture that adapts the behavior of the ubicomp application to the users' context, in order to satisfy the level of QoP, that both, the application and the user have agreed upon. To illustrate this architecture, we extend a handheld-based mobile hospital information system in order to preserve privacy in ubicomp hospitals environment.","Privacy,
Pervasive computing,
Hospitals,
Physics computing,
Embedded computing,
Quality of service,
Space technology,
Ubiquitous computing,
Costs,
Computer architecture"
On the critical total power for asymptotic k-connectivity in wireless networks,"In this paper, we investigate the minimum total power (termed as critical total power) required to ensure asymptotic k-connectivity in heterogeneous wireless networks where nodes may transmit using different levels of power. We show that under the assumption that wireless nodes form a homogeneous Poisson point process with density /spl lambda/ on a unit square region [0, 1]/sup 2/ and the Toroidal model [M.D. Penrose, 1997], the critical total power required for maintaining k-connectivity is /spl theta/((/spl Gamma/(e/2+k))/((k-1)l)/spl lambda//sup 1-e/2/) with probability approaching one as /spl lambda/ goes to infinity, where e is the path loss exponent. Compared with the results that all nodes use a common critical transmission power for maintaining k-connectivity [M.D. Penrose, 1999], [P.-J. Wan and C. Yi, 2004], we show that the critical total power can be reduced by an order of (log /spl lambda/)e/2 by allowing nodes to optimally choose different levels of transmission power. This result is not subject to any specific power/topology control algorithm, but rather a fundamental property in wireless networks.","Intelligent networks,
Wireless networks,
Energy consumption,
Computer science,
Region 10,
H infinity control,
Propagation losses,
Network topology,
Centralized control,
Stochastic processes"
Mobile RFID technology for improving m-commerce,"The RFID technology is called a revolution to our life in new century. This paper introduces how to improve m-commerce utilizing mobile RFID technology. Complete and validated information could be gathered for the later m-commerce wherever and whenever you see nice products. On the way of transportation, the exception events could be detected in real-time and used to notice deliver people and control center. When receiving, the customer could check the messages in the RFID tag along with the products and verify quality of the products. The mobile RFID middleware and monitor device are also designed and implemented to support the practice application. Mobile RFID technology makes convenient to the information gathering and enhances customer's confidence of m-commerce. It also improves the retailer's management level, competition ability and commerce profit",
Facilitators and inhibitors of end-user development by teachers in a school,"This paper describes the perceptions and attitudes of teachers who are end-user developers. A semi-structured interview of 22 teachers was carried out. It revealed motivational, situational, knowledge, and tool factors that affect their programming efforts. Five of the survey participants also carried out several program maintenance tasks in order to identify additional challenges in performing end-user programming tasks. The contribution of the study is an understanding of the facilitators and inhibitors of end-user development in a particular professional setting.","Inhibitors,
Programming profession,
Educational institutions,
Computational modeling,
Education,
Visual BASIC,
Books,
Writing,
Debugging,
Facial animation"
Unconstrained handwritten character recognition using metaclasses of characters,"In this paper we tackle the problem of unconstrained handwritten character recognition using different classification strategies. For such an aim, four multilayer perceptron classifiers (MLP) were built and used into three different classification strategies: combination of two 26-class classifiers; 26-metaclass classifier; 52-class classifier. Experimental results on the NIST SD19 database have shown that the recognition rate achieved by the metaclass classifier (87.8%) outperforms the other approaches (82.9% and 86.3%).","Character recognition,
Handwriting recognition,
Writing,
NIST,
Spatial databases,
Feature extraction,
Image databases,
Shape,
Histograms,
Computer science"
Dynamic phase analysis for cycle-close trace generation,"For embedded system development, several companies provide cross-platform development tools to aid in debugging, prototyping and optimization of programs. These are full system emulation systems that can emulate the final binary to be run on the real board, its operating system and devices. Many of these emulation systems do not provide cycle level information due to the time consuming nature of cycle accurate simulation.In this paper we propose a method to provide Cycle-Close Traces of cycle-level statistics for the complete execution of the program in orders of magnitude less time than performing full cycle accurate simulation, with an average error of 3.2%. Our approach uses dynamic phase analysis to generate targeted cycle-close simulation samples. Detailed simulation results for these samples are used to produce fast cycle-close traces during a program's execution, so the user can also watch, pause and debug the currently executing code and its corresponding architecture performance characteristics at any point during execution.","Analytical models,
Emulation,
Software debugging,
Software performance,
Embedded system,
Operating systems,
Hardware,
Permission,
Computer science,
Design engineering"
A forwarding scheme for reliable and energy-efficient data delivery in cluster-based sensor networks,"This letter proposes a novel forwarding scheme for reliable and energy-efficient data delivery in the cluster-based sensor networks. While multiple nodes in a cluster receive a packet, only one node among them is elected to send the acknowledgement back and then to broadcast it to the next cluster. With the binary exponential backoff algorithm for the election, the proposed scheme is more reliable and energy-efficient than existing forwarding schemes for the cluster-based sensor networks.","Energy efficiency,
Intelligent networks,
Wireless sensor networks,
Clustering algorithms,
Nominations and elections,
Radio broadcasting,
Receivers,
Topology,
Computer science,
Maintenance"
Fault-tolerant routing in meshes/tori using planarly constructed fault blocks,"A few faulty nodes can make an n-dimensional mesh or torus network unsafe for fault-tolerant routing methods based on the block fault model, where the whole system (n-dimensional space) forms a fault block. A new concept, called extended local safety information in meshes or tori, is proposed to guide fault-tolerant routing, and classifies fault-free nodes inside 2-dimensional planes. Many nodes globally marked as unsafe become locally enabled inside 2-dimensional planes. A fault-tolerant routing algorithm based on extended local safety information is proposed for k-ary n-dimensional meshes/tori. Our method does not need to disable any fault-free nodes, unlike many previous methods, and this enhances the computational power of the system and improves performance of the routing algorithm greatly. All fault blocks are constructed inside 2-dimensional planes rather than in the whole system. Extensive simulation results are presented and compared with the previous methods.","Fault tolerance,
Routing,
Safety,
Fault tolerant systems,
Sun,
Computer science,
Partitioning algorithms,
Intelligent networks,
Computational modeling,
Costs"
Controlling spam Emails at the routers,"Like it or not, unsolicited bulk commercial Email (aka ""spam"") has become a regular menu item on the Internet information diet. Every day, millions of people find their Email in-boxes clogged with vast quantities of spam. Moreover, the daily replenishment of all those in-boxes with new spam also consumes significant amount of network bandwidth. Dealing with spam is like fighting a battle against a large army; the most effective approach is to employ multiple tactics. However, almost all spam control methods that have been proposed and implemented follow the same basic theme of establishing a ""front line"" of defense at the end-user level. Thus, in this paper we propose a method for blocking the supply lines. More specifically, we identify spam at the router level and control it via rate limiting. Spam identification is done in two phases. In the first phase, we identify the bulk stream of Email messages and in second phase we apply Bayesian classifier to identify whether it is a spam. If a bulk Email stream is classified as a spam then we rate limit it (e.g. no more than one copy per minute). Our proposed method exploits the short timespan delivery and bulkiness of spam Emails. We use publicly available spam corpus to evaluate our proposed scheme and in the other set of experiments, we work on one month sanitized log of our department Emails to provide the representative results.","Unsolicited electronic mail,
Electronic mail,
Bandwidth,
Bayesian methods,
Communication system traffic control,
Computer science,
Internet,
Network servers,
Web server,
Probes"
2D statistical models of facial expressions for realistic 3D avatar animation,We address the issue of modelling facial expressions for realistic 3D avatar animation. We introduce a hierarchical decomposition of a human face into different components and model them according to their intrinsic functionalities. The parametrisation of the expressions is achieved in a two-level framework. First level accounts for the low level component facial actions and is represented by hierarchical latent variable models. The second level models the final expressions as a combination of subcomponent information extracted from the lower level using combinatorial logic. Finally we produce continuous animation curves that are used to animate 3D avatar in a morph-based fashion. Our approach is entirely based on 2D information extracted from the input source.,"Avatars,
Facial animation,
Humans,
Face,
Data mining,
Facial muscles,
Deformable models,
Computer science,
Logic,
Relays"
A survey of neural network ensembles,"A neural network ensemble combines a finite number of neural networks or other types of predictors, which are trained simultaneously for a common classification task. Compared with a single neural network, the ensemble is able to efficiently improve the generalization ability of the classifier. The objective of this paper is to introduce existing research work on the neural network ensembles, including effective analysis, general implement steps of ensembles, and traditional technologies for training component neural networks, and also description the applications of it","Neural networks,
Machine learning,
Computer networks,
Optical computing,
Intelligent networks,
Machine intelligence,
Electronic mail,
Support vector machines,
Support vector machine classification,
Forward contracts"
Similarity measurement between images,"Experimental results of applying two similarity measurements, Euclidean distance and chord distance, to test a set of six Brodatz's textures are reported. Experiments show that in addition to feature extraction, a similarity measurement between images should be simultaneously considered. We also review some other similarity measurements.","Image databases,
Spatial databases,
Euclidean distance,
Shape measurement,
Feature extraction,
Multimedia databases,
Information retrieval,
Pattern recognition,
Distance measurement,
Computer science"
Recognizing hand-raising gestures using HMM,"Automatic attention-seeking gesture recognition is an enabling element of synchronous distance learning. Recognizing attention seeking gestures is complicated by the temporal nature of the signal that must be recognized and by the similarity between attention seeking gestures and non-attention seeking gestures. Here we describe two approaches to the recognition problem that utilize HMMs to learn the class of attention seeking gestures. An explicit approach that encodes the temporal nature of the gestures within the HMM, and an implicit approach that augments the input token sequence with temporal markers are presented. Experimental results demonstrate that the explicit approach is more accurate.","Hidden Markov models,
State estimation,
Computer science,
Computer aided instruction,
Parameter estimation,
Computer vision,
Robot vision systems"
Smart program visualization technologies: planning a next step,"Learning to program is a difficult and complex process that needs to be aided by proper educational tools. The crucial question is if the tool can support the learning or not. The potentials of program visualization (PV) tools, especially essential in novice programmers training, were shown in the past. Unfortunately, they are still underutilized and the results of their use are inconclusive. Moreover, the approach of creating general-purpose tools for a general-user is no longer bearable. The tools should be smart and accommodate to the changing needs, goals, and context of the users. This can increase the efficiency, acceptance and usage of PV tools. We perform a critical analysis of the current state-of-practice in PV and smart technologies and propose a taxonomy linking these research tracks. In addition, we present directions for the future of the research in smart program visualization tools.","Visualization,
Technology planning,
Programming profession,
Computer science,
Performance analysis,
Taxonomy,
Joining processes,
Educational technology,
Context awareness"
Influence of leakage reduction techniques on delay/leakage uncertainty,"One of the main challenges for design in the presence of process variations is to cope with the uncertainties in delay and leakage power. In this paper, the influence of leakage reduction techniques on delay/leakage uncertainty is examined through Monte-Carlo analysis. The techniques investigated in this paper include increasing gate length, stack forcing, body biasing, and V/sub dd//V/sub th/ optimization. The impact of technology scaling and temperature sensitivity on the uncertainty reduction are also evaluated. We investigate the uncertainty-power-delay trade-off and suggest techniques for designs targeting different requirements.","Delay,
Uncertainty,
Threshold voltage,
Frequency,
Transistors,
Leakage current,
Energy consumption,
Computer science,
Design engineering,
Power engineering and energy"
Incremental maintenance of software artifacts,"We have built a software development tool, CLIME, that uses constraints implemented as database queries to ensure the consistency of the different artifacts of software development. This approach makes the environment responsible for detecting inconsistencies between software design, specifications, documentation, source code, and test cases without requiring any of these to be a primary representation. The tool works incrementally as the software is written and evolves without imposing a particular methodology or process. It includes a front end that lets the user explore and fix current inconsistencies. This paper describes the techniques underlying the tool, concentrating on the user interface and the incremental maintenance of constraints between these artifacts.","Software maintenance,
Documentation,
Software systems,
Software testing,
System testing,
Programming profession,
Computer science,
Databases,
Software design,
Software tools"
Tonotopic multi-layered perceptron: a neural network for learning long-term temporal features for speech recognition,"We have been reducing word error rates (WER) on conversational telephone speech (CTS) tasks by capturing long-term (/spl sim/500ms) temporal information using multilayered perceptrons (MLP). In this paper we experiment with an MLP architecture called tonotopic MLP (TMLP), incorporating two hidden layers. The first of these is tonotopically organized: for each critical band, there is a disjoint set of hidden units that use the long-term energy trajectory as the input. Thus, each of these subsets of hidden units learns to discriminate single band energy trajectory patterns. The rest of the layers are fully connected to their inputs. When used in combination with an intermediate-term (/spl sim/100ms) MLP system to augment standard PLP features, the TMLP reduces the WER on the 2001 Nist Hub-5 CTS evaluation set (Eval2001) by 8.87% relative. We show some practical advantages over our previous methods. We also report results from a series of experiments to determine the best ranges of hidden layer sizes and total parameters with respect to the number of training patterns for this task and architecture.","Multilayer perceptrons,
Neural networks,
Multi-layer neural network,
Speech recognition,
Frequency,
Automatic speech recognition,
Error analysis,
Telephony,
NIST,
Computer science"
Semantic Video Summarization Using Mutual Reinforcement Principle and Shot Arrangement Patterns,"We propose a novel semantic video summarization framework, which generates video skimmings that guarantee both the balanced content coverage and the visual coherence. First, we collect video semantic information with a semi-automatic video annotation tool. Secondly, we analyze the video structure and determine each video sceneâ€™s target skim length. Then, mutual reinforcement principle is used to compute the relative importance value and cluster the video shots according to their semantic descriptions. Finally, we analyze the arrangement pattern of the video shots, and the key shot arrangement patterns are extracted to form the final video skimming, where the video shot importance value is used as guidance. Experiments are conducted to evaluate the effectiveness of our proposed approach.","Videoconference,
Motion pictures,
Computer science,
Pattern analysis,
Large-scale systems,
Software libraries,
Content management,
Power system management,
Tree graphs,
Layout"
Using Transparent Shaping and Web Services to Support Self-Management of Composite Systems,"Increasingly, software systems are constructed by composing multiple existing applications. The resulting complexity increases the need for self-management of the system. However, adding autonomic behavior to composite systems is difficult, especially when the existing components were not originally designed to support such interactions. Moreover, entangling the code for integrated self-management with the code for the business logic of the original applications may actually increase the complexity of the system, counter to the desired goal. In this paper, we propose a technique to enable self-managing behavior to be added to composite systems transparently, that is, without requiring manual modifications to the existing code. The technique uses transparent shaping, developed previously to enable dynamic adaptation in existing programs, to weave self-managing behavior into existing applications, which interact through Web services. A case study demonstrates the use of this technique to construct a fault-tolerant surveillance application from two existing applications, one developed in .NET and the other in CORBA, without the need to modify the source code of the original applications","Web services,
Interconnected systems,
Middleware,
Internet,
Disaster management,
Computer languages,
Simple object access protocol,
Computer science,
Application software,
Technology management"
A New Video Encryption Algorithm for H.264,"H.264 is the new Video Encoding World Standard, how to guarantee its security is an urgent problem. In this paper a new video encryption algorithm for H.264 is proposed, which utilized scrambling of intra-prediction, inter-prediction mode and encryption of transform result, motion vector. The results show that the proposed method exhibits nice security, has low impact on compression ration and support direct bit-rate control, moreover it can be realized in real time environment",
Fusion of dynamic and static features for gait recognition over time,"Gait recognition aims to identify people at a distance by the way they walk. This paper deals with a problem of recognition by gait when time-dependent covariates are added. Properties of gait can be categorized as static and dynamic features, which we derived from sequences of images of walking subjects. We show that recognition rates fall significantly when gait data is captured over a lengthy time interval. A new fusion algorithm is suggested in the paper wherein the static and dynamic features are fused to obtain optimal performance. The new fusion algorithm divides decision situations into three categories. The first case is when more than two thirds of the classifiers agreed to assign identity to the same class. The second case is when the two different classes are selected by each half of classifiers. The rest falls into the third case. The suggested fusion rule was compared with the most popular fusion rules for biometrics. It is shown that the new fusion rule over-performs the established techniques.",
Speeding-up Cache Lookups in Wireless Ad-Hoc Routing using Bloom Filters,"On demand routing protocols that exploit local caches have received a lot of attention lately in wireless ad-hoc networking. In this paper, we specifically address cache management, an issue that has been a main source of criticism for the applicability of such protocols. In particular, we tackle the problem of accessing the cache content efficiently. To this end, we propose summarizing the cache content so that we achieve efficient lookups. This not only saves both the restrictive resources of the wireless devices such as computational power and energy but also improves the overall protocol performance. We use Bloom filters as summaries. Our experimental results using the ns simulator show that both resource savings and performance improvements are attained when such filters are integrated within the DSR protocol which is one the most widely used instance of an on demand protocol","Routing protocols,
Filters,
Costs,
Access protocols,
Mobile communication,
Intelligent networks,
Computer science,
Wireless application protocol,
Computational modeling,
Manufacturing"
On the design of guillotine traps for vibratory bowl feeders,"The vibratory bowl feeder remains the most common approach to the automated feeding (orienting) of industrial parts. We study the algorithmic design of a trap in the bowl feeder track that filters out all but one orientation of a given polygonal part. We propose a new class of traps that we call guillotine traps, which remove a portion of the track between two parallel lines. A major advantage of guillotine traps over previously studied traps is that they permit feeding the part in a user-specified stable orientation, whereas these other traps offered no control over the orientation to be fed. The capability of feeding a part in any priorly specified orientation for example offers the user a means of control over the feed rate. We present a complete algorithm that takes as input any polygonal part consisting of n vertices, along with its center of mass, and a desired output orientation of the part. Our algorithm computes a guillotine trap for a vibratory bowl feeder that outputs parts in the desired orientation, or reports that no such trap exists. The algorithm runs in O(n/spl alpha/(n) log n + nk), where /spl alpha/(n) is the extremely slowly growing inverse of the Ackermann function, and k is the number of candidate solutions. Although the value of k is trivially bounded by O(n), we conjecture that k is a small constant except for highly symmetric and regular parts. Surprisingly, our algorithm is considerably more efficient than the algorithm for the more restricted and hence less powerful gap trap, which was shown to run in O(n/sup 2/ log n) time. We have implemented our complete algorithm in Mathematica and C++.","Feeds,
Costs,
Filters,
Belts,
Design automation,
Manufacturing automation,
Industrial engineering,
Operations research,
Computer industry,
Algorithm design and analysis"
Cross-dimensional gestural interaction techniques for hybrid immersive environments,"We present a set of cross-dimensional interaction techniques for a hybrid user interface that integrates existing 2D and 3D visualization and interaction devices. Our approach is built around one-and two-handed gestures that support the seamless transition of data between co-located 2D and 3D contexts. Our testbed environment combines a 2D multi-user, multi-touch, projection surface with 3D head-tracked, see-through, head-worn displays and 3D tracked gloves to form a multi-display augmented reality. We address some of the ways in which we can interact with private data in a collaborative, heterogeneous workspace. We also report on a pilot usability study to evaluate the effectiveness and ease of use of the cross-dimensional interactions.","User interfaces,
Three dimensional displays,
Two dimensional displays,
Augmented reality,
Virtual reality,
Computer displays,
Computer science,
Data visualization,
Testing,
Collaborative work"
Performance modeling of subnet management on fat tree InfiniBand networks using OpenSM,"InfiniBand is becoming increasingly popular in the area of cluster computing due to its open standard and high performance. Fat tree is a primary interconnection topology for building large scale InfiniBand clusters. Instead of using a shared bus approach, InfiniBand employs an arbitrary switched point-to-point topology. In order to manage the subnet, InfiniBand specifies a basic management infrastructure responsible for discovery, configuration and maintaining the active state of the network. In the literature, simulation studies have been done on irregular topologies to characterize the subnet management mechanism. However, there is no study to model subnet management mechanism on regular topologies using actual implementations. In this paper, we take up the challenge of modeling subnet management mechanism for fat tree InfiniBand networks using a popular subnet manager OpenSM. We present the timings for various subnet management phases namely topology discovery, path computation and path distribution for large scale fat tree InfiniBand subnets and present basic performance evaluation on small scale InfiniBand cluster. We verify our model with the basic set of results obtained, and present the results for the model by varying different parameters on fat trees.","Network topology,
Computer network management,
Computer networks,
Timing,
High performance computing,
Large-scale systems,
Distributed computing,
Personal communication networks,
Engineering management,
Computer science"
Metric embeddings with relaxed guarantees,"We consider the problem of embedding finite metrics with slack: we seek to produce embeddings with small dimension and distortion while allowing a (small) constant fraction of all distances to be arbitrarily distorted. This definition is motivated by recent research in the networking community, which achieved striking empirical success at embedding Internet latencies with low distortion into low-dimensional Euclidean space, provided that some small slack is allowed. Answering an open question of Kleinberg, Slivkins, and Wexler (2004), we show that provable guarantees of this type can in fact be achieved in general: any finite metric can be embedded, with constant slack and constant distortion, into constant-dimensional Euclidean space. We then show that there exist stronger embeddings into /spl lscr//sub 1/ which exhibit gracefully degrading distortion: these is a single embedding into /spl lscr//sub 1/ that achieves distortion at most O(log 1//spl epsi/) on all but at most an /spl epsi/ fraction of distances, simultaneously for all /spl epsi/ > 0. We extend this with distortion O(log 1//spl epsi/)/sup 1/p/ to maps into general /spl lscr//sub p/, p /spl ges/ 1 for several classes of metrics, including those with bounded doubling dimension and those arising from the shortest-path metric of a graph with an excluded minor. Finally, we show that many of our constructions are tight, and give a general technique to obtain lower bounds for /spl epsi/-slack embeddings from lower bounds for low-distortion embeddings.",
Dialog-based protocol: an empirical research method for cognitive activities in software engineering,"This paper proposes dialog-based protocol for the study of the cognitive activities during software development and evolution. The dialog-based protocol, derived from the idea of pair programming, is a significant alternative to the common think-aloud protocol, because it lessens the Hawthorne and placebo effects. Using screen-capturing and voice recording instead of videotaping further reduces the Hawthorne effect. The self-directed learning theory provides an encoding scheme and can be used in analyzing the data. A case study illustrates this new approach.","Software engineering,
Data analysis,
Encoding,
Access protocols,
Computer science,
Programming,
Video recording,
Ontologies,
Humans,
Performance analysis"
Implementation and performance measurement of an island multicast protocol,"With the availability and penetration of multicast-capable routers, many local networks in today's Internet are multicast-capable. However, achieving global IP multicast is still hindered by many management and technical difficulties. This is because routers interconnecting these local multicast-capable networks, or so-called ""islands,"" are often either multicast-incapable or multicast-disabled. Traditional application-level multicast (ALM) only makes use of unicast connections to form delivery trees and has not fully taken advantage of the local multicast capability of an island. As a result, these protocols are not very efficient. In order to achieve efficient global multicast, we propose and study island multicast (IM) where unicast connections are used between islands while IP multicast is used within islands. We present the detailed mechanisms of the IM centralized approach. IM is simple to implement and is based on minimum spanning tree, and hence is applicable to many-to-many communication. We have implemented the protocol and done real measurements on PlanetLab. We show that our protocol significantly improves network performance (in terms of stress, delay and nodal degrees) as compared to using ALM alone.","Multicast protocols,
Unicast,
Availability,
IP networks,
Stress,
Relays,
Councils,
Computer science,
Extraterrestrial measurements,
Data security"
"Rapid ""crash testing"" for continuously evolving GUI-based software applications","Several rapid-feedback-based quality assurance mechanisms are used to manage the quality of continuously evolving software. Even though graphical user interfaces (GUIs) are one of the most important parts of software, there are currently no mechanisms to quickly retest evolving GUI software. We leverage our previous work on GUI testing to define a new automatic GUI re-testing process called ""crash testing"" that is integrated with GUI evolution. We describe two levels of crash testing: (1) immediate feedback-based in which a developer indicates that a GUI bug was fixed in response to a previously reported crash; only select crash test cases are rerun and the developer is notified of the results in a matter of seconds, and (2) between code changes in which new crash test cases are generated on-the-fly and executed on the GUI. Since the code may be changed by another developer before all the crash tests have been executed, hence requiring restarting of the process, we use a simple rotation-based scheme to ensure that all crash tests are executed over a series of code changes. We show, via empirical studies, that our crash tests are effective at revealing serious problems in the GUI.","Vehicle crash testing,
Application software,
Graphical user interfaces,
Open source software,
Software maintenance,
Software quality,
Computer crashes,
Automatic testing,
Computer bugs,
Computer science"
Comprehensive software understanding with SEXTANT,"Current tools for software understanding mostly concentrate on one comprehension technique, e.g., visualization, or bottom-up navigation through software elements via hyperlinks. In this paper, we argue that to effectively assist developers in understanding today's software systems, a combination of several comprehension techniques is needed including seamless integration of top-down querying and bottom-up navigation strategies that work across different kinds of software artifacts; furthermore, application-domain and/or technology specific relationships between software elements should be taken into consideration; last but not least, a tight integration of such tools into development environments is crucial. We present SEXTANT, a software exploration tool tightly integrated into the Eclipse IDE that satisfies these requirements. In two case studies, we demonstrate how SEXTANT's features are conducive in tracking down the source of erroneous behavior, respectively, in discovering 'bad smells' in the software structure which should lead to code refactorings.","Software tools,
Software systems,
Visualization,
Navigation,
Software maintenance,
Documentation,
Computer science,
Knowledge engineering,
Java,
Software libraries"
The utility of heterogeneous swarms of simple UAVs with limited sensory capacity in detection and tracking tasks,"We present a physically realizable UAV model for locating and tracking chemical clouds. Simulation results are presented for implementations of this model with two configurations, one that is faster and requires more space to avoid collisions, and one that is slower and can cover an area more densely. Heterogeneous swarms of agents are shown to have better performance than homogeneous swarms of similar size because they take advantage of the strengths of each configuration.","Unmanned aerial vehicles,
Chemical sensors,
Clouds,
Robot sensing systems,
Vehicle detection,
Tactile sensors,
Phase detection,
Computer science,
Chemical engineering,
Computational modeling"
Interactions and dependencies in estimation of distribution algorithms,"In this paper, we investigate two issues related to probabilistic modeling in estimation of distribution algorithms (EDAs). First, we analyze the effect of selection in the arousal of probability dependencies in EDAs for random functions. We show that, for these functions, independence relationships not represented by the function structure are likely to appear in the probability model. Second, we propose an approach to approximate probability distributions in EDAs using a subset of the dependencies that exist in the data. An EDA that employs only malign interactions is introduced. Preliminary experiments presented show how the probability approximations based solely on malign interactions, can be applied to EDAs.","Electronic design automation and methodology,
Probability distribution,
Graphical models,
Artificial intelligence,
Evolutionary computation,
Sampling methods,
Intelligent systems,
Computer science,
Computational modeling,
Genetic algorithms"
VLEI code: an efficient labeling method for handling XML documents in an RDB,"A number of XML labeling methods have been proposed to store XML documents in relational databases. However, they have a vulnerable point, in insertion operations. We propose the variable length endless insertable (VLEI) code and apply it to XML labeling to reduce the cost of insertion operations. Results of our experiments indicate that a combination of the VLEI code and Dewey order is effective for handling skewed insertions.","Labeling,
Document handling,
XML,
Relational databases,
Costs,
Transaction databases,
Space technology,
Computer science,
Concurrency control,
Binary codes"
Structural group classification technique based on regional fMRI BOLD responses,"This paper presents a new multigroup classification method based on subtle differences in regional brain activity during the completion of a functional magnetic resonance imaging (fMRI) challenge paradigm. Classification is performed based on features derived from BOLD time intensity curves in selected regions of interest (ROI). For each ROI, a mean time intensity curve [called mean regional response (MRR)] is calculated from realigned and normalized datasets. The overall subject performance is characterized with a vector of features obtained using nonlinear modeling of all subject's MRRs with a mixture of time shifted Gaussian functions. The classification is performed in the reduced-dimension optimal discrimination space, obtained through canonical transformations of original feature space. In order to demonstrate feasibility of the proposed method, classification of three groups of subjects is presented. The three groups are defined as heavy marijuana smokers after 24 hours of abstinence, heavy marijuana smokers after 28 days of abstinence, and healthy nonusing controls. The proposed method can be useful as an analytic tool for the discrimination of different groups of subjects based on temporal features of functional magnetic resonance imaging activation.","Analysis of variance,
Brain,
Magnetic resonance imaging,
Neuroimaging,
Hospitals,
Biomedical imaging,
Independent component analysis,
Blood flow,
Principal component analysis,
Extraterrestrial measurements"
An architecture and a wrapper synthesis approach for multi-clock latency-insensitive systems,"This paper presents an architecture and a wrapper synthesis approach for the design of multi-clock systems-on-chips. We build upon the initial work on multi-clock latency-insensitive systems by Singh and Theobald (2004), and provide a detailed system architecture with the following capabilities and benefits: (i) modules arc stalled only when needed, thereby avoiding unnecessary stalling, (ii) adequate metastability resolution is provided, (iii) handshake interfaces between modules are high-performance and low-latency, i.e., capable of transferring data packets on every clock cycle, (iv) IP cores with large clock distribution delays are correctly handled, and (v) an automated approach is provided for wrapper synthesis from formal specifications. For wrapper synthesis, we have developed an automated tool which accepts interface specifications in a high-level language (Component Wrapper Language, or CWL), and automatically produces gate-level implementations of wrapper circuitry that will correctly and efficiently stall the synchronous modules depending on the availability of I/O channels. An optimization is introduced to reduce the cost of the wrapper circuitry by eliminating ""busy waiting."" A small set of benchmark examples is also proposed, and synthesis results for the tool are promising.","Clocks,
Ring oscillators,
Computer architecture,
Circuit synthesis,
Metastasis,
Delay,
Jitter,
Computer science,
High level languages,
Cost function"
Physics and fracture,"Attempting to understand the relevant material properties that are vital to an object's construction is as old as humankind itself, and today, it justifies the intense research effort that goes under the label of materials science. An important subfield of materials science is the study of how materials react to external forces. Some materials fracture, for instance, whereas others flow: a glass rod will break if bent strongly enough, but a lead rod will simply change its shape internally. The importance of these questions can't be overemphasized - the probability of finding a crack that is at least 5 centimeters long in any randomly chosen commercial aircraft is 50 percent, for example; how can we be sure that this isn't dangerous? How can we trust that a given high-rise building is safe? Such questions touch our lives directly. Computers have made profound changes to the structure of physics. The physics community now actively pursues problems that were first thought to be too messy, such as strength and economics (called econophysics in this context). There might be a grain of truth to the view that physics has changed from being defined as the study of a certain class of problems to becoming a toolbox for solving various problems. If we were to define the fundamental difference in traditional versus statistical physics approaches, we'd see that researchers have traditionally approached the strength problem as perturbations on a homogeneous, disorderless problem. The statistical physics approach, on the other hand, takes the opposite limit as a starting point. The percolation problem, mentioned earlier in this article, can be formulated as an infinite disorder limit, meaning we treat the strength problem as perturbations away from the infinite-disorder limit.","Physics,
Fuses,
Bonding,
Electric breakdown,
Breakdown voltage,
Stress,
Predictive models,
Lattices,
Conducting materials,
Humans"
Performance comparison of OCS and OBS switching paradigms,"Optical burst switching (OBS) is a new optical switching paradigm where traffic can be switched and groomed at a lower level compared to optical switched circuit (OCS). While OCS is useful in carrying highly aggregated long-lived streams that require absolute quality of service (QoS) guarantees, OBS has a role in efficiently carrying bursty best-effort traffic. In this paper, we investigate further the comparative goodput and bandwidth efficiency of OBS and OCS using, on one hand, an adaptive deflection OBS routing that aims at minimizing the burst loss rate and, on the other hand, a routing and wavelength OCS strategy using the best possible routing paths for minimizing the blocking rate.","Optical wavelength conversion,
Switching circuits,
Wavelength routing,
Optical buffering,
Optical fiber networks,
Bandwidth,
Telecommunication traffic,
Telecommunication switching,
Optical burst switching,
Computer science"
Detecting cuts in sensor networks,"We propose a low overhead scheme for detecting a network partition or cut in a sensor network. Consider a network S of n sensors, modeled as points in a two-dimensional plane. An /spl epsiv/-cut, for any 0","Intelligent networks,
Wireless sensor networks,
Base stations,
Monitoring,
Biosensors,
Space technology,
Computer science,
Mathematics,
Sampling methods,
Instruments"
The parking permit problem,"We consider online problems where purchases have time durations which expire regardless of whether the purchase is used or not. The parking permit problem is the natural analog of the well-studied ski rental problem in this model, and we provide matching upper and lower bounds on the competitive ratio for this problem. By extending the techniques thus developed, we give an online-competitive algorithm for the problem of renting steiner forest edges with time durations.","Costs,
Web server,
Drives,
Assembly,
Web and internet services,
Pricing,
Network servers,
Telecommunication traffic,
Traffic control,
Frequency"
On refactoring support based on code clone dependency relation,"Generally, code clones are regarded as one of the factors that make software maintenance more difficult. A code clone is a set of source code fragments identical or similar to each other from the viewpoint of software maintainability, code clones should be removed. However, sometimes there are dependency relations among each of which belong to the different code clone, and it is advisable to refactor all of such code clones at once. In this paper, we focus on the case that such code fragment corresponds to a method body in Java programs. We defined ""chained method"" as a set of methods that have dependency relations. A set of ""chained methods"" whose elements are each other's code clone is called ""chained clone"", and an equivalence class of ""chained clone"" is called a ""chained clone set"". We propose a refactoring support method for ""chained clone set"" by providing an appropriate refactoring pattern to them. Finally, we present the ""chained clone set"" refactoring support tool that we have developed, together with some case studies to show the usefulness of the proposed method","Cloning,
Software maintenance,
Java,
Application software,
Information science,
Computer industry,
Industrial relations,
Large-scale systems,
Software tools,
Open source software"
An initial study of a lightweight process for change identification and regression test selection when source code is not available,"Various regression test selection techniques have been developed and have shown to improve testing cost effectiveness via improving efficiency. The majority of these test selection techniques rely on access to source code for change identification. However, when new releases of COTS components are made available for integration and testing, source code is often not available to guide in regression test selection. In this paper we describe a lightweight integrated-black-box approach for component change identification (I-BACCI) process for selection of regression tests for user/glue code that uses COTS components. I-BACCI is applicable when component licensing agreements do not preclude binary code analysis. A case study of the process was conducted on an ABB product that uses a medium-scale internal ABB software component. Six releases of the component were examined to evaluate the efficacy of the proposed process. The result of the case study indicates that this process can reduce the required regression tests by 40% on average","System testing,
Software testing,
Costs,
Binary codes,
Computer science,
Licenses,
Hardware,
Application software,
Large-scale systems,
Software libraries"
Partial elastic matching of time series,"We consider the problem of elastic matching of time series. We propose an algorithm that determines a subsequence of a target time series that best matches a query series. In the proposed algorithm, we map the problem of the best matching subsequence to the problem of a cheapest path in a DAG (directed acyclic graph). The proposed approach allows us to also compute the optimal scale and translation of time series values, which is a nontrivial problem in the case of subsequence matching.","Data mining,
Euclidean distance,
Information science,
Computer science,
Pervasive computing,
Clustering algorithms,
Dynamic programming,
Time measurement"
Quasi-perfect codes with small distance,"The main purpose of this paper is to give bounds on the length of the shortest and longest binary quasi-perfect codes with a given Hamming distance, covering radius, and redundancy. We consider codes with Hamming distance 4 and 5 and covering radius 2 and 3, respectively. We discuss the blockwise direct sum (BDS) construction which has an important role in finding these bounds.","Hamming distance,
Cities and towns,
Binary codes,
Information theory,
Computer science,
Materials science and technology,
Mathematics"
Real-time multistage attack awareness through enhanced intrusion alert clustering,"Correlation and fusion of intrusion alerts to provide effective situation awareness of cyber-attacks has become an active area of research. Snort is the most widely deployed intrusion detection sensor. For many networks and their system administrators, the alerts generated by Snort are the primary indicators of network misuse and attacker activity. However, the volume of the alerts generated in typical networks makes real-time attack scenario comprehension difficult. In this paper, we present an attack-stage oriented classification of alerts using Snort as an example and demonstrate that this effectively improves real-time situation awareness of multistage attacks. We also incorporate this scheme into a real-time attack detection framework and prototype presented by the authors in previous work and provide some results from testing against multistage attack scenarios",
A seamless approach to multiscale complex fluid simulation,"Dissipative particle dynamics is an efficient and accurate mesoscale simulation method that bridges the gap between molecular dynamics and continuum hydrodynamics. Using time-staggered algorithms, it can simulate complex liquids and dense suspensions more than 100,000 faster than molecular dynamics.","Hydrodynamics,
Polymers,
Stochastic processes,
Fluid flow,
Liquids,
Fluctuations,
Fluid dynamics,
Portable computers,
Lattice Boltzmann methods,
Bridges"
Combining static analysis and dynamic learning to build accurate intrusion detection models,"Anomaly detection based on monitoring of sequences of system calls has been shown to be an effective method for detection of previously unseen, potentially damaging attacks on hosts. This paper presents a new model for profiling normal program behavior for use in detection of intrusions that change application execution flow. This model is compact and efficient to operate and can be acquired using a combination of static analysis and dynamic learning. Our model (hybrid push down automata, HPDA) incorporates call stack information in the automata model and effectively captures the control flow of a program. Several important properties of the model are based on a unique correspondence relation between addresses and instructions within the model. These properties allow the HPDA to be acquired by dynamic analysis of an audit of the call stack log. Our strategy is to use static analysis to acquire a base model and then to use dynamic learning as a supplement to capture those aspects of behavior that are difficult to capture with static analysis due to techniques commonly used in modern programming environments. The model created by this combination method is shown to have a higher detection capability than models acquired by static analysis alone and a lower false positive rate than models acquired by dynamic learning alone.",
Detecting indirect coupling,"Coupling is considered by many to be an important concept in measuring design quality There is still much to be learned about which aspects of coupling affect design quality or other external attributes of software. Much of the existing work concentrates on direct coupling, that is, forms of coupling that exists between entities that are directly related to each other. A form of coupling that has so far received little attention is indirect coupling, that is, coupling between entities that are not directly related. What little discussion there is in the literature suggests that any form of indirect coupling is simple the transitive closure of a form of direct coupling. We demonstrate that this is not the case, that there are forms of indirect coupling that cannot be represented in this way and suggest ways to measure it. We present a tool that identifies a particular form of indirect coupling that is integrated in the Eclipse IDE.","Costs,
Software quality,
Programming,
Computer science,
Software maintenance,
Detectors,
Australia,
Software engineering"
Structural Damage Detection Using Wireless Sensor-Actuator Networks,"Structural health monitoring (SHM) is a well-established multi-disciplinary research field, in which the goal is to develop technologies and techniques to automatically detect, localize, and classify damage in large structures. This paper is focused, starting with a simple SHM problem and a simple structure, on how wireless sensor networking technologies can help advance SHM. Specifically, a distributed damage detection algorithm is developed based on shifted spectra of response at the sensor nodes. While damage detection using frequency shifts is fairly well-understood in the SHM community, the development here adheres to sensor network architectural principles: the technique is amenable to in-network processing and duty-cycling, and can be implemented on a long-running sensor network. The efficacy is demonstrated using simulations on structural models, and actual measurements on real, albeit simple, structures","Wireless sensor networks,
Computerized monitoring,
Costs,
Vibration measurement,
Sensor phenomena and characterization,
Computer science,
Detection algorithms,
Frequency,
Buildings,
Marine vehicles"
Co-reservation with the concept of virtual resources,"We present an architectural framework for specifying and processing co-reservations in grid environments. Compared to other approaches, our co-reservation framework is more general. It can be applied to the reservation of applications running concurrently on multiple resources (multi-site applications) and to the planning of job flows, where the components may be linked by some temporal or spatial relationship. We introduce the concept of virtual resources that allows to compose resources by abstracting from their specific features. Among other features, virtual resources enable advanced co-reservation with nested resource levels, resource aggregation, and transparent fault recovery.","Streaming media,
Quality of service,
Resource management,
Processor scheduling,
Carbon capture and storage,
Middleware,
Protocols,
Runtime,
Virtual reality,
Computer science"
Low Power Test Compression Technique for Designs with Multiple Scan Chain,"This paper presents a new DFT technique that can significantly reduce test data volume as well as scan-in power consumption for multiscan-based designs. It can also help to reduce test time and tester channel requirements with small hardware overhead. In the proposed approach, we start with a pre-computed test cube set and fill the donâ€™t-cares with proper values for joint reduction of test data volume and scan power consumption. In addition we explore the linear dependencies of the scan chains to construct a fanout structure only with inverters to achieve further compression. Experimental results for the larger ISCASâ€™89 benchmarks show the efficiency of the proposed technique.","Energy consumption,
Hardware,
Design for testability,
Inverters,
Benchmark testing,
Computer science,
Production systems,
System testing,
Electronic design automation and methodology,
Manufacturing"
Energy-Saving 3-Step Velocity Control Algorithm for Battery-Powered Wheeled Mobile Robots,"Energy of Wheeled Mobile Robot (WMR) is usually supplied by batteries with finite energy. In order to extend run-time of battery-powered WMR, it is necessary to minimize the energy consumption. The energy is dissipated mostly in the motors, which strongly depends on the velocity profile. This paper investigates energy efficiency for typical 3-step velocity control methods to minimize a new energy object function which considers practical energy consumption dissipated in motors related to motor dynamics, velocity profile, and motor control input. We performed analyses on energy consumption for the four typical 3-step velocity profiles: S-C-S, T-C-T, S-C-T, and T-C-S (S is an acceleration/deceleration section by step control input, T is an acceleration/deceleration section by acceleration/deceleration phase of trapezoidal velocity profile, and C is a cruise section with a constant velocity). Also we suggested an efficient iterative search algorithm with binary search which can find the numerical solution quickly. We performed various computer simulations to show the performance of energy-efficient 3-step velocity control in comparison with a conventional 3-step trapezoidal profile with a reasonable constant acceleration rate as a benchmark. Simulation results reveal that the S-C-T is the most energy efficient 3-step velocity control profile, which enables WMR to extend working time up to 30%.","Velocity control,
Mobile robots,
Acceleration,
Energy consumption,
Energy efficiency,
Batteries,
Runtime,
Motor drives,
Performance analysis,
Iterative algorithms"
Automated object extraction through simplification of the differential morphological profile for high-resolution satellite imagery,,"Satellites,
Data mining,
Principal component analysis,
Morphology,
Robustness,
Computer science,
Buildings,
Layout,
Information analysis,
Information retrieval"
On LUT cascade realizations of FIR filters,"This paper first defines the n-input q-output WS function, as a mathematical model of the combinational part of the distributed arithmetic of a finite impulse response (FIR) filter. Then, it shows a method to realize the WS function by an LUT cascade with k-input q-output cells. Furthermore, it 1) shows that LUT cascade realizations require much smaller memory than the single ROM realizations; 2) presents new design method for a WS function by arithmetic decomposition, and 3) shows design results of FIR filters using FPGAs with embedded memories.","Finite impulse response filter,
Table lookup,
Field programmable gate arrays,
Digital filters,
IIR filters,
Digital arithmetic,
Hardware,
Logic,
Computer science,
Mathematical model"
Disciplined methods of software specification: a case study,"We describe our experience applying tabular mathematical approaches to software specifications. Our purpose is to show alternative approaches to writing tabular specifications and to help practitioners who want to apply such methods by allowing them to pick the best one for their problem. The object for the case study is software used by Dell Products for testing the functionality of the keyboards on notebook computers. Starting from informal documents, we developed a variety of tabular representations of finite state machine specifications and tabular trace specifications. We found that the discipline required by these methods raised issues that had never been considered and resulted in documents that were both more complete and much clearer. The various tabular representations are compared from a user's point of view, i.e., clarity, consistency, unambiguity, completeness, suitability, etc.","Computer aided software engineering,
Software testing,
Automata,
Software quality,
Keyboards,
Inspection,
Laboratories,
Computer science,
Information systems,
Writing"
Assignment of movies to heterogeneous video servers,"A video-on-demand (VOD) system provides an electronic video rental service to geographically distributed users. It can adopt multiple servers to serve many users concurrently. As a VOD system is being used and evolved, its servers probably become heterogeneous. For example, if a new server is added to expand the VOD system or replace a failed server, the new server may be faster with a larger storage size. This paper investigates how to assign movies to heterogeneous servers in order to minimize the blocking probability. It is proven that this assignment problem is NP-hard, and a lower bound is derived on the minimal blocking probability. The following approach is proposed for assignment: 1) problem relaxation-a relaxed assignment problem is formulated and solved to determine the ideal load that each server should handle, and 2) goal programming-an assignment and reassignment are performed iteratively while fulfilling all the constraints so that the load handled by each server is close to the ideal one. This approach is generic and applicable to many assignment problems. This approach is adopted to design two specific algorithms for movie assignment with and without replication. It is demonstrated that these algorithms can find optimal or close-to-optimal assignments.","Motion pictures,
Network servers,
Distributed computing,
Watches,
Web server,
Algorithm design and analysis,
Iterative algorithms,
Web and internet services,
Communication networks,
Computer science"
Leading conversations: Communication behaviours of emergent leaders in virtual teams,"Virtual teams and their leaders are key players in global organisations. Using teams of workers dispersed temporally and geographically has changed the way people work in groups and has redefined the nature of teamwork. Emergent leadership issues in computer-mediated communication are vital today because of the increasing prevalence of the virtual organisation, the flattening of organisational structures and the corresponding interest in managing virtual groups and teams. This paper examines the communication behaviours of participants in two different case studies to determine if number, length and content of messages are sufficient criteria to identify emergent leaders in asynchronous and synchronous environments. The methodology used can be embedded in collaborative virtual environments as technology for detecting potential leaders.","Virtual groups,
Teamwork,
Organizational aspects,
Collaborative work,
Virtual environment,
Asynchronous communication,
Communication networks,
Context awareness,
International collaboration"
A new total-dose-induced parasitic effect in enclosed-geometry transistors,"We present data showing a new total-dose-induced parasitic effect in enclosed-geometry transistors. A model for this new effect shows that the normal radiation-induced edge leakage current becomes gate controlled in ringed-source transistors. Small width-to-length (W/L) N-channel field-effect transistors (NFETs), often used in analog designs, show an additional drain current and transconductance, because the gate-controlled leakage current approaches the magnitude of the ideal channel current.",
"Registration of multiple range scans as a location recognition problem: hypothesis generation, refinement and verification","This paper addresses the following version of the multiple range scan registration problem. A scanner with an associated intensity camera is placed at a series of locations throughout a large environment; scans are acquired at each location. The problem is to decide automatically which scans overlap and to estimate the parameters of the transformations aligning these scans. Our technique is based on (1) detecting and matching keypoints - distinctive locations in range and intensity images, (2) generating and refining a transformation estimate from each keypoint match, and (3) deciding if a given refined estimate is correct. While these steps are familiar, we present novel approaches to each. A new range keypoint technique is presented that uses spin images to describe holes in smooth surfaces. Intensity keypoints are detected using multiscale filters, described using intensity gradient histograms, and backprojected to form 3D keypoints. A hypothesized transformation is generated by matching a single keypoint from one scan to a single keypoint from another, and is refined using a robust form of the ICP algorithm in combination with controlled region growing. Deciding whether a refined transformation is correct is based on three criteria: alignment accuracy, visibility, and a novel randomness measure. Together these three steps produce good results in test scans of the Rensselaer campus.","Testing,
Image registration,
Computer science,
Cameras,
Parameter estimation,
Image generation,
Filters,
Histograms,
Robust control,
Iterative closest point algorithm"
Single-trial variable model for event-related fMRI data analysis,"Most methods for fMRI data analysis assume that the hemodynamic responses (HRs) across similar experimental events are same. This assumption is not appropriate when HRs vary unpredictably from trial to trial. Here, we introduce a new method for fMRI data analysis. The main features of the proposed method are as follows: 1) The trial-to-trial variability is modeled as meaningful signal rather than assuming that the same HR is evoked in each trial; 2) Since the proposed method is a constrained optimization based general framework, it could be extended by utilizing prior knowledge of HR; 3) The traditional deconvolution method can be included into our method as a special case. A comparison of performance on simulated fMRI datasets is made using the general linear model, the deconvolution method and the proposed method with receiver operating characteristic (ROC) methodology. In addition, we examined the effectiveness and usefulness of our method on real experimental data.","Data analysis,
Deconvolution,
Independent component analysis,
Hemodynamics,
Statistics,
Principal component analysis,
Analysis of variance,
Pattern recognition,
Automation,
Yield estimation"
Learning non-negative sparse image codes by convex programming,"Example-based learning of codes that statistically encode general image classes is of vital importance for computational vision. Recently non negative matrix factorization (NMF) was suggested to provide image code that was both sparse and localized, in contrast to established non local methods like PCA. In this paper, we adopt and generalize this approach to develop a novel learning framework that allows to efficiently compute sparsity-controlled invariant image codes by a well defined sequence of convex conic programs. Applying the corresponding parameter-free algorithm to various image classes results in semantically relevant and transformation-invariant image representations that are remarkably robust against noise and quantization","Computer vision,
Sparse matrices,
Robustness,
Signal processing algorithms,
Quantization,
Application software,
Constraint optimization,
Bayesian methods,
Mathematics,
Computer science"
Semantic Web services discovery and ranking,"The World Wide Web is evolving from a huge information repository to a service oriented marketplace. Web service is becoming the next generation of Web based application. Due to the dramatic increasing number of available Web services, how to locate the right services is a big challenge. In this paper, a new approach is proposed for Web services matching within the UDDI registry, based on the matching results a ranking process significantly reduce the number of recommend services and increase the accuracy. This process is based on semantics in domain ontology. The whole solution provides a novel way to discover and utilize published Web services. It is flexible and extensible to accomplish complex Web service requests.","Semantic Web,
Web services,
Ontologies,
Web sites,
Microstrip,
XML,
Computer science,
Application software,
Web and internet services,
Pervasive computing"
Exploiting temporal idleness to reduce leakage power in programmable architectures,"One of the biggest challenges that programmable devices like FPGAs are facing in ultra deep sub-micron regime is the exponential rise in leakage power consumption. As technology shrinks below 90nm, a new design paradigm has to evolve to tackle the issue of leakage power consumption. In this work we focus on a new design methodology for reducing leakage power by exploiting temporal locality in designs and accordingly group them into clusters that can be switched on and off. We propose a power state controller based method, which controls the switching of the clusters from one state to another. We show our technique using data flow graphs where temporal locality can be effectively explored. Our results show that substantial leakage savings can be achieved if temporal idleness of designs can be exploited effectively.","Field programmable gate arrays,
Logic,
Energy consumption,
Switches,
Computer architecture,
Rails,
Automatic control,
Power engineering and energy,
Computer science,
Integrated circuit technology"
Recovering facial shape and albedo using a statistical model of surface normal direction,"This paper describes how facial shape can be modelled using a statistical model that captures variations in surface normal direction. To construct this model, we make use of the azimuthal equidistant projection to map surface normals from the unit sphere to points on a local tangent plane. The variations in surface normal direction are captured using the covariance matrix for the projected point positions. This allows us to model variations in face shape using a standard point distribution model. We train the model on fields of surface normals extracted from range data and show how to fit the model to intensity data using constraints on the surface normal direction provided by Lambert's law. We demonstrate that this process yields accurate facial shape recovery and allows an estimate of the albedo map to be made from single, real world face images.","Shape,
Surface fitting,
Principal component analysis,
Azimuth,
Computer science,
Covariance matrix,
Data mining,
Yield estimation,
Nose,
Photometry"
A comparison between the Pittsburgh and Michigan approaches for the binary PSO algorithm,"This paper shows the performance of the binary PSO algorithm as a classification system. These systems are classified in two different perspectives: the Pittsburgh and the Michigan approaches. In order to implement the Michigan approach binary PSO algorithm, the standard PSO dynamic equations are modified, introducing a repulsive force to favor particle competition. A dynamic neighborhood, adapted to classification problems, is also defined. Both classifiers are tested using a reference set of problems, where both classifiers achieve better performance than many classification techniques. The Michigan PSO classifier shows clear advantages over the Pittsburgh one both in terms of success rate and speed. The Michigan PSO can also be generalized to the continuous version of the PSO","Computer science,
Equations,
History,
Multidimensional systems,
Classification algorithms,
Testing,
Particle swarm optimization,
Evolutionary computation,
Genetic algorithms,
Optimization methods"
Image analysis for assessing molecular activity changes in time-dependent geometries,"In vivo fluorescence molecular imaging and tomography has facilitated monitoring of genomics and proteomics over time and on the same animal. A highly important issue, however, has been the robust registration of animals imaged at different time points to obtain accurate description of activity and location. This paper presents a method for aligning temporal data of small animals based on surface anatomical features and improving the accuracy of monitoring fluorophore distribution. The method can account for differences in the positioning and compression of small animals and can be extended to three-dimensional as well as to other imaging modalities.","Image analysis,
Geometry,
Animals,
Monitoring,
In vivo,
Fluorescence,
Molecular imaging,
Tomography,
Genomics,
Bioinformatics"
eSports: collaborative and synchronous video annotation system in grid computing environment,"We designed eSports - a collaborative and synchronous video annotation platform, which is to be used in Internet scale cross-platform grid computing environment to facilitate computer supported cooperative work (CSCW) in education settings such as distance sport coaching, distance classroom etc. Different from traditional multimedia annotation systems, eSports provides the capabilities to collaboratively and synchronously play and archive real time live video, to take snapshots, to annotate video snapshots using whiteboard and to play back the video annotations synchronized with original video streams. eSports is designed based on the grid based collaboration paradigm $the shared event model using NaradaBrokering, which is a publish/subscribe based distributed message passing and event notification system. In addition to elaborate the design and implementation of eSports, we analyze the potential use cases of eSports under different education settings. We believed that eSports is very useful to improve the online collaborative coaching and education.","Collaboration,
Grid computing,
Collaborative work,
Video sharing,
Streaming media,
Internet,
Computer science education,
Multimedia systems,
Real time systems,
Message passing"
A visual data mining framework for convenient identification of useful knowledge,"Data mining algorithms usually generate a large number of rules, which may not always be useful to human users. In this project, we propose a novel visual data-mining framework, called Opportunity Map, to identify useful and actionable knowledge quickly and easily from the discovered rules. The framework is inspired by the House of Quality from Quality Function Deployment (QFD) in Quality Engineering. It associates discovered rules, related summarized data and data distributions with the application objective using an interactive matrix. Combined with drill down visualization, integrated visualization of data distribution bars and rules, visualization of trend behaviors, and comparative analysis, the Opportunity Map allows users to analyze rules and data at different levels of detail and quickly identify the actionable knowledge and opportunities. The proposed framework represents a systematic and flexible approach to rule analysis. Applications of the system to large-scale data sets from our industrial partner have yielded promising results.","Data mining,
Data visualization,
Quality function deployment,
Manufacturing,
Bars,
Data analysis,
Computer science,
Humans,
Large-scale systems,
Quality management"
A modified particle swarm optimization algorithm and its application in optimal power flow problem,"A modified particle swarm optimization (MPSO) algorithm is presented. In the new algorithm, particles not only studies from itself and the best one but also from other individuals. By this enhanced study behavior, the opportunity to find the global optimum is increased and the influence of the initial position of the particles is decreased At last, the method adopting MPSO algorithm to solve the optimal power flow problem is given. The numeric simulation for a 5-bus system shows that this algorithm is feasible to solve optimal power flow problem.",
Atomic wedgie: efficient query filtering for streaming time series,"In many applications, it is desirable to monitor a streaming time series for predefined patterns. In domains as diverse as the monitoring of space telemetry, patient intensive care data, and insect populations, where data streams at a high rate and the number of predefined patterns is large, it may be impossible for the comparison algorithm to keep up. We propose a novel technique that exploits the commonality among the predefined patterns to allow monitoring at higher bandwidths, while maintaining a guarantee of no false dismissals. Our approach is based on the widely used envelope-based lower bounding technique. Extensive experiments demonstrate that our approach achieves tremendous improvements in performance in the offline case, and significant improvements in the fastest possible arrival rate of the data stream that can be processed with guaranteed no false dismissal.","Filtering,
Insects,
Patient monitoring,
Cardiology,
Space technology,
Computerized monitoring,
XML,
Matched filters,
Costs,
Computer science"
Rational dither modulation watermarking using a perceptual model,"Quantization index modulation (QIM) is a computationally efficient method of informed watermarking. However, the original method is particularly sensitive to variations in the amplitude of the signal. Previously, we proposed using a modification of Watson's perceptual model to adaptively adjust the quantization index step size. This simultaneously improved both the robustness and fidelity of the watermarked image and, most importantly, provided invariance (to a large degree) to valumetric scaling. Contemporaneously, rational dither modulation was proposed as an alternative QIM with valumetric invariance. In this paper, we combine the two methods and compare the performance of the new algorithm with our previous results. Experimental results demonstrate that the new algorithm outperforms the previous algorithms over the entire range of valumetric scale factors, albeit at the expense of a small decrease in fidelity. However all algorithms have a superior performance and improved fidelity compared with QIM",
A powerful direct mechanism for optimal WWW content replication,"This paper addresses the problem of fine-grained data replication in large distributed systems, such as the Internet, so as to minimize the user access delays. With fine-grained data replication, certain data objects, as opposed to a complete site, are duplicated at multiple servers. In this paper, we abstract the distributed system as an agent-based model wherein mobile agents on behalf of their nodes continuously compete for allocation and reallocation of data objects. However, since these agents do not have a global view of the system, the optimization process becomes highly local. This localization may encourage these selfish agents to alter the output of the resource allocation mechanism in their favor by misreporting critical data such as the objects' popularity. This paper proposes a game theoretical resource allocation mechanism involving selfish agents. The mechanism ensures that the agents do not misreport, always follow the rules, and that a global optima is achieved. The mechanism is extensively evaluated against some well-known algorithms, such as: greedy, branch and bound, game theoretical auctions and genetic algorithms. The experimental results reveal that the mechanism provides excellent solution quality, while maintaining fast execution time.","World Wide Web,
Resource management,
Game theory,
Delay,
Internet,
Mobile agents,
Distributed computing,
Computer science,
Data engineering,
Power engineering and energy"
Using Continuous Face Verification to Improve Desktop Security,"In this paper we describe the architecture, implementation, and performance of a face verification system that continually verifies the presence of a logged-in user at a computer console. It maintains a sliding window of about ten seconds of verification data points and uses them as input to a Bayesian framework to compute a probability that the logged-in user is still present at the console. If the probability falls below a threshold, the system can delay or freeze operating system processes belonging to the logged-in user. This helps prevent misuse of computer resources when an unauthorized user maliciously takes the place of an authorized user. Processes may be unconditionally frozen (they never return from a system call) or delayed (it takes longer to complete a system call, or appropriate action may be taken for certain classes of system calls, such as those that are considered security critical. We believe that the integrated system presented here is the first of its kind. Furthermore, we believe that the analysis of the tradeoffs between verification accuracy, processor overhead, and system security that we do in this paper has not been done elsewhere","Face detection,
Intrusion detection,
Authentication,
Computer security,
Delay systems,
Operating systems,
Access control,
National security,
Computer science,
Drives"
Exploration of underwater structures with cooperative heterogeneous robots,"This paper describes ideas in the field of cooperative underwater robotics, which can considerably enhance the exploration capabilities of underwater robots. Three heterogeneous planned or existing underwater robots are presented and possible approaches for cooperative behavior are discussed. Possible application scenarios for future deployment are presented.","Underwater structures,
Cognitive robotics,
Intelligent robots,
Control systems,
Robot sensing systems,
Optical sensors,
Laboratories,
Computer science,
Bridges,
Artificial intelligence"
A semantic-serializability based fully-distributed concurrency control mechanism for mobile multi-database systems,"The nodes of a mobile ad hoc network (MANET) represent mobile computers in which database systems (DBSs) may reside. In such an environment, we may have a mobile multidatabase system (mobile MDBS), i.e., a collection of autonomous, distributed, heterogeneous and mobile DBSs, where each mobile computer can access multiple DBSs of that collection by means of global transactions. In this paper we propose SESAMO, which is a concurrency control mechanism that does not require that a single mobile host (a node in MANET) plays the role of the centralized coordinator for all global transactions executed in the mobile MDBS. Moreover, SESAMO allows that subtransactions commit independently of the commitment of their respective global transactions, since it is based on the semantic serializability, which relaxes global serializability.","Concurrency control,
Mobile computing,
Mobile ad hoc networks,
Transaction databases,
Concurrent computing,
Database systems,
Satellite broadcasting,
Computer science,
Electronic mail,
Computer networks"
Combining matching scores in identification model,"The paper discusses a problem of combining recognition scores for different classes produced by one recognizer during one recognition attempt. This problem arises in identification problems which we define as 1:N classification problems with big or variable N. By using artificial example we show that intuitive solution of making identification decision based solely on the best matching score is frequently suboptimal. Paper presents reasons for such behavior, and draws parallels with score normalization technique used in speaker identification. Two examples of real life applications illustrate the possible benefits of properly combining recognition scores.","Pattern recognition,
Handwriting recognition,
Pattern matching,
Artificial intelligence,
Text analysis,
Speech,
Biometrics,
Venus,
Computer science,
Pattern classification"
Discriminative training of CDHMMs for maximum relative separation margin,"In this paper, we propose a new discriminative training method for estimating CDHMM (continuous density hidden Markov model) in speech recognition, based on the principle of maximizing the minimum relative multi-class separation margin. We show that the new training criterion can be formulated as a standard constrained minimax optimization problem. Then we show that the optimization problem can be solved by a GPD (generalized probabilistic descent) algorithm. Experimental results on E-set and Alphabet tasks (ISOLET database) showed that the new training criterion can achieve significant (up to 21%) error rate reduction over the popular MCE (minimum classification error) training method.",
Self-Adjusting Trust and Selection for Web Services,"Service-oriented architectures enable services to be dynamically selected and integrated at runtime, thus enabling system flexibility and adaptiveness - autonomic attributes that are key for modern business needs. However, current techniques provide no support for actually making rational selections, which are key to accomplishing autonomic behavior. We develop a multiagent framework based on an ontology for QoS and a new model of trust. The agents form an ecosystem in which they help each other select services best matching their preferences. We evaluate by simulation the agents' effectiveness in maintaining self-adjusting trust",
An active buffer management technique for providing interactive functions in broadcast video-on-demand systems,"Multicast delivery is an efficient approach to the provision of a video-on-demand (VoD) service. Interacting with the video stream is a desirable feature for users. However, it is a challenging task to provide the functionality in the multicast environment because a lot of users share multicast delivery channels. In this paper, we propose an active buffer management technique to provide interactive functions in broadcast VoD systems. In our scheme, the client can selectively prefetch segments from broadcast channels based on the observation of the play point in its local buffer. The content of the buffer is adjusted in such a way that the relative position of the play point is kept in the middle part of the buffer. Our simulations show that the active buffer management scheme can implement interactive actions through buffering with a high probability in a wide range of user interaction levels.","Video on demand,
Multimedia communication,
Streaming media,
Delay,
Digital video broadcasting,
Video sharing,
Prefetching,
Unicast,
Computer science,
Information systems"
Leap-frog packet linking and diverse key distributions for improved integrity in network broadcasts,"We present two new approaches to improving the integrity of network broadcasts and multicasts with low storage and computation overhead. The first approach is a leapfrog linking protocol for securing the integrity of packets as they traverse a network during a broadcast, such as in the setup phase for link-state routing. This technique allows each router to gain confidence about the integrity of a packet before passing it on to the next router; hence, allows many integrity violations to be stopped immediately in their tracks. The second approach is a novel key predistribution scheme that we use in conjunction with a small number of hashed message authentication codes (HMAC), which allows end-to-end integrity checking as well as improved hop-by-hop integrity checking. Our schemes are suited to environments, such as in ad hoc and overlay networks, where routers can share only a small number of symmetric keys. Moreover, our protocols do not use encryption (which, of course, can be added as an optional security enhancement). Instead, security is based strictly on the use of one-way hash functions; hence, our algorithms are considerably faster than those based on traditional public-key signature schemes. This improvement in speed comes with only modest reductions in the security for broadcasting, as our schemes can tolerate small numbers of malicious routers, provided they do not form significant cooperating coalitions.","Joining processes,
Intelligent networks,
Broadcasting,
Public key,
Computer networks,
Routing protocols,
Cryptography,
Ad hoc networks,
Computer science,
Message authentication"
Joyce: a multi-player game on one-on-one digital classroom environment for practicing fractions,"The past researches indicated that game-based learning attracts and motivates children. More and more educational games are developed for children in classroom. At the same time, a growing number of children have their own portable computing devices such as personal digital assistant (PDA), notebook, tablet PC, etc. in their life. One-on-one digital classroom environment means every student uses digital learning devices to participate learning activities. In this paper, a multi-player game system supported for one-on-one educational computing named ""Joyce"" is presented. In Joyce system, the game mechanisms are applied for learners to practice fractions, and dynamic grouping mechanism is implemented to enhance interaction of competitive environment.","Personal digital assistants,
Portable computers,
Collaborative work,
Computer science,
Logistics,
Dispatching,
Education,
Portfolios,
Computerized monitoring,
Internet"
Two-level adaptive denoising using Gaussian scale mixtures in overcomplete oriented pyramids,"We describe an adaptive denoising method for images decomposed in overcomplete oriented pyramids. Our approach integrates two kinds of adaptation: 1) a 'coarse' adaptation, where a large window is used within each subband to estimate the local signal covariance; 2) a 'fine' adaptation, which uses small neighborhoods of coefficients modelled as the product of a Gaussian and a hidden multiplier, i.e., as Gaussian scale mixtures (GSM). The former provides adaptation to local spectral features, whereas the latter adapts to local energy fluctuations. We formulate our method as a Bayes least squares estimator using spatially variant GSMs. We also discuss the importance of image representation, compare the results using two different representations with complementary features, and study the effect of merging their results. We demonstrate through simulation that our method surpasses the state-of-the-art performance, in a L/sub 2/-norm sense.","Noise reduction,
Image representation,
GSM,
Fluctuations,
Least squares approximation,
Image denoising,
Bayesian methods,
Frequency estimation,
Information processing,
Computer science"
Maximum Likelihood Set for Estimating a Probability Mass Function,"We propose a new method for estimating the probability mass function (pmf) of a discrete and finite random variable from a small sample. We focus on the observed countsâ€”the number of times each value appears in the sampleâ€”and define the maximum likelihood set (MLS) as the set of pmfs that put more mass on the observed counts than on any other set of counts possible for the same sample size. We characterize the MLS in detail in this article. We show that the MLS is a diamond-shaped subset of the probability simplex [0, 1]k bounded by at most k Ã— (k âˆ’ 1) hyper-planes, where k is the number of possible values of the random variable. The MLS always contains the empirical distribution, as well as a family of Bayesian estimators based on a Dirichlet prior, particularly the well-known Laplace estimator. We propose to select from the MLS the pmf that is closest to a fixed pmf that encodes prior knowledge. When using Kullback-Leibler distance for this selection, the optimization problem comprises finding the minimum of a convex function over a domain defined by linear inequalities, for which standard numerical procedures are available. We apply this estimate to language modeling using Zipf's law to encode prior knowledge and show that this method permits obtaining state-of-the-art results while being conceptually simpler than most competing methods.",
A novel vertical handoff strategy for integrated IEEE 802.11 WLAN/CDMA networks,"The integration of WLANs and CDMA networks has recently evolved into hot issue. We propose a vertical handoff algorithm between WLAN and CDMA network. In this algorithm, a vertical handoff is classified into handoff decision making and MIP (mobile IP) registration process. Then, a handoff initiation is decided by the received signal strength (RSS). To reduce the unnecessary handoff probability, we consider a hand-off initiation model based on the criteria of RSS. As a mechanism of network selection, we also propose a context based network selection and the corresponding algorithms between WLAN to CDMA network, based on wireless channel assignment. We focus on a handoff initiation criteria which uses both the RSS and the network selection method which uses context information such as dropping probability, blocking probability, GOS (grade of service), velocity. As a decision making criterion, velocity threshold is determined to optimize the system performance. The optimal velocity threshold is adjusted to assign available channels to the mobile stations. The proposed scheme is validated by computer simulation.","Wireless LAN,
Multiaccess communication,
Decision making,
System performance,
GSM,
Traffic control,
Computer networks,
Computer simulation,
Communications technology,
Ground penetrating radar"
Steganalysis of audio: attacking the Steghide,"In this paper, we present a steganalytic method that can reliably detect messages hidden in WAV files using the steganographic tool Steghide. The key element of the method is mining the correlation between wavelet coefficients in a short-duration (about 20ms) in each subband. This is done by performing a four-level 1D wavelet decomposition of the audio signals, using a linear predictor for the magnitude of wavelet subband coefficients to extract significant statistics features, and employing support vector machines to detect the existence of hidden messages. Experimental results indicate that the messages embedded as small as 5% of the steganographic capacity can be reliably detected.",
Performance comparison of mobile IPv6 and fast handovers for mobile IPv6 over wireless LANs,,"Wireless LAN,
Local area networks,
Delay,
Mobile computing,
Internet,
Protocols,
Computer science,
Testing,
Authentication,
Timing"
A framework for automatic and dynamic composition of personalized Web services,"This paper presents a framework that enables automatic and dynamic composition of Web services, and allows them to be displayed on virtually any mobile device. The framework proposes new context-aware techniques that make this possible. The design and implementation of a new service registry is discussed; this registry is more efficient that current techniques that involve semantics with UDDI. The CC/PP is used to aid in the compatibility of devices that need to use our system based on our personalized service finder (PSF) framework. We propose the use of output dependencies to customize the discovery process of services, and to help ensure the quality of the results.","Web services,
Vehicle dynamics,
Application software,
Hardware,
Java,
Delay,
Computer displays,
Information science,
Mobile computing,
Technological innovation"
Variants of multidimensional scaling for node localization,"Recently multidimensional scaling (MDS) has been successfully applied to the problem of node localization in ad-hoc networks, such as wireless sensor networks. The MDS-MAP method uses MDS to compute a local, relative map at each node from the distance or proximity information of its neighboring nodes. Based on the local maps and the locations of a set of anchor nodes with known locations, the absolute positions of unknown nodes in the network can be computed. We investigate several variants of MDS and their effects on the accuracy of localization in wireless sensor networks. We compare metric scaling and non-metric scaling methods, each with several different optimization criteria. Simulation results show that different optimization models of metric scaling achieve comparable localization accuracy and non-metric scaling achieves more accurate results than metric scaling for sparse networks at the expense of higher computational cost.","Multidimensional systems,
Wireless sensor networks,
Optimization methods,
Ad hoc networks,
Computer science,
Computer networks,
Computational modeling,
Computational efficiency,
Data visualization,
Cities and towns"
Consensus and dissention: theory and properties,"The Likert scale is a popular tool for assessing peoples attitude about various questions. This papers introduces two new measures, dissension and consensus, that provide a better statistic for assessing the amount of overall agreement in a population when data is extracted from the population using a Likert scale. The measures are applicable to any discrete random variable.","Random variables,
Statistical distributions,
Probability distribution,
Terrorism,
Entropy,
Computer science,
Statistics,
Data mining,
Testing,
Frequency conversion"
Clondike: Linux cluster of non-dedicated workstations,"Clusters of workstations are a promising platform for high-performance computing. Most of such clusters are built of dedicated workstations that cannot be used for other purposes. Even though these clusters offer good price/performance ratio, their costs are not negligible. On the other hand, there exist many idle workstations connected via computer networks. The idea of exploiting such idle resources is obvious. In this paper, we describe an architecture of clusters made of non-dedicated idle workstations. These clusters aim at providing a single-system-image Linux environment. The main innovative feature is that a cluster administration is separate from administration of individual workstations. We discuss issues of security, availability, performance, and administration of such clusters. We also describe a pilot implementation and give promising experimental results.","Linux,
Workstations,
High performance computing,
Costs,
Computer networks,
Computer architecture,
Hardware,
Computer science,
Availability,
Cables"
Visual Realism for the Visualization of Software Metrics,"The visualization techniques used in current software visualization frameworks make use of a limited set of graphical elements to highlight relevant aspects of a software system. Typical examples of such elements are text, simple geometric shapes and uniform color fills. Although human visual perception enables rapid processing of additional visual cues like shading and texture, they are not used. We contend that such 2D and 3D computer graphics techniques for achieving visual realism can be used to increase the information throughput of software visualization techniques. Visualization results are presented to show how treemaps, cushions, color, texture, and bump mapping can be used to visualize software metrics of hierarchically organized elements of a software system","Software metrics,
Software systems,
Data visualization,
Shape,
Computer graphics,
Humans,
Throughput,
Mathematics,
Computer science,
Visual perception"
Globally Multimodal Problem Optimization Via an Estimation of Distribution Algorithm Based on Unsupervised Learning of Bayesian Networks,"Many optimization problems are what can be called globally multimodal, i.e., they present several global optima. Unfortunately, this is a major source of difficulties for most estimation of distribution algorithms, making their effectiveness and efficiency degrade, due to genetic drift. With the aim of overcoming these drawbacks for discrete globally multimodal problem optimization, this paper introduces and evaluates a new estimation of distribution algorithm based on unsupervised learning of Bayesian networks. We report the satisfactory results of our experiments with symmetrical binary optimization problems.","unsupervised learning,
Estimation of distribution algorithms,
Bayesian networks"
Robust mixed-size placement under tight white-space constraints,"A novel and very simple correct-by-construction top-down methodology for high-utilization mixed-size placement is presented. The Polarbear algorithm combines recursive cut-size-driven partitioning with fast and scalable legalization of every placement subproblem generated by every partitioning. The feedback provided by the legalizer at all stages of partitioning improves final placement quality significantly on standard IBM benchmarks and dramatically on low-white-space adaptations of them. Compared to Feng Shui 5.1 and Capo 9.3, Polarbear is the only tool that can consistently find high-quality placements for benchmarks with less than 5% white space. With white space at 5%, Polarbear beats Capo 9.3 by 10% in average total wirelength while Feng Shui 5.1 frequently fails to find legal placements altogether. With 20% white space, POLARBEAR still beats Capo 9.3 by 1% and Feng Shui 5.1 by 1% in average total wirelength, in comparable run times.","Robustness,
White spaces,
Law,
Legal factors,
Algorithm design and analysis,
Design automation,
Timing,
Costs,
Computer science,
Partitioning algorithms"
A compiler and runtime infrastructure for automatic program distribution,"This paper presents the design and the implementation of a compiler and runtime infrastructure for automatic program distribution. We are building a research infrastructure that enables experimentation with various program partitioning and mapping strategies and the study of automatic distribution's effect on resource consumption (e.g., CPU, memory, communication). Since many optimization techniques are faced with conflicting optimization targets (e.g., memory and communication), we believe that it is important to be able to study their interaction. We present a set of techniques that enable flexible resource modeling and program distribution. These are: dependence analysis, weighted graph partitioning, code and communication generation, and profiling. We have developed these ideas in the context of the Java language. We present in detail the design and implementation of each of the techniques as part of our compiler and runtime infrastructure. Then, we evaluate our design and present preliminary experimental data for each component, as well as for the entire system.","Program processors,
Runtime,
Application software,
Java,
Concurrent computing,
Distribution strategy,
Mobile computing,
Libraries,
Distributed computing,
Computer science"
EASE: an energy-efficient in-network storage scheme for object tracking in sensor networks,,
Improved quantitation of dynamic SPECT via fully 4-D joint estimation of compartmental models and blood input function directly from projections,"Quantitative kinetic analysis of dynamic cardiac single-photon emission computed tomography (SPECT) data provides unique information that can enable improved discrimination between healthy and diseased tissue, compared to static imaging. In particular, compartmental model analysis can provide quantitative measures of myocardial perfusion, viability, and coronary flow reserve. In this work we investigate whether precision of kinetic parameter estimates is improved by additional temporal regularization provided by estimating compartmental models directly from projection data, rather than using ""semidirect"" methods that estimate time-activity curves first and then fit compartmental models to the curves. Methods are implemented to accelerate fully 4-D direct joint estimation of compartmental models for tissue volumes and B-spline time-activity curves for the blood input and other volumes that do not obey a compartmental model. Computer simulations of a dynamic /sup 99m/Tc-teboroxime cardiac SPECT study show that the additional temporal regularization provided by direct compartmental modeling results in improved precision of parameter estimates, as well as comparable or improved accuracy. Notably, for small myocardial defects the sample standard deviation of uptake and washout parameters was reduced by 17-41%. These results suggest that direct joint estimation of compartmental models and blood input function can improve quantitation of dynamic SPECT. These methods can also be applied to dynamic positron emission tomography (PET).","Blood,
Kinetic theory,
Myocardium,
Parameter estimation,
Positron emission tomography,
Image analysis,
Information analysis,
Computed tomography,
Fluid flow measurement,
Particle measurements"
Sequential pattern mining in multiple streams,"In this paper, we deal with mining sequential patterns in multiple data streams. Building on a state-of-the-art sequential pattern mining algorithm PrefixSpan for mining transaction databases, we propose MILE, an efficient algorithm to facilitate the mining process. MILE recursively utilizes the knowledge of existing patterns to avoid redundant data scanning, and can therefore effectively speed up the new patterns' discovery process. Another unique feature of MILE is that it can incorporate some prior knowledge of the data distribution in data streams into the mining process to further improve the performance. Extensive empirical results show that MILE is significantly faster than PrefixSpan. As MILE consumes more memory than PrefixSpan, we also present a solution to balance the memory usage and time efficiency in memory constrained environments.","Heart rate,
Pattern matching,
Computer science,
Transaction databases,
Steady-state,
History"
Visualizing coordination in situ,"Exploratory visualization environments allow users to build and browse coordinated multiview visualizations interactively. As the number of views and amount of coordination increases, conceptualizing coordination structure becomes more and more important for successful data exploration. Integrated metavisualization is exploratory visualization of coordination and other interactive structure directly inside a visualization's own user interface. This paper presents a model of integrated metavisualization, describes the problem of capturing dynamic interface structure as visualizable data, and outlines three general approaches to integration. Metavisualization has been implemented in improvise, using views, lenses, and embedding to reveal the dynamic structure of its own highly coordinated visualizations.","Data visualization,
Lenses,
Management information systems,
Computer science,
User interfaces,
Chromium,
Software engineering,
Software design,
Visual databases,
Environmental management"
Design of a High-Performance Switch for Circuit-Switched On-Chip Networks,"System-on-a-chip (SoC) designs provide designers to integrate dozens of heterogeneous IP blocks together by a dedicated interconnect network. The major problems in the ultra deep sub-micron technology SoC design arise from the interconnection networks, such as non-scalable global wire delay, failure to achieve global synchronization, and errors due to signal integrity issues. These problems might be mitigated by the network-on-chip (NoC) approach based on regular on-chip communication networks. In this paper, the authors propose the pre-scheduled circuit-switched network for NoC architectures. The authors have designed the switch supporting the network. Such architectures based on circuit switching with efficient buffer management can achieve guaranteed transmission latencies","Switches,
Switching circuits,
Network-on-a-chip,
System-on-a-chip,
Communication switching,
Integrated circuit interconnections,
Signal design,
Multiprocessor interconnection networks,
Wire,
Communication networks"
A platform FPGA-based hardware-software undergraduate laboratory,"Almost all universities offer introductory courses that focus on microcontroller-based systems and embedded programming. Advanced course offerings vary, and are often not available until the graduate level, leaving a gap in training undergraduates. However, courses are emerging that take advantage of new embedded development platforms that support hardware-software codesign. At Iowa State University, the Department of Electrical and Computer Engineering is developing a new upper-level design course on embedded systems design (CPRE 488) that sits between the introductory course on microcontrollers (CPRE 211) and a graduate course on system-level design (CPRE 588). CPRE 488 pulls together pedagogy from leading textbooks in embedded systems (such as Wolf and also Vahid and Givargis) and puts the concepts into an intensive laboratory incorporating platform FPGA technology. The lab utilizes Xilinx's Virtex II Pro FPGA, which includes a hard-core dedicated processor as well as FPGA fabric, allowing for a complete hardware-software system to be explored entirely within the FPGA.","Laboratories,
System-level design,
Embedded system,
Field programmable gate arrays,
Microcontrollers,
Hardware,
Education,
Timing,
Embedded computing,
Fabrics"
Fault-tolerant target detection in sensor networks,"Fault-tolerant target detection and localization is a challenging task in collaborative sensor networks. The paper introduces our exploratory work toward identifying a stationary target in sensor networks with faulty sensors. We explore both spatial and temporal dimensions for data aggregation to decrease the false alarm rate and improve the target position accuracy. To filter out extreme measurements, the median of all readings in the close neighborhood is used to approximate the local observation to the target. The sensor whose observation is a local maximum computes a position estimate at each epoch. Results from multiple epochs are combined to decrease the false alarm rate further and improve the target localization accuracy. Our algorithms have low computation and communication overheads. A simulation study demonstrates the validity and efficiency of our design.","Fault tolerance,
Object detection,
Intelligent networks,
Intelligent sensors,
Fault diagnosis,
Wireless sensor networks,
Base stations,
Computer science,
Collaborative work,
Computational modeling"
Perceptive particle swarm optimisation: an investigation,"Conventional particle swarm optimisation relies on exchanging information through social interaction among individuals. However for real-world problems involving control of physical agents (i.e., robot control), such detailed social interaction is not always possible. Recently, the perceptive particle swarm optimisation (PPSO) algorithm was proposed to mimic behaviours of social animals more closely through both social interaction and environmental interaction for applications such as robot control. In this study, we investigate the PPSO algorithm on complex function optimisation problems and its ability to cope with noisy environments.","Particle swarm optimization,
Robot control,
Working environment noise,
Insects,
Social network services,
Computer science,
Educational institutions,
Animals,
Global communication,
Optimization methods"
Critical-path and priority based algorithms for scheduling workflows with parameter sweep tasks on global grids,"Parameter-sweep has been widely adopted in large numbers of scientific applications. Parameter-sweep features need to be incorporated into grid workflows so as to increase the scale and scope of such applications. New scheduling mechanisms and algorithms are required to provide optimized policy for resource allocation and task arrangement in such a case. This paper addresses scheduling sequential parameter-sweep tasks in a fine-grained manner. The optimization is produced by pipelining the subtasks and dispatching each of them onto well-selected resources. Two types of scheduling algorithms are discussed and customized to adapt the characteristics of parameter-sweep, as well as their effectiveness has been compared under multifarious scenarios.","Scheduling algorithm,
Application software,
Sensor arrays,
Grid computing,
Filtering,
Data visualization,
Processor scheduling,
Resource management,
Laboratories,
Computer science"
FPGA-based customizable systolic architecture for image processing applications,"The present work focuses on the development of a reconfigurable systolic-based architecture for low-level image processing. The architecture is customizable providing the possibility to perform window operations for masks of 3 times3, 5 times5 and 7 times7 coefficients. A 2D systolic array of processing elements have been implemented, based on parallel modules with internal pipeline operation where every processing element can be configured according to a control word. In addition the array is provided with a group of image buffers to reduce the number of access to data memory and to extend the array capabilities allowing the possibility of chaining interconnection of multiple processing blocks. Every buffer constitutes a repository of data that can be reused for different processing blocks. Preliminary results using 640 times 480 gray level images show that window-based operations can be performed in real time, processing an image frame in 5 ms, achieving a throughput of around 3.6 GOPS","Image processing,
Computer architecture,
Systolic arrays,
Application software,
Real time systems,
Convolution,
Routing,
Buffer storage,
Field programmable gate arrays,
Computer science"
Client controlled security for Web applications,"The main contribution of this paper is an encryption system for Web applications, where the encryption is done on the client side. By a Web application we mean an application that uses a web browser as a user interface and the content is in HTML or equivalent. In our application the client creates and stores an encryption key. The data is always encrypted when in transit through the transport media, and cannot be decrypted on the server without an explicit consent of the client. Even a malicious server software cannot be used to disclose the confidential data. Furthermore, the client will detect any attempt to tamper with the encrypted data. We show how to create a Web application that uses client side encryption and key generation. Our approach delivers confidentiality, integrity, and user trust. Furthermore it doesn't require any additional hardware or any software installations on the client side","Application software,
Cryptography,
Hospitals,
Protection,
Privacy,
Costs,
Internet,
Databases,
Computer science,
User interfaces"
Evolving improved incremental learning schemes for neural network systems,"It is well known that incremental learning can often be difficult for traditional neural network systems, due to newly learned information interfering with previously learned information. In this paper, we present simulation results which demonstrate how evolutionary computation techniques can be used to generate neural network incremental learners that exhibit improved performance over existing systems.","Neural networks,
Computational modeling,
Evolutionary computation,
Artificial neural networks,
Stability,
Learning systems,
Computer science,
Training data,
Large-scale systems,
Humans"
On the monomiality of nice error bases,"Unitary error bases generalize the Pauli matrices to higher dimensional systems. Two basic constructions of unitary error bases are known: An algebraic construction by Knill that yields nice error bases, and a combinatorial construction by Werner that yields shift-and-multiply bases. An open problem posed by Schlingemann and Werner relates these two constructions and asks whether each nice error basis is equivalent to a shift-and-multiply basis. We solve this problem and show that the answer is negative. However, we find that nice error bases have more structure than one can anticipate from their definition. In particular, we show that nice error bases can be written in a form in which at least half of the matrix entries are 0.","Quantum mechanics,
Error correction codes,
Information theory,
Teleportation,
Inspection,
Engineering profession,
Computer science,
Combinatorial mathematics,
Information processing"
Gaia microserver: an extendable mobile middleware platform,"The Gaia ubiquitous computing platform currently supports mobile devices through a thin client proxy architecture. Mobile devices run a lightweight proxy client written in J2ME to join an active space. While this approach allows a wide variety of devices to interact with active spaces, it lacks the ability to use device specific functionality. This problem is addressed by combining the J2ME client with a microserver, which is a bridge from the native language to J2ME. The microserver-proxy approach enables thin clients to fully access device-specific features while respecting security through a standard interface as Gaia services.","Middleware,
Mobile computing,
Pervasive computing,
Manufacturing,
Security,
Java,
Hardware,
Computer science,
Ubiquitous computing,
Computer architecture"
Improving sensor network performance by deploying mobile sensors,"Sensor networks hold the promise of facilitating large-scale, real-time data processing in complex environments. In most existing research, sensors are assumed to be static nodes. In this paper, we propose a novel idea: to improve sensor network performance by deploying a small number of mobile sensors in a sensor network where most nodes are static sensors. Specifically, we want to increase the network coverage, provide better routing performance and better connectivity for sensor networks by using mobile sensors. We propose several effective schemes about where to move the mobile sensors, which can provide the most cost-effective approach to improve network performance. These schemes are evaluated by simulation experiments. Our simulations show that a small number of mobile sensors can significantly improve network coverage, increase throughput, reduce delay and energy consumption, and a sensor network with a small portion of mobile sensors performs better than a sensor network with more static sensors.","Chemical sensors,
Computer science,
Throughput,
Energy consumption,
Wireless sensor networks,
Monitoring,
Base stations,
Network topology,
Routing protocols,
Scattering"
How to Secure a Wireless Sensor Network,"The security of wireless sensor networks (WSNs) is a complex issue. While security research of WSNs is progressing at a tremendous pace, and many security techniques have been proposed, no comprehensive framework has so far emerged that attempts to tie the bits and pieces together to ease the implementors' adoption of the technologies. We answer the challenge by proposing a guidelines according to which WSN security can be implemented in practice.","Wireless sensor networks,
Guidelines,
Routing,
Hardware,
Energy efficiency,
Public key cryptography,
Mathematics,
Computer science,
Computer security,
Art"
Multiregion bicentric-spheres models of the head for the simulation of bioelectric phenomena,"Equations are derived for the electric potentials [electroencephalogram (EEG)] produced by dipolar sources in a multiregion bicentric-spheres volume-conductor head model. Being the equations valid for an arbitrary number of regions, our proposal is a generalization of many spherical models presented so far in literature, each of those regarded as a particular case of our multiregion model. Moreover, our approach allows considering new features of the head volume-conductor to better approximate electrical properties of the real head.","Bioelectric phenomena,
Brain modeling,
Electric potential,
Electroencephalography,
Computer science education,
Solid modeling,
Equations,
Clinical diagnosis,
Conductivity,
Scalp"
Pricing and resource allocation in computational grid with utility functions,"To solve the problem of heterogeneous demand in the grid, grid users' preferences are summarized by means of their utility functions. This paper proposes to use measurable characteristics to formulate utility functions, rather than abstract utility function used in other works. The grid user utility is defined as a function of the grid user's the resource units allocated. An optimal solution maximizes the aggregate utilities of all grid users subject to the grid resource capacity and job complete times constraints. At same time, the grid provider adjusts the unit price in order to maximize its revenue, which is measured as the sum of the individual payments. This paper proposes an iterative algorithm that computes the price and resource allocation, which is proved to converge to the optimal point. Simulation results show that our proposed algorithm is more efficient than conventional allocation scheme.","Pricing,
Resource management,
Grid computing,
Aggregates,
Time factors,
Iterative algorithms,
Costs,
Computer science,
Time measurement,
Computational modeling"
Coverage-directed test generation with model checkers: challenges and opportunities,"When using tools to automatically generate tests-suites from a specification, the selection of coverage criterion that guides the generation process is of imperative importance. In a previous study that evaluated test generation with model checking, we observed that although a coverage criterion may seem reasonable when instrumenting a model or code to measure the adequacy of a test suite, it may be unsuitable when formalized and used to guide the model checker to generate a test suite; the generated tests technically provide adequate coverage according to the formalization, but do so in a way that exercises only small portions of the system under study and finds few faults. Based on those results, we concluded that fully automated test-suite generation techniques must be pursued with great caution and that coverage criteria specifically addressing test-suite generation from formal specifications are needed. In this report, we attempt to better understand these concerns by evaluating several coverage criteria that bring together aspects from condition and control based criteria. We evaluate the fault finding capability of the criteria on a close to production flight guidance system and discuss the opportunities and challenges that arise from the increased use of fully automated model-based testing.","Automatic testing,
System testing,
Logic testing,
Software testing,
Instruments,
Automatic control,
Application software,
Aerospace electronics,
NASA,
Computer science"
A tree-decomposition approach to protein structure prediction,"This paper proposes a tree decomposition of protein structures, which can be used to efficiently solve two key subproblems of protein structure prediction: protein threading for backbone prediction and protein side-chain prediction. To develop a unified tree-decomposition based approach to these two subproblems, we model them as a geometric neighborhood graph labeling problem. Theoretically, we can have a low-degree polynomial time algorithm to decompose a geometric neighborhood graph G=(V,E) into components with size O(|V|/sup 2/spl bsol/3/ log|V|). The computational complexity of the tree-decomposition based graph labeling algorithms is O(|V|/spl Delta//sup tw+1/]) where /spl Delta/ is the average number of possible labels for each vertex and tw(=O(|V|/sup 2/3/ log|V|)) the tree width of G. Empirically, tw is very small and the tree-decomposition method can solve these two problems very efficiently. This paper also compares the computational efficiency of the tree-decomposition approach with the linear programming approach to these two problems and identifies the condition under which the tree-decomposition approach is more efficient than the linear programming approach. Experimental result indicates that the tree-decomposition approach is more efficient most of the time.","Proteins,
Tree graphs,
Spine,
Linear programming,
Labeling,
Predictive models,
Mathematics,
Computer science,
Solid modeling,
Polynomials"
Bitwidth-aware scheduling and binding in high-level synthesis,"Many high-level description languages, such as C/C++ or Java, lack the capability to specify the bitwidth information for variables and operations. Synthesis from these specifications without bitwidth analysis may introduce wasted resources. Furthermore, conventional high-level synthesis techniques usually focus on uniform-width resources, thus they cannot obtain the full resource savings even with bitwidth information. This work develops a bitwidth-aware synthesis flow, including bitwidth analysis, scheduling and binding, and register allocation and binding, to exploit the multi-bitwidth nature of operations and variables for area-efficient designs. We also develop lower bound estimation to evaluate the efficiency of our proposed solutions for register allocation and binding. The flow is implemented in the MCAS synthesis system (Cong et al., 2004). Experimental results show that our proposed bitwidth-aware synthesis flow reduces area by 36% and wire-length by 52% on average compared to the uniform-width MCAS flow, while achieving the same performance.","High level synthesis,
Productivity,
Costs,
High level languages,
Hardware,
Logic devices,
Processor scheduling,
Computer science,
Partial response channels,
Java"
Facilitating the implementation and evolution of business rules,"Many software systems implement, amongst other things, a collection of business rules. However, the process of evolving the business rules associated with a system is both time consuming and error prone. In this paper, we propose a novel approach to facilitating business rule evolution through capturing information to assist with the evolution of rules at the point of implementation. We analyse the process of rule evolution, in order to determine the information that must be captured. Our approach allows programmers to implement rules by embedding them into application programs (giving the required performance and genericity), while still easing the problems of evolution.","Programming profession,
Software systems,
Software maintenance,
Computer science,
Computer errors,
Information analysis,
Application software,
Business communication,
Database systems,
Engines"
Model-based approaches for predicting gait changes over time,"Interest in automated biometrics continues to increase, but has little consideration of time which are especially important in surveillance and scan control. This paper deals with a problem of recognition by gait when time-dependent covariates are added, i.e. when 6 or 12 months have passed between recording of the gallery and the probe sets, and in some cases some extra covariates present as well. We have shown previously how recognition rates fall significantly when data is captured between lengthy time intevals. Under the assumption that it is possible to have some subjects from the probe for training and that similar subjects have similar changes in gait over time, we suggest predictive models of changes in gait due both to time and now to time-invariant covariates. Our extended time-dependent predictive model derives high recognition rates when time-dependent or subject-dependent covariates are added. However it is not able to cope with time-invariant covariates, therefore a new time-invariant predictive model is suggested to accommodate extra covariates. These are combined to achieve a predictive model which takes into consideration all types of covariates. A considerable improvement in recognition capability is demonstrated, showing that changes can be modelled successfully by the new approach.","Predictive models,
Biological system modeling,
Probes,
Face recognition,
Databases,
Aging,
Surveillance,
Biometrics,
Humans,
Computer science"
An adaptive model of virtual enterprise based on dynamic web service composition,"With the development of Web services related technologies, the enterprise business systems can be encapsulated through Web services. The service oriented architecture can be adopted for the enterprise business processes. Specific interfaces are introduced between the Web services of different enterprises for data exchanging to achieve the cross-enterprise systems integration. Different enterprises can be organized into virtual enterprise through Web services to share resources and achieve win-win. This paper introduces an adaptive model of virtual enterprise based on dynamic composition of Web services (DCWS-VE). A service execution plan will be dynamically built to aim at the target of the virtual enterprise through the composition of Web services. The development and deployment of Web services is separated to support the dynamically deploying and binding of the Web service at run time. The members in the virtual enterprise will be located through the dynamic Web service discovery based on DAML+OIL logic reasoning, and selected through the dynamic Web services negotiation with multi-steps protocol to organize the virtual enterprise at run time. Electronic contract is adopted in the organizing process of virtual enterprise to achieve the stable running","Virtual enterprises,
Web services,
Educational institutions,
Computer science,
Service oriented architecture,
Microstrip,
Logic,
Protocols,
Contracts,
Organizing"
The use of pre-evaluation phase in dynamic CMOS logic,"Dynamic logic families have been shown to offer performance advantages over traditional CMOS logic. Their operation is based on the use of a clock signal that provides two operation phases: the precharge phase and evaluation phase. The precharge phase is setting the circuit at a predefined initial state while the actual logic response is determined during the evaluation phase. In this paper we examine potential advantages when an additional phase, called pre-evaluation, is introduced. During this phase a restricted voltage swing occurs depending on the desired outcome. This voltage swing is amplified during the final evaluation in order to produce the final logic response. By restricting the required voltage swing at internal logic nodes (especially in case of those presenting high capacitance) we are able to achieve higher performance coupled with reduced power consumption.","CMOS logic circuits,
Voltage,
Logic circuits,
Clocks,
Logic design,
Capacitance,
Coupling circuits,
Energy consumption,
Very large scale integration,
Computer science"
Head-to-TOE Evaluation of High-Performance Sockets over Protocol Offload Engines,"Despite the performance drawbacks of Ethernet, it still possesses a sizable footprint in cluster computing because of its low cost and backward compatibility to existing Ethernet infrastructure. In this paper, we demonstrate that these performance drawbacks can be reduced (and in some cases, arguably eliminated) by coupling TCP offload engines (TOEs) with 10-Gigabit Ethernet (10GigE). Although there exists significant research on individual network technologies such as 10GigE, InfiniBand (IBA), and Myrinet; to the best of our knowledge, there has been no work that compares the capabilities and limitations of these technologies with the recently introduced 10GigE TOEs in a homogeneous experimental testbed. Therefore, we present performance evaluations across 10GigE, IBA, and Myrinet (with identical cluster-compute nodes) in order to enable a coherent comparison with respect to the sockets interface. Specifically, we evaluate the network technologies at two levels: (i) a detailed micro-benchmark evaluation and (ii) an application-level evaluation with sample applications from different domains, including a bio-medical image visualization tool known as the Virtual Microscope, an iso-surface oil reservoir simulator, a cluster file-system known as the parallel virtual file-system (PVFS), and a popular cluster management tool known as Ganglia. In addition to 10GigE's advantage with respect to compatibility to wide-area network infrastructures, e.g., in support of grids, our results show that 10GigE also delivers performance that is comparable to traditional high-speed network technologies such as IBA and Myrinet in a system-area network environment to support clusters and that 10GigE is particularly well-suited for sockets-based applications","Sockets,
Protocols,
Engines,
Ethernet networks,
Costs,
Testing,
Visualization,
Microscopy,
Petroleum,
Hydrocarbon reservoirs"
Service-oriented design with aspects (SODA),"In this paper, we introduce a service-oriented design method by integrating the concept of aspects, which is called service-oriented design with aspects (SODA) in order to utilize services and aspects as fundamental and abstract elements in the design phase of software lifecycle. The service model is represented in the structural and behavior views using UML with its extension mechanism and Petri net respectively. Aspects are used to capture service-specific concerns required for delivering high-quality and user-friendly services. By weaving services and aspects, we can generate various versions of a service system as well as their Petri net based service semantics that also facilitates the verification of service design results. We exemplify a service design result of SODA by applying it to a supply-chain management application.","Design methodology,
Unified modeling language,
Computer science,
Software design,
Weaving,
Distributed computing,
Quality of service,
Security,
Context-aware services,
Petri nets"
High performance communication between parallel programs,"We present algorithms for high performance communication between message-passing parallel programs, and evaluate the algorithms as implemented in InterComm. InterComm is a framework to couple parallel programs in the presence of complex data distributions within a coupled application. Multiple parallel libraries and languages may be used in the different programs of a single coupled application. The ability to couple such programs is required in many emerging application areas, such as complex simulations that model physical phenomena at multiple scales and resolutions, and image data analysis applications. We describe the new algorithms we have developed for computing inter-program communication patterns. We present experimental results showing the performance of various algorithmic tradeoffs, and also compare performance against an earlier system.","Libraries,
Wind,
Chaotic communication,
Adaptive arrays,
Computer science,
Educational institutions,
Computational modeling,
Analytical models,
Image resolution,
Data analysis"
Machine translation in the year 2004,"Machine translation (MT) accuracy has recently increased, due to better techniques and to the availability of larger parallel training sets. Statistical MT systems are now able to translate across a wide variety of language pairs. This paper covers the basic elements of state-of-the-art, statistical MT, including modeling, decoding, evaluation, and data preparation.","Decoding,
Humans,
NIST,
Page description languages,
Resumes,
Security,
Councils,
Computer science,
Viterbi algorithm,
Dictionaries"
Intersection characteristics of end-to-end Internet paths and trees,"This paper focuses on understanding the scale and the distribution of ""state overhead'' (briefly load) that is incurred on the routers by various value-added network services, e.g., IP multicast and IP traceback. This understanding is essential to developing appropriate mechanisms and provisioning resources so that the Internet can support such value-added services in an efficient and scalable manner. We mainly consider the number of end-to-end paths or trees intersecting at a router to represent the amount of state overhead at that router. Hence, we analyze the router-level intersection characteristics of end-to-end Internet paths or trees to approximate the state overhead distribution in the Internet. For the reliability of our analysis, a representative, end-to-end router-level Internet map is essential. Although several maps are available, they are at best insufficient for our analysis. Therefore, in the first part of our work, we exert a measurement study to obtain a large size end-to-end router-level map conforming to our constraints. In the second part, we conduct various experiments using our map and shed some light on the scale and distribution of state overhead of value-added Internet services in both unicast and multicast environments.","Web and internet services,
Diffserv networks,
Computer science,
Size measurement,
Unicast,
Spine,
IP networks,
Inductors,
Proposals,
Topology"
Multi-Planar Projection by Fixed-Center Pan-Tilt Projectors,"We describe a new steerable projector, whose projection center precisely corresponds with its rotation center, which we call a ""fixed-center pan-tilt (FC-PT) projector."" This mechanism allows it be set up more easily to display graphics precisely on the planes in the environment than for other steerable projectors; wherever we would like to display graphics, all we have to do are locating the FC-PT projector in the environment, and directing it to the corners of the planes whose 2D sizes have been measured. Moreover, as the FC-PT projector can recognize automatically whether each plane is connected to others, it can display visual information that lies across the boundary line of two planes in a similar way to a paper poster folded along the planes.",
Ant routing algorithm for mobile ad-hoc networks based on adaptive improvement,"A mobile ad-hoc network is a collection of mobile nodes without any existing infrastructure or central administrator. A lot of research work has been developed to find a path between end points, which is aggravated through the flexible node mobility. In this paper, an ant colony algorithm adaptive improvement on routing is presented (ARAAI). It is based on swarm intelligence and especially on the ant colony based meta-heuristic. Considering the stagnation behavior of ant colony algorithm, the method of adaptive parameters coordination is put forward to construct a globally optimizing algorithm. The ns-2 simulation results show the routing protocol is highly efficient and scalable comparing with existing AODV and DSR protocol.","Ad hoc networks,
Adaptive systems,
Routing protocols,
Network topology,
Computer science,
Computer networks,
Software engineering,
Mobile computing,
Mobile communication,
Helium"
Adapting to different needs in different locations: handheld computers in university education,"Educational use of handheld computers is becoming more common. It is therefore important to examine differences between standard educational interactions, and possibilities offered by handheld devices. Handheld computers allow the opportunity to study at times and locations where individualised interactions would not normally be possible or convenient. Conditions in such locations may differently affect a user's choice of task or ability to carry out an activity successfully. This paper introduces two adaptive systems: (1) a system that adapts to learner knowledge as in a standard intelligent tutoring system, but also to time availability for study and location-related features of concentration level and likelihood of interruption; (2) a system providing easy access to commonly used files, applications and tasks, according to location of use.","Handheld computers,
Computer science education,
Educational technology,
Intelligent systems,
Appropriate technology,
Context-aware services,
Art,
Adaptive systems,
Infrared sensors,
Infrared detectors"
Practical and accurate low-level pointer analysis,"Pointer analysis is traditionally performed once, early in the compilation process, upon an intermediate representation (IR) with source-code semantics. However, performing pointer analysis only once at this level imposes a phase-ordering constraint, causing alias information to become stale after subsequent code transformations. Moreover, high-level pointer analysis cannot be used at link time or run time, where the source code is unavailable. This paper advocates performing pointer analysis on a low-level intermediate representation. We present the first context-sensitive and partially flow-sensitive points-to analysis designed to operate at the assembly level. As we will demonstrate, low-level pointer analysis can be as accurate as high-level analysis. Additionally, our low-level pointer analysis also enables a quantitative comparison of propagating high-level pointer analysis results through subsequent code transformations, versus recomputing them at the low level. We show that, for C programs, the former practice is considerably less accurate than the latter.","Performance analysis,
Information analysis,
Scheduling,
Algorithm design and analysis,
Bridges,
Computer science,
Assembly,
Optimizing compilers,
Tail,
Computational complexity"
Resource Co-Allocation : A Complementary Technique that Enhances Performance in Grid Computing Environment,"This paper introduces an Availability Check technique (ACT) as a complementary technique to most resource coallocation protocols in the literature. For a given resource co-allocation protocol, ACT tries to reduce the conflicts that happen between co-allocators when they try to allocate multiple resources simultaneously. In ACT, each job checks for the availability state of required resources and gets informed with updates each time one of the resources availability state changes until all the resources become available. Once all required resources become available a job starts applying the given resource co-allocation protocol. Two co-allocation protocols: All-or-Nothing (AONP) and Order-based Deadlock Prevention (ODP2) Protocols are chosen to be the case studies to simulate the proposed technique (ACT). To simulate ACT, each job is allowed to allocate from 1 to 5 different types of resources simultaneously using a uniform distribution. The results show that applying ACT to one of the two protocols outperform the original one. The resource utilization is improved by up to 34% and 41% for AONP and ODP2, respectively. The job response time is improved by up to 13% and 8% for AONP and ODP2, respectively. And the communication overhead is improved by up to 96% and 94% for AONP and ODP2, respectively. Also, applying ACT to AONP represents the most scalable and fully distributed scheme that outperforms original schemes (AONP and ODP2) and competes well with all other schemes presented in this paper.","Grid computing,
Protocols,
Resource management,
Availability,
Collaboration,
System recovery,
Web and internet services,
Permission,
Computer science,
Delay"
On the use of rule-sharing in learning classifier system ensembles,"This paper presents an investigation into exploiting the population-based nature of learning classifier systems for their use within highly-parallel systems. In particular, the use of simple accuracy-based learning classifier systems within the ensemble machine approach is examined. Results indicate that inclusion of a rule migration mechanism inspired by parallel genetic algorithms is an effective way to improve learning speed","Machine learning,
Data mining,
Machine learning algorithms,
Parallel processing,
Genetic algorithms,
Production systems,
Computer science,
Data analysis,
Evolutionary computation,
Large-scale systems"
Design concept of a human-like robot head,"For humanoid robots able to assist humans in their daily life, the capability for adequate interaction with human operators is a key feature. If one considers that more than 60% of human communication is conducted non-verbally (by using facial expressions and gestures), an important research topic is how interfaces for this non-verbal communication can be developed. To achieve this goal, several robotic heads have been designed. However, it remains unclear how exactly such a head should look like and what skills it should have to be able to interact properly with humans. This paper describes an approach that aims at answering some of these design choices. Based on parameters obtained from a simulation system that was used to test facial expressions, the design of a human-like head developed at the University of Kaiserslautern is described. Additionally, the mechatronical design of the head and the accompanying neck joint are given. Finally, a real-time capable method for image based face detection is presented, which is a basic ability needed for interaction with humans","Humanoid robots,
Head,
Human robot interaction,
Robot sensing systems,
Computer science,
System testing,
Face detection,
Skin,
Muscles,
Neck"
Policy Based Adaptive Services for Mobile Commerce,"In this paper we describe a novel adaptation architecture as well as a process for the development of policy based adaptive services for mobile commerce. Our architecture is based on three basic requirements and defines the four core elements: context, policies, policy decision point and policy enforcement point. The proposed approach is based on the reuse and adaptation of existing and matured standards, APIs and middleware for representing context information, usage of policies for reasoning and for the communication between the involved parties. Our aim is to present a simple architecture taking as much as possible available work and software into account to support the rapid development of context aware mobile services. Furthermore we present a novel methodology for the definition of context information and policies that is supported by a new UML based diagram and a module pipeline. We show the feasibility of the architecture as well as of the process based on a prototype which implements a typical scenario for an adaptive mobile service","Business,
Computer architecture,
Middleware,
Context,
Context-aware services,
Resource description framework,
Mobile handsets,
Artificial intelligence,
Expert systems,
Engines"
Dig-CoLa: directed graph layout through constrained energy minimization,"We describe a new method for visualization of directed graphs. The method combines constraint programming techniques with a high performance force directed placement (FDP) algorithm so that the directed nature of the graph is highlighted while useful properties of FDP - such as emphasis of symmetries and preservation of proximity relations - are retained. Our algorithm automatically identifies those parts of the digraph that contain hierarchical information and draws them accordingly. Additionally, those parts that do not contain hierarchy are drawn at the same quality expected from a nonhierarchical, undirected layout algorithm. An interesting application of our algorithm is directional multidimensional scaling (DMDS). DMDS deals with low dimensional embedding of multivariate data where we want to emphasize the overall flow in the data (e.g. chronological progress) along one of the axes.","Constraint optimization,
Electronic mail,
Minimization methods,
Computer science,
Software engineering,
Visualization,
Multidimensional systems,
Chromium,
Numerical analysis,
Algorithm design and analysis"
Enabling High Data Availability in a DHT,"Many decentralized and peer-to-peer applications require some sort of data management. Besides P2P file-sharing, there are already scenarios (e.g. BRICKS project) that need management of finer-grained objects including updates and, keeping them highly available in very dynamic communities of peers. In order to achieve project goals and fulfill the requirements, a decentralized/P2P XML storage on top of a DHT (distributed hash table) overlay has been proposed. Unfortunately, DHTs do not provide any guarantees that data will be highly available all the time. A self-managed approach is proposed where availability is stochastically guaranteed by using a replication protocol. The protocol recreates periodically missing replicas dependent on the availability of peers. We are able to minimize generated costs for requested data availability. The protocol is fully decentralized and adapts itself on changes in community maintaining the requested availability. Finally, the approach is evaluated and compared with replication mechanisms embedded in other decentralized storages","Availability,
Protocols,
XML,
Costs,
Peer to peer computing,
Cultural differences,
Global communication,
Management information systems,
Computer science,
Application software"
Polyphonic Audio Key Finding Using the Spiral Array CEG Algorithm,"Key finding is an integral step in content-based music indexing and retrieval. In this paper, we present an O(n) real-time algorithm for determining key from polyphonic audio. We use the standard Fast Fourier Transform with a local maximum detection scheme to extract pitches and pitch strengths from polyphonic audio. Next, we use Chew's Spiral Array Center of Effect Generator (CEG) algorithm to determine the key from pitch strength information. We test the proposed system using Mozart's Symphonies. The test data is audio generated from MIDI source. The algorithm achieves a maximum correct key recognition rate of 96% within the first fifteen seconds, and exceeds 90% within the first three seconds. Starting from the extracted pitch strength information, we compare the CEG algorithm's performance to the classic Krumhansl-Schmuckler (K-S) probe tone profile method and Temperley's modified version of the K-S method. Correct key recognition rates for the K-S and modified K-S methods remain under 50% in the first three seconds, with maximum values of 80% and 87% respectively within the first fifteen seconds for the same test set. The CEG method consistently scores higher throughout the fifteen-second selections.",
Using multiple communication channels for efficient data dissemination in wireless sensor networks,"This paper presents McTorrent and McSynch, two multichannel sensor network protocols for data dissemination. Both protocols are designed to take advantage of the spatial multiplexing properties of the half-duplex radio transceivers available on the current generation of sensor nodes. McTorrent is used for reliable end-to-end dissemination of a large data object. Compared to existing protocols, we show that McTorrent significantly reduces the amount of time required to propagate a large data object throughout a sensor network. McSynch is used to achieve data object synchronization within a local cluster of nodes. By using a scheduled channel access approach and an appropriate number of transmission channels, McSynch can significantly reduce the amount of time required to update a local cluster. We also describe our experiences implementing a multichannel system, and report on lessons learned for channel and frequency settings","Communication channels,
Intelligent networks,
Wireless sensor networks,
Access protocols,
Frequency synchronization,
Switches,
Computer science,
Wireless application protocol,
Radio transceivers,
Energy management"
Design and Implementation of Low Power Hardware Encryption for Low Cost Secure RFID Using TEA,"This paper discusses the design and implementation of a hardware encryption core for low cost RFID using TEA algorithm. Low cost RFIDs have stringent requirements in terms of cost related to area and power consumption, making conventional encryption unsuitable. This paper proposes the use of TEA algorithm for medium security systems. Three new implementations based on multiple, single, and digit-serial adders are designed and evaluated for their suitability. It is found that the multiple-adder design meets the requirements with the lowest power consumption. Based on 0.35 mum CMOS implementation, the TEA core has an area of 0.21 mm and is estimated to consume 7.37 muW of power","Hardware,
Cryptography,
Costs,
Radiofrequency identification,
Power system security,
ISO standards,
Privacy,
Authentication,
Algorithm design and analysis,
Energy consumption"
Mobile contagion: simulation of infection & defense,"For worms with known signatures, properly configured firewalls can prevent infection of a network from the outside. However, as several recent worms have shown, portable computers provide worms with an entry point into such networks, since these computers are connected behind the firewall. Once inside, the firewall provides no protection against the worm's further spread. Wireless networks are particularly dangerous in this regard, as the act of connection is often invisible, and improperly configured wireless networks allows anyone within radio range to connect. In this paper, we use real data on a large-scale wireless deployment to analyze the speed with which a worm could spread if it used only this propagation vector. We discuss several possible solutions and provide analysis on how much protection those solutions would provide.","Computer worms,
Wireless networks,
Computer science,
Protection,
Portable computers,
Computer networks,
Computational modeling,
Internet,
Information filtering,
Information filters"
A cellular multi-objective genetic algorithm for optimal broadcasting strategy in metropolitan MANETs,"Mobile ad-hoc networks (MANETs) are composed of a set of communicating devices, which are able to spontaneously interconnect without any pre-existing infrastructure. In such scenario, broadcasting becomes an operation of capital importance for the own existence and operation of the network. Optimizing a broadcast strategy in MANETs is a multi-objective problem accounting for three goals: reaching as many stations as possible, minimizing the network utilization, and reducing the makespan. In this paper, we study the fine-tuning of broadcast strategies by using a cellular multi-objective genetic algorithm (cMOGA) that computes a Pareto front of the solutions to empower a human designer with the ability of choosing the preferred configuration for the network. We define two formulations of the problem, one with three objectives and another one with two objectives plus a constraint. Our experiments using a complex and realistic MANET simulator reveal that using cMOGA is a promising approach to solve the optimum broadcast problem.","Genetic algorithms,
Broadcasting,
Mobile ad hoc networks,
Pareto optimization,
Ad hoc networks,
Cellular networks,
Evolutionary computation,
Intelligent networks,
Computer science,
Mobile computing"
Fuzzy support vector machines based on FCM clustering,"Fuzzy support vector machines based on fuzzy c-means clustering are proposed in this paper. They apply the fuzzy c-means clustering technique to each class of the training set. During the clustering with a suitable fuzziness parameter q, the more important samples, such as support vectors, become the cluster centers respectively. All the cluster centers generated by fuzzy c-means clustering are selected as the representations of the other similar samples close to the cluster centers. The new training set consisting of all the centers is used to form fuzzy support vector machines. Experimental results on the benchmark data sets show that the proposed fuzzy support vector machines need less training data and less quadratic programming time compared with the conventional fuzzy support vector machines, and their classification accuracy rates are acceptable.","Support vector machines,
Support vector machine classification,
Training data,
Fuzzy sets,
Constraint optimization,
Computer science,
Quadratic programming,
Learning systems,
Risk management,
Pattern recognition"
Efficient SOAP binding for mobile Web services,"Existing Web services rely on HTTP and TCP as the underlying transport protocols for SOAP messaging. While these protocols provide a number of benefits, including being able to pass through firewalls and are universally supported across different platforms, they were designed for wired networks with high bandwidth, low latency and low error rate transmissions. Due to the variability of wireless channels however, these assumptions do not hold in wireless environments. In this paper, we investigate the performance of HTTP and TCP as transport protocols for SOAP in wireless environments. Through extensive testing, we show that SOAP-over-HTTP and SOAP-over-TCP are inefficient and lead to high latency and transmission overhead for wireless applications. To overcome these limitations, we study the use of UDP as a binding protocol for SOAP. The results obtained are promising and show that SOAP-over-UDP provides throughput that is ten times higher compared to SOAP-over-HTTP in a wireless setting. Furthermore, using UDP to transport SOAP messages reduces transmission overhead by more than 50% compared to SOAP-over-HTTP. Finally, to illustrate where UDP binding can be useful, example applications are also described in this paper","Simple object access protocol,
Web services,
Transport protocols,
TCPIP,
Delay,
Bandwidth,
Application software,
Mobile computing,
Propagation losses,
Computer science"
A New Method for Feature Subset Selection for Handling Classification Problems,"In this paper, we present a new method for dealing with feature subset selection for handling classification problems. We discriminate numeric features to construct the membership function of each fuzzy subset of each feature. Then, we select the feature subset based on the proposed fuzzy entropy measure with boundary samples. The proposed feature subset selection method cam select relevant features from sample data to get higher average classification accuracy rates than the ones selected by the existing methods","Entropy,
Fuzzy sets,
Filters,
Classification algorithms,
Computer science,
Design methodology,
Algorithm design and analysis,
Gain measurement,
Heuristic algorithms,
Genetic algorithms"
A distributed reputation management scheme for mobile agent based e-commerce applications,"This paper examines current approaches used in reputation management and defines five basic metrics for the evaluation of reputation of trusted third party (TTP) hosts employed in mobile agent-based e-commerce applications. By taking these metrics into account, a new distributed reputation management scheme is proposed for quantifying and managing the reputation of the TTP-hosts based on a transaction-based feedback system. The scheme exhibits several interesting features not seen in previous work: firstly, it offers better efficiency and robustness for an application that employs it, and secondly, it credits/penalizes a TTP-host according to its transactional behavior, the transaction value and the reputation of the source of feedback. In this way, TTP-hosts may be deterred from misbehaving.","Mobile agents,
Protocols,
Application software,
Mobile computing,
Electronic commerce,
Feedback,
Robustness,
Information security,
Computer science,
Business"
Efficient broadcasting and gathering in wireless ad-hoc networks,"This paper considers the problem of broadcasting and information gathering in wireless ad-hoc networks, i.e. in wireless networks without any infrastructure in addition to the mobile hosts. Broadcasting is the problem of sending a packet from a source node in the network to all other nodes in the network. Information gathering is the problem of sending one packet each from a subset of the nodes to a single sink node in the network. Most of the proposed theoretical wireless network models oversimplify wireless communication properties. We use a model that takes into account that nodes have different transmission and interference ranges, and we propose algorithms in this model that achieve a high time and work-efficiency. We present algorithms for broadcasting a single or multiple message(s), and for information gathering. Our algorithms have the advantage that they are very simple and self-stabilizing, and would therefore even work in a dynamic environment. Also, our algorithms require only a constant amount of storage at any host. Thus, our algorithms can be used in wireless systems with very simple devices, such as sensors.","Broadcasting,
Intelligent networks,
Ad hoc networks,
Interference,
Wireless networks,
Wireless communication,
Computer science,
Wireless sensor networks,
Packet radio networks,
Algorithm design and analysis"
Analysis of Reed-Solomon Coding Combined with Cyclic Redundancy Check in DVB-H link layer,"DVB-H, which is an amendment of DVB-T, offers reliable high data rate reception for mobile handheld and battery-powered devices. A link layer with error correction was defined to work on top of the DVB-T physical layer. The DVB-H standard suggests to use Reed-Solomon coding combined with CRC-32 error detection as the link layer FEC. This paper investigates the performance of the proposed error correction scheme, which is first analyzed theoretically and then by computer simulations. Results are compared to conventional Reed-Solomon decoding without utilizing CRC-32 error detection to illustrate drawbacks of the decoding solution in DVB-H standard","Reed-Solomon codes,
Cyclic redundancy check codes,
Digital video broadcasting,
Error correction,
Computer errors,
Decoding,
Physical layer,
Forward error correction,
Performance analysis,
Computer simulation"
Adaptive Equalization for Indoor Visible-Light Wireless Communication Systems,"White LED is considered as a strong candidate for the future lighting technology. We have proposed an optical wireless communication system that employs white LEDs for indoor wireless networks. In this system, LED is not only used as a lighting device, but also used as a communication device. The system has large optical power and large emission characteristic at transmitter to function as lighting device. Therefore, the BER performance is degraded excessively due to the effects of inter-symbol interference. In this paper, we evaluate the performance of adaptive equalizer with LMS algorithm in visible-light wireless environment. And we show the interval of training sequence for channel estimation","Adaptive equalizers,
Wireless communication,
Optical transmitters,
Light emitting diodes,
LED lamps,
Stimulated emission,
Optical fiber networks,
Wireless networks,
Optical devices,
Bit error rate"
An architecture for developing aspect-oriented Web services,"Current Web services approaches have many limitations, especially with description, discovery and integration mechanisms. In this paper we present a novel software architecture called aspect-oriented Web services (AOWS) which addresses these problems. AOWS uses descriptions of cross-cutting concerns between Web services to give more complete descriptions of services, supporting richer dynamic discovery and seamless integration. We describe our architecture, a formal specification of it and an implementation using .NET Web services technology.","Service oriented architecture,
Web services,
Computer architecture,
Formal specifications,
Security,
Sun,
Computer science,
Software architecture,
Software systems,
XML"
Mobile code enabled Web services,"A primary benefit of Web services is that they provide a uniform implementation-independent mechanism for accessing distributed services. Building and deploying such services do not benefit from the same advantages, however. Different Web services containers are implemented in different programming languages, with different constraints and requirements placed on the programmer. Moreover, client side programmers must use the Web service interface specified by the service developer. Therefore, the kinds of applications and uses for a Web service are unnecessarily restrictive, constrained by the granularity of access defined by the interface and by the characteristics of the service functions. This paper describes an approach that addresses both of these drawbacks by enabling Web service containers with the ability to accept new mobile code on the fly, and to run it within the containers, providing direct local access to the containers' other services. The code can be specified in a small simple language (a subset of C), and translated and passed to the container in a common XML-based intermediate language called X#. This approach effectively removes the dependence on any single implementation environment. Our prototype implementation for two different containers demonstrates the feasibility of the approach, which represents a first step toward write-once deploy-anywhere Web services.","Web services,
Containers,
Programming profession,
XML,
Access protocols,
Simple object access protocol,
Computer science,
Computer languages,
Software prototyping,
Prototypes"
Kernel-density-based clustering of time series subsequences using a continuous random-walk noise model,"Noise levels in time series subsequence data are typically very high, and properties of the noise differ front those of white noise. The proposed algorithm incorporates a continuous random-walk noise model into kernel-density-based clustering. Evaluation is done by testing to what extent the resulting clusters are predictive of the process that generated the time series. It is shown that the new algorithm not only outperforms partitioning techniques that lead to trivial and unsatisfactory results under the given quality measure, but also improves upon other density-based algorithms. The results suggest that the noise elimination properties of kernel-density-based clustering algorithms can be of significant value for the use of clustering in preprocessing of data.","Clustering algorithms,
Partitioning algorithms,
Data mining,
Noise level,
Testing,
Time measurement,
Computer science,
White noise,
Density measurement,
Signal to noise ratio"
VIPS - a highly tuned image processing software architecture,"This paper describes the VIPS image processing library and user-interface. VIPS is used in many museums and galleries in Europe, America and Australia for image capture, analysis and output. VIPS is popular because it is free, cross-platform, fast, and can manage images of unlimited size. It also has good support for color, an important feature in this sector. Its architecture will be illustrated through examples of its use in a range of museum-driven applications. VIPS is free software distributed under the LGPL license.","Image processing,
Software architecture,
Read-write memory,
Computer architecture,
Application software,
Kernel,
Kirk field collapse effect,
Computer science,
Software libraries,
Europe"
Interpretation and Prediction of Loop Gain Characteristics For Switching Power Converters Loaded with General Load Subsystem,"This paper presents a qualitative method to analyze and interpret loop gain characteristics of switching power converters loaded with a practical load subsystem. Effects of the load subsystem's input impedance on the loop gain of switching power converters are investigated. Based on the analysis results, a simple qualitative method is established that facilitates the interpretation and prediction of the loop gain characteristics for switching power converters. Theoretical predictions of the paper are supported by both computer simulations and experimental results",
Truth-value based interval neutrosophic sets,"Neutrosophic set is a part of neutrosophy, which studies the origin, nature, and scope of neutralities, as well as their interactions with different ideational spectra. Neutrosophic set is a powerful general formal framework that has been recently proposed. However, neutrosophic set needs to be specified from a technical point of view. To this effect, we define the set-theoretic operators on an instance of neutrosophic set, we call it truth-value based interval neutrosophic set. We provide various properties of truth-value based interval neutrosophic sets, which are connected to the operations and relations over truth-value based interval neutrosophic sets.","Fuzzy sets,
Voting,
Computer science,
Uncertainty,
Tellurium,
Fuzzy logic,
Hybrid intelligent systems,
Fuzzy systems,
Expert systems,
Sensor fusion"
A new active contour method based on elastic interaction,"Image segmentation is defined as partitioning an image into non-overlapping regions based on the intensity or texture. The active contour methods provide an effective way for segmentation, in which the boundary of an object (usually with large image gradient value) is detected by an evolving curve. But, these methods have limitations due to the fact that real images may have objects with complex geometric structures and shapes, and are often corrupted by noise. Developing more robust and accurate active contour methods has been an active research area since the idea of the methods was proposed. In this paper, we propose a new active contour method and apply the method to medical image segmentation. This new method uses a long-ranged interaction between image boundaries and the moving curves, which is inspired by the elastic interaction between line defects in solids (dislocations). The new method is more efficient and effective, especially in detecting thin, weak and blurred structures such as the images of blood vessels.",
Hardness of approximating the closest vector problem with pre-processing,"We show that, unless NP/spl sube/DTIME(2/sup poly log(n)/) the closest vector problem with pre-processing, for /spl lscr//sub p/ norm for any p /spl ges/ 1, is hard to approximate within a factor of (log n)/sup 1/p - /spl epsi//' /P for any /spl epsi/ > 0. This improves the previous best factor of 3/sup 1/p/ - /spl epsi/ due to Regev (2004). Our results also imply that under the same complexity assumption, the nearest codeword problem with pre-processing is hard to approximate within a factor of (log n)/sup 1 - /spl epsi//' for any /spl epsi/ > 0.","Lattices,
Cryptography,
Polynomials,
Mathematics,
Application software,
Computer science,
Gaussian processes,
Equations,
Computer applications,
Educational institutions"
Dynamic Value Webs in Mobile Environments Using Adaptive Location-Based Services,"Web Services appear promising to facilitate interoperability and enable Dynamic Value Webs - the on-demand aggregation of services even across enterprise boundaries. However, past experiences have shown interoperability limitations in practice mainly due to information asymmetries on the Web Services markets. This Paper describes a service-oriented architecture for building Dynamic Value Webs in mobile environments using adaptive Location-Based Services. Our concept addresses the following three predominant difficulties: (1) service adaptivity to changing conditions in mobile environments, (2) interoperability including higher levels of semantics, and (3) assuring trustworthiness to all affected parties.","Web services,
Economic forecasting,
Environmental economics,
Web and internet services,
Service oriented architecture,
Programming,
Uncertainty,
Mobile computing,
Computer science,
Information management"
Optimal multi-installments algorithm for divisible load scheduling,"In this paper, we study optimal algorithms for scheduling divisible load on a star network in multi-installments, with the objective to minimize the processing time. Firstly, we consider approach based on linear programming. We found that optimal load sizes of each installment are determined by the number of processor and the rate of processor power to the communication speed. If the number of processor is given, we derive the optimal installments. And for achieving optimal processing time it is preferable to use more processors rather than more installments. Next, we consider optimal parameters of periodic multi-installment algorithm. For a given computation system, we derive the analytic expression of optimal installments. We further prove that, for large-scale workloads, any algorithm which keeps the communication link busy is asymptotically optimal, which gives a way to determine how many processors should be involved in processing","Scheduling algorithm,
Processor scheduling,
Large-scale systems,
Linear programming,
Load modeling,
Sun,
Computer science,
Cost function,
Distributed computing,
Memory management"
Lighting normalization with generic intrinsic illumination subspace for face recognition,"In this paper, we introduce the concept of intrinsic illumination subspace which is based on the intrinsic images. This intrinsic illumination subspace enables an analytic generation of the illumination images under varying lighting conditions. When objects of the same class are concerned, our method allows a class-based generic intrinsic illumination subspace to be constructed in advance. We propose a lighting normalization method based on the generic intrinsic illumination subspace, which is used as a bootstrap subspace for novel images. Face recognition experiments are performed to demonstrate the effectiveness of our method","Lighting,
Face recognition,
Reflectivity,
Face detection,
Maximum likelihood estimation,
Image recognition,
Information science,
Image analysis,
Geometry,
Testing"
Multithreaded value prediction,"This paper introduces a technique which leverages value prediction and multithreading on a simultaneous multithreading processor to achieve higher performance in a single threaded application. By allowing the value-speculative execution to proceed in a separate thread, this technique overcomes barriers that make traditional value prediction relatively ineffective for tolerating long latency loads. It shows that this technique can be as much as 2-5 times more effective than traditional value prediction, achieving more than 40% average performance gain on the SPEC benchmarks with realistic hardware parameters. These gains come from two effects: allowing greater separation between the stalled load and the speculative execution, and the ability to speculate on multiple values for a single load.","Delay,
Yarn,
Multithreading,
Hardware,
Computer science,
Application software,
Performance gain,
Microprocessors,
Microarchitecture,
Circuit synthesis"
Exploiting dynamic workload variation in low energy preemptive task scheduling,"A novel energy reduction strategy to maximally exploit the dynamic workload variation is proposed for the offline voltage scheduling of preemptive systems. The idea is to construct a fully preemptive schedule that leads to minimum energy consumption when the tasks take on approximately the average execution cycles yet still guarantees no deadline violation during the worst-case scenario. End-time for each sub-instance of the tasks obtained from the schedule is used for the on-line dynamic voltage scaling (DVS) of the tasks. For the tasks that normally require a small number of cycles but occasionally a large number of cycles to complete, such a schedule provides more opportunities for slack utilization and hence results in larger energy saving. The concept is realized by formulating the problem as a nonlinear programming (NLP) optimization problem. Experimental results show that, by using the proposed scheme, the total energy consumption at runtime is reduced by as much as 60% for randomly generated task sets when compared with the static scheduling approach only using worst case workload.","Dynamic scheduling,
Processor scheduling,
Runtime,
Energy consumption,
Dynamic voltage scaling,
Voltage control,
Real time systems,
Vehicle dynamics,
Power engineering and energy,
Computer science"
Eager normal form bisimulation,"This paper describes two new bisimulation equivalences for the pure untyped call-by-value /spl lambda/-calculus, called enf bisimilarity and enf bisimilarity up to /spl eta/. They are based on eager reduction of terms to eager normal form (enf), analogously to co-inductive bisimulation characterizations of Levy-Longo tree equivalence and Bohm tree equivalence (up to /spl eta/). We argue that enf bisimilarity is the call-by-value analogue of Levy-Longo tree equivalence. Enf bisimilarity (up to /spl eta/) is the congruence on source terms induced by the call-by-value CPS transform and Bohm tree equivalence (up to /spl eta/) on target terms. Enf bisimilarity and enf bisimilarity up to /spl eta/ enjoy powerful bisimulation proof principles which, among other things, can be used to establish a retraction theorem for the call-by-value CPS transform.","Testing,
Data structures,
Logic,
Computer science,
Differential equations,
Transforms"
Difficulties in providing certification and assurance for software defined radios,"Certification and assurance processes have historically exhibited difficulties when there exists the potential for significant non-functional attributes. We define a non-functional attribute as a condition where the cause-effect behavior cannot readily be specified. Complex systems commonly exhibit such non-functional attributes due to the exceedingly large potential state space. In such systems, analysis based on formal methods becomes very difficult and emergent behaviors (or malicious behaviors that exploit non-functional attributes) can lead to a variety of unintended consequences-some benign and others harmful. Simply put, it is difficult to assure the behavior of a complex system. The shift towards highly flexible and adaptive software defined radios creates a potential complex system problem, and thereby exposes certain assurance and certification challenges for the present regulatory processes. We use certification experiences from the security community to motivate and highlight potential difficulties that may arise within the software defined radio (SDR) space. We then recommend some steps that limit or account for these difficulties","Certification,
Software radio,
History,
Government,
Radio transmitters,
Computer science,
State-space methods,
Security,
Telephone sets,
Cellular phones"
A scalable semantic routing architecture for grid resource discovery,"Grids technology enables the sharing and collaborating of wide variety of resources. To fully utilize these resources, effective discovery techniques are necessities. However, the complicated and heterogeneous characteristics of the grid resource make sharing and discovering a challenging issue. In this paper, we propose a comprehensive semantic-based resource discovery framework, which performs an effective searching according to the semantic properties of what is searched. In the framework, nodes are grouped into clusters according to some criteria. Resources are indexed and aggregated with a highly compressed format. The summarized index can act as network knowledge to guide routing in the network. Intra-cluster and inter-cluster routing strategies are proposed to support scalable and efficient searching. Results from simulation demonstrate that this architecture is very effective in grid resource discovery.","Routing,
Peer to peer computing,
Grid computing,
Computer architecture,
Computer networks,
Resource description framework,
Clustering algorithms,
Computer science,
Collaboration,
Computational modeling"
Second-order backpropagation algorithms for a stagewise-partitioned separable Hessian matrix,"Recent advances in computer technology allows the implementation of some important methods that were assigned lower priority in the past due to their computational burdens. Second-order backpropagation (BP) is such a method that computes the exact Hessian matrix of a given objective function. We describe two algorithms for feed-forward neural-network (NN) learning with emphasis on how to organize Hessian elements into a so-called stagewise-partitioned block-arrow matrix form: (1) stagewise BP, an extension of the discrete-time optimal-control stagewise Newton of Dreyfus 1966; and (2) nodewise BP, based on direct implementation of the chain rule for differentiation attributable to Bishop 1992. The former, a more systematic and cost-efficient implementation in both memory and operation, progresses in the same layer-by-layer (i.e., stagewise) fashion as the widely-employed first-order BP computes the gradient vector. We also show intriguing separable structures of each block in the partitioned Hessian, disclosing the rank of blocks.","Backpropagation algorithms,
Optimal control,
Computer science,
Feedforward systems,
Neural networks,
Industrial engineering,
Operations research,
Mathematics,
Optimization methods,
Multi-layer neural network"
An Email Classification Scheme Based on Decision-Theoretic Rough Set Theory and Analysis of Email Security,"The effects of spam on network is discussed. Unsolicited messages or spam, flood our email boxes, viruses, worms, and denial-of service attacks that cripple computer networks may secret in spam. This threaten network security, stability and reliability seriously. In this paper, A new scheme based on decision-theoretic rough sets is introduced to classify emails into three categories - spam, no-spam and suspicious. By comparing with popular classification methods like Naive Bayes classification, our anti-Spam filter model reduce the error ratio that a non-spam is discriminated to spam, and we can find potential security problems of some email systems.","Set theory,
Security,
Filters,
Computer network reliability,
Rough sets,
Filtering,
Machine learning algorithms,
Machine learning,
Text categorization,
Unsolicited electronic mail"
Validation methods for calibrating software effort models,"COCONUT calibrates effort estimation models using an exhaustive search over the space of calibration parameters in a Cocomo I model. This technique is much simpler than other effort estimation method yet yields PRED levels comparable to those other methods. Also, it does so with less project data and fewer attributes (no scale factors). However, a comparison between COCONUT and other methods is complicated by differences in the experimental methods used for effort estimation. A review of those experimental methods concludes that software effort estimation models should be calibrated to local data using incremental holdout (not jack knife) studies, combined with randomization and hypothesis testing, repeated a statistically significant number of times.","Calibration,
Computer industry,
Computer science,
Propulsion,
Laboratories,
Aerospace industry,
State estimation,
Regression analysis,
Yield estimation,
Costs"
An evaluation of content browsing techniques for hierarchical space-filling visualizations,"Space-filling visualizations, such as the TreeMap, are well suited for displaying the properties of nodes in hierarchies. To browse the contents of the hierarchy, the primary mode of interaction is by drilling down through many successive layers. In this paper we introduce a distortion algorithm based on fisheye and continuous zooming techniques for browsing data in the TreeMap representation. The motivation behind the distortion approach is for assisting users to rapidly browse information displayed in the TreeMap without opening successive layers of the hierarchy. Two experiments were conducted to evaluate the new approach. In the first experiment (N=20) the distortion approach is compared to the drill down method. Results show that subjects are quicker and more accurate in locating targets of interest using the distortion method. The second experiment (N=12) evaluates the effectiveness of the two approaches in a task requiring context, we define as the context browsing task. The results show that subjects are quicker and more accurate in locating targets with the distortion technique in the context browsing task.","Navigation,
Displays,
Drilling,
Computer science,
Data visualization,
Data structures,
File systems,
Stock markets,
Disk drives,
Operating systems"
Mean curvature mapping for detection of corneal shape abnormality,"Corneal topography is used to measure the anterior surface of the cornea. It is conventionally represented as radial slope, radial curvature, and elevation. In this paper, we introduce the application of mean curvature mapping as an alternative representation of the corneal topography. The purpose is to improve the detection of keratoconus and other diseases characterized by local increase in corneal curvature. Both simulated keratoconic cornea and real keratoconus data exported from the corneal topography system were analyzed. Four representations of corneal topography were generated and compared. It was found that mean curvature mapping provided the most precise cone location in simulated keratoconus. In both actual and simulated keratoconus cases, the appearance of the cone-like distortion is more consistent on mean curvature maps. Mean curvature mapping may improve the detection and localization of corneal shape abnormalities.","Surface topography,
Cornea,
Optical refraction,
Shape measurement,
Diseases,
Optical distortion,
Lenses,
Retina,
Analytical models,
Labeling"
Performance and cost analysis of time-multiplexed execution on the dynamically reconfigurable processor,"Dynamically reconfigurable processors with multi-context facility have been used for various applications. The relationship between context size and performance of such processors is analyzed based on real designs. The parallelism diagram which shows the required PEs in each step of the algorithm is introduced as the basis of the analysis, and models for performance and cost are shown. Evaluation results show that the performance is degraded about 23% when the size of a context becomes 1/2. The performance per cost is improved 7-14 times than that of the case without time-multiplexed execution.",
A Cost Effective Symmetric Key Cryptographic Algorithm for Small Amount of Data,"Once an application steps out of the bounds of a single-computer box, its external communication is immediately exposed to a multitude of outside observers with various intentions, good or bad. In order to protect sensitive data while these are en route, applications invoke different methods. In today's world, most of the means of secure data and code storage and distribution rely on using cryptographic schemes, such as certificates or encryption keys. Cryptography is the science of writing in secret code and is an ancient art. Some experts argue that cryptography appeared spontaneously sometime after writing was invented, with applications ranging from diplomatic missives to war-time battle plans. It is no surprise, then, that new forms of cryptography came soon after the widespread development of computer communications. There are two basic types of cryptography: symmetric key and asymmetric key. Symmetric key algorithms are the quickest and most commonly used type of encryption. Here, a single key is used for both encryption and decryption. There are few well-known symmetric key algorithms i.e. DES, RC2, RC4, IDEA etc. This paper describes cryptography, various symmetric key algorithms in detail and then proposes a new symmetric key algorithm. Algorithms for both encryption and decryption are provided here. The advantages of this new algorithm over the others are also explained","Costs,
Public key cryptography,
Protection,
Data security,
Writing,
Business,
History,
Encoding,
US Government,
National security"
Automatic Sleep Staging using Support Vector Machines with Posterior Probability Estimates,"This paper describes attempts at constructing an automatic sleep stage classifier using EEG recordings. Three different feature extraction schemes were compared together with two different pattern classifiers, the recently introduced support vector machine and the well known k-nearest neighbor classifier. Using estimates of posterior probabilities for each of the sleep stages it was possible to devise a simple post-processing rule which leads to improved accuracy. Compared to a human expert the accuracy of the best classifier is 81%","Sleep,
Support vector machines,
Electroencephalography,
Support vector machine classification,
Feature extraction,
Computer science,
Electrooculography,
Electromyography,
Band pass filters,
Information filtering"
"On extractors, error-correction and hiding all partial information","Randomness extractors (Nisan and Zuckerman, 1996) allow one to obtain nearly perfect randomness from highly imperfect sources randomness, which are only known to contain ""scattered"" entropy. Not surprisingly, such extractors have found numerous applications in many areas of computer science including cryptography. Aside from extracting randomness, a less known usage of extractors comes from the fact that they hide all deterministic functions of their (high-entropy) input (Dodis and Smith, 2005): in other words, extractors provide certain level of privacy for the imperfect source that they use. In the latter kind of applications, one typically needs extra properties of extractors, such as invertibility, collision-resistance or error-correction. In this abstract we survey some of such usages of extractors, concentrating on several recent results by the author (Dodis et al., 2004 and Dodis and Smith, 2005). The primitives we survey include several flavors of randomness extractors, entropically secure encryption and perfect one-way hash functions. The main technical tools include several variants of the leftover hash lemma, error correcting codes, and the connection between randomness extraction and hiding all partial information. Due to space constraints, many important references and results are not mentioned here; interested reader can find those in the works of Dodis et al. (2004) and Dodis and Smith (2005)","Data mining,
Cryptography,
Random variables,
Computer science,
Entropy,
Application software,
Computer errors,
Privacy,
Error correction codes,
Information security"
Steady state distribution for stochastic knapsack with bursty arrivals,"In this letter, we develop a methodology for obtaining an approximate steady state occupancy distribution for a multiclass stochastic knapsack with bursty call arrivals, and exponential holding times. Preliminary results indicate that the proposed technique is effective in studying the knapsack behavior.","Steady-state,
Stochastic processes,
Bandwidth,
Computer science,
Stochastic systems,
IP networks,
Web sites,
Autocorrelation,
Telecommunication traffic"
Service-Oriented E-Learning Architecture Using Web Service-Based Intelligent Agents,"There is no doubt that e-learning has found its way in our lives. From the very start to the Ph.D. level one can find e-learning courses every where and all the big names are supporting it. One thing that is needed to be understood is that e-learning is basically the integration of various technologies. Now this technology is maturing and we can find different standards for e-learning .New technologies such as agents and web services are promising better results. In this paper we have proposed an e-learning architecture that is dependent on multi-agent systems and web services. These communication technologies will make the architecture more robust, scalable and efficient.","Electronic learning,
Service oriented architecture,
Intelligent agent,
Web services,
Internet,
Web sites,
Simple object access protocol,
Packaging,
Data models,
Multiagent systems"
On the optimality of group network codes,"It is well-known that linear network codes are sufficient to achieve maximal network throughput (or bandwidth utilization efficiency) in the single-session multicast scenario. However, it is not known whether they are still optimal in maximizing network throughput in the multiple-session scenario (either single-source or multiple-source ones). In this paper, we prove that a more general class of network codes - group network codes - are sufficient to achieve the maximal throughput in the single-source multiple-session multicast scenario","Throughput,
Communication networks,
Network coding,
Bandwidth,
Computer science,
Communication channels,
Tail"
Supporting iWARP Compatibility and Features for Regular Network Adapters,"With several recent initiatives in the protocol offloading technology present on network adapters, the user market is now distributed amongst various technology levels including regular Ethernet network adapters, TCP Offload Engines (TOEs) and the recently introduced iWARP-capable networks. While iWARP-capable networks provide all the features provided by their predecessors (TOEs and regular Ethernet network adapters) and a new richer programming interface, they lack with respect to backward compatibility. In this aspect, two important issues need to be considered. First, not all network adapters support iWARP; thus, software compatibility for regular network adapters (which have no offloaded protocol stack) with iWARP capable network adapters needs to be achieved. Second, several applications on top of regular Ethernet as well as TOE based adapters have been written with the sockets interface; rewriting such applications using the new iWARP interface is cumbersome and impractical. Thus, it is desirable to have an interface which provides a two-fold benefit: (i) it allows existing applications to run directly without any modifications and (ii) it exposes the richer feature set of iWARP to the applications to be utilized with minimal modifications. In this paper, we design and implement a software stack to handle these issues. Specifically, (i) the software stack emulates the functionality of the iWARP stack in software to provide compatibility for regular Ethernet adapters with iWARP capable networks and (ii) it provides applications with an extended sockets interface that provides the traditional sockets functionality as well as functionality extended with the rich iWARP features","Ethernet networks,
Sockets,
TCPIP,
Application software,
Engines,
Hardware,
Network servers,
Computer science,
Transport protocols,
US Department of Energy"
Approximation of discrete phase-type distributions,"The analysis of discrete stochastic models such as generally distributed stochastic Petri nets can be done using state space-based methods. The behavior of the model is described by a Markov chain that can be solved mathematically. The phase-type distributions that are used to describe non-Markovian distributions have to be approximated. An approach for the fast and accurate approximation of discrete phase-type distributions is presented. This can be a step towards a practical state space-based simulation method, whereas formerly this approach often had to be discarded as unfeasible due to high memory and runtime costs. Discrete phases also fit in well with current research on proxel-based simulation. They can represent infinite support distribution functions with considerably fewer Markov chain states than proxels. Our hope is that such a combination of both approaches will lead to a competitive simulation algorithm.","Stochastic processes,
Distribution functions,
Optimization methods,
Mathematical model,
Analytical models,
Computer science,
Petri nets,
Runtime,
Costs,
Discrete event simulation"
Quantitative analysis of probabilistic pushdown automata: expectations and variances,"Probabilistic pushdown automata (pPDA) have been identified as a natural model for probabilistic programs with recursive procedure calls. Previous works considered the decidability and complexity of the model-checking problem for pPDA and various probabilistic temporal logics. In this paper we concentrate on computing the expected values and variances of various random variables defined over runs of a given probabilistic pushdown automaton. In particular, we show how to compute the expected accumulated reward and the expected gain for certain classes of reward functions. Using these results, we show how to analyze various quantitative properties of pPDA that are not expressible in conventional probabilistic temporal logics.","Analysis of variance,
Automata,
Computer science,
Probabilistic logic,
Random variables,
Informatics,
Probability distribution,
Performance analysis,
H infinity control"
A* based joint segmentation and classification of dialog acts in multiparty meetings,"We investigate the use of the A* algorithm for joint segmentation and classification of dialog acts (DAs) of the ICSI Meeting Corpus. For the heuristic search a probabilistic framework is used that is based on DA-specific N-gram language models. Furthermore, two new metrics for performance evaluation are motivated and described and the influence of different metrics for performance evaluation is demonstrated. The proposed method is evaluated on both traditional and new metrics, and compared with our previous work on the same task","Computer science,
Information retrieval,
Ambient intelligence,
Contracts,
Degradation,
Speech processing,
Solids,
Distributed computing"
TCP connection game: a study on the selfish behavior of TCP users,"We present a game-theoretic study of the selfish behavior of TCP users when they are allowed to use multiple concurrent TCP connections so as to maximize their goodputs or other utility functions. We refer to this as the TCP connection game. A central question we ask is whether there is a Nash equilibrium in such a game, and if it exists, whether the network operates efficiently at such a Nash equilibrium. Combined with the well known PFTK TCP model (1998), we study this question for three utility functions that differ in how they capture user behavior. The bad news is that the loss of efficiency or price of anarchy can be arbitrarily large if users have no resource limitations and are not socially responsible. The good news is that, if either of these two factors is considered, efficiency loss is bounded. This may partly explain why there will be no congestion collapse if many users use multiple connections.","Costs,
Nash equilibrium,
Stability,
Internet,
Aggregates,
Computer science,
IP networks,
Software agents,
Accelerated aging,
Telecommunication traffic"
An approximation algorithm for the disjoint paths problem in even-degree planar graphs,"In joint work with Eva Tardos in 1995, we asked whether it was possible to obtain a polynomial-time, polylogarithmic approximation algorithm for the disjoint paths problem in the class of all even-degree planar graphs. This paper answers the question in the affirmative, by providing such an algorithm. The algorithm builds on recent work of C. Chekuri et al. (2004, 2005), who considered muting problems in planar graphs where each edge can carry up to two paths.","Approximation algorithms,
Optimized production technology,
Graph theory,
Polynomials,
Routing,
Upper bound,
Tree graphs,
Joining processes,
Very large scale integration,
Design optimization"
A methodology for designing countermeasures against current and future code injection attacks,"This paper proposes a methodology to develop countermeasures against code injection attacks, and validates the methodology by working out a specific countermeasure. This methodology is based on modeling the execution environment of a program. Such a model is then used to build countermeasures. The paper justifies the need for a more structured approach to protect programs against code injection attacks: we examine advanced techniques for injecting code into C and C++ programs and we discuss state-of-the-art (often ad hoc) approaches that typically protect singular memory locations. We validate our methodology by building countermeasures that prevent attacks by protecting a broad variety of memory locations that may be used by attackers to perform code injections. The paper evaluates our approach and discusses ongoing and future work.",
End-to-end pairwise key establishment using multi-path in wireless sensor network,"Resource aware random key pre-distribution schemes have been proposed to overcome the limitations of energy constrained wireless sensor networks. In most of these schemes, each sensor node is loaded with a key ring. Neighboring nodes are considered to be connected through a secure link if they share a common key. Nodes which are not directly connected establish a secure path which is then used to negotiate a symmetric key. However, since different symmetric keys are used for different links along the secure path, each intermediate node must first decrypt the message received from the upstream node. Notice that during this process, the negotiated key will be revealed to each node along the secure path. The objective of this paper is to address this short-coming. To this end, we propose an end-to-end pairwise key establishment scheme which uses a properly selected set of node-disjoint paths to securely negotiate symmetric keys between sensor nodes. We show through analysis that our scheme is highly secure against node captures in wireless sensor networks. The proposed scheme can be combined with any existing key pre-distribution scheme to enhance the security of its path-key establishment procedure.",
Intelligent autonomous navigation for mobile robots: spatial concept acquisition and object discrimination,"An autonomous system able to construct its own navigation strategy for mobile robots is proposed. The navigation strategy is molded from navigation experiences (succeeding as the robot navigates) according to a classical reinforcement learning procedure. The autonomous system is based on modular hierarchical neural networks. Initially, the navigation performance is poor (many collisions occur). Computer simulations show that after a period of learning, the autonomous system generates efficient obstacle avoidance and target seeking behaviors. Experiments also offer support for concluding that the autonomous system develops a variety of object discrimination capability and of spatial concepts.","Intelligent robots,
Navigation,
Mobile robots,
Robot kinematics,
Learning,
Neural networks,
Computer science,
Competitive intelligence,
Biological system modeling,
Computational intelligence"
A survey of IPv6 site multihoming proposals,,"Proposals,
IP networks,
Web and internet services,
Advertising,
Routing,
Mission critical systems,
Protection,
Switches,
Scientific computing,
Computer science"
An SVM-based soccer video shot classification,"Video shot classification is an important component of content-based video retrieval. It is also a basic step towards video abstract, event detection and content filtering. According to the characters of shots for soccer videos, an integrating color and edge distribution shot classification method is presented. First, A GMM model is trained for grass pixel. And then the grass distribution is computed based on the trained GMM. Due to the sensitivity to light, field and time for color features, edge distribution is computed through canny operator. These features are reasonable complementarities to color features and have no sensitivity to light, field and time. Integrating two types of features embodies main characters of different shot types. One-against-others SVMs are designed for shots classification. Experiments show that our method performs better results.","Filtering,
Event detection,
Distributed computing,
Computer science,
Information science,
Computer networks,
Telecommunication computing,
Software,
Communication networks,
Communications technology"
Online Ranking by Projecting,"We discuss the problem of ranking instances. In our framework, each instance is associated with a rank or a rating, which is an integer in 1 to k. Our goal is to find a rank-prediction rule that assigns each instance a rank that is as close as possible to the instance's true rank. We discuss a group of closely related online algorithms, analyze their performance in the mistake-bound model, and prove their correctness. We describe two sets of experiments, with synthetic data and with the Each Movie data set for collaborative filtering. In the experiments we performed, our algorithms outperform online algorithms for regression and classification applied to ranking.",
Obtaining best parameter values for accurate classification,"In this paper we examine the effect that the choice of support and confidence thresholds has on the accuracy of classifiers obtained by classification association rule mining. We show that accuracy can almost always be improved by a suitable choice of threshold values, and we describe a method for finding the best values. We present results that demonstrate this approach can obtain higher accuracy without the need for coverage analysis of the training data.","Data mining,
Association rules,
Training data,
Costs,
Computer science,
Software testing,
Machine learning,
Smoothing methods"
Inferring motion from the rank constraint of the phase matrix,"We investigate the rank constraint of the discrete phase difference, and derive its exact parametric model. We show that the discrete phase difference of two shifted images, or their subregions, is a 2-dimensional sawtooth signal. This allows us to determine the motion parameters to subpixel accuracy by simply counting the number of cycles of the phase difference along each frequency axis. The subpixel portion is given by the non-integer fraction of the last cycle along each axis. The problem is formulated as a homogeneous cost function under rank constraint for the phase matrix, and the shape constraint for the filter that computes the group delay, and is solved using a robust technique.",
Intelligent Location of Tropical Cyclone Center,"The tropical cyclone center location is a very important aspect in weather forecast and typhoon analysis, which is also a difficult issue when done manually. Satellite digital photograph makes it possible to carry out such task by aid of modern digital and computer means of processing. Based on image process technology, it is possible to apply new method according to the feature of digital imaging which was useless in human based center location method. In order to improve the objectivity and precision of the location, a novel intelligent automatic system framework will be proposed, to locate the tropical cyclone center intelligently and automatically, based on the satellite photograph. After pre-processing, several center location technologies will be put forward, based on tropical cyclone's structure, the combining movement of whirl and translation. According to the tropical cyclone's symmetry shape and its spiral movement feature, logarithmic helix is used to fit the edge or skeleton of the cyclone feature cloud, which can be used to estimate the center of the cyclone. According to its movement feature, a rotation matching methodology will be proposed to catch the track through imaging sequence. As an example, the computing methodology produced in this paper was applied in experimental cyclone forecasting by the Shanghai Meteorology Center. The proposed solutions was confirmed to have the potential for successful application to the problem of tropical cyclone center tracking, and the system is effective and the performance errors are acceptable for practical applications.","Tropical cyclones,
Weather forecasting,
Satellites,
Typhoons,
Digital images,
Humans,
Intelligent systems,
Shape,
Spirals,
Skeleton"
Some New Parallel Fast Fourier Transform Algorithms,"Discrete Fourier transform (DFT) has many applications in digital signal and image processing and other scientific and technological domains, but its time complexity of direct computation is O(n2), limiting greatly its application range. Thus many people have developed fast Fourier transform (FFT) algorithms, reducing the complexity from O(n2) to O(nlogn)(In this paper logn denotes log2n).But for large n, O(nlogn) is still very high. So multiprocessor systems have been used to speed up the computation of DFT. This paper first introduces a new general method to deduce FFT algorithms, then transforms the deduced second radix-2 decimation-in-time FFT algorithm into another parallelizable sequential form, and finally transforms the latter algorithm into a new parallel FFT algorithm, reducing the time complexity of DFT to O(nlogn/p) (where p is the number of processors). Using similar methods, the authors can also design other new parallel 1-D and 2-D FFT algorithms.","Fast Fourier transforms,
Discrete Fourier transforms,
Multiprocessing systems,
Algorithm design and analysis,
Concurrent computing,
Computer science,
Educational institutions,
Application software,
Signal processing,
Image processing"
SAWAN: a survivable architecture for wireless LANs,"This paper describes survivability schemes against access point (AP) failures in wireless LANs. It particularly aims for resiliency and survivability against multistage attacks where the adversary is successful in compromising the AP, and then targets the survived but more vulnerable network. This is true in real life where the adversary knows that survivability is a design consideration built into the network. It then performs a multistage targeted attack that is aimed at compromising the survived network that may have vulnerabilities. We first present a unique infrastructure for an ad-hoc migration scheme (IAMS) where the nodes under a failed AP form an ad-hoc network and reconnect to the network using available neighboring APs. We then present a scheme for isolating and removing any malicious nodes from the ad-hoc network routes in a transparent manner once the malicious nodes have been identified. This will minimize the chances of further attacks in the survived network, and the removal is done in a distributed fashion without the nodes exchanging any information between them. We report the results of our simulations performed using the network simulation tool GloMoSim.","Wireless LAN,
Local area networks,
Cities and towns,
Computer science,
Ad hoc networks,
Quality of service,
Robustness,
Switches,
Computer architecture,
Ubiquitous computing"
A Dynamic Group Mutual Exclusion Algorithm Using Surrogate-Quorums,"The group mutual exclusion problem extends the traditional mutual exclusion problem by associating a type with each critical section. In this problem, processes requesting critical sections of the same type can execute their critical sections concurrently. However, processes requesting critical sections of different types must execute their critical sections in a mutually exclusive manner. In this paper, we provide a distributed algorithm for solving the group mutual exclusion problem based on the notion of surrogate-quorum. Intuitively, the algorithm uses the quorum that has been successfully locked by a request as a surrogate to service other compatible requests for the same type of critical section. Unlike the existing quorum-based algorithms for group mutual exclusion, the algorithm achieves a low message complexity of O(q), where q is the maximum size of a quorum, while maintaining both synchronization delay and waiting time at two message hops. Moreover, like the existing quorum-based algorithms, the algorithm has high maximum concurrency of n, where n is the number of processes in the system. The existing quorum-based algorithms assume that the number of groups is static and does not change during runtime. However, the algorithm can adapt without performance penalties to dynamic changes in the number of groups. Simulation results indicate that our algorithm outperforms the existing quorum-based algorithms for group mutual exclusion by as much as 50% in some cases","Computer science,
Delay effects,
Web services,
Distributed algorithms,
Concurrent computing,
Runtime,
Distributed computing"
Management and Retrieval of Web Services Based on Formal Concept Analysis,"After introducing some basic concepts of Web services and the definition of their relationships, we formally define the problem of managing and retrieving of Web services. This paper presents formal concept analysis to manage Web services and generate the concept lattice describing the relationship between them. We demonstrate how to express the relationship between Web services using concept lattice effectively and develop algorithms to retrieve Web services accurately in the lattices. Meanwhile, some strategies are also proposed to minimize the impacts of factors on the efficiency of Web service retrieval. Experimental results show our approach can manage Web services effectively and retrieve them with high efficiency and accuracy","Web services,
Lattices,
Business,
Computer science,
Educational institutions,
Technology management,
Engineering management,
Web and internet services,
Keyword search,
Computer science education"
Coalescent multi-robot teaming through ASyMTRe: a formal analysis,"This paper describes a general approach for automatically synthesizing task solutions for heterogeneous robot teams. In particular, our approach enables multiple robots to coalesce into teams to solve a task through tightly-coupled sensor sharing. Instead of designing special solution strategies for the team, our ASyMTRe approach enables the robot team to generate solutions autonomously according to the current robot team composition. In this paper, we first formulate the problems that the ASyMTRe approach addresses, and then present the anytime ASyMTRe configuration algorithm. We prove that the configuration algorithm is correct, and is guaranteed to find the optimal solution given enough time. Empirical results are also presented validating this analysis, and showing that the ASyMTRe configuration algorithm has good scalability and can quickly find a good solution with the solution quality increasing as additional planning time is available. By analyzing the configuration algorithm, we show that ASyMTRe is applicable to a large class of challenging multi-robot problems","Robot sensing systems,
Robot kinematics,
Navigation,
Robotics and automation,
Mobile robots,
Intelligent robots,
Intelligent sensors,
Algorithm design and analysis,
Laboratories,
Computer science"
A multi-formalism modeling composability framework: agent and discrete-event models,It is common practice to build complex systems from disparate sub-systems. Model composability is concerned with techniques for developing a whole model of a system from the models of its sub-systems. In this paper we present a new kind of multi-formalism modeling composability framework which introduces the concept of knowledge interchange broker for composing disparate modeling formalisms. The approach offers separation of concerns between model specifications and execution protocols across multiple modeling formalisms. The framework is exemplified via vehicle and agent models described in the discrete-event system specification and reactive action planning formalisms. A high-level software specification that illustrates an implementation of this framework is described. Ongoing and future research directions are also briefly presented.,
A Universal Control Approach for a Family of Uncertain Nonlinear Systems,"We study the problem of global adaptive stabilization by output feedback for nonlinear systems with unknown parameters. The class of uncertain systems under consideration is assumed to be dominated by a bounding system which is linear growth in the unmeasurable states but can be a polynomial function of the system output, with unknown growth rate. To achieve global stabilization in the presence of parametric uncertainty, we propose a non-identifier based output feedback control law using the idea of universal control integrated with the linear-like output feedback control scheme proposed recently. In particular, we explicitly design a universal output feedback controller which globally regulates all the states of the uncertain system while maintaining global boundedness of the closed-loop system.","Nonlinear control systems,
Control systems,
Nonlinear systems,
Output feedback,
Uncertain systems,
Linear feedback control systems,
Polynomials,
Uncertainty,
Laboratories,
Nonlinear dynamical systems"
Rule-based verification of scenarios with pre-conditions and post-conditions,"Scenarios that describe concrete situations of software operation play an important role in software development, especially in requirements engineering. Since scenarios are informal, the correctness of scenarios is hard to verify. The authors have developed a language for describing scenarios in which simple action traces are embellished. The purposes are to include typed frames based on a simple case grammar of actions and to describe the sequence among events. Based on this scenario language, this paper describes both (1) a correctness-verification method using rules to detect errors (lack of events, extra events, and wrong sequence among events) in a scenario and (2) a retrieval method of rules from rule DB that applicable to scenarios using pre and postconditions.",
Handling Asymmetry in Power Heterogeneous Ad Hoc Networks: A Cross Layer Approach,"Power heterogeneous ad hoc networks are characterized by link layer asymmetry: the ability of lower power nodes to receive transmissions from higher power nodes but not vice versa. This not only poses challenges at the routing layer, but also results in an increased number of collisions at the MAC layer due to high power nodes initiating transmissions while low power communications are in progress. Previously proposed routing protocols for handling unidirectional links largely ignore MAC layer dependencies. In this paper, we propose a cross layer framework that effectively improves the performance of the MAC layer in power heterogeneous ad hoc networks. In addition, our approach seamlessly supports the identification and usage of unidirectional links at the routing layer. The framework is based on intelligently propagating low power MAC layer control messages to higher power nodes so as to preclude them from initiating transmissions while the low power communications are in progress within their sensing range. The integrated approach also constructs reverse tunnels to bridge unidirectional links thereby facilitating their effective usage at the routing layer. Extensive simulations are performed to study the proposed framework in various settings. The use of our framework improves the overall throughput of the power heterogeneous network by as much as 25% over traditional layered approaches. In summary, our framework offers a simple, yet effective and viable approach for media access control and to support routing in power heterogeneous ad hoc networks","Intelligent networks,
Ad hoc networks,
Cross layer design,
Media Access Protocol,
Routing protocols,
Bidirectional control,
Throughput,
Degradation,
Computer science,
Power engineering and energy"
"A real time, isotope identifying gamma spectrometer for monitoring of pedestrians","The demand for installation and use of radiation monitors at border crossing points , and other locations in a country has significantly increased due to the fact that terrorist threats may involve the use of radiation dispersal devices (RDDs, dirty bombs). One of the problems that customs officers and security forces experience is a high frequency of ""innocent"" radiation alarms caused by airport passengers who have undergone a medical treatment. Since the half-life of the isotopes that are used for medical treatment ranges from hours to several days, the dose-rate for days and even weeks after the treatment is high enough to trigger a radiation alarm of a border monitor. In this paper we describe the development and testing of a real time gamma spectrometer based on a commercially available large volume NaI-detector and a computer-coupled multichannel analyzer (MCA) with fast data collection, stabilization of the energy scale, and isotope identification software. The system is capable of measuring a burst of gamma spectra in second intervals, to identify the isotopes and to produce a ""green"" alarm in real time when a medical isotope is present and a ""red"" alarm in other cases. The system has successfully been tested under laboratory conditions, as well as at an international airport and on patients in the radiation ward of hospitals. This work has been performed under IAEA Research Agreements with the Atom Institute of the Austrian Universities, the Austrian Research Center Seibersdorf, and the International Atomic Energy Agency (IAEA)","Isotopes,
Spectroscopy,
Monitoring,
Medical treatment,
Airports,
Atomic measurements,
Weapons,
Security,
Frequency,
Computer displays"
Active contours using a constraint-based implicit representation,"We present a new constraint-based implicit active contour, which shares desirable properties of both parametric and implicit active contours. Like parametric approaches, their representation is compact and can be manipulated interactively. Like other implicit approaches, they can naturally adapt to nonsimple topologies. Unlike implicit approaches using level-set methods, representation of the contour does not require a dense mesh. Instead, it is based on specified on-curve and off-curve constraints, which are interpolated using radial basis functions. These constraints are evolved according to specified forces drawn from the relevant literature of both parametric and implicit approaches. This new type of active contour is demonstrated through synthetic images, photographs, and medical images with both simple and nonsimple topologies. For complex input, this approach produces results comparable to those of level set or parameterized finite-element active models, but with a compact analytic representation. As with other active contours they can also be used for tracking, especially for multiple objects that split or merge.","Active contours,
Topology,
Image segmentation,
Biomedical imaging,
Computer science,
Computer vision,
Robustness,
High performance computing,
Libraries,
Level set"
Automatic construction and evaluation of performance skeletons,"The performance skeleton of an application is a short running program whose execution time in any scenario reflects the estimated execution time of the application it represents. Such a skeleton can be employed to quickly estimate the performance of a large application under existing network and node sharing. This paper presents a framework for automatic construction of performance skeletons of a specified execution time and evaluates their use in performance prediction with CPU and network sharing. The approach is based on capturing the execution behavior of an application and automatically generating a synthetic skeleton program that reflects that execution behavior. The paper demonstrates that performance skeletons running for a few seconds can predict the application execution time fairly accurately. Relationship of skeleton execution time, application characteristics, and nature of resource sharing, to accuracy of skeleton based performance prediction, is analyzed in detail. The goal of this research is accurate performance estimation in heterogeneous and shared computation environments.","Skeleton,
Resource management,
Grid computing,
High performance computing,
Availability,
Bandwidth,
Monitoring,
Application software,
Computer science,
Performance analysis"
SCTP versus TCP for MPI,"SCTP (Stream Control Transmission Protocol) is a recently standardized transport level protocol with several features that better support the communication requirements of parallel applications; these features are not present in traditional TCP (Transmission Control Protocol). These features make SCTP a good candidate as a transport level protocol for MPI (Message Passing Interface). MPI is a message passing middleware that is widely used to parallelize scientific and compute intensive applications. TCP is often used as the transport protocol for MPI in both local area and wide area networks. Prior to this work, SCTP has not been used for MPI. We compared and evaluated the benefits of using SCTP instead of TCP as the underlying transport protocol for MPI. We re-designed LAM-MPI, a public domain version of MPI, to use SCTP.We describe the advantages and disadvantages of using SCTP, the necessary modifications to the MPI middleware to use SCTP, and the performance of SCTP as compared to the stock implementation that uses TCP.","Transport protocols,
Delay,
Wide area networks,
Computer science,
Middleware,
TCPIP,
Communication system control,
Message passing,
Computer networks,
Distributed computing"
A multi-objective parameter estimator for image mosaicing,,"Parameter estimation,
Equations,
Maximum likelihood estimation,
Gold,
Layout,
Cameras,
Computer science,
Image sensors,
Signal processing,
Image processing"
Design theoretic approach to replicated declustering,"Declustering techniques reduce query response times through parallel I/O by distributing data among multiple devices. Most of the research on declustering is targeted at spatial range queries and investigates schemes with low additive error. Recently, declustering using replication is proposed to reduce the additive overhead. Replication significantly reduces retrieval cost of arbitrary queries. In this paper, we propose a disk allocation and retrieval mechanism for arbitrary queries based on design theory. Using proposed c-copy replicated declustering scheme, (c - 1)k/sup 2/ + ck buckets can be retrieved using at most k disk accesses. Retrieval algorithm is very efficient and is asymptotically optimal with /spl Theta/(|Q|) complexity for a query Q. In addition to the deterministic worst-case bound and efficient retrieval, proposed algorithm handles nonuniform data, high dimensions, supports incremental declustering and has good fault-tolerance property.","Information retrieval,
Costs,
Fault tolerance,
Spatial databases,
Computer science,
Delay,
Computer errors,
Visual databases,
Information systems,
Data visualization"
Comparison among evolutionary algorithms and classical optimization methods for circuit design problems,"This work concerns the comparison of evolutionary algorithms and standard optimization methods on two circuit design problems: the parameter extraction of device circuit model and the multi-objective optimization of an operational transconductance amplifier. We compare standard optimization techniques and evolutionary algorithms in terms of quality of the solutions and computational effort, that is, objective function evaluations needed to compute them. The experimental results obtained show as standard techniques are robust with respect evolutionary algorithms, while the latter are more effective in terms of the standard metrics and function calls. In particular for the multiobjective problem, the observed Pareto front determined by evolutionary algorithms has a better spread of solutions with a larger number of nondominated solutions with respect to the standard multi-objective techniques","Evolutionary computation,
Optimization methods,
Circuit synthesis,
Inductors,
Mathematics,
Computer science,
Parameter extraction,
Design optimization,
Transconductance,
Operational amplifiers"
An Approach for Programming Robots by Demonstration: Generalization Across Different Initial Configurations of Manipulated Objects,"Imitation is a powerful learning tool that can be used by a robotic agent to socially learn new skills and tasks. One of the fundamental problems in imitation is the correspondence problem, how to map between the actions, states and effects of the model and imitator agents, when the embodiment of the agents is dissimilar. In our approach, the matching depends on different metrics and granularity. Focusing on object manipulation and arrangement demonstrated by a human, this paper presents Jabberwocky, a system that uses different metrics and granularity to produce action command sequences that when executed by an imitating agent can achieve corresponding effects (manipulandum absolute/relative position, displacement, rotation and orientation). Based on a single demonstration of an object manipulation task by a human and using a combination of effect metrics, the system is shown to produce correspondence solutions that are then performed by an imitating agent, generalizing with respect to different initial object positions and orientations in the imitator's workspace. Depending on the particular metrics and granularity used, the corresponding effects will differ (shown in examples), making the appropriate choice of metrics and granularity depend on the task and context","Robot programming,
Humans,
Cognitive robotics,
Orbital robotics,
Educational robots,
Adaptive systems,
Autonomous agents,
Computer science,
Educational institutions,
Power system modeling"
Distributed Surveillance System,"The present paper describes a distributed surveillance system that has the ability to detect, track and snapshot subjects moving around in a certain space. The system is composed by several surveillance agents and a central surveillance agent which communicates with them and process data. The surveillance agents are composed by a camera and a computer with wireless communication. The central surveillance agent runs on a computer also with wireless communication. In order to support the communication between the two types of agents, was created a language called VigiLANG. This language allows a high-level communication interface between the several surveillance agents and the central surveillance agent. The proposed surveillance system performs robust image segmentation providing the user images with the relevant information; provides robust subject detection and identification; supports expansion; etc. The system is totally configurable through the definition of new surveillance policies and may be easily expanded through the addition of new surveillance agents. The system was validated by performing a vast set of experiments, achieving very good results in the detection and position estimation of the subjects under surveillance","Surveillance,
Wireless communication,
Robustness,
Computer architecture,
Image processing,
Motion detection,
Artificial intelligence,
Computer science,
Laboratories,
Cameras"
Ad hoc network security: peer identification and authentication using signal properties,"As networking architectures grow and develop, the pace of security in these networks must keep pace. This paper is interested in identification and authentication in ad hoc networks, which are particularly susceptible to identity attacks, such as masquerading and malicious alias attacks. To mitigate these identity attacks, we propose to associate the message transmitter with a location and use this location information to reason about identity. There are several cooperative location schemes detailed in the literature, but because we cannot assume that a malicious party would cooperate in a location scheme, we propose to determine transmitter location by using the physical properties of the received signal.","Ad hoc networks,
Authentication,
Signal processing,
Peer to peer computing,
Transmitters,
Intelligent sensors,
Computer science,
Information security,
Computer architecture,
Computer security"
Towards a policy-driven framework for adaptive Web services composition,"The variation of contexts in which a Web service could be used and the resulting variation in functional and quality of service (QoS) requirements motivates further research to extend Web services platforms to cater for differentiated service offerings and dynamic adaptability. Adaptability is an important requirement in the context of Web services to cater for the need of diverse set of client applications requesting customized view of consumed Web services to fit their own contexts and preferences. This paper presents an ongoing project to devise a novel service composition framework, named AdaptiveBPEL that leverages aspect-oriented software development (AOSD) techniques to open up the service composition for dynamic change in order to provide a greater degree of configurability and dynamic adaptability of Web services. This framework can: 1) easily adapt to changes in business rules, and collaboration policies governing service interactions; 2) provide different levels of functional and QoS offerings. The adaptation process is policy-driven to declaratively define the adaptive service behavior through dynamic injection of functional and non-functional extensions into a core service composition to allow on-demand and per-instance service adaptability.","Web services,
Quality of service,
Context-aware services,
Application software,
Software engineering,
Collaboration,
Computer science,
Australia,
Programming,
Machine vision"
Sequential Source Coding: An optimization viewpoint,"The problem of sequential source coding is to minimize the average entropy rate subject to a constraint on the average distortion and a causality constraint on codewords. This is cast as an optimization problem on an appropriate convex set of probability measures. Existence and properties of optimal sequential codes are explored. A sequential rate distortion theorem is proved and a construction given to show that in general, a ""causality gap"" exists.",
Deciding what to design: closing a gap in software engineering education,"Software has jumped ""out of the box"" - it controls critical systems; it pervades business and commerce; it is embedded in myriad mechanisms; it infuses entertainment, communication, and other activities of everyday life. Designs for these applications are constrained not only by traditional considerations of capability and performance but also by economic, business, market, and policy issues and the context of intended use. The diversity of applications requires adaptability in responding to client needs, and the diversity of clients and contexts requires the ability to discriminate among criteria for success. As a result, software designers must also get out of their boxes: in addition to mastering traditional software development skills, they must understand the contextual issues that discriminate good solutions from merely competent ones. Current software engineering education, however, remains largely ""in the box"": it neglects the rich fabric of issues that lie between the client's problem and actual software development. At Carnegie Mellon we have addressed this major shortcoming with a course that teaches students to understand both the capabilities required by the client and the constraints imposed by the client's context. This paper presents our view of the engineering character of software engineering, describes the content and organization of our new course, reports on our experience from the first three offerings of our course, and suggests ways to adapt our course for other educational settings.","Software engineering,
Application software,
Programming,
Control engineering education,
Communication system software,
Embedded software,
Communication system control,
Control systems,
Business communication,
Context"
An empirical exploration of using Wiki in an English as a second language course,"In this paper, we present an empirical study of using a new and cost-effective Web-based collaboration software, Wiki, in a freshman-level English as a second language (ESL) course. This paper explores and observes the scenario: what if the Wiki tool were to be used in an English as a second language course in Taiwan? Students who attended this study practiced English writing on a Wiki Web site. The data about their usage and learning achievements was collected and analyzed. Our finding of a significant, but inverse, relation between students' editing usage and academic performance challenges some idealistic hypotheses that Wiki technology is ""naturally beneficial"" to learning. We believe that building an instructive or constructive instructional model with Wiki in a rigorous manner requires more empirical evidence. This study provides fresh evidence that will hopefully serve as an impetus to fill that gap.","Natural languages,
Educational institutions,
Collaborative software,
Writing,
Collaboration,
Collaborative work,
Application software,
Information science,
Computer science education,
Buildings"
Progressive d-separating edge set bounds on network coding rates,"A bound on network coding rates is developed that generalizes an edge-cut bound on routing rates. The bound involves progressively removing edges from a network graph and checking whether certain strengthened d-separation conditions are satisfied. The bound improves on the cut-set bound, and its efficacy is demonstrated by showing that routing is rate-optimal for some commonly cited examples in the networking literature","Network coding,
Routing,
Bayesian methods,
Random variables,
Computer science,
Artificial intelligence"
BMPGA: a bi-objective multi-population genetic algorithm for multi-modal function optimization,"This paper introduces two innovations into the world of multi-modal function optimization: a new multi-population genetic algorithm (GA), with two complementary fitness terms (called BMPGA); and a new similarity function that is used to decide whether two points belong to the same cluster or not, called recursive middling (RM). An empirical comparative study is carried out to provide evidence that RM is a better measure of similarity than Ursem's hill-valley (or HV) function. Another comparative study compares the performance of BMGA with our own single-fitness-term multi-population GA (SMPGA), and with Ursem's multinational GA (MGA). The results show the clear superiority of RM and BMPGA over HV and MGA, respectively. The results also point to the potential of introducing a new aspect to the field of multi-modal optimization, where various complementary (as opposed to competitive) objectives are used to maintain diversity, so the GA can find all the optima of a given fitness surface","Genetic algorithms,
Genetic engineering,
Computer science,
Software engineering,
Technological innovation,
Search methods"
Integrating Genetic Algorithms and Fuzzy c-Means for Anomaly Detection,"The goal of intrusion detection is to discover unauthorized use of computer systems. New intrusion types, of which detection systems are unaware, are the most difficult to detect. The amount of available network audit data instances is usually large; human labeling is tedious, time-consuming, and expensive. Traditional anomaly detection algorithms require a set of purely normal data from which they train their model. In this paper we propose an intrusion detection method that combines Fuzzy Clustering and Genetic Algorithms. Clustering-based intrusion detection algorithm which trains on unlabeled data in order to detect new intrusions. Fuzzy c-Means allow objects to belong to several clusters simultaneously, with different degrees of membership. Genetic Algorithms (GA) to the problem of selection of optimized feature subsets to reduce the error caused by using land-selected features. Our method is able to detect many different types of intrusions, while maintaining a low false positive rate. We used data set from 1999 KDD intrusion detection contest.",
Self-organization in Cooperative Content Distribution Networks,"Traditional client-server content distribution techniques usually suffer from scalability problems when dealing with large client population or sizable content. The advent of peer-to-peer (P2P) network offers the technical means to efficiently distribute data to millions of clients simultaneously with very low infrastructural cost. Previous studies of content distribution architectures have primarily focused on homogeneous systems, where the bandwidth capacities of all peers are similar. In this paper, we address the problem of heterogeneity and we propose mechanisms to improve content distribution efficiency by dynamically reorganizing the P2P network based on the effective bandwidth of the peers. Our techniques have been designed to be efficient in heterogeneous settings, adaptive so as to tolerate runtime changes like bandwidth fluctuations, and practical enough to be implementable in real systems. We analyze their effectiveness by the means of simulations and experimental evaluation","Intelligent networks,
Bandwidth,
Peer to peer computing,
Costs,
Runtime,
Fluctuations,
Analytical models,
Computer networks,
Network servers,
Multicast protocols"
On error detection and error synchronization of reversible variable-length codes,"Reversible variable-length codes (RVLCs) are not only prefix-free but also suffix-free codes. Due to the additional suffix-free condition, RVLCs are usually nonexhaustive codes. When a bit error occurs in a sentence from a nonexhaustive RVLC, it is possible that the corrupted sentence is not decodable. The error is said to be detected in this case. We present a model for analyzing the error detection and error synchronization characteristics of nonexhaustive VLCs. Six indices, the error detection probability, the mean and the variance of forward error detection delay length, the error synchronization probability, the mean and the variance of forward error synchronization delay length are formulated based on this model. When applying the proposed model to the case of nonexhaustive RVLCs, these formulations can be further simplified. Since RVLCs can be decoded in backward direction, the mean and the variance of backward error detection delay length, the mean and the variance of backward error synchronization delay length are also introduced as measures to examine the error detection and error synchronization characteristics of RVLCs. In addition, we found that error synchronization probabilities of RVLCs with minimum block distance greater than 1 are 0.","Delay,
Decoding,
Forward contracts,
Error analysis,
Length measurement,
Multimedia communication,
Laboratories,
Computer science,
Electronic mail,
Video coding"
Web service group testing with windowing mechanisms,"ASTRAR provides a framework for testing Web services (WS) using the group testing technique. This paper extends the basic two-phase testing process and introduces the windowing mechanism to further improve testing efficiency. Rather than testing a large number of WS simultaneously, WS are divided into subsets called windows and testing is exercised window by window. Testing results are analyzed for different strategies such as using all of the historical data, using the most recent windows, and using the current window only. Based on the results, test cases are ranked according to their potency to detect faults; and oracles and the confidence level of each oracle are established for individual test cases at runtime. In addition, different strategies are proposed to determine the optimal window size at runtime. By incorporating the windowing mechanism, the two-phase training and volume testing process becomes a continuous learning process and the basic group testing process becomes more adaptive to dynamically changing environment.","Web services,
Service oriented architecture,
Runtime,
Automatic testing,
System testing,
Computer science,
Fault detection,
Software systems,
Collaboration,
Sun"
Fuzzy Logic-based Recognition of Gait Changes due to Trip-related Falls,"The main aim of this paper is to explore application of fuzzy rules for automated recognition of gait changes due to falling behaviour. Minimum foot clearance (MFC) during continuous walking on a treadmill was recorded on 10 healthy elderly and 10 elderly with reported balance problem and tripping falls. MFC histogram characteristic features were used as inputs to the set of fuzzy rules; the features were extracted based on estimating the clusters in the data. Each of the clusters found corresponded to a new fuzzy rule, which were then applied to associate the input space to an output region. Gradient descent method was used to optimise the rule parameters. Both cross-validation and Jack-knife (leave-one-out) techniques were utilized for training the models and subsequently, testing the performance of the optimized fuzzy model. Receiver operating characteristics (ROC) plots, as well as accuracy rates were used to evaluate the performance of the developed model. Test results indicated up to a maximum of 95% accuracy in discriminating the healthy and balance-impaired gait patterns. These results suggest good potentials for fuzzy logic to use as gait diagnostics","Fuzzy logic,
Senior citizens,
Testing,
Foot,
Legged locomotion,
Histograms,
Fuzzy sets,
Feature extraction,
Data mining,
Optimization methods"
Recursive polymorphic types and parametricity in an operational framework,"We construct a realizability model of recursive polymorphic types, starting from an untyped language of terms and contexts. An orthogonality relation e/spl perp//spl pi/ indicates when a term e and a context /spl pi/ may be safely combined in the language. Types are interpreted as sets of terms closed by biorthogonality. Our main result states that recursive types are approximated by converging sequences of interval types. Our proof is based on a ""type-directed"" approximation technique, which departs from the ""language-directed"" approximation technique developed by MacQueen, Plotkin and Sethi in the ideal model. We thus keep the language elementary (a call-by-name /spl lambda/-calculus) and unstratified (no typecase, no reduction labels). We also include a short account of parametricity, based on an orthogonality relation between quadruples of terms and contexts.","Equations,
Context modeling,
Layout,
H infinity control,
Lattices,
Terminology,
Logic,
Computer science"
Multiple signal detection using the Benjamini-Hochberg procedure,"We treat the detection problem for multiple signals embedded in noisy observations from a sensor array as a multiple hypothesis test based on log-likelihood ratios. To control the global level of the multiple test, we apply the false discovery rate (FDR) criterion proposed by Benjamini and Hochberg. The power of this multiple test has been investigated through narrow band simulations in previous studies. Here we extend the proposed method to broadband signals. Unlike the narrow band case where the test statistics are characterized by F-distribution, in the broadband case the test statistics have no closed form distribution function. We apply the bootstrap technique to overcome this difficulty. Simulations show that the FDR-controlling procedure always provides more powerful results than the FWE controlling procedure. Furthermore, the reliability of the proposed test is not affected by the gain in power","Signal detection,
Testing,
Sensor arrays,
Statistical analysis,
Statistical distributions,
Narrowband,
Communication system control,
Error correction,
Information science,
Computer science"
Magnetoencephalography cortical source imaging using spherical mapping,"This paper proposes a novel approach to enhancing results of magnetoencephalography cortical source imaging. The proposed approach utilizes bell-shaped functions defined on an inflated cortical surface, which has one-to-one correspondence with original tessellated cortical surface. The coefficients of the functions are then determined using sensitivity analysis with conjugate gradient updating scheme. Applications of the approach to a simulation study and a practical experiment have resulted in more stable and smoother brain source distribution, compared to conventional linear inverse approach.","Magnetoencephalography,
Image reconstruction,
Inverse problems,
Magnetic field measurement,
Multiple signal classification,
Surface reconstruction,
Computer science,
Sensitivity analysis,
Brain modeling"
A locating method for WLAN based location service,"General wireless local area network (WLAN) based location systems use the received signal strength (RSS) as location fingerprint without any additional changes to the WLAN infrastructure. However, in real environment RSS are impacted by many factors, such as multiple path transmission (multi-path), temperature, body, etc, which harm the accuracy of location. This paper presents a method which uses both the angle of arrival (AOA) and RSS as location fingerprints based on AP scanning. Compared to other WLAN based location systems, this method takes advantages of fewer AP, less suffering from the influence of signal and higher accuracy. Furthermore, in some ""clean"" outdoor/indoor environments, the method can even used to locate WLAN clients without the radio map which is the basic infrastructure of general WLAN based location systems. Finally, this paper analyzes the characteristics of the AP scanning based location systems, and makes a further discussion on client-side location system using the same method","Wireless LAN,
Fingerprint recognition,
Wireless networks,
Disaster management,
Global Positioning System,
Radar antennas,
Programming,
Computer science,
Temperature,
Indoor environments"
Understanding patterns of TCP connection usage with statistical clustering,"We describe a new methodology for understanding how applications use TCP to exchange data. The method is useful for characterizing TCP workloads and synthetic traffic generation. Given a packet header trace, the method automatically constructs a source-level model of the applications using TCP in a network without any a priori knowledge of which applications are actually present in a network. From this source-level model, statistical feature vectors can be defined for each TCP connection in the trace. Hierarchical cluster analysis can then be performed to identify connections that are statistically homogeneous and that are likely exerting similar demands on a network. We apply the methods to packet header traces taken from the UNC and Abilene networks and show how classes of similar connections can be automatically detected and modeled.","Telecommunication traffic,
Traffic control,
Protocols,
Web and internet services,
IP networks,
Computer science,
Statistics,
Operations research,
Application software,
Character generation"
A Distributed Q-Learning Algorithm for Multi-Agent Team Coordination,"Q-learning is an effective model-free reinforcement learning algorithm. However, Q-learning is centralized and competent only for single agent learning but not multi-agent learning because in later case the size of state-action space is huge and will grow exponentially with the number of agents increasing. In the paper we present a distributed Q-learning algorithm to solving this problem. In our algorithm, the tasks of learning optimal action policy are distributed to each agent in team but not a central agent. In order to reduce the size of action-state space of multi-agent team we introduce a state-action space sharing strategy of agent team, through which one agent in team can use the states already explored by other agents before and need not take time to explore these states again. Additionally, our algorithm has the ability to allocate sub-goals dynamically among agents according to environment changing, which can make agent team coordinate more efficiently. Experiments show the efficiency of our algorithm when it is applied to the benchmark problem of predator-prey pursuit game, also called pursuit game, in which a team of predators coordinate to capture a prey.","Machine learning algorithms,
Pursuit algorithms,
Multiagent systems,
Educational institutions,
Computer science,
Educational technology,
Laboratories,
Knowledge engineering,
Computer science education,
Learning systems"
Executable visual contracts,"Design by contract (DbC) is widely acknowledged to be a powerful technique for creating reliable software. DbC allows developers to specify the behavior of an operation precisely by pre- and post-conditions. Existing DbC approaches predominantly use textual representations of contracts to annotate the actual program code with assertions. In the unified modeling language (UML), the textual object constraint language (OCL) supports the specification of preand post-conditions by constraining the model elements that occur in UML diagrams. However, textual specifications in OCL can become complex and cumbersome, especially for software developers who are typically not used to OCL. In this paper, we propose to specify the pre-and post-conditions of an operation visually by a pair of UML object diagrams (visual contract). We define a mapping of visual contracts into Java classes that are annotated with behavioral interface specifications in the Java modeling language (JML). The mapping supports testing the correctness of the implementation against the specification using JML tools, which include a runtime assertion checker. Thus we make the visual contracts executable.","Contracts,
Unified modeling language,
Java,
Computer languages,
Software systems,
Robustness,
Programming,
Computer science,
Testing,
Runtime"
Sparse incremental regression modeling using correlation criterion with boosting search,"A novel technique is presented to construct sparse generalized Gaussian kernel regression models. The proposed method appends regressors in an incremental modeling by tuning the mean vector and diagonal covariance matrix of an individual Gaussian regressor to best fit the training data, based on a correlation criterion. It is shown that this is identical to incrementally minimizing the modeling mean square error (MSE). The optimization at each regression stage is carried out with a simple search algorithm re-enforced by boosting. Experimental results obtained using this technique demonstrate that it offers a viable alternative to the existing state-of-the-art kernel modeling methods for constructing parsimonious models.","Boosting,
Kernel,
Training data,
Covariance matrix,
Space technology,
Mean square error methods,
Solid modeling,
Computer science,
Radial basis function networks,
Learning systems"
Sequential performance of asynchronous conservative PDES algorithms,"The widespread use of sequential simulation in large scale parameter studies means that large cost savings can be made by improving the performance of these simulators. Sequential discrete event simulation systems usually employ a central event list to manage future events. This is a priority queue ordered by event timestamps. Many different priority queue algorithms have been developed with the aim of improving simulator performance. Researchers developing asynchronous conservative parallel discrete event simulations have reported exceptional performance for their systems running sequentially in certain cases. This paper compares the performance of simulations using a selection of high performance central event list implementations to that achieved using techniques borrowed from the parallel simulation community. Theoretical and empirical analysis of the algorithms is presented demonstrating the range of performance that can be achieved, and the benefits of employing parallel simulation techniques in a sequential execution environment.","Discrete event simulation,
Performance analysis,
Computational modeling,
Computer simulation,
Costs,
Algorithm design and analysis,
Telecommunication traffic,
Traffic control,
Computer science,
Large-scale systems"
Web services-based collaborative and cooperative computing,"This paper presents an integrated development process for Web services. The key differences with the traditional software development are that this new process involves collaboration and cooperation among all parties involved: developers, brokers, and clients. The process defines a new way of developing trustworthy Web services based on the existing Internet infrastructure. The paper serves as a roadmap to research on Web services specification, discovery, ontology, composition, re-composition, testing, reliability assessing, ranking, and collaboration and cooperation among all parities involved in Web services research, development, and application.","Collaboration,
Web services,
Testing,
Publishing,
Modular construction,
Web and internet services,
Runtime,
Computer science,
Programming,
Collaborative software"
Systematic debugging of real-time systems based on incremental satisfiability counting,"Real-time logic (RTL) (F. Jahanian et al., 1986, 1987, F. Wang et al., 1994) is useful for the verification of a safety assertion with respect to the specification of a real-time system. Since the satisfiability problem for RTL is undecidable, the systematic debugging of a real-time system appears impossible. This paper provides a first step towards this challenge. With RTL, each propositional formula corresponds to a verification condition. The number of truth assignments of a propositional formula helps to determine the timing constraints which should be added or modified to the system's specification. We have implemented a tool (called SDRTL, (S. Andrei et al., 2004)) that is able to perform systematic debugging. The confidence of our approach is high as we have evaluated SDRTL on several existing industrial-based applications.","Debugging,
Real time systems,
Safety,
Timing,
Computer science,
Logic,
Constraint theory"
Analog complex wavelet filters,"This paper presents an analog implementation of the complex wavelet transform using both the complex first order system (CFOS) and the Pade approximation. The complex wavelet filter design is based on the combination of the real and the imaginary state-space descriptions that implement the respective transfer functions. In other words, a complex filter is implemented by an ordinary state-space structure for the real part and an extra C matrix for the imaginary part. Several complex wavelets, such as Gabor, Gaussian and Morlet complex wavelets, are obtained and simulations demonstrate excellent approximations to the ideal wavelets.","Wavelet transforms,
Filters,
Mathematics,
Transfer functions,
Signal analysis,
Laboratories,
Computer science,
Signal detection,
Wavelet analysis,
Information analysis"
Scalable and adaptive multicast video streaming for heterogeneous and mobile users,"To provide scalable multicast video streaming services for heterogeneous and mobile users, we design a protocol called SAMP. It is based on a multicast service overlay model. By employing layered encoding, aggregated multicast, and user clustering, SAMP can handle heterogeneity, scalability and mobility very effectively. We conduct simulations and the results show the promising performance of SAMP.","Streaming media,
Multicast protocols,
Mobile computing,
Videoconference,
Scalability,
Computer science,
Encoding,
Video compression,
Wireless networks,
Bandwidth"
GIPSE: streamlining the management of simulation on the grid,"Although the grid allows the researcher to tap a vast amount of resources, the complexity involved in utilizing this power can make it unwieldy and time-consuming. The Grid Interface for Parameter Sweeps and Exploration (GIPSE) toolset aims to solve this issue by freeing users from script debugging, storage issues, and other minutiae involved in managing simulations on the grid. GIPSE, which interacts seamlessly with existing grid software, abstracts interactions with the grid to present a research-centric view of the process rather than the typical task-centric view. GIPSE offers an alternative interface to the grid that removes the need for application specific wrappers around parameter-driven simulations and provides an interface to build data sets for visualization. In this paper, the authors discussed how GIPSE bridges a critical gap between existing tools and management of the overarching data result.","XML,
Computational modeling,
Data visualization,
Bridges,
Grid computing,
Computer science,
Power engineering and energy,
Debugging,
Abstracts,
Application software"
Comparing Persistent Computing with Autonomic Computing,"This paper presents a comparative study to examine the relationship between autonomic computing and persistent computing from the various aspects including motivation problems, ideas, purposes, goals, underlying principles, design methodologies, and architectures","Biology computing,
Computer vision,
Concurrent computing,
Distributed computing,
Design methodology,
Computer architecture,
Protection,
Security,
Engineering management,
Software engineering"
Rigorous modal analysis of two-dimensional photonic crystal waveguides,"A rigorous approach for modal analysis of two-dimensional photonic crystal waveguides consisting of layered arrays of circular cylinders is presented. The mode propagation constants and the mode field profiles can be accurately obtained by a simpler matrix calculus, using the one-dimensional lattice sums, the T matrix of an isolated circular cylinder, and the generalized reflection matrices for a multilayered system. Numerical examples of the dispersion characteristics and field distributions are presented for lowest even and odd transverse electric modes of a coupled two-parallel photonic crystal waveguide with a square lattice of dielectric circular cylinders in a background free space.",
Formal Verification of Robot Movements - a Case Study on Home Service Robot SHR100,"Home service robots have received much attention from both academia and industry because home service robots have wide range of potential applications such as home security, cleaning, etc. The robots need to add or update services frequently according to the changing needs of human users. Furthermore, reactive nature of the robots add complexity to develop robot applications. These challenges raise safety issues seriously. Considering that safe operation of home service robots is crucial, current practice of validating robot applications is, however, not mature enough for wide deployment of home service robots. In this paper, we present our experience of developing and formally verifying discrete control software of Samsung Home Robot (SHR) using Esterel. We give a brief background on Esterel, then illuminate our result in formally verifying stopping behavior of SHR. Through the verification, we could detect and solve a feature interaction problem which caused the robot not to stop when a user commanded the robot to stop.","Formal verification,
Computer aided software engineering,
Service robots,
Robot kinematics,
Testing,
Application software,
Intelligent robots,
Computer science,
Humans,
Safety"
A fair resource allocation algorithm for peer-to-peer overlays,"Over the past few years, peer-to-peer (P2P) systems have become very popular for constructing overlay networks of many nodes (peers) that allow users geographically distributed to share data and resources. One non-trivial question is how to distribute the data in a fair and fully decentralized manner among the peers. This is important because it can improve resource usage, minimize network latencies and reduce the volume of unnecessary traffic incurred in large-scale P2P systems. In this paper we present a technique for fair resource allocation in unstructured peer-to-peer systems. Our technique uses the fairness index of a distribution as a measure of fairness and shows how to optimize the fairness of the distribution using only local decisions. Load balancing is achieved by replicating documents across multiple nodes in the system. Our experimental results demonstrate that our technique is scalable, has low overhead and achieves good load balance even under skewed demand.","Resource management,
Peer to peer computing,
Large-scale systems,
Sea measurements,
Information retrieval,
Protocols,
Computer science,
Data engineering,
Delay,
Telecommunication traffic"
Extended whole program paths,"We describe the design, generation and compression of the extended whole program path (eWPP) representation that not only captures the control flow history of a program execution but also its data dependence history. This representation is motivated by the observation that typically a significant fraction of data dependence history can be recovered from the control flow trace. To capture the remainder of the data dependence history we introduce disambiguation checks in the program whose control flow signatures capture the results of the checks. The resulting extended control flow trace enables the recovery of otherwise unrecoverable data dependences. The code for the checks is designed to minimize the increase in the program execution time and the extended control flow trace size when compared to directly collecting control flow and dependence traces. Our experiments show that compressed eWPPs are only 4% of the size of combined compressed control flow and dependence traces and their collection requires 20% more runtime overhead than overhead required for directly collecting the control flow and dependence traces.","History,
Size control,
Runtime,
Application software,
Software debugging,
Software testing,
Intrusion detection,
Optimizing compilers,
Registers,
Computer science"
Underwater mosaic creation using video sequences from different altitudes,"This paper presents a method for the automatic creation of 2D mosaics of the sea floor, using video sequences acquired at different altitudes above the sea floor. The benefit of using different altitude sequences comes from the fact that higher altitude sequences can be used to guide the motion estimation of the lower ones, thus increasing the robustness and efficiency of the mosaicing process. When compared to the case of single sequence mosaic creation, we show that by combining geometric information from different sequences, we are able to successfully estimate the registration topology of much lower altitude sequences. This results in higher resolution mapping of the sea floor. Illustrative results are presented using sequences of the same coral reef patch, captured with a single video camera. The sequences present some of the common difficulties of underwater 2D mosaicing, namely non-flat, moving environment and changing lighting conditions. The importance of this work is emphasized by fact that the presented methods require inexpensive image acquisition and processing equipment, thus potentially benefiting a very large group of marine scientists","Video sequences,
Sea floor,
Motion estimation,
Cameras,
Topology,
Optical sensors,
Navigation,
Testing,
Rendering (computer graphics),
Optical imaging"
Effectively using recurrently-connected spiking neural networks,"Recurrently connected spiking neural networks are difficult to use and understand because of the complex nonlinear dynamics of the system. Through empirical studies of spiking networks, we deduce several principles which are critical to success. Network parameters such as synaptic time delays and time constants and the connection probabilities can be adjusted to have a significant impact on accuracy. We show how to adjust these parameters to fit the type of problem.","Neural networks,
Recurrent neural networks,
Neurons,
Biological neural networks,
Biological information theory,
Delay effects,
Biological system modeling,
Computer science,
Electronic mail,
Muscles"
Distributed Blinding for Distributed ElGamal Re-Encryption,"A protocol is given to take an ElGamal ciphertext encrypted under the key of one distributed service and produce the corresponding ciphertext encrypted under the key of another distributed service, but without the plaintext ever becoming available. Each distributed service comprises a set of servers and employs threshold cryptography to maintain its service private key. Unlike prior work, the protocol requires no assumptions about execution speeds or message delivery delays. The protocol also imposes fewer constraints on where and when various steps are performed, which can bring improvements in end-to-end performance for some applications (e.g., a trusted publish/subscribe infrastructure). Two new building blocks employed - a distributed blinding protocol and verifiable dual encryption proofs - could have uses beyond re-encryption protocols",
Recognizing /spl omega/-regular languages with probabilistic automata,"Probabilistic finite automata as acceptors for languages over finite words have been studied by many researchers. In this paper, we show how probabilistic automata can serve as acceptors for /spl omega/-regular languages. Our main results are that our variant of probabilistic Buchi automata (PBA) is more expressive than non-deterministic /spl omega/-automata, but a certain subclass of PBA, called uniform PBA, has exactly the power of /spl omega/-regular languages. This also holds for probabilistic /spl omega/-automata with Streett or Rabin acceptance. We show that certain /spl omega/-regular languages have uniform PBA of linear size, while any nondeterministic Streett automaton is of exponential size, and vice versa. Finally, we discuss the emptiness problem for uniform PBA and the use of PBA for the verification of Markov chains against qualitative linear-time properties.","Automata,
Biological processes,
Speech recognition,
Process planning,
Costs,
Power measurement,
Probabilistic logic,
Computer science,
Polynomials"
"Comments and an improvement on ""A distributed algorithm of delay-bounded multicast routing for multimedia applications in wide area Networks""","In this correspondence, we first point out an error in Jia's algorithm (1998) by a counterexample. Second, we provide a fix and make an improvement to the part of Dynamic-Join. Finally, we give an analysis and a correctness proof of our algorithm.","Distributed algorithms,
Routing,
Intelligent networks,
Delay,
Multicast algorithms,
Computer science,
Algorithm design and analysis,
Costs,
Solids,
Councils"
Constraint-Based Motion Planning of Deformable Robots,"We present a novel algorithm for motion planning of a deformable robot in a static environment. Given the initial and final configuration of the robot, our algorithm computes an approximate path using the probabilistic roadmap method. We use ""constraint-based planning"" to simulate robot deformation and make appropriate path adjustments and corrections to compute a collision-free path. Our algorithm takes into account geometric constraints like non-penetration and physical constraints like volume preservation. We highlight the performance of our planner on different scenarios of varying complexity.","Motion planning,
Deformable models,
Path planning,
Orbital robotics,
Computational modeling,
Service robots,
Medical robotics,
Space exploration,
Robotics and automation,
Computer science"
Performance evaluation. Of the beacon period contraction algorithm in UWB MBOA MAC,"Beacon period (BP) is an overhead in UWB MBOA MAC, since no data frame is allowed during a BP. In this letter, we model the current BP contraction scheme in MBOA spec as a greedy distance-2 graph coloring algorithm, and formulate the BP contraction problem as a 0-1 programming problem. The theoretical numerical results show that the current BP contraction algorithm has good performance. Furthermore, we study the impact of device joining/leaving order on the performance of the algorithm. Simulation results demonstrate that the impact is not significant and the performance of the algorithm is stable with varied device ordering. Therefore, we conclude that there is no much room for any further significant improvement over the current BP contraction algorithm in MBOA spec.","Asia,
OFDM,
Wideband,
Job shop scheduling,
Personal area networks,
Broadcasting,
Telecommunications,
Computer science,
Power engineering computing,
Power engineering and energy"
A system for predicting the run-time behavior of Web services,"In a service oriented architecture requestors should be able to use the services that best fit their needs. In particular, for Web services it should be possible to fully exploit the advantages of dynamic binding. Up to now, no proposed solution allows the requesting agent to dynamically select the most ""convenient"" service at invoke time. The reason is that currently the requesting agents do not compare the runtime behavior of different services. In this paper, we propose a system that provides and exploits predictions about the behavior of Web services, expressed in terms of availability, reliability and completion time. We also describe a first prototype (eUDDIr) of the specification. EUDDIr relies on a scalable agents-based monitoring architecture that collects data on Web services runtime activity. The computed predictions are used by requesting agents to implement an effective dynamic service selection. Our proposal is well suited whenever requestors do not wish to explicitly deal with QoS aspects, or in the case that provider agents have no convenience in building up the infrastructure for guaranteed QoS, at the same time aiming to provide services of good quality to their customers. Furthermore, the adoption of eUDDIr effectively improves the service requestors ""satisfaction"" when they are involved in a Web services composition process.","Runtime,
Web services,
Service oriented architecture,
Computer science,
Monitoring,
Quality of service,
Mathematics,
Councils,
Availability,
Prototypes"
Concept Oriented Imitation Towards Verbal Human-Robot Interaction,"Imitation equips robots with a simple and natural interface to learn new tasks. Although abstraction is a remarkable feature of imitation that discriminates it from mimicking, there has been no enough research on this dimension of imitation. Relational concepts are the simplest type of abstract concepts and can be an appropriate start point. These concepts may be learned by combining perceptual categorization and classical conditioning. The paper will first formalize relational concept learning within an imitative context. Internal modules of the learning agent are considered to be functions. We will prove that in this case the concept-motor mapping becomes one-to-one which simplifies learning. A learning algorithm for the model will be also proposed and evaluated in a phoneme acquisition experiment with a large number of highly overlapped samples.","Cognitive robotics,
Intelligent robots,
Learning,
Humans,
Robotic assembly,
Process control,
Intelligent control,
Senior citizens,
Positron emission tomography,
Educational robots"
Integrating formal verification into an advanced computer architecture course,"This paper presents a sequence of three projects on design and formal verification of pipelined and superscalar processors: 1) a single-issue, five-stage DLX (an academic processor used widely for teaching pipelined execution and defined by Hennessy and Patterson in the first edition of their graduate textbook); 2) an extension of the DLX with exceptions and branch prediction; and 3) a dual-issue superscalar DLX. The projects were integrated into two editions of an advanced computer architecture course that was offered at the Georgia Institute of Technology, Atlanta, in the summer and fall 2002 and was taught to 67 students (25 of whom were undergraduates) in a way that required them to have no prior knowledge of formal methods. Preparatory homework problems included an exercise on design and formal verification of a staggered Arithmetic Logic Unit (ALU), pipelined in the style of the integer ALUs in the Intel Pentium 4. The processors were designed and formally verified with a tool flow that was used to formally verify the M/spl middot/CORE processor at Motorola and detected bugs.","Computer science education,
Microprocessors"
Learning styles in the classroom: approaches to enhance student motivation and learning,"A growing body of research suggests that increased learning gains can be achieved with college students when instruction is designed with learning styles in mind. In addition, several practitioners within the domains of science and engineering education have noted the importance of embedding a learning style approach within a variety of teaching strategies. Furthermore, attention to learning styles and learner diversity has been shown to increase student motivation to learn. In this interactive workshop, the research base on teaching and learning styles was outlined. Emphasis was placed on specific applications of teaching and learning styles in science, mathematics, engineering, and technology (SMET) education. Additional emphasis was placed on the critical role that a learning style approach can play in terms of SMET education. This workshop begins with an overview of several available learning style models and instruments. Information regarding how to choose the right assessment tool(s) for use with science, mathematics, engineering and technology students was shared. In addition, a description of some of the highlights of reported studies and programs involving learning styles in SMET education were given.","Engineering education,
Mathematics,
Instruments,
Psychology,
Civil engineering,
Educational institutions,
Educational technology,
Educational programs,
Physics education,
Engineering students"
A Robust Localization Algorithm for Mobile Robots with Laser Range Finders,"We present a robust localization algorithm for mobile robots with laser range finders, which takes feature based approach for reliable matching process as well as point based approach for accurate and stable pose calculation. An efficient sequential segmentation algorithm is suggested, which also performs least square fitting processes simultaneously. Hence robust line segment features can be obtained with much less computation time. A novel cost function for pose calculation is suggested and formulated, which makes it possible to compute the location and orientation accurately and reliably. We examined the validity of the proposed algorithm with various simulations and experiments, and revealed its robustness and accuracy compared to other typical localization algorithms. The results show that the localization error of the proposed algorithm is 40% to 80% less than conventional localization algorithms.","Mobile robots,
Cost function,
Laser theory,
Least squares methods,
Image segmentation,
Noise robustness,
Computer science,
Microwave integrated circuits,
Laser stability,
Computational modeling"
On Development and Evaluation of Prototype Mobile Decision Support for Hospital Triage,"Ambiguous triage scenarios in hospital emergency departments are often difficult to assess without decision support. Subjective assessments of such scenarios can either lead to under-triaging or over-triaging for which true conditions of patients are often not addressed within the required time. In this paper, we propose a decision support model that can guide a clinician when identifying the urgency of medical intervention when patient presents with ambiguous triage case. Our model is a heuristic approach that selects the best triage category, identifies corresponding discriminating attribute of the patient, and allows clinician to attach a level of confidence in the decision. We implemented this model as a mobile decision support system, called iTriage. Results of an initial evaluation of iTriage using fourteen paper-based adult triage scenarios showed that our model produced robust decisions for urgent scenarios. For non-urgent scenarios, the proposed model provided guidance especially when the scenarios were ambiguously stated.","Prototypes,
Hospitals,
Fingers,
Australia,
Pain,
Helium,
Health information management,
Computer science,
Software engineering,
Decision support systems"
Layered active appearance models,"Active appearance models (AAMs) provide a framework for modeling the joint shape and texture of an image. An AAM is a compact representation of both factors in a conditionally linear model. However, the standard AAM framework does not handle images which have missing features, or allow modification of certain structures in the image while leaving neighboring ones undeformed. We introduce the layered active appearance model (LAAM), which allows for missing features, occlusion, substantial spatial rearrangement of features, and which provides a more general representation that extends the applicability of the active appearance model","Active appearance model,
Principal component analysis,
Licenses,
Active shape model,
Computer science,
Eyes,
Nose,
Mouth,
Computer vision"
ROI Medical Image Watermarking Using DWT and Bit-plane,"Recently, the medical image has been digitized by the development of computer science and digitization of the medical devices. There are needs for database service of the medical image and long term storage because of the construction of PACS (picture archiving and communication system) following DICOM (digital imaging communications in medicine) standards, tele-medicine, and et al. Furthermore, authentication and copyright protection are required to protect the illegal distortion and reproduction of the medical information data. In this paper, we propose digital watermarking technique for medical image that prevents illegal forgery that can be caused after transmitting medical image data remotely. A wrong diagnosis may be occurred if the watermark is embedded into the whole area of image. Therefore, we embed the watermark into some area of medical image, except the decision area that makes a diagnosis so called region of interest (ROI) area in our paper, to increase invisibility. The watermark is the value of bit-plane in wavelet transform of the ROI for integrity verification. The experimental results show that the watermark embedded by the proposed algorithm can survive successfully in image processing operations such as JPEG lossy compression","Biomedical imaging,
Watermarking,
Discrete wavelet transforms,
Medical diagnostic imaging,
Image storage,
Picture archiving and communication systems,
Computer science,
Image databases,
DICOM,
Digital images"
Effective Work Practices for FLOSS Development: A Model and Propositions,"We review the literature on Free/Libre Open Source Software (FLOSS) development and on software development, distributed work and teams more generally to develop a theoretical model to explain the performance of FLOSS teams. The proposed model is based on Hackman's model of effectiveness of work teams, with coordination theory and collective mind to extend Hackman's model by elaborating team practices relevant to effectiveness in software development. We propose a set of propositions to guide further research.","Programming,
Open source software,
Licenses,
Application software,
Inspection,
Linux,
Operating systems,
Web server,
Internet,
Computer languages"
Integrating object and grasp recognition for dynamic scene interpretation,"Understanding and interpreting dynamic scenes and activities is a very challenging problem. In this paper, we present a system capable of learning robot tasks from demonstration. Classical robot task programming requires an experienced programmer and a lot of tedious work. In contrast, programming by demonstration is a flexible framework that reduces the complexity of programming robot tasks, and allows end-users to demonstrate the tasks instead of writing code. We present our recent steps towards this goal. A system for learning pick-and-place tasks by manually demonstrating them is presented. Each demonstrated task is described by an abstract model involving a set of simple tasks such as what object is moved, where it is moved, and which grasp type was used to move it","Layout,
Robot programming,
Writing,
Humans,
Sensor phenomena and characterization,
Object recognition,
Computer vision,
Laboratories,
Numerical analysis,
Computer science"
Sequential relevance vector machine learning from time series,"This paper presents an approach to sequential training of the relevance vector machine suitable for Bayesian learning from time series. The key idea is to perform simultaneous incremental optimization of both the weight parameters and their prior hyperparameters using data arriving successively one at a time. Algorithms for efficient sequential regularized dynamic learning rate training of the weights and gradient-descent training of their corresponding individual priors are derived. It is shown that this fast sequential RVM can outperform similar Bayesian kernel methods, like: batch RVM, fast RVM, variational RVM, and Gaussian processes on multistep ahead forecasting of time series.","Machine learning,
Kernel,
Bayesian methods,
Educational institutions,
Computer science,
Gaussian processes,
Algorithm design and analysis,
Data analysis,
History,
Working environment noise"
Fuzzy adaptive turbulent particle swarm optimization,"In this paper, we introduce turbulence in the particle swarm optimization (TPSO) and illustrate how this approach could be used for function optimization problems involving high dimensions. The proposed algorithm uses a minimum velocity threshold to control the velocity of particles. TPSO mechanism is similar to a turbulent pump, which supplies some power to the swarm system to explore new search spaces (better solutions). The minimum velocity threshold of the particles is tuned adoptively by using a fuzzy logic controller, which is further called as fuzzy adaptive TPSO (FATPSO). We evaluate and compare the performance of SPSO (Standard PSO), TPSO and FATPSO. Empirical results clearly demonstrate that the performance of SPSO degrades remarkably with the increase in the dimensions of the problem, while the influence is very little in the case of TPSO and FATPSO.","Particle swarm optimization,
Velocity control,
Fuzzy logic,
Chaos,
Computer science,
Space exploration,
Fuzzy control,
Programmable control,
Adaptive control,
Convergence"
Differentiated bandwidth sharing with disparate flow sizes,"We consider a multi-class queueing system operating under the discriminatory processor-sharing (DPS) discipline. The DPS discipline provides a natural approach for modeling the flow-level performance of differentiated bandwidth-sharing mechanisms. Motivated by the extreme diversity in flow sizes observed in the Internet, we examine the system performance in an asymptotic regime where the flow dynamics of the various classes occur on separate time scales. Specifically, from the perspective of a given class, the arrival and service completions of some of the competing classes (called mice) evolve on an extremely fast time scale. In contrast, the flow dynamics of the remaining classes (referred to as elephants) occur on a comparatively slow time scale. Assuming a strict separation of time scales, we obtain simple explicit expressions for various performance measures of interest, such as the distribution of the numbers of flows, mean delays, and flow throughputs. In particular, the latter performance measures are insensitive, in the sense that they only depend on the service requirement distributions through their first moments. Numerical experiments show that the limiting results provide remarkably accurate approximations in certain cases.","Bandwidth,
Delay,
Global Positioning System,
Mathematics,
Computer science,
Internet,
System performance,
Mice,
Fluid flow measurement,
Time measurement"
Forensic analysis of file system intrusions using improved backtracking,"Intrusion detection systems alert the system administrators of intrusions but, in most cases, do not provide details about which system events are relevant to the intrusion and how the system events are related. We consider intrusions of file systems. Existing tools, like BackTracker, help the system administrator backtrack from the detection point, which is a file with suspicious contents, to possible entry points of the intrusion by providing a graph containing dependency information between the various files and processes that could be related to the detection point. We improve such backtracking techniques by logging certain additional parameters of the file system during normal operations (real-time) and examining the logged information during the analysis phase. In addition, we use dataflow analysis within the processes related to the intrusion to prune unwanted paths from the dependency graph. This results in significant reduction in search space, search time, and false positives. We also analyze the effort required in terms of storage space and search time.","Forensics,
File systems,
Intrusion detection,
Event detection,
Information analysis,
Data analysis,
Computer science,
Real time systems,
Flow graphs,
Linux"
Self-managing systems: a control theory foundation,"The high cost of operating large computing installations has motivated a broad interest in reducing the need for human intervention by making systems self-managing. This paper explores the extent to which control theory can provide an architectural and analytic foundation for building self-managing systems, either from new components or layering on top of existing components. Further, we propose a deployable testbed for autonomic computing (DTAC) that we believe will reduce the barriers to addressing key research problems in autonomic computing. The initial DTAC architecture is described along with several problems that it can be used to investigate.","Control theory,
Resource management,
Computer architecture,
Costs,
Monitoring,
Conference management,
Engineering management,
Humans,
Buildings,
Automation"
Advance reservation and co-allocation protocol for grid computing,"This paper specifies an advance reservation and co-allocation protocol for grid computing, and a simple framework for supporting well defined charging models. The protocol supports arbitrary level of nesting, that is a reservation may be to a virtual resource (a set of resources). The work is motivated by the need for advance reservation in the RealityGrid project. However the protocol is a generic protocol that can reserve any type of resource (e.g. clusters, network, and storage) and for any grid project where users need to reserve one or more resources in advance",
Watershed segmentation for carotid artery ultrasound images,"Summary form only given. This paper introduces a novel segmentation scheme for carotid artery ultrasound images. The proposed scheme is based on watershed segmentation algorithm. It consists of four major stages. These stages are preprocessing, watershed segmentation, region merging and finally boundary extraction. The proposed scheme is tested using a set of carotid artery ultrasound images. The experimental results show that the proposed scheme can produce accurate contours.","Image segmentation,
Carotid arteries,
Ultrasonic imaging,
Morphology,
Angiography,
Data mining,
Deformable models,
Computer science,
Merging,
Testing"
Ant colony optimization based network intrusion feature selection and detection,"This paper proposes a novel intrusion detection approach by applying ant colony optimization for feature selection and SVM for detection. The intrusion features are represented as graph-ere nodes, with the edges between them denoting the adding of the next feature. Ants traverse through the graph to add nodes until the stopping criterion is satisfied. The fisher discrimination rate is adopted as the heuristic information for ants' traversal. In order to avoid training of a large number of SVM classifier, the least square based SVM estimation is adopted. Initially, the SVM is trained based on grid search method to obtain discrimination function using the training data based on all features available. Then the feature subset produced during the ACO search process is evaluated based on their abilities to reconstruct the reference discriminative function using linear least square estimation. Finally SVM is retrained using the train data based on the obtained optimal feature subset to obtain intrusion detection model. The MIT's KDD Cup 99 dataset is used to evaluate our present method, the results clearly demonstrate that the method can be an effective way for intrusion feature selection and detection.","Ant colony optimization,
Computer vision,
Intrusion detection,
Support vector machines,
Support vector machine classification,
Machine learning,
Information security,
Least squares approximation,
Learning systems,
Information science"
Incorporating a secure coprocessor in the database-as-a-service model,"In this paper, we suggest an extension to the database-as-a-service (DAS) model that introduces a secure coprocessor (SC) at an untrusted database service provider in order to overcome drawbacks in the plain DAS model. The processor serves as a neutral party between the clients and service providers with the goal of increasing security of outsourced data. Additionally, it supports a much broader range of queries performed and reduces both bandwidth and computational burdens on the client. We expect these improvements to make the DAS model more viable and attractive from a client's perspective.","Coprocessors,
Databases,
Bandwidth,
Cryptography,
Data security,
Computer science,
Costs,
Physics computing,
Hardware,
Application software"
Virtual reality in science and engineering education,"This article discusses the current use of virtual reality tools and their potential in science and engineering education. One programming tool in particular, the Virtual Reality Modeling Language (VRML) is presented in light of its applications and possibilities in the development of computer visualization tools for education. VRML can be combined with other software development tools to create interactive dynamic computer graphics to assist in the teaching and understanding of scientific and technological principles. One contribution of this article is to present software tools and provide examples that may encourage educators to develop virtual reality models to enhance teaching in their own discipline","Virtual reality,
Engineering education,
Visualization,
Computer graphics,
Software tools,
Computer science education,
Educational robots,
Educational programs,
Layout,
Computer languages"
Software Architecture Reliability Analysis Using Failure Scenarios,We propose a Software Architecture Reliability Analysis (SARA) approach that benefits from both reliability engineering and scenario-based software architecture analysis to provide an early reliability analysis of the software architecture. SARA makes use of failure scenarios that are prioritized with respect to the user-perception in order to provide a severity analysis for the software architecture and the individual components.,"Software architecture,
Failure analysis,
Fault trees,
Computer architecture,
Reliability engineering,
Probability,
Computer science,
Embedded system,
Fault diagnosis,
Decoding"
Minimization of randomized unit test cases,"We describe a framework for randomized unit testing, and give empirical evidence that generating unit test cases randomly and then minimizing the failing test cases results in significant benefits. Randomized generation of unit test cases (sequences of method calls) has been shown to allow high coverage and to be highly effective. However, failing test cases, if found, are often very long sequences of method calls. We show that Zeller and Hildebrandt's test case minimization algorithm significantly reduces the length of these sequences. We study the resulting benefits qualitatively and quantitatively, via a case study on found open-source data structures and an experiment on lab-built data structures","Computer aided software engineering,
Software testing,
Debugging,
Automatic testing,
System testing,
Minimization methods,
Open source software,
Data structures,
Performance evaluation,
Computer science"
Java learning object ontology,"This paper presents an ontology, the Java learning object ontology - JLOO, for organizing learning objects of Java courses in an adaptive e-learning environment. The classification is based on the computing curricula CC2001 of the ACM and IEEE/CS. Using the curriculum as a guideline, the ontology defines the atomic knowledge units (i.e. learning objects) for an introductory course of Java programming. The most significant contributions of JLOO are: 1) defining the atomic knowledge units of introductory courses of Java language, and the relationships among them, 2) making the knowledge units of JLOO sharable and reusable, 3) allowing different learning strategies of a e-learning environment to choose dynamically, using JLOO as a guideline, different learning paths, and 4) making the realization of adaptive learning easy.","Java,
Ontologies,
Programming profession,
Guidelines,
Organizing,
Electronic learning,
Intelligent networks,
Object oriented modeling,
Computer science,
Systems engineering and theory"
Effective generation of test sequences for structural testing of concurrent programs,"One common approach to test sequence generation for structurally testing concurrent programs involves constructing a reachability graph (RG) and selecting a set of paths from the graph to satisfy some coverage criterion. It is often suggested that test sequence generation methods for testing sequential programs based on a control flow graph (CFG) can also be used to select paths from a RG for testing concurrent programs. However, there is a major difference between these two, as the former suffers from a feasibility problem (i.e., some paths in a CFG may not be feasible at run-time) and the latter does not. As a result, even though test sequence generation methods for sequential programs can be applied to concurrent programs, they may not be efficient. Moreover, in order to reduce testing effort and costs, it is important to reduce the number of test sequences being generated. Stated differently, we need methods which can generate efficient test sequences to increase the coverage in an effective way. We propose four different methods - two based on hot spot prioritization and two based on topological sort -to effectively generate a small set of test sequences that cover all the nodes in a RG. The same methods are also applied to the corresponding dual graph for generating test sequences to cover all the edges. A case study was conducted to demonstrate the use of our methods.","Roentgenium,
Sequential analysis,
Computer science,
Yarn,
Computer bugs,
System testing,
Flow graphs,
Runtime,
Costs,
Delay"
Single-user tracing and disjointly superimposed codes,"The zero-error capacity region of r-out-of-T user multiple-access OR channel is investigated. A family F of subsets of [n] = {1, ..., n} is an r-single-user-tracing superimposed code (r-SUT) if there exists such a single-user-tracing function /spl phi/:2/sup [n]/ /spl rarr/ F that for all F' /spl sube/ F with 1 /spl les/ |F'| /spl les/ r, /spl phi/(/spl cup//sub A/spl isin/F/'A) /spl isin/ F'. In this correspondence, we introduce the concept of these codes and give bounds on their rate. We also consider disjointly r-superimposed codes.","Genomics,
Bioinformatics,
Cloning,
Testing,
DNA,
Licenses,
Information theory,
Computer science,
Operations research,
Automation"
Knowledge discovery in clinical databases with neural network evidence combination,"Diagnosis of diseases and disorders afflicting mankind has always been a candidate for automation. Numerous attempts made at classification of symptoms and characteristic features of disorders have rarely used neural networks due to the inherent difficulty of training with sufficient data. But, the inherent robustness of neural networks and their adaptability in varying relationships of input and output justifies their use in clinical databases. To overcome the problem of training under conditions of insufficient and incomplete data, we propose to use three different neural network classifiers, each using a different learning function. Consequent combination of their beliefs by Dempster-Shafer evidence combination overcomes weaknesses exhibited by any one classifier to a particular training set. We prove with conclusive evidence that such an approach would provide a significantly higher accuracy in the diagnosis of disorders and diseases.","Intelligent networks,
Databases,
Neural networks,
Diseases,
Medical diagnostic imaging,
Data mining,
Robustness,
Cancer,
Back,
Computer science"
Distributed multi-robot coalitions through ASyMTRe-D,"This paper presents a distributed reasoning system, called ASyMTRe-D, which enables a team of robots to form coalitions to accomplish a multi-robot task through tightly-coupled sensor sharing. The theoretical foundation of the negotiation protocol is ASyMTRe, an approach we developed previously to synthesize task solutions according to the task requirements and the team composition. The goal of the ASyMTRe approach is to increase the task solution capabilities of heterogeneous multi-robot teams by changing the fundamental abstraction from the typical ""task"" abstraction to a ""schema"" abstraction and automatically reconfigure the schemas to address the task at hand. The decision-making in this prior work was fully centralized; the current paper presents a distributed version of this approach based on the contract net protocol, which can achieve higher levels of robustness than the centralized version. The purpose here is not to improve the original protocol, but to apply it to our problem so that the autonomous task solution capabilities of robots can be achieved in a distributed manner. Simulation results are provided to validate the protocol with performance analysis. Finally, we compare ASyMTRe-D with the centralized ASyMTRe. Our future objective is to enable the human designer to specify the desired balance between solution quality and robustness, enabling the reasoning approach to invoke the appropriate level of information-sharing among robots to reach the specified solution characteristics.","Robot sensing systems,
Robotics and automation,
Protocols,
Robustness,
Humans,
Intelligent robots,
Intelligent sensors,
Navigation,
Laboratories,
Computer science"
Architecture recovery and abstraction from the perspective of processes,"For the increasing complexity of software systems, the main work of software development, maintenance and evolution has been focused on the comprehension of the existing systems. In order to help users comprehend at all aspects and levels of a target system, it is necessary to reversely recover and abstract its high-level architecture, which can reflect the framework and holistic behavioral features of the software system. This paper deals with the problems of architecture recovery from the perspective of process. An approach of extracting process structure graph (PSG) from a target system is presented based on the features of the relations among processes on UNIX platform. First, the static code fragment of a dynamic process can be recognized, then a mapping algorithm that can identify the correspondence between the dynamic process ID and the static process module is given. On the basis of the algorithm, an incremental construction algorithm of PSG and a slicing algorithm for class structure in a process module are implemented respectively. The experimental results show that the extracted PSG is correct, effective and can reflect the high-level structure of the target system in detail at the process level.","Software systems,
Computer architecture,
Software maintenance,
Costs,
Software engineering,
Programming,
Reverse engineering,
Software design,
Modular construction,
Computer science"
Isomap and Nonparametric Models of Image Deformation,"Isomap is an exemplar of a set of data driven non-linear dimensionality reduction techniques that have shown promise for the analysis of images and video. These methods parameterize each image as coordinates on a low-dimensional manifold, but, unlike PCA, the low dimensional parameters do not have an explicit meaning, and are not natural projection operators between the high and low-dimensional spaces. For the important special case of image sets of an unknown object undergoing an unknown deformation, we show that Isomap gives a valuable pre-processing step to find an ordering of the images in terms of their deformation. Using the continuity of deformation implied in the Isomap ordering allows more accurate solutions for a thin-plate spline deformation from a specific image to all others. This defines a mapping between the Isomap coordinates and a specific deformation, which is extensible to give projection functions between the image space and the Isomap space. Applications of this technique are shown for cardiac MRI images undergoing chest cavity deformation due to patient breathing.",
"Performance analysis of the confidentiality security service in the IEEE 802.11 using WEP, AES-CCM, and ECC","The wired equivalent privacy (WEP) protocol presents several vulnerabilities and therefore the IEEE 802.11 should use other cryptosystem, symmetric or asymmetric. For the former, this paper proposes the usage of advanced encryption standard-counter with cipher block chaining-message authentication code (AES-CCM); for the second the usage of elliptic curve cryptosystems (ECC). This paper presents the time required for encrypting the frame body field of the IEEE 802.11 MAC frame using WEP, AES-CCM, and ECC for several data sizes and comparable key sizes. Results confirm that symmetric crypto algorithms are more efficient than those asymmetries for providing the confidentiality service, with WEP being fastest followed by AES-CCM.","Performance analysis,
Cryptography,
Payloads,
Algorithm design and analysis,
Counting circuits,
Authentication,
Galois fields,
Computer security,
Computer science,
Cities and towns"
An online partially fractional knapsack problem,"The knapsack problem can and has been used to model many resource sharing problems. The allocation of a portion of a resource to a particular agent provides a benefit to the system, but also blocks other agents from utilizing that portion of the resource. For a problem where the number of agents as well as each agent's demand and potential benefit are known prior to any decision being made, the optimal allocation and its value can be calculated. In many situations these values are not known initially, but only learned over time. Online algorithms and competitive analysis are often employed when a problem requires decisions to be made prior to having all information available. In this paper we suggest an online version of the knapsack problem, provide some justification for the model, give the exact competitive ratio for the problem in the deterministic case, and provide bounds on the competitive ratio in the randomized case.","Bandwidth,
Resource management,
Cost function,
Computer science,
Algorithm design and analysis,
Information analysis,
Hard disks,
Operations research,
Performance analysis,
Parallel architectures"
Handling nominal features in anomaly intrusion detection problems,"Computer network data stream used in intrusion detection usually involve many data types. A common data type is that of symbolic or nominal features. Whether being coded into numerical values or not, nominal features need to be treated differently from numeric features. This paper studies the effectiveness of two approaches in handling nominal features: a simple coding scheme via the use of indicator variables and a scaling method based on multiple correspondence analysis (MCA). In particular, we apply the techniques with two anomaly detection methods: the principal component classifier (PCC) and the Canberra metric. The experiments with KDD 1999 data demonstrate that MCA works better than the indicator variable approach for both detection methods with the PCC coming much ahead of the Canberra metric.","Intrusion detection,
Computer networks,
Traffic control,
Telecommunication traffic,
Data mining,
Computer science,
Laboratories,
National security,
Application software,
Feature extraction"
Reliable estimation of influence fields for classification and tracking in unreliable sensor networks,"The influence field of an object, a commonly exploited feature in science and engineering applications, is the region where the object is detectable by a given sensing modality. Being spatially distributed, this feature allows us to tradeoff nodal computation with network communication. By the same token, not only is its calculation subject to nodal failures and false detections, but also to channel fading and channel contention. In this paper, we study how to accurately and efficiently estimate the influence fields of objects in such an unreliable setting and how this reliable estimation of influence fields can be used to classify and track different types of objects. We derive, for node and network fault models, the necessary nodal density for reliably estimating the influence fields so that objects can be classified and tracked. We present four algorithmic techniques: temporal aggregation, probabilistic reporting, temporal segregation and spatial reconstruction, to deal with cases where the effective network density differs from this minimum. We provide corroboration of our analysis through field experiments with Mica2 sensor nodes wherever appropriate. Finally, we demonstrate how these results and techniques were applied to achieve reliable and efficient classification and tracking in a fielded system of 90 Mica2 sensor nodes that we called ""A Line In The Sand'.",
Incorporating Object Tracking Feedback into Background Maintenance Framework,"Adaptive background modeling/subtraction techniques are popular, in particular, because they are able to cope with background variations that are due to lighting variations. Unfortunately these models also tend to adapt to foreground objects that become stationary for a period of time; as a result such objects are no longer considered for further processing. In this paper, we propose the first (to our knowledge) statistically consistent method for incorporating feedback from high-level motion model to modify adaptation behavior. Our approach is based on formulating the background maintenance problem as inference in a continuous state Hidden Markov Model, and combining it with a similarly formulated object tracker in a multichain graphical model framework. We demonstrate that the approximate filtering algorithm in such a framework outperforms the common feed-forward system while not imposing a significant extra computational burden.","Feedback,
Layout,
Hidden Markov models,
Graphical models,
Filtering algorithms,
Computer science,
Artificial intelligence,
Laboratories,
Subtraction techniques,
Adaptation model"
Features and advantages of WME: A Web-based mathematics education system,"An ongoing project creates a Web-based mathematics education (WME) system by an innovative combination of standard Web technologies. WME delivers classroom-ready lessons that are well-prepared, interesting, effective, as well as interoperable. Lesson pages contain easy to use special-purpose manipulatives to help students understand and explore mathematical concepts and skills through hands-on activities. Manipulatives are parameterized for easy customization and object-oriented for interoperability. Mathematics-oriented chat and bulletin boards facilitate communication and discussion. Additional features enable teachers to easily tailor lessons to suit the needs of their classes, to create and conduct assessments, and to help students overcome difficulties. A pilot project puts WME through in-class trials with a prototype site, a step towards a model WME site that can be easily deployed to different schools. WME has many advantages and the potential to create a Web for mathematics education, to foster a new paradigm for supporting and delivering mathematics education, and to help mathematics curricula improve exponentially.",
Identity-based ring signcryption schemes: cryptographic primitives for preserving privacy and authenticity in the ubiquitous world,"In this paper, we present a new concept called an identity based ring signcryption scheme (IDRSC,). We argue that this is an important cryptographic primitive that must be used to protect privacy and authenticity of a collection of users who are connected through an ad-hoc network, such as Bluetooth. We also present an efficient IDRSC scheme based on bilinear pairing. As a regular signcryption scheme, our scheme combines the functionality of signature and encryption schemes. However, the idea is to have an identity based system. In our scheme, a user can anonymously sign-crypts a message on behalf of the group. We show that our scheme outperforms a traditional identity based scheme, that is obtained by a standard sign-then-encrypt mechanism, in terms of the length of the ciphertext. We also provide a formal proof of our scheme with the chosen cipher-text security under the decisional bilinear Diffie-Hellman assumption, which is believed to be intractable.",
Analytical model of sparse-partial wavelength conversion in wavelength-routed WDM networks,"Wavelength conversion is one of the key techniques to improve the blocking performance in wavelength-routed WDM networks. Given that wavelength converters nowadays remain very expensive, how to make effective use of wavelength converters becomes an important issue. In this letter, we analyze the sparse-partial wavelength conversion network architecture and demonstrate that it can significantly save the number of wavelength converters, yet achieving excellent blocking performance. Theoretical and simulation results indicate that, the performance of a wavelength-routed WDM network with only 1-5% off wavelength conversion capability is very close to that with full-complete wavelength conversion capability.","Analytical models,
Intelligent networks,
WDM networks,
Optical wavelength conversion,
Wavelength routing,
Wavelength conversion,
Costs,
Computer science,
Performance analysis,
Wavelength division multiplexing"
The WK-recursive pyramid: an efficient network topology,"In this paper, we present and evaluate a new topology for interconnection networks which we refer to as WK-pyramid. This network has a recursive definition quite similar to that of conventional pyramid networks, but is based on the WK-recursive mesh. The traditional pyramid, a desirable network topology, was employed as both software data structure and hardware architecture. However, the new network topology which we propose has, in addition to almost all the properties of the pyramid, a number of superior properties, such as expandability to any base size. We show that this hierarchical topology is of interesting topological characteristics, making it suitable for utilization as the base topology of large-scale multicomputer systems.",
Approach to steer-by-wire system design,"Steer-by-wire (SbW) systems are candidate to substitute the conventional (mechanical or hydraulic) steering systems in the new generation of vehicles. The task of a SbW system is twofold: turning the road wheels tracking the handwheel rotation and providing the driver with a feeling of the steering effort. In this paper, the issue of designing a SbW system is faced. An approach is proposed, based on three steps. Firstly, the model of a conventional steering system is formulated. Then the SbW system is developed with the same structure as the conventional ones. Finally, performance indexes for the steering maneuver are defined and utilized to set up the parameters of the SbW system. As a case of study, the SbW system of a lift truck is designed according to the proposed approach. Diagrams are given to demonstrate the effectiveness of the resultant SbW system","Vehicle safety,
Steering systems,
Wheels,
Shafts,
Vehicle driving,
Road vehicles,
Road safety,
Torque,
Computer science,
Turning"
Proof-theoretic approach to description-logic,"In recent work Baader has shown that a certain description logic with conjunction, existential quantification and with circular definitions has a polynomial time subsumption problem both under an interpretation of circular definitions as greatest fixpoints and under an interpretation as arbitrary fixpoints (introduced by Nebel). This was shown by translating definitions in the description logic (""TBoxes"") into a labelled transition system and by reducing subsumption to a question of the existence of certain simulations. In the case of subsumption under the descriptive semantics a new kind of simulation, called synchronised simulation, had to be introduced. In this paper, we also give polynomial-time decision procedures for these logics; this time by devising sound and complete proof systems for them and demonstrating that proof search is polynomial for these systems. We then use the proof-theoretic method to study the hitherto unknown complexity of description logic with universal quantification, conjunction, and GCI axioms. Finally, we extend the proof-theoretic method to negation and thus obtain a decision procedure for the description logic ALC with fixpoints. This last section is only sketched.","Polynomials,
Equations,
Automatic logic units,
Computer science"
A matrix product coprocessor for FPGA embedded soft processors,"This paper present a matrix coprocessor specialized for computing matrix product. The proposed circuit allows a significant reduction in the computational time required by a general-purpose processor for the sequential execution of the matrix product. The coprocessor has been integrated with a RISC processor and a field programmable system on chip has been realized. The new circuit uses p processor elements to perform a parallel computation of the matrix product during the data exchange with the host processor. Thanks to this, the communication time overhead is minimised and the whole computational time is reduced from O(n/sup 3/) to O(n/sup 2/) when n /spl les/ p.","Coprocessors,
Field programmable gate arrays,
Registers,
Computer architecture,
Computer science,
System-on-a-chip,
Reduced instruction set computing,
Testing,
Performance evaluation,
Technological innovation"
Position estimation for wireless sensor networks,"In wireless sensor networks, estimating nodal positions is important for routing efficiency and location-based services. Traditional techniques based on precise measurements are often expensive and power-inefficient, while approaches based on landmarks often require bandwidth-inefficient flooding and hence are not scalable for large networks. In this paper, we propose and investigate a cost-effective and distributed algorithm to accurately estimate nodal positions for wireless sensor networks. In our algorithm, a node only needs to identify and exchange information with a certain number of neighbors (around 30) in its proximity in order to estimate its relative nodal position accurately. For location-identification, only a small number of nodes (around 10) are needed to have additional GPS capabilities to accurately estimate the absolute position of every node in the network. Our algorithm is shown to have fast convergence with low estimation error, even for large networks","Wireless sensor networks,
Routing,
Global Positioning System,
Distributed algorithms,
Convergence,
Computer networks,
Councils,
Machine learning algorithms,
Computer science,
Estimation error"
Continuous tracking of user location in WLANs using recurrent neural networks,"Location is one of the contextual variables most relevant to the design of context-aware computing systems. These applications need to know the physical location of users in order to provide information relevant to their position. Radiofrequency (RF) signals received by mobile devices can be measured to obtain the signal strength. These signals can be used to estimate the approximate location of a user. In this paper, we present a technique based on recurrent neural networks to infer user location in WLANs inside buildings. The approach uses information from previous location estimations to address the problem of continuous user tracking. This means that we take advantage of the user trajectory to reduce the inherent error causing the user to ""jump"" between two places separated by large distances. We present the results of the proposed approach and analysis intended to reduce the effort of measuring RF signals.","Intelligent networks,
Wireless LAN,
Recurrent neural networks,
Context-aware services,
Context,
Radio frequency,
RF signals,
Human factors,
Application software,
Hospitals"
Towards the framework of adaptive user interfaces for eHealth,"Diversity inside a group of users having their individual abilities, interests, and needs challenge the developers of eHealth projects with heterogeneous needs in information delivery and/or other eHealth services. This paper considers an adaptive user interface approach as an opportunity in addressing this challenge. We briefly overview the recent achievements in the area of user interface adaptation and discuss application of these achievements in the eHealth context. We introduce the basic elements of our framework for adaptive user interface (AUI) for eHealth systems. Then, we use this framework in our review of work in the area of AUI for eHealth applications. As a result, we conclude with a brief discussion on the current focus on AUI research in eHealth, and interesting directions for further research.","User interfaces,
Environmental management,
Knowledge management,
Quality management,
Risk management,
Medical services,
Intelligent systems,
Internet,
Computer science,
Information systems"
E-ODMRP: enhanced ODMRP with motion adaptive refresh,"On demand multicast routing protocol (ODMRP) is a multicast routing protocol for mobile ad hoc networks. Its efficiency, simplicity, and robustness to mobility renders it one of the most widely used MANET multicast protocols. At the heart of the ODMRP's robustness is the periodic route refreshing. ODMRP rebuilds the data forwarding ""mesh"" on a fixed short interval. The route refresh interval has critical impact on protocol overhead and thus efficiency. If it is too high, the network undergoes too much routing overhead wasting valuable resources. If it is too low, ODMRP cannot keep up with network dynamics. In this paper, we present an enhancement of ODMRP with refresh rate dynamically adapted to the environment. An additional enhancement is ""unified"" local recovery and receiver joining. On joining or upon detection of a broken route, a node performs an expanding ring search to graft to the forwarding mesh. Simulation results show that the enhanced ODMRP (E-ODMRP) reduces overhead by up to 90% yet keeping similar packet delivery ratio compared to the original ODMRP.","Multicast protocols,
Routing protocols,
Mobile ad hoc networks,
Robustness,
Computer science,
Heart,
Bandwidth,
Unicast,
Admission control,
Multicast algorithms"
Pervasive Computing Based Multimodal Tele-Home Healthcare System,This paper discusses a pervasive computing based multimodal tele-home healthcare system in terms of a human-centered pervasive computing system model,"Pervasive computing,
Medical services,
Humans,
Application software,
Computer networks,
Biological system modeling,
Open systems,
Network servers,
Context awareness,
Software design"
On Space Exploration And Human Error - A Paper on Reliability and Safety,"NASA space exploration should largely address a problem class in reliability and risk management stemming primarily from human error, system risk and multi-objective trade-off analysis, by conducting research into system complexity, risk characterization and modeling, and system reasoning. In general, in every mission we can distinguish risk in three possible ways: a) known-known, b) known-unknown, and c) unknown-unknown. It is probable almost certain that space exploration will partially experience similar known or unknown risks embedded in the Apollo missions, Shuttle or Station unless something alters how NASA will perceive and manage safety and reliability.","Space exploration,
Humans,
Safety,
NASA,
Risk management,
Costs,
Space technology,
Space shuttles,
Mars,
Computer errors"
Analysis of an optical burst switching router with tunable multiwavelength recirculating buffers,"Optical burst switching (OBS) presents challenges to the design of optical routers. This paper considers how to dimension a router of N input data ports with an additional M fiber delay lines (FDLs) in an OBS network. The router incorporates tunable FDLs that can vary their size to fit the burst being buffered. Tunable FDLs can be approximated using a set of static FDLs of unequal sizes. For this, the size of static FDL set is monotonically increased, in step size increments, from minimum burst size until the throughput increase is equal to the corresponding tunable FDL configuration. Simulation results for a 32-input port router with 256 tunable delays achieve up to 20% higher throughput than static delays at high input port load. Multiple recirculations are a critical requirement; when packets can circulate only once through the buffer, no measurable improvement is achieved after the number of FDLs becomes equal to the number of input data ports. When recirculation is permitted, throughput increases by up to 40%, depending on a combination of the number of FDLs added and the recirculation limit, which must increase in tandem (e.g., for 32 buffers with eight recirculations or 256 buffers with 16 recirculations). For a given number of FDLs, there is an optimal recirculation limit beyond which there is no measurable throughput benefit. By varying the recirculation limit or number of FDLs, tunable buffering can match the gain achieved by wavelength conversion, possibly at lower hardware cost.","Optical burst switching,
Optical buffering,
Optical wavelength conversion,
Optical switches,
High speed optical techniques,
Optical packet switching,
Throughput,
Fabrics,
Optical fiber networks,
Computer science"
Evaluation of three wearable computer pointing devices for selection tasks,"This paper presents the results of an experiment comparing three commercially available pointing devices (a trackball, gyroscopic mouse and Twiddler2 mouse) performing selection tasks for use with wearable computers. The study involved 30 participants performing selection tasks with the pointing devices while wearing a wearable computer on their back and using a head-mounted display. The error rate and time to complete the selection of the circular targets was measured. When examining the results, the gyroscopic mouse showed the fastest mean time for selecting the targets, while the trackball performed with the lowest error rate.","Wearable computers,
Mice,
Error analysis,
Usability,
Laboratories,
Australia,
Target tracking,
Performance evaluation,
Thumb,
Information science"
The GriddLeS data replication service,"The grid provides infrastructure that allows an arbitrary application to be executed on a range of different computational resources. When input files are very large, or when fault tolerance is important, the data may be replicated. Existing grid data replication middleware suffers from two shortcomings. First, it typically requires modification to existing applications. Second, there is limited support on automatic resource selection and a user usually chooses the replica manually to optimize the performance of the system. In this paper we discuss a middleware layer called the GriddLeS replication service (GRS) that sits above existing replication services, solving both of these shortcomings. Two case studies are presented that illustrate the effectiveness of the approach","Grid computing,
Middleware,
Bandwidth,
Fault tolerance,
Availability,
Computer science,
Software engineering,
Australia,
Application software,
Processor scheduling"
"Fault-tolerant routing schemes in RDT(2,2,1)//spl alpha/-based interconnection network for networks-on-chip design","It has been well recognized that the fault-tolerance capability is vital for a NoC system, since one faulty link/processor may isolate a large fraction of processors. Continuing from a previous paper where a RDT(2,2,1)//spl alpha/-based interconnection network for NoC designs was proposed, we investigate fault-tolerant routing schemes on NoC systems featuring a RDT-based interconnection network. In particular, we propose two fault-tolerant routing schemes in the presence of either single link/node failure or multiple link/node failures. The proposed routing schemes are based on deterministic routing. Alternative routes are discovered by properly selecting the intermediate nodes between the source and the destination nodes on the rank tori. As of the single link/node failure case, we show that the number of routers on the detoured route generated by the proposed routing scheme is at most 2 more than the number of routers on the original route.","Fault tolerance,
Routing,
Intelligent networks,
Multiprocessor interconnection networks,
Network-on-a-chip,
Isolation technology,
Fault tolerant systems,
Very large scale integration,
Computer science,
Transistors"
Sentire: a framework for building middleware for sensor and actuator networks,"Sentire represents a framework for building extensible middleware for sensor and actuator networks (SANET). The fundamental principle behind Sentire is the partitioning of SANET middleware into logically related components in order to allow developers of different aspects of the middleware to share a common plug-in infrastructure where their developed artifacts can interact. In this paper, an extended introduction of Sentire is presented, followed by a practical illustration of how this framework can be used to build SANET middleware.","Middleware,
Actuators,
Application software,
Pervasive computing,
Intelligent sensors,
Sensor phenomena and characterization,
Resource management,
Sensor systems,
Conferences,
Computer science"
Enhancing the student project team experience with blended learning techniques,"Integrating student team projects into the engineering and computing curriculum presents a variety of challenges. At the Rochester Institute of Technology, the undergraduate introductory software engineering course has been redesigned from a traditional lecture-lab format to a project-centric studio format. The student team focus has allowed us to better incorporate active learning by promoting collaboration among students and instructors in the development of a term long software project. In order to make the studio format more effective we have adopted a blended learning approach. Blended learning aims to join the best of face-to-face classroom learning with the best of online teaching and learning to promote active independent learning and reduce class seat time. Student satisfaction in courses requiring a collaborative effort among their peers is heavily influenced by their project team experience. The use of blended learning techniques helps to make the experience a more satisfying one by increasing the effectiveness of collaboration in team activities and interaction with the instructor through the application of distance learning and social computing technologies. The challenges faced and the techniques and strategies utilized in the planning and delivery of the course was discussed, including the utilization of online learning support infrastructure. This paper presents instructor experiences, analysis of student feedback, lessons learned and recommendations for other educators considering the application of blended learning techniques for their courses","Software engineering,
Education,
Collaborative software,
Collaboration,
Educational programs,
Computer aided instruction,
Social network services,
Strategic planning,
Feedback,
Application software"
Construct Collaborative Distance Learning Environment with VNC Technology,"The collaborative distant learning environment has an extensive application prospect in modern computer supported network education. This paper presents a framework of constructing the distance collaborative learning environment with the virtual network computing technology. The cooperative participators use the VNC virtual desktop to share the same teaching program and cooperate in the common education tasks. The environment realizes and supports the collaborative distant learning functions of the one to one, one to more and more to more teaching modes with share-screen technology. The framework can be applied to build the distance collaborative learning environment with original teaching program quickly and economically.","Collaboration,
Computer aided instruction,
Collaborative work,
Computer science education,
Computer networks,
Application software,
Educational programs,
Educational technology,
Operating systems,
Environmental economics"
Particle Swarm Optimization with Local Search,"In this paper, we propose a hybrid algorithm of particle swarm optimization and local search (PSO-LS). In PSO-LS, each particle has a chance of self-improvement by applying local search algorithm before it communicates information with other particles in the swarm. Then we modify our basic PSO-LS by choosing specific good particles as initial solutions for local search. The comparative experiments were made between PSO-LS, modified PSO-LS and PSO with linearly decreasing inertia weight (PSO-LDW) on three benchmark functions. Results show hybrid algorithms of combining particle swarm optimization with local search techniques outperform PSO-LDW","Particle swarm optimization,
Equations,
Computer science,
Electronic mail,
Evolutionary computation,
Simulated annealing,
Stochastic processes,
Computational modeling,
Cultural differences,
Global communication"
Radix converters: complexity and implementation by LUT cascades,"In digital signal processing, we often use higher radix system to achieve high-speed computation. In such cases, we require radix converters. This paper considers the design of LUT cascades that convert p-nary numbers to g-nary numbers. In particular, we derive several upper bounds on the column multiplicities of decomposition charts that represent radix converters. From these, we can estimate the size of LUT cascades to realize radix converters. These results are useful to design compact radix converters, since these bounds show strategies to partition the outputs into groups.",
VQ-RED: An efficient virtual queue management approach to improve fairness in infrastructure WLAN,"In this paper, we consider two fairness problems (downlink/uplink fairness and fairness among flows in the same direction) that arise in the infrastructure WLAN. We propose a virtual queue management approach, named VQ-RED to address the fairness problems. We demonstrate the effectiveness of our approach by conducting a series of simulations. The results show that compared with standard DCF, VQRED not only greatly improves the fairness, but also reduces packet delays",
Preemptive rate-based operator scheduling in a data stream management system,"Summary form only given. Data stream management systems are being developed to process continuous queries over multiple data streams. These continuous queries are typically used for monitoring purposes where the detection of an event might trigger a sequence of actions or the execution of a set of specified tasks. Such events are identified by tuples produced by a query and hence, it is important to produce the available portions of a query result as early as possible. A core element for improving the interactive performance of a continuous query is the operator scheduler. An operator scheduler is particularly important when the processing requirements and the productivity of different streams are highly skewed. The need for an operator scheduler becomes even more crucial when tuples from different streams arrive asynchronously. To meet these needs, we are proposing a preemptive rate-based scheduling policy that handles the asynchronous nature of tuple arrival and the heterogeneity in the query plan. Experimental results show the significant improvements provided by our proposed policy.","Event detection,
Databases,
Processor scheduling,
Remote monitoring,
Computer science,
Computerized monitoring,
Productivity,
Stock markets,
Prototypes,
Data processing"
Visual code widgets for marker-based interaction,"We present a set of graphical user interface elements, called widgets, for 2 dimensional visual codes. The proposed widgets are suitable for printing on paper as well as showing on electronic displays. Visual code markers and their orientation parameters are recognizable by camera-equipped mobile devices in real time in the live camera image. The associated widgets are specifically designed for marker-based interaction. They define basic building blocks for creating applications that incorporate mobile devices as well as resources in the user's environment, such as paper documents, posters, and public electronic displays. In particular, we present visual code menus (vertical menus and pie menus), check boxes, radio buttons, sliders, dials, and free-form input widgets. We describe the associated interaction idioms and outline potential application areas.",
Test generation for ultra-high-speed asynchronous pipelines,"We propose a methodology for testing ultra-high-speed asynchronous pipelines, the latest and most promising asynchronous circuit design style. Unlike traditional delay-insensitive asynchronous micro-pipelines, which use slow capture-pass latches, these circuits employ aggressive handshaking protocols and transparent latches between fine-grain pipeline stages, in order to achieve high performance. Their functional robustness, however, relies on certain timing constraints that need to be satisfied. As a result, these circuits are no longer delay-insensitive, which means that stuck-at faults are not always leading to pipeline stalling. In addition, delay faults may result in violation of these timing constraints, thus affecting not only performance, as in delay-insensitive micro-pipelines, but also functional correctness. To address these new challenges, we develop a test method for both stuck-at and timing constraint violation faults in fine-grain ultra-high-speed asynchronous pipelines. The efficiency of the proposed method is demonstrated on MOUSETRAP, a recently developed pipeline for high-speed applications","Pipelines,
Circuit testing,
Delay,
Timing,
Circuit faults,
Clocks,
Protocols,
Computer science,
Latches,
Asynchronous circuits"
A context-aware approach to conserving energy in wireless sensor networks,"As sensor hardware technology proliferates, research in prolonging sensor battery life gains more interest. Conservation of sensor energy, therefore, becomes a practical approach to prolonging sensor life and eventually reducing the frequency of battery replacement. In retrospect, hardware-driven approaches have enabled significant power savings but on the expense of genericity as they are typical to certain sensor hardware. In this paper, we propose a software-based approach that is not contrained by sensor hardware and which is independent of any specific application domain. An implementation is done utilising the underlying framework and the results obtained validate our approach.","Intelligent networks,
Wireless sensor networks,
Batteries,
Hardware,
Energy management,
Frequency,
Context awareness,
Conferences,
Computer science,
Software engineering"
Prosody based audiovisual coanalysis for coverbal gesture recognition,"Despite recent advances in vision-based gesture recognition, its applications remain largely limited to artificially defined and well-articulated gesture signs used for human-computer interaction. A key reason for this is the low recognition rates for ""natural"" gesticulation. Previous attempts of using speech cues to reduce error-proneness of visual classification have been mostly limited to keyword-gesture coanalysis. Such scheme inherits complexity and delays associated with natural language processing. This paper offers a novel ""signal-level"" perspective, where prosodic manifestations in speech and hand kinematics are considered as a basis for coanalyzing loosely coupled modalities. We present a computational framework for improving continuous gesture recognition based on two phenomena that capture voluntary (coarticulation) and involuntary (physiological) contributions of prosodic synchronization. Physiological constraints, manifested as signal interruptions during multimodal production, are exploited in an audiovisual feature integration framework using hidden Markov models. Coarticulation is analyzed using a Bayesian network of naive classifiers to explore alignment of intonationally prominent speech segments and hand kinematics. The efficacy of the proposed approach was demonstrated on a multimodal corpus created from the Weather Channel broadcast. Both schemas were found to contribute uniquely by reducing different error types, which subsequently improves the performance of continuous gesture recognition.","Hidden Markov models,
Speech recognition,
Computer science,
Natural language processing,
Kinematics,
Speech analysis,
Human computer interaction,
Delay,
Bayesian methods,
Broadcasting"
Software development effort estimation using fuzzy logic: a case study,"Software estimation has been identified as one of the three great challenges for half-century-old computer science. Developers should be able to achieve practices containing effort estimation based on their own programs. New paradigms as fuzzy logic may offer an alternative for software effort estimation. This paper describes an application whose results are compared with those of a multiple regression. A subset of 41 modules developed from ten programs are used as data. Result shows that the value of MMRE (an aggregation of magnitude of relative error, MRE) applying fuzzy logic was slightly higher than MMRE applying multiple regression; while the value of Pred(20) applying fuzzy logic was slightly higher than Pred(20) applying multiple regression. Moreover, six of 41 MRE was equal to zero (without any deviation) when fuzzy logic was applied (not any similar case was presented when multiple regression was applied).",
An analytical model for IEEE 802.11e EDCA,"Quality of service (QoS) support in wireless LAN (WLAN) is the main mission of the IEEE 802.11 group e. It uses enhanced distributed channel access (EDCA) to differentiate service of priorities by means of various inter-frame spaces (IFS) and contention windows (CW). In order to efficiently manage QoS for the IEEE 802.11e networks, throughput and MAC delay for several traffic flows from 4 different access categories (AC) should be more accurately estimated. A four-dimensional Markov model is proposed to evaluate these. The correctness of our analysis has been validated via simulation results. Throughout our model, call admission control (CAC) can be easily applied, and thus resource management for QoS support for multimedia applications is well achieved.","Analytical models,
Quality of service,
Wireless LAN,
Throughput,
Delay estimation,
Traffic control,
Call admission control,
Computer science,
Communication system traffic control,
Resource management"
Distributed access control in CROWN groups,"Security in collaborative groups is an active research topic and has been recognized by many organizations in the past few years. In this paper, we propose a fine-grained and attribute-based access control framework for our key project, CROWN grid. To avoid single point of failure and enhance scalability of the system, we employ a distributed delegation authorization mechanism. We successfully implement our proposed access control in CROWN grid, and evaluate this approach by comprehensive experiments.",
Distributed Cross-Cultural Student Software Project: A Case Study,"Software is more and more developed in international cooperative efforts where project team members have different educational and cultural backgrounds and the team may even be distributed among sites in several countries. Software engineering education should also involve distributed and cross-cultural project work to prepare the future software developers for this kind of work environment. In a pilot project of two universities in Finland and in Russia a cross-cultural student team gathered experience by developing a common software product in a distributed project. While in general the project was successful, the team encountered some problems mainly due to the need to work in a foreign language and the practical limitations of remote connections","Software engineering,
Computer science,
Cultural differences,
Computer science education,
Programming,
Mathematics,
Educational products,
Data communication,
Global communication,
Team working"
Damage Detection and Correlation-Based Localization Using Wireless Mote Sensors,This study focuses on an experimental damage detection and correlation-based localization demonstration using wireless sensors. A simple cantilever beam has been constructed in the laboratory to serve as a test bed for measuring acceleration responses with these devices. The goal of this preliminary investigation is to test the feasibility and functionality of a wireless sensor network (WSN) to detect and localize damage utilizing current wireless mote technology,"Wireless sensor networks,
Testing,
Civil engineering,
Accelerometers,
Sensor arrays,
Sensor systems,
Computer science,
Micromechanical devices,
Power engineering computing,
Power engineering and energy"
"Intrinsic shortest path length: a new, accurate a priori wirelength estimator","A priori wirelength estimation is concerned with predicting various wirelength characteristics before placement. In this work we propose a novel, accurate estimator of net lengths. We observe that in ""good"" placements, the length of a net is very strongly correlated with the numbers of nets in the shortest paths connecting node pairs of the net, when each shortest path is computed under the restriction that the net itself does not exist. We refer to this as the net's intrinsic shortest path length (ISPL). Using ISPL as a wirelength estimator has several advantages: (1) it transparently handles multi-pin nets and is a strong predictor of their length; (2) it strongly correlates with the average netlist wirelength; (3) it has a distribution that is similar to that of wirelength; and (4) it acts as a good predictor for individual net lengths. Based on ISPLs, we characterize VLSI netlists with a single value and develop an intuitive, empirical link between our proposed value and the Rent parameter. We also analytically model the relationship between ISPL and wirelength, and use ISPLs in two practical applications: a priori total wirelength estimation and a priori global interconnect prediction.",
A Study of Project Management System Acceptance,"This study surveyed 497 project management software users in a wide variety of project-driven organizations to examine the relationships among: computer self-efficacy, information quality, system functionality, ease of use, project complexity, performance impact, organization size, project size, and user education, training and experience level. The findings indicated that information quality and project complexity are the dominant factors in explaining the levels of perceived system utilization; system functionality and ease of use have a significant effect on software usage; and that a strong relationship exists between perceptions of usage of software and project managers' performance. Inconsistent with prior research, training level was found to have no influence on project management software usage. However, software experience and education level had a moderate effect on the use of the software. Both organization size and project size had significant effects on the use of the software.","Project management,
Management training,
Software performance,
Software quality,
Quality management,
Software tools,
Financial management,
Electronic mail,
Computer science education,
Environmental management"
New approach for selfish nodes detection in mobile ad hoc networks,"A mobile ad hoc network (MANET) is a temporary infrastructureless network, formed by a set of mobile hosts that dynamically establish their own network on the fly without relying on any central administration. Mobile hosts used in MANET have to ensure the services that were ensured by the powerful fixed infrastructure in traditional networks, the packet forwarding is one of these services. The resource limitation of nodes used in MANET, particularly in energy supply, along with the multi-hop nature of this network may cause new phenomena which do not exist in traditional networks. To save its energy a node may behave selfishly and uses the forwarding service of other nodes without correctly forwarding packets for them. This deviation from the correct behavior represents a potential threat against the quality of service (QoS), as well as the service availability, one of the most important security requirements. Some solutions have been recently proposed, but almost all these solutions rely on the watchdog technique as stated in S. Marti et al. (2000) in their monitoring components, which suffers from many problems. In this paper we propose an approach to mitigate some of these problems, and we assess its performance by simulation.","Intelligent networks,
Mobile ad hoc networks,
Quality of service,
Monitoring,
Laboratories,
Computer science,
Mobile computing,
Spread spectrum communication,
Availability,
Security"
Computation of dynamic slices for object-oriented concurrent programs,"This paper proposes a novel dynamic slicing technique for object oriented concurrent programs. We introduce the notion of object oriented concurrent program dependence graph (OOCPDG). Our dynamic slicing technique uses OOCPDG as the intermediate representation and is based on marking and unmarking the dependence edges as and when the dependences arise and cease at runtime. Our approach eliminates the use of trace files and is more efficient than existing algorithms. Besides, it encompasses different aspects of object oriented programming paradigm viz. inheritance, polymorphism from the slicing arena. It can handle dynamically created object based processes. It can also handle process interactions through shared memory and message passing. The updating to the intermediate representation is truly concurrent. Multiple processors execute different object based processes concurrently and require special handling. We also report a dynamic slicing tool called CDSOOCP (concurrent dynamic sheer for object oriented concurrent programs) which implements our dynamic slicing technique.","Concurrent computing,
Debugging,
Heuristic algorithms,
Computer science,
Runtime,
Object oriented programming,
Message passing,
Software maintenance,
Software testing,
Encapsulation"
"Teaching an Advanced Design, Team-Oriented Software Project Course","Students learn about design principles and ""best practices"" in many courses. However, small scale assignments do not give enough opportunity for students to appreciate the value of software design principles or even to learn how to apply principles in practice. To fill the gap between theoretical and experiential knowledge, we introduced a team-based project course focused on design and implementation phases of the software development lifecycle. We teach design principles and team work in problem-based way, through architectural concepts and iterative development process. The product students build must meet stated quality requirements in terms of reliability, reusability and documentation. We trust this kind of the course is essential in curricula as it allows students better absorb knowledge learned in other software engineering courses. Such course also plays a role in better preparing students for industrial work. We describe a teaching method, course infrastructure and lessons learned over three years of teaching of our course. Based on experiences, we postulate and motivate the need for teaching at least two project courses in undergraduate curricula, one dealing with design and process issues, and the other focused on unstable requirements",
Matching TCP packets and its application to the detection of long connection chains on the Internet,"Network attackers usually launch their attacks behind a long connection chain. One way to stop such attacks is to prevent the attackers from using computers as ""stepping-stones"" for their attacks. A ""Step-Function"" method has been proposed to detect the length of a connection chain from a host to the victim machine. The algorithm is based on the changes in packet round trip times. Due to many network protocol issues, it is impossible to match all such packets correctly. We propose two algorithms to match TCP packet in real-time. The first algorithm matched fewer packets but the matching is correct. The second one matches more packets with some uncertainty on the correctness. The two algorithms gave almost identical results in determining the length of a connection chain. The algorithm gives a way to stop stepping-stone intrusion on the Internet in real-time.",
Closed and open loop optimal control of buffer and energy of a wireless device,"We study a decision problem faced by an energy limited wireless device that operates in discrete time. There is some external arrival to the device's transmit buffer. The possible decisions are: a) to serve some of the buffer content; b) to reorder a new battery after serving the maximum possible amount that it can; or c) to remain idle so that the battery charge can increase owing to diffusion process (which is possible in some commercially available batteries). We look at open and closed-loop controls of the system. The closed-loop control problem is a using the framework of Markov decision processes. We address both finite and infinite horizon discounted costs as well as average cost minimization problems. Without using any second order characteristics, we obtain results that include i) optimality of bang-bang control, ii) the optimality of threshold based policies, iii) parametric monotonicity of the threshold, and iv) uniqueness of the threshold. For the open-loop control setting we use recent advances in application of multimodular functions to establish optimality of bracket sequence based control.",
Generalizations of the Nevanlinna-Pick interpolation Problem,"This paper aims at generalizing the well-known Nevanlinna-Pick interpolation problem by considering additional constraints. The first type of constraints we consider requires the interpolation function to be of a given degree. Several results are provided for different degree constraints. These results offer feasibility tests via linear matrix inequalities. We have identified a number of degree constraints for which the feasibility tests are exact. For other degree constraints, we offer a relaxation scheme for checking the feasibility. The second type of constraints we study is about spectral zero assignment, which demands the zeros of the spectral factorization of the interpolation function to be at given locations. This problem can be solved using an iterative algorithm by Byrnes, Georgiou and Linquist. However, we provide a much faster iterative algorithm for this problem, although a proof of convergence is yet to be offered.","Interpolation,
Testing,
Iterative algorithms,
Linear matrix inequalities,
Convergence,
Circuits and systems,
Stability analysis,
Control design,
Signal processing,
Stochastic systems"
A highly efficient block-based dynamic background model,"A block-based dynamic background modelling technique featuring incremental update is presented, with the aim of increasing the sensitivity of background detection by considering the local connectivity of pixels. An accumulative model is maintained on-line by a heuristic algorithm to build up a statistical representation of a dynamic scene from a fixed camera view for foreground-background segmentation. The model is compared with a block-based technique employing cooccurrence [M. Seki et al., June 2003], and shown to be favourable in terms of both performance and conceptual simplicity. Using predominantly simple integer operations, the algorithm is highly amenable to direct implementation in hardware.","Pixel,
Computer science,
Layout,
Computer vision,
Application software,
Training data,
Heuristic algorithms,
Cameras,
Hardware,
Image segmentation"
An empirical evaluation of test case filtering techniques based on exercising complex information flows,"Some software defects trigger failures only when certain complex information flows occur within the software. Profiling and analyzing such flows therefore provides a potentially important basis for filtering test cases. We report the results of an empirical evaluation of several test case filtering techniques that are based on exercising complex information flows. Both coverage-based and profile-distribution-based filtering techniques are considered. They are compared to filtering techniques based on exercising basic blocks, branches, function calls, and def-use pairs, with respect to their effectiveness for revealing defects.",
EMG-based high level human-robot interaction system for people with disability,"This paper presents an innovative semiautonomous human-robot interaction concept for people with severe disability and discusses a proof-of-concept prototype system. The user communicates with the robot through electromyographic (EMG) signals. However, unlike most EMG controlled robotic operations, here the user can issue high-level commands to the robot. This would allow people with severe disability to access robots that can help them in activities of daily living. The robot controller is designed such that it can decompose highlevel user commands into primitives. The proposed concept eliminates the need for continuous control of the robot and hence makes the system easier to use and less error-prone.","Electromyography,
Robot control,
Rehabilitation robotics,
Control systems,
Spinal cord injury,
Speech,
Switches,
Signal generators,
Computer science,
Prototypes"
Managing the co-existing network of IPv6 and IPv4 under various transition mechanisms,"Even though IPv6 has been developed for more than a decade, IPv4 is still the most commonly adopted network protocol. However, the significant changes in the new version may cause the transition procedure to continue for several years. Currently, three transition mechanisms, dual stack, tunneling, and translation, are proposed to solve the problems due to the co-existence of IPv6 and IPv4. In many occasions, there could be more than one transition mechanism between network connections. In order to effectively manage the co-existing environment with various transition mechanisms, this paper presents a practical approach to deal with the management of co-existing networks. In this approach, we firstly determine the underlying transitions by gathering and analyzing corresponding IPv6 addresses. Once transitions are discovered, appropriate management facilities are developed and applied accordingly.","Tunneling,
Protocols,
Environmental management,
Telecommunication traffic,
Information technology,
Monitoring,
Computer network management,
Computer science,
Internet,
IP networks"
Evaluation of an efficient compensation method for quantitative fan-beam brain SPECT reconstruction,"Fan-beam collimators are designed to improve the system sensitivity and resolution for imaging small objects such as the human brain and breasts in single photon emission computed tomography (SPECT). Many reconstruction algorithms have been studied and applied to this geometry to deal with every kind of degradation factor. This work presents a new reconstruction approach for SPECT with circular orbit, which demonstrated good performance in terms of both accuracy and efficiency. The new approach compensates for degradation factors including noise, scatter, attenuation, and spatially variant detector response. Its uniform attenuation approximation strategy avoids the additional transmission scan for the brain attenuation map, hence reducing the patient radiation dose and furthermore simplifying the imaging procedure. We evaluate and compare this new approach with the well-established ordered-subset expectation-maximization iterative method, using Monte Carlo simulations. We perform quantitative analysis with regional bias-variance, receiver operating characteristics, and Hotelling trace studies for both methods. The results demonstrate that our reconstruction strategy has comparable performance with a significant reduction of computing time.","Image reconstruction,
Attenuation,
Degradation,
Optical collimators,
Image resolution,
Humans,
Breast,
Single photon emission computed tomography,
Reconstruction algorithms,
Geometry"
THALIA: Test Harness for the Assessment of Legacy Information Integration Approaches,"We introduce our new, publicly available testbed and benchmark called THALIA (Test Harness for the Assessment of Legacy information Integration Approaches) for testing and evaluating integration technologies. THALIA provides researchers with a collection of 40 downloadable data sources representing University course catalogs from computer science departments worldwide. In addition, THALIA currently provides a set of twelve challenge queries as well as a scoring function for ranking the performance of an integration system. A second contribution is a systematic classification of the types of syntactic and semantic heterogeneities, which directly lead to the twelve challenge. We have chosen course information as our domain of discourse because it is well known and easy to understand. Furthermore, there is an abundance of data sources publicly available that allowed us to develop a testbed exhibiting all of the syntactic and semantic heterogeneities that we have identified.","Catalogs,
Benchmark testing,
System testing,
XML,
Information science,
Computer science,
Data mining,
Artificial intelligence,
Laboratories,
Knowledge representation"
Performance Evaluation of Flooding in MANETs in the Presence of Multi-Broadcast Traffic,"Broadcasting has many important uses and several mobile ad hoc networks (MANETs) protocols assume the availability of an underlying broadcast service. Applications, which make use of broadcasting, include LAN emulation, paging a particular node. However, broadcasting induces what is known as the ""broadcast storm problem"" which causes severe degradation in network performance, due to excessive redundant retransmission, collision, and contention. Although probabilistic flooding has been one of the earliest suggested approaches to broadcasting. There has not been so far any attempt to analyse its performance behaviour in MANETs. This paper investigates using extensive ns-2 simulations the effects of a number of important parameters in a MANET, including node speed, pause time and, traffic load, on the performance of probabilistic flooding. The results reveal that while these parameters have a critical impact on the reachability achieved by probabilistic flooding, they have relatively a lower effect on the number of saved rebroadcast packets","Mobile ad hoc networks,
Broadcasting,
Telecommunication traffic,
Storms,
Routing protocols,
Computer networks,
Availability,
LAN emulation,
Degradation,
Performance analysis"
NUMA-aware Java heaps for server applications,"We introduce a set of techniques to both measure and optimize memory access locality of Java applications running on cc-NUMA servers. These techniques work at the object level and use information gathered from embedded hardware performance monitors. We propose a new NUMA-aware Java heap layout. In addition, we propose using dynamic object migration during garbage collection to move objects local to the processors accessing them most. Our optimization technique reduced the number of non-local memory accesses in Java workloads generated from actual runs of the SPECjbb2000 benchmark by up to 41%, and also resulted in 40% reduction in workload execution time.","Java,
Hardware,
Sun,
Fires,
Application software,
File servers,
Delay,
Distributed computing,
Computer science,
Educational institutions"
Steering behaviors for autonomous vehicles in virtual environments,"This paper presents steering behaviors that control autonomous vehicles populating roadways in virtual urban environments. Behavior programming is facilitated by a set of representations of the environment that use convenient frames of reference in natural coordinate systems. Roadway surfaces are modeled as three-dimensional ribbons that make the local orientation of the road explicit and allow relative distances on the road to be simply computed. Roads and intersections are connected to form a ribbon network. An egocentric representation called a path melds road and intersection segments into a single, continuous ribbon that captures the vehicle's short-term plan of navigation. A topological structure called a route supports wayfinding. We describe how the interrelated ribbon, path, and route representations are used to build multi-component behaviors that plan routes and safely navigate through traffic filled road networks - tracking lanes, shifting lanes to avoid congestion, anticipating lane changes needed to make turns dictated by the route, negotiating intersections, and respecting the rules of the road.",
Extending the potential fields approach to avoid trapping situations,"Reactive mobile robot navigation based on potential field methods has shown to be a good solution for dealing with unknown and dynamic environments, where timely responses are required. Unfortunately, the complexity of the tasks which can successfully be carried out is restricted by the inherent shortcomings of the approach such as trapping situations due to local minima, difficulties passing among closely spaced obstacles, oscillations in narrow corridors, etc... This paper proposes a novel strategy which overcomes totally the first limitation and partially the others by computing an adaptive navigation function on the basis of such artificial potential fields. As a result, navigation is achieved in very difficult scenarios such as maze-like environments. A comparative study on the path length performance of our proposal with regard to other algorithms from the related literature is also presented. Both simulation and real tests are performed.","Navigation,
Robot kinematics,
Proposals,
Robot motion,
Control systems,
Mathematics,
Computer science,
Mobile robots,
Testing,
Performance evaluation"
The Danger of Interval-Based Power Efficiency Metrics: When Worst Is Best,"This paper shows that if the execution of a program is divided into distinct intervals, it is possible for one processor or configuration to provide the best power efficiency over every interval, and yet have worse overall power efficiency over the entire execution than other configurations. This unintuitive behavior is a result of a seemingly intuitive use of power efficiency metrics, and can result in suboptimal design and execution decisions. This behavior may occur when using the energy-delay product and energy-delay product metrics but not with the energy metric.",
A study of a single robot interacting with groups of children in a rotation game scenario,"We tested the hypothesis that children are more attentive to a robot if the robot appears to be interested in the children. In addition, we investigated if and how the quality and quantity of a child's attentive behaviour varies with the distance to the robot, reflecting the notion of ""social spaces"". Hereto, 16 groups of up to 10 children each were engaged in a play scenario in which they had to move closer to a robot over 6 successive rounds. The robot was endowed with a ""camera eye"" and an arm and hand. The camera could either be nonmoving (""static"") or actively ""searching"" (""active searching""), giving the impression it was trying to select a child to focus on. Likewise, the arm and hand could either be fixed in a permanent pointing position (""permanent pointing"") or actively rise to point selectively at a particular child when it stopped facing it (""selective pointing""). The results showed that: 1) the mean frequency of overall attentive behaviour by the children (including attention towards other children) was significantly higher when the robot was not selectively pointing at the children and independent of the state of the camera; 2) ""looking at"" was the most frequently scored attentive activity for the children and was mainly targeted to the robot, but not correlated with any of their other attentive activities; 3) there was an interaction effect between the state of the camera and of the pointer: looking at the robot by the children occurred significantly more often when the camera and the arm were consistent in signaling apparent interest (i.e. camera ""active searching"" and hand ""selectively pointing"" or camera ""static"" and hand ""permanently pointing""); and 4) there was no demonstrable effect of distance to the robot on the overall attentive behaviour of the children.","Orbital robotics,
Robot vision systems,
Cameras,
Humans,
Tellurium,
Adaptive systems,
Computer science,
Educational institutions,
Testing,
Frequency"
Mosaicing of 3D sonar data sets - techniques and applications,"This paper presents a system for registration of 3D sonar data sets based on the Echoscope 1600 3D sonar. Although a 3D sonar image provides instantaneous volumetric data sets, many applications require the data sets to be mosaic'ed into larger data sets. The lack of navigation and motion sensors being able to produce mosaics with accuracies comparable to the 3D sonar resolution necessitates the use of registration techniques. Techniques for filtering of 3D sonar data, mesh generation, registration and geometric fusion applied to both on-line and offline mosaicing will be discussed. On-line mosaicing is done at a rate of 5 to 10 data sets per second, a rate that is acceptable in most practical situations. The techniques will find a number of applications, such as scanning harbors and seabed for potentially dangerous objects as well as seabed surveys. As the registration provides the sensor position relative to an object, object related positioning and navigation is also feasible, either as a stand alone underwater navigation system or as a system that improves current techniques like LBL, SBL and others.",
Gist: A Mobile Robotics Application of Context-Based Vision in Outdoor Environment,"We present context-based scene recognition for mobile robotics applications. Our classifier is able to differentiate outdoor scenes without temporal filtering relatively well from a variety of locations at a college campus using a set of features that together capture the ""gist"" of the scene. We compare the classification accuracy of a set of scenes from 1551 frames filmed outdoors along a path and dividing them to four and twelve different legs while obtaining a classifi- cation rate of 67.96 percent and 48.61 percent, respectively. We also tested the scalability of the features by comparing the classification results from the previous scenes with four legs with a longer path with eleven legs while obtaining a classification rate of 55.08 percent. In the end, some ideas are put forth to improve the theoretical strength of the gist features.","Mobile robots,
Robot vision systems,
Layout,
Leg,
Robot sensing systems,
Application software,
Computer science,
Scalability,
Sonar navigation,
Computer vision"
Learning Effectiveness in Web-Based Technology-Mediated Virtual Learning Environment,"A framework that delineates the relationships between learner control and learning effectiveness is absent. This study aims to fill this void. Unlike previous research, this study compares the learning effectiveness between two learning environments: traditional classroom and Technology-mediated Virtual Learning Environment (TVLE). Our work focuses on the effectiveness of a TVLE in the context of basic information technology skills training. Grounded in the technology-mediated learning literature, this study presents a framework that addresses the relationship between the learner control and learning effectiveness, which contains four categories: learning achievement, self-efficacy, satisfaction, and learning climate. In order to compare the learning effectiveness under traditional classroom and TVLE, we conducted a field experiment. Data were collected from a junior high school of Taiwan. A total of 210 usable responses were analyzed. We identified four results from this study. (1) Students in the TVLE environment achieve better learning performance than their counterparts in the traditional environment; (2) Students in the TVLE environment report higher levels of computer self-efficacy than their counterparts in the traditional environment; (3) Students in the TVLE environment report different levels of satisfaction than students in the traditional environment; and (4) Students in the TVLE environment report higher levels of learning climate than their counterparts in the traditional environment. The implications of this study are discussed, and further research directions are proposed.","Information technology,
Educational institutions,
Educational technology,
Information management,
Environmental management,
Technology management,
High performance computing,
Electrical equipment industry,
Internet,
Open systems"
Impact of receiver cheating on the stability of ALM tree,"Application layer multicast (ALM) is an effective supplement to IP multicast, but it has the potential trouble of trust on end systems. For instance, multicast receivers may cheat in order to obtain a better position in the multicast tree. Receiver cheating may transform the multicast tree, and lead to its instability. We establish the cheating model of ALM receivers and analyze the stability of ALM tree when receiver cheating occurs. Simulation results show that receiver cheating has considerably negative effects on the stability of ALM tree. This discovery brings forward an issue in ALM study, that is, we should take receiver cheating into consideration to maintain a stable ALM tree when designing ALM protocols","Stability,
Costs,
Propagation delay,
Peer to peer computing,
Internet,
Current measurement,
Distance measurement,
Multicast protocols,
Network topology,
Computer science"
CarCoach: a generalized layered architecture for educational car systems,"Car accidents are a major concern. Consequently, a lot of research is carried out on car user interfaces. For each such research, usually a special simulator or car is developed, algorithms and tools are redeveloped, and similar issues arise. We propose CarCoach, an educational car system, based on a generalized layered architecture. We present the system design, the intelligent modular architecture, its layers, including details of some of its relevant modules. Using the Chrysler 300M IT-Edition car as a platform, a prototype was implemented and initial experimentation was carried out and is reported. We demonstrate that CarCoach provides a flexible environment for car research and support of varied car applications.",
Stochastic modelling and analysis of horizontally-operated power systems with a high wind energy penetration,"A methodology for the modelling and analysis of horizontally-operated power systems, i.e. systems with a high penetration of stochastic renewable generation, is presented. The objective is to obtain insight in the steady-state of the transmission system when a high penetration level of stochastic distributed generation (in this study case wind power), is present in the underlying distribution systems. The results can be used for the adequacy assessment and risk management of the system. For the system stochastic modelling, the methodology proposes the decoupling of the individual (marginal) behavior of the input random variables from the dependence structure between them. The stochastic dependence is shown to be a major factor for the assessment of the aggregated effect of the distributed stochastic generation on the system. In particular, the stress in the system increases in cases of positive dependence between the inputs and the maximum stress, i.e. the worst-case scenario for the system, occurs when extreme positive dependencies are present between the inputs. Based on this modelling principle, the system operational planning and design can be performed by modelling the extreme dependencies in the system. This powerful computational method can be easily applied to large systems with a high number of stochastic generators.","Power system modeling,
Stochastic systems,
Power system analysis computing,
Wind energy,
Power generation,
Stress,
Wind energy generation,
Steady-state,
Distributed control,
Risk management"
Year,,
Using mobile agents for intrusion detection in wireless ad hoc networks,"Many attempts were made to secure wireless ad hoc networks (WAHNs), but due to their special ad hoc nature and strict constraints, finding an optimal and comprehensive security solution is still a research challenge. In this paper we present the main security challenges of WAHNs. Then we study and analyze mobile agents and their attributes against those challenging requirements. The analyses show a great feasibility and promising suitability for mobile-agent-based solutions to be adopted by the WAHNs intrusion detection systems. The paper also surveys, studies and compares the existing WAHNs mobile-agents-based intrusion detection designs.","Mobile agents,
Intrusion detection,
Intelligent networks,
Ad hoc networks,
Mobile ad hoc networks,
Wireless sensor networks,
Computer networks,
Communication system security,
Information science,
Information security"
Evolving swarms that build 3D structures,"The complex interactions of natural swarms, for example formed by some social insects, are difficult to comprehend. Considering tasks such as nest-building, the necessary underlying communication presumably happens indirectly by changing and reacting on the environment. This paper presents an overall approach to interactively evolve rule-based swarms that create three-dimensional structures in continuous space. The approach comprises the design of the swarm agent, details about the breeding process and first results. A swarm is determined by a set of flocking parameters and a set of instructional rules that allow the agents to change their local structural environment. The center or focus of the swarm's endeavour may be shifted either on a swarm agent or on a fixed point in space. The alteration of the supplied 3D structure during the course of evolution enables an external supervisor to interactively guide the development of a swarm.","Jacobian matrices,
Insects,
Buildings,
Computational modeling,
Computer science,
Drives,
Self-assembly,
Anthropomorphism,
Lattices,
Evolutionary computation"
"Creating a sense of ""Presence"" in a web-based PSI course: the search for Mark Hopkins' log in a digital world","The technical term presence concerns the feeling that a student is in a real classroom, in real time, with a real person managing the learning. This paper considers the two major factors affecting presence: 1) the choice of pedagogical strategy and 2) its implementation using a freshman computer course taught over the Web by the Personalized System of Instruction (PSI) method. Special attention is paid to the location of professor, proctor, and students in this study as well as other strategies to heighten a sense of presence in Web-based courses.",
A novel audio watermarking technique based on low frequency components,"In this paper, we present a novel audio watermarking technique that utilizes the low frequency components (LFCs) of an audio signal to identify the location of the embedded watermarks. The embedding takes place by modifying the amplitude of selected samples determined by the LFCs of the audio signal. The amount of modification to the amplitude is determined by the amount of distortion detected by the human ear. This technique is blind where the decoder does not need the original audio file to extract the watermarks. In this technique, we use a novel data recovery scheme to recover any watermarks that were lost because of an intentional or unintentional attempt of watermark removal (attack). Experimental results show that this technique is highly robust against single and double attacks with watermark recovery rates greater than 90%.",
TCC: a two-category classifier for AQM routers supporting TCP flows,"Active Queue Management (AQM) can maintain smaller queuing delay and higher throughput by purposefully dropping packets at intermediate nodes. Most of the existing AQM schemes follow the probability dropping mechanism originated from Random Early Detection (RED). In this letter, we develop a novel packet dropping mechanism for AQM through designing a two-category classifier based on the Fisher Linear Discriminate approach. The simulation results show that the new scheme outperforms other well-known AQM schemes, such as RED, PI and REM, in the integrated performance.","Space technology,
Delay,
Throughput,
Internet,
Robustness,
Random number generation,
Probability,
Decision making,
Computer science,
State-space methods"
A boosting approach to remove class label noise,"Ensemble methods have been known to improve prediction accuracy over the base learning algorithm. AdaBoost is well-recognized for that in its class. However, it is susceptible to overfitting the training instances corrupted by class label noise. This paper proposes a modification to AdaBoost that is more tolerant to class label noise, which further enhances its ability to boost prediction accuracy. In particular, we observe that in Adaboost, the weight-hike of noisy examples can be constrained by careful application of a cut-off in their weights. Effectiveness of our algorithm is demonstrated empirically using some artificially generated data. We also corroborate this on a number of data sets from UCI repository (Blake and Mertz, 1998). In both experimental settings, the results obtained affirm the efficacy of our approach. Finally, some of the significant characteristics of our technique related to noisy environments have been investigated.",
Autonomic cluster management system (ACMS): a demonstration of autonomic principles at work,"Cluster computing, whereby a large number of simple processors or nodes are combined together to apparently function as a single powerful computer, has emerged as a research area in its own right. The approach offers a relatively inexpensive means of achieving significant computational capabilities for high-performance computing applications, while simultaneously affording the ability to increase that capability simply by adding more (inexpensive) processors. However, the task of manually managing and configuring a cluster quickly becomes impossible as the cluster grows in size. Autonomic computing is a relatively new approach to managing complex systems that can potentially solve many of the problems inherent in cluster management. We describe the development of a prototype automatic cluster management system (ACMS) that exploits autonomic properties in automating cluster management.","High performance computing,
NASA,
Space technology,
Computer networks,
Power system management,
Concurrent computing,
Computer science,
Computer architecture,
Automation,
Computer applications"
PaMira - A Parallel SAT Solver with Knowledge Sharing,"In this paper we describe PaMira, a powerful distributed SAT solver. PaMira is based on the highly optimized, sequential SAT engine Mira, incorporating all essential optimization techniques modern algorithms utilize to maximize performance. For the distributed execution an efficient work stealing method has been implemented. PaMira also employs the exchange of conflict clauses between the processes to guide the search more efficiently. We provide experimental results showing linear speedup on a multiprocessor environment with four AMD Opteron processors","Engines,
Computer science,
Master-slave,
Message passing,
Design automation,
Automatic test pattern generation,
Timing,
Pattern analysis,
Sorting,
Business continuity"
Semantic-compensation-based recovery in multi-agent systems,"In agent systems, an agent's recovery from, execution problems is often complicated by constraints that are not present in a more traditional distributed, database systems environment. An analysis of agent-related crash recovery issues is presented, and requirements for achieving 'acceptable' agent crash recovery are discussed. Motivated by this analysis, a novel approach to managing agent recovery is presented. It utilises an event-and task-driven model for employing semantic compensation; task retries, and checkpointing. The compensation/retry model requires a situated model of action and failure, and provides the agent with an emergent unified, treatment of both crash recovery and run-time failure-handling. This approach helps the agent to recover acceptably from crashes and execution problems; improve system predictability; manage inter-task dependencies; and address the way in which exogenous events or crashes can trigger the need for a re-decomposition of a task. Agent architecture is then presented, which uses pair processing to leverage these recovery techniques and increase the agent's availability on crash restart.",
Simulation of heterogeneous nanodielectrics using the local field method,"The dipole-dipole interaction is significantly responsible for the dielectric behaviour of matter. In the past, several analytic models have been developed to describe these dielectric properties. With increasing performance of personal computers, bigger and bigger systems can be simulated numerically on a microscopic scale by calculating the local electric fields at each dipole site. In doing so and applying the method of image dipoles in order to consider the electrodes we can easily calculate the local fields, polarization and effective susceptibility for a system of nanometric size with dimensions smaller than 100 /spl Aring/ which consists of more than one sort of atoms, e. g. binary mixtures, or different lattice constants. With this method all depolarization fields within the sample are regarded inherently. The different phases can be arranged differently so that we can investigate the influence of the granularity of the mixture. Defects and inclusions can also be easily included. We have studied for example the impact of an air-filled sphere in a dielectric on the effective susceptibility and simulated local fields for a pyramidal inclusion. For a given plane in the three-dimensional sample we can determine the local fields at (grain) boundaries.","Polarization,
Lattices,
Electrodes,
Dielectric materials,
Permittivity,
Atomic layer deposition,
Computational modeling,
Microscopy,
Grain boundaries,
Electrons"
Approximate VCCs: a new characterization of multimedia workloads for system-level MpSoC design,"System-level design methods specifically targeted towards multimedia applications have recently received a lot of attention. Multimedia workloads are known to have a high degree of variability. Therefore, designs based on a worst-case analysis of such workloads tend of be overly pessimistic. We address this issue by introducing a new concept called approximate variability characterization curves (or approximate VCCs), to characterize the ""average-case"" behavior of multimedia workloads in a parameterized fashion. Since most multimedia applications only have soft real-time constraints, it is often possible to tolerate a small amount of performance degradation. By allowing such small degradations in the performance, large amounts of resource savings are possible. The concept of approximate VCCs that we present in this paper allows a designer to quantitatively account for the performance degradation and the associated resource savings. We illustrate this using two typical system design cases.","Multimedia systems,
Degradation,
System-on-a-chip,
Permission,
Computer science,
System-level design,
Application software,
Digital TV,
Instruments,
Clocks"
Maintaining flow isolation in work-conserving flow aggregation,"In order to improve the scalability of scheduling protocols with bounded end-to-end delay, much effort has focused on reducing the amount of per-flow state at routers. One technique to reduce this state is flow aggregation, in which multiple individual flows are aggregated into a single aggregate flow. In addition to reducing per-flow state, flow aggregation has the advantage of a per-hop delay that is inversely proportional to the rate of the aggregate flow, while in the case of no aggregation, the per-hop delay is inversely proportional to the (smaller) rate of the individual flow. Flow aggregation in general is non-work-conserving. Recently, a work-conserving flow aggregation technique has been proposed. However, it has the disadvantage that the end-to-end delay of an individual flow is related to the burstiness of other flows sharing its aggregate flow. Here, we show how work-conserving flow aggregation may be performed without this drawback, that is, the end-to-end delay of an individual flow is independent of the burstiness of other flows.","Delay,
Aggregates,
Quality of service,
Protocols,
Diffserv networks,
Scalability,
Scheduling algorithm,
Computer science,
Processor scheduling,
Web and internet services"
Face recognition with weighted locally linear embedding,"We present an approach to recognizing faces with varying appearances which also considers the relative probability of occurrence for each appearance. We propose and demonstrate extending dimensionality reduction using locally linear embedding (LLE), to model the local shape of the manifold using neighboring nodes of the graph, where the probability associated with each node is also considered. The approach has been implemented in software and evaluated on the Yale database of face images (Belhumeur et al., 1997). Recognition rates are compared with non-weighted LLE and principal component analysis (PCA), and in our setting, weighted LLE achieves superior performance.",
Visualization research problems in next-generation educational software,"The next-generation educational software must i) be modular and interoperable to satisfy just-in-time demands, ii) have multiple levels of detail, iii) be usable in both synchronous and asynchronous learning modes, and iv) foster interactive involvement of the learner and her community. In addition, this software must address audiences of different ages, goals, and learning styles, in contexts that range from classroom to problem-solving, team-structured labs to Web-based individual study to as yet unknown modes. Clearly, learning environments are not restricted to static rooms with desktop terminals but evolve into both formal and informal learning communities. We need integrated organizational approaches to the research necessary to create such next-generation challenges. The Learning Federation explores issues in creating next-generation educational software and directs a focused, sustained research investment effort to create such software. Its purpose is to provide a critical mass of funding for long-term basic and applied precompetitive research in learning science and technology, this research, which interdisciplinary teams conduct, is meant to lead the development of next-generation authoring tools and sample curricula for both synchronous and asynchronous learning.","Visualization,
Power system modeling,
Biological system modeling,
Buildings,
Research and development,
Humans,
Educational technology,
Stress,
Mathematics,
Intelligent structures"
Informative Gene Discovery for Cancer Classification from Microarray Expression Data,"Gene expression data analysis from microarray is a new advance of cancer diagnosis. However, the gene expression data often have high dimensionality and small sample size. These properties cause severe difficulties in classification. Gene selection is thus a crucial pre-processing step to filter out uninformative genes prior to the classification step. Our approach to perform gene selection is an information theoretic approach combining with sequential forward floating search. Experimental results show that our method is capable of efficiently finding a compact set of informative genes which can effectively discriminate different classes","Cancer,
Gene expression,
Data analysis,
Mutual information,
Degradation,
Diseases,
Computer science,
Data engineering,
Filters,
Diversity reception"
A situated learning perspective on learning object design,"Recent work suggests that learning object design can be improved by greater integration of instructional design, learning theory and software development methodologies. Despite this, there is a lack of research in the field that seeks to establish an association between the contextualised nature of learning object design and empirical properties of learner-computer interaction. In addressing this issue, we argue for a situated learning perspective on learning object design. Using the CASE framework as an exemplar of situated learning, we describe an holistic approach to eliciting socio-cultural properties of learning objects.","Computer aided software engineering,
Collaborative work,
Collaboration,
Process design,
Computer science,
Software engineering,
Programming,
Design methodology,
Learning systems,
Robustness"
"A design process for the development of innovative smart clothing that addresses end-user needs from technical, functional, aesthetic and cultural view points","The research and development of attractive smart clothing, with truly embedded technologies, demands the merging of science and technology with art and design. This paper looks at a comparatively new and unique design discipline that has been given little prior consideration. The concept of 'wearables' crosses the boundaries between many disciplines. A gap exists for a common 'language' that facilitates creativity and a systematic design process. Designers of smart clothing require guidance in their enquiry, as gaining an understanding of issues such as; usability, manufacture, fashion, consumer culture, recycling and end user needs can seem forbidding and difficult to prioritise. This paper presents a design tool in a format that can be understood by practitioners who come from a range of backgrounds. The representation of the 'critical path' is intended as a tool to guide the design research and development process in the application of smart technologies.","Process design,
Clothing,
Cultural differences,
Textile technology,
Art,
Research and development,
Aging,
Merging,
Appropriate technology,
Consumer electronics"
Towards a Web service composition management framework,"We suggest that the composition of Web services is an activity that needs to be managed, and that Web service composition management is distinct from the management of individual Web services. We describe a set of requirements to help make this distinction. The four main groups of these requirements are: service discovery, service selection and contract formation, composition verification, composition management. Then, we discuss architectural alternatives (centralized, federated, and peer-to-peer) for a Web service composition management framework.",
TCP Adaptation for MPI on Long-and-Fat Networks,"Typical MPI applications work in phases of computation and communication, and messages are exchanged in relatively small chunks. This behavior is not optimal for TCP because TCP is designed only to handle a contiguous flow of messages efficiently. This behavior anomaly is well-known, but fixes are not integrated into today's TCP implementations, even though performance is seriously degraded, especially for MPI applications. This paper proposes three improvements in the Linux TCP stack: i.e., pacing at start-up, reducing Retransmit-Timeout time, and TCP parameter switching at the transition of computation phases in an MPI application. Evaluation of these improvements using the NAS parallel benchmarks shows that the BT, CG, IS, and SP benchmarks achieved 10 to 30 percent improvements. On the other hand, the FT and MG benchmarks showed no improvement because they have the steady communication that TCP assumes, and the LU benchmark became slightly worse because it has very little communication","Bandwidth,
Linux,
TCPIP,
Communication industry,
Computer industry,
Character generation,
Telecommunication traffic,
Computer networks,
Grid computing,
Degradation"
The inverse kinematics solutions of fundamental robot manipulators with offset wrist,"In this work, the inverse kinematics of sixteen fundamental 6-DOF robot manipulators with offset wrist were solved analytically and numerically. Analytical and numerical techniques are the most common approaches for solving inverse kinematics problems. They are generally desired to be solved analytically in order to have complete solution and fast computation. This approach is also called as closed form solution and in the absence of it, numerical techniques are used for solving the inverse kinematics problem. As examples, the inverse kinematics solutions of RS (Scara robot), CS (cylindrical robot), and NN (spherical robot) robot manipulators with offset wrist were presented in this paper. Also, the inverse kinematics solution techniques for sixteen fundamental robot manipulators equipped with offset wrist were summarized in a table.","Kinematics,
Manipulators,
Wrist,
Service robots,
Orbital robotics,
Robot control,
Educational robots,
Closed-form solution,
Equations,
Computer science education"
Access router information protocol with FMIPv6 for efficient handovers and their implementations,"Mobile IPv6, which is a protocol being standardized by IETF for global IP mobility support, is expected to play a key role for future all-IP wireless networks. As the demands for delay sensitive real-time applications increase, the handover latency of mobile IPv6 becomes inadequate for these applications. There have been many proposals, such as fast handovers for mobile IPv6 (FMIPv6), to tackle this problem. However, FMIPv6 only concentrates on the protocol operation while it does not address other issues, such as radio access discovery and candidate access router discovery, that are critical to the handover performance of FMIPv6. Therefore, in this paper, we propose ARIP (access router information protocol), which performs a candidate access router discovery and a radio access network discovery in one protocol suite, to provide an efficient handover framework for MIPv6 and FMIPv6. ARIP is designed to cope with heterogeneous networks as well as homogeneous networks and to be closely coupled with FMIPv6 to provide an efficient handover operation. The performance evaluation of MIPv6, FMIPv6, and FMIPv6 with ARIP is performed in an actual network environment and the performance results show that FMIPv6 with ARIP gives the best performance in terms of handover latency","Access protocols,
Delay,
Wireless application protocol,
Wireless networks,
Proposals,
Switches,
Computer science,
Mobile computing,
Radio access networks,
Performance evaluation"
Software testing education and training in Hong Kong,"While the use of computer applications is widely spread in every business and, hence, the reliability of software is critical, it is believed that many organizations involved in software development do not take software testing sufficiently seriously as an important task. It is worthwhile to find out how far organizations are carrying out software testing in a systematic and structured manner or still taking on an ad-hoc approach. A survey was conducted to understand the software testing practices and the level of related education and training in Hong Kong. It was found that most testing team members did not have formal training in software testing. University curricula generally did not prepare graduates with enough coverage in software testing. It is proposed that a review of the current software engineering curricula in the universities to examine the coverage of software testing will be useful to the development of quality software.",
Analysis of Overlay Network Impact on Dependability,"Recently, peer-to-peer systems have become widely accepted and are probably the most recognizable examples of distributed applications. As they are maturing and their functionalities are becoming increasingly complex, the need for dependable solutions arises. In particular peer-to-peer systems' dependability is immensely influenced by their virtual overlay networks. The paper presents the results of analysis of diverse overlay networks with respect to their support for dependability.","Peer to peer computing,
Network topology,
Computer networks,
Application software,
Computer science,
Distributed processing,
Distributed computing,
High performance computing,
Scientific computing,
Floods"
Collaborative virtual environment technology for people with autism,"This paper focuses on an investigation of collaborative virtual environment (CVE) technology for people with autism, paying particular attention to the use of avatar representations of emotions. A prima facie case for a CVE technological approach is developed, which argues that CVE offers great potential benefits for people with autism. An empirical study of people with autism using a simulated CVE system has been carried out. Results suggest that, on the whole, the participants were able to recognise and operate the avatar representations when communicating via the CVE system.","Collaboration,
Virtual environment,
Autism,
Avatars,
Educational institutions,
Displays,
Humans,
Computer science education,
Interactive systems,
Distributed computing"
A decentralized strategy for resource allocation,"This paper presents an approach for resource allocation in a grid based on ""spatial computing"" concepts. We model a grid using a flat architecture consisting of nodes connected by an overlay network. The 2D spatial distribution of the nodes of the grid, together with the quantity of resource available in each node, forms a 3D surface, where valleys correspond to nodes with a large quantity of available resource. We propose an algorithm for resource allocation that is based on surfing such a 3D surface, in search for the deepest valley (global minimum). The algorithm, which aims at fairly distributing among nodes the quantity of leased resource, is based on some heuristics that mimic the laws of kinematics. Experimental results show the effectiveness of the algorithm.","Resource management,
Peer to peer computing,
Grid computing,
Kinematics,
Computer networks,
Distributed computing,
Availability,
Computer science,
Telecommunication computing,
Computer architecture"
A Comparison of IP Datagrams Transmission using MPE and ULE over Mpeg-2/DVB Networks,Digital video broadcasting (DVB) defines the carriage of multimedia and Internet information to client by means of MPEG-2 transport stream. The DVB standards also allow the same system to transmit Internet protocol (IP) data. This data broadcasting technology enables the broadcast of large amount of data to the personal computer over satellite or other broadcasting network. This allows the MPEG-2 TS bearer become a hop in IP network. The intention of this paper is to compare the existing multi-protocol encapsulation and ultra lightweight encapsulation methods for IP transmission over DVB networks,"Digital video broadcasting,
Satellite broadcasting,
Multimedia communication,
Broadcast technology,
Streaming media,
Internet,
Encapsulation,
Digital multimedia broadcasting,
Protocols,
Microcomputers"
Speaker detection without models,"In order to capture sequential information and to take advantage of extended training data conditions, we developed an algorithm for speaker detection that scores a test segment by comparing it directly to similar instances of that speech in the training data. This non-parametric technique, though at an early stage in its development, achieves error rates close to 1% on the NIST 2001 extended data task and performs extremely well in combination with a standard Gaussian mixture model system. We also present a new scoring method that significantly improves performance by capturing only positive evidence.","Training data,
Hidden Markov models,
NIST,
Acoustic testing,
Loudspeakers,
Automatic speech recognition,
Cepstral analysis,
Computer science,
Sequential analysis,
Error analysis"
Wireless Distributed Measurement System by Using Mobile Devices,"The innovative architecture of distributed measurement systems (DMS) is proposed in order to increase the adaptability characteristic to the new necessity of the industrial applications. This architecture is based on (i) the wireless technology to realize the communication structure and (ii) the personal digital assistant to manage and control the measurement instrumentations. The wireless protocol IEEE 802.11 is used to realize the local area network. The aspects concerning both the software and the hardware to realize the wireless DMS are discussed. In order to evidence innovative features, two applicative examples are shown and discussed.","Electrical equipment industry,
Application software,
Computer architecture,
Mobile communication,
Personal digital assistants,
Technology management,
Communication system control,
Instruments,
Wireless application protocol,
Local area networks"
ARCADE: Augmented Reality Computing Arena for Digital Entertainment,"Augmented reality (AR) technology for digital composition of animation with real scenes is to bring new digital entertainment experience to the viewers. Augmented reality is a form of human-machine interaction. The key feature of the augmented reality technology is to present auxiliary information in the field of view for an individual automatically without human intervention. The effect is similar to composing computer-animated images with real scenes. To achieve the new augmented reality experience, two main problems are: How to keep track of the viewing parameters of the individual viewer? How to render a virtual image in the field of view correctly and seamlessly? To tackle the above, we are designing and implementing an enabling framework, called augmented reality computing arena for digital entertainment (ARCADE), to support the creation of augmented reality entertainment applications. Moreover, we are also developing two new video object tracking algorithms, 3D surface markers tracking and human head and face tracking. In this paper, ARCADE's system architecture and its components are described in detail and illustrated with an application example","Augmented reality,
Robots,
Voting,
Rendering (computer graphics),
Humans,
Head,
Layout,
Face,
Aircraft propulsion,
Aircraft manufacture"
Configuration logic: a multi-site modal logic,"We introduce a logical formalism for describing properties of configurations of computing systems. This logic of trees allows quantification on node labels, which are modalities containing variables. We explain the motivation behind our formalism and give both a classical semantics and a new equivalent one based on partial functions on variables.","Logic devices,
Computer science,
Power system management,
Computer networks,
Computer errors,
Application software,
Computer network management"
Accurate object localization in 3D laser range scans,"This paper presents a novel method for object detection and classification in 3D laser range data that is acquired by an autonomous mobile robot. Unrestricted objects are learned using classification and regression trees (CARTs) and using an Ada Boost learning procedure. Off-screen rendered depth and reflectance images serve as an input for learning. The performance of the classification is improved by combining both sensor modalities, which are independent from external light. This enables highly accurate, fast and reliable 3D object localization with point matching. Competitive learning is used for evaluating the accuracy of the object localization","Object detection,
Mobile robots,
Laser theory,
Regression tree analysis,
Rendering (computer graphics),
Computer science,
Knowledge based systems,
Intelligent systems,
Intelligent robots,
Cognitive robotics"
Information-centric assessment of software metrics practices,"This paper presents an approach to enhance managerial usage of software metrics programs. The approach combines an organizational problem-solving process with a view on metrics programs as information media. A number of information-centric views are developed to engage key stakeholders in debating current metrics practices and identifying possible improvements. We present our experiences and results of using the approach at Software Inc., we offer comparisons to related approaches to software metrics, and we discuss how to use the information-centric approach for improvement purposes.","Problem-solving,
Software metrics"
A hybrid approach for the dynamic vehicle routing problem with time windows,"The vehicle routing problem with time windows (VRPTW) is a well-known and complex combinatorial problem. The static version of this problem has received special attention. However, most real world vehicle routing problems are dynamic, where information relevant to the routing can change after the initial routes have been constructed. The common approach in this scenario is to restart the algorithm when novel information arrives, but this type of restart methods does not use the memory to improve the quality of the final result. In this paper, the hybrid CGH column generation heuristic (Alvarenga et al., 2005) is modified in order to use the routes generated during the optimization process.","Vehicle dynamics,
Routing,
Computer science,
Automation,
Intelligent vehicles,
Hybrid power systems,
Genetic algorithms,
Customer service,
Cost function,
Partitioning algorithms"
On finding the best partial multicast protection tree under dual-homing architecture,"In this paper, we introduce the concept of partial protection and propose an efficient solution for providing partial multicast protection given the dual-homing architecture in the access network. In the dual-homing architecture, each destination is connected to two edge routers to enhance the survivability in the access network. The routing algorithm which finds a path from the source to each edge router holds the key for the multicast protection. We study the problem of finding the best partial multicast protection tree for the multicast session given the dual-homing architecture assuming that the hop count on each path is limited. We show the NP-completeness of the problem and propose the partition and sharing (PAS) algorithm to solve the problem efficiently. Simulation results show that the PAS algorithm achieves performance very close to the computed lower bounds. The solution proposed in this paper fills the gap between traditional 100% protection and non-protection subject to single link failure.","Protection,
Hydrogen,
Multicast algorithms,
Computer architecture,
Partitioning algorithms,
High definition video,
Videoconference,
Computer science,
Electronic mail,
Routing"
E-government and e-democracy in Latin America,"Implementing governmental and democratic processes using electronic systems is the subject of much debate around the world. Electronic voting has grabbed the headlines, but in reality, this constitutes just a small part of the effort to establish electronic communication between citizens and governmental functions (G2C and C2G). The goal is to provide access to information and to open up decision-making processes to citizens (e-participation) to encourage a grass-roots engagement of citizens with democratic processes.","Electronic government,
Electronic voting,
Brazil Council,
Intelligent systems,
Fixtures,
Continents,
Nominations and elections,
Web and internet services,
World Wide Web,
Computer science"
Query planning for the grid: adapting to dynamic resource availability,"The availability of massive datasets, comprising sensor measurements or the results of scientific simulations, has had a significant impact on the methodology of scientific reasoning. Scientists require storage, bandwidth and computational capacity to query and analyze these datasets, to understand physical phenomena or to test hypotheses. This paper addresses the challenge of identifying and selecting resources to develop an evaluation plan for large scale data analysis queries when data processing capabilities and datasets are dispersed across nodes in one or more computing and storage clusters. We show that generating an optimal plan is hard and we propose heuristic techniques to find a good choice of resources. We also consider heuristics to cope with dynamic resource availability; in this situation we have stale information about reusable cached results (datasets) and the load on various nodes.","Availability,
Data analysis,
Educational institutions,
Query processing,
Data processing,
Memory,
Analytical models,
Drives,
Computer science,
Sensor phenomena and characterization"
A comparison of reliable multicast protocols for mobile ad hoc networks,"Reliable multicast plays a significant role in many applications of mobile ad hoc networks (MANETs). In recent years, a number of protocols have been proposed to deliver multicast packets reliably. These protocols have shown distinguishing features and have used different recovery mechanisms. In order to provide a comprehensive understanding of these protocols, we present in this paper a survey of the protocols and compare the advantages and disadvantages of different design features as well as protocol performance. The protocols being surveyed are classified into three categories, namely, ARQ-based, gossip-based and FEC-based.","Multicast protocols,
Mobile ad hoc networks,
Peer to peer computing,
Computer network reliability,
Application software,
Telecommunication network reliability,
Computer science,
Mobile communication,
Mobile computing,
Spread spectrum communication"
Meta Service in Intelligent Platform of Virtual Travel Agency,"IPVita is an intelligent platform of virtual travel agency. Supported by travel ontology, the author presents the meta service and the meta process to deal with the various requirements of tourists and services providers more effective. Based on meta service, IPVita can automatically creates meta process for travel using an auto generation algorithm for meta process (MPGA), then precisely matches between requirements of tourists and services' registration information, finally meta process become a travel process for tourists and implements it.","Travel services,
Web services,
Technology planning,
Ontologies,
Web and internet services,
Computer science,
Paper technology,
Publishing,
Rail transportation"
Expression-invariant face recognition via spherical embedding,"Recently, it was proven empirically that facial expressions can be modelled as isometries, that is, geodesic distances on the facial surface were shown to be significantly less sensitive to facial expressions compared to Euclidean ones. Based on this assumption, the 3DFACE face recognition system was built. The system efficiently computes expression invariant signatures based on isometry-invariant representation of the facial surface. One of the crucial steps in the recognition system was embedding of the face geometric structure into a Euclidean (flat) space. Here, we propose to replace the flat embedding by a spherical one to construct isometric invariant representations of the facial image. We refer to these new invariants as spherical canonical images. Compared to its Euclidean counterpart, spherical embedding leads to notably smaller metric distortion. We demonstrate experimentally that representations with lower embedding error lead to better recognition. In order to efficiently compute the invariants we introduce a dissimilarity measure between the spherical canonical images based on the spherical harmonic transform.",
Improving plane extraction from 3D data by fusing laser data and vision,"The problem of extracting three-dimensional structures from data acquired with mobile robots has received considerable attention over the past years. Robots that are able to perceive their three-dimensional environment are envisioned to more robustly perform tasks like navigation, rescue, and manipulation. In this paper we present an approach that simultaneously uses color and range information to cluster 3D points into planar structures. Our current system also is able to calibrate the camera and the laser based on the remission values provided by the range scanner and the brightness of the pixels in the image. It has been implemented on a mobile robot equipped with a manipulator that carries a range scanner and a camera for acquiring colored range scans. Several experiments carried out on real data and in simulations demonstrate that our approach yields highly accurate results also in comparison with previous approaches.","Data mining,
Cameras,
Clustering algorithms,
Laser theory,
Mobile robots,
Robot vision systems,
Cities and towns,
Art,
Computer science,
Robustness"
Discriminant analysis: a unified approach,"Linear discriminant analysis (LDA) as a dimension reduction method is widely used in data mining and machine learning. It however suffers from the small sample size (SSS) problem when data dimensionality is greater than the sample size. Many modified methods have been proposed to address some aspect of this difficulty from a particular viewpoint. A comprehensive framework that provides a complete solution to the SSS problem is still missing. In this paper, we provide a unified approach to LDA, and investigate the SSS problem in the framework of statistical learning theory. In such a unified approach, our analysis results in a deeper understanding of LDA. We demonstrate that LDA (and its nonlinear extension) belongs to the same framework where powerful classifiers such as support vector machines (SVMs) are formulated. In addition, this approach allows us to establish an error bound for LDA. Finally our experiments validate our theoretical analysis results.","Linear discriminant analysis,
Statistical learning,
Data mining,
Equations,
Kernel,
Computer science,
Mathematics,
Machine learning,
Support vector machines,
Support vector machine classification"
Neuro-fuzzy Prediction of Biological Activity and Rule Extraction for HIV-1 Protease Inhibitors,A fuzzy neural network (FNN) and multiple linear regression (MLR) were used to predict biological activities of 26 newly designed HIV-1 protease potential inhibitory compounds. Molecular descriptors of 151 known inhibitors were used to train and test the FNN and to develop MLR models. The predictive ability of these two models was investigated and compared. We found the predictive ability of the FNN to be generally superior to that of MLR. The fuzzy IF/THEN rules were extracted from the trained network. These rules map chemical structure descriptors to predicted inhibitory values. The obtained rules can be used to analyze the influence of descriptors. Our results indicate that FNN and fuzzy IF/THEN rules are powerful modeling tools for QSAR studies.,"Inhibitors,
Fuzzy neural networks,
Neural networks,
Biological system modeling,
Chemical compounds,
Predictive models,
Biological information theory,
Chemistry,
Databases,
Integrated circuit modeling"
Universal Designated Multi Verifier Signature Schemes,"The notion of universal designated-verifier signatures was put forth by Steinfeld et. al. in Asiacrypt 2003. This notion allows a signature holder to designate the signature to a desired designated-verifier. In this paper, we extend this notion to allow a signature holder to designate the signature to multi verifiers, and hence, we call our scheme as universal designated multi verifier signatures. We provide security proofs for our schemes based on the random oracle model",
Efficient probabilistic packet marking,"Probabilistic packet marking is a general technique which routers can use to reveal internal network information to end-hosts. Such information is probabilistically set by the routers in headers of regular IP packets on their way to destinations. A number of potential applications have been identified, such as IP traceback, congestion control, robust routing algorithms, dynamic network reconfiguration, and locating Internet bottlenecks, etc. In this paper, we define EPPM, an efficient general probabilistic packet marking scheme with a wide range of potential applications, of which locating Internet bottlenecks and IP traceback are investigated as two representative examples to demonstrate its effectiveness. Our proposed scheme imposes only a single-bit overhead in the IP packet headers. More importantly, it significantly reduces the number of IP packets required to convey the relevant information when compared to the prior best known scheme (almost by two orders of magnitude)","IP networks,
Robust control,
Routing,
Heuristic algorithms,
Internet,
Computer networks,
Computer science,
Telecommunication traffic,
Computer crime,
Decoding"
Toward the design of robust interdomain routing protocols,"The border gateway protocol, the interdomain routing protocol for the Internet, allows for a wide variety of routing policies that may interact in unintended and unstable ways. Recent work on BGP and related protocols has begun to incorporate formal protocol models, which have enabled rigorous descriptive analyses of BGP. More recently, such models have been used to give prescriptive guidelines for the design of new protocols. These guidelines include both sufficient conditions for good routing behavior and limitations on what can be achieved without coordination between routers. Here we review potential routing problems, various formal protocol models, and the design guidelines they have been used to prove.","Robustness,
Routing protocols,
Guidelines,
Constraint theory,
Instruments,
Computer science,
Internet,
Sufficient conditions,
Information filtering,
Information filters"
Learning the Behavior of Users in a Public Space through Video Tracking,"The paper describes a video tracking system that tracks and analyzes the behavioral pattern of users in a public space. We have obtained important statistical measurements about users' behavior, which can be used to evaluate architectural design in terms of human spatial behavior and model the behavior of users in public spaces. Previously, such measurements could only be obtained through costly manual processes, e.g. behavioral mapping and time-lapse filming with human examiners. Our system has automated the process of analyzing the behavior of users. The system consists of a head detector for detecting people in each single frame of the video and data association for tracking people through frames. We compared the results obtained using our system with those obtained by manual counting, for a small data set, and found the results to be fairly accurate. We then applied the system to a large-scale data set and obtained substantial statistical measurements of parameters such as the total number of users who entered the space, the total number of users who sat by a fountain, the time that each spent by the fountain, etc. These statistics allow fundamental rethinking of the way people use a public space. This research is a novel application of computer vision in evaluating architectural design in terms of human behavior","Humans,
Computer science,
Extraterrestrial measurements,
Time measurement,
Application software,
Computer vision,
Cities and towns,
Data analysis,
Computer architecture,
Pattern analysis"
PRBAC: an extended role based access control for privacy preserving data mining,"Issues about privacy-preserving data mining have emerged globally, but still the main problem is that non-sensitive information or unclassified data, one is able to infer sensitive information that is not supposed to be disclosed. This paper proposes an approach to PPDM based on an extended role based access control called privacy preserving data mining using an extended role based access control(PRBAC); Sensitive objects (SOBS) component is added to the model in order to privacy protecting during data mining. Users are allowed to access and thereby mine different sets of data according to their roles. Our proposed model can be used over the existing technologies. The paper goal is to preserve individual's privacy.","Access control,
Data privacy,
Data mining,
Databases,
Algorithm design and analysis,
Permission,
Association rules,
Computer science,
Consumer electronics,
Protection"
Peers-assisted dynamic content distribution networks,"Content distribution networks (CDNs) have been proposed to primarily distribute Web and some limited streaming audio/video content over the Internet. Current CDNs consist of fixed content nodes placed at strategic locations on the Internet to improve the service latency and to reduce server and network load. Although the approach of delivering content with a fixed CDN has enjoyed some successes, it is often difficult to upgrade and extend a CDN to serve more users due to a very high deployment cost. In this paper, we propose that a CDN be agile while delivering content. We make the case for a dynamic CDN architecture in which a CDN leverages local peers to deliver the CDN's content to local users. We define a formal problem for using peers to increase the capacity of a CDN and we propose an algorithm to help a CDN to dynamically recruit local peers to be part of the CDN. The CDN and the recruited local peers cooperate to serve content to other local users. Our performance evaluation shows promising results with the new dynamic CDN architecture. Our simple CDN growing strategies dramatically improve the average service latency in streaming video content",
Performance modelling of a distributed approach to interference mitigation in license-exempt IEEE 802.16 systems,"In this paper, three approaches to modelling a distributed approach to interference mitigation in 802.16 license-exempt (LE) systems are described. An interference-free (IF) approach in which no interference is permitted at any node of the system; a controlled-interference (CI-T) and a controlled-interference with fairness (CI-F) approaches where interference is permitted at nodes that do not benefit from receiving the current transmission. The results show that the CI-F and CI-T schemes result in much better overall performance that either the IF scheme or the distributed scheme. Further, they result in much greater levels of BS activity. Despite being very conservative, even the IF scheme performs better than the distributed scheme in terms of throughput. This can be attributed to the fact that the distributed scheme suffers from collisions",
Web Services Based Robot Control Platform for Ubiquitous Functions,"In this paper, we employ Web services, programmable application logic accessible using standard Internet protocol, to enable a robot to access the distributed application logic based on the recent network technologies like XML, SOAP, WSDL, UDDI. Applications can communicate with each other in a platform and programming language independent manner. We first discuss a ubiquitous control platform based on ubiquitous functions to allow a robot the infinitive freedom of movement and knowledge acquirement. We then explain the fundamental ubiquitous functions management framework for a robot. In this framework, the applications are constructed from multiple Web services that work together to provide data and services for the application. Finally, the proposed scheme is applied to the control of the knowledge distributed robot system.","Web services,
Robot control,
Logic programming,
IP networks,
Web and internet services,
Access protocols,
XML,
Simple object access protocol,
Computer languages,
Control systems"
Collaborative knowledge management by integrating knowledge modeling and workflow modeling,"Recently both industrial and academic researches show great interest in knowledge management. However, users still find it hard to obtain a suitable knowledge management tool that fits for their needs. Although business requirements always change, current software systems still cannot be adapted to these changes quickly. In this paper, a framework is put forward to facilitate the design of an adaptive knowledge management system. In this framework, the structural knowledge modeling is combined with processes, which are used for ensuring the quality of knowledge acquisition in the framework. Two kinds of spaces, knowledge space and process space, and a knowledge state model are introduced. Finally, application systems based on this framework, which are being used in three real business enterprises for controlling life cycle of knowledge management in China, are discussed.","Collaborative work,
Knowledge management,
Ontologies,
Knowledge acquisition,
Knowledge engineering,
Computer science,
Information retrieval,
Application software,
Programming profession,
Computer industry"
Controlling the Coverage of Grid Information Dissemination Protocols,"Grid information dissemination protocols distribute information about the dynamic state of computational resources throughout interconnected wide area grids. Performance metrics for these protocols include the overhead of information packets, and the accuracy of the information at the time it is used to schedule applications. Our previous work advocated non-uniform protocols to keep dissemination local to the information source, as a method of keeping overhead manageable while achieving adequate freshness and accuracy. This paper considers the problem of providing better control over the dissemination of information and influencing the ""coverage footprint"" that defines where the information reaches within the grid. The paper describes work that investigates the coverage characteristics of existing protocols and refines and combines them into hybrid protocols that are more controllable. We consider this work to be a necessary step toward adaptive dissemination protocols that would be able to react to the state of grid resources to change dynamically how and where information is disseminated. This in turn increases the effectiveness of grid schedulers under various load levels and distributions","Protocols,
Processor scheduling,
Distributed computing,
Grid computing,
Computer applications,
Controllability,
Programmable control,
Adaptive control,
Computer science,
Measurement"
Interactive Tooth Segmentation of Dental Models,"The accurate segmentation of the teeth from the triangle mesh is an important step in computer-aided orthodontic. Because teeth come in different shapes and their arrangements vary substantially from one individual to another, tooth segmentation is difficult. This paper proposes a new method to accurately segment the teeth interactively. Based on curvature values of the triangle mesh, feature points are connected to feature regions. After feature lines are extracted from regions, feature contour can be obtained with the help of user supplied information. Using feature contour, the tooth are segmented accurately and individually","Teeth,
Dentistry,
Feature extraction,
Computer science,
Shape,
Data mining,
Application software,
Design automation,
Image segmentation,
Inspection"
Sequential correction of perspective warp in camera-based documents,"Documents captured with hand-held devices, such as digital cameras often exhibit perspective warp artifacts. These artifacts pose problems for OCR systems which at best can only handle in-plane rotation. We propose a method for recovering the planar appearance of an input document image by examining the vertical rate of change in scale of features in the document. Our method makes fewer assumptions about the document structure than do previously published algorithms.","Optical character recognition software,
Image edge detection,
Optical distortion,
Text analysis,
Image analysis,
Rivers,
Computer science,
Digital cameras,
Nearest neighbor searches,
Information analysis"
Short Paper: Towards a Location-Aware Role-Based Access Control Model,"With the growing use of wireless networks and mobile devices, we are moving towards an era where location information will be necessary for access control. The use of location information can be used for enhancing the security of an application, and it can also be exploited to launch attacks. For critical applications, a formal model for location-based access control is needed that increases the security of the application and ensures that the location information cannot be exploited to cause harm. In this paper, we show how the Role-Based Access Control (RBAC) model can be extended to incorporate the notion of location. We show how the different components in the RBAC model are related with location and how this location information can be used to determine whether a subject has access to a given object. This model is suitable for applications consisting of static and dynamic objects, where location of the subject and object must be considered before granting access.","Access control,
Information security,
Missiles,
Mobile computing,
Wireless sensor networks,
Global Positioning System,
Privacy,
Computer science,
Wireless networks,
Ubiquitous computing"
A general architecture for demand migration in a demand-driven execution engine in a heterogeneous and distributed environment,"This article is an overview of the GIPSY demand migration system (DMS). This system brings a demand driven execution engine like the one used in the GIPSY to a high level of distributiveness and interoperability of operational nodes, by mixing together advanced distributed technologies. The main demand migration system's artifacts are discussed, and their different distributions within the GIPSY are surveyed. The article concludes with a presentation of a successful GIPSY demand migration implementation, based on JINI. This paper describes only the aspects of the GIPSY demand migration, i.e. it does not deal with load balancing and efficiency aspects of the GIPSY, as these are to be tackled by other subsystems of the GIPSY.","Engines,
Distributed computing,
Logic programming,
Middleware,
Asynchronous communication,
Computer architecture,
Computer science,
Software engineering,
Load management,
Java"
On schedulability bounds of static priority schedulers,"While utilization bound based schedulability test is simple and effective, it is often difficult to derive the bound itself. For its analytical complexity, utilization bound results are usually obtained on a case-by-case basis. In this paper, we develop a general framework that allows one to effectively derive schedulability bounds for a wide range of real-time systems with different workload patterns and schedulers. Our analytical model is capable of describing a wide range of tasks and schedulers' behaviors. We propose a new definition of utilization, called workload rate. While similar to utilization, workload rate enables flexible representation of different scheduling and workload scenarios and leads to uniform derivation of schedulability bounds. We derive a parameterized schedulability bound for static priority schedulers with arbitrary priority assignment. Existing utilization bounds for different priority assignments and task releasing patterns can be derived from our closed-form formula by simple assignments of proper parameters.","Processor scheduling,
System testing,
Real time systems,
Analytical models,
Delay,
Time measurement,
Length measurement,
Computer science,
Timing,
Computer applications"
On SAT-based Bounded Invariant Checking of Blackbox Designs,"Design verification by property checking is a mandatory task during circuit design. In this context, bounded model checking (BMC) has become popular for falsifying erroneous designs. Additionally, the analysis of partial designs, i.e., circuits that are not fully specified, has recently gained attraction. In this work we analyze how BMC can be applied to such partial designs. Our experiments show that even with the most simple modelling scheme, namely 01X-simulation, a relevant number of errors can be detected. Additionally, we propose a SAT-solver that directly can handle 01X-logic","Sequential circuits,
Data structures,
Boolean functions,
Computer science,
Circuit synthesis,
Context modeling,
Chip scale packaging,
Explosions,
Automatic logic units,
Councils"
Online detection and diagnosis of multiple configuration upsets in LUTs of SRAM-based FPGAs,This paper proposes a new CLB architecture for FPGAs and associated online testing and reconfiguration techniques that detect configuration upsets in the LUTs of SRAM-based FPGAs and correct them using partial reconfiguration. These configuration upsets may either be single event upsets (SEUs) or even multiple configuration upsets. Any error in a CLB is detected with a latency of just 16 clock cycles and the errors are diagnosed by propagating them to a single output port by a chain-like shift register. The proposed CLB architectures require only 2 additional SRAM configuration bits per LUT for a Xilinx Virtex II architecture. This is extremely low when compared to the 16 additional SRAM configuration bits required by CLB architectures used to implement standard DWC techniques for detecting configuration upsets in LUTs.,"Table lookup,
Field programmable gate arrays,
Testing,
Circuit faults,
Aerospace electronics,
Computer science,
Delay,
Random access memory,
Reconfigurable logic,
Logic circuits"
VLAN-based minimal paths in PC cluster with Ethernet on mesh and torus,"In a PC cluster with Ethernet, well-distributed multiple paths among hosts can be obtained by applying VLAN technology. In this paper, we propose VLAN topology sets and path assignment methods in mesh and torus. The proposed VLAN-based methods on mesh require N/sup M-1/ and /spl lfloor/N/sup M-1//2/spl rfloor/+1 VLANs to provide balanced minimal paths and partially balanced ones respectively, where N is the number of switches per dimension and M is the number of dimensions. Similarly, those on torus require 2N/sup M-1/ and N/sup M-1/+2 VLANs respectively. Simulation results show that the proposed methods improve up to 902% and 706% of throughput respectively.","Ethernet networks,
Switches,
Network topology,
Throughput,
Routing,
Parallel processing,
Communication switching,
Bandwidth,
Concurrent computing,
Computer science"
Exploring semantic technologies in service matchmaking,"With the extensive applications of Web services on the Internet, to locate target services in an accurate and efficient way becomes increasingly difficult. At present, semantic Web service is regarded as the most promising approach to address the challenge. In this paper, an ontology service description language and a query language, OSDL and OSQL are proposed respectively, and then a service matchmaking algorithm based on interface semantics is proposed. The problem of service matchmaking can be attributed to the similarity between the semantic pair, and the similarity is the quantization of their relationship, which is discovered through reasoning on ontology. Unlike the existing semantics-based service matchmaking algorithms, which focus on the relationships among the successors and ancestors of ontology classes, the proposed one also pays attention to the comparability among classes and their property classes. The experiment data indicate that it is worthwhile to take the point into consideration to receive a higher recall and precision.","Web services,
Ontologies,
Semantic Web,
Database languages,
Educational institutions,
Computer science,
Application software,
Web and internet services,
Quantization,
Distributed computing"
Additive modeling of English F0 contour for speech synthesis,"In this paper, we present an approach to fundamental frequency contour modeling of English for speech synthesis, based on a statistical learning technique called additive models that was successfully applied to the modeling of Japanese F/sub 0/ contours previously. In an attempt to model English F/sub 0/ contours, we defined a three-layer additive model consisting of an intonational phrase component, a word-level component representing lexical stress types, and a pitch-accent component related to accented syllables. These component functions are estimated simultaneously using a backfitting algorithm derived from a regularized least-squares error criterion specified on the model with regard to the training data. The proposed method was trained and tested using the widely used ToBI-labeled speech corpus and promising results were obtained.",
BiRD: A Strategy to Autonomously Supplement Clinical Practice Guidelines with Related Clinical Studies,"In this paper we introduce a framework to supplement and tag computerized CPG with related best-evidence automatically sourced from on-line medical literature repositories. The idea is to provide CPG users with additional published evidence pertaining to the different sections of a CPG of their interest. We present a web-enabled Best-evidence Retrieval and Delivery (BiRD) system that autonomously retrieves pertinent medical literature with respect to user-specified content from a GEM-encoded CPG. This is achieved via a multi-level literature search strategy that uses the actual CPG content to autonomously generate a search query. The featured search strategy firstly categorizes the search query towards a priori defined clinical query subjects and secondly filters out insignificant medical terms from the search query. The technical architecture comprises existing medical language processing tools and vocabularies, together with newly developed tools to automatically (a) generate optimum search queries; (b) retrieve medical articles from MEDLINE; and (c) embed the retrieved medical articles within XML-based CPG encoded according to the GEM formalism.","Birds,
Guidelines,
Medical diagnostic imaging,
Biomedical informatics,
Content based retrieval,
Filters,
Medical services,
Computer science,
Vocabulary,
Standardization"
Real stock trading using soft computing models,The main focus of this study is to compare different performances of soft computing paradigms for predicting the direction of individuals stocks. Three different artificial intelligence techniques were used to predict the direction of both Microsoft and Intel stock prices over a period of thirteen years. We explore the performance of artificial neural networks trained using backpropagation and conjugate gradient algorithm and a Mamdani and Takagi Sugeno fuzzy inference system learned using neural learning and genetic algorithm. Once all the different models were built the last part of the experiment was to determine how much profit can be made using these methods versus a simple buy and hold technique.,"Fuzzy neural networks,
Genetic algorithms,
Fuzzy systems,
Artificial intelligence,
Artificial neural networks,
Inference algorithms,
Backpropagation algorithms,
Economic indicators,
Computer science,
Learning"
Visual/Acoustic Emotion Recognition,"To recognize and understand a person's emotion has been known as one of the most important issue in human-computer interaction. In this paper, we present a multimodal system that supports emotion recognition from both visual and acoustic feature analysis. Our main achievement is that with this bimodal method, we can effectively extend the recognized emotion categories compared to when only visual or acoustic feature analysis works alone. We also show that by carefully cooperating bimodal features, the recognition precision of each emotion category will exceed the limit set up by the single modality, both visual and acoustic. Moreover, we believe our system is closer to real human perception and experience and hence will make emotion recognition closer to practical application in the future",
Look there! Predicting where to look for motion in an active camera network,"A framework is proposed that answers the following question: if a moving object is observed by one camera in a pan-tilt-zoom (PTZ) camera network, what other camera(s) might be foveated on that object within a predefined time window, and what would be the corresponding PTZ parameter settings? No calibration is assumed, and there are no restrictions on camera placement or initial parameter settings. The framework accrues a predictive model over time. To start out, the cameras follow randomized ""tours"" in discretized PTZ space. If a moving object is detected in the field of view of more than one camera at a particular instant or within a predefined time window, then the model is updated to record the cameras' associations and the corresponding parameter settings. As more and more moving objects are observed, the model adapts and the most frequent associations are discovered. The formulation also allows for verification of its predictions, and reinforces its correct predictions. The system is demonstrated in observing people in an office environment with a three PTZ camera network.","Intelligent networks,
Cameras,
Calibration,
Computer science,
Solid modeling,
Predictive models,
Object detection,
Surveillance,
Layout,
Information geometry"
Analyzing the Impact of Components Replication in High Available J2EE Clusters,"Clustering is a well known technique that allows scalability and fault tolerance in distributed systems. In the J2EE framework, clustering can be used to improve the performance and the availability of an application. In this context, however, great care has to be taken with respect to the replication of stateful components; specifically, stateful components replication can adversely affect the performance of the system and can introduce unexpected modifications in the semantics of the application. In this paper, exploiting our experience in this area, we analyze and discuss these issues, and propose possible solutions. We also describe how component replication is implemented in a number of (open source and commercial) J2EE compliant application servers. The aim of this paper is to provide the insight to better evaluate the software solutions and the related costs/benefits trade-off issues that a software architect has to address when designing a critical, high available, system","Application software,
Scalability,
Availability,
Open source software,
Business,
Computer science,
Fault tolerant systems,
Robustness,
Load management,
Java"
An intelligent knowledge sharing strategy featuring item-based collaborative filtering and case based reasoning,"In this paper, we propose a new approach for combining item-based collaborative filtering (CF) with case based reasoning (CBR) to pursue personalized information filtering in a knowledge sharing context. Functionally, our personalized information filtering approach allows the use of recommendations by peers with similar interests and domain experts to guide the selection of information deemed relevant to an active user's profile. We apply item-based similarity computation in a CF framework to retrieve N information objects based on the user's interests and recommended by peer. The N information objects are then subjected to a CBR based compositional adaptation method to further select relevant information objects from the N retrieved past cases in order to generate a more fine-grained recommendation.","Collaboration,
Computer aided software engineering,
Information filtering,
Information filters,
Information retrieval,
Collaborative work,
Computer science,
Peer to peer computing,
Knowledge management,
Content based retrieval"
Hardware friendly vector quantization algorithm,This paper proposes a new vector quantization algorithm that can be directly implemented by hardware and its performance is discussed. The algorithm is based on the assumption that most of the vector elements in the same class fall within a certain range. The proposed algorithm provides very fast vector quantization with a dedicated hardware. VHDL simulations with real world data verify the feasibility of the system followed by the circuit size and speed evaluation. Results show that the proposed system has higher performance than those of K-nearest-neighbor method or neural network approach.,"Hardware,
Vector quantization,
Prototypes,
Testing,
Computer science,
Intelligent systems,
Circuit simulation,
Neural networks,
Virtual colonoscopy,
Clustering algorithms"
A multi-property trust model for reconfiguring component software,"While component software continues to grow in size and complexity, it becomes increasingly difficult to ensure qualities for component services, especially at run-time. This paper focuses on a framework on middleware for dynamic re-configuration of components of different qualities from the view of trust. Firstly, a trust management model is built, in which the management framework and measurement model are presented. Secondly, the reconfiguration algorithm is described based on the model. Thirdly, the trust management is implemented as a kind of public services and some tools on a J2EE-compliant middleware platform, i.e., PKUAS. Finally, the related work is discussed and compared with our research.","Quality of service,
Middleware,
Runtime,
Software quality,
Software systems,
Application software,
Fault tolerance,
Quality management,
Mechanical factors,
Computer science"
Efficient integration of fine-grained access control in large-scale grid services,"In this paper, we present a scalable authorization service, based on the concept of fine-grained access control (FGAC), for large-scale grid infrastructures that span multiple independent domains. FGAC enables participating resource owners to specify fine-grained policies concerning which user can access can their resources under which mode. We argue that such an authorization service must be integrated with the resource broker service to avoid scheduling requests onto resources which do not authorize the user request. For this reason, we develop a novel resource broker service that integrates access control with resource scheduling. In our system, both resource owners and users define their resource access and usage policies. The resource broker schedules a user request only within the set of resources whose policies match the user credentials (and vice-versa). Since this process of evaluating authorization policies of resources and user, in addition to checking the resource requirement, can be a potential bottleneck for a large scale grid, we also analyze the problem of efficient evaluation of FGAC policies. In this context, we present a novel method for policy organization and compare its performance with other strategies. Preliminary results show that the proposed method can significantly enhance performance.","Access control,
Large scale integration,
Authorization,
Security,
Computer science,
Resource management,
Large-scale systems,
Grid computing,
Processor scheduling,
Throughput"
Multiple-key cryptography-based distributed certificate authority in mobile ad-hoc networks,"Most prevalent distributed certificate authority (DCA) schemes in the MANET are based upon threshold cryptography, which is invulnerable to mobile adversaries and tolerable to missing or faulty DCA server nodes, and thus becomes the ""de facto"" standard for the security framework in the MANET. However, this scheme cannot defeat Sybil attacks, in which a malicious node impersonates many identities. To solve the problem, a multiple-key cryptography-based DCA scheme, namely the MC-DCA scheme, is proposed in the paper. It is invulnerable to Sybil attacks, and achieves lower communication overhead and moderate latency compared with the threshold-based scheme, which is supported by the simulation results.",
A mechanism for range image integration without image registration,"A mechanism is introduced that automatically integrates multi-view range images without registering the images. The mechanism is based on a reference double-frame that acts as the coordinate system of the scene. A single-view range image of a scene is obtained by sweeping a laser line over the scene by hand and analyzing the acquired light stripes. Range images captured from different views of the scene are in the coordinate system of the double-frame, and thus, automatically integrate without further processing.","Image registration,
Layout,
Cameras,
Image analysis,
Geometrical optics,
Computer vision,
Computer science,
Computational geometry,
Stereo vision,
Optical interferometry"
Demand-driven structural testing with dynamic instrumentation,"Producing reliable and robust software has become one of the most important software development concerns. Testing is a process by which software quality can be assured through the collection of information. While testing can improve software reliability, current tools typically are inflexible and have high overheads, making it challenging to test large software projects. In this paper, we describe a new scalable and flexible framework for testing programs with a novel demand-driven approach based on execution paths to implement test coverage. This technique uses dynamic instrumentation on the binary code that can be inserted and removed on-the-fly to keep performance and memory overheads low. We describe and evaluate implementations of the framework for branch, node and def-use testing of Java programs. Experimental results for branch testing show that our approach has, on average, a 1.6 speed up over static instrumentation and also uses less memory.","Instruments,
Software testing,
Probes,
Software quality,
Computer science,
Robustness,
Programming,
Software tools,
Java,
Computer languages"
On Simulation of Adaptive Learner Control Considering Students' Cognitive Styles Using Artificial Neural Networks (ANNs),"The field of the learning sciences is represented by a growing community internationally. Many experts now recognize that conventional ways of conceiving knowledge, educational systems and technology-mediated learning are facing increasing challenges in this time of rapid technological and social changes. Herein, a suggested innovative, challenging trend at the field of the learning sciences is adopted. This trend deals with complex issues associated to deferent educational/learning phenomena. This paper presents a novel realistic simulation of some observed educational phenomena using ANNs modeling. Results obtained after practical application of a computer assisted instructional program are mainly motivating our realistic ANNs computer models. In some details, suggested ANNs models simulate realistically students' learning behavioral phenomena for two different cognitive styles. These styles are field dependent (FD) style and field independent (FI) one. Both were simulated via two types of ANNs models obeying two learning paradigms. That is supervised and unsupervised learning paradigms simulating FD and FI cognitive styles respectively. Moreover, both models simulate adaptive performance of learner control as learning process proceeds. That is fulfilled after following of suggested deferent learner control indices to simulate deferent levels of learner control. Interestingly, obtained results shown to be agree well with results obtained by practical educational experimental work. Additionally, our results give a valuable interpretation to some other results obtained after some educational studies. Conclusively, this paper, in few words, classified as an interdisciplinary research work dealing with some complex challenging educational issues. In practice, these issues arise recently due to excessive progress in computer and information technologies. That applied in educational practical field aiming to reach non traditional (non classical) solutions for confronted issues","Programmable control,
Adaptive control,
Artificial neural networks,
Computational modeling,
Educational technology,
Computer aided instruction,
Application software,
Computer applications,
Unsupervised learning,
Process control"
A T-decomposition algorithm with O(n log n) time and space complexity,"T-decomposition maps a finite string into a series of parameters for a recursive string construction algorithm. Initially developed for the communication of coding trees (M. R. Titchener, June 1996), (U. Guenther, Feb. 2001), T-decomposition has since been studied within the context of information measures. This involves the parsing of potentially very large strings, which in turn requires algorithms with good time complexity. This paper presents a T-decomposition algorithm with O(n log n) time and space complexity",
Flexible multi-agent decision making under time pressure,"Autonomous agents need considerable computational resources to perform rational decision making. These demands are even more severe when other agents are present in the environment. In these settings, the quality of an agent's alternative behaviors depends not only on the state of the environment, but also on the actions of other agents, which in turn depend on the others' beliefs about the world, their preferences, and further on the other agents' beliefs about others, and so on. The complexity becomes prohibitive when large number of agents are present and when decisions have to be made under time pressure. In this paper, we investigate strategies intended to tame the computational burden by using offline computation in conjunction with online reasoning. We investigate two approaches. First, we use rules compiled offline to constrain alternative actions considered during online reasoning. This method minimizes overhead, but is not sensitive to changes in real-time demands of the situation at hand. Second, we use performance profiles computed offline and the notion of urgency (i.e., the value of time) computed online to choose the amount of information to be included during online deliberation. This method can adjust to various levels of real-time demands, but incurs some overhead associated with iterative deepening. We test our framework with experiments in a simulated anti-air defense domain. The experiments show that both procedures are effective in reducing computation time while offering good performance under time pressure.",
An attribute and role based access control model for Web services,"Based on the analysis of the access control requirements for Web services, this paper points out the limitation of current access control models for Web services, and presents an attribute and role based access control model for Web services. The model automatically produces the role set, accomplishes the mapping among users, permissions and roles, and unifies the access control for Web services and data resources involved.","Access control,
Web services,
Access protocols,
Web and internet services,
Transport protocols,
Simple object access protocol,
Information security,
Computer science,
Authorization,
Educational institutions"
A Service Learning Program for CSE Students,"We describe a service-learning program within the CSE department at the University of Notre Dame. Started in 1997 as the first affiliate of the Purdue EPICS program, this service-learning program involves volunteer faculty-mentored teams of students applying engineering skills in local, national, and international consulting projects for various community, educational, and not-for-profit organizations. Student-led teams, functioning as small engineering consulting firms, participate in identifying and selecting potential consulting engagements followed by the delivery of solutions to those clients. We describe 1) our unique adaptation of the EPICS model 2) supporting resources, and 3) a review of computer science centric education in multidisciplinary service engineering projects. The practical considerations of real world project management, client interaction, and liability are discussed in correlation with recent service projects undertaken by the students. Advice for interested educational institutions for developing their own engineering service learning program is presented","Educational institutions,
Computer science,
Project management,
Concrete,
Databases,
Computer networks,
Computer science education,
Educational programs,
Civil engineering,
Programming profession"
Integrating Database Technology with Comparison-based Parallel Performance Diagnosis: The PerfTrack Performance Experiment Management Tool,"PerfTrack is a data store and interface for managing performance data from large-scale parallel applications. Data collected in different locations and formats can be compared and viewed in a single performance analysis session. The underlying data store used in PerfTrack is implemented with a database management system (DBMS). PerfTrack includes interfaces to the data store and scripts for automatically collecting data describing each experiment, such as build and platform details. We have implemented a prototype of PerfTrack that can use Oracle or PostgreSQL for the data store. We demonstrate the prototype's functionality with three case studies: one is a comparative study of an ASC purple benchmark on high-end Linux and AIX platforms; the second is a parameter study conducted at Lawrence Livermore National Laboratory (LLNL) on two high end platforms, a 128 node cluster of IBM Power 4 processors and BlueGene/L; the third demonstrates incorporating performance data from the Paradyn Parallel Performance Tool into an existing PerfTrack data store.","Databases,
Technology management,
Performance analysis,
Government,
Laboratories,
Large scale integration,
Prototypes,
Computer science,
Computer interfaces,
Concurrent computing"
Knowledge management support for collaborative emergency response,"An emergency response activity usually involves several teams from different organizations working cooperatively for the purpose of saving lives or properties. These teams have to make many decisions under time pressure to accomplish their goals. Most decisions require contextual knowledge coming from the emergency settings, including those which report the activities of other emergency teams. The goal of this paper is to describe a system which aims at storing and disseminating the contextual knowledge required by teams during the course of an emergency.",
ParaScale: exploiting parametric timing analysis for real-time schedulers and dynamic voltage scaling,"Static timing analysis safely bounds worst-case execution times to determine if tasks can meet their deadlines in hard real-time systems. However, conventional timing analysis requires that the upper bound of loops be known statically, which limits its applicability. Parametric timing analysis methods remove this constraint by providing the WCET as a formula parameterized on loop bounds. This paper contributes a novel technique to allow parametric timing analysis to interact with dynamic real-time schedulers. By dynamically detecting actual loop bounds, a lower WCET bound can be calculated, on-the-fly, for the remaining execution of a task. We analyze the benefits from parametric analysis in terms of dynamically discovered slack in a schedule. We then assess the potential for dynamic power conservation by exploiting parametric loop bounds for ParaScale, our intra-task dynamic voltage scaling (DVS) approach. Our results demonstrate that the parametric approach to timing analysis provides 66%-80% additional savings in power consumption. We further show that using this approach combined with online intra-task DVS to exploit parametric execution times results in much lower power consumption. Hence, even in the absence of dynamic scheduling, significant savings in power can be obtained, e.g., in the case of cyclic executives","Timing,
Dynamic scheduling,
Dynamic voltage scaling,
Real time systems,
Computer science,
Job shop scheduling,
Embedded system,
Upper bound,
Vehicle dynamics,
Voltage control"
Maintainability prediction: a regression analysis of measures of evolving systems,"In order to build predictors of the maintainability of evolving software, we first need a means for measuring maintainability as well as a training set of software modules for which the actual maintainability is known. This paper describes our success at building such a predictor. Numerous candidate measures for maintainability were examined, including a new compound measure. Two datasets were evaluated and used to build a maintainability predictor. The resulting model, Maintainability Prediction Model (MainPredMo), was validated against three held-out datasets. We found that the model possesses predictive accuracy of 83% (accurately predicts the maintainability of 83% of the modules). A variant of MainPredMo, also with accuracy of 83%, is offered for interested researchers.","Regression analysis,
Software maintenance,
Predictive models,
Software measurement,
Performance analysis,
Computer science,
Lab-on-a-chip,
Accuracy,
Software systems,
Error correction"
A scenario-based performance evaluation of multicast routing protocols for ad hoc networks,"Current ad hoc multicast routing protocols have been designed to build and maintain a tree or mesh in the face of a mobile environment, with fast reaction to network changes in order to minimize packet loss. However, the performance of these protocols has not been adequately examined under realistic scenarios. Existing performance studies generally use a single, simple mobility model, with low density and often very low traffic rates. We explore the performance of ad hoc multicast routing protocols under scenarios that include realistic mobility patterns, high density and high traffic load. We use these scenarios to identify cases where existing protocols can improve their performance. Based on our observations, we make a series of recommendations for designers of multicast protocols.","Multicast protocols,
Routing protocols,
Ad hoc networks,
Telecommunication traffic,
Computer science,
Mobile computing,
Traffic control,
Mobile ad hoc networks,
Displays,
Wireless application protocol"
Online Context Recognition in Multisensor Systems using Dynamic Time Warping,"In this paper we present our system for online context recognition of multimodal sequences acquired from multiple sensors. The system uses Dynamic Time Warping (DTW) to recognize multimodal sequences of different lengths, embedded in continuous data streams. We evaluate the performance of our system on two real world datasets: 1) accelerometer data acquired from performing two hand gestures and 2) NOKIA's benchmark dataset for context recognition. The results from both datasets demonstrate that the system can perform online context recognition efficiently and achieve high recognition accuracy.","Multisensor systems,
Multimodal sensors,
Pervasive computing,
Hidden Markov models,
Speech recognition,
Computer science,
Sensor systems,
Accelerometers,
Performance evaluation,
Embedded computing"
MAC layer security of 802.15.4-compliant networks,"The paper discusses security issues of networks compliant with the recent IEEE 802.15.4 standard for low rate WPANs. A number of vulnerabilities at the MAC and PHY layer are identified, and a number of possible attacks at the MAC layer are outlined, some of which can be easily launched with devices that are fully compliant with the 802.15.4 standard. Some remedial measures are proposed to help defend against those attacks or at least alleviate their impact on the performance of the network","Wireless sensor networks,
Physical layer,
Routing,
Information security,
Jamming,
Media Access Protocol,
Computer science,
Computer security,
Structural engineering,
Agriculture"
"An efficient algorithm for the extended (l,d)-motif problem with unknown number of binding sites","Finding common patterns, or motifs, from a set of DNA sequences is an important problem in molecular biology. Most motif-discovering algorithms/software require the length of the motif as input. Motivated by the fact that the motifs length is usually unknown in practice, Styczynski et al. introduced the extended (l,d)-motif problem (EMP), where the motifs length is not an input parameter. Unfortunately, the algorithm given by Styczynski et al. to solve EMP can take an unacceptably long time to run, e.g. over 3 months to discover a length-14 motif. This paper makes two main contributions. First, we eliminate another input parameter from EMP: the minimum number of binding sites in the DNA sequences. Fewer input parameters not only reduces the burden of the user, but also may give more realistic/robust results since restrictions on length or on the number of binding sites make little sense when the best motif may not be the longest nor have the largest number of binding sites. Second, we develop an efficient algorithm to solve our redefined problem. The algorithm is also a fast solution for EMP (without any sacrifice to accuracy) making EMP practical.","Sequences,
Pulse width modulation,
EMP radiation effects,
Partitioning algorithms,
DNA,
Biological system modeling,
Portable media players,
Proteins,
Computer science,
Biology"
Shape classifier based on generalized probabilistic descent method with hidden Markov descriptor,"The goal of this paper is to present a weighted likelihood discriminant for minimum error shape classification. Different from traditional maximum likelihood (ML) methods, in which classification is based on probabilities from independent individual class models as is the case for general hidden Markov model (HMM) methods, proposed method utilizes information from all classes to minimize classification error. The proposed approach uses a HMM for shape curvature as its 2D shape descriptor. In this contribution, we introduce a weighted likelihood discriminant function and present a minimum error classification strategy based on generalized probabilistic descent (GPD) method. We believe our sound theory based implementation reduces classification error by combining HMM with GPD theory. We show comparative results obtained with our approach and classic ML classification along with Fourier descriptor and Zernike moments based classification for fighter planes and vehicle shapes.","Shape,
Hidden Markov models,
Topology,
Computer errors,
Object recognition,
Computer vision,
Feature extraction,
Robustness,
Computer science,
Vehicles"
Drift and Scaling in Estimation of Distribution Algorithms,"This paper considers a phenomenon in Estimation of Distribution Algorithms (EDA) analogous to drift in population genetic dynamics. Finite population sampling in selection results in fluctuations which get reinforced when the probability model is updated. As a consequence, any probability model which can generate only a single set of values with probability 1 can be an attractive fixed point of the algorithm. To avoid this, parameters of the algorithm must scale with the system size in strongly problem-dependent ways, or the algorithm must be modified. This phenomenon is shown to hold for general EDAs as a consequence of the lack of ergodicity and irreducibility of the Markov chain on the state of probability models. It is illustrated in the case of UMDA, in which it is shown that the global optimum is only found if the population size is sufficiently large. For the needle-in-a haystack problem, the population size must scale as the square-root of the size of the search space. For the one-max problem, the population size must scale as the square-root of the problem size.","counting-ones problem,
Estimation of distribution algorithms,
probability building genetic algorithms,
Markov chain analysis,
convergence analysis,
needle-in-a-haystack problem"
Software Engineering Projects in Distant Teaching,"Software engineering education is most often complemented by a software engineering project where a team of students has to develop a large software system. At a distance teaching university such projects challenge the students in communication, coordination, and collaboration, because team members work in different places, many miles away from each other. We present an ECLIPSE-based unified platform that leverages available tools and solutions and discuss the problems involved. Besides using plug-ins that support the students during implementation, our platform integrates a collaborative distant education environment and a software project management system that eases the students' collaboration in the software engineering project",
On the hardness of distinguishing mixed-state quantum computations,"This paper considers the following problem. Two mixed-state quantum circuits Q/sub 0/ and Q/sub 1/ are given, and the goal is to determine which of two possibilities holds: (i) Q/sub 0/ and Q/sub 1/ act nearly identically on all possible quantum state inputs, or (ii) there exists some input state /spl rho/ that Q/sub 0/ and Q/sub 1/ transform into almost perfectly distinguishable outputs. This may be viewed as an abstraction of the problem that asks, given two discrete quantum mechanical processes described by sequences of local interactions, are the processes effectively the same or are they different? We prove that this promise problem is complete for the class QIP of problems having quantum interactive proof systems, and is therefore PSPACE-hard. This is in contrast to the fact that the analogous problem for classical (probabilistic) circuits is in AM, and for unitary quantum circuits is in QMA.","Quantum computing,
Power system modeling,
Computational modeling,
Quantum mechanics,
Circuit simulation,
Noise measurement,
Circuit noise,
Computer science,
Discrete transforms,
Complexity theory"
Haplotype phasing using semidefinite programming,"Diploid organisms, such as humans, inherit one copy of each chromosome (haplotype) from each parent. The conflation of inherited haplotypes is called the genotype of the organism. In many disease association studies, the haplotype data is more informative than the genotype data. Unfortunately, getting haplotype data experimentally is both expensive and difficult. The haplotype inference with pure parsimony (HPP) problem is the problem of finding a minimal set of haplotypes that resolve a given set of genotypes. We provide a quadratic integer program (QIP) formulation for the HPP problem, and describe an algorithm for the HPP problem based on a semi-definite programming (SDP) relaxation of that QIP program. We compare our approach with existing approaches. Further, we show that the proposed approach is capable of incorporating a variety of additional constraints, such as missing or erroneous genotype data, outliers etc.","Humans,
Biological cells,
Organisms,
Diseases,
Sequences,
Computer science,
Linear programming,
Inference algorithms,
Quadratic programming,
Genetic mutations"
WSPeer - an interface to Web service hosting and invocation,"This paper introduces WSPeer, a high level interface to hosting and invoking Web services. WSPeer aims to support the diversification of Web service deployments by providing a pluggable architecture that can be used to host Web services in a number of different ways. In particular, we are interested in the cross-fertilisation between Web services and peer-to-peer (P2P) systems by enabling entities to act as both service providers and consumers in a flexible and extensible way. Further, we support the behaviour needed within a P2P environment by allowing Web services to be dynamically deployed through the use of a lightweight container that is more suited to highly transient connectivity. This approach allows standard Web services to be deployed within a P2P environment and yet still adhere to a service oriented architecture. We argue therefore that both Web services and P2P are exponents of service oriented architecture, approaching the same goals from very different directions, and that a combination of the strengths of these approaches can lead to more robust and ubiquitous Web service and P2P deployments.","Web services,
Service oriented architecture,
Computer science,
Peer to peer computing,
Protection,
Computer architecture,
Containers,
Robustness,
Transport protocols,
Standards development"
Educational Results of the Personal Exploration Rover Museum Exhibit,"The Personal Rover Project produces technology, curriculum and evaluation techniques for robotic educational use in formal and informal (after-school, out-of-school) learning environments. Our specific aim for this phase of the project is to create and evaluate human-robot interactions that educate members of the general public in an informal learning environment, specifically museums. Our educational goals are to further an appreciation and understanding of NASA's Mars Exploration Rovers (MERs), to illustrate the role of robotic rovers in scientific exploration, and to provide hands-on learning experiences that demonstrate robot autonomy. We have designed a new robot, the Personal Exploration Rover (PER) and the related interactive components of a museum exhibit to achieve these goals. Here we describe the exhibits developed and the formal evaluation results of the exhibits' educational impact and efficacy. These results suggest techniques by which learning can be measured and used as an indicator of successful human-robot interaction.",
Free-viewpoint image synthesis from multiple-view images taken with uncalibrated moving cameras,"In this paper, we propose methods for free-viewpoint image synthesis from multiple-view images taken with uncalibrated cameras. In our method, two viewpoints are selected as basis images for defining a projective 3D coordinate of the object scene in which the scene structure is recovered from the input multiple-view images. The 3D coordinate is defined by the image coordinates of the selected two basis images according to the epipolar geometry between those two images, which is represented by a fundamental matrix. The multiple-view images are also related to the projective 3D coordinate by their epipolar geometry to the basis images. Based on such a framework, we do not need to strongly calibrate the cameras, so we can recover 3D structure of the scene without effort for the camera calibration. In addition to that, we can also render free-viewpoint images from hand-held moving cameras. We will demonstrate the effectiveness of the proposed method by showing free-viewpoint images from multi-view images taken with hand-held moving cameras.","Image generation,
Cameras,
Calibration,
Layout,
Humans,
Shape,
Geometry,
Rendering (computer graphics),
Indium tin oxide,
Computer science"
A Survey on Multimodal Video Representation for Semantic Retrieval,"This paper surveys the approaches to video representation, focusing on semantic analysis for content-based indexing and retrieval. A problem of adaptive representation of digital multimedia is critically assessed and some novel ideas are presented. Furthermore, the concept of video multimodality is reevaluated and redefined in order to introduce modalities such as editing technique or affect to the audience",
Feasibility of half-data image reconstruction in 3-D reflectivity tomography with a spherical aperture,"Reflectivity tomography is an imaging technique that seeks to reconstruct certain acoustic properties of a weakly scattering object. Besides being applicable to pure ultrasound imaging techniques, the reconstruction theory of reflectivity tomography is also pertinent to hybrid imaging techniques such as thermoacoustic tomography. In this work, assuming spherical scanning apertures, redundancies in the three-dimensional (3-D) reflectivity tomography data function are identified and formulated mathematically. These data redundancies are used to demonstrate that knowledge of the measured data function over half of its domain uniquely specifies the 3-D object function. This indicates that, in principle, exact image reconstruction can be performed using a ""half-scan"" data function, which corresponds to temporally untruncated measurements acquired on a hemi-spherical aperture, or using a ""half-time"" data function, which corresponds to temporally truncated measurements acquired on the entire spherical aperture. Both of these minimal scanning configurations have important biological imaging applications. An iterative reconstruction method is utilized for reconstruction of a simulated 3-D object from noiseless and noisy half-scan and half-time data functions.","Image reconstruction,
Reflectivity,
Tomography,
Apertures,
Ultrasonic imaging,
Acoustic imaging,
Acoustic scattering,
Performance evaluation,
Iterative methods,
Reconstruction algorithms"
Compensating respiratory motion in PET image reconstruction using 4D PET/CT,"Respiratory motion of the patient can cause image blur and inaccurate radioactivity quantification in PET imaging. Gated PET acquisition freezes the motion but suffers in image quality due to insufficient photon statistics. We present in this paper a novel approach that incorporates motion information into the reconstruction process and reconstruct the motion-free image with all data acquired. Computer simulation and phantom study showed good motion compensation capability of this algorithm, with no obvious motion artifacts visible on the reconstructed image","Positron emission tomography,
Image reconstruction,
Computed tomography,
Imaging phantoms,
Reconstruction algorithms,
Statistics,
Computer simulation,
Motion compensation,
Image quality,
Optical imaging"
2D tracking performance evaluation using the cricket location-support system,"One of the most interesting applications of sensor networks is the localization and tracking of moving objects. The cricket location-support system (CLS), originally developed at MIT and now distributed by Crossbow Technologies, tracks a moving object using time-difference-of-arrival (TDOA) between radio frequency (RF) and ultrasonic signals. In this work we evaluate the ability of CLS to track people with various levels of mobility within a 2-dimensional space. The results presented here are preliminary results in a larger effort to determine whether or not CLS can be used to track people in a smart assistive technology living space","Space technology,
Radio frequency,
Intelligent sensors,
Time difference of arrival,
RF signals,
Wheelchairs,
Hardware,
Computer science,
Application software,
Sensor systems and applications"
Obstacle avoidance with multi-objective optimization by PSO in dynamic environment,"The second order motion model is one of the fundamental questions, a mostly important object in motion planning research of mobile robots, especially in complex environment. Based on the research of the second order motion model, this paper puts forward a new method for adjusting robots to avoid obstacles in dynamic environment. A mathematical model is first established in which environmental information such as, destination of a mobile robot, velocity and direction of obstacles are considered. Secondly, a new particle swarm optimization (PSO) algorithm is used to search for solution of the multi-objective optimization problem as described in the mathematical model. Finally, by adjusting the velocity and direction of the mobile robot to avoid obstacles in real time, the robot can reach the goal safely. Simulation experiment shows that this method is better than tradition artificial potential field (APF) algorithm and its improved algorithm based on genetic algorithm for obstacle avoidance.",
Awareness of IT Control Frameworks in an Australian State Government: A Qualitative Case Study,"IT control frameworks set out best practices for IT actions, processes and monitoring within organisations, and are believed to lead to more effective IT governance. It is difficult to assess the adoption, awareness and perception of the value of the frameworks in the Australian public sector, due to the limited academic literature available. The exploratory research reported in this paper evaluated the awareness and understanding of IT control frameworks in three public sector agencies within an Australian state government, adopting a flexible definition of IT control frameworks. Comparison was made between the level of awareness and the outcomes of recent internal and external IT audits for the agencies, where available. Qualitative data were gathered from internal documentation and nine interviews, seeking IT and business-oriented perspectives. No use of formalised IT control frameworks was found, although informal approaches were noted. Awareness and understanding of IT control frameworks in the agencies appeared limited. The agencies investigated will lack motivation to derive value from utilising IT control frameworks without increased awareness of their purpose. Further research is warranted in this area, including the investigation of effective mechanisms for raising awareness of the potential of the frameworks.","Australia,
Government,
Computer aided software engineering,
Control systems,
Information systems,
Best practices,
Business,
Monitoring,
Documentation,
Legislation"
Partitioning and Pipelined Scheduling of Embedded System Using Integer Linear Programming,"In this paper, an integer linear programming (ILP) based approach is proposed for integrated hardware/software (HW/SW) partitioning and pipelined scheduling of embedded systems for multimedia applications. The ILP approach not only partitions and maps each computation task of a particular multimedia application onto a component of the heterogeneous multiprocessor architecture, but also schedules and pipelines the execution of these computation tasks while considering communication time. The objective is to minimize the total component cost and the number of pipeline stages subject to the throughput constraint on the pipelined architecture. Experiments on two real multimedia applications are used to demonstrate the effectiveness of the proposed approach",
Improving particle filter with support vector regression for efficient visual tracking,"Particle filter is a powerful visual tracking tool based on sequential Monte Carlo framework, and it needs large numbers of samples to properly approximate the posterior density of the state evolution. However, its efficiency degenerates if too many samples are applied. In this paper, an improved particle filter is proposed by integrating support vector regression into sequential Monte Carlo framework to enhance the performance of particle filter with small sample set. The proposed particle filter utilizes an SVR based re-weighting scheme to re-approximate the posterior density and avoid sample impoverishment. Firstly, a regression function is obtained by support vector regression method over the weighted sample set. Then, each sample is re-weighted via the regression function. Finally, ameliorative posterior density of the state is re-approximated to maintain the effectiveness and diversity of samples. Experimental results demonstrate that the proposed particle filter improves the efficiency of tracking system effectively and outperforms classical particle filter.","Particle filters,
Particle tracking,
Monte Carlo methods,
Computer science,
Computer vision,
Linearity,
Computational efficiency,
Degradation,
Smoothing methods,
Sampling methods"
Routing reliability analysis of segmented backup paths in mobile ad hoc networks,"Several real-time applications (e.g., video conferencing, remote control systems) demand guarantees on message delivery latency. Supporting such QoS constrained connections requires the existence of a routing mechanism which computes paths that satisfy QoS constraints. In a mobile ad hoc network, wireless links tend to fail frequently as nodes move in and out of transmission range of one another. Providing fault tolerance with QoS guarantees in such networks is challenging. An algorithm called segmented backup source routing (SBSR) provides fault tolerance with end-to-end delay as the QoS parameter. The algorithm constructs a set of delay-constrained segmented backup paths. Each such segmented backup path protects a segment of the primary path rather than the entire path. This approach has two advantages. First, one is able to identify backup paths for any selected primary path, as long as there exists a pair of node disjoint paths from source to destination (in other words, if there are two node disjoint paths, it is not required to use either of them as the primary path). This has the advantage that the primary path may be selected based on QoS considerations rather than a consideration of fault tolerance. The second significant advantage is that the connection reliability of the segmented backup path set is higher than the two disjoint paths. We design a framework to analyse the connection reliability of such a segmented backup path set.",
Optimizing collective communications on SMP clusters,"We describe a generic programming model to design collective communications on SMP clusters. The programming model utilizes shared memory for collective communications and overlapping inter-node/intra-node communications, both of which are normally platform specific approaches. Several collective communications are designed based on this model and tested on three SMP clusters of different configurations. The results show that the developed collective communications can, with proper tuning, provide significant performance improvements over existing generic implementations. For example, when broadcasting an 8 MB message our implementations outperform the vendor's MPl/spl I.bar/Bcast by 35% on an IBM SP system, 51% on a G4 cluster, and 63% on an Intel cluster, the latter two using MPICH's MPl/spl I.bar/Bcast. With all-gather operations using 8 MB messages, our implementation outperform the vendor's MPI/spl I.bar/Allgather by 75% on the IBM SP, 60% on the Intel cluster, and 48% on the G4 cluster.","Laboratories,
US Department of Energy,
Clustering algorithms,
Design optimization,
Computer science,
Communication networks,
Testing,
Broadcasting,
Pipelines,
Parallel architectures"
Markov chain-based models for missing and faulty data in MICA2 sensor motes,"We have developed Markov chain-based techniques for infield modeling the missing and faulty data for the widely used MICA2 sensor motes. These models help designers of sensor nodes and sensor networks to gain insights into the behavior of any particular sensor platform. The models also enable users of sensor networks to collect high integrity data from the deployed networks in a more efficient and reliable way. The new approach for development and validation of faults and missing data has two phases. In the first phase, we conduct exploratory analysis of data traces collected from the deployed sensor networks. In the second phase, we use the density estimation-based procedure to derive semi Markov models that best capture the patterns and statistics of missing and faulty data in the analyzed sensor data streams. We have applied the fault detection and missing data modeling procedure on light, temperature and humidity sensors on MICA2 motes in sensor networks deployed in office space and natural habitats. The technical highlight of the research presented in this paper include: (i) exploratory data analysis and studying the properties of the sensor data streams; and (ii) adoption of a new class of semi Markov-chain models for capturing and predicting missing and faulty data in actual data trace streams","Sensor phenomena and characterization,
Data analysis,
Temperature sensors,
Wireless sensor networks,
Computer science,
Phase estimation,
Statistical analysis,
Pattern analysis,
Fault detection,
Humidity"
Simultaneous scheduling of replication and computation for bioinformatics applications on the grid,"One of the first motivations of using grids comes from applications managing large data sets infield such as high energy physics or life sciences. To improve the global throughput of software environments, replicas are usually put at wisely selected sites. Moreover, computation requests have to be scheduled among the available resources. To get the best performance, scheduling and data replication have to be tightly coupled. However, there are few approaches that provide this coupling. This paper presents an algorithm that combines data management and scheduling using a steady-state approach. Our theoretical results are validated using simulation and logs from a large life science application (ACI GRID GriPPS).","Processor scheduling,
Computer applications,
Grid computing,
Bioinformatics,
Proteins,
Scheduling algorithm,
Steady-state,
Application software,
Databases,
Computer networks"
Work in progress - case study of a technological literacy and non-majors engineering course,"Since 1995, engineering faculty at Hope College have taught a course for non-engineering students called: ""Science and Technology of Everyday Life"". The course examines the science and engineering underlying modern consumer technological devices. Distinguishing features are study of a broad sample of familiar technological devices, construction by students of working devices, and writing assignments on technological topics. For over nine years, the total enrollment of more than 1000 students has averaged 60% women and 26% pre-service teachers. To evaluate student outcomes, the Motivated Strategies for Learning Questionnaire (MSLQ) was applied. Statistically significant increases were found in intrinsic motivation, task value, and self-efficacy. A decrease in test anxiety was also found. The results are consistent across all semesters analyzed. The case study shows that non-engineering students can have increased motivation for learning science and technology, increased perceived value for science and technology, increased self-confidence about learning science and technology","Computer aided software engineering,
Laboratories,
Educational institutions,
Educational technology,
Education,
Springs,
Writing,
Testing,
Councils,
History"
A genetic algorithm for test-suite reduction,"In order to reduce the cost of regression testing, researchers have investigated the use of test-suite reduction techniques, which identify a reduced test suite that provides the same coverage of the software according to some criterion as the original test suite. Existing test-suite reduction techniques consider test-case coverage criteria, other criteria such as risk or fault-detection effectiveness, or combinations of these criteria, but ignore the test-execution cost because of the complexity. Firstly, this paper presents a mathematical model for this test-suite reduction problem and transforms it into a linear integer-programming form. Then the paper investigates the use of an evolutionary approach, called genetic algorithm, for this test-suite reduction problem. Unlike other algorithms, our algorithm uses a new criteria, which is a combination of a block based coverage criteria and a test-execution cost criteria, to make decisions about reducing a test suite. Finally, the paper presents the results of the empirical studies of our algorithm. The studies show that our algorithm can significantly reduce the size and the cost of the test-suite for regression testing, and the test-execution cost is one of the most important features that must be taken into consideration for test-suite reduction.","Genetic algorithms,
Software testing,
Costs,
Software maintenance,
Educational institutions,
Computer science,
Information management,
Finance,
Software algorithms,
Helium"
Collaboration in Context-Aware Mobile Phone Applications,"Context-aware applications are expected to become a remarkable application area within future mobile computing. As mobile phones form a natural tool for interaction between people, the influence of the current context on collaboration is desirable to take into account to enhance the efficiency and quality of the interaction. This paper presents role of context information in improving the collaboration of mobile communication by supplying relevant information to the cooperating parties, one being a mobile terminal user and the other either another person, group of people, or a mobile service provider.","Collaboration,
Mobile handsets,
Context,
Mobile communication,
Mobile computing,
Calendars,
Collaborative work,
Handheld computers,
Writing,
Collaborative tools"
Automatic Diagnosis of Performance Problems in Database Management Systems,"Database performance is directly linked to database management system (DBMS) resource allocation. Complex relationships between resources make problem diagnosis and performance tuning difficult, time-consuming tasks. Database administrators (DBAs) currently tune and re-tune the DBMS as the database grows and workloads change. Increased performance and reduced cost of ownership can be achieved by automating the tuning process, starting with resource allocation problem diagnosis. In this paper we overview an automatic diagnosis framework designed to diagnose resource problems. Our diagnosis model and results demonstrate an ability to correctly identify system bottlenecks for a generic on-line transaction processing workload","Database systems,
Resource management,
Testing,
Costs,
Automation,
Computer science,
Transaction databases,
Humans,
Memory management,
Feedback control"
A Communication-Induced Checkpointing and Asynchronous Recovery Protocol for Mobile Computing Systems,"Mobile computing systems have many constraints such as low battery power, low bandwidth , high mobility and lack of stable storage which are not presented in static distributed systems. In this paper, we propose an efficient communication-induced checkpointing protocol for mobile computing systems. We also propose an asynchronous recovery protocol based on the checkpointing protocol. Mobile support stations control major parts of the checkpointing and recovery such as storing and tracing the checkpoints, requesting rollback and logging messages, so that mobile hosts do not incur much overhead. The recovery algorithm has no domino effect and a failed process needs to roll back to its latest checkpoint and request only a subset of the processes to rollback to a consistent checkpoint. Our recovery protocol uses selective message logging at the mobile support station to handle the messages lost due to rollback.",
Reverse engineering goal models from legacy code,"A reverse engineering process aims at reconstructing high-level abstractions from source code. This paper presents a novel reverse engineering methodology for recovering stakeholder goal models from both structured and unstructured legacy code. The methodology consists of the following major steps: 1) Refactoring source code by extracting methods based on comments; 2) Converting the refactored code into an abstract structured program through statechart refactoring and hammock graph construction; 3) Extracting a goal model from the structured program's abstract syntax tree; 4) Identifying nonfunctional requirements and derive soft goals based on the traceability between the code and the goal model. To illustrate this requirements recovery process, we refactor stakeholder goal models from two legacy software code bases: an unstructured Web-based email in PHP (SquirrelMail) and a structured email client system in Java (Columba).","Reverse engineering,
Computer architecture,
Service oriented architecture,
Computer science,
Software systems,
Tree graphs,
Electronic mail,
Java,
Design engineering,
Proposals"
Reactive systems over cospans,"The theory of reactive systems, introduced by Leifer and Milner and previously extended by the authors, allows the derivation of well-behaved labelled transition systems (LTS) for semantic models with an underlying reduction semantics. The derivation procedure requires the presence of certain colimits (or, more usually and generally, bicolimits) which need to be constructed separately within each model. In this paper, we offer a general construction of such bicolimits in a class of bicategones of cospans. The construction sheds light on as well as extends Ehrig and Konig's rewriting via borrowed contexts and opens the way to a unified treatment of several applications.","Computer science,
Bipartite graph,
Concurrent computing,
Computer security,
National security,
Mobile computing,
Computer architecture,
Algebra,
Logic"
Imaging Breathing Rate in the CO2Absorption Band,"Following up on our previous work, we have developed one more non-contact method to measure human breathing rate. We have retrofitted our mid-wave infra-red (MWIR) imaging system with a narrow band-pass filter in the CO2 absorption band (4.3 mum). This improves the contrast between the foreground (i.e., expired air) and background (e.g., wall). Based on the radiation information within the breath flow region, we get the mean dynamic thermal signal. This signal is quasi-periodic due to the interleaving of high and low intensities corresponding to expirations and inspirations respectively. We sample the signal at a constant rate and then determine the breathing frequency through Fourier analysis. We have performed experiments on 9 subjects at distances ranging from 6 - 8 ft. We compared the breathing rate computed by our novel method with ground-truth measurements obtained via a traditional contact device (PowerLab/4SP from ADInstruments with an abdominal transducer). The results show high correlation between the two modalities. For the first time, we report a Fourier based breathing rate computation method on a MWIR signal in the CO2 absorption band. The method opens the way for desktop, unobtrusive monitoring of an important vital sign, that is, breathing rate. It may find widespread applications in preventive medicine as well as sustained physiological monitoring of subjects suffering from chronic ailments",
"Ludics nets, a game model of concurrent interaction","We introduce L-nets as a game model of concurrent interaction. L-nets, which correspond to strategies (in Games Semantics) or designs (in Ludics), are graphs, and the interactions (plays) result into partial orders, hence allowing for parallelism.","Tree graphs,
Logic,
Parallel processing,
Concurrent computing,
Calculus,
Additives,
Computational modeling,
Testing,
Bridges,
Constraint theory"
Mining emerging patterns and classification in data streams,"A data stream model has been proposed recently for those data intensive applications such as financial applications, manufacturing, and others (Babcock et al., 2002). In this model, data arrives in multiple, continuous, rapid, time-varying data streams. These characteristics make it infeasible for traditional classification and mining techniques to deal with data streams. In this paper, we propose a novel method for mining emerging patterns (EPs) in data streams. Moreover, we show how these EPs can be used to classify data streams. EPs (Dong and Li, 1999) are those itemsets whose supports in one class are significantly higher than their supports in the other classes. The experimental evaluation shows that our proposed method can achieve up to 10% increase in accuracy compared to the other methods.","Itemsets,
Sampling methods,
Application software,
Computer science,
Software engineering,
Computer aided manufacturing,
Virtual manufacturing,
Machine learning,
Machine learning algorithms,
Training data"
A progressive register allocator for irregular architectures,"Register allocation is one of the most important optimizations a compiler performs. Conventional graph-coloring based register allocators are fast and do well on regular, RISC-like, architectures, but perform poorly on irregular, CISC-like, architectures with few registers and non-orthogonal instruction sets. At the other extreme, optimal register allocators based on integer linear programming are capable of fully modeling and exploiting the peculiarities of irregular architectures but do not scale well. We introduce the idea of a progressive allocator. A progressive allocator finds an initial allocation of quality comparable to a conventional allocator, but as more time is allowed for computation the quality of the allocation approaches optimal. This paper presents a progressive register allocator which uses a multi-commodity network flow model to elegantly represent the intricacies of irregular architectures. We evaluate our allocator as a substitute for gcc 's local register allocation pass.","Registers,
Computer architecture,
Optimizing compilers,
Computer science,
Instruction sets,
Integer linear programming,
Thumb,
National electric code,
Cost function,
Sections"
BlueBot: asset tracking via robotic location crawling,"From manufacturers, distributors, and retailers of consumer goods to government departments, enterprises of all kinds are gearing up to use RFID technology to increase the visibility of goods and assets within their supply chain and on their premises. However, RFID technology alone lacks the capability to track the location of items once they are moved within a facility. We present a prototype automatic location sensing system that combines RFID technology and off-the-shelf Wi-Fi based continuous positioning technology for tracking RFID-tagged assets. Our prototype system employs a robot, with an attached RFID reader, which periodically surveys the space, associating items it detects with its own location determined with the Wi-Fi positioning system. We propose four algorithms that combine the detected tag's reading with previous samples to refine its location. Our experiments have shown that our positioning algorithms can bring a two to three fold improvement over the positional accuracy limitations in both the RFID reader and the positioning technology.","Robots,
Radiofrequency identification,
Books,
Supply chains,
Space technology,
Libraries,
Government,
Global Positioning System,
Satellite broadcasting,
Computer science"
An empirical user-based study of text drawing styles and outdoor background textures for augmented reality,"A challenge in presenting augmenting information in outdoor augmented reality (AR) settings lies in the broad range of uncontrollable environmental conditions that may be present, specifically large-scale fluctuations in natural lighting and wide variations in likely backgrounds or objects in the scene. In this paper, we present a user-based study which examined the effects of outdoor background textures, changing outdoor illuminance values, and text drawing styles on user performance of a text identification task with an optical, see-through augmented reality system. We report significant effects for all of these variables, and discuss design guidelines and ideas for future work.",
Modeling Protocol Offload for Message-oriented Communication,"In this paper, we present a new, conceptual model that captures the benefits of protocol offload in the context of high performance computing systems. In contrast to the LAWS model, the extensible message-oriented offload model (EMO) emphasizes communication in terms of messages rather than flows. In contrast to the LogP model, EMO emphasizes the performance of the network protocol rather than the parallel algorithm. The extensible message-oriented offload model allows protocol developers to consider the tradeoffs and specifics associated with offloading protocol processing including the reduction in message latency along with benefits associated with reduction in overhead and improvements to throughput. We give an overview of the EMO model and show how our model can be mapped to the LAWS and LogP models. We also show how it can be used to analyze individual messages within TCP flows by contrasting full offload (TCP offload engines) with other approaches, e.g., interrupt coalescing and splintered TCP","Protocols,
High performance computing,
Delay,
Parallel algorithms,
Ethernet networks,
Context modeling,
Computer science,
Concurrent computing,
Throughput,
Performance analysis"
GUARD: gossip used for autonomous resource detection,"A growing trend in the development and deployment of grid computing systems is decentralization. Decentralizing these systems helps make the more scalable and robust, but poses several challenges. In this paper we address one such problem - that of locating computing resources meeting specified requirements in a large scale heterogeneous system. The heterogeneous and dynamic nature, coupled with the multiple occurrences of these resources, makes the problem distinct from traditional data location problems found in the context of content-sharing systems. We propose GUARD (gossip used for autonomous resource detection), a protocol that uses gossiping between neighbors to propagate the current knowledge of distances from available resources. GUARD is autonomous (all decisions are made locally, using knowledge based only on interaction with immediate neighbors) and does not make any assumptions about the underlying network topology. Our simulations show GUARD is more efficient than other techniques such as random routing, history-based routing and frequency-based routing that have been used for similar purposes. We also show how GUARD can be modified to locate multiple categories of resources meeting multiple criteria.","Protocols,
Routing,
Scalability,
Centralized control,
Computer science,
Grid computing,
Robustness,
Large-scale systems,
Network topology,
Frequency"
Zippy - a coarse-grained reconfigurable array with support for hardware virtualization,"This paper motivates the use of hardware visualization on coarse-grained reconfigurable architectures. We introduce Zippy, a coarse-grained multi-context hybrid CPU with architectural support for efficient hardware virtualization. The architectural details and the corresponding tool flow are outlined. As a case study, we compare the non-virtualized and the virtualized execution of an ADPCM decoder.","Platform virtualization,
Hardware,
Virtual machining,
Reconfigurable architectures,
Computer architecture,
Application virtualization,
Concrete,
Computer networks,
Computer science,
Decoding"
Modification of the ART-1 architecture based on category theoretic design principles,"Many studies have addressed the knowledge representation capability of neural networks. A recently-developed mathematical semantic theory explains the relationship between knowledge and its representation in connectionist systems. The theory yields design principles for neural networks whose behavioral repertoire expresses any desired capability that can be expressed logically. In this paper, we show how the design principle of limit formation can he applied to modify the ART-1 architecture, yielding a discrimination capability that goes beyond vigilance. Simulations of this new design illustrate the increased discrimination ability it provides for multi-spectral image analysis.","Neural networks,
Knowledge representation,
Electronic mail,
Multispectral imaging,
Image analysis,
Mathematical model,
Computer architecture,
Computer science,
Computer networks,
Design engineering"
Bridging organizational network boundaries on the grid,"The grid offers significant opportunities for performing wide area distributed computing, allowing multiple organizations to collaborate and build dynamic and flexible virtual organisations. However, existing security firewalls often diminish the level of collaboration that is possible, and current grid middleware often assumes that there are no restrictions on the type of communication that is allowed. Accordingly, a number of collaborations have failed because the member sites have different and conflicting security policies. In this paper we present an architecture that facilitates inter-organization communication using existing grid middleware, without compromising the security policies in place at each of the participating sites. Our solutions are built on a number of standard secure communication protocols such as SSH and SOCKS. We call this architecture Remus, and will demonstrate its effectiveness using the Nimrod/G tools.",
Neural network classifiers for language identification using phonotactic and prosodic features,"In this paper, we explore phonotactic and prosodic features derived from the speech signal and its transcription for identification of a language. The characteristics of languages represented by phonotactic and prosodic features at the trisyllabic level are used to train feedforward neural network (FFNN) classifiers to discriminate among languages. We demonstrate that these features indeed contain language-specific information. We also show that phonotactic features in terms of broad phonetic categories are sufficient to represent the phonotactic regularities/constraints of languages. The performance of the FFNN classifier based on these features is evaluated for three Indian languages.",
On the complexity of two-player win-lose games,"The efficient computation of Nash equilibria is one of the most formidable challenges in computational complexity today. The problem remains open for two-player games. We show that the complexity of two-player Nash equilibria is unchanged when all outcomes are restricted to be 0 or 1. That is, win-or-lose games are as complex as the general case for two-player games.",
Interactive implicit modeling with hierarchical spatial caching,"Complex implicit CSG models can be represented hierarchically as a tree of nodes (the BlobTree) . However, current methods cannot be used to visualize changes made to these models at interactive rates due to the large number of potential field evaluations required. A hierarchical spatial caching technique is presented which accelerates evaluations of the potential function. This method introduces the concept of a caching node inserted into the implicit model tree. Caching nodes store exact potential field values at the vertices of a voxel grid and rely on tri-linear and tri-quadratic reconstruction filters to locally approximate the potential field of a sub-tree. A lazy evaluation scheme is used to avoid expensive pre-computation. Polygonization timings with and without caching are compared for a complex model undergoing manipulation in an interactive modeling tool. An order-of-magnitude improvement in visualization time is achieved for complex implicit models containing thousands of primitives.","Visualization,
Computer science,
Filters,
Assembly,
Feedback,
Shape,
Acceleration,
Timing,
Costs,
Surface reconstruction"
Adding confidentiality to application-level multicast by leveraging the multicast overlay,"While scalability, routing and performance are core issues for application-level multicast (ALM) protocols, an important but less studied problem is security. In particular, confidentiality (i.e. data secrecy, achieved through data encryption) in ALM protocols is needed. Key management schemes must be simple, scalable, and must not degrade the performance of the ALM protocol. We explore three key management schemes that leverage the underlying overlay to distribute the key(s) and secure ALM. We evaluate their impact on three well-known ALM protocols: ESM, ALMI and NICE. Through analysis and simulations, we show that utilizing the ALM overlay to distribute key(s) is feasible. For a given ALM protocol, choice of the best key management scheme depends on the application needs: minimizing rekeying latency or minimizing data multicasting latency.","Multicast protocols,
Data security,
Delay,
National security,
Analytical models,
Peer to peer computing,
Computer science,
Application software,
Scalability,
Routing protocols"
Using Fuzzy Logic and Entropy Theory to Risk Assessment of the Information Security,"In the previous research of the risk assessment, AHP method and Fuzzy logical method used are obvious subjectivity and limitation. In this paper, AHP method and Fuzzy logical method are improved, the formula of risk degree and entropy-weight coefficient are put forward to the estimation of the information security. Firstly, the hierarchy structure of the risk assessment is constructed and the method of fuzzy comprehensive judgment is improved according to the actual condition of the information security. Secondly, the risk degree put forward is likelihood estimation of the risk probability, the risk impact severity and risk uncontrollability. Finally, for the determination of the weight vector of the risk factor, a method of entropy-weight coefficient is applied to objective computation, and subjective judgment is overcome. The study of the case shows that the method can be easily used to the risk assessment of the information security and its results conform to the actual situation.",
The Automatic Generation of Basis Set of Path for Path Testing,"Basis set of path is consisted of some of the programâ€™s paths. The automatic generation method of basis set of path is discussed in this paper. It is built by searching the control flow graph of a program by depth-first searching method. In order to avoiding that the algorithm will never stop and reducing the searching procedure, the sub-path from the multi-indegree nodes to the end node of a program and the sub-path that contains a loop is recorded during the construction of a basis path. Some new basis paths can be constructed by merging these two kinds of sub-paths.","Automatic testing,
Computers,
Software testing,
Agriculture,
Automatic control,
Flow graphs,
Merging,
Genetic algorithms"
Reasoning about software architecture-based regression testing through a case study,"Two main issues need to be covered when dealing with the dependability of component-based systems: quality assurance of reusable software components and quality assurance of the assembled component-based system. By focussing on the assembly, a software architecture specification of a component-based system allows to explicitly model the structure and required system behavior by specifying how components and connectors are intended to interact. Software architecture-based conformance testing techniques can yield confidence on the implementation conformance to expected structural and behavioral properties as specified in the architectural models. In this paper we explore software architecture-based regression testing methods that enable reuse of earlier saved results to test if a different assembly of components conforms to the evolved software architecture. The approach is presented through a running example.","Software testing,
Computer aided software engineering,
Assembly systems,
System testing,
Software architecture,
Connectors,
Quality assurance,
Computer science,
Software quality,
Software reusability"
Distributed reasoning based on problem solver markup language (PSML): a demonstration through extended OWL,"Since the World Wide Web is enlarging its scale, users cannot find and utilize information easily. Hence problem-solving systems in the Web environment are required. The core of such systems is the problem solver markup language (PSML) and PSML-based distributed Web inference engines. In this paper, we demonstrate a possible implementation of certain distributed reasoning capabilities as required in the future PSML. In particular, our proposed implementation, called /spl beta/-PSML, is based on the combination of OWL (Web ontology language) with Horn clauses. From the viewpoint of expressive power, the proposed /spl beta/-PSML can represent multi-argument relation that is an extension of the OWL capability, and models domains with a rich hierarchical structure for Horn clauses. Furthermore, we discuss how to extend the /spl beta/-PSML for solving problems in a large-scale distributed Web environment.","Markup languages,
OWL,
Portals,
Semantic Web,
Computer science,
Web sites,
Engines,
Web services,
Problem-solving,
Logic"
Tracking non-stationary optimal solution by particle swarm optimizer,"In the real world, we have to frequently deal with searching for and tracking an optimal solution in a dynamic environment. This demands that the algorithm not only find the optimal solution but also track the trajectory of the solution in a dynamic environment. Particle swarm optimization (PSO) is a population-based stochastic optimization technique, which can find an optimal, or near optimal, solution to a numerical and qualitative problem. However, the traditional PSO algorithm lacks the ability to track the optimal solution in a dynamic environment. In this paper, we present a modified PSO algorithm that can be used for tracking a non-stationary optimal solution in a dynamically changing environment.",
A DCT-domain system for hiding fractal compressed images,"In this paper, we propose an approach for hiding a secret image in a cover image. In the beginning, the fractal image compression method is used to compress the secret image, and then we encrypt this compressed data by DES. Finally, we embed the encrypted data into the middle-frequency domain of DCT. After embedding the secret image, the goal of steganography can be successfully achieved.","Fractals,
Image coding,
Steganography,
Frequency domain analysis,
Discrete cosine transforms,
Computer science,
Cryptography,
Data encapsulation,
Robustness,
Computer science education"
New e-learning tools for DC-DC converters,"The paper gives an overview of the Yoto project, a novel e-learning initiative to build up an electrical engineering knowledge base. It also presents a new e-module of the project relating to the steady-state operation of DC-DC converters. The multimedia rich module contains numerous interactive tools promoting the high level education in the academy sector and the vocational trainings in the industry. The interactive tools also highly support the circuit design process by enabling fast calculation and access to relevant circuit quantities and waveforms (voltages and currents) in steady-state without long simulations","Electronic learning,
DC-DC power converters,
Humans,
Steady-state,
Laboratories,
Informatics,
Automation,
Business,
Computer science education,
Electrical engineering"
Haptic-assisted guidance system for navigating volumetric data sets,"In this paper, a new approach for navigation assistance through sets of volumetric data is presented. This innovative method uses force feedback in two dimensions to guide the user to a region of interest. The haptic feedback is calculated under two methods and we explain the implementation of both approaches. The output is displayed by the proactive desk, a two dimensional force feedback device developed at the Advanced Telecommunications Research Institute. This new approach is targeted to scientists who work with sets of volumetric data and who wish to find information using visual scanning enhanced with haptics. Our system is designed for magnetic resonance images (MRI) overlaid with functional MRI.","Navigation,
Haptic interfaces,
Magnetic resonance imaging,
Force feedback,
Data visualization,
Virtual environment,
Computer science,
Information science,
Laboratories,
Magnetic resonance"
An efficient reconstruction method for nonuniform attenuation compensation in nonparallel beam geometries based on Novikov's explicit inversion formula,"This paper investigates an accurate reconstruction method to invert the attenuated Radon transform in nonparallel beam (NPB) geometries. The reconstruction method contains three major steps: 1) performing one-dimensional phase-shift rebinning; 2) implementing nonuniform Hilbert transform; and 3) applying Novikov's explicit inversion formula. The method seems to be adaptive to different settings of fan-beam geometry from very long to very short focal lengths without sacrificing reconstruction accuracy. Compared to the conventional bilinear rebinning technique, the presented method showed a better spatial resolution, as measured by modulation transfer function. Numerical experiments demonstrated its computational efficiency and stability to different levels of Poisson noise. Even with complicated geometries such as varying focal-length and asymmetrical fan-beam collimation, the presented method achieved nearly the same reconstruction quality of parallel-beam geometry. This effort can facilitate quantitative reconstruction of single photon emission computed tomography for cardiac imaging, which may need NPB collimation geometries and require high computational efficiency.","Reconstruction algorithms,
Attenuation,
Geometry,
Image reconstruction,
Computational efficiency,
Spatial resolution,
Transfer functions,
Stability,
Noise level,
Collimators"
Reliability-aware Checkpoint/Restart Scheme: A Performability Trade-off,"In previous years, large scale clusters have been commonly deployed to solve important grand-challenge scientific problems. In order to reduce computational time, the system size has been increasingly expanded. Unfortunately, the reliability of such cluster systems goes in the opposite direction, as the extension of a system scale. Since failures of a single node could result in a system outage, it is essential to effectively deal with faulty situations in the grand challenge problem-solving environment. Checkpointing is one of common fault tolerance techniques. However, there are many challenges in checkpointing such as overhead, latency and consistency, as well as recovery. In this paper, a reliability-aware checkpoint/restart method was introduced. It is a novel technique to consider checkpointing placement based on system reliability. We constructed a cost model and derived an optimal checkpoint placement function based on failure rates: A trade-off between performance and reliability (i.e. performability) was a key consideration. We also implemented a proof-of-concept and demonstrated improvements resulting from our techniques for fault-tolerant MPI applications on an HA-OSCAR cluster","Checkpointing,
Availability,
Large-scale systems,
Fault tolerance,
Runtime environment,
Redundancy,
Contracts,
Open source software,
Resilience,
Message passing"
Comparison between digital and analog pulse shape discrimination techniques for neutron and gamma ray separation,"Recent advancements in digital signal processing (DSP) using fast processors and a computer allows one to envision using it in pulse shape discrimination. In this study, we have investigated the feasibility of using a DSP to distinguish between neutrons and gamma rays by the shape of their pulses in a liquid scintillator detector (BC501). For neutron/gamma discrimination, the advantage of using a DSP over the analog method is that in an analog system, two separate charge-sensitive ADCs are required. One ADC is used to integrate the beginning of the pulse rise time while the second ADC is for integrating the tail part. In DSP techniques the incoming pulses coming directly from the detector are immediately digitized and can be decomposed into individual pulses waveforms. This eliminates the need for separate ADCs as one can easily get the integration of two parts of the pulse from the digital waveforms. This work describes the performance of these DSP techniques and compares the results with the analog method.","Pulse shaping methods,
Shape,
Neutrons,
Digital signal processing,
Signal processing,
Gamma ray detection,
Gamma ray detectors,
Gamma rays,
Preamplifiers,
Telephony"
Modeling and Simulation of DC-DC Power Converters in CCM and DCM Using the Switching Functions Approach: Application to the Buck and C&#249;k Converters,"In this paper, new mathematical models of DC-DC switch-mode converters, which are valid under both continuous and discontinuous operating modes, are presented. The modeling technique is based on the switching functions concept. The obtained models are represented by time-variant state equations, which offer high simplicity for computer implementation. For illustration purpose, the proposed modeling approach is applied to the conventional buck and CÃ¹k converters. The obtained models are then tested through computer implementation using the Simulink tool of Matlab.","DC-DC power converters,
Switching converters,
Mathematical model,
Switching circuits,
Static power converters,
Switches,
Buck converters,
Frequency conversion,
Electromagnetic analysis,
Numerical models"
Combining statistical similarity measures for automatic induction of semantic classes,"In this paper, an unsupervised semantic class induction algorithm is proposed that is based on the principle that similarity of context implies similarity of meaning. Two semantic similarity metrics that are variations of the vector product distance are used in order to measure the semantic distance between words and to automatically generate semantic classes. The first metric computes ""wide-context"" similarity between words using a ""bag-of-words"" model, while the second metric computes ""narrow-context"" similarity using a bigram language model. A hybrid metric that is defined as the linear combination of the wide and narrow-context metrics is also proposed and evaluated. To cluster words into semantic classes an iterative clustering algorithm is used. The semantic metrics are evaluated on two corpora: a semantically heterogeneous Web news domain (HR-Net) and an application-specific travel reservation corpus (ATIS). For the hybrid metric, semantic class member precision of 85% is achieved at 17% recall for the HR-Net task and precision of 85% is achieved at 55% recall for the ATIS task","Natural languages,
Natural language processing,
Application software,
Speech,
Data mining,
Ontologies,
Computer science,
Induction generators,
Clustering algorithms,
Iterative algorithms"
An experiential course in wireless networks and mobile systems,"In spring 2005, for the third time, the electrical and computer engineering faculty and computer science faculty at Virginia Tech are teaching the course Wireless Networks and Mobile Systems, ECE/CS 4570. This course has some particularly interesting features. It offers integrated coverage of a broad set of topics that we would normally find in separate courses, often in different departments. It gives undergraduate students access to cutting-edge topics usually reserved for graduate courses. It also departs from the traditional lecture format to combine lectures with moderated laboratory sessions and at-home exercises and design projects. In this paper, we describe an innovative, senior- and graduate-level course on wireless networks and mobile systems. We discuss the course's history and guiding principles, the laboratory exercises and design projects, and outcomes, including dissemination activities.","Intelligent networks,
Wireless networks,
Springs,
Laboratories,
Education,
Proposals,
History,
Mobile computing,
Collaboration,
Local area networks"
Generative technique of version control systems for software diagrams,"In iterative software development methodology, a version control system is used in order to record and manage modification histories of products such as source codes and models described in diagrams. However, conventional version control systems cannot manage the models in a logical unit because the systems mainly handle with source codes. In this paper, we propose a technique of version control based in a logical unit for models described in diagrams. Then we illustrate the feasibility of our approach with the implementation of version control functions on a meta-CASE tool that is able to generate a modeling tool in order to deal with various diagrams.","Control systems,
Software systems,
Programming,
Control system synthesis,
Computer aided software engineering,
Unified modeling language,
XML,
Computer science,
Iterative methods,
Software development management"
De-identification algorithm for free-text nursing notes,"All personally identifiable information must be removed from patient medical records before the data can be shared with other researchers. We present an automated method of removing protected health information (PHI) from free-text nursing notes taken from a U.S. hospital. We have previously shown that one clinician can locate PHI in nursing notes with an average sensitivity of 0.81, and for teams of two clinicians the sensitivity is 0.94. Our method uses lexical look-up tables, regular expressions, and simple heuristics to locate PHI with an overall sensitivity of 0.92 (0.98 for names, 0.96 for dates), which is significantly better than the average sensitivity of a single human. The algorithm has a positive predictive value of only 0.44, so additional software was developed to allow the user to review the terms identified as PHI and manually eliminate false positives. The algorithm is open-source and will be made freely available on PhysioNet together with a re-identified corpus of nursing notes","Medical services,
Protection,
Hospitals,
Open source software,
Terminology,
Roentgenium,
Computer science,
Humans,
Software algorithms,
Guidelines"
Understanding protocol performance and robustness of ad hoc networks through structural analysis,"There has recently been renewed interest among various research communities in understanding the structure of social and infrastructure networks. Motivated by this line of research, we conduct an in-depth structural analysis of large ad hoc networks derived by placing nodes randomly as well as by placing them in realistic urban environments, a scenario that is rapidly gaining interest (K. Jain, et al, 2003). We use structural analysis in two illustrative settings. First, we use it to study the performance of network protocols. Our results indicate that structural analysis of interference graphs that model ad hoc networks can yield a good first order prediction of the overall protocol performance. Second, we study the robustness of a network to random node and edge failures. This study is important in the context of ad hoc networks wherein one expects nodes/edges to fail due various natural or system dependent reasons. The experimental results presented in this paper show the following: (i) structural properties of ad hoc networks depend crucially on the spatial distribution of the nodes. (ii) Structural properties of the network significantly affect the performance of protocols. (iii) Graph theoretic measures can provide good first order insights into the network protocol performance. (iv) The measures are also useful in characterizing the robustness of such networks.","Protocols,
Robustness,
Ad hoc networks,
Performance analysis,
Transceivers,
Interference,
Computer science,
Analytical models,
Social network services,
Wireless networks"
A simple improved distributed algorithm for minimum CDS in unit disk graphs,"Several routing schemes in ad hoc networks first establish a virtual backbone and then route messages via back-bone nodes. One common way of constructing such a backbone is based on the construction of a minimum connected dominating set (CDS). In this paper we present a very simple distributed algorithm for computing a small CDS. Our algorithm has an approximation factor of at most 6.91, improving upon the previous best known approximation factor of 8 due to Wan et al. [INFOCOM'02], The improvement relies on a refined analysis of the relationship between the size of a maximal independent set and a minimum CDS in a unit disk graph. This subresult also implies improved approximation factors for many existing algorithm.","Distributed algorithms,
Spine,
Interference,
Ad hoc networks,
Mobile ad hoc networks,
Computer science,
Electronic mail,
Systems engineering and theory,
Routing,
Distributed computing"
Classifying the multiplicity of the EEG source models using sphere-shaped support vector Machines,"Support vector machines (SVMs) are learning algorithms derived from statistical learning theory, and originally designed to solve binary classification problems. How to effectively extend SVMs for multiclass classification problems is still an ongoing research issue. In this paper, a sphere-shaped SVM for multiclass problems is presented. Compared with the classical plane-shaped SVMs, the number of convex quadratic programming problems and the number of variables in each programming are smaller. Such SVM classifier is applied to the electroencephalogram (EEG) source localization problem, and the multiplicity of source models is determined according to the potentials recorded on the scalp. Experimental results indicate that the sphere-shaped SVM based classifier is an effective and promising approach for this task.","Electroencephalography,
Brain modeling,
Support vector machines,
Support vector machine classification,
Scalp,
Quadratic programming,
Predictive models,
Electromagnetic modeling,
Computer science,
Electromagnetic fields"
Analytic calibration of cone-beam scanners,"We are investigating a direct analytic method of determining all geometric calibration parameters for a cone-beam scanner. Each projection is independently calibrated using a common calibration object that remains fixed in the laboratory frame. We make no assumptions or restrictions on the path of the cone vertex (e.g. the X-ray source) or the movement of the detector but we do assume a known field-of-view common to all views. The method uses a calibration object consisting of 30 small balls lying on 3 orthogonal concentric circles and one additional ball at the center of the calibration object. From parametrizations of the 3 image ellipses it is possible to analytically obtain expressions for all the calibration parameters. Using simulations we present our first results with this method, and by adding uncertainty to the locations of the projected balls, we determine the stability of the method. Our ultimate goal is to develop a completely automated calibration procedure requiring no user intervention whatsoever, and providing a seamless link to the reconstruction procedure","Calibration,
Laboratories,
Detectors,
Equations,
Object detection,
Computer vision,
Nuclear and plasma sciences,
Cameras,
Image converters,
Collimators"
Introducing agile into a software development Capstone project,"Conveying principles of software analysis, design and implementation in a classroom setting is problematic. When the course involves actual hands on development with clients drawn from industry, the challenges are magnified. This paper discusses the experiences and observations of a set of 10 month independent external projects undertaken by final year students in the computer systems technology program using agile for the first time. We compare situations and observations of projects developed following an agile approach with XP programming by K. Beck in ""embracing change with extreme programming"" (1999), to our previous projects developed in a traditional approach. Based on these observations, an agile approach seems to support learning, provide a valuable practical experience and produce useable software within an academic environment.","Application software,
Prototypes,
Kelvin,
Software design,
Programming profession,
Information analysis,
Information technology,
Educational programs,
Software prototyping,
Graphical user interfaces"
Benchmark measurements of current UPC platforms,"UPC is a parallel programming language based on the concept of partitioned shared memory. There are now several UPC compilers available and several different parallel architectures that support one or more of these compilers. This paper is the first to compare the performance of most of the currently available UPC implementations on several commonly used parallel platforms. These compilers are the GASNet UPC compiler from UC Berkeley, the v1.1 MuPC compiler from Michigan Tech, the Hewlet-Packard v2.2 compiler, and the Intrepid UPC compiler. The parallel architectures used in this study are a 16-node x86 Myrinet cluster, a 31-processor AlphaServer SC-40, and a 48-processor Cray T3E. A STREAM-like microbenchmark was developed to measure fine- and course-grained shared-memory accesses. Also measured are five NPB kernels using existing UPC implementations. These measurements and associated observations provide a snapshot of the relative performance of current UPC platforms.","Current measurement,
Yarn,
Parallel programming,
Parallel architectures,
Computer science,
Kernel,
Computer languages,
Government,
Linux,
ISO standards"
Two Cache Replacement Algorithms Based on Association Rules and Markov Models,"In this paper, two cache replacement algorithms are presented. One is based on association rules, in which we extend the LRU replacement algorithm by making it sensible to Web access models extracted from Web log data using Web mining techniques. The other one is based on Markov models, in which we improve the LRU replacement algorithm by applying Markov models.",
Query Routing: Finding Ways in the Maze of the DeepWeb,"This paper presents a source selection system based on attribute co-occurrence framework for ranking and selecting Deep Web sources that provide information relevant to users requirement. Given the huge number of heterogeneous Deep Web data sources, the end users may not know the sources that can satisfy their information needs. Selecting and ranking sources in relevance to the user requirements is challenging. Our system finds appropriate sources for such users by allowing them to input just an imprecise initial query. As a key insight, we observe that the semantics and relationships between deep Web sources are self-revealing through their query interfaces, and in essence, through the co-occurrences between attributes. Based on this insight, we design a co-occurrence based attribute graph for capturing the relevances of attributes, and using them in ranking of sources in the order of relevance to userâ€™s requirement. Further, we present an iterative algorithm that realizes our model. Our preliminary evaluation on real-world sources demonstrates the effectiveness of our approach.","Query processing,
Books,
Computer science,
Iterative algorithms,
HTML,
Large scale integration,
Uniform resource locators,
Educational institutions,
Conferences,
Information retrieval"
Robot Workspace Optimization Based on a Novel Local and Global Performance Indices,,"Manipulators,
Robot kinematics,
Orbital robotics,
Service robots,
Jacobian matrices,
Performance analysis,
Shape,
Computational geometry,
Optimization methods,
Computer science education"
Complex 3D shape recovery using a dual-space approach,"This paper presents a novel method for reconstructing complex 3D objects with unknown topology using silhouettes extracted from image sequences. This method exploits the duality principle governing surface points and their corresponding tangent planes, and enables a direct estimation of points on the contour generators. A major problem in other related works concerns with the search for a tangent basis at singularities in the dual tangent space. This problem is addressed here by utilizing the epipolar parameterization for identifying a well-defined basis at each point, and thus avoids any form of search. For the degenerate cases where epipolar parameterization breaks up, a fast on-the-fly validation is performed for each computed surface point, which consequently leads to a significant improvement in robustness. As the resulting contour generator points are not suitable for direct triangulation, a topologically correct surface extracting method based on slicing plane is presented. Both experiments on synthetic and real world data show that the proposed method has comparable robustness as those existing volumetric methods regarding surface of complex topology, whilst producing more accurate estimation of surface points.","Shape,
Robustness,
Surface reconstruction,
Cameras,
Topology,
Data mining,
Surface treatment,
Computer science,
Image reconstruction,
Image sequences"
Aggregating processor free time for energy reduction,"Even after carefully tuning the memory characteristics to the application properties and the processor speed, during the execution of real applications there are times when the processor stalls, waiting for data from the memory. Processor stall can be used to increase the throughput by temporarily switching to a different thread of execution, or reduce the power and energy consumption by temporarily switching the processor to low-power mode. However, any such technique has a performance overhead in terms of switching time. Even though over the execution of an application the processor is stalled for a considerable amount of time, each stall duration is too small to profitably perform any state switch. In this paper, we present code transformations to aggregate processor free time. Our experiments on the Intel XScale and Stream kernels show that up to 50,000 processor cycles can be aggregated, and used to profitably switch the processor to low-power mode. We further show that our code transformations can switch the processor to low-power mode for up to 75% of kernel runtime, achieving up to 18% of processor energy savings on multimedia applications. Our technique requires minimal architectural modifications and incurs negligible ( < 1%) performance loss.","Switches,
Embedded computing,
Application software,
Embedded system,
Algorithm design and analysis,
Energy consumption,
Kernel,
Delay,
Permission,
Computer science"
The manifolds of spatial hearing,"We present exploratory studies on learning the non-linear manifold structure, in head related impulse responses (HRIRs). We use the recently popular locally linear embedding technique. The lower dimensional manifold encodes the perceptual information in the HRIRs, namely the direction of the sound source. Based on this, we propose a new method for HRIR interpolation. We also propose that the distance between two HRIRs of an individual be taken as the geodesic distance on the learned manifold.","Auditory system,
Humans,
Acoustic scattering,
Azimuth,
Torso,
Filtering,
Computer interfaces,
Computer science,
Educational institutions,
Interpolation"
An active detecting method against SYN flooding attack,"SYN flooding attacks are a common type of distributed denial-of-service (DDoS) attack. Early detection is desirable but traditional passive detection methods are inaccurate in the early stages due to their reliance on passively sniffing an attacking signature. The method presented in this paper captures attacking signatures using an active probing scheme that ensures the efficient early detection. The active probing scheme - DARB obtains the delay of routers by sending packets containing special time-to-live set at the IP headers. The results of the probe are used to perform SYN flooding detection, which is reliable and with little overhead. This approach is more independent than other methods that require cooperation from network devices. Experiments show that this delay-probing approach distinguishes half-open connections caused by SYN flooding attacks from those arising from other causes accurately and at an early stage.","Floods,
Computer crime,
Delay estimation,
Distributed computing,
Helium,
Computer science,
Probes,
Time of arrival estimation,
Protection,
Filtering"
Self-adaptive scheduler parameterization via online simulation,"Although thoroughly investigated, job scheduling for high-end parallel systems remains an inexact science, requiring significant experience and intuition from system administrators to properly configure batch schedulers. Production schedulers provide many parameters for their configuration, but tuning these parameters appropriately can be very difficult - their effects and interactions are often nonintuitive. In this paper, we introduce a methodology for automating the difficult process of job scheduler parameterization. Our proposed methodology is based on using past workload behavior to predict future workload, and on online simulations of a model of the actual system to provide on-the-fly suggestions to the scheduler for automated parameter adjustment. Detailed performance comparisons via simulation using actual supercomputing traces indicate that out methodology consistently outperforms other workload-aware methods for scheduler parameterization.","Job shop scheduling,
Processor scheduling,
Computational modeling,
Computer science,
Predictive models,
Delay estimation,
Educational institutions,
Production,
Performance analysis,
Analytical models"
Knowledge as a service and knowledge breaching,"In this paper, we introduce and explore a new computing paradigm we call knowledge as a service, in which a knowledge service provider, via its knowledge server, answers queries presented by some knowledge consumers. The knowledge server's answers are based on knowledge models that may be expensive or impossible to obtain for the knowledge consumers. While this new paradigm of computing is promising, we must establish a solid foundation to ensure its utility. We focus on the security aspect of the paradigm, and particularly on the problem we call knowledge breaching attack, which may allow an adversary to recover the knowledge underlying a knowledge service. Without being able to adequately handling such an attack, the knowledge service providers would never have any economic incentives to develop such a paradigm. Unfortunately, this paper theoretically shows that any interesting knowledge is subject to the knowledge breaching attack, and empirically shows that some knowledge models could be breached after a very small number of queries (e.g., 0.2-]% portion of the domain). Thus we need to investigate technical means that can alleviate such powerful attacks (at least for most practical knowledge models).","Data mining,
Insurance,
Companies,
Data privacy,
Protection,
Data security,
Solids,
Road accidents,
Investments,
Computer science"
Minimum exact word error training,"In this paper we present the minimum exact word error (exactMWE) training criterion to optimise the parameters of large scale speech recognition systems. The exactMWE criterion is similar to the minimum word error (MWE) criterion, which minimises the expected word error, but uses the exact word error instead of an approximation based on time alignments as used in the MWE criterion. It is shown that the exact word error for all word sequence hypotheses can be represented on a word lattice. This can be accomplished using transducer-based methods. The result is a word lattice of slightly refined topology. The accumulated weights of each path through such a lattice then represent the exact number of word errors for the corresponding word sequence hypothesis. Using this compressed representation of the word error of all word sequences represented in the original lattice, exactMWE can be performed using the same lattice-based re-estimation process as for MWE training. First experiments on the Wall Street Journal dictation task do not show significant differences in recognition performance between exactMWE and MWE at comparable computational complexity and convergence behaviour of the training",
Simple yet effective algorithms for block and I/O buffer placement in flip-chip design,"We study the problem of block and I/O buffer placement in flip-chip design. The goal of the problem is to minimize simultaneously the total path delay and the total skew of all input/output signals. We present two simple, yet effective, algorithms for the problem. Both algorithms place blocks to minimize the total path delay, and place I/O buffers to minimize the total skew. As compared to an existing method (Peng, C.-Y., 2003), the experimental results show that both algorithms are able to get better placement solutions with improvement rates of up to 65% and 77.5%, respectively, and run much faster.","Algorithm design and analysis,
Delay,
Signal design,
Bonding,
Thermal conductivity,
Routing,
Computer science,
Packaging,
Application specific integrated circuits,
Power system interconnection"
An edge detection method by combining fuzzy logic and neural network,"An edge detection method by combining fuzzy logic and neural network is proposed in this paper. First, the distance measures between the feature vector in 4 directions and the six edge prototype vectors for each pixel are taken as input pattern and fed into input layer of the self-organizing competitive neural network. Classifying the type of edge through this network, the thick edge image is obtained. After classifying, we utilize the competitive rule to thin the thick edge image in order to get the fine edge image. At last, the speckle edges are discarded from the edge image, thus the final optimal edge image is got. We compared the edge images got from our method with that from Canny's one and Sobel's one in our experiments. The experimental results show that the effect of our method is superior to other two methods and the robustness of our method is better.","Image edge detection,
Fuzzy logic,
Neural networks,
Image processing,
Image texture analysis,
Computer vision,
Energy measurement,
Noise measurement,
Information science,
Design engineering"
Fully decentralized DHT based approach to grid service discovery using overlay networks,"Distributed hash table (DHT) is widely used in peer-to-peer networks for its scalability, adaptability, and fault-tolerance in large and dynamic environments. In this paper, we study on the application of DHT for grid service discovery. Grid service is represented by multiple (attribute, value) pairs. The virtual organization (VO) is also self-organized into a 2-dimensional network overlays - one for attributes, and the other for values - as nodes join and publish services in the VO. Service discovery in a VO is firstly directed to the attribute overlay to find the nodes with the discovered attributes, then to the value overlay until the nodes which have the same (attribute, value) are reached. Gateway servers are deployed on the border to interconnect different VO's. The service discovery request can be redirected to other VO's in case that there is no provider in the requester's own VO. Our approach is highly efficient, scalable and quick responsive.","Peer to peer computing,
Protocols,
Space technology,
Scalability,
Network servers,
Computer science,
Information science,
Middleware,
Routing,
Fault tolerance"
Asymmetries in soft-error rates in a large cluster system,"Early in the deployment of the ASC Q cluster supercomputer system, an unexpectedly high rate of soft errors were observed in the board-level cache subsystems of the constituent AlphaServer ES45 systems that make up the compute component of this large cluster. A series of tests and experiments was undertaken to validate the hypothesis that this frequency was consistent with the high level of terrestrial secondary cosmic-ray neutron flux resulting from the high elevation of its installation site. The overall success of this effort is reported elsewhere in this issue. This paper reports on three secondary phenomena that were observed during these tests and experiments: Error logs were collected from all servers during a representative period and examined for nonrandom event rates, which would indicate a systematic cause. The only significant result of this exploration was the discovery of a latent soft-error discovery effect, and a self-shielding effect, whereby the servers positioned physically higher in their racks suffered disproportionately higher soft-error rates. This excess was examined and found to be consistent with established shielding effect of the high-Z composition of the constituents of the overlying systems. Experiments with individual ES45 systems in an artificial neutron beam at the Los Alamos Neutron Science Center facility have established that the soft-error rates observed in the SRAM parts is significantly dependent on the incident direction of the neutrons in the beam. These asymmetries could be exploited as part of a strategy for mitigating the frequency of soft errors in future computer systems.","Neutrons,
System testing,
Error analysis,
Semiconductor device testing,
Statistical analysis,
Frequency,
Particle beams,
Random access memory,
Computer errors,
Central Processing Unit"
On-line Detection of Patient Specific Neonatal Seizures using Support Vector Machines and Half-Wave Attribute Histograms,An efficient and effective support vector machine for online seizures detection is presented. The kernel designed is based on features generated from bivariate histograms of EEG half-wave attributes. The training is online using a simple heuristic known as chunking. The case study presented illustrates the performance of the method on typical neonatal seizures,"Pediatrics,
Support vector machines,
Histograms,
Electroencephalography,
Kernel,
Computer science,
Detectors,
Autocorrelation,
Spectral analysis,
Digital filters"
Analyzing myopic approaches for multi-agent communication,"Choosing when to communicate is a fundamental problem in multi-agent systems. This problem becomes particularly hard when communication is constrained and each agent has different partial information about the overall situation. Although computing the exact value of communication is intractable, it has been estimated using a standard myopic assumption. However, this assumption - that communication is only possible at the present time ntroduces error that can lead to poor agent behavior. We examine specific situations in which the myopic approach performs poorly and demonstrate an alternate approach that relaxes the assumption to improve the performance. The results provide an effective method for value-driven communication policies in multi-agent systems.",
Novel hybrid hierarchical-K-means clustering method (H-K-means) for microarray analysis,"Hierarchical and k-means clustering are two major analytical tools for unsupervised microarray datasets. However, both have their innate disadvantages. Hierarchical clustering cannot represent distinct clusters with similar expression patterns. Also, as clusters grow in size, the actual expression patterns become less relevant. K-means clustering requires a specified number of clusters in advance and chooses initial centroids randomly: in addition, it is sensitive to outliers. We present a novel hybrid approach to combined merits of the two and discard disadvantages we mentioned above. It is different from existed method: carry out hierarchical clustering first to decide location and number of clusters in the first round and run the K-means clustering in another round. The brief idea is we cluster around half data through hierarchical clustering and succeed by K-means for the rest half in one single round. Also, our approach provides a mechanism to handle outliers. Comparing with existed hybrid clustering approach and K-means clustering in 2 different distance measure on Eisen's yeast microarray data, our method always generate much higher quality clusters.","Clustering methods,
Clustering algorithms,
Partitioning algorithms,
Fungi,
Bioinformatics,
Distance measurement,
Computer science,
Hybrid power systems,
Monitoring,
Genomics"
Irregular pattern matching using projections,"Recently, a novel approach to pattern matching was introduced, which reduces time complexity by two orders of magnitude over traditional approaches. It uses an efficient projection scheme which bounds the distance between a pattern and an image window using very few operations on average. The projection framework combined with a rejection scheme allows rapid rejection of image windows that are distant from the pattern. One of the limitations of this approach is the restriction to square dyadic patterns. In this paper we introduce a scheme, based on this approach which allows fast search for patterns of any size and shape. The suggested method takes advantage of the inherent recursive tree-structure introduced in the original scheme.","Pattern matching,
Euclidean distance,
Computer science,
Shape,
Kernel,
Performance evaluation,
Pixel,
Noise shaping"
Text categorization rule extraction based on fuzzy decision tree,"In this paper, a new method for text categorization rule extraction based on fuzzy decision tree is presented. An improved chi-square statistic is adopted. The new method reduces features of text in terms of the improved chi-square statistic, and so largely reduces the dimensions of the vector space. And then, a new method for the construction of membership functions is presented, which reduces the time of data fuzzification largely and increase categorization accuracy consequently. Finally, the fuzzy decision tree is applied to the text categorization. Both the understandable categorization rules and the better accuracy of categorization can be acquired.",
Performance analysis of generics in scientific computing,"This paper studies the performance of generics, or templates as they are sometimes called, for scientific computing in various programming languages. In order to understand the cost of using generics, we develop a test suite for generics based on a standard numeric benchmark. We compare the results of this new benchmark for generics in C++, C# and Java, both between language implementations and against the specialized, non-generic benchmark. We also compare the efficiency of C++ with Aldor a language originally for computer algebra relying entirely on generics. We find that the implementation of generics in current compilers must be improved before they are used for efficiency-critical scientific applications, and we identify specific areas for potential optimization.","Performance analysis,
Scientific computing,
Benchmark testing,
Computer languages,
Costs,
Standards development,
Java,
Algebra,
Application software,
Optimizing compilers"
An Artificial Neural-Network-Based Approach to Software Reliability Assessment,"In this paper, we propose an artificial neural- network-based approach for software reliability estimation and modeling. We first explain the network networks from the mathematical viewpoints of software reliability modeling. That is, we will show how to apply neural network to predict software reliability by designing different elements of neural networks. Furthermore, we will use the neural network approach to build a dynamic weighted combinational model. The applicability of proposed model is demonstrated through four real software failure data sets. From experimental results, we can see that the proposed model significantly outperforms the traditional software reliability models.","Software reliability,
Neural networks,
Mathematical model,
Artificial neural networks,
Application software,
Predictive models,
Computer science,
Software design,
Software testing,
Aircraft"
Probabilistic modeling based vessel enhancement in thoracic CT scans,"Vessel enhancement in volumetric data is a necessary prerequisite in various medical imaging applications with particular importance for automated nodule detection. Ideally, vessel enhancement filters should enhance vessels and vessel junctions while suppressing nodules and other non-vessel elements. A distinction between vessels and nodules is normally obtained through eigenvalue analysis of the curvature tensor which is a second order differential quantity and so is sensitive to noise. Furthermore, by relying on principal curvatures alone, existing vessel enhancement filters are incapable of distinguishing between nodules and vessel junctions. In this paper we propose probabilistic vessel models from which novel vessel enhancement filters capable of enhancing junctions while suppressing nodules are derived. The proposed filters are based on eigenvalue analysis of the structure tensor which is a first order differential quantity and so are less sensitive to noise. The proposed filters are evaluated and compared to known techniques based on actual clinical data.","Computed tomography,
Nonlinear filters,
Biomedical imaging,
Eigenvalues and eigenfunctions,
Tensile stress,
Blood vessels,
Lungs,
Image segmentation,
Computer science,
Application software"
Optimizing checkpoint sizes in the C3 system,"The running times of many computational science applications are much longer than the mean-time-between-failures (MTBF) of current high-performance computing platforms. To run to completion, such applications must tolerate hardware failures. Checkpoint-and-rest art (CPR) is the most commonly used scheme for accomplishing this - the state of the computation is saved periodically on stable storage, and when a hardware failure is detected, the computation is restarted from the most recently saved state. Most automatic CPR, schemes in the literature can be classified as system-level checkpointing schemes because they take core-dump style snapshots of the computational state when all the processes are blocked at global barriers in the program. Unfortunately, a system that implements this style of checkpointing is tied to a particular platform amd cannot optimize the checkpointing process using application-specific knowledge. We are exploring an alternative called automatic application-level checkpointing. In our approach, programs are transformed by a pre-processor so that they become self-checkpointing and self-rest art able on any platform. In this paper, we evaluate a mechanism that utilizes application knowledge to minimize the amount of information saved in a checkpoint.","Checkpointing,
Hardware,
Application software,
Concurrent computing,
Programming profession,
Automatic logic units,
Protocols,
Computer science,
Computer applications,
Software systems"
Merging parallel simulation programs,"In earlier work cloning is proposed as a means for efficiently splitting a running simulation midway through its execution into multiple parallel simulations. In simulation cloning, clones usually are able to share computations that occur early in the simulation, but as their states diverge individual LPs are replicated as necessary so that their computations proceed independently. However, if, over time the state of the clones (or their constituent LPs) converges there is, as of yet, no means for recombining them. In this case some efficiency is lost because they will execute identical events. This idea is the reverse of cloning, as we merge logical processes that have been previously cloned and we show that this can further increase efficiency because the new un-cloned LPs will complete computations that would otherwise be duplicated. We discuss our implementation of merging, and illustrate its effectiveness in several example simulation scenarios.","Merging,
Computational modeling,
Cloning,
Computer simulation,
Context modeling,
Computer science,
Telecommunication traffic,
Traffic control,
Conferences,
Software systems"
Palmprint identification algorithm using Hu invariant moments and Otsu binarization,"Recently, biometrics-based personal identification is regarded as an effective method of person's identity with recognition automation and high performance. In this paper, the palmprint recognition method based on Hu invariant moment is proposed. And the low-resolution(75dpi) palmprint image(135/spl times/135 Pixel) is used for the small scale database of the effectual palmprint recognition system. The proposed system is consists of two parts: firstly, the palmprint fixed equipment for the acquisition of the correctly palmprint image and secondly, the algorithm of the efficient processing for the palmprint recognition. And the palmprint identification step is limited 3 times. As a results, when the coefficient is 0.001 then FAR and GAR are 0.038% and 98.1% each other. The authors confirmed that FAR is improved 0.002% and GAR is 0.1% each other compared with online palmprint identification algorithm.",
An image inpainting method,"Image inpainting technique has been widely used for reconstructing damaged old photographs and removing unwanted objects from images. In this paper, we present an image inpainting method based on the existing exemplar-based image inpainting idea. Our method improves the robustness and effectiveness by rational confidence computing method, matching strategy and filling scheme. Therefore, our method effectively prevents ""growing garbage"", which is a common problem in other methods. With our method, we can obtain preferable results to those obtained by other similar methods.","Filling,
Image restoration,
Robustness,
Painting,
Diffusion processes,
Computer science,
Image reconstruction,
Art,
Digital images,
Partial differential equations"
Efficient Locomotion for a Self-Reconfiguring Robot,In this paper we describe a modular self-reconfiguring robot composed of Molecule robot modules. We present the architecture of this robot and discuss how self-reconfiguration can be used as a locomotion gait for this system. We present two types of locomotion algorithms for this robot: a statically stable tumbling algorithm and a dynamically stable algorithm that achieves locomotion by modifying the center of mass of the robot. For each algorithm we analyze the efficiency of the self-reconfiguration gait for locomotion. Finally we present experimental data for the tumbling algorithm implemented on a four-module Molecule robot.,"Lattices,
Intelligent robots,
Robot sensing systems,
Laboratories,
Heuristic algorithms,
Orbital robotics,
Robotics and automation,
Algorithm design and analysis,
Grippers,
Automatic control"
Feature uncertainty arising from covariant image noise,"Uncertainty estimates related to the position of image features are seeing increasing use in several computer vision problems. Many of these have been recast from standard least squares model fitting to techniques that minimize the Mahalanobis distance, which weighs each error vector by covariance of the observations. These include structure from motion and traditional geometric camera calibration. Uncertainty estimates previously derived for the case of corner localization are based on implicit assumptions that preclude sophisticated image noise models. Uncertainties associated with these features tend to be over estimated. In this work, we introduce a new formulation for feature location uncertainty that supports arbitrary pixel covariance to derive a more accurate positional uncertainty estimate. The method is developed and evaluated in the case of a traditional interest operator that is in widespread use. Results show that uncertainty estimates based on this new formulation better reflect the error distribution in feature location.","Uncertainty,
Computer vision,
Least squares methods,
Computer errors,
Motion estimation,
Stereo vision,
Computer science,
Cameras,
Calibration,
Predictive models"
Context-aware implementation based on CBR for smart home,"Context awareness is emphasized in order to provide automatic services in smart home. This paper uses case based reasoning as the reasoning method which solves the problem ""in the first phase, we don't know exactly about the key processes and their interdependencies in smart home's context"". The context's contents in smart home are described in this paper. Also, case representation, case storage and similarity calculation are discussed in smart home's context awareness. We propose a framework of context aware based on CBR, and discuss the case adaptation in detail.","Smart homes,
Context awareness,
Ubiquitous computing,
Pervasive computing,
Home computing,
Information science,
Physics computing,
Application software,
Design engineering,
Educational institutions"
An optimized adder accumulator for high speed MACs,"A novel architecture of adder with accumulation register here called adder accumulator (AAC) is presented. When used for the implementation of a MAC, it drastically reduces the delay of the final adder portion with a very small extra area consumption. The novel architecture merges the adder block and the accumulator register present in the MAC operator furnishing the possibility to use two separate n/2 bit adders instead of the n bit adder generally employed to accumulate the n bit MAC result",
A compatible and scalable clock synchronization protocol in IEEE 802.11 ad hoc networks,"This paper studies the scalability and compatibility problems of clock synchronization in IEEE 802.11 ad hoc networks. The scalability problem of 802.11 timing synchronization has been recognized and studied by researchers in the field, but the proposed solutions are not meeting industry expectation. The compatibility issue is not well investigated by the research community yet. The compatibility issue is very important and practical because of the large deployment base of 802.11 networks. In this paper, we try to address both issues. We propose a simple, compatible protocol without any change of beacon format. The frequency adjustment is proved to be bounded and the maximum clock offset is controlled under 20 /spl mu/s. It is a significant improvement over the current results in the field. The current solutions with similar complexity can only control the maximum clock offset around 125 /spl mu/s for compatible solutions and 50 /spl mu/s for non-compatible protocols.","Clocks,
Protocols,
Intelligent networks,
Ad hoc networks,
Scalability,
Frequency synchronization,
Application specific processors,
Timing,
Computer science,
Peer to peer computing"
BLAM: an energy-aware MAC layer enhancement for wireless adhoc networks,"In wireless adhoc networks, channel and energy capacities are scarce resources. However, the design of the IEEE 802.11 DCF protocol leads to an inefficient utilization of these resources. We introduce BLAM, a new battery level aware MAC protocol, which is developed from an energy-efficiency point of view to extend the useful lifetime of an adhoc network. We modify the IEEE 802.11 DCF protocol to enable BLAM to tune the random deferring time for fresh and collided data packets dynamically, based on the node's energy. We show that BLAM can achieve an increase of 15% in network lifetime and an increase of about 35% in the total number of received packets.",
Path Planning for Variable Resolution Minimal-Energy Curves of Constant Length,"We present a new approach to path planning for flexible wires. We introduce a method for computing stable configurations of a wire subject to manipulation constraints. These configurations correspond to minimal-energy curves. The representation is adaptive in the sense that the number of parameters automatically varies with the complexity of the underlying curve. We introduce a planner that computes paths from one minimal-energy curve to another such that all intermediate curves are also minimal-energy curves. Using a simplified model for obstacles, we can find minimal-energy curves of fixed length that pass through specified tangents at given control points. Our work has applications in motion planning for surgical suturing and snake-like robots.","Path planning,
Wire,
Surgery,
Motion planning,
Robots,
Surges,
Shape measurement,
Computer science,
Virtual reality,
Routing"
Unified Target Detection and Tracking Using Motion Coherence,"This paper presents a unified approach to adaptive target detection and tracking. The unifying concept is ""coherent motion energy"", a measure of the extent to which a single motion dominates local spatiotemporal structure. There are three major components to the approach. First, a multiresolution analysis of coherent motion energy is used to detect salient dynamic targets. Second, a robust affine transformation estimator is used to recover frame-to-frame target motion across regions of interest defined by coherent motion. Third, a method of template adaptation based on coherent motion weighted goodness of match is used to drive automatic template update. Empirical evaluation of the approach shows the contribution of the various components and documents strong performance of the integrated whole.","Object detection,
Target tracking,
Energy measurement,
Motion measurement,
Spatiotemporal phenomena,
Multiresolution analysis,
Motion analysis,
Motion detection,
Robustness,
Motion estimation"
To unify structured and unstructured P2P systems,"Most of current peer-to-peer designs build their own system overlays independent of the physical one. Nodes within unstructured systems form a random overlay, on the contrary, structured designs normally organize peers into an elegant identifier ring. However, all of those overlays are far from the physical one. Noticed that the system overlay is crucial for building a distributed system, this paper proposes to build system overlays based on the physical overlay. By making full use of physical network characteristics and taking advantages of both structured and unstructured protocols, a network-based peer-to-peer system is built in this paper. Not only the system is highly efficient (the stretch is equal to one), but also it can adapt extremely system churning. The most important is that the maintenance overhead is very low, even under highly dynamic environment.","Peer to peer computing,
Protocols,
Scalability,
Bandwidth,
Physics computing,
Design engineering,
Computer science,
Buildings,
Routing,
File systems"
A category-theoretic approach to syntactic software merging,"Software merging is a common and essential activity during the lifespan of large-scale software systems. Traditional textual merge techniques are inadequate for detecting syntactic merge conflicts. In this paper, we propose a domain-independent approach for syntactic software merging that exploits the graph-based structure(s) of programs. We use morphisms between fuzzy graphs to capture the relationships between the structural elements of the programs to be merged, and apply a truth ordering lattice to express inconsistencies and evolutionary properties as we compute the merge. We demonstrate the approach with a three-way consolidation merge in a commercial software system; in particular, we show how analyzing merged call structures can help developers gain a better understanding and control of software evolution.","Merging,
Software systems,
Software maintenance,
Reverse engineering,
Large-scale systems,
Lattices,
Fuzzy sets,
Application software,
Computer science,
Control systems"
"Ghost Process: a Sound Basis to Implement Process Duplication, Migration and Checkpoint/Restart in Linux Clusters","Process management mechanisms (process duplication, migration and checkpoint/restart) are very useful for high performance and high availability in clustering systems. The single system image approach aims at providing a global process management service with mechanisms for process checkpoint, process migration and process duplication. In this context, a common mechanism for process virtualization is highly desirable but traditional operating systems do not provide such a mechanism. This paper presents a kernel service for process virtualization called ghost process, extending the Linux kernel. The ghost process mechanism has been implemented in the Kerrighed single system image based on Linux","Linux,
Kernel,
Government,
Resource management,
Operating systems,
Fault tolerance,
Checkpointing,
Computer science,
Mathematics,
Laboratories"
On the freeze quantifier in constraint LTL: decidability and complexity,"Constraint LTL, a generalization of LTL over Presburger constraints, is often used as a formal language to specify the behavior of operational models with constraints. The freeze quantifier can be part of the language, as in some real-time logics, but this variable-binding mechanism is quite general and ubiquitous in many logical languages (first-order temporal logics, hybrid logics, logics for sequence diagrams, navigation logics, etc.). We show that Constraint LTL over the simple domain augmented with the freeze operator is undecidable which is a surprising result regarding the poor language for constraints (only equality tests). Many versions of freeze-free constraint LTL are decidable over domains with qualitative predicates and our undecidability result actually establishes /spl Sigma//sub 1//sup 1/ -completeness. On the positive side, we provide complexity results when the domain is finite (EXPSPACE-completeness) or when the formulae are flat in a sense introduced in the paper.","Logic,
Automata,
Counting circuits,
Computational complexity,
Clocks,
Computer science,
Formal languages,
Navigation,
Testing,
Protocols"
LEO-15 Observatory - the next generation,"The cabled LEO-15 observatory was a vision of Fred Grassle and Chris von Alt that became a reality in 1996 with the deployment of Nodes A and B in 15 meters of water off of Tuckerton, New Jersey. These nodes have served the scientific community well for almost a decade providing power to a variety of sensors and bi-directional real-time communication between the sensors and the PIs computer located on shore. However, technology and scientific needs have changed since these nodes were deployed making it necessary to upgrade the nodes to meet not only today's demands but also to provide expandability and flexibility for the future. The nodes must be able to do more than provide real-time data. They need to be a part of a sustained and interactive network of autonomous and remote platforms that coordinate sampling in space as well as in time. Rutgers University Mid-Atlantic Bight National Undersea Research Center (MABNURC) has partnered with WETSAT, Inc. to accomplish the LEO upgrade and expansion. The nodes will be expanded to include 10 guest ports for visiting scientists to plug their sensors into as well as ports for an auto-profiling unit, and two video ports (with lights and pan/tilt capability). There will also be expansion capabilities with two 10/100BASE-TX ports so that more guest ports can be added if necessary. Communications will be upgraded to TCP/IP over Gigabit Ethernet. Each science port will have regulated isolated power, at a software selectable voltage where necessary that is individually ground fault and over-current protected. Finally, the DACNet ocean observatory operating system software will be used to control the observatory. Phase 1 of the node upgrade, will be completed in the summer of 2005, and is concentrated on the refurbishment of Node A and installation of DACNet. In addition to the nodes, plans are underway to deploy an instrumented buoy and bottom mooring on the Endurance Line at the 60 in isobath to augment the Slocum Gliders that operate between LEO-15 and the shelf break.","Observatories,
Bidirectional control,
Sampling methods,
Low earth orbit satellites,
Plugs,
TCPIP,
Ethernet networks,
Voltage,
Power system protection,
Oceans"
Experiences in teaching grid computing to advanced level students,"The development of teaching materials for future software engineers is critical to the long term success of the grid. At present however there is considerable turmoil in the grid community both within the standards and the technology base underpinning these standards. In this context, it is especially challenging to develop teaching materials that have some sort of lifetime beyond the next wave of grid middleware and standards. In addition, the current way in which grid security is supported and delivered has two key problems. Firstly in the case of the UK e-Science community, scalability issues arise from a central certificate authority. Secondly, the current security mechanisms used by the grid community are not line grained enough. In this paper we outline how these issues are being addressed through the development of a grid computing module supported by an advanced authorisation infrastructure at the University of Glasgow.","Education,
Grid computing,
Middleware,
Educational technology,
Data security,
Standards development,
Scalability,
Authorization,
Internet,
Veins"
Hierarchical shaped deficit round-robin scheduling,"We describe a hierarchical traffic shaper-scheduler, hierarchical SDRR (HSDRR), for flows of variable-length packets that is low-complexity (scales with the number of queues). That is, HSDRR can be used channelize the output link of a router to satisfy service-level agreements (token bucket constraints) struck at network-to-network boundaries. HSDRR is a hybrid round-robin/time-stamp scheduler (S. Ramabhadran and J. Pasquale, 2003) that employs shaped deficit round-robin (SDRR) (S. Jiwasurat and G. Kesidis, 2004) scheduling in the first stage and shaped virtual clock (SVC) (D. Stiliadis and A. Varma, 1997) in the second and final stage.","Bandwidth,
Processor scheduling,
Internet,
Computer science,
Telecommunication traffic,
Traffic control,
Clocks,
Static VAr compensators,
Scheduling algorithm,
Feeds"
Build-by-number: rearranging the real world to visualize novel architectural spaces,"We present build-by-number, a technique for quickly designing architectural structures that can be rendered photorealistically at interactive rates. We combine image-based capturing and rendering with procedural modeling techniques to allow the creation of novel structures in the style of real-world structures. Starting with a simple model recovered from a sparse image set, the model is divided into feature regions, such as doorways, windows, and brick. These feature regions essentially comprise a mapping from model space to image space, and can be recombined to texture a novel model. Procedural rules for the growth and reorganization of the model are automatically derived to allow for very fast editing and design. Further, the redundancies marked by the feature labeling can be used to perform automatic occlusion replacement and color equalization in the finished scene, which is rendered using view-dependent texture mapping on standard graphics hardware. Results using four captured scenes show that a great variety of novel structures can be created very quickly once a captured scene is available, and rendered with a degree of realism comparable to the original scene.","Visualization,
Layout,
Rendering (computer graphics),
Buildings,
Computer graphics,
Image reconstruction,
Design automation,
Urban planning,
Computer science,
Labeling"
A tight small gain theorem for not necessarily ISS systems,"A new version of the small-gain theorem is presented for nonlinear finite dimensional systems. The result provides conditions for global asymptotic stability under relaxed assumptions, in particular the two interconnected subsystems need not be Input-to-State stable in open loop.",
"Bridging the Digital Divide - The Roles of Internet Self-Efficacy towards Learning Computer and the Internet among Elderly in Hong Kong, China","Experts in areas such as Public Policy, Communications, Business Management, and Economics have addressed the phenomenon of the digital divide since the 1990s. Based on previous validated studies, the researchers offered a new theoretical model in examining the influences of the two mediating factors - Internet self-efficacy and outcome expectations - on the elderly's intentions in adopting a new technology (i.e. the Internet), and their perceptions of their knowledge in utilizing this technology. The elderly are categorized as one of the technologically and sociologically disadvantaged groups within the context of the digital divide. Two studies (repeated measures) were conducted and a set of conclusions was reached. In general, the findings validated the impacts and antecedents of Internet self-efficacy and outcome expectations on usage intention and perceived user competence. Limitations and implications are provided following the sections on research framework, model and hypotheses, and discussion on findings.","Internet,
Senior citizens,
Public policy,
Business communication,
Communications technology,
Unemployment,
Information systems,
Psychology,
Social network services,
Couplings"
Error-correcting codes for automatic control,"In many control-theory applications one can classify all possible states of the device by an infinite state graph with polynomially-growing expansion. In order for a controller to control or estimate the state of such a device, it must receive reliable communications from its sensors; if there is channel noise, the encoding task is subject to a stringent real-time constraint. We show a constructive on-line error correcting code that works for this class of applications. Our code is computationally efficient and enables on-line estimation and control in the presence of channel noise. It establishes a constructive (and optimal-within-constants) analog, for control applications, of the Shannon coding theorem.",
Discontinuous spinning gait of a quadruped walking robot with waist-joint,"In this paper, a new gait is presented for a quadruped walking robot with waist-joint. Although various types of walking robot have already been developed, their structure and gaits are still quite distinct from the natural walking motion of an animal, and the key point is that animals have waist-joint and use their waist-joint to walk, but, on the other hand a conventional 4-legged walking robot even has single rigid body which has no waist joint. Especially in turning, most animals bend oneself on one side, which makes turn faster and more stable. Spinning gait is very important as same as straight forward/backward gait. All animals need a spinning gait to avoid obstacle or to change walking direction. Therefore, this paper proposes a discontinuous gait for a quadruped walking robot with waist-joint which divides body into the front and rear parts of the body. Firstly, we describe a kinematic relation of waist-joint, the hip, and the centre of gravity of body, and then find the optimal values of parameters to spin most stable with simulation. We implemented a waist-jointed walking robot and showed that proposed spinning gait using waist-joint is more stable than that of a conventional walking robot.","Legged locomotion,
Spinning,
Robot kinematics,
Turning,
Mobile robots,
Animals,
Stability,
Robotics and automation,
Leg,
Computer science"
PBExplore: a framework for compiler-in-the-loop exploration of partial bypassing in embedded processors,"Varying partial bypassing in pipelined processors is an effective way to make performance, area and energy tradeoffs in embedded processors. However, performance evaluation of partial bypassing in processors has been inaccurate, largely due to the absence of bypass-sensitive retargetable compilation techniques. Furthermore no existing partial bypass exploration framework estimates the power and cost overhead of partial bypassing. In this paper we present PBExplore: a framework for compiler-in-the-loop exploration of partial bypassing in processors. PBExplore accurately evaluates the performance of a partially bypassed processor using a generic bypass-sensitive compilation technique. It synthesizes the bypass control logic and estimates the area and energy overhead of each bypass configuration. PBExplore is thus able to effectively perform multidimensional exploration of the partial bypass design space. We present experimental results on the Intel XScale architecture on MiBench benchmarks and demonstrate the need, utility and exploration capabilities of PBExplore.","Costs,
Logic,
Registers,
Performance evaluation,
Embedded computing,
Microprocessors,
Computer science,
Hazards,
Timing,
Delay"
Shared packet loss recovery for Internet telephony,"We consider an Internet telephony system in which the service provider operates a telephone gateway in each servicing city to serve the general public. We propose a shared packet loss recovery scheme for this system. Using this scheme, each gateway uses erasure code to add redundant packets such that its outgoing voice streams share these redundant packets for packet loss recovery. This scheme has two advantages: (1) it has a small probability of packet loss because multiple voice streams can share the redundant packets for effective packet loss recovery, and (2) it involves a small recovery delay because it uses the packets from multiple voice streams for packet loss recovery.","Internet telephony,
Cities and towns,
Streaming media,
Bandwidth,
Delay effects,
Web and internet services,
IP networks,
Bridges,
Computer science,
Computer errors"
"Development of Head-Mounted Projection Displays for Distributed, Collaborative, Augmented Reality Applications","Distributed systems technologies supporting 3D visualization and social collaboration will be increasing in frequency and type over time. An emerging type of head-mounted display referred to as the head-mounted projection display (HMPD) was recently developed that only requires ultralight optics (i.e., less than 8 g per eye) that enables immersive multiuser, mobile augmented reality 3D visualization, as well as remote 3D collaborations. In this paper a review of the development of lightweight HMPD technology is provided, together with insight into what makes this technology timely and so unique. Two novel emerging HMPD-based technologies are then described: a teleportal HMPD (T-HMPD) enabling face-to-face communication and visualization of shared 3D virtual objects, and a mobile HMPD (M-HMPD) designed for outdoor wearable visualization and communication. Finally, the use of HMPD in medical visualization and training, as well as in infospaces, two applications developed in the ODA and MIND labs respectively, are discussed.",
Scaling Gaussian RBF kernel width to improve SVM classification,"Support vector classification with Gaussian RBF kernel is sensitive to the kernel width. Small kernel width may cause over-fitting, and large one under-fitting. The so-called optimal kernel width is merely selected based on the tradeoff between under-fitting loss and over-fitting loss. So, there exists urgent need to further reduce the tradeoff loss. To circumvent this, we scale the kernel width in a distribution-dependent way. Experiments validate the feasibility of this method. Existing problems are also discussed",
Geometry synthesis by example,"In this paper we present a method for geometry synthesis by example, inspired by techniques from texture synthesis. Given an example of input geometry, we synthesize new output geometry that is perceived similar to the input geometry, but at the same time differs in its local appearance. We assume the input geometry satisfies the constraints of a Markov Random Field model, and represent the input geometry by a hierarchical distance field. This allows us to perform fast matching between a target distance field that is partially synthesized, and the input distance field. Once the target distance field is completed, we copy the original corresponding geometry elements to the synthesized result. We show that automatically generating geometry by example can be achieved within reasonable computing times, and is able to produce convincing results.","Solid modeling,
Computational geometry,
Markov random fields,
Stochastic processes,
Sampling methods,
Frequency estimation,
Computer science,
Humans,
Shape,
Probability distribution"
Design and implementation of Web services QoS broker,"Quality of service is one of the most important factors for user's choice of Web service. However, current Web service environments do not offer any information on QoS of Web services. We design and develop a Web service QoS broker system which actively monitors QoS of Web services. With this information a user can select a Web service best suited for his/her needs. Availability, performance, and reliability were the metrics used for QoS monitoring.","Web services,
Quality of service,
Monitoring,
Throughput,
Availability,
Delay,
Information analysis,
Protocols,
Service oriented architecture,
Computer science"
Algorithm for optimizing energy use and path resilience in sensor networks,"Sensor networks will change the way computers interface with our world and with each other. This transformation will be shaped by the network centric paradigm demonstrated in sensor networks. Sensor networks also require a data-centric communication paradigm to efficiently and effectively share data. Directed diffusion is a data-centric communication paradigm that forms a foundation of this paper. Energy efficient routing algorithms have been developed for directed diffusion; however, we have developed improved algorithms, which are described in this paper. We also present computer simulation results, which verify the effectiveness of previously established routing algorithms and compare them to our new and improved routing algorithms. The results show significant increase in energy efficiency and resilience. Finally, the paper incorporates effective techniques for modeling of sensor networks to demonstrate the usefulness of the new algorithms.","Resilience,
Intelligent networks,
Sensor phenomena and characterization,
Energy efficiency,
Floods,
Computer simulation,
Batteries,
Energy conservation,
Routing protocols,
Computer science"
A robust content-based image retrieval system using multiple features representations,"The similarity measurements and the representation of the visual features are two important issues in content-based image retrieval (CBIR). In this paper, we compared between the combination of wavelet-based representations of the texture feature and the color feature with and without using the color layout feature. To represent the color information, we used global color histogram (GCH) beside the color layout feature and with respect to the texture information; we used Haar and Daubechies wavelets. Based on some commonly used Euclidean and Non-Euclidean similarity measures, we tested different categories of images and measured the retrieval accuracy when combining such techniques. The experiments showed that the combination of GCH and 2-D Haar wavelet transform using the cosine distance gives good results while the best results obtained when adding the color layout feature to this combination by using the Euclidean distance. The results reflected the importance of using the spatial information beside the color feature itself and the importance of choosing good similarity distance measurements.","Image retrieval,
Content based retrieval,
Histograms,
Image databases,
Spatial databases,
Feature extraction,
Computer science,
Testing,
Wavelet transforms,
Distance measurement"
Application of wearable inertial sensors in stroke rehabilitation,"We introduce a human arm movement tracking system that has been developed to aid the rehabilitation of stroke patients. A wearable 3-axis inertial sensor is used to capture arm movements in 3-D space and in real time. The tracking algorithm is based on a kinematical model that considers the upper and lower forearm. To improve accuracy and consistency, a weighted least square filtering strategy is adopted. The calculated motion trajectory was compared with that measured using a commercially available Qualysis tracking system. For 3D cyclical rotation, the mean wrist position error was 2.45 cm without filtering and 1.79 cm after the filtering algorithm was applied. The experimental results demonstrate the favorable performance of the proposed framework in estimation of upper limb motion in stroke rehabilitation","Wearable sensors,
Humans,
Filtering,
Biomedical monitoring,
Tracking,
Wrist,
Least squares methods,
Trajectory,
Motion measurement,
Motion estimation"
Time-tunnel: visual analysis tool for time-series numerical data and its extension toward parallel coordinates,"Time-tunnel proposed by Akaishi et al. is a multidimensional data analysis tool. Time-tunnel visualizes any number of time series numerical data records as individual charts each of which is displayed on an individual rectangular plane called data-wing in a 3D virtual space. Through direct manipulations on a computer screen, the user easily puts more than two data-wings overlapped together to compare their charts in order to recognize the similarity or the difference among those data records. Simultaneously a radar chart among those data at any time point is displayed to recognize the similarity and the correlation among them. Since only one chart is displayed on one data-wing, so if there is a number of data records, the user has to prepare accordingly a huge number of data-wings and practically it becomes impossible to manipulate them. To deal with this problem, the authors enhanced the functionality of time-tunnel to enable it to display more than two charts on each data-wing. In other word, the authors made each data-wing have the functionality of the parallel coordinates. This paper proposes the usefulness of this enhancement of time-tunnel by showing data analysis examples.","Time series analysis,
Data analysis,
Data visualization,
Multidimensional systems,
Companies,
Application software,
Information analysis,
Information science,
Radar,
Displays"
The Research and Implematation of Services Discovery Agent in Web Services Composition Framework,"Web services are emerging as a dominant paradigm for constructing and composing distributed business applications and enabling enterprise-wide interoperability. A critical factor to the overall utility of web services composition is a flexible services discovery mechanism. The goal of this paper is to propose an agent-based approach to web services discovery. Firstly, an agent-based framework for web services composition is presented. Next, web services QoS model and description are introduced and an algorithm of selecting services from the perspective of composition is given. Finally, web services discovery agent is designed and its implementation based on JADE platform is presented in detail.",
On scheduling complex dags for Internet-based computing,"Conceptual tools are developed to aid in crafting a theory of scheduling complex computation-dags for Internet-based computing. The goal of the schedules produced is to render tasks eligible for allocation to remote clients (hence for execution) at the maximum possible rate. This allows one to utilize remote clients well, and also lessen the likelihood of the ""gridlock"" that ensues when a computation stalls for lack of eligible tasks. Earlier work has introduced formalism for studying this optimization problem and has identified optimal schedules for several significant families of structurally uniform dags. The current paper extends this work via a methodology for devising optimal schedules for a much broader class of complex dags. These dags are obtained via composition from a prespecified collection of simple building-block dags. The paper introduces a suite of algorithms that decompose a given dag to expose its building-blocks, and a priority relation on building-blocks. When the building-blocks are appropriately interrelated, the dag can be scheduled optimally.","Processor scheduling,
Internet,
Optimal scheduling,
Grid computing,
Computer science,
Abstracts,
Monitoring,
Scheduling algorithm,
Distributed processing"
Project based learning as a pedagogical tool for embedded system education,"In this paper, a project-based learning strategy is proposed as a pedagogical tool for embedded system education targeting for undergraduate engineering students. The proposed project-based learning can motivate students to integrate and formulate the multi-disciplinary knowledge previous learned into a real-world embedded system project development. To investigate the effectiveness of the project-based learning, a post-project review is conducted for groups of junior-year engineering students participating in embedded system projects development. Post-project review results shown that the knowledge and skill build in earlier learning play major role in the embedded system project development. Evidences also shown that situational factors, such as project subject selection, learning curve of individuals and capability of working under time pressure, are considered influential to the effectiveness of project-based learning in embedded system education as well.","Embedded system,
Educational institutions,
Embedded computing,
Computer science education,
Aerospace electronics,
Systems engineering education,
Cities and towns,
Engineering students,
Communications technology,
Educational technology"
Standardized evaluation method for Web clustering results,Web clustering assists users of a search engine by presenting search results as clusters of related pages. Many clustering algorithms with different characteristics have been developed: but the lack of a standardized Web clustering evaluation method that can evaluate clusterings with different characteristics has prevented effective comparison of algorithms. The paper solves this by introducing a new structure for defining general ideal clusterings and new measurements for evaluating clusterings with different characteristics by comparing them against the general ideal clustering.,
Aggregation policies over RSVP tunnels,,"Australia,
Protocols,
Bandwidth,
Intserv networks,
Frequency,
Resource management,
Traffic control,
Computer science,
Cost function,
Scalability"
A comparison of network level fault injection with code insertion,"This paper describes our research into the application of fault injection to Simple Object Access Protocol (SOAP) based service oriented-architectures (SOA). We show that our previously devised WS-FIT method, when combined with parameter perturbation, gives comparable performance to code insertion techniques with the benefit that it is less invasive. Finally we demonstrate that this technique can be used to compliment certification testing of a production system by strategic instrumentation of selected servers in a system.","Simple object access protocol,
Measurement techniques,
Application software,
Service oriented architecture,
Web services,
Computer science,
System testing,
Software tools,
Predictive models,
Computer errors"
Density based clustering with crowding differential evolution,"The aim of this paper is to analyze the applicability of crowding differential evolution to unsupervised clustering. The basic idea of this approach, interpreting the clustering problem as a multi-modal optimization one, is similar to that of unsupervised niche clustering proposed by Nasraoui et al. (2005) but instead of evolving only the clusters centers and statistically estimating the other parameters (scales and orientation) we evolve both the centers and the scale parameters of the clusters. Moreover, to simplify the evolutionary process, especially in the case of high-dimensional data, we evolve only hyper-ellipsoids parallel with the axes. In order to describe rotated clusters we used a multi-center representation, i.e. the cluster is covered by several normally oriented hyper-ellipsoids. Besides the fact that it simplifies the evolutionary process, this multi-center representation allows describing almost arbitrary shaped clusters. Preliminary experimental results suggest that the proposed approach ensures a reliable identification of clusters in noisy data providing in the same time multi-center synthetic descriptions for them.","Clustering algorithms,
Partitioning algorithms,
Density functional theory,
Density measurement,
Mathematics,
Computer science,
Parameter estimation,
Multi-stage noise shaping,
Clustering methods,
Constraint optimization"
Mixture segmentation and background suppression in chemosensor arrays with a model of olfactory bulb-cortex interaction,"We present a model of olfactory bulb-cortex interaction for the purpose of mixture processing with gas sensor arrays. The olfactory bulb is modeled with a neurodynamic model whose lateral inhibitory connections are learned through a modified Hebbian-anti-Hebbian rule. Bulbar outputs are then projected in a non-topographic fashion onto the olfactory cortex. Associational connections within cortex using Hebbian learning form a content addressable memory. Finally, inhibitory feedback from cortex is used to modulate bulbar activity. Depending on the form of feedback, Hebbian or anti-Hebbian, the model is able to perform background suppression or mixture segmentation. The model is validated on experimental data from a gas sensor array.","Olfactory,
Sensor arrays,
Neurons,
Brain modeling,
Gas detectors,
Neurodynamics,
Neurofeedback,
Computational modeling,
Computer science,
Hebbian theory"
Extended analysis with reduced pessimism for systems with limited parallelism,"Under limited parallelism, processes competing for a single processor may issue at any time operations on remote co-processors, during which the processor is not idled but granted to other ready processes instead. We reduce the pessimism in existing worst-case response time (WCRT) analysis for such systems by examining temporal patterns of local/remote execution. We extend to multi-CPU variants of the model and offer a WCRT-based feasibility test for symmetric multiprocessor (SMP) systems.","Interference,
Coprocessors,
Parallel processing,
Reconfigurable logic,
Pattern analysis,
Equations,
Computer aided software engineering,
Computer science,
Delay,
System testing"
An improved low computation cost user authentication scheme for mobile communication,"Most of existing user authentication schemes require high computation cost of exponentiation operations for the purpose of security. However, it is not suitable for mobile devices. In this article, we propose a low computation cost user authentication scheme for mobile communication. Our scheme uses only one-way hash functions and smart cards and can be implemented efficiently. The proposed scheme not only resolves the weakness appeared in existing methods but also suits for mobile communication.","Computational efficiency,
Authentication,
Mobile communication,
Smart cards,
Computer science,
Mobile computing,
Network servers,
Testing,
Computer security,
Information security"
Promoting diversity using migration strategies in distributed genetic algorithms,This paper presents a new migration strategy that improves the overall quality of solutions in a distributed genetic algorithm (DGA) involving a number of concurrently evolving populations. The idea behind this improvement is to incorporate a diversity guided selection mechanism that selects a diverse set of individuals for migration from the evolving populations. To accompany this selection mechanism an alternative replacement policy which replaces individuals that have more than one of their copies present in the population (clones) is also investigated. This increases diversity within a population and reduces premature convergence. Results show that it leads to a better performance when compared with the send-best-replace-worst strategy.,
Unifying computer forensics modeling approaches: a software engineering perspective,"As an effort to introduce formalism into computer forensics, researchers have presented various modeling techniques for planning, analysis, and documentation of forensics activities. These modeling techniques provide representations of various forensics subjects such as investigative processes, chain of events, and evidence tests. From a software engineering perspective, it seems that several of these computer forensics modeling approaches may be unified to create a more complete, multi-view modeling methodology for examination planning and analysis. This paper proposes a core set of modeling views for a unified computer forensics modeling methodology: investigative process view, case domain view, and, evidence view. An example email threat case scenario is used as the context for a multi-view modeling example.","Forensics,
Software engineering,
Unified modeling language,
Concrete,
Military computing,
Computer science,
Documentation,
Testing,
Context modeling,
Application software"
Feature subset selection for support vector machines using confident margin,"The aim of this study is to develop a feature subset selection (FSS) method based on the margin of support vector machines (SVM). The problem of directly using the SVM margin is that it does not always provide clear relationship between its value and the performance of SVM, and the best obtained subset is not guaranteed to be the best possible one. In this paper, a new solution is describe by the introduction of the confident margin (CM) in the subset criterion, which permits to get near the best recognition rate by monitoring the peak of CM curve without directly calculating the recognition rate, in order to save computational time. The performance of the proposed method was evaluated in artificial and real-world data experiments.","Support vector machines,
Support vector machine classification,
Frequency selective surfaces,
Pattern recognition,
Filters,
Computer science,
Electronic mail,
Monitoring,
Iron,
Algorithm design and analysis"
Hierarchical navigation interface: leveraging multiple coordinated views for level-of-detail multiresolution volume rendering of large scientific data sets,"We present a new hierarchical navigation interface for level-of-detail selection and rendering of multiresolution volumetric data. The interface consists of multiple coordinated views based on concepts from information visualization as well as scientific visualization literature. With key features such as brushing and linking, and focus and context, it gives the users full control over the level-of-detail selection when navigating through large multiresolution data hierarchies. The navigation interface can also be integrated with traditional level-of-detail selection methods for more effective visual data exploration. We test the utility and effectiveness of this hierarchical navigation interface on a couple of large-scale three-dimensional steady and time-varying data sets.","Navigation,
Rendering (computer graphics),
Data visualization,
Graphical user interfaces,
Large-scale systems,
Hardware,
Automatic control,
Chaos,
Joining processes,
Focusing"
Sensor Network Design and Implementation for Health Telecare and Diagnosis Assistance Applications,"The attempts to develop a ubiquitous health care monitoring system arisen from the need of automatic real-time medical services for emergent diseases. Besides, the physiological statuses gathered and maintained by this system are very helpful for diagnosis and early warning. To improve the medical services and diagnosis accuracy, a Wireless Health Advanced Mobile Bio-diagnostic System (abbreviated as WHAM-BioS) is proposed. This study focuses on network/communication technology in the WHAM-BioS and proposes a novel clustered sensor network (CSN) architecture for long-term periodical telecare applications. In the proposed CSN architecture, most network functions are concentrated in a special purpose device called the human body gateway (HBG). The sensor nodes focus on detecting and reporting their detection results to their HBG. To reduce the design complexity and the implementation cost for the sensor nodes, the proposed architecture proposed several protocols to help each HBG to provide a contention free environment for their sensor nodes. The contention free environment significantly reduces the power consumption in data retransmission. Besides, to further reduce the power consumption of the sensor nodes, this study also proposes a power saving mechanism, which reduces the power consumption in idle listening. Based on the proposed network architecture and protocols, a prototype system is implemented",
Improving requirements engineering communication in multiproject environments,"In complex, multiproject environments, communication is the key to successful requirements engineering. An information model helps with this problem by capturing stakeholders documents and responsibilities during RE. The information model effectively and practically ensures that stakeholders of dependent projects are mutually aware of critical communication needs. The author present the information model developed at Nokia Smart Traffic Products and shows how they defined it in a two-day workshop.","Project management,
Marketing management,
Software development management,
Conferences,
Traffic control,
Computer science,
Software engineering,
Processor scheduling,
Marketing and sales,
Software design"
Using occurrence properties of defect report data to improve requirements,"Defect reports generated for faults found during testing provide a rich source of information regarding problematic phrases used in requirements documents. These reports indicate that faults often derive from instances of ambiguous, incorrect or otherwise deficient language. In this paper, we report on a method combining elements of linguistic theory and information retrieval to guide the discovery of problematic phrases throughout a requirements specification, using defect reports and correction requests generated during testing to seed our detection process. We found that phrases known from these materials to be problematic have occurrence properties in requirements documents that both allow the direction of resources to prioritize their correction, and generate insights characterizing more general locations of difficulty within the requirements. Our findings lead to some recommendations for more efficiently and effectively managing certain natural language issues in the creation and maintenance of requirements specifications.",
An efficient program for phylogenetic inference using simulated annealing,"Inference of phylogenetic trees comprising thousands of organisms based on the maximum likelihood method is computationally expensive. A new program RAxML-SA (Randomized Axelerated Maximum Likelihood with Simulated-Annealing) is presented that combines simulated annealing and hill-climbing techniques to improve the quality of final trees. In addition, to the ability to perform backward steps and potentially escape local maxima provided by simulated-annealing, a large number of ""good"" alternative topologies is generated- which can be used to build a consensus tree on the fly. Though, slower than some of the fastest hill-climbing programs such as RAxML-III and PHYML, RAxML-SAfinds better trees for large real data alignments containing more than 250 sequences. Furthermore, the performance on 40 simulated500-taxon alignments is reasonable in comparison to PHYML. Finally, a straight-forward and efficient OpenMP parallelization of RAxML is presented.","Phylogeny,
Simulated annealing,
Organisms,
Computational modeling,
Sequences,
Topology,
Biology computing,
Computer science,
History,
DNA"
Successive Interference Cancellation for Hierarchical Parallel Optical Wireless Communication Systems,"A parallel optical wireless communication system using two-dimensional LED array (2D LED array) and two-dimensional image sensor (2D image sensor) was proposed for the visible light communication systems. In this system, each LED of the 2D LED array is individually modulated, and the 2D image sensor recognizes each LED modulated data. However, the received data pattern is degraded due to the reduction of pixel size and/or defocusing of the LED data pattern. This phenomenon corresponds to the degradation of the high spatial frequency components of the received data pattern. To overcome this, we employ a hierarchical transmission scheme at a transmitter and a successive interference cancellation at a receiver. By allocating high priority data to low frequency components and low priority data to high frequency components, the reception of high priority data can be guaranteed. We also attempt to apply interference cancellation to overcome the degradation of high spatial frequency components of the received data pattern. By evaluating the bit error rate, we clarify the effect of the proposed system",
FSM-based programmable memory BIST with macro command,"We propose a structured design methodology to construct FSM-based programmable memory BIST. The proposed BIST can be programmed on-line, with a ""macro command"", to select a test algorithm from a predetermined set of algorithms that are built in the memory BIST. In general, there are a variety of heterogeneous memory modules in SOC, and it is not possible to test all of them with a single algorithm. Thus, the proposed scheme greatly simplifies the testing process. Besides, the proposed is more efficient in terms of circuit size and test data to be applied, and it requires less time to configure the BIST. We also develop a programmable memory BIST generator that automatically produces RTL model of the proposed BIST architecture for a given set of test algorithms. Experimental results show that the proposed method achieves a good flexibility with smaller circuit size compared with previous methods.","Built-in self-test,
Automatic testing,
Circuit testing,
Costs,
Algorithm design and analysis,
Circuit faults,
Automatic control,
Computer science,
Design methodology,
Flexible printed circuits"
Hybrid intelligent systems: selecting attributes for soft-computing analysis,"It is difficult to provide significant insight into any hybrid intelligent system design. We offer an informative account of the basic ideas underlying hybrid intelligent systems. We propose a balanced approach to constructing a hybrid intelligent system for a medical domain, along with arguments in favor of this balance and mechanisms for achieving a proper balance. This first of a series of contributions to hybrid intelligent systems design focuses on selecting attributes for soft-computing analysis. One part of this first contribution in our system is developed. Two definitions, probe and probe reducts, are introduced. Our CDispro algorithm can produce the core attribute and reducts that are essential condition attributes in data sets. Our initial study tests data from the UCI repository and geriatric data from DalMedix. The performance and utility of generated reducts are evaluated by 3-fold cross-validation that illustrates reduced dimensionality and complexity of data sets and processes.","Hybrid intelligent systems,
Medical diagnostic imaging,
Data analysis,
Data mining,
Rough sets,
Computer science,
Mathematics,
Probes,
Testing,
Databases"
The closest substring problem with small distances,"In the closest substring problem k strings s/sub 1/, ..., s/sub k/ are given, and the task is to find a string s of length L such that each string s/sub i/, has a consecutive substring of length L whose distance is at most d from s. The problem is motivated by applications in computational biology. We present two algorithms that can be efficient for small fixed values of d and k: for some functions f and g, the algorithms have running time f(d) /spl middot/ n(O(log d)) and g(d,k) /spl middot/ n(O(log log k)), respectively. The second algorithm is based on connections with the extremal combinatorics of hypergraphs. The closest substring problem is also investigated from the parameterized complexity point of view. Answering an open question from (Evans et al., 2003; Fellows et al.; Gramm et al., 2003), we show that the problem is W[1] hard even if both d and k are parameters. It follows as a consequence of this hardness result that our algorithms are optimal in the sense that the exponent of n in the running time cannot be improved to o(log d) or to o(log log k) (modulo some complexity-theoretic assumptions). Another consequence is that the running time n/sup O(1//spl epsiv/4)/ of the approximation scheme for closest substring presented in (Li et al., 2002) cannot be improved to f(/spl epsiv/) /spl middot/ n/sup c/, i.e. the /spl epsiv/ has to appear in the exponent of n.","NP-hard problem,
Computational biology,
Sequences,
Polynomials,
Combinatorial mathematics,
Pattern matching,
DNA,
RNA,
Proteins,
Approximation algorithms"
Interactive Web service choice-making based on extended QoS model,"Quality of service is a key factor in Web service advertising, choosing and runtime monitoring, which is multi-faceted, fuzzy and dynamic. Current researches on Web service QoS focus on implementation level performance assurance, ignoring domain specific metrics at application level which are also very important to service users. Industry Web service standards lack of QoS expressions, and the support for QoS based service choice-making is very limited. An extended web service QoS model is proposed. Web service QoS metrics are evaluated dynamically according to the service context and the overall QoS is evaluated from multiple metrics according to a configurable fuzzy synthetic evaluation system. A QoS requirement description model is defined to express user's flexible demands on service's performance. An interactive Web service choice-making process is also provided, which includes QoS as a key factor.","Web services,
Quality of service,
Context-aware services,
Computer science,
Advertising,
Runtime,
Computerized monitoring,
Fuzzy systems,
Collaboration,
Performance evaluation"
View-oriented update protocol with integrated diff for view-based consistency,This paper proposes a view-oriented update protocol with integrated diff for efficient implementation of a view-based consistency model which supports a novel view-oriented parallel programming style based on distributed shared memory. View-oriented parallel programming requires the programmer to divide the shared data into views according to the nature of the parallel algorithm and its memory access pattern. The advantage of this programming style is that it offers the potential for the underlying distributed shared memory system to optimize consistency maintenance. The View-oriented update protocol with integrated diff is proposed to exploit this performance potential. This protocol is compared with a traditional diff-based protocol and an existing home-based protocol. Experimental results demonstrate that the performance of the proposed protocol is significantly better than the diff-based protocol and the home-based protocol.,"Programming profession,
Access protocols,
Parallel programming,
Message passing,
Virtual colonoscopy,
Computer science,
Information science,
Parallel algorithms,
Costs,
Optimization"
Efficient analog platform characterization through analog constraint graphs,"We propose a scheme for improving the efficiency of the characterization process for system-level models of analog circuits within the analog platform based design paradigm. We leverage designer knowledge to map basic functional requirements of the circuit into circuit parameters relations so that the sampling space can be significantly reduced. A set of equalities and inequalities in the circuit parameters is used to represent the constraints. A feasible parameter space lies at the intersection of the sets of design parameters that satisfy equalities and inequalities, defining a manifold in the parameter space. We introduce a bipartite graph representation denoted analog constraint graphs (ACG) to represent these constraints. ACGs are instrumental for obtaining a random configuration generator that samples configurations in the manifold. The sampler is automatically translated into executable code to fit the characterization framework starting from a mathematical description of constraints. Results show that the automatically generated samplers are comparable in terms of code efficiency with hand-written ones. Furthermore, a heuristics to generate uniformly distributed configuration enabled by the tools is presented and applied to a complex ADC design, yielding a reduction in power consumption by more than 28%.","Sampling methods,
Signal design,
Design methodology,
Support vector machines,
Character generation,
Computer science,
Analog circuits,
Bipartite graph,
Instruments,
Distributed power generation"
Locality-aware process scheduling for embedded MPSoCs,"Utilizing on-chip caches in embedded multiprocessor-system-on-a-chip (MPSoC) based systems is critical from both performance and power perspectives. While most of the prior work that targets at optimizing cache behavior are performed at hardware and compilation levels, operating system (OS) can also play major role as it sees the global access pattern information across applications. This paper proposes a cache-conscious OS process scheduling strategy based on data reuse. The proposed scheduler implements two complementary approaches. First, the processes that do not share any data between them are scheduled at different cores if it is possible to do so. Second, the processes that could not be executed at the same time (due to dependences) but share data among each other are mapped to the same processor core so that they share the cache contents. Our experimental results using this new data locality aware OS scheduling strategy are promising, and show significant improvements in task completion times.","Processor scheduling,
Hardware,
Computer science,
Power engineering and energy,
System-on-a-chip,
Operating systems,
Network-on-a-chip,
Communication networks,
Engineering profession,
Degradation"
Using aspect oriented techniques to support separation of concerns in model driven development,"Model driven development (MDD) tackles software complexity through the use of models. However, managing relationships and specifying transformations between models at various levels of abstraction are complex tasks. System models tangled with concerns such as security and middleware make it difficult to develop complex systems and specify model transformations. This paper presents an MDD framework that uses aspect oriented techniques to facilitate separation of concerns. We argue that using the framework will simplify both the model development task and the task of specifying transformations. The conceptual model of the framework is presented and illustrated using distributed transactions at the PIM and PSM levels.","Middleware,
Programming,
Unified modeling language,
Scattering,
Application software,
Computer science,
Concrete,
Computer architecture,
Computational modeling,
Computer integrated manufacturing"
A communication architecture for reaching consensus in decision for a large network,"One of the most challenging aspects in applying decentralized detection in sensor networks is the efficient exchange of small messages required for data fusion. In this work, we propose a novel communication architecture for a canonical decentralized detection problem where the sensor nodes exchange continuously their local decisions until consensus is reached among all nodes. Our methodology capitalizes on the observation that the information embedded in the exchanged messages decreases to zero as the decisions gradually converge. By using a data-driven multiple access scheme, we show that the number of channel accesses required for each round of message exchange decreases, following the same trend as the aggregate entropy of the sensor decisions. The main contribution is to show that data-driven multiple access strategies can overcome the backlog of communications that many distributed computing algorithms generate in a wireless network setting","Intelligent networks,
Computer networks,
Distributed computing,
Computer architecture,
Testing,
Sensor phenomena and characterization,
Aggregates,
Entropy,
Wireless sensor networks,
Computer science"
"The distinctiveness, detectability, and robustness of local image features","We introduce a new method that characterizes typical local image features (e.g., SIFT, phase feature) in terms of their distinctiveness, detectability, and robustness to image deformations. This is useful for the task of classifying local image features in terms of those three properties. The importance of this classification process for a recognition system using local features is as follows: a) reduce the recognition time due to a smaller number of features present in the test image and in the database of model features; b) improve the recognition accuracy since only the most useful features for the recognition task are kept in the model database; and c) increase the scalability of the recognition system given the smaller number of features per model. A discriminant classifier is trained to select well behaved feature points. A regression network is then trained to provide quantitative models of the detection distributions for each selected feature point. It is important to note that both the classifier and the regression network use image data alone as their input. Experimental results show that the use of these trained networks not only improves the performance of our recognition system, but it also significantly reduces the computation time for the recognition process.","Robustness,
Image recognition,
System testing,
Scalability,
Computer science,
Image databases,
Spatial databases,
Character recognition,
Phase detection,
Computer networks"
Zoom on target while tracking,"In this paper, the problem of continuous tracking of moving objects with a PTZ camera is addressed. In particular, the problem of tracking moving objects during zoom phases is solved by using a feature clustering technique. In order to adopt such a method, we need, first, a step where during tracking with a pan&tilt camera we can identify the mobile objects in the monitored scene. Therefore, a set of good trackable features belonging to the selected target is extracted. In this research, we adopt a feature clustering method that is able to discriminate between features associated with the background and features associated with different moving objects. As a result, for each moving object, we have a set of correctly tracked features that is used to track the objects. Experiments have been performed on outdoor environments where either people or vehicles have been tracked. The results highlight how such a technique can be included in a more complex system able to maintain targets in the field of view of the camera, and to zoom on an object of interest when desired.","Target tracking,
Cameras,
Motion detection,
Object detection,
Layout,
Mathematics,
Computer science,
Object recognition,
Monitoring,
Clustering methods"
A fault localized scheme for false report filtering in sensor networks,"Sensor networks frequently deploy many tiny and inexpensive devices over large regions to detect events of interest. It can be easy to compromise sensors, enabling attackers to use the keys and other information stored at the sensors to inject false reports, forging fake events. Existing approaches do not localize the impact of such node compromises, so that compromises in one sensing region may compromise other parts of the system. In this paper, we propose two fault localized schemes for false report filtering. In our basic scheme, sensors signal events using one-way hash chains, which allows en-route nodes to verify the authenticity of received reports based on commitments of detecting sensors, but prevents them from forging events. We extend this basic scheme to a collaborative filtering scheme using commitment predistribution, making it more adaptable for mobile sensor networks and high-density sensor networks. Our scheme can also provide localized protection for areas that require special protection. Our security analysis shows that our schemes can offer stronger security protection than existing schemes, and are efficient.","Filtering,
Intelligent networks,
Event detection,
Protection,
Wireless sensor networks,
Cryptography,
Computer science,
Fault detection,
Fires,
Telecommunication traffic"
A Web services approach to learning path composition,"The increasing availability of open and large e-learning repositories and the advent of distributed technologies such as Web services have led to a renewed interest in distributed e-learning. Composition rather than aggregation is seen as a way forward in the design and implementation of learning paths. In this paper, a framework for courseware composition is presented in terms of workflow management and semantic Web technologies. In order to cater for the needs of learners and to provide flexible learning content, a compositional platform, BPEL4WS, is enhanced by the introduction of virtual Web services, which are initially unbound. The resolution of these services is performed by agents, through search, discovery and late binding. This approach has the advantage that it overcomes some of the limitations of BPEL4WS and that it decouples the design of learning paths from the delivery of learning content. It also enhances adaptivity in e-learning provision.","Web services,
Electronic learning,
Courseware,
Semantic Web,
Standards development,
Computer science,
Availability,
Technology management,
Filters,
Ontologies"
Mappings for accurately reverse engineering UML class models from C++,"The paper introduces a number of mapping rules for reverse engineering UML class models from C++ source code. The mappings focus on accurately identifying such elements as relationship types, multiplicities, and aggregation semantics. These mappings are based on domain knowledge of the C++ language and common programming conventions and idioms. An application implementing these heuristics is used to reverse engineer a moderately sized open source, C++ application, and the resultant class model is compared against those produced by other UML reverse engineering applications. A comparison shows that these presented mapping rules effectively produce meaningful, semantically accurate UML models","Reverse engineering,
Unified modeling language,
Application software,
Computer architecture,
Computer science,
Open source software,
Software tools,
Logic,
Libraries,
Testing"
Location-based services in Internet telephony,"Many applications used in the Internet today benefit from using location information. To better handle location information in Internet telephony applications, we did a comprehensive application-layer analysis of location information and location-based communication services. We first summarize and categorize end-user-oriented location description and location detection approaches. We then summarize and categorize how to use location information to provide communication services and introduce several interesting location based communication services. Based on the analysis, we have incorporated location-based service handling in our session initiation protocol (SIP) based Internet telephony infrastructure and our language for end system services (LESS).","Internet telephony,
Application software,
Information analysis,
Privacy,
Web and internet services,
Motion pictures,
Routing,
Humans,
Computer science,
Protocols"
Bootstrapping semantic annotation for content-rich HTML documents,"Enormous amount of semantic data is still being encoded in HTML documents. Identifying and annotating the semantic concepts implicit in such documents makes them directly amenable for semantic Web processing. In this paper we describe a highly automated technique for annotating HTML documents, especially template-based content-rich documents, containing many different semantic concepts per document. Starting with a (small) seed of hand-labeled instances of semantic concepts in a set of HTML documents we bootstrap an annotation process that automatically identifies unlabeled concept instances present in other documents. The bootstrapping technique exploits the observation that semantically related items in content-rich documents exhibit consistency in presentation style and spatial locality to learn a statistical model for accurately identifying different semantic concepts in HTML documents drawn from a variety of Web sources. We also present experimental results on the effectiveness of the technique.","HTML,
Ontologies,
Semantic Web,
Labeling,
Computer science,
Next generation networking,
Pricing,
Resource description framework,
XML,
Vehicles"
An efficient direct mapped instruction cache for application-specific embedded systems,"Caches may consume half of a microprocessor's total power and cache misses incur accessing off-chip memory, which is both time consuming and energy costly. Therefore, minimizing cache power consumption and reducing cache misses are important to reduce total energy consumption of embedded systems. Direct mapped caches consume much less power than that of same sized set associative caches but with a poor hit rate on average. Through experiments, we observe that memory space of direct mapped instruction caches is not used efficiently in most embedded applications. We design an efficient cache - a configurable instruction cache that can be tuned to utilize the cache sets efficiently for a particular application such that cache memory is exploited more efficiently by index remapping. Experiments on 11 benchmarks drawn from Mediabench show that the efficient cache achieves almost the same miss rate as a conventional two-way set associative cache on average and with total memory-access energy savings of 30% compared with a conventional two-way set associative cache.","Embedded system,
Energy consumption,
Delta modulation,
Cache memory,
Algorithm design and analysis,
Cities and towns,
Computer science,
Clocks,
Frequency,
Application specific processors"
P-Hera: scalable fine-grained access control for P2P infrastructures,"In this paper, we present P-Hera, a peer-to-peer (P2P) infrastructure for scalable and secure content hosting. P-Hera allows the users and content owners to dynamically establish trust using fine-grained access control. In P-Hera, resource owners can specify fine-grained restrictions on who can access their resources and which user can access which part of data. We differentiate our work with traditional works of fine-grained access control on Web services, as our system in addition to handling access constraints of the service provider (which is the case in Web services), it also handles security constraints regarding actions performed on data: replication and modification. We believe this is of immense significance for wide-range of applications such as data grids, information grids and Web content delivery networks. In addition to presenting the overall system architecture, we also study the problem of evaluating these fine-grained access policies in depth and propose a novel means of organizing these policies that can result in faster evaluation. We demonstrate the effectiveness of our approach using prototype implementation.","Access control,
Computer science,
Peer to peer computing,
Grid computing,
Web services,
Information security,
Carbon capture and storage,
Data security,
Organizing,
Prototypes"
A Case Study: GQM and TSP in a Software Engineering Capstone Project,"This paper presents a case study, describing the use of a hybrid version of the team software process (TSP) in a capstone software engineering project. A mandatory subset of TSP scripts and reporting mechanisms were required, primarily for estimating the size and duration of tasks and for tracking project status against the project plan. These were supplemented by metrics and additional processes developed by students. Metrics were identified using the goal-question-metric (GQM) process and used to evaluate the effectiveness of project management roles assigned to each member of the project team. TSP processes and specific TSP forms are identified as evidence of learning outcome attainment. The approach allowed for student creativity and flexibility and limited the perceived overhead associated with use of the complete TSP. Students felt that the experience enabled them to further develop and demonstrate teamwork and leadership skills. However, limited success was seen with respect to defect tracking, risk management, and process improvement. The case study demonstrates that the approach can be used to assess learning outcome attainment and highlights for students the significance of software engineering project management","Software engineering,
Project management,
Quality management,
Teamwork,
Process planning,
Risk management,
Computer science,
Information technology,
Software measurement,
Process design"
Mobile agents in a multi-agent e-commerce system,"Among features often attributed to software agents are autonomy and mobility. Autonomy of e-commerce agents involves adaptability to engage in negotiations governed by mechanisms not known in advance, while their mobility entails such negotiations taking place at remote locations. This paper aims at combining adaptability with mobility, by joining rule-based mechanism representation with modular agent design, and at UML-formalizing selected aspects of the resulting system. Furthermore, we discuss the issue of agent mobility and argue why such agents have been proposed for the system under consideration.","Mobile agents,
Intelligent agent,
Computer science,
Software agents,
Software engineering,
Resource management,
Stock markets,
Network servers,
Autonomous agents,
Humans"
Privacy and Access Control Issues in Financial Enterprise Content Management,"Financial Enterprise Content Management Systems (FECMS) have been recently deployed not only in intra-enterprises but also over the Internet to interact with customers. As FECMS contains a lot of sensitive and confidential information, there is an urgent need for tackling privacy and access control issues in these systems. In this paper, we proceed with our case study in an international banking enterprise on these issues. The FECMS is based on Web services technologies and we demonstrate the key privacy and access control policies for internal content flow management (such as content editing, approval, and usage) as well as external access control for Web portal and institutional programmatic users. Through the modular design of an integrated FECMS, we illustrate how we can systematically specify privacy and access control policies in each part of the system with the technology of Enterprise Privacy Authorization Language (EPAL)..",
An empirical assessment of function point-like object-oriented metrics,"Since object-oriented programming became a popular development practice, researchers and practitioners have defined several techniques aimed at measuring object-oriented software. Among these, several function point-like approaches have been proposed. However, mapping the concepts at the basis of function point analysis onto object-oriented concepts is not straightforward; therefore, there is the need to test the validity of FP-based object-oriented metrics. This paper presents an analysis of a set of programs developed by masteral students of a software engineering course employing object-oriented techniques (UML and Java). Different kinds of FP-based object-oriented metrics were applied, and the results analysed. The work done addresses questions like the following: is there a correlation between object-oriented FPs and LOCs? How do object-oriented FPs compare with the function points defined by Albrecht? How do object-oriented FPs compare with non FP-like OO metrics? How do object-oriented FPs compare with each other?",
Almost-Sensorless Localization,"We present a localization method for robots equipped with only a compass, a contact sensor and a map of the environment. In this framework, a localization strategy can be described as a sequence of directions in which the robot moves maximally. We show that a localizing sequence exists for any simply connected polygonal environment by presenting an algorithm for computing such a sequence. We have implemented the algorithm and we present several computed examples. We also show that the sensing model is minimal by showing that replacement of the compass by an angular odometer precludes the possibility of performing localization.","Robot sensing systems,
Uncertainty,
Navigation,
Orbital robotics,
Computer science,
Costs,
Robustness,
Decision trees,
Greedy algorithms"
An instruction set architecture based code compression scheme for embedded processors,"Summary form only given. We propose a general purpose code compression scheme for embedded systems, based on the instruction set architecture and report results on the Intel StrongARM, a low-cost, low-power RISC architecture and TI TMS320C62x, a widely used VLIW architecture. Fast decompression techniques are explored to improve the decompression overhead of the compression scheme. Compression ratios ranging from 68% to 75% were obtained for TMS320C62x and 69% to 78% for the StrongARM processor. The basic idea of the compression scheme is to divide the instructions into different logical classes and to build multiple dictionaries for them. The size and the number of multiple dictionaries are fixed for a given processor and are determined by the partitioning algorithm which works over the instruction set architecture supplied as input. Frequently occurring unique instruction segments are inserted into the dictionaries and the instructions are encoded as pointers to the respective entries. An opcode, which helps in fast decompression, is attached to an instruction segment to identify its logical class and the dictionary to be accessed.","Dictionaries,
Computer architecture,
VLIW,
Data compression,
Computer science,
Automation,
Embedded system,
Reduced instruction set computing,
Hamming distance,
Decoding"
A framework for testing Web services and its supporting tool,"With the increase of the popularity of Web services, more and more Web applications are developed with this new kind of components. This new way of software development brings about new issues for software testing, which has been widely recognized as a realistic means for ensuring the quality of software systems. In this paper, we focus on facilitating the testing of Web services. In particular, we propose a framework for testing Web services, which can help a tester of Web services in two ways: firstly, it can help the tester to acquire effective test data; and secondly, it can help the tester to execute the test data for the testing. We also discuss some issues for the implementation of its supporting tool. Furthermore, we report an experimental study of our framework. The results can validate the effectiveness of our approach.","Web services,
Simple object access protocol,
Software testing,
Automatic testing,
Application software,
Runtime,
Programming,
User interfaces,
Databases,
Computer science"
Experimental Evaluation and Pilot Assessment Study of a Virtual and Remote Laboratory on Robotic Manipulation,,"Remote laboratories,
Educational robots,
Application software,
Computer science education,
Systems engineering education,
Protocols,
Video sharing,
Streaming media,
Space technology,
Educational programs"
Logical topology design for dynamic traffic grooming in mesh WDM optical networks,"Traffic grooming is an operation to consolidate client traffic onto lightpaths in the interworking of the optical network and client networks. Depending on whether the client traffic is static or dynamic, it can be classified into static and dynamic traffic grooming. This paper studies how to design logical topology (using minimum network resource) for dynamic traffic grooming, to meet the given traffic blocking probability requirements. We will formulate this problem into an integer linear programming (ILP) problem. In the formulation, we will consider wavelength assignment for lightpaths, and wavelength conversion in the optical network. The formulation is demonstrated to be highly effective for small to medium-size networks. Furthermore, for large networks, we propose a simple heuristic that can obtain near-optimal performance.","Network topology,
Optical design,
Telecommunication traffic,
Intelligent networks,
Wavelength division multiplexing,
WDM networks,
Optical fiber networks,
Asynchronous transfer mode,
Bandwidth,
Computer science"
