Title,Abstract,Keywords
Data Mining and Analytics in the Process Industry: the Role of Machine Learning,"Data mining and analytics have played an important role in knowledge discovery and decision making/supports in the process industry over the past several decades. As a computa-tional engine to data mining and analytics, machine learning serves as basic tools for information extraction, data pattern recognition and predictions. From the perspective of machine learning, this paper provides a review on existing data mining and analytics applications in the process industry over the past several decades. The state-of-the-art of data mining and analytics are reviewed through eight unsupervised learning and ten supervised learning algorithms, as well as the application status of semi-supervised learning algorithms. Several perspectives are highlighted and discussed for future researches on data mining and analytics in the process industry.","Data mining,
Industries,
Data models,
Machine learning algorithms,
Analytical models,
Manufacturing,
Predictive models"
Automatically Classifying Functional and Non-functional Requirements Using Supervised Machine Learning,"In this paper, we take up the second RE17 data challenge: the identification of requirements types using the ""Quality attributes (NFR)"" dataset provided. We studied how accurately we can automatically classify requirements as functional (FR) and non-functional (NFR) in the dataset with supervised machine learning. Furthermore, we assessed how accurately we can identify various types of NFRs, in particular usability, security, operational, and performance requirements. We developed and evaluated a supervised machine learning approach employing meta-data, lexical, and syntactical features. We employed under-and over-sampling strategies to handle the imbalanced classes in the dataset and cross-validated the classifiers using precision, recall, and F1 metrics in a series of experiments based on the Support Vector Machine classifier algorithm. We achieve a precision and recall up to ~92% for automatically identifying FRs and NFRs. For the identification of specific NFRs, we achieve the highest precision and recall for security and performance NFRs with ~92% precision and ~90% recall. We discuss the most discriminating features of FRs and NFRs as well as the sampling strategies used with an additional dataset and their impact on the classification accuracy.","Training,
Usability,
Security,
Feature extraction,
Vegetation,
Support vector machines"
MACORD: Online Adaptive Machine Learning Framework for Silent Error Detection,"Future high-performance computing (HPC) systems with ever-increasing resource capacity (such as compute cores, memory and storage) may significantly increase the risks on reliability. Silent data corruptions (SDCs) or silent errors are among the major sources that corrupt HPC execution results. Unlike fail-stop errors, SDCs can be harmful and dangerous in that they cannot be detected by hardware. To remedy this, we propose an online MAchine-learning-based silent data CORruption Detection framework (abbreviated as MACORD) for detecting SDCs in HPC applications. In our study, we comprehensively investigate the prediction ability of a multitude of machine-learning algorithms and enable the detector to automatically select the best-fit algorithms at runtime to adapt to the data dynamics. Because it takes only spatial features (i.e., neighboring data values for each data point in the current time step) into the training data, our learning framework exhibits low memory overhead (less than 1%). Experiments based on real-world scientific applications/benchmarks show that our framework can elevate the detection sensitivity (i.e., recall) up to 99%. Meanwhile the false positive rate is limited to 0.1% in most cases, which is one order of magnitude improvement compared with the latest state-of-the-art spatial technique.","Training,
Algorithm design and analysis,
Heuristic algorithms,
Vegetation,
Prediction algorithms,
Machine learning algorithms,
Mathematical model"
TGE: Machine Learning Based Task Graph Embedding for Large-Scale Topology Mapping,"Task mapping is an important problem in parallel and distributed computing. The goal in task mapping is to find an optimal layout of the processes of an application (or a task) onto a given network topology. We target this problem in the context of staging applications. A staging application consists of two or more parallel applications (also referred to as staging tasks) which run concurrently and exchange data over the course of computation. Task mapping becomes a more challenging problem in staging applications, because not only data is exchanged between the staging tasks, but also the processes of a staging task may exchange data with each other. We propose a novel method, called Task Graph Embedding (TGE), that harnesses the observable graph structures of parallel applications and network topologies. TGE employs a machine learning based algorithm to find the best representation of a graph, called an embedding, onto a space in which the task-to-processor mapping problem can be solved. We evaluate and demonstrate the effectiveness of TGE experimentally with the communication patterns extracted from runs of XGC, a large-scale fusion simulation code, on Titan.","Topology,
Benchmark testing,
Network topology,
Data visualization,
Resource management,
Heuristic algorithms"
Amazon?s Echo Look: Harnessing the Power of Machine Learning or Subtle Exploitation of Human Vulnerability?,"Here we are in 2017, but, at times, it feels as though we are back in the 1950s. Apparently, how women look and present themselves to the world is so crucial that they must sacrifice their privacy, security, and trust to Amazon's algorithms, just to gain societal acceptance for their fashion choices. Or at least, this is how it appears in the Amazon Echo Look video.","Algorithm design and analysis,
Consumer electronics,
Electronic commerce,
Market research,
Computer security,
Ethics,
Companies"
The Trust Value Calculating for Social Network Based on Machine Learning,"In this paper, a social network model is built for the social network information in the social network, and the machine learning method is used to calculate the node trust value. First, the results calculated by the traditional node trust value calculation method and some auxiliary information are used as the training feature of the machine learning, and the measurement whether there is edge between nodes as label information. Second, the node logistic regression model is used as the training model to calculate the node trust value. Then, recommendation algorithm which is analogous to the user collaborative filtering algorithm is used to calculate node trust value. At last, the simulation is used to verify the performance of the improved method, and the results show that the prediction accuracy of node trust value computing by improved algorithm is significantly higher than that of node trust value computing by formula.","Training,
Social network services,
Logistics,
Prediction algorithms,
Indexes,
Machine learning algorithms,
Predictive models"
Japanese Fingerspelling Recognition Based on Classification Tree and Machine Learning,"Sign language is a very important communication tool for hearing-impaired people and also for the communication between hearing-impaired and non-handicapped people. There are many methods for sign language recognition, some of which are based on Hidden Markov Model (HMM) and others are based on Support Vector Machine (SVM) and so forth. In fact, the most of previous methods recognize fingerspelling using video sequence because some fingerspellings are accompanied by movement. Some methods use Microsoft Kinect or Leap Motion controller to obtain the finger movement. Some fingerspellings, however, are not accompanied by movement and can be recognized with just one snap shot of fingerspelling. Therefore, this paper proposes a recognition method of fingerspelling without movement. The target fingerspellings are 41 characters without movement in Japanese sign language, and the method uses only one picture. Some of fingerspellings are easily recognized and others are not so that the method is based on pattern recognition using classification tree and machine learning with SVM for easily recognized fingerspellings and difficultly recognized ones, respectively. As the result of the experiment, the averaged recognition rate was 86%.","Gesture recognition,
Assistive technology,
Support vector machines,
Feature extraction,
Character recognition,
Hidden Markov models"
A fully configurable and scalable neural coprocessor IP for SoC implementations of machine learning applications,"This paper presents a fully configurable and programmable coprocessor IP core to efficiently compute Artificial Neural Networks (ANNs) in heterogeneous System-on-Chips (SoC). There is an increasing interest in moving applications involving streamed data such as those arising in machine-learning systems (machine-vision, speech-recognition, etc.) to highly-integrated low-power embedded devices. In this context efficient memory utilization, which is critical to both performance and energy consumption, requires handling multiple memory resources and different address spaces. The proposed IP core cooperates with a companion interconnect IP that acts as an abstraction layer to allow seamless integration of such heterogeneous resources. Dynamic (re)allocation is supported, so that embedded memory resources (Block RAMs) and external blocks (DDR memory) can be transparently used. The coprocessor itself is a configurable vector-matrix product computer with optional non-linear filtering, thus any single layer and multilayer ANN model can be easily implemented. The applicability of the IPs is demonstrated in a Hyperspectral Image (HSI) real-time classification problem.","Coprocessors,
Computer architecture,
IP networks,
Program processors,
Field programmable gate arrays,
Hardware,
Clocks"
Category Classification of Text Data with Machine Learning Technique for Visualizing Flow of Conversation in Counseling,"The beginner counselors have more likely to continue counseling in their own interest, they have a high tendency to make great use of the closed-ended question in order to confirm the interpretation with the client. While expert counselors are instructing the counseling skill to beginner counselors, we consider that the reaction of a client for a beginner counselor's question is important to visualize in an appropriate method. To respond the request, we have developed a system for visualizing the flow of conversation in counseling. However, the expert counselor as the system user requires to correct the initial classification result manually, and the work burden is large, because the accuracy of the category classification of conversation data is very low in the current system. To improve this problem, we have implemented on the category classification method of text data with SVM (Support Vector Machine) as machine learning technique to visualize the flow of conversation in counseling. In addition, we have compared and evaluated with results of the initial classification method of the current system. As these results, we have shown that the accuracy rate of the classification method with SVM become higher than the results in the current system.","Employee welfare,
Support vector machines,
Data visualization,
Psychology,
Dictionaries,
Data models,
Employment"
Understanding the feasibility of machine learning algorithms in a game theoretic environment for dynamic spectrum access,"The key enabling technology in dynamic spectrum access is Cognitive Radio that allows unlicensed secondary users to access the licensed bands without causing any interference to the primary users. In any situation where there are a certain number of secondary networks trying to get an available channel, there arises a game theoretic competition where they want to get the channel for themselves by incurring as minimum cost as possible. The increase in cost is equivalent to the increase in time caused by the need of a search for an available channel. This process could be sped up if the networks had a predictive mechanism to determine the optimal strategy. In this paper, we investigate various predictive algorithms: Linear regression, Support Vector Regression and Elastic Net and compare them with other traditional non-predictive game theoretic mechanisms. We measure the accuracy of these algorithms in terms of time taken to reach the system convergence. We also observe how a self-learning approach can be helpful in maximizing utilities of the players in comparison to traditional game theoretic approaches.","Games,
Switches,
Machine learning algorithms,
Interference,
Prediction algorithms,
Dynamic spectrum access,
Cognitive radio"
Machine learning approach for optimal determination of wave parameter relationships,"Wave parameter relationships have long been determined using methods that give non-standard and often inaccurate results. With increased commercial activity in the marine sector, the importance of accurate wave parameter relationship determination has become increasingly apparent. The outputs of many numerical models and buoy datasets do not include all requisite wave parameters, and a typical approach is to use a constant conversion factor or relationship based on defined spectra such as the Bretschneider or the joint North Sea wave observation project (JONSWAP) spectrum to determine these parameters. Given that relationships between wave parameters vary significantly over both hourly and seasonal and annual timescales, the currently employed methods are lacking, as subtleties are missed by the simpler approach. This paper addresses the determination of wave parameter relationships using a machine learning (ML)-based model, identifying and selecting the optimal method for the conversion of wave parameters (Te, T01) in coastal Irish Waters. This approach is then validated at two sites on the West coast of Ireland. The aim is to highlight the utility of ML in approximating the relationship between wave parameters; using both buoy and modelled data, and mapping the predicted outcomes for a wave energy converter based on a variety of ML and measure correlate predict approaches.","learning (artificial intelligence),
ocean waves,
power engineering computing,
wave power generation"
Audio Classification Method Based on Machine Learning,"Audio classification has very large theoretical and practical values in both pattern recognition and artificial intelligence. In this paper, we propose a novel audio classification method based on machine learning technique. Firstly, we illustrate the hierarchical structure of audio data, which is made up of four layers: 1) Audio frame, 2) Audio clip, 3) Audio shot, and 4) Audio high level semantic unit. Secondly, three types of audio data feature are extracted to construct feature vector, including 1) Short time energy, 2) Zero crossing rate and 3) Mel-Frequency cepstral coefficients. Thirdly, we discuss how to classify audio data using the SVM classifier with Gaussian kernel. Finally, experimental results demonstrate that the proposed method is able to achieve higher audio classification accuracy.","Transportation,
Big Data,
Smart cities"
Matheuristic with machine-learning-based prediction for software-defined mobile metro-core networks,"In general, humans follow a routine with highly predictable daily movements. For instance, we commute from home to work on a daily basis and visit a selected set of places for commercial and recreational purposes during the nights and weekends. The use of mobile phones increases when commuting via public transportation, during lunch breaks, and at night. Such regular behavior creates predictable spatiotemporal fluctuations of traffic patterns. In this paper, we introduce a matheuristic for dynamic optical routing, which can be implemented as an application into a software-defined mobile carrier network. We use machine learning to predict tidal traffic variations in a mobile metro-core network, which allows us to solve off-line mixed-integer linear programming instances of an optical routing (and wavelength) assignment optimization problem. The optimal results are used to favor near-optimal on-line routing decisions. Results demonstrate the effectiveness of our on-line methodology, with results that match almost perfectly the behavior of a network that performs optical routing reconfiguration with a perfect, oracle-like traffic prediction and the solution of an optimization problem.","Routing,
Mobile communication,
Planning,
Mobile computing,
Prediction algorithms,
Optimization,
Heuristic algorithms"
Machine Learning Techniques for Analyzing Training Behavior in Serious Gaming,"Training time is a costly, scarce resource across domains such as commercial aviation, healthcare, and military operations. In the context of military applications, serious gaming -- the training of warfighters through immersive, real-time environments rather than traditional classroom lectures -- offers benefits to improve training not only in its hands-on development and application of knowledge, but also in data analytics via machine learning. In this paper, we explore an array of machine learning techniques that allow teachers to visualize the degree to which training objectives are reflected in actual play. First, we investigate the concept of discovery: learning how warfighters utilize their training tools and develop military strategies within their training environment. Second, we develop machine learning techniques that could assist teachers by automatically predicting player performance, identifying player disengagement, and recommending personalized lesson plans. These methods could potentially provide teachers with insight to assist them in developing better lesson plans and tailored instruction for each individual student.","Games,
Training,
Tools,
Hidden Markov models,
Measurement,
Missiles"
A Comparison of Distributed Machine Learning Platforms,"The proliferation of big data and big computing boosted the adoption of machine learning across many application domains. Several distributed machine learning platforms emerged recently. We investigate the architectural design of these distributed machine learning platforms, as the design decisions inevitably affect the performance, scalability, and availability of those platforms. We study Spark as a representative dataflow system, PMLS as a parameter- server system, and TensorFlow and MXNet as examples of more advanced dataflow systems. We take a distributed systems perspective, and analyze the communication and control bottlenecks for these approaches. We also consider fault-tolerance and ease-of-development in these platforms. In order to provide a quantitative evaluation, we evaluate the performance of these three systems with basic machine learning tasks: logistic regression, and an image classification example on the MNIST dataset.","Sparks,
Computational modeling,
Servers,
Training,
Computer architecture,
Data models,
Machine learning algorithms"
Machine-Learning Based Threat-Aware System in Software Defined Networks,"Software-Defined Networking (SDN) is an emerging network architecture that decouples the control plane and the data plane to provide unprecedented programmability, automation, and network control. The SDN controller exercises centralized control over network software, and in doing so, it can monitor and respond to malicious traffic for network protection. This paper proposes a threat-aware system based on machine-learning for timely detection and response against network intrusion in SDN. Our proposed system consists of data preprocessing for feature selection, predictive data modeling for machine-learning and anomaly detection, and decision making for intrusion response in SDN. Due to the time-critical nature of SDN, we propose a practical approach utilizing machine-learning techniques to protect against network intrusion and reduce uncertainty in decision-making outcomes. The maliciousness of most uncertain network traffic subsets is evaluated with selected significant feature sets. Our experimental results show that the proposed approach achieves high performance and significantly reduces uncertainty in the decision process with a small number of feature sets. The results help the SDN controller to properly react against known or unknown attacks that cannot be prevented by signature-based network intrusion detection systems.","Intrusion detection,
Data models,
Predictive models,
Routing,
Real-time systems,
Classification algorithms,
Systems architecture"
Machine-Learning Classifiers for Security in Connected Medical Devices,"Medical devices equipped with wireless connectivity and remote monitoring features are increasingly becoming connected to each other, to an outside programmer and even to the Internet. While Internet of Things technology enables health-care professionals to fine tune or modify medical device settings without invasive procedures, this also opens up large attack surfaces and introduces potential security vulnerabilities. Medical device hacks are slowly becoming a reality and it becomes more critical than ever to defend and protect these devices from security attacks. In this paper, we assess the feasibility of using machine learning models to efficiently determine attacks targeted on a medical device. Specifically, we develop feature sets to accurately profile a medical device and observe any deviation from its normal behavior. We test our method using different machine learning algorithms and provide a comparison analysis of the detection results.","Security,
Medical diagnostic imaging,
Logic gates,
Wireless communication,
Communication system security,
Monitoring,
Machine learning algorithms"
Machine learning based prediction of thermal comfort in buildings of equatorial Singapore,"Majority of energy consumption in Singapore buildings is due to air-conditioning, because of its hot and humid weather. Besides attaining a healthy indoor environment, a prior knowledge about the occupant's thermal comfort can be beneficial in reducing energy consumption, as it can save energy which is otherwise spent in extra cooling. This paper proposes a data-driven approach to predict individual thermal comfort level (‘cool-discomfort’, ‘comfort’, ‘warm-discomfort’) using environmental and human factors as input. Six types of classifiers have been implemented-Support Vector Machine (SVM), Artificial Neural Network (ANN), Logistic Regression (LR), Linear Discriminant Analysis (LDA), K-Nearest Neighbors (KNN), and Classification Trees (CT), on a publicly available database of 817 occupants for air-conditioned and free-running buildings separately. Results show that our approach achieves prediction accuracies of 73.14–81.2%, outperforming the traditional Fanger's PMV (Predicted Mean Vote) model, which has accuracies of only 41.68–65.5%. Age, gender, and outdoor effective temperature, which are not included in the PMV model, are found to be important factors for thermal comfort. The proposed approach also outperforms modified PMV models-the extended PMV model and the adaptive PMV model which attain accuracies of 61.75% and 35.51% respectively.","Buildings,
Adaptation models,
Meteorology,
Temperature,
Atmospheric modeling,
Support vector machines"
Mesh Convolutional Restricted Boltzmann Machines for Unsupervised Learning of Features With Structure Preservation on 3-D Meshes,"Discriminative features of 3-D meshes are significant to many 3-D shape analysis tasks. However, handcrafted descriptors and traditional unsupervised 3-D feature learning methods suffer from several significant weaknesses: 1) the extensive human intervention is involved; 2) the local and global structure information of 3-D meshes cannot be preserved, which is in fact an important source of discriminability; 3) the irregular vertex topology and arbitrary resolution of 3-D meshes do not allow the direct application of the popular deep learning models; 4) the orientation is ambiguous on the mesh surface; and 5) the effect of rigid and nonrigid transformations on 3-D meshes cannot be eliminated. As a remedy, we propose a deep learning model with a novel irregular model structure, called mesh convolutional restricted Boltzmann machines (MCRBMs). MCRBM aims to simultaneously learn structure-preserving local and global features from a novel raw representation, local function energy distribution. In addition, multiple MCRBMs can be stacked into a deeper model, called mesh convolutional deep belief networks (MCDBNs). MCDBN employs a novel local structure preserving convolution (LSPC) strategy to convolve the geometry and the local structure learned by the lower MCRBM to the upper MCRBM. LSPC facilitates resolving the challenging issue of the orientation ambiguity on the mesh surface in MCDBN. Experiments using the proposed MCRBM and MCDBN were conducted on three common aspects: global shape retrieval, partial shape retrieval, and shape correspondence. Results show that the features learned by the proposed methods outperform the other state-of-the-art 3-D shape features.","Shape,
Machine learning,
Solid modeling,
Feature extraction,
Convolution,
Unsupervised learning"
GPU-Accelerated Parallel Hierarchical Extreme Learning Machine on Flink for Big Data,"The extreme learning machine (ELM) has become one of the most important and popular algorithms of machine learning, because of its extremely fast training speed, good generalization, and universal approximation/classification capability. The proposal of hierarchical ELM (H-ELM) extends ELM from single hidden layer feedforward networks to multilayer perceptron, greatly strengthening the applicability of ELM. Generally speaking, during training H-ELM, large-scale datasets (DSTs) are needed. Therefore, how to make use of H-ELM framework in processing big data is worth further exploration. This paper proposes a parallel H-ELM algorithm based on Flink, which is one of the in-memory cluster computing platforms, and graphics processing units (GPUs). Several optimizations are adopted to improve the performance, such as cache-based scheme, reasonable partitioning strategy, memory mapping scheme for mapping specific Java virtual machine objects to buffers. Most importantly, our proposed framework for utilizing GPUs to accelerate Flink for big data is general. This framework can be utilized to accelerate many other variants of ELM and other machine learning algorithms. To the best of our knowledge, it is the first kind of library, which combines in-memory cluster computing with GPUs to parallelize H-ELM. The experimental results have demonstrated that our proposed GPU-accelerated parallel H-ELM named as GPH-ELM can efficiently process large-scale DSTs with good performance of speedup and scalability, leveraging the computing power of both CPUs and GPUs in the cluster.","Approximation algorithms,
Big Data,
Acceleration,
Training,
Libraries,
Machine learning algorithms,
Clustering algorithms"
A Machine Learning Algorithm for Identifying Atopic Dermatitis in Adults from Electronic Health Records,"The current work aims to identify patients with atopic dermatitis for inclusion in genome-wide association studies (GWAS). Here we describe a machine learning-based phenotype algorithm. Using the electronic health record (EHR), we combined coded information with information extracted from encounter notes as features in a lasso logistic regression. Our algorithm achieves high positive predictive value (PPV) and sensitivity, improving on previous algorithms with low sensitivity. These results demonstrate the utility of natural language processing(NLP) and machine learning for EHR-based phenotyping.","Machine learning algorithms,
Prediction algorithms,
Diseases,
Skin,
Feature extraction,
Gold,
Standards"
A machine learning approach to characterizing the effect of asynchronous distributed electrical stimulation on hippocampal neural dynamics in vivo,"Asynchronous distributed microelectrode theta stimulation (ADMETS) of the hippocampus has been shown to reduce seizure frequency in the tetanus toxin rat model of mesial temporal lobe epilepsy suggesting a hypothesis that ADMETS induces a seizure resistant state. Here we present a machine learning approach to characterize the nature of neural state changes induced by distributed stimulation. We applied the stimulation to two animals under sham and ADMETS conditions and used a combination of machine learning techniques on intra-hippocampal recordings of Local Field Potentials (LFPs) to characterize the difference in the neural state between sham and ADMETS. By iteratively fitting a logistic regression with data from the inter-stimulation interval under sham and ADMETS condition we found that the classification performance improves for both animals until 90s post stimulation before leveling out at AUC of 0.64 ± 0.2 and 0.67 ± 0.02 when all inter-stimulation data is included. The models for each animal were re-fit using elastic net regularization to force many of the model coefficients to 0, identifying those that do not optimally contribute to the classifier performance. We found that there is significant variation in the non-zero coefficients between animals (p < 0.01), suggesting that the ADMETS induced state is represented differently between subject. These findings lay the foundation for using machine learning to robustly and quantitatively characterize neural state.","Animals,
Electrodes,
Data models,
Hippocampus,
Logistics,
Biological system modeling,
Biomarkers"
Comparing deep neural network and other machine learning algorithms for stroke prediction in a large-scale population-based electronic medical claims database,"Electronic medical claims (EMCs) can be used to accurately predict the occurrence of a variety of diseases, which can contribute to precise medical interventions. While there is a growing interest in the application of machine learning (ML) techniques to address clinical problems, the use of deep-learning in healthcare have just gained attention recently. Deep learning, such as deep neural network (DNN), has achieved impressive results in the areas of speech recognition, computer vision, and natural language processing in recent years. However, deep learning is often difficult to comprehend due to the complexities in its framework. Furthermore, this method has not yet been demonstrated to achieve a better performance comparing to other conventional ML algorithms in disease prediction tasks using EMCs. In this study, we utilize a large population-based EMC database of around 800,000 patients to compare DNN with three other ML approaches for predicting 5-year stroke occurrence. The result shows that DNN and gradient boosting decision tree (GBDT) can result in similarly high prediction accuracies that are better compared to logistic regression (LR) and support vector machine (SVM) approaches. Meanwhile, DNN achieves optimal results by using lesser amounts of patient data when comparing to GBDT method.","Electromagnetic compatibility,
Databases,
Prediction algorithms,
Support vector machines,
Training,
Diseases,
Time measurement"
Detecting Cognitive Distortions Through Machine Learning Text Analytics,"Machine learning and text analytics have proven increasingly useful in a number of health-related applications, particularly in the context of analyzing online data for disease epidemics and warning signs of a variety of mental health issues. We follow in this tradition here, but focus our attention on cognitive distortion, a precursor and symptom of disruptive psychological disorders such as anxiety, anorexia and depression. We collected a number of personal blogs from the Tumblr API, and labeled them based on whether they exhibited distorted thought patterns. We then used LIWC to extract textual features and applied machine learning to the resulting vectors. Our findings show that it is possible to detect cognitive distortions automatically from personal blogs with relatively good accuracy (73.0%) and false negative rate (30.4%).","Distortion,
Logistics,
Psychology,
Blogs,
Social network services,
Pragmatics,
Decision trees"
Efficient and Rapid Machine Learning Algorithms for Big Data and Dynamic Varying Systems,"With the exponential growth of data and complexity of systems, fast machine learning/artificial intelligence and computational intelligence techniques are highly required. Many conventional computational intelligence techniques face bottlenecks in learning (e.g., intensive human intervention and convergence time) [item 1) in the Appendix]. However, efficient learning algorithms alternatively offer significant benefits including fast learning speed, ease of implementation, and minimal human intervention. The need for efficient and fast implementation of machine learning techniques in big data and dynamic varying systems poses many research challenges. This special issue highlights some latest development in the related areas.","Big Data,
Feature extraction,
Support vector machines,
Cybernetics,
Acceleration,
Kernel,
Delays"
Semisupervised Incremental Support Vector Machine Learning Based on Neighborhood Kernel Estimation,"Semisupervised scheme has emerged as a popular strategy in the machine learning community due to the expensiveness of getting enough labeled data. In this paper, a semisupervised incremental support vector machine (SE-INC-SVM) algorithm based on neighborhood kernel estimation is proposed. First, kernel regression is constructed to estimate the unlabeled data from the labeled neighbors and its estimation accuracy is discussed from the analogy with tradition RBF neural network. The incremental scheme is derived to improve the learning efficiency and reduce the computing time. Simulations for manual data set and industrial benchmark-penicillin fermentation process demonstrate the effectiveness of the proposed SE-INC-SVM method.","Support vector machines,
Kernel,
Estimation,
Data models,
Interpolation,
Semisupervised learning,
Training"
Retinal hemorrhage detection by rule-based and machine learning approach,"Robust detection of hemorrhages (HMs) in color fundus image is important in an automatic diabetic retinopathy grading system. Detection of the hemorrhages that are close to or connected with retinal blood vessels was found to be challenge. However, most methods didn't put research on it, even some of them mentioned this issue. In this paper, we proposed a novel hemorrhage detection method based on rule-based and machine learning methods. We focused on the improvement of detection of the hemorrhages that are close to or connected with retinal blood vessels, besides detecting the independent hemorrhage regions. A preliminary test for detecting HM presence was conducted on the images from two databases. We achieved sensitivity and specificity of 93.3% and 88% as well as 91.9% and 85.6% on the two datasets.","Blood vessels,
Biomedical imaging,
Hemorrhaging,
Image segmentation,
Retina,
Image color analysis,
Optical filters"
Classifying osteosarcoma patients using machine learning approaches,"Metabolomic data analysis presents a unique opportunity to advance our understanding of osteosarcoma, a common bone malignancy for which genomic and proteomic studies have enjoyed limited success. One of the major goals of metabolomic studies is to classify osteosarcoma in early stages, which is required for metastasectomy treatment. In this paper we subject our metabolomic data on osteosarcoma patients collected by the SJTU team to three classification methods: logistic regression, support vector machine (SVM) and random forest (RF). The performances are evaluated and compared using receiver operating characteristic curves. All three classifiers are successful in distinguishing between healthy control and tumor cases, with random forest outperforming the other two for cross-validation in training set (accuracy rate for logistic regression, support vector machine and random forest are 88%, 90% and 97% respectively). Random forest achieved overall accuracy rate of 95% with 0.99 AUC on testing set.","Logistics,
Radio frequency,
Support vector machines,
Benign tumors,
Metabolomics,
Testing"
Catching Zika Fever: Application of Crowdsourcing and Machine Learning for Tracking Health Misinformation on Twitter,"In February 2016, World Health Organization declared the Zika outbreak a Public Health Emergency of International Concern. With developing evidence it can cause birth defects, and the Summer Olympics coming up in the worst affected country, Brazil, the virus caught fire on social media. In this work, we use Zika as a case study in building a tool for tracking the misinformation around health concerns on Twitter. We collect more than 13 million tweets regarding the Zika outbreak and track rumors outlined by the World Health Organization and Snopes fact checking website. The tool pipeline, which incorporates health professionals, crowdsourcing, and machine learning, allows us to capture health-related rumors around the world, as well as clarification campaigns by reputable health organizations. We discover an extremely bursty behavior of rumor-related topics, and show that, once the questionable topic is detected, it is possible to identify rumor-bearing tweets using automated techniques.","Twitter,
Crowdsourcing,
Organizations,
Public healthcare,
Tools,
Vaccines"
Supervised Machine Learning to Predict Follow-Up Among Adjuvant Endocrine Therapy Patients,"Long-term adjuvant endocrine therapy patients often fail to follow-up with their care providers for the recommended duration of time. We used electronic health record data, tumor registry records, and appointment logs to predict follow-up for an adjuvant endocrine therapy patient cohort. Learning predictors for follow-up may facilitate interventions that improve follow-up rates, and ultimately improve patient care in the adjuvant endocrine therapy patient population.We selected 1455 adjuvant endocrine therapy patients at Vanderbilt University Medical Center, and modeled them as a matrix of medical-related, appointment-related, and demographic related features derived from EHR data. We built and optimized a random forest classifier and neural network to differentiate between patients that follow-up, or fail to follow-up, with their care provider for at least five years. We measured follow-up three different ways: thought appointments with any care providers, appointments with an oncologist, and adjuvant endocrine therapy medication records. Classifiers make predictions at the start of adjuvant endocrine therapy, and additionally use temporal subsets of data to learn the change in accuracy as patient data accrues.Our best model is a random forest classifier combining medical-related, appointment-related, and demographic-related features to achieve an AUC of 0.74. The most predictive features for follow-up in our random forest model are total medication counts, patient age, and median income for zip code. We suggest that reliable prediction for follow-up may be correlated with amount of care received at VUMC (i.e., VUMC primary care).This study achieved moderately accurate prediction for followup in adjuvant endocrine therapy patients from electronic health record data. Predicting follow-up can facilitate interventions for improving follow-up rates and improve patient care for adjuvant endocrine therapy cohorts. This study demonstrates the ability to find opportunities for patient care improvement from EHR data.","Medical diagnostic imaging,
Tumors,
Electronic medical records,
Neural networks,
Drugs,
Breast cancer"
Detection of needle to nerve contact based on electric bioimpedance and machine learning methods,"In an ongoing project for electrical impedance-based needle guidance we have previously showed in an animal model that intraneural needle positions can be detected with bioimpedance measurement. To enhance the power of this method we in this study have investigated whether an early detection of the needle only touching the nerve also is feasible. Measurement of complex impedance during needle to nerve contact was compared with needle positions in surrounding tissues in a volunteer study on 32 subjects. Classification analysis using Support-Vector Machines demonstrated that discrimination is possible, but that the sensitivity and specificity for the nerve touch algorithm not is at the same level of performance as for intra-neuralintraneural detection.","Needles,
Electrodes,
Phase measurement,
Impedance measurement,
Impedance,
Muscles,
Training"
Machine learning techniques for taming the complexity of modern hardware design,"The continual quest to improve performance and efficiency for new generations of IBM servers leads to a corresponding increase in system complexity. As hardware complexity increases, i.e., more complicated hardware architectures requiring more design choices, the level of sophistication in automation also increases to manage the design challenges. The number of design choices in modern hardware design calls for intelligent automated techniques to navigate the design space. This paper covers three machine learning-based automation techniques used during the design and lifetime of IBM systems. In particular, we describe applying these techniques to the IBM z13 mainframe. During the presilicon design phase, a software system called synthesis tuning system is employed to optimize the parameters of the synthesis program vital to hardware implementation. During both the presilicon and postsilicon phases of the design, a framework called MicroProbe automatically generates microbenchmarks, i.e., small programs, to determine power, performance, and resilience characteristics of the system. Following system product deployment in customer environments, the Call Home facility monitors and analyzes a wide range of in-field usage metrics to help administrators understand current system behavior and improve future designs. Beyond existing IBM system contributions, this high-level overview paper also describes additional machine learning (and related) techniques in the field of hardware design, along with future directions for such work.","Hardware,
Complexity theory,
Automation,
Microarchitecture,
Servers"
Drowsy Driving Warning System Based on GS1 Standards with Machine Learning,"Drowsy driving is the primary cause of motor vehicle accidents and is a risk factor that leads to the loss of human life, remaining as a challenge for the global automotive industry. Recently, drowsy monitoring system has been actively studied for prediction system based machine learning. However, the challenges of automotive real-time constraints and flexibility should be taken into consideration against a large amount of heterogeneous data from vehicle network and other device. To solve this problem, we propose drowsy monitoring system based machine learning using GS1 standard. First, vehicle motion data is defined and modeled using the GS1 standard language for drowsy predict. Second, we propose an optimal algorithm selection and detail architecture for automotive real-time environments through machine learning algorithms (KNN, Naïve Bayes, Logistic Regression) and deep learning algorithms (RNN-LSTM). Finally, we describe system-wide integration and implementation through the open source hardware Raspberry Pi and the machine learning SW framework. We provide optimal LSTM architecture and implementation that takes into account the real-time environmental conditions and how to improve the readability and usability of the vehicle motion data. We also share the rapid prototyping methodology case of connected car systems without other sensor devices.","Standards,
Monitoring,
Machine learning algorithms,
Real-time systems,
Automobiles,
Automotive engineering"
Adaptive Cost Efficient Framework for Cloud-Based Machine Learning,"Machine learning is an increasingly important form of cognitive computing, making progress in several application areas. Machine learning often involves big data sets and is computationally challenging, requiring efficient use of resources. The use of cloud computing as the platform for machine learning offers advantages of scalability and efficient use of hardware. It may, however, be difficult to provision appropriate cost-effective resources for a machine learning task. Our experiments have shown that there can be radical differences between different datasets and different algorithms on the same dataset. The cloud-based machine learning framework presented here aims to provide multiple levels of efficient use of resources and uses a high-level cost model to deal with overall cost-efficiency with respect to cloud service providers. The cost model allows evaluation of trade-offs and supports the choice of appropriate provider resources based on user-defined criteria. A user may choose to prioritize performance, prioritize cost or specify a cost-performance balance. An Amazon AWS cost model for instances is used to illustrate the practical benefits of using the approach - it is seen that large savings can be made by employing this job-specific monitoring and cost-performance analysis. The method can provide all the information for a comparison across different cloud service providers as well as comparisons across the Amazon AWS offerings.","Cloud computing,
Feature extraction,
Computational modeling,
Measurement,
Pricing,
Unified modeling language,
Monitoring"
Application of machine learning method in bridge health monitoring,"Machine learning algorithms have been a typical type of highly efficient method for data processing in these recent decades, and data-driven approaches for bridge health monitoring is particularly useful since a large quantity of sensor data are available. In this paper, a review of most popular applications of machine learning method are presented in order to illustrate their utilities and limitations in the field of bridge health monitoring.","Bridges,
Monitoring,
Support vector machines,
Data processing,
Neural networks,
Structural beams,
Machine learning algorithms"
A Study of Machine Learning in Healthcare,"In the past few years, there has been significant developments in how machine learning can be used in various industries and research. This paper discusses the potential of utilizing machine learning technologies in healthcare and outlines various industry initiatives using machine learning initiatives in the healthcare sector.","Medical services,
Industries,
Medical diagnostic imaging,
Big Data,
Market research,
Data models"
Comprehensive Predictions of Tourists' Next Visit Location Based on Call Detail Records Using Machine Learning and Deep Learning Methods,"Recent developments in data mining and machine learning have helped to solve many issues in prediction and recommendation. In this project, we run a comprehensive study on individual behavior patterns from call detail records (CDR) data to predict tourists' future stops. Multiple classification algorithms are employed, including Decision Tree, Random Forest, Neural Network, Naïve Bayes and SVM. In addition, a Recurrent Neural Network-Long Short Term Memory (LSTM) that is ordinarily applied to language inference problems is tested. Surprisingly, we find that LSTM provides us with the best prediction (94.8%), while Random Forest/Neural Network give the second best (85%). Our investigation suggests that the memory-dependence property of LSTM architecture gives it great expressive power to model our time-series location data, making it an outstanding classifier.","Support vector machines,
Vegetation,
Poles and towers,
Decision trees,
Machine learning,
Recurrent neural networks,
Urban areas"
A machine learning based method of constructing virtual inertial measurement predictor of human body,"In recent years, using human characteristics to assist exoskeleton robot control is one of the hot spots in the field of robot technology. For the problem that extremity installation of inertial measurement components in human body cannot achieve effective measurement, a method of constructing virtual inertial measurement predictor of human body based on machine learning is studied. The method uses the outputs of the inertial measurement components synchronously installed on the extremities and other parts of the body as the data samples, through recurrent neural network, it realizes the construction of virtual inertial measurement components and their predictors. In order to improve the training effect, the training samples are filtered based on gait phase detection. The simulation based on Anybody and MATLAB shows that, by installing the inertial measurement component near hip joint, the proposed method can effectively simulate and predict the inertial measurement component's output of centroid position of foot and surface of shank.","Recurrent neural networks,
Foot,
Mathematical model,
Legged locomotion,
Extraterrestrial measurements,
Biological system modeling"
A Machine Learning-Based Approach to Detect Web Service Design Defects,"Design defects are symptoms of poor design and implementation solutions adopted by developers during the development of their software systems. While the research community devoted a lot of effort to studying and devising approaches for detecting the traditional design defects in object-oriented (OO) applications, little knowledge and support is available for an emerging category of Web service interface design defects. Indeed, it has been shown that service designers and developers tend to pay little attention to their service interfaces design. Such design defects can be subjectively interpreted and hence detected in different ways. In this paper, we propose a novel approach, named WS3D, using machine learning techniques that combines Support Vector Machine (SVM) and Simulated Annealing (SA) to learn from real world examples of service design defects. WS3D has been empirically evaluated on a benchmark of Web services from 14 different application domains. We compared WS3D with the state-of-theart approaches which rely on traditional declarative techniques to detect service design defects by combining metrics and threshold values. Results show that WS3D outperforms the the compared approaches in terms of accuracy with a precision and recall scores of 91% and 94%, respectively.","Web services,
Support vector machines,
Measurement,
Training,
Software systems,
Computer bugs,
Simulated annealing"
Improving Web Services Design Quality Using Heuristic Search and Machine Learning,"Web services evolve over time to fix bugs or update and add new features. However, the design of the Web service's interface may become more complex when aggregating many unrelated operations in terms of context and functionalities. A possible solution is to refactor the Web services interface into different modules that help the user quickly identifying relevant operations. The most challenging issue when refactoring a Web services interface is the high number of possible modularization solutions. The evaluation of these solutions is subjective and difficult to quantify. This paper introduces the use of a neural network-based evaluation function for the problem of Web services interface modularization. The users evaluate manually the suggested modularization solutions by a Genetic Algorithm (GA) for a number of iterations then an Artificial Neural Network (ANN) uses these training examples to evaluate the proposed Web services design changes for the remaining iterations. We evaluated the efficiency of our approach using a benchmark of 82 Web services from different domains and compared the performance of our technique with several existing Web services modularization studies in terms of generating well-designed Web services interface for users.","Web services,
Measurement,
Genetic algorithms,
Predictive models,
Artificial neural networks,
Ports (Computers),
Training"
Predicting Application Failure in Cloud: A Machine Learning Approach,"Despite employing the architectures designed for high service reliability and availability, cloud computing systems do experience service outages and performance slowdown. In addition to these, large-scale cloud systems experience failures in their hardware and software components which often result in node and application (e.g., jobs and tasks) failures. Therefore, to build a reliable cloud system, it is important to understand and characterize the observed failures. The goal of this work is to identify the key features that correlate to application failures in cloud and present a failure prediction model that can correctly predict the outcome of a task or job before it actually finishes, fails or gets killed. To accomplish this, we perform a failure characterization study of the Google cluster workload trace. Our analysis reveals that, there is a significant consumption of resources due to failed and killed jobs. We further explore the potential for failure prediction in cloud applications so that we can reduce the wastage of resources by better managing the jobs and tasks that ultimately fail or get killed. For this, we propose a prediction method based on a special type of Recurrent NeuralNetwork (RNN) named Long Short-Term Memory Network(LSTM) to identify application failures in cloud. It takes resource usage measurements or performance data for each job and task, and the goal is to predict the termination status (e.g., failed and finished etc.) of them. Our algorithm can predict task failures with 87%accuracy and achieves a true positive rate of 85% and false positive rate of 11%.","Cloud computing,
Google,
Reliability,
Correlation,
Recurrent neural networks,
Runtime,
Predictive models"
Root Cause Analysis of Network Failures Using Machine Learning and Summarization Techniques,"Root cause analysis includes the methods to identify the sources of errors in a network. Most techniques rely on knowledge models of the system, which are usually built by using network operators' expertise. This presents problems related to knowledge extraction, scalability, and understandability. We propose an offline method based on machine learning techniques for the automatic identification of dependencies between system events, enhanced with summarization, operations on graphs, and visualization that help network operators identify the root causes of errors. We illustrate it with examples from a corporate network.","Machine learning algorithms,
Predictive models,
Knowledge engineering,
Data mining,
Knowledge based systems,
Machine learning"
A Machine Learning Approach for Efficient Parallel Simulation of Beam Dynamics on GPUs,"Parallel computing architectures like GPUs have traditionally been used to accelerate applications with dense and highly-structured workloads; however, many important applications in science and engineering are irregular and dynamic in nature, making their effective parallel implementation a daunting task. Numerical simulation of charged particle beam dynamics is one such application where the distribution of work and data in the accurate computation of collective effects at each time step is irregular and exhibits control-flow and memory access patterns that are not readily amenable to GPU's architecture. Algorithms with these properties tend to present both significant branch and memory divergence on GPUs which leads to severe performance bottlenecks.We present a novel cache-aware algorithm that uses machine learning to address this problem. The algorithm presented here uses supervised learning to adaptively model and track irregular access patterns in the computation of collective effects at each time step of the simulation to anticipate the future control-flow and data access patterns. Access pattern forecast are then used to formulate runtime decisions that minimize branch and memory divergence on GPUs, thereby improving the performance of collective effects computation at a future time step based on the observations from earlier time steps. Experimental results on NVIDIA Tesla K40 GPU shows that our approach is effective in maximizing data reuse, ensuring workload balance among parallel threads, and in minimizing both branch and memory divergence. Further, the parallel implementation delivers up to 485 Gflops of double precision performance, which translates to a speedup of up to 2.5X compared to the fastest known GPU implementation.","Computational modeling,
Data models,
Heuristic algorithms,
Graphics processing units,
Predictive models,
Two dimensional displays,
Instruction sets"
Runtime Data Layout Scheduling for Machine Learning Dataset,"Machine Learning (ML) approaches are widely-used classification/regression methods for data mining applications. However, the time-consuming training process greatly limits the efficiency of ML approaches. We use the example of SVM (traditional ML algorithm) and DNN (state-of-the-art ML algorithm) to illustrate the idea in this paper. For SVM, a major performance bottleneck of current tools is that they use a unified data storage format because the data formats can have a significant influence on the complexity of storage and computation, memory bandwidth, and the efficiency of parallel processing. To address the problem above, we study the factors influencing the algorithm's performance and conduct auto-tuning to speed up SVM training. DNN training is even slower than SVM. For example, using a 8-core CPUs to train AlexNet model by CIFAR-10 dataset costs 8.2 hours. CIFAR-10 is only 170 MB, which is not efficient for distributed processing. Moreover, due to the algorithm limitation, only a small batch of data can be processed at each iteration. We focus on finding the right algorithmic parameters and using auto-tuning techniques to make the algorithm run faster. For SVM training, our implementation achieves 1.7-16.3× speedup (6.8× on average) against the non-adaptive case (using the worst data format) for various datasets. For DNN training on CIFAR-10 dataset, we reduce the time from 8.2 hours to only roughly 1 minute. We use the benchmark of dollars per speedup to help the users to select the right deep learning hardware.","Support vector machines,
Training,
Prediction algorithms,
Kernel,
Bandwidth,
Sparse matrices,
Parallel processing"
Litho-aware Machine Learning for Hotspot Detection,"In this paper, we propose a novel methodology for machine learning (ML) based hotspot detection that uses lithography information to build SVM (Support Vector Machine) during its learning process. Unlike previous studies that use only geometric information or require a post-OPC (Optical Proximity Correction) mask, this proposed method utilizes detailed optical information but bypasses post-OPC mask by sampling latent image intensity and uses those points to train an SVM model. The results suggest high accuracy and low false alarm, and faster runtime compared with methods that require a post-OPC mask.","Support vector machines,
Pattern matching,
Lithography,
Layout,
Kernel,
Lighting,
Training data"
Evaluate different machine learning techniques for classifying sleep stages on single-channel EEG,"In this paper, we propose 3 different machine learning techniques such as Random Forest, Bagging and Support Vector Machine along with time domain feature for classifying sleep stages based on single-channel EEG. Whole-night polysomnograms from 25 subjects were recorded employing R&K standard. The evolved process investigated the EEG signals of (C4-A1) for sleep staging. Automatic and manual scoring results were associated on an epoch-by-epoch basis. An entire 96,000 data samples 30s sleep EEG epoch were calculated and applied for performance evaluation. The epoch-by-epoch assessment was created by classifying the EEG epochs into six stages (W/S1/S2/S3/S4/REM) according to proposed method and manual scoring. Result shows that Random Forest classifiers achieve the overall accuracy; specificity and sensitivity level of 97.73%, 96.3% and 99.51% respectively.","Sleep,
Electroencephalography,
Support vector machines,
Radio frequency,
Feature extraction,
Bagging,
Standards"
Facilitating Workload Aware Storage Platform by Using Machine Learning Technics,"In this paper, we present our proof-of-concept of a workload aware storage platform. The POC demonstrates the feasibility of building a machine learning technics facilitated middleware for storage management. The middleware is capable of providing optimal assignments of storage workloads to backends as well as continuously on-the-fly optimization thereafter. Experiment indicates that the proposed middleware can efficiently and dynamically adapt the storage backend to satisfy the SLA requirements with minimum impact on the workloads.","Engines,
Containers,
Virtualization,
Servers,
Cloud computing,
Hardware"
Signal Processing and Machine Learning for Mental Health Research and Clinical Applications [Perspectives],"Formally, the problem that we present is that of identifying the hidden attributes of the system that modulates the body's signals, uncovered through novel signal processing and machine learning on large-scale multimodal data (Figure 1). Signal processing is the keystone that supports this mapping from data to representations of behaviors and mental states. The pipeline first begins with raw signals, such as from visual, auditory, and physiological sensors. Then, we need to localize information coming from corresponding behavioral channels, such as the face, body, and voice. Next, the signals are denoised and modeled to extract meaningful information like the words that are said and patterns of how they are spoken. The coordination of channels can also be assessed via time-series modeling techniques. Moreover, since an individual's behavior is not isolated, but influenced by a communicative partners' actions and the environment (e.g., interview versus casual discussion, home versus clinic), temporal modeling must account for these contextual effects. Finally, having achieved a representation of behavior derived from the signals, machine learning is used to make inferences on mental states to support human or autonomous decision making.","Signal processing,
Speech processing,
Physiology,
Current measurement,
Behavioral sciences,
Sensors,
Decision making"
Current Mirror Array: A Novel Circuit Topology for Combining Physical Unclonable Function and Machine Learning,"Edge analytics support industrial Internet of Things by pushing some data processing capacity to the edge of the network instead of sending the streaming data captured by the sensor nodes directly to the cloud. It is advantageous to endow machine learners for data reduction with suitable security primitives for privacy protection in edge computing devices to conserve area and power consumption. In this paper, we propose a novel physical unclonable function (PUF) based on current mirror array (CMA) circuits that reuses the circuit implementation of a machine learner-the extreme learning machine (ELM), which is a randomized neural network. Seven different challenge activation and response readout schemes are proposed to realize different weak and strong PUF functions from within the same CMA array. ELM endowed with such reconfigurable challenge-response mechanism is more robust and adaptable to different authentication protocols and security functions. Measurement results on 0.35μm test chips demonstrate that the proposed strong PUF outperforms other state-of-the-art designs with smaller area/bit of 9x10⁻³⁶ μm² and lower native bit error rate (BER) of 0.16% with an added overhead of less than 2.5% power and 2.9% area over the native ELM implementation.","Mirrors,
Reliability,
Delays,
Cloud computing,
Cryptography,
Authentication"
Experimental comparison of machine learning-based available bandwidth estimation methods over operational LTE networks,"We propose PathML, an available bandwidth (i.e., unused capacity of an end-to-end path) estimation method based on a data-driven paradigm that uses machine learning with a large amount of data. An experiment over an operational LTE network was performed to compare our method with prior work. Now we are living in the cloud era: an age in which large amounts of data are collected from numerous devices and accumulated on clouds. To take advantage of the characteristics of this cloud era, we propose a new method of available bandwidth estimation that uses the accumulated mass data. In prior work, network specialists constructed simplified models of complex network behavior based on a relatively small amount of measured data and designed estimation algorithms based on the simplified model. However, when network behavior not assumed in the model construction occurs, these algorithms are not equipped to deal with the newly obtained data and cannot extract sufficient information from it. This adverse simplification effect decreases the estimation accuracy. In contrast, our method based on machine learning can extract information that is ignored or overlooked by humans, thus enabling the estimation of available bandwidth with high accuracy even in the uncertain situations with which prior work struggles. We selected four machine learning techniques suitable for available bandwidth estimation: namely, support vector regression, kernel ridge regression, random forests, and convolutional neural network. We performed an experiment over an operational LTE network of Japan's primary mobile operator to compare our method with prior work. Results showed that our method clearly outperformed the prior work in terms of available bandwidth estimation accuracy. The most accurate technique was the convolutional neural network: its estimation accuracy was 83.7% compared to the 74.2% of the prior work. Specifically, under the 40-50 Mbps broadband condition, the mean absolute error of the values estimated by our method was just 2.2 Mbps while that of the prior work was 16.4 Mbps-in other words, only about 1/8 that of the prior work.","Estimation,
Bandwidth,
Long Term Evolution,
Receivers,
Data models,
Data mining,
Scheduling algorithms"
Predictive analytics for chronic kidney disease using machine learning techniques,"Predictive analytics for healthcare using machine learning is a challenged task to help doctors decide the exact treatments for saving lives. In this paper, we present machine learning techniques for predicting the chronic kidney disease using clinical data. Four machine learning methods are explored including K-nearest neighbors (KNN), support vector machine (SVM), logistic regression (LR), and decision tree classifiers. These predictive models are constructed from chronic kidney disease dataset and the performance of these models are compared together in order to select the best classifier for predicting the chronic kidney disease.","Kidney,
Diseases,
Support vector machines,
Logistics,
Decision trees,
Predictive models,
Sensitivity"
A comprehensive investigation and comparison of Machine Learning Techniques in the domain of heart disease,"This paper aims to investigate and compare the accuracy of different data mining classification schemes, employing Ensemble Machine Learning Techniques, for the prediction of heart disease. The Cleveland data set for heart diseases, containing 303 instances, has been used as the main database for the training and testing of the developed system. 10-Fold Cross-Validation has been applied in order to increase the amount of data, which would otherwise have been limited. Different classifiers, namely Decision Tree (DT), Naïve Bayes (NB), Multilayer Perceptron (MLP), K-Nearest Neighbor (K-NN), Single Conjunctive Rule Learner (SCRL), Radial Basis Function (RBF) and Support Vector Machine (SVM), have been employed. Moreover, the ensemble prediction of classifiers, bagging, boosting and stacking, has been applied to the dataset. The results of the experiments indicate that the SVM method using the boosting technique outperforms the other aforementioned methods.","Support vector machines,
Heart,
Niobium,
Diseases,
Bagging,
Stacking,
Boosting"
Combining a machine learning and optimization for early pre-FEC BER degradation to meet committed QoS,"Monitoring the physical layer is key to detect bit error rate (BER) degradation caused by failures and to identify the cause of the failure and localize the failed elements. Once the failure has been detected, actions can be taken to reduce as much as possible its impact on the network. Commercially available optical equipment are able to correct degraded optical signals by means of Forward Error Correction (FEC) algorithms. A value of pre-FEC BER over a pre-defined threshold would imply a non-error-free post-FEC transmission and, as a result, communication would be disrupted. Therefore, a prompt detection of lightpaths with excessive pre-FEC BER can help to greatly reduce such SLA violations, in particular when supporting vlinks. As a result of the above, it would be desirable to anticipate such degradations and apply re-optimization to re-route those affected demands according to their SLAs in order to reduce the affected traffic after the degradation is detected. Designing algorithms capable of promptly detect distinct BER anomaly patterns would be desirable. The objective would be to anticipate intolerable BER values as much as possible aiming at leaving enough time to plan a re-routing procedure during off-peak hours. In this paper, we propose an effective machine learning-based algorithm to localize and identify the most probable cause of failure impacting a given service, as well as a re-optimization algorithm to re-route affected demands, targeting at reducing SLA violation. Results show that the proposed detection and re-routing algorithms noticeably reduce bandwidth and number of demands affected.","Degradation,
Quality of service,
Bit error rate,
Bandwidth,
Monitoring,
Adaptive optics,
Optical receivers"
