Title,Abstract,Keywords
Graph Embedding and Extensions: A General Framework for Dimensionality Reduction,"A large family of algorithms - supervised or unsupervised; stemming from statistics or geometry theory - has been designed to provide different solutions to the problem of dimensionality reduction. Despite the different motivations of these algorithms, we present in this paper a general formulation known as graph embedding to unify them within a common framework. In graph embedding, each algorithm can be considered as the direct graph embedding or its linear/kernel/tensor extension of a specific intrinsic graph that describes certain desired statistical or geometric properties of a data set, with constraints from scale normalization or a penalty graph that characterizes a statistical or geometric property that should be avoided. Furthermore, the graph embedding framework can be used as a general platform for developing new dimensionality reduction algorithms. By utilizing this framework as a tool, we propose a new supervised dimensionality reduction algorithm called marginal Fisher analysis in which the intrinsic graph characterizes the intraclass compactness and connects each data point with its neighboring points of the same class, while the penalty graph connects the marginal points and characterizes the interclass separability. We show that MFA effectively overcomes the limitations of the traditional linear discriminant analysis algorithm due to data distribution assumptions and available projection directions. Real face recognition experiments show the superiority of our proposed MFA in comparison to LDA, also for corresponding kernel and tensor extensions","Kernel,
Tensile stress,
Linear discriminant analysis,
Principal component analysis,
Laplace equations,
Algorithm design and analysis,
Vectors,
Statistics,
Geometry,
Face recognition"
MonoSLAM: Real-Time Single Camera SLAM,"We present a real-time algorithm which can recover the 3D trajectory of a monocular camera, moving rapidly through a previously unknown scene. Our system, which we dub MonoSLAM, is the first successful application of the SLAM methodology from mobile robotics to the ""pure vision"" domain of a single uncontrolled camera, achieving real time but drift-free performance inaccessible to structure from motion approaches. The core of the approach is the online creation of a sparse but persistent map of natural landmarks within a probabilistic framework. Our key novel contributions include an active approach to mapping and measurement, the use of a general motion model for smooth camera movement, and solutions for monocular feature initialization and feature orientation estimation. Together, these add up to an extremely efficient and robust algorithm which runs at 30 Hz with standard PC and camera hardware. This work extends the range of robotic systems in which SLAM can be usefully applied, but also opens up new areas. We present applications of MonoSLAM to real-time 3D localization and mapping for a high-performance full-size humanoid robot and live augmented reality with a hand-held camera","Cameras,
Simultaneous localization and mapping,
Robot vision systems,
Layout,
Mobile robots,
Real time systems,
Motion measurement,
Motion estimation,
Robustness,
Hardware"
Saliency Detection: A Spectral Residual Approach,"The ability of human visual system to detect visual saliency is extraordinarily fast and reliable. However, computational modeling of this basic intelligent behavior still remains a challenge. This paper presents a simple method for the visual saliency detection. Our model is independent of features, categories, or other forms of prior knowledge of the objects. By analyzing the log-spectrum of an input image, we extract the spectral residual of an image in spectral domain, and propose a fast method to construct the corresponding saliency map in spatial domain. We test this model on both natural pictures and artificial images such as psychological patterns. The result indicate fast and robust saliency detection of our method.","Object detection,
Computational modeling,
Humans,
Visual system,
Image analysis,
Object recognition,
Machine vision,
Redundancy,
Image coding,
Statistical distributions"
Parallel Tracking and Mapping for Small AR Workspaces,"This paper presents a method of estimating camera pose in an unknown scene. While this has previously been attempted by adapting SLAM algorithms developed for robotic exploration, we propose a system specifically designed to track a hand-held camera in a small AR workspace. We propose to split tracking and mapping into two separate tasks, processed in parallel threads on a dual-core computer: one thread deals with the task of robustly tracking erratic hand-held motion, while the other produces a 3D map of point features from previously observed video frames. This allows the use of computationally expensive batch optimisation techniques not usually associated with real-time operation: The result is a system that produces detailed maps with thousands of landmarks which can be tracked at frame-rate, with an accuracy and robustness rivalling that of state-of-the-art model-based systems.",
Object retrieval with large vocabularies and fast spatial matching,"In this paper, we present a large-scale object retrieval system. The user supplies a query object by selecting a region of a query image, and the system returns a ranked list of images that contain the same object, retrieved from a large corpus. We demonstrate the scalability and performance of our system on a dataset of over 1 million images crawled from the photo-sharing site, Flickr [3], using Oxford landmarks as queries. Building an image-feature vocabulary is a major time and performance bottleneck, due to the size of our dataset. To address this problem we compare different scalable methods for building a vocabulary and introduce a novel quantization method based on randomized trees which we show outperforms the current state-of-the-art on an extensive ground-truth. Our experiments show that the quantization has a major effect on retrieval quality. To further improve query performance, we add an efficient spatial verification stage to re-rank the results returned from our bag-of-words model and show that this consistently improves search quality, though by less of a margin when the visual vocabulary is large. We view this work as a promising step towards much larger, ""web-scale "" image corpora.","Vocabulary,
Image retrieval,
Quantization,
Information filtering,
Information filters,
Silicon,
Large-scale systems,
Scalability,
Humans,
Information retrieval"
Robust Object Recognition with Cortex-Like Mechanisms,"We introduce a new general framework for the recognition of complex visual scenes, which is motivated by biology: We describe a hierarchical system that closely follows the organization of visual cortex and builds an increasingly complex and invariant feature representation by alternating between a template matching and a maximum pooling operation. We demonstrate the strength of the approach on a range of recognition tasks: From invariant single object recognition in clutter to multiclass categorization problems and complex scene understanding tasks that rely on the recognition of both shape-based as well as texture-based objects. Given the biological constraints that the system had to satisfy, the approach performs surprisingly well: It has the capability of learning from only a few training examples and competes with state-of-the-art systems. We also discuss the existence of a universal, redundant dictionary of features that could handle the recognition of most object categories. In addition to its relevance for computer vision, the success of this approach suggests a plausibility proof for a class of feedforward models of object recognition in cortex","Robustness,
Object recognition,
Layout,
Computer vision,
Brain modeling,
Neuroscience,
Humans,
Gabor filters,
Streaming media,
Face detection"
Python for Scientific Computing,"Python is an excellent ""steering"" language for scientific codes written in other languages. However, with additional basic tools, Python transforms into a high-level language suited for scientific and engineering code that's often fast enough to be immediately useful but also flexible enough to be sped up with additional extensions.","Scientific computing,
High level languages,
Libraries,
Writing,
Application software,
Embedded software,
Software standards,
Standards development,
Internet,
Prototypes"
IPython: A System for Interactive Scientific Computing,"Python offers basic facilities for interactive work and a comprehensive library on top of which more sophisticated systems can be built. The IPython project provides on enhanced interactive environment that includes, among other features, support for data visualization and facilities for distributed and parallel computation","Scientific computing,
Libraries,
Data visualization,
Spine,
Supercomputers,
Hardware,
Data analysis,
Testing,
Production,
Parallel processing"
Improved Techniques for Grid Mapping With Rao-Blackwellized Particle Filters,"Recently, Rao-Blackwellized particle filters (RBPF) have been introduced as an effective means to solve the simultaneous localization and mapping problem. This approach uses a particle filter in which each particle carries an individual map of the environment. Accordingly, a key question is how to reduce the number of particles. In this paper, we present adaptive techniques for reducing this number in a RBPF for learning grid maps. We propose an approach to compute an accurate proposal distribution, taking into account not only the movement of the robot, but also the most recent observation. This drastically decreases the uncertainty about the robot's pose in the prediction step of the filter. Furthermore, we present an approach to selectively carry out resampling operations, which seriously reduces the problem of particle depletion. Experimental results carried out with real mobile robots in large-scale indoor, as well as outdoor, environments illustrate the advantages of our methods over previous approaches","Particle filters,
Simultaneous localization and mapping,
Proposals,
Computer science,
Distributed computing,
Orbital robotics,
Contracts,
Robot sensing systems,
Uncertainty,
Mobile robots"
Steps toward a science of service systems,"The service sector accounts for most of the world's economic activity, but it's the least-studied part of the economy. A service system comprises people and technologies that adaptively compute and adjust to a system's changing value of knowledge. A science of service systems could provide theory and practice around service innovation","Educational institutions,
Government,
Costs,
Outsourcing,
Application software,
Control systems,
Equations,
Testing,
Standardization,
Packaging"
Duplicate Record Detection: A Survey,"Often, in the real world, entities have two or more representations in databases. Duplicate records do not share a common key and/or they contain errors that make duplicate matching a difficult task. Errors are introduced as the result of transcription errors, incomplete information, lack of standard formats, or any combination of these factors. In this paper, we present a thorough analysis of the literature on duplicate record detection. We cover similarity metrics that are commonly used to detect similar field entries, and we present an extensive set of duplicate detection algorithms that can detect approximately duplicate records in a database. We also cover multiple techniques for improving the efficiency and scalability of approximate duplicate detection algorithms. We conclude with coverage of existing tools and with a brief discussion of the big open problems in the area","Computer errors,
Mirrors,
Computer Society,
Detection algorithms,
Cleaning,
Scalability,
Couplings,
Uncertainty,
Relational databases,
Cost function"
Power Control By Geometric Programming,"In wireless cellular or ad hoc networks where Quality of Service (QoS) is interference-limited, a variety of power control problems can be formulated as nonlinear optimization with a system-wide objective, e.g., maximizing the total system throughput or the worst user throughput, subject to QoS constraints from individual users, e.g., on data rate, delay, and outage probability. We show that in the high Signal-to- interference Ratios (SIR) regime, these nonlinear and apparently difficult, nonconvex optimization problems can be transformed into convex optimization problems in the form of geometric programming; hence they can be very efficiently solved for global optimality even with a large number of users. In the medium to low SIR regime, some of these constrained nonlinear optimization of power control cannot be turned into tractable convex formulations, but a heuristic can be used to compute in most cases the optimal solution by solving a series of geometric programs through the approach of successive convex approximation. While efficient and robust algorithms have been extensively studied for centralized solutions of geometric programs, distributed algorithms have not been explored before. We present a systematic method of distributed algorithms for power control that is geometric-programming-based. These techniques for power control, together with their implications to admission control and pricing in wireless networks, are illustrated through several numerical examples.","Power control,
Quality of service,
Constraint optimization,
Throughput,
Distributed algorithms,
Cellular networks,
Ad hoc networks,
Interference constraints,
Robustness,
Admission control"
Data Mining Static Code Attributes to Learn Defect Predictors,"The value of using static code attributes to learn defect predictors has been widely debated. Prior work has explored issues like the merits of ""McCabes versus Halstead versus lines of code counts"" for generating defect predictors. We show here that such debates are irrelevant since how the attributes are used to build predictors is much more important than which particular attributes are used. Also, contrary to prior pessimism, we show that such defect predictors are demonstrably useful and, on the data studied here, yield predictors with a mean probability of detection of 71 percent and mean false alarms rates of 25 percent. These predictors would be useful for prioritizing a resource-bound exploration of code that has yet to be inspected","Data mining,
Bayesian methods,
Artificial intelligence,
Software testing,
System testing,
Learning systems,
Art,
Software quality,
Software systems,
Financial management"
t-Closeness: Privacy Beyond k-Anonymity and l-Diversity,"The k-anonymity privacy requirement for publishing microdata requires that each equivalence class (i.e., a set of records that are indistinguishable from each other with respect to certain ""identifying"" attributes) contains at least k records. Recently, several authors have recognized that k-anonymity cannot prevent attribute disclosure. The notion of l-diversity has been proposed to address this; l-diversity requires that each equivalence class has at least l well-represented values for each sensitive attribute. In this paper we show that l-diversity has a number of limitations. In particular, it is neither necessary nor sufficient to prevent attribute disclosure. We propose a novel privacy notion called t-closeness, which requires that the distribution of a sensitive attribute in any equivalence class is close to the distribution of the attribute in the overall table (i.e., the distance between the two distributions should be no more than a threshold t). We choose to use the earth mover distance measure for our t-closeness requirement. We discuss the rationale for t-closeness and illustrate its advantages through examples and experiments.","Privacy,
Earth,
Computer science,
Publishing,
Motion measurement,
Databases,
Data security,
Diseases,
Remuneration,
Protection"
Matching Local Self-Similarities across Images and Videos,"We present an approach for measuring similarity between visual entities (images or videos) based on matching internal self-similarities. What is correlated across images (or across video sequences) is the internal layout of local self-similarities (up to some distortions), even though the patterns generating those local self-similarities are quite different in each of the images/videos. These internal self-similarities are efficiently captured by a compact local ""self-similarity descriptor""', measured densely throughout the image/video, at multiple scales, while accounting for local and global geometric distortions. This gives rise to matching capabilities of complex visual data, including detection of objects in real cluttered images using only rough hand-sketches, handling textured objects with no clear boundaries, and detecting complex actions in cluttered video data with no prior learning. We compare our measure to commonly used image-based and video-based similarity measures, and demonstrate its applicability to object detection, retrieval, and action detection.","Object detection,
Pixel,
Image edge detection,
Distortion measurement,
Video sequences,
Image retrieval,
Filters,
Heart,
Image recognition,
Computer science"
A Review of Methods for Correction of Intensity Inhomogeneity in MRI,"Medical image acquisition devices provide a vast amount of anatomical and functional information, which facilitate and improve diagnosis and patient treatment, especially when supported by modern quantitative image analysis methods. However, modality specific image artifacts, such as the phenomena of intensity inhomogeneity in magnetic resonance images (MRI), are still prominent and can adversely affect quantitative image analysis. In this paper, numerous methods that have been developed to reduce or eliminate intensity inhomogeneities in MRI are reviewed. First, the methods are classified according to the inhomogeneity correction strategy. Next, different qualitative and quantitative evaluation approaches are reviewed. Third, 60 relevant publications are categorized according to several features and analyzed so as to reveal major trends, popularity, evaluation strategies and applications. Finally, key evaluation issues and future development of the inhomogeneity correction field, supported by the results of the analysis, are discussed",
Image Classification using Random Forests and Ferns,"We explore the problem of classifying images by the object categories they contain in the case of a large number of object categories. To this end we combine three ingredients: (i) shape and appearance representations that support spatial pyramid matching over a region of interest. This generalizes the representation of Lazebnik et al., (2006) from an image to a region of interest (ROI), and from appearance (visual words) alone to appearance and local shape (edge distributions); (ii) automatic selection of the regions of interest in training. This provides a method of inhibiting background clutter and adding invariance to the object instance 's position; and (iii) the use of random forests (and random ferns) as a multi-way classifier. The advantage of such classifiers (over multi-way SVM for example) is the ease of training and testing. Results are reported for classification of the Caltech-101 and Caltech-256 data sets. We compare the performance of the random forest/ferns classifier with a benchmark multi-way SVM classifier. It is shown that selecting the ROI adds about 5% to the performance and, together with the other improvements, the result is about a 10% improvement over the state of the art for Caltech-256.","Image classification,
Shape,
Layout,
Computer vision,
Support vector machines,
Support vector machine classification,
Image representation,
Benchmark testing,
Turning"
Sharing Visual Features for Multiclass and Multiview Object Detection,"We consider the problem of detecting a large number of different classes of objects in cluttered scenes. Traditional approaches require applying a battery of different classifiers to the image, at multiple locations and scales. This can be slow and can require a lot of training data since each classifier requires the computation of many different image features. In particular, for independently trained detectors, the (runtime) computational complexity and the (training-time) sample complexity scale linearly with the number of classes to be detected. We present a multitask learning procedure, based on boosted decision stumps, that reduces the computational and sample complexity by finding common features that can be shared across the classes (and/or views). The detectors for each class are trained jointly, rather than independently. For a given performance level, the total number of features required and, therefore, the runtime cost of the classifier, is observed to scale approximately logarithmically with the number of classes. The features selected by joint training are generic edge-like features, whereas the features chosen by training each class separately tend to be more object-specific. The generic features generalize better and considerably reduce the computational cost of multiclass object detection","Object detection,
Detectors,
Runtime,
Training data,
Layout,
Batteries,
Computational complexity,
Costs,
Computational efficiency,
Boosting"
Geometrically Accurate Topology-Correction of Cortical Surfaces Using Nonseparating Loops,"In this paper, we focus on the retrospective topology correction of surfaces. We propose a technique to accurately correct the spherical topology of cortical surfaces. Specifically, we construct a mapping from the original surface onto the sphere to detect topological defects as minimal nonhomeomorphic regions. The topology of each defect is then corrected by opening and sealing the surface along a set of nonseparating loops that are selected in a Bayesian framework. The proposed method is a wholly self-contained topology correction algorithm, which determines geometrically accurate, topologically correct solutions based on the magnetic resonance imaging (MRI) intensity profile and the expected local curvature. Applied to real data, our method provides topological corrections similar to those made by a trained operator",
Generating Cancelable Fingerprint Templates,"Biometrics-based authentication systems offer obvious usability advantages over traditional password and token-based authentication schemes. However, biometrics raises several privacy concerns. A biometric is permanently associated with a user and cannot be changed. Hence, if a biometric identifier is compromised, it is lost forever and possibly for every application where the biometric is used. Moreover, if the same biometric is used in multiple applications, a user can potentially be tracked from one application to the next by cross-matching biometric databases. In this paper, we demonstrate several methods to generate multiple cancelable identifiers from fingerprint images to overcome these problems. In essence, a user can be given as many biometric identifiers as needed by issuing a new transformation ""key"". The identifiers can be cancelled and replaced when compromised. We empirically compare the performance of several algorithms such as Cartesian, polar, and surface folding transformations of the minutiae positions. It is demonstrated through multiple experiments that we can achieve revocability and prevent cross-matching of biometric databases. It is also shown that the transforms are noninvertible by demonstrating that it is computationally as hard to recover the original biometric identifier from a transformed version as by randomly guessing. Based on these empirical results and a theoretical analysis we conclude that feature-level cancelable biometric construction is practicable in large biometric deployments",
"Globally Maximizing, Locally Minimizing: Unsupervised Discriminant Projection with Applications to Face and Palm Biometrics","This paper develops an unsupervised discriminant projection (UDP) technique for dimensionality reduction of high-dimensional data in small sample size cases. UDP can be seen as a linear approximation of a multimanifolds-based learning framework which takes into account both the local and nonlocal quantities. UDP characterizes the local scatter as well as the nonlocal scatter, seeking to find a projection that simultaneously maximizes the nonlocal scatter and minimizes the local scatter. This characteristic makes UDP more intuitive and more powerful than the most up-to-date method, locality preserving projection (LPP), which considers only the local scatter for clustering or classification tasks. The proposed method is applied to face and palm biometrics and is examined using the Yale, FERET, and AR face image databases and the PolyU palmprint database. The experimental results show that UDP consistently outperforms LPP and PCA and outperforms LDA when the training sample size per class is small. This demonstrates that UDP is a good choice for real-world biometrics applications","Biometrics,
Scattering,
Linear discriminant analysis,
Principal component analysis,
Face recognition,
Kernel,
Image databases,
Pattern recognition,
Laplace equations,
Computer science"
Rapid Biologically-Inspired Scene Classification Using Features Shared with Visual Attention,"We describe and validate a simple context-based scene recognition algorithm for mobile robotics applications. The system can differentiate outdoor scenes from various sites on a college campus using a multiscale set of early-visual features, which capture the ""gist"" of the scene into a low-dimensional signature vector. Distinct from previous approaches, the algorithm presents the advantage of being biologically plausible and of having low-computational complexity, sharing its low-level features with a model for visual attention that may operate concurrently on a robot. We compare classification accuracy using scenes filmed at three outdoor sites on campus (13,965 to 34,711 frames per site). Dividing each site into nine segments, we obtain segment classification rates between 84.21 percent and 88.62 percent. Combining scenes from all sites (75,073 frames in total) yields 86.45 percent correct classification, demonstrating the generalization and scalability of the approach","Layout,
Mobile robots,
Object recognition,
Image segmentation,
Computer vision,
Robot vision systems,
Sonar navigation,
Robustness,
Robot sensing systems,
Educational institutions"
Intrinsic Motivation Systems for Autonomous Mental Development,"Exploratory activities seem to be intrinsically rewarding for children and crucial for their cognitive development. Can a machine be endowed with such an intrinsic motivation system? This is the question we study in this paper, presenting a number of computational systems that try to capture this drive towards novel or curious situations. After discussing related research coming from developmental psychology, neuroscience, developmental robotics, and active learning, this paper presents the mechanism of Intelligent Adaptive Curiosity, an intrinsic motivation system which pushes a robot towards situations in which it maximizes its learning progress. This drive makes the robot focus on situations which are neither too predictable nor too unpredictable, thus permitting autonomous mental development. The complexity of the robot's activities autonomously increases and complex developmental sequences self-organize without being constructed in a supervised manner. Two experiments are presented illustrating the stage-like organization emerging with this mechanism. In one of them, a physical robot is placed on a baby play mat with objects that it can learn to manipulate. Experimental results show that the robot first spends time in situations which are easy to learn, then shifts its attention progressively to situations of increasing difficulty, avoiding situations in which nothing can be learned. Finally, these various results are discussed in relation to more complex forms of behavioral organization and data coming from developmental psychology","Autonomous mental development,
Psychology,
Intelligent robots,
Cognitive robotics,
Humans,
Computer science,
Laboratories,
Neuroscience,
Computational intelligence,
Pediatrics"
"What, where and who? Classifying events by scene and object recognition","We propose a first attempt to classify events in static images by integrating scene and object categorizations. We define an event in a static image as a human activity taking place in a specific environment. In this paper, we use a number of sport games such as snow boarding, rock climbing or badminton to demonstrate event classification. Our goal is to classify the event in the image as well as to provide a number of semantic labels to the objects and scene environment within the image. For example, given a rowing scene, our algorithm recognizes the event as rowing by classifying the environment as a lake and recognizing the critical objects in the image as athletes, rowing boat, water, etc. We achieve this integrative and holistic recognition through a generative graphical model. We have assembled a highly challenging database of 8 widely varied sport events. We show that our system is capable of classifying these event classes at 73.4% accuracy. While each component of the model contributes to the final recognition, using scene or objects alone cannot achieve this performance.","Layout,
Object recognition,
Image recognition,
Humans,
Snow,
Lakes,
Boats,
Graphical models,
Assembly,
Image databases"
Total Recall: Automatic Query Expansion with a Generative Feature Model for Object Retrieval,"Given a query image of an object, our objective is to retrieve all instances of that object in a large (1M+) image database. We adopt the bag-of-visual-words architecture which has proven successful in achieving high precision at low recall. Unfortunately, feature detection and quantization are noisy processes and this can result in variation in the particular visual words that appear in different images of the same object, leading to missed results. In the text retrieval literature a standard method for improving performance is query expansion. A number of the highly ranked documents from the original query are reissued as a new query. In this way, additional relevant terms can be added to the query. This is a form of blind rele- vance feedback and it can fail if 'outlier' (false positive) documents are included in the reissued query. In this paper we bring query expansion into the visual domain via two novel contributions. Firstly, strong spatial constraints between the query image and each result allow us to accurately verify each return, suppressing the false positives which typically ruin text-based query expansion. Secondly, the verified images can be used to learn a latent feature model to enable the controlled construction of expanded queries. We illustrate these ideas on the 5000 annotated image Oxford building database together with more than 1M Flickr images. We show that the precision is substantially boosted, achieving total recall in many cases.",
A Consensus Model for Group Decision Making With Incomplete Fuzzy Preference Relations,"Two processes are necessary to solve group decision making problems: A consensus process and a selection process. The consensus reaching process is necessary to obtain a final solution with a certain level of agreement between the experts; and the selection process is necessary to obtain such a final solution. In a previous paper, we present a selection process to deal with group decision making problems with incomplete fuzzy preference relations, which uses consistency measures to estimate the incomplete fuzzy preference relations. In this paper we present a consensus model. The main novelty of this consensus model is that of being guided by both consensus and consistency measures. Also, the consensus reaching process is guided automatically, without moderator, through both consensus and consistency criteria. To do that, a feedback mechanism is developed to generate advice on how experts should change or complete their preferences in order to reach a solution with high consensus and consistency degrees. In each consensus round, experts are given information on how to change their preferences, and to estimate missing values if their corresponding preference relation is incomplete. Additionally, a consensus and consistency based induced ordered weighted averaging operator to aggregate the experts' preferences is introduced, which can be used in consensus models as well as in selection processes. The main improvement of this consensus model is that it supports the management of incomplete information and it allows to achieve consistent solutions with a great level of agreement.","Decision making,
Feedback,
Aggregates,
Information management,
Computer science,
Artificial intelligence,
Computational intelligence,
Humans"
Research Challenges for On-Chip Interconnection Networks,"On-chip interconnection networks are rapidly becoming a key enabling technology for commodity multicore processors and SoCs common in consumer embedded systems, the National Science Foundation initiated a workshop that addressed upcoming research issues in OCIN technology, design, and implementation and set a direction for researchers in the field.",
Group Decision-Making Model With Incomplete Fuzzy Preference Relations Based on Additive Consistency,"In decision-making problems there may be cases in which experts do not have an in-depth knowledge of the problem to be solved. In such cases, experts may not put their opinion forward about certain aspects of the problem, and as a result they may present incomplete preferences, i.e., some preference values may not be given or may be missing. In this paper, we present a new model for group decision making in which experts' preferences can be expressed as incomplete fuzzy preference relations. As part of this decision model, we propose an iterative procedure to estimate the missing information in an expert's incomplete fuzzy preference relation. This procedure is guided by the additive-consistency (AC) property and only uses the preference values the expert provides. The AC property is also used to measure the level of consistency of the information provided by the experts and also to propose a new induced ordered weighted averaging (IOWA) operator, the AC-IOWA operator, which permits the aggregation of the experts' preferences in such a way that more importance is given to the most consistent ones. Finally, the selection of the solution set of alternatives according to the fuzzy majority of the experts is based on two quantifier-guided choice degrees: the dominance and the nondominance degree","Decision making,
Fuzzy sets,
Computer science,
Artificial intelligence,
Computational intelligence"
A Biologically Inspired System for Action Recognition,"We present a biologically-motivated system for the recognition of actions from video sequences. The approach builds on recent work on object recognition based on hierarchical feedforward architectures [25, 16, 20] and extends a neurobiological model of motion processing in the visual cortex [10]. The system consists of a hierarchy of spatio-temporal feature detectors of increasing complexity: an input sequence is first analyzed by an array of motion- direction sensitive units which, through a hierarchy of processing stages, lead to position-invariant spatio-temporal feature detectors. We experiment with different types of motion-direction sensitive units as well as different system architectures. As in [16], we find that sparse features in intermediate stages outperform dense ones and that using a simple feature selection approach leads to an efficient system that performs better with far fewer features. We test the approach on different publicly available action datasets, in all cases achieving the highest results reported to date.",
Three-Dimensional Face Recognition in the Presence of Facial Expressions: An Annotated Deformable Model Approach,"In this paper, we present the computational tools and a hardware prototype for 3D face recognition. Full automation is provided through the use of advanced multistage alignment algorithms, resilience to facial expressions by employing a deformable model framework, and invariance to 3D capture devices through suitable preprocessing steps. In addition, scalability in both time and space is achieved by converting 3D facial scans into compact metadata. We present our results on the largest known, and now publicly available, face recognition grand challenge 3D facial database consisting of several thousand scans. To the best of our knowledge, this is the highest performance reported on the FRGC v2 database for the 3D modality","Face recognition,
Deformable models,
Databases,
Computer science,
Hardware,
Prototypes,
Automation,
Resilience,
Scalability,
Information retrieval"
Projected Gradient Methods for Nonnegative Matrix Factorization,"Nonnegative matrix factorization (NMF) can be formulated as a minimization problem with bound constraints. Although bound-constrained optimization has been studied extensively in both theory and practice, so far no study has formally applied its techniques to NMF. In this letter, we propose two projected gradient methods for NMF, both of which exhibit strong optimization properties. We discuss efficient implementations and demonstrate that one of the proposed methods converges faster than the popular multiplicative update approach. A simple Matlab code is also provided.",
Retinal Blood Vessel Segmentation Using Line Operators and Support Vector Classification,"In the framework of computer-aided diagnosis of eye diseases, retinal vessel segmentation based on line operators is proposed. A line detector, previously used in mammography, is applied to the green channel of the retinal image. It is based on the evaluation of the average grey level along lines of fixed length passing through the target pixel at different orientations. Two segmentation methods are considered. The first uses the basic line detector whose response is thresholded to obtain unsupervised pixel classification. As a further development, we employ two orthogonal line detectors along with the grey level of the target pixel to construct a feature vector for supervised classification using a support vector machine. The effectiveness of both methods is demonstrated through receiver operating characteristic analysis on two publicly available databases of color fundus images.",
Illumination Invariant Face Recognition Using Near-Infrared Images,"Most current face recognition systems are designed for indoor, cooperative-user applications. However, even in thus-constrained applications, most existing systems, academic and commercial, are compromised in accuracy by changes in environmental illumination. In this paper, we present a novel solution for illumination invariant face recognition for indoor, cooperative-user applications. First, we present an active near infrared (NIR) imaging system that is able to produce face images of good condition regardless of visible lights in the environment. Second, we show that the resulting face images encode intrinsic information of the face, subject only to a monotonic transform in the gray tone; based on this, we use local binary pattern (LBP) features to compensate for the monotonic transform, thus deriving an illumination invariant face representation. Then, we present methods for face recognition using NIR images; statistical learning algorithms are used to extract most discriminative features from a large pool of invariant LBP features and construct a highly accurate face matching engine. Finally, we present a system that is able to achieve accurate and fast face recognition in practice, in which a method is provided to deal with specular reflections of active NIR lights on eyeglasses, a critical issue in active NIR image-based face recognition. Extensive, comparative results are provided to evaluate the imaging hardware, the face and eye detection algorithms, and the face recognition algorithms and systems, with respect to various factors, including illumination, eyeglasses, time lapse, and ethnic groups","Lighting,
Face recognition,
Infrared imaging,
Optical imaging,
Statistical learning,
Feature extraction,
Engines,
Optical reflection,
Hardware,
Face detection"
Objects in Context,"In the task of visual object categorization, semantic context can play the very important role of reducing ambiguity in objects' visual appearance. In this work we propose to incorporate semantic object context as a post-processing step into any off-the-shelf object categorization model. Using a conditional random field (CRF) framework, our approach maximizes object label agreement according to contextual relevance. We compare two sources of context: one learned from training data and another queried from Google Sets. The overall performance of the proposed framework is evaluated on the PASCAL and MSRC datasets. Our findings conclude that incorporating context into object categorization greatly improves categorization accuracy.","Layout,
Context modeling,
Computer vision,
Image segmentation,
Psychology,
Computer science,
Training data,
Object recognition,
Statistics,
Electrical capacitance tomography"
Relay Node Placement in Wireless Sensor Networks,"A wireless sensor network consists of many low-cost, low-power sensor nodes, which can perform sensing, simple computation, and transmission of sensed information. Long distance transmission by sensor nodes is not energy efficient since energy consumption is a superlinear function of the transmission distance. One approach to prolonging network lifetime while preserving network connectivity is to deploy a small number of costly, but more powerful, relay nodes whose main task is communication with other sensor or relay nodes. In this paper, we assume that sensor nodes have communication range r>0, while relay nodes have communication range Rgesr, and we study two versions of relay node placement problems. In the first version, we want to deploy the minimum number of relay nodes so that, between each pair of sensor nodes, there is a connecting path consisting of relay and/or sensor nodes. In the second version, we want to deploy the minimum number of relay nodes so that, between each pair of sensor nodes, there is a connecting path consisting solely of relay nodes. We present a polynomial time 7-approximation algorithm for the first problem and a polynomial time (5+epsi)-approximation algorithm for the second problem, where epsi>0 can be any given constant","computational complexity,
wireless sensor networks"
Social Computing: From Social Informatics to Social Intelligence,"Social computing represents a new computing paradigm and an interdisciplinary research and application field. Undoubtedly, it strongly influences system and software developments in the years to come. We expect that social computing's scope continues to expand and its applications multiply. From both theoretical and technological perspectives, social computing technologies moves beyond social information processing towards emphasizing social intelligence. As we've discussed, the move from social informatics to social intelligence is achieved by modeling and analyzing social behavior, by capturing human social dynamics, and by creating artificial social agents and generating and managing actionable social knowledge","Social network services,
Informatics,
Application software,
Computational modeling,
Online Communities/Technical Collaboration,
Humans,
Concurrent computing,
Software tools,
Psychology,
Multimedia databases"
Examining the Challenges of Scientific Workflows,"Workflows have emerged as a paradigm for representing and managing complex distributed computations and are used to accelerate the pace of scientific progress. A recent National Science Foundation workshop brought together domain, computer, and social scientists to discuss requirements of future scientific applications and the challenges they present to current workflow technologies.",
Probabilistic Linear Discriminant Analysis for Inferences About Identity,"Many current face recognition algorithms perform badly when the lighting or pose of the probe and gallery images differ. In this paper we present a novel algorithm designed for these conditions. We describe face data as resulting from a generative model which incorporates both within-individual and between-individual variation. In recognition we calculate the likelihood that the differences between face images are entirely due to within-individual variability. We extend this to the non-linear case where an arbitrary face manifold can be described and noise is position-dependent. We also develop a ""tied"" version of the algorithm that allows explicit comparison across quite different viewing conditions. We demonstrate that our model produces state of the art results for (i) frontal face recognition (ii) face recognition under varying pose.","Linear discriminant analysis,
Face recognition,
Probes,
Image recognition,
Lighting,
Vectors,
Computer science,
Educational institutions,
Inference algorithms,
Algorithm design and analysis"
An Optimal Radial Profile Order Based on the Golden Ratio for Time-Resolved MRI,"In dynamic magnetic resonance imaging (MRI) studies, the motion kinetics or the contrast variability are often hard to predict, hampering an appropriate choice of the image update rate or the temporal resolution. A constant azimuthal profile spacing (111.246deg), based on the Golden Ratio, is investigated as optimal for image reconstruction from an arbitrary number of profiles in radial MRI. The profile order is evaluated and compared with a uniform profile distribution in terms of signal-to-noise ratio (SNR) and artifact level. The favorable characteristics of such a profile order are exemplified in two applications on healthy volunteers. First, an advanced sliding window reconstruction scheme is applied to dynamic cardiac imaging, with a reconstruction window that can be flexibly adjusted according to the extent of cardiac motion that is acceptable. Second, a contrast-enhancing k-space filter is presented that permits reconstructing an arbitrary number of images at arbitrary time points from one raw data set. The filter was utilized to depict the T1-relaxation in the brain after a single inversion prepulse. While a uniform profile distribution with a constant angle increment is optimal for a fixed and predetermined number of profiles, a profile distribution based on the Golden Ratio proved to be an appropriate solution for an arbitrary number of profiles","Magnetic resonance imaging,
Image reconstruction,
Spatial resolution,
Filters,
Biomedical imaging,
Europe,
Kinetic theory,
Image resolution,
Signal to noise ratio,
Biomedical engineering"
Unsupervised Activity Perception by Hierarchical Bayesian Models,"We propose a novel unsupervised learning framework for activity perception. To understand activities in complicated scenes from visual data, we propose a hierarchical Bayesian model to connect three elements: low-level visual features, simple ""atomic"" activities, and multi-agent interactions. Atomic activities are modeled as distributions over low-level visual features, and interactions are modeled as distributions over atomic activities. Our models improve existing language models such as latent Dirichlet allocation (LDA) and hierarchical Dirichlet process (HDP) by modeling interactions without supervision. Our data sets are challenging video sequences from crowded traffic scenes with many kinds of activities co-occurring. Our approach provides a summary of typical atomic activities and interactions in the scene. Unusual activities and interactions are found, with natural probabilistic explanations. Our method supports flexible high-level queries on activities and interactions using atomic activities as components.","Bayesian methods,
Layout,
Linear discriminant analysis,
Surveillance,
Video sequences,
Road vehicles,
Computer science,
Artificial intelligence,
Unsupervised learning,
Traffic control"
Unsupervised Learning of Invariant Feature Hierarchies with Applications to Object Recognition,"We present an unsupervised method for learning a hierarchy of sparse feature detectors that are invariant to small shifts and distortions. The resulting feature extractor consists of multiple convolution filters, followed by a feature-pooling layer that computes the max of each filter output within adjacent windows, and a point-wise sigmoid non-linearity. A second level of larger and more invariant features is obtained by training the same algorithm on patches of features from the first level. Training a supervised classifier on these features yields 0.64% error on MNIST, and 54% average recognition rate on Caltech 101 with 30 training samples per category. While the resulting architecture is similar to convolutional networks, the layer-wise unsupervised training procedure alleviates the over-parameterization problems that plague purely supervised learning procedures, and yields good performance with very few labeled training samples.","Unsupervised learning,
Object recognition,
Feature extraction,
Computer vision,
Detectors,
Computer architecture,
Gabor filters,
Object detection,
Convolution,
Supervised learning"
Implicit Active Contours Driven by Local Binary Fitting Energy,"Local image information is crucial for accurate segmentation of images with intensity inhomogeneity. However, image information in local region is not embedded in popular region-based active contour models, such as the piecewise constant models. In this paper, we propose a region-based active contour model that is able to utilize image information in local regions. The major contribution of this paper is the introduction of a local binary fitting energy with a kernel function, which enables the extraction of accurate local image information. Therefore, our model can be used to segment images with intensity inhomogeneity, which overcomes the limitation of piecewise constant models. Comparisons with other major region-based models, such as the piece-wise smooth model, show the advantages of our method in terms of computational efficiency and accuracy. In addition, the proposed method has promising application to image denoising.","Active contours,
Image segmentation,
Computational efficiency,
Level set,
Kernel,
Force control,
Fitting,
Data mining,
Image denoising,
Mathematics"
Toward a Deeper Understanding of the Role of Interaction in Information Visualization,"Even though interaction is an important part of information visualization (Infovis), it has garnered a relatively low level of attention from the Infovis community. A few frameworks and taxonomies of Infovis interaction techniques exist, but they typically focus on low-level operations and do not address the variety of benefits interaction provides. After conducting an extensive review of Infovis systems and their interactive capabilities, we propose seven general categories of interaction techniques widely used in Infovis: 1) Select, 2) Explore, 3) Reconfigure, 4) Encode, 5) Abstract/Elaborate, 6) Filter, and 7) Connect. These categories are organized around a user's intent while interacting with a system rather than the low-level interaction techniques provided by a system. The categories can act as a framework to help discuss and evaluate interaction techniques and hopefully lay an initial foundation toward a deeper understanding and a science of interaction.","Visual analytics,
Taxonomy,
Data visualization,
Research and development,
Filters,
Computer graphics,
Rendering (computer graphics),
Computer displays,
Human computer interaction,
Conference proceedings"
On Three Types of Covering-Based Rough Sets,"Rough set theory is a useful tool for data mining. It is based on equivalence relations and has been extended to covering-based generalized rough set. This paper studies three kinds of covering generalized rough sets for dealing with the vagueness and granularity in information systems. First, we examine the properties of approximation operations generated by a covering in comparison with those of the Pawlak's rough sets. Then, we propose concepts and conditions for two coverings to generate an identical lower approximation operation and an identical upper approximation operation. After the discussion on the interdependency of covering lower and upper approximation operations, we address the axiomization issue of covering lower and upper approximation operations. In addition, we study the relationships between the covering lower approximation and the interior operator and also the relationships between the covering upper approximation and the closure operator. Finally, this paper explores the relationships among these three types of covering rough sets.",
A Benchmark for the Comparison of 3-D Motion Segmentation Algorithms,"Over the past few years, several methods for segmenting a scene containing multiple rigidly moving objects have been proposed. However, most existing methods have been tested on a handful of sequences only, and each method has been often tested on a different set of sequences. Therefore, the comparison of different methods has been fairly limited. In this paper, we compare four 3D motion segmentation algorithms for affine cameras on a benchmark of 155 motion sequences of checkerboard, traffic, and articulated scenes.","Motion segmentation,
Computer vision,
Layout,
Cameras,
Statistical analysis,
Databases,
Application software,
Image segmentation,
Benchmark testing,
Traffic control"
Model-driven Development of Complex Software: A Research Roadmap,"The term model-driven engineering (MDE) is typically used to describe software development approaches in which abstract models of software systems are created and systematically transformed to concrete implementations. In this paper we give an overview of current research in MDE and discuss some of the major challenges that must be tackled in order to realize the MDE vision of software development. We argue that full realizations of the MDE vision may not be possible in the near to medium-term primarily because of the wicked problems involved. On the other hand, attempting to realize the vision will provide insights that can be used to significantly reduce the gap between evolving software complexity and the technologies used to manage complexity.","Software systems,
Model driven engineering,
Programming,
Computer science,
Unified modeling language,
Software testing,
Systems engineering and theory,
Computer industry,
Streaming media,
Reliability engineering"
Detecting Pedestrians by Learning Shapelet Features,"In this paper, we address the problem of detecting pedestrians in still images. We introduce an algorithm for learning shapelet features, a set of mid-level features. These features are focused on local regions of the image and are built from low-level gradient information that discriminates between pedestrian and non-pedestrian classes. Using Ad-aBoost, these shapelet features are created as a combination of oriented gradient responses. To train the final classifier, we use AdaBoost for a second time to select a subset of our learned shapelets. By first focusing locally on smaller feature sets, our algorithm attempts to harvest more useful information than by examining all the low-level features together. We present quantitative results demonstrating the effectiveness of our algorithm. In particular, we obtain an error rate 14 percentage points lower (at 10-6 FPPW) than the previous state of the art detector of Dalal and Triggs on the INRIA dataset.",
Tracking People by Learning Their Appearance,"An open vision problem is to automatically track the articulations of people from a video sequence. This problem is difficult because one needs to determine both the number of people in each frame and estimate their configurations. But, finding people and localizing their limbs is hard because people can move fast and unpredictably, can appear in a variety of poses and clothes, and are often surrounded by limb-like clutter. We develop a completely automatic system that works in two stages; it first builds a model of appearance of each person in a video and then it tracks by detecting those models in each frame (""tracking by model-building and detection""). We develop two algorithms that build models; one bottom-up approach groups together candidate body parts found throughout a sequence. We also describe a top-down approach that automatically builds people-models by detecting convenient key poses within a sequence. We finally show that building a discriminative model of appearance is quite helpful since it exploits structure in a background (without background-subtraction). We demonstrate the resulting tracker on hundreds of thousands of frames of unscripted indoor and outdoor activity, a feature-length film (""Run Lola Run""), and legacy sports footage (from the 2002 World Series and 1998 Winter Olympics). Experiments suggest that our system 1) can count distinct individuals, 2) can identify and track them, 3) can recover when it loses track, for example, if individuals are occluded or briefly leave the view, 4) can identify body configuration accurately, and 5) is not dependent on particular models of human motion","Video sequences,
Tracking,
Humans,
Surveillance,
Hidden Markov models,
Buildings,
Biological system modeling,
Computer vision,
Computer interfaces,
Data mining"
Mechanism Design via Differential Privacy,"We study the role that privacy-preserving algorithms, which prevent the leakage of specific information about participants, can play in the design of mechanisms for strategic agents, which must encourage players to honestly report information. Specifically, we show that the recent notion of differential privacv, in addition to its own intrinsic virtue, can ensure that participants have limited effect on the outcome of the mechanism, and as a consequence have limited incentive to lie. More precisely, mechanisms with differential privacy are approximate dominant strategy under arbitrary player utility functions, are automatically resilient to coalitions, and easily allow repeatability. We study several special cases of the unlimited supply auction problem, providing new results for digital goods auctions, attribute auctions, and auctions with arbitrary structural constraints on the prices. As an important prelude to developing a privacy-preserving auction mechanism, we introduce and study a generalization of previous privacy work that accommodates the high sensitivity of the auction setting, where a single participant may dramatically alter the optimal fixed price, and a slight change in the offered price may take the revenue from optimal to zero.","Algorithm design and analysis,
Data privacy,
Silicon,
Computer science,
Concrete,
Protection,
Data analysis,
Robustness,
Utility theory,
Pricing"
Fuzzy-Rough Sets Assisted Attribute Selection,"Attribute selection (AS) refers to the problem of selecting those input attributes or features that are most predictive of a given outcome; a problem encountered in many areas such as machine learning, pattern recognition and signal processing. Unlike other dimensionality reduction methods, attribute selectors preserve the original meaning of the attributes after reduction. This has found application in tasks that involve datasets containing huge numbers of attributes (in the order of tens of thousands) which, for some learning algorithms, might be impossible to process further. Recent examples include text processing and web content classification. AS techniques have also been applied to small and medium-sized datasets in order to locate the most informative attributes for later use. One of the many successful applications of rough set theory has been to this area. The rough set ideology of using only the supplied data and no other information has many benefits in AS, where most other methods require supplementary knowledge. However, the main limitation of rough set-based attribute selection in the literature is the restrictive requirement that all data is discrete. In classical rough set theory, it is not possible to consider real-valued or noisy data. This paper investigates a novel approach based on fuzzy-rough sets, fuzzy rough feature selection (FRFS), that addresses these problems and retains dataset semantics. FRFS is applied to two challenging domains where a feature reducing step is important; namely, web content classification and complex systems monitoring. The utility of this approach is demonstrated and is compared empirically with several dimensionality reducers. In the experimental studies, FRFS is shown to equal or improve classification accuracy when compared to the results from unreduced data. Classifiers that use a lower dimensional set of attributes which are retained by fuzzy-rough reduction outperform those that employ more attributes returned by the existing crisp rough reduction method. In addition, it is shown that FRFS is more powerful than the other AS techniques in the comparative study",
Spray and Focus: Efficient Mobility-Assisted Routing for Heterogeneous and Correlated Mobility,"Intermittently connected mobile networks are wireless networks where most of the time there does not exist a complete path from the source to the destination. There are many real networks that follow this model, for example, wildlife tracking sensor networks, military networks, vehicular ad hoc networks (VANETs), etc. To deal with such networks researchers have suggested to use controlled replication or ""spraying "" methods that can reduce the overhead of flooding-based schemes by distributing a small number of copies to only a few relays. These relays then ""look"" for the destination in parallel as they move into the network. Although such schemes can perform well in scenarios with high mobility (e.g. VANETs), they struggle in situations were mobility is slow and correlated in space and/or time. To route messages efficiently in such networks, we propose a scheme that also distributes a small number of copies to few relays. However, each relay can then forward its copy further using a single-copy utility-based scheme, instead of naively waiting to deliver it to the destination itself. This scheme exploits all the advantages of controlled replication, but is also able to identify appropriate forwarding opportunities that could deliver the message faster. Simulation results for traditional mobility models, as well as for a more realistic ""community-based"" model, indicate that our scheme can reduce the delay of existing spraying techniques up to 20 times in some scenarios","Spraying,
Relays,
Ad hoc networks,
Routing protocols,
Computer science,
Mobile computing,
Wildlife,
Military computing,
Mobile ad hoc networks,
Internet telephony"
High-Performance Rotation Invariant Multiview Face Detection,"Rotation invariant multiview face detection (MVFD) aims to detect faces with arbitrary rotation-in-plane (RIP) and rotation-off-plane (ROP) angles in still images or video sequences. MVFD is crucial as the first step in automatic face processing for general applications since face images are seldom upright and frontal unless they are taken cooperatively. In this paper, we propose a series of innovative methods to construct a high-performance rotation invariant multiview face detector, including the width-first-search (WFS) tree detector structure, the vector boosting algorithm for learning vector-output strong classifiers, the domain-partition-based weak learning method, the sparse feature in granular space, and the heuristic search for sparse feature selection. As a result of that, our multiview face detector achieves low computational complexity, broad detection scope, and high detection accuracy on both standard testing sets and real-life images",
COMPARE: Classification of Morphological Patterns Using Adaptive Regional Elements,"This paper presents a method for classification of structural brain magnetic resonance (MR) images, by using a combination of deformation-based morphometry and machine learning methods. A morphological representation of the anatomy of interest is first obtained using a high-dimensional mass-preserving template warping method, which results in tissue density maps that constitute local tissue volumetric measurements. Regions that display strong correlations between tissue volume and classification (clinical) variables are extracted using a watershed segmentation algorithm, taking into account the regional smoothness of the correlation map which is estimated by a cross-validation strategy to achieve robustness to outliers. A volume increment algorithm is then applied to these regions to extract regional volumetric features, from which a feature selection technique using support vector machine (SVM)-based criteria is used to select the most discriminative features, according to their effect on the upper bound of the leave-one-out generalization error. Finally, SVM-based classification is applied using the best set of features, and it is tested using a leave-one-out cross-validation strategy. The results on MR brain images of healthy controls and schizophrenia patients demonstrate not only high classification accuracy (91.8% for female subjects and 90.8% for male subjects), but also good stability with respect to the number of features selected and the size of SVM kernel used",
Feature Location Using Probabilistic Ranking of Methods Based on Execution Scenarios and Information Retrieval,"This paper recasts the problem of feature location in source code as a decision-making problem in the presence of uncertainty. The solution to the problem is formulated as a combination of the opinions of different experts. The experts in this work are two existing techniques for feature location: a scenario-based probabilistic ranking of events and an information-retrieval-based technique that uses latent semantic indexing. The combination of these two experts is empirically evaluated through several case studies, which use the source code of the Mozilla Web browser and the Eclipse integrated development environment. The results show that the combination of experts significantly improves the effectiveness of feature location as compared to each of the experts used independently",
"Uncertain Fuzzy Clustering: Interval Type-2 Fuzzy Approach to
C
-Means","In many pattern recognition applications, it may be impossible in most cases to obtain perfect knowledge or information for a given pattern set. Uncertain information can create imperfect expressions for pattern sets in various pattern recognition algorithms. Therefore, various types of uncertainty may be taken into account when performing several pattern recognition methods. When one performs clustering with fuzzy sets, fuzzy membership values express assignment availability of patterns for clusters. However, when one assigns fuzzy memberships to a pattern set, imperfect information for a pattern set involves uncertainty which exist in the various parameters that are used in fuzzy membership assignment. When one encounters fuzzy clustering, fuzzy membership design includes various uncertainties (e.g., distance measure, fuzzifier, prototypes, etc.). In this paper, we focus on the uncertainty associated with the fuzzifier parameter m that controls the amount of fuzziness of the final C-partition in the fuzzy C-means (FCM) algorithm. To design and manage uncertainty for fuzzifier m, we extend a pattern set to interval type-2 fuzzy sets using two fuzzifiers m1 and m2 which creates a footprint of uncertainty (FOU) for the fuzzifier m. Then, we incorporate this interval type-2 fuzzy set into FCM to observe the effect of managing uncertainty from the two fuzzifiers. We also provide some solutions to type-reduction and defuzzification (i.e., cluster center updating and hard-partitioning) in FCM. Several experimental results are given to show the validity of our method","Fuzzy sets,
Pattern recognition,
Computational complexity,
Employment,
Computer science,
Clustering algorithms,
Availability,
Measurement uncertainty,
Prototypes,
Fuzzy control"
Research Directions in Requirements Engineering,"In this paper, we review current requirements engineering (RE) research and identify future research directions suggested by emerging software needs. First, we overview the state of the art in RE research. The research is considered with respect to technologies developed to address specific requirements tasks, such as elicitation, modeling, and analysis. Such a review enables us to identify mature areas of research, as well as areas that warrant further investigation. Next, we review several strategies for performing and extending RE research results, to help delineate the scope of future research directions. Finally, we highlight what we consider to be the ""hot"" current and future research topics, which aim to address RE needs for emerging systems of the future.","Software engineering,
Computer science,
Context modeling,
Software systems,
Model driven engineering,
Conferences,
Buildings,
Documentation,
Software design,
USA Councils"
Global Software Engineering: The Future of Socio-technical Coordination,"Globally-distributed projects are rapidly becoming the norm for large software systems, even as it becomes clear that global distribution of a project seriously impairs critical coordination mechanisms. In this paper, I describe a desired future for global development and the problems that stand in the way of achieving that vision. I review research and lay out research challenges in four critical areas: software architecture, eliciting and communicating requirements, environments and tools, and orchestrating global development. I conclude by noting the need for a systematic understanding of what drives the need to coordinate and effective mechanisms for bringing it about.","Software engineering,
Open source software,
Computer science,
Collaborative work,
Collaborative software,
Computer industry,
Software architecture,
Collaborative tools,
International collaboration,
Software systems"
"Air Pollution, Ultrafine and Nanoparticle Toxicology: Cellular and Molecular Interactions","Nanotechnology is involved with the creation and/or manipulation of materials at the nanometer (nm) scale, and has arisen as a consequence of the novel properties that materials exhibit within the ""nano"" size range. The attraction of producing, and exploiting nanoparticles (NPs; one dimension less than 100 nm) is a consequence of the fact that the properties are often strikingly different from bulk forms composed from the same material. As a consequence, the field of nanotechnology has generated substantial interest resulting in incorporation of NPs into a wide variety of products including electronics, food, clothing, medicines, cosmetics and sporting equipment. While there is general recognition that nanotechnology has the potential to advance science, quality of life and to generate substantial financial gains, a number of reports suggest that potential toxicity should be considered in order to allow the safe and sustainable development of such products. For example, substances which are ordinarily innocuous can elicit toxicity due to the altered chemical and physical properties that become evident within nano dimensions leading to potentially detrimental consequences for the producer, consumer or environment. Research into respirable air pollution particles (PM10) has focused on the role of ultra fine particle (diameter less than 100 nm) in inducing oxidative stress leading to inflammation and resulting in exacerbation of preexisting respiratory and cardiovascular disease. Epidemiological studies have repeatedly found a positive correlation between the level of particulate air pollution and increased morbidity and mortality rates in both adults and children. Such studies have also identified a link between respiratory ill health and the number of ambient ultrafine particles. In vivo and in vitro toxicology studies confirm that for low solubility, low toxicity materials such as TiO2, carbon black and polystyrene beads, ultrafine particles are more toxic and inflammogenic than fine particles. In many of these studies the term ""ultrafine particle"" can be directly exchanged for nanoparticle, as these particles are manufactured industrially. In such studies the NPs generate reactive oxygen species (ROS) to a greater extent than larger particles leading to increased transcription of pro-inflammatory mediators via intracellular signaling pathways including calcium and oxidative stress. To date, only limited NP compositions and structures have been tested, including materials such as carbon, polystyrene beads and TiO2 as surrogate particles that aimed to represent particulate air pollution. All of these materials are generally low toxicity and low solubility. Much work is required to identify whether the conclusions made for such materials can be extrapolated to engineered nanoparticles varying not only in size but also, shape, composition, structure, surface area, surface coating, and aggregation state. Therefore, it is necessary to reveal if the diversity of NPs available will confer to a varied extent and mechanisms of toxicity.","Air pollution,
Toxicology,
Nanotechnology,
Biological materials,
Nanoparticles,
Stress,
Organic materials,
Carbon dioxide,
Composite materials,
Surface contamination"
NodeTrix: a Hybrid Visualization of Social Networks,"The need to visualize large social networks is growing as hardware capabilities make analyzing large networks feasible and many new data sets become available. Unfortunately, the visualizations in existing systems do not satisfactorily resolve the basic dilemma of being readable both for the global structure of the network and also for detailed analysis of local communities. To address this problem, we present NodeTrix, a hybrid representation for networks that combines the advantages of two traditional representations: node-link diagrams are used to show the global structure of a network, while arbitrary portions of the network can be shown as adjacency matrices to better support the analysis of communities. A key contribution is a set of interaction techniques. These allow analysts to create a NodeTrix visualization by dragging selections to and from node-link and matrix forms, and to flexibly manipulate the NodeTrix representation to explore the dataset and create meaningful summary visualizations of their findings. Finally, we present a case study applying NodeTrix to the analysis of the InfoVis 2004 coauthorship dataset to illustrate the capabilities of NodeTrix as both an exploration tool and an effective means of communicating results.",
Non-homogeneous Content-driven Video-retargeting,"Video retargeting is the process of transforming an existing video to fit the dimensions of an arbitrary display. A compelling retargeting aims at preserving the viewers' experience by maintaining the information content of important regions in the frame, whilst keeping their aspect ratio. An efficient algorithm for video retargeting is introduced. It consists of two stages. First, the frame is analyzed to detect the importance of each region in the frame. Then, a transformation that respects the analysis shrinks less important regions more than important ones. Our analysis is fully automatic and based on local saliency, motion detection and object detectors. The performance of the proposed algorithm is demonstrated on a variety of video sequences, and compared to the state of the art in image retargeting.","Motion detection,
Face detection,
Detectors,
TV,
Computer science,
Computer displays,
Image motion analysis,
Object detection,
Video sequences,
Large screen displays"
The Current State and Future of Search Based Software Engineering,"This paper describes work on the application of optimization techniques in software engineering. These optimization techniques come from the operations research and metaheuristic computation research communities. The paper briefly reviews widely used optimization techniques and the key ingredients required for their successful application to software engineering, providing an overview of existing results in eight software engineering application domains. The paper also describes the benefits that are likely to accrue from the growing body of work in this area and provides a set of open problems, challenges and areas for future work.","Software engineering,
Application software,
Software testing,
Educational institutions,
Gold,
Optimizing compilers,
Computer science,
Instruments,
Councils,
Operations research"
Stochastic Fluid Theory for P2P Streaming Systems,"We develop a simple stochastic fluid model that seeks to expose the fundamental characteristics and limitations of P2P streaming systems. This model accounts for many of the essential features of a P2P streaming system, including the peers' realtime demand for content, peer churn (peers joining and leaving), peers with heterogeneous upload capacity, limited infrastructure capacity, and peer buffering and playback delay. The model is tractable, providing closed-form expressions which can be used to shed insight on the fundamental behavior of P2P streaming systems. The model shows that performance is largely determined by a critical value. When the system is of moderate-to-large size, if a certain ratio of traffic loads exceeds the critical value, the system performs well; otherwise, the system performs poorly. Furthermore, large systems have better performance than small systems since they are more resilient to bandwidth fluctuations caused by peer churn. Finally, buffering can dramatically improve performance in the critical region, for both small and large systems. In particular, buffering can bring more improvement than can additional infrastructure bandwidth.","Stochastic systems,
Streaming media,
Peer to peer computing,
USA Councils,
Bandwidth,
Bit rate,
Internet,
Communications Society,
Information science,
Delay"
Biometric Recognition Using 3D Ear Shape,"Previous works have shown that the ear is a promising candidate for biometric identification. However, in prior work, the preprocessing of ear images has had manual steps and algorithms have not necessarily handled problems caused by hair and earrings. We present a complete system for ear biometrics, including automated segmentation of the ear in a profile view image and 3D shape matching for recognition. We evaluated this system with the largest experimental study to date in ear biometrics, achieving a rank-one recognition rate of 97.8 percent for an identification scenario and an equal error rate of 1.2 percent for a verification scenario on a database of 415 subjects and 1,386 total probes.","Biometrics,
Ear,
Shape,
Image recognition,
Image segmentation,
Face recognition,
Error analysis,
Skin,
Active contours,
Humans"
An Efficient Earth Mover's Distance Algorithm for Robust Histogram Comparison,"We propose EMD-L1: a fast and exact algorithm for computing the earth mover's distance (EMD) between a pair of histograms. The efficiency of the new algorithm enables its application to problems that were previously prohibitive due to high time complexities. The proposed EMD-L1 significantly simplifies the original linear programming formulation of EMD. Exploiting the L1 metric structure, the number of unknown variables in EMD-L1 is reduced to O(N) from O(N2) of the original EMD for a histogram with N bins. In addition, the number of constraints is reduced by half and the objective function of the linear program is simplified. Formally, without any approximation, we prove that the EMD-L1 formulation is equivalent to the original EMD with a L1 ground distance. To perform the EMD-L1 computation, we propose an efficient tree-based algorithm, Tree-EMD. Tree-EMD exploits the fact that a basic feasible solution of the simplex algorithm-based solver forms a spanning tree when we interpret EMD-L1 as a network flow optimization problem. We empirically show that this new algorithm has an average time complexity of O(N2), which significantly improves the best reported supercubic complexity of the original EMD. The accuracy of the proposed methods is evaluated by experiments for two computation-intensive problems: shape recognition and interest point matching using multidimensional histogram-based local features. For shape recognition, EMD-L1 is applied to compare shape contexts on the widely tested MPEG7 shape data set, as well as an articulated shape data set. For interest point matching, SIFT, shape context and spin image are tested on both synthetic and real image pairs with large geometrical deformation, illumination change, and heavy intensity noise. The results demonstrate that our EMD-L1-based solutions outperform previously reported state-of-the-art features and distance measures in solving the two tasks","Earth,
Robustness,
Histograms,
Shape,
Testing,
Linear programming,
Multidimensional systems,
MPEG 7 Standard,
Lighting,
Noise shaping"
Image Segmentation by Probabilistic Bottom-Up Aggregation and Cue Integration,"We present a parameter free approach that utilizes multiple cues for image segmentation. Beginning with an image, we execute a sequence of bottom-up aggregation steps in which pixels are gradually merged to produce larger and larger regions. In each step we consider pairs of adjacent regions and provide a probability measure to assess whether or not they should be included in the same segment. Our probabilistic formulation takes into account intensity and texture distributions in a local area around each region. It further incorporates priors based on the geometry of the regions. Finally, posteriors based on intensity and texture cues are combined using a mixture of experts formulation. This probabilistic approach is integrated into a graph coarsening scheme providing a complete hierarchical segmentation of the image. The algorithm complexity is linear in the number of the image pixels and it requires almost no user-tuned parameters. We test our method on a variety of gray scale images and compare our results to several existing segmentation algorithms.","Image segmentation,
Pixel,
Statistics,
Partitioning algorithms,
Computer science,
Mathematics,
Information geometry,
Testing,
Optimization methods,
Robustness"
A Fingerprint Orientation Model Based on 2D Fourier Expansion (FOMFE) and Its Application to Singular-Point Detection and Fingerprint Indexing,"In this paper, we have proposed a fingerprint orientation model based on 2D Fourier expansions (FOMFE) in the phase plane. The FOMFE does not require prior knowledge of singular points (SPs). It is able to describe the overall ridge topology seamlessly, including the SP regions, even for noisy fingerprints. Our statistical experiments on a public database show that the proposed FOMFE can significantly improve the accuracy of fingerprint feature extraction and thus that of fingerprint matching. Moreover, the FOMFE has a low-computational cost and can work very efficiently on large fingerprint databases. The FOMFE provides a comprehensive description for orientation features, which has enabled its beneficial use in feature-related applications such as fingerprint indexing. Unlike most indexing schemes using raw orientation data, we exploit FOMFE model coefficients to generate the feature vector. Our indexing experiments show remarkable results using different fingerprint databases",
A General Framework for Wireless Spectrum Auctions,"We propose a real-time spectrum auction framework to distribute spectrum among a large number wireless users under interference constraints. Our approach achieves conflict-free spectrum allocations that maximize auction revenue and spectrum utilization. Our design includes a compact and yet highly expressive bidding language, various pricing models to control tradeoffs between revenue and fairness, and fast auction clearing algorithms to compute revenue-maximizing prices and allocations. Both analytical and experimental results verify the efficiency of the proposed approach. We conclude that bidding behaviors and pricing models have significant impact on auction outcomes. A spectrum auction system must consider local demand and spectrum availability in order to maximize revenue and utilization.",
Adaptive Control of Duty Cycling in Energy-Harvesting Wireless Sensor Networks,"Increasingly many wireless sensor network deployments are using harvested environmental energy to extend system lifetime. Because the temporal profiles of such energy sources exhibit great variability due to dynamic weather patterns, an important problem is designing an adaptive duty-cycling mechanism that allows sensor nodes to maintain their power supply at sufficient levels (energy neutral operation) by adapting to changing environmental conditions. Existing techniques to address this problem are minimally adaptive and assume a priori knowledge of the energy profile. While such approaches are reasonable in environments that exhibit low variance, we find that it is highly inefficient in more variable scenarios. We introduce a new technique for solving this problem based on results from adaptive control theory and show that we achieve better performance than previous approaches on a broader class of energy source data sets. Additionally, we include a tunable mechanism for reducing the variance of the node's duty cycle over time, which is an important feature in tasks such as event monitoring. We obtain reductions in variance as great as two-thirds without compromising task performance or ability to maintain energy neutral operation.",
A Comparison of Decision Tree Ensemble Creation Techniques,"We experimentally evaluate bagging and seven other randomization-based approaches to creating an ensemble of decision tree classifiers. Statistical tests were performed on experimental results from 57 publicly available data sets. When cross-validation comparisons were tested for statistical significance, the best method was statistically more accurate than bagging on only eight of the 57 data sets. Alternatively, examining the average ranks of the algorithms across the group of data sets, we find that boosting, random forests, and randomized trees are statistically significantly better than bagging. Because our results suggest that using an appropriate ensemble size is important, we introduce an algorithm that decides when a sufficient number of classifiers has been created for an ensemble. Our algorithm uses the out-of-bag error estimate, and is shown to result in an accurate ensemble for those methods that incorporate bagging into the construction of the ensemble","Decision trees,
Bagging,
Boosting,
Testing,
Classification tree analysis,
Performance evaluation,
Statistical analysis,
Sampling methods,
Training data"
A Formal Study of Shot Boundary Detection,"This paper conducts a formal study of the shot boundary detection problem. First, a general formal framework of shot boundary detection techniques is proposed. Three critical techniques, i.e., the representation of visual content, the construction of continuity signal and the classification of continuity values, are identified and formulated in the perspective of pattern recognition. Meanwhile, the major challenges to the framework are identified. Second, a comprehensive review of the existing approaches is conducted. The representative approaches are categorized and compared according to their roles in the formal framework. Based on the comparison of the existing approaches, optimal criteria for each module of the framework are discussed, which will provide practical guide for developing novel methods. Third, with all the above issues considered, we present a unified shot boundary detection system based on graph partition model. Extensive experiments are carried out on the platform of TRECVID. The experiments not only verify the optimal criteria discussed above, but also show that the proposed approach is among the best in the evaluation of TRECVID 2005. Finally, we conclude the paper and present some further discussions on what shot boundary detection can learn from other related fields","Gunshot detection systems,
Videos,
Content based retrieval,
NIST,
Support vector machines,
Indexing,
Computer science,
Signal processing,
Pattern recognition,
Multiresolution analysis"
Connected Shape-Size Pattern Spectra for Rotation and Scale-Invariant Classification of Gray-Scale Images,"In this paper, we describe a multiscale and multishape morphological method for pattern-based analysis and classification of gray-scale images using connected operators. Compared with existing methods, which use structuring elements, our method has three advantages. First, in our method, the time needed for computing pattern spectra does not depend on the number of scales or shapes used, i.e., the computation time is independent of the dimensions of the pattern spectrum. Second, size and strict shape attributes can be computed, which we use for the construction of joint 2D shape-size pattern spectra. Third, our method is significantly less sensitive to noise and is rotation-invariant. Although rotation invariance can also be approximated by methods using structuring elements at different angles, this tends to be computationally intensive. The classification performance of these methods is discussed using four image sets: Brodatz, COIL-20, COIL-100, and diatoms. The new method obtains better or equal classification performance to the best competitor with a 5 to 9-fold speed gain","Gray-scale,
Shape,
Filters,
Filtering,
Image databases,
Noise shaping,
Performance gain,
Morphology,
Image analysis,
Pattern analysis"
Bidding Protocols for Deploying Mobile Sensors,"Constructing a sensor network with a mix of mobile and static sensors can achieve a balance between sensor coverage and sensor cost. In this paper, we design two bidding protocols to guide the movement of mobile sensors in such sensor networks to increase the coverage to a desirable level. In the protocols, static sensors detect coverage holes locally by using Voronoi diagrams and bid mobile sensors to move. Mobile sensors accept the highest bids and heal the largest holes. Simulation results show that our protocols achieve suitable trade-off between coverage and sensor cost","Protocols,
Costs,
Wireless sensor networks,
Distributed algorithms,
Wireless communication,
Spread spectrum communication,
Scattering,
Network servers"
Efficient Mesh Router Placement in Wireless Mesh Networks,"The placement of mesh routers (MRs) in building a wireless mesh network (WMN) is the first step to ensure the desired network performance. Given a network domain, the fundamental issue in placing MRs is to find the minimal configuration of MRs so as to satisfy the network coverage, connectivity, and Internet traffic demand. In this paper, the problem is addressed under a constraint network model in which the traffic demand is non-uniformly distributed and the candidate positions for MRs are pre-decided. After formulating the MR placement problem, we first provide the theoretical analysis to validate the traffic demand and determine the optimal position of Internet gateway (IGW). To reduce complexity of determining the locations of MRs while satisfying the traffic constraint, we propose an effective heuristic algorithm to obtain an close-to-optimal solution. Finally, our simulation results verify our analytical model and show the effectiveness of our proposed algorithm.","Wireless mesh networks,
Traffic control,
Telecommunication traffic,
Internet,
Throughput,
Computer science,
Heuristic algorithms,
Analytical models,
Interference constraints,
Costs"
Real-Time Visibility-Based Fusion of Depth Maps,"We present a viewpoint-based approach for the quick fusion of multiple stereo depth maps. Our method selects depth estimates for each pixel that minimize violations of visibility constraints and thus remove errors and inconsistencies from the depth maps to produce a consistent surface. We advocate a two-stage process in which the first stage generates potentially noisy, overlapping depth maps from a set of calibrated images and the second stage fuses these depth maps to obtain an integrated surface with higher accuracy, suppressed noise, and reduced redundancy. We show that by dividing the processing into two stages we are able to achieve a very high throughput because we are able to use a computationally cheap stereo algorithm and because this architecture is amenable to hardware-accelerated (GPU) implementations. A rigorous formulation based on the notion of stability of a depth estimate is presented first. It aims to determine the validity of a depth estimate by rendering multiple depth maps into the reference view as well as rendering the reference depth map into the other views in order to detect occlusions and free- space violations. We also present an approximate alternative formulation that selects and validates only one hypothesis based on confidence. Both formulations enable us to perform video-based reconstruction at up to 25 frames per second. We show results on the multi-view stereo evaluation benchmark datasets and several outdoors video sequences. Extensive quantitative analysis is performed using an accurately surveyed model of a real building as ground truth.","Noise reduction,
Noise generators,
Fusion power generation,
Fuses,
Redundancy,
Throughput,
Computer architecture,
Stability,
Image reconstruction,
Video sequences"
Node Placement Algorithm for Deployment of Two-Tier Wireless Mesh Networks,"In the deployment of wireless mesh networks (WMNs) the placement of Mesh Nodes (MNs) is an important design issue. The performance of WMNs is greatly affected by the location of the MNs. As it is difficult to place the MNs in a regular pattern in the real deployment, finding the optimal locations in the deployment environment is of much interest for the service providers. For a given possible locations for the MNs and the user density in the deployment environment, we aim to find the locations of the MNs to be used that maximizes the coverage and the connectivity of the network together. Due to high computational complexity of the exhaustive searching algorithm, an efficient local searching algorithm is proposed. Numerical results show that, the local search algorithm can give close to optimal performance with much lower time complexity than exhaustive searching.","Wireless mesh networks,
Area measurement,
Phase measurement,
Buildings,
Architecture,
Telecommunication traffic,
Internet,
Spread spectrum communication,
Signal processing,
Computer science"
Geographic Routing Using Hyperbolic Space,"We propose a scalable and reliable point-to-point routing algorithm for ad hoc wireless networks and sensor-nets. Our algorithm assigns to each node of the network a virtual coordinate in the hyperbolic plane, and performs greedy geographic routing with respect to these virtual coordinates. Unlike other proposed greedy routing algorithms based on virtual coordinates, our embedding guarantees that the greedy algorithm is always successful in finding a route to the destination, if such a route exists. We describe a distributed algorithm for computing each node's virtual coordinates in the hyperbolic plane, and for greedily routing packets to a destination point in the hyperbolic plane. (This destination may be the address of another node of the network, or it may be an address associated to a piece of content in a Distributed Hash Table. In the latter case we prove that the greedy routing strategy makes a consistent choice of the node responsible for the address, irrespective of the source address of the request.) We evaluate the resulting algorithm in terms of both path stretch and node congestion.","Routing,
Peer to peer computing,
Extraterrestrial measurements,
Wireless sensor networks,
Communications Society,
Computer science,
Fellows,
Computer network reliability,
Greedy algorithms,
Distributed algorithms"
Approximate Labeling via Graph Cuts Based on Linear Programming,"A new framework is presented for both understanding and developing graph-cut-based combinatorial algorithms suitable for the approximate optimization of a very wide class of Markov random fields (MRFs) that are frequently encountered in computer vision. The proposed framework utilizes tools from the duality theory of linear programming in order to provide an alternative and more general view of state-of-the-art techniques like the alpha-expansion algorithm, which is included merely as a special case. Moreover, contrary to alpha-expansion, the derived algorithms generate solutions with guaranteed optimality properties for a much wider class of problems, for example, even for MRFs with nonmetric potentials. In addition, they are capable of providing per-instance suboptimality bounds in all occasions, including discrete MRFs with an arbitrary potential function. These bounds prove to be very tight in practice (that is, very close to 1), which means that the resulting solutions are almost optimal. Our algorithms' effectiveness is demonstrated by presenting experimental results on a variety of low-level vision tasks, such as stereo matching, image restoration, image completion, and optical flow estimation, as well as on synthetic problems.","Labeling,
Linear programming,
Stereo vision,
Image restoration,
Cost function,
Markov random fields,
Computer vision,
Image motion analysis,
Optimization methods,
Pixel"
Empirical Validation of Three Software Metrics Suites to Predict Fault-Proneness of Object-Oriented Classes Developed Using Highly Iterative or Agile Software Development Processes,"Empirical validation of software metrics suites to predict fault proneness in object-oriented (OO) components is essential to ensure their practical use in industrial settings. In this paper, we empirically validate three OO metrics suites for their ability to predict software quality in terms of fault-proneness: the Chidamber and Kemerer (CK) metrics, Abreu's Metrics for Object-Oriented Design (MOOD), and Bansiya and Davis' Quality Metrics for Object-Oriented Design (QMOOD). Some CK class metrics have previously been shown to be good predictors of initial OO software quality. However, the other two suites have not been heavily validated except by their original proposers. Here, we explore the ability of these three metrics suites to predict fault-prone classes using defect data for six versions of Rhino, an open-source implementation of JavaScript written in Java. We conclude that the CK and QMOOD suites contain similar components and produce statistical models that are effective in detecting error-prone classes. We also conclude that the class components in the MOOD metrics suite are not good class fault-proneness predictors. Analyzing multivariate binary logistic regression models across six Rhino versions indicates these models may be useful in assessing quality in OO classes produced using modern highly iterative or agile software development processes.","Software metrics,
Programming,
Software quality,
Object oriented modeling,
Mood,
Java,
Software maintenance,
Computer industry,
Open source software,
Logistics"
Detection of Anatomic Structures in Human Retinal Imagery,"The widespread availability of electronic imaging devices throughout the medical community is leading to a growing body of research on image processing and analysis to diagnose retinal disease such as diabetic retinopathy (DR). Productive computer-based screening of large, at-risk populations at low cost requires robust, automated image analysis. In this paper we present results for the automatic detection of the optic nerve and localization of the macula using digital red-free fundus photography. Our method relies on the accurate segmentation of the vasculature of the retina followed by the determination of spatial features describing the density, average thickness, and average orientation of the vasculature in relation to the position of the optic nerve. Localization of the macula follows using knowledge of the optic nerve location to detect the horizontal raphe of the retina using a geometric model of the vasculature. We report 90.4% detection performance for the optic nerve and 92.5% localization performance for the macula for red-free fundus images representing a population of 345 images corresponding to 269 patients with 18 different pathologies associated with DR and other common retinal diseases such as age-related macular degeneration.","Humans,
Retina,
Biomedical optical imaging,
Geometrical optics,
Image analysis,
Availability,
Optical imaging,
Biomedical imaging,
Medical diagnostic imaging,
Image processing"
LogTM-SE: Decoupling Hardware Transactional Memory from Caches,"This paper proposes a hardware transactional memory (HTM) system called LogTM Signature Edition (LogTM-SE). LogTM-SE uses signatures to summarize a transactions read-and write-sets and detects conflicts on coherence requests (eager conflict detection). Transactions update memory ""in place"" after saving the old value in a per-thread memory log (eager version management). Finally, a transaction commits locally by clearing its signature, resetting the log pointer, etc., while aborts must undo the log. LogTM-SE achieves two key benefits. First, signatures and logs can be implemented without changes to highly-optimized cache arrays because LogTM-SE never moves cached data, changes a blocks cache state, or flash clears bits in the cache. Second, transactions are more easily virtualized because signatures and logs are software accessible, allowing the operating system and runtime to save and restore this state. In particular, LogTM-SE allows cache victimization, unbounded nesting (both open and closed), thread context switching and migration, and paging","Hardware,
Memory management,
Yarn,
Broadcasting,
Operating systems,
Protocols,
Decoding,
Programming profession,
Acceleration"
A Seeded Image Segmentation Framework Unifying Graph Cuts And Random Walker Which Yields A New Algorithm,"In this work, we present a common framework for seeded image segmentation algorithms that yields two of the leading methods as special cases - The graph cuts and the random walker algorithms. The formulation of this common framework naturally suggests a new, third, algorithm that we develop here. Specifically, the former algorithms may be shown to minimize a certain energy with respect to either an l1 or an l2 norm. Here, we explore the segmentation algorithm defined by an linfin norm, provide a method for the optimization and show that the resulting algorithm produces an accurate segmentation that demonstrates greater stability with respect to the number of seeds employed than either the graph cuts or random walker methods.","Image segmentation,
Labeling,
Stability,
Joining processes,
Pixel,
Computer science,
Visualization,
Optimization methods,
Computer vision,
Application software"
A Leaf Recognition Algorithm for Plant Classification Using Probabilistic Neural Network,"In this paper, we employ probabilistic neural network (PNN) with image and data processing techniques to implement a general purpose automated leaf recognition for plant classification. 12 leaf features are extracted and orthogonalized into 5 principal variables which consist the input vector of the PNN. The PNN is trained by 1800 leaves to classify 32 kinds of plants with an accuracy greater than 90%. Compared with other approaches, our algorithm is an accurate artificial intelligence approach which is fast in execution and easy in implementation.","Classification algorithms,
Neural networks,
Feature extraction,
Artificial neural networks,
Signal processing algorithms,
Shape,
Humans,
Biology computing,
Principal component analysis,
Computer science"
Anonymizing Classification Data for Privacy Preservation,"Classification is a fundamental problem in data analysis. Training a classifier requires accessing a large collection of data. Releasing person-specific data, such as customer data or patient records, may pose a threat to an individual's privacy. Even after removing explicit identifying information such as Name and SSN, it is still possible to link released records back to their identities by matching some combination of nonidentifying attributes such as {Sex, Zip, Birthdate}. A useful approach to combat such linking attacks, called k-anonymization, is anonymizing the linking attributes so that at least k released records match each value combination of the linking attributes. Previous work attempted to find an optimal k-anonymization that minimizes some data distortion metric. We argue that minimizing the distortion to the training data is not relevant to the classification goal that requires extracting the structure of predication on the ""future"" data. In this paper, we propose a k-anonymization solution for classification. Our goal is to find a k-anonymization, not necessarily optimal in the sense of minimizing data distortion, which preserves the classification structure. We conducted intensive experiments to evaluate the impact of anonymization on the classification on future data. Experiments on real-life data show that the quality of classification can be preserved even for highly restrictive anonymity requirements","Data privacy,
Joining processes,
Protection,
Data analysis,
Data mining,
Medical diagnostic imaging,
Back,
Training data,
Data security,
Information security"
Discriminative Training for Large-Vocabulary Speech Recognition Using Minimum Classification Error,"The minimum classification error (MCE) framework for discriminative training is a simple and general formalism for directly optimizing recognition accuracy in pattern recognition problems. The framework applies directly to the optimization of hidden Markov models (HMMs) used for speech recognition problems. However, few if any studies have reported results for the application of MCE training to large-vocabulary, continuous-speech recognition tasks. This article reports significant gains in recognition performance and model compactness as a result of discriminative training based on MCE training applied to HMMs, in the context of three challenging large-vocabulary (up to 100 k word) speech recognition tasks: the Corpus of Spontaneous Japanese lecture speech transcription task, a telephone-based name recognition task, and the MIT Jupiter telephone-based conversational weather information task. On these tasks, starting from maximum likelihood (ML) baselines, MCE training yielded relative reductions in word error ranging from 7% to 20%. Furthermore, this paper evaluates the use of different methods for optimizing the MCE criterion function, as well as the use of precomputed recognition lattices to speed up training. An overview of the MCE framework is given, with an emphasis on practical implementation issues",
Eyeblink-based Anti-Spoofing in Face Recognition from a Generic Webcamera,"We present a real-time liveness detection approach against photograph spoofing in face recognition, by recognizing spontaneous eyeblinks, which is a non-intrusive manner. The approach requires no extra hardware except for a generic webcamera. Eyeblink sequences often have a complex underlying structure. We formulate blink detection as inference in an undirected conditional graphical framework, and are able to learn a compact and efficient observation and transition potentials from data. For purpose of quick and accurate recognition of the blink behavior, eye closity, an easily-computed discriminative measure derived from the adaptive boosting algorithm, is developed, and then smoothly embedded into the conditional model. An extensive set of experiments are presented to show effectiveness of our approach and how it outperforms the cascaded Adaboost and HMM in task of eyeblink detection.","Face recognition,
Face detection,
Humans,
Fingerprint recognition,
Hardware,
Cameras,
Mouth,
Head,
Hidden Markov models,
Authentication"
Matplotlib: A 2D Graphics Environment,"Matplotlib is a 2D graphics package used for Python for application development, interactive scripting,and publication-quality image generation across user interfaces and operating systems","Graphics,
Interpolation,
Equations,
Graphical user interfaces,
Packaging,
Image generation,
User interfaces,
Operating systems,
Computer languages,
Programming profession"
Animated Transitions in Statistical Data Graphics,"In this paper we investigate the effectiveness of animated transitions between common statistical data graphics such as bar charts, pie charts, and scatter plots. We extend theoretical models of data graphics to include such transitions, introducing a taxonomy of transition types. We then propose design principles for creating effective transitions and illustrate the application of these principles in DynaVis, a visualization system featuring animated data graphics. Two controlled experiments were conducted to assess the efficacy of various transition types, finding that animated transitions can significantly improve graphical perception.","Animation,
Graphics,
Data visualization,
Marketing and sales,
Scattering,
Taxonomy,
Guidelines,
Information analysis,
Drilling,
Collaboration"
A Cross-Layer Framework for Exploiting Virtual MISO Links in Mobile Ad Hoc Networks,"Space-time communications can help combat fading and, hence, can significantly increase the capacity of ad hoc networks. Cooperative diversity or virtual antenna arrays facilitate spatio-temporal communications without actually requiring the deployment of physical antenna arrays. Virtual MISO entails the simultaneous transmission of appropriately encoded information by multiple nodes to effectively emulate a transmission on an antenna array. We present a novel multilayer approach for exploiting virtual MISO links in ad hoc networks. The approach spans the physical, medium access control and routing layers, and provides 1) a significant improvement in the end-to-end performance in terms of throughput and delay and 2) robustness to mobility and interference-induced link failures. The key physical layer property that we exploit is an increased transmission range due to achieved diversity gain. Except for space-time signal processing capabilities, our design does not require any additional hardware. We perform extensive simulations to quantify the benefits of our approach using virtual MISO links. As compared to using only SISO links, we achieve an increase of up to 150 percent in terms of the end-to-end throughput and a decrease of up to 75 percent in the incurred end-to-end delay. Our results also demonstrate a reduction in the route discovery attempts due to link failures by up to 60 percent, a direct consequence of the robustness that our approach provides to link failures","Mobile ad hoc networks,
Antenna arrays,
Mobile communication,
Ad hoc networks,
Throughput,
Delay,
Fading,
Transmitting antennas,
Nonhomogeneous media,
Media Access Protocol"
High-Dimensional Spatial Normalization of Diffusion Tensor Images Improves the Detection of White Matter Differences: An Example Study Using Amyotrophic Lateral Sclerosis,"Spatial normalization of diffusion tensor images plays a key role in voxel-based analysis of white matter (WM) group differences. Currently, it has been achieved using low-dimensional registration methods in the large majority of clinical studies. This paper aims to motivate the use of high-dimensional normalization approaches by generating evidence of their impact on the findings of such studies. Using an ongoing amyotrophic lateral sclerosis (ALS) study, we evaluated three normalization methods representing the current range of available approaches: low-dimensional normalization using the fractional anisotropy (FA), high-dimensional normalization using the FA, and high-dimensional normalization using full tensor information. Each method was assessed in terms of its ability to detect significant differences between ALS patients and controls. Our findings suggest that inadequate normalization with low-dimensional approaches can result in insufficient removal of shape differences which in turn can confound FA differences in a complex manner, and that utilizing high-dimensional normalization can both significantly minimize the confounding effect of shape differences to FA differences and provide a more complete description of WM differences in terms of both size and tissue architecture differences. We also found that high-dimensional approaches, by leveraging full tensor features instead of tensor-derived indices, can further improve the alignment of WM tracts.","Tensile stress,
Shape,
Diffusion tensor imaging,
Image analysis,
Diseases,
Anisotropic magnetoresistance,
Magnetic resonance imaging,
Microscopy,
Water,
In vivo"
What Supercomputers Say: A Study of Five System Logs,"If we hope to automatically detect and diagnose failures in large-scale computer systems, we must study real deployed systems and the data they generate. Progress has been hampered by the inaccessibility of empirical data. This paper addresses that dearth by examining system logs from five supercomputers, with the aim of providing useful insight and direction for future research into the use of such logs. We present details about the systems, methods of log collection, and how alerts were identified; propose a simpler and more effective filtering algorithm; and define operational context to encompass the crucial information that we found to be currently missing from most logs. The machines we consider (and the number of processors) are: Blue Gene/L (131072), Red Storm (10880), Thunderbird (9024), Spirit (1028), and Liberty (512). This is the first study of raw system logs from multiple supercomputers.","Supercomputers,
Laboratories,
Computer science,
Storms,
Tagging,
Large-scale systems,
Filtering algorithms,
Pressing,
Chaotic communication,
Power system reliability"
The Future of Social Networks on the Internet: The Need for Semantics,"Everyone on the Internet knows the buzzword social networking. Social networking services (SNS) usually offer the same basic functionalities: network of friends listings, person surfing, private messaging etc. With such features, SNSs demonstrate how the Internet continues to better connect people for various social and professional purposes. The fundamental problems with today's SNSs block their potential to access the full range of available content and networked people online. A possible solution is to build semantic social networking into the fabric of the next-generation Internet itself-interconnecting both content and people in meaningful ways. The semantic Web is a useful platform for linking and for performing operations on diverse person-and object-related data gathered from heterogeneous social networking sites. In the other direction, object-centered networks can serve as rich data sources for semantic Web applications.","Social network services,
IP networks,
Planets,
MySpace,
Internet,
Joining processes,
Rain,
YouTube,
Facebook,
Discussion forums"
DECLARE: Full Support for Loosely-Structured Processes,"Traditional workflow management systems (WFMSs) are not flexible enough to support loosely-structured processes. Furthermore, flexibility in contemporary WFMSs usually comes at a certain cost, such as lack of support for users, lack of methods for model analysis, lack of methods for analysis of past executions, etc. DECLARE is a proto-type of a WFMS that uses a constraint-based process modeling language for the development of declarative models describing loosely-structured processes. In this paper we show how DECLARE can support loosely-structured processes without sacrificing important WFMSs features like user support, model verification, analysis of past executions, changing models at run-time, etc.",
Atlas Renormalization for Improved Brain MR Image Segmentation Across Scanner Platforms,"Atlas-based approaches have demonstrated the ability to automatically identify detailed brain structures from 3-D magnetic resonance (MR) brain images. Unfortunately, the accuracy of this type of method often degrades when processing data acquired on a different scanner platform or pulse sequence than the data used for the atlas training. In this paper, we improve the performance of an atlas-based whole brain segmentation method by introducing an intensity renormalization procedure that automatically adjusts the prior atlas intensity model to new input data. Validation using manually labeled test datasets has shown that the new procedure improves the segmentation accuracy (as measured by the Dice coefficient) by 10% or more for several structures including hippocampus, amygdala, caudate, and pallidum. The results verify that this new procedure reduces the sensitivity of the whole brain segmentation method to changes in scanner platforms and improves its accuracy and robustness, which can thus facilitate multicenter or multisite neuroanatomical imaging studies","Image segmentation,
Biomedical imaging,
Magnetic resonance imaging,
Medical diagnostic imaging,
Hippocampus,
Degradation,
Brain modeling,
Neuroscience,
Image analysis,
Hospitals"
Power-Laws in a Large Object-Oriented Software System,"We present a comprehensive study of an implementation of the Smalltalk object oriented system, one of the first and purest object-oriented programming environment, searching for scaling laws in its properties. We study ten system properties, including the distributions of variable and method names, inheritance hierarchies, class and method sizes, system architecture graph. We systematically found Pareto - or sometimes log-normal - distributions in these properties. This denotes that the programming activity, even when modeled from a statistical perspective, can in no way be simply modeled as a random addition of independent increments with finite variance, but exhibits strong organic dependencies on what has been already developed. We compare our results with similar ones obtained for large Java systems, reported in the literature or computed by ourselves for those properties never studied before, showing that the behavior found is similar in all studied object oriented systems. We show how the Yule process is able to stochastically model the generation of several of the power-laws found, identifying the process parameters and comparing theoretical and empirical tail indexes. Lastly, we discuss how the distributions found are related to existing object-oriented metrics, like Chidamber and Kemerer's, and how they could provide a starting point for measuring the quality of a whole system, versus that of single classes. In fact, the usual evaluation of systems based on mean and standard deviation of metrics can be misleading. It is more interesting to measure differences in the shape and coefficients of the data?s statistical distributions.","Software systems,
Object oriented modeling,
Object oriented programming,
Power system modeling,
Shape measurement,
Java,
Power generation,
Tail,
Statistical distributions,
Statistical analysis"
Toeplitz-Structured Compressed Sensing Matrices,"The problem of recovering a sparse signal x Rn from a relatively small number of its observations of the form y = Ax Rk, where A is a known matrix and k « n, has recently received a lot of attention under the rubric of compressed sensing (CS) and has applications in many areas of signal processing such as data cmpression, image processing, dimensionality reduction, etc. Recent work has established that if A is a random matrix with entries drawn independently from certain probability distributions then exact recovery of x from these observations can be guaranteed with high probability. In this paper, we show that Toeplitz-structured matrices with entries drawn independently from the same distributions are also sufficient to recover x from y with high probability, and we compare the performance of such matrices with that of fully independent and identically distributed ones. The use of Toeplitz matrices in CS applications has several potential advantages: (i) they require the generation of only O(n) independent random variables; (ii) multiplication with Toeplitz matrices can be efficiently implemented using fast Fourier transform, resulting in faster acquisition and reconstruction algorithms; and (iii) Toeplitz-structured matrices arise naturally in certain application areas such as system identification.","Compressed sensing,
Signal processing,
Probability distribution,
Sparse matrices,
Data compression,
Image processing,
System identification,
Gas insulated transmission lines,
Optical computing,
Random variables"
A Unified Computational Framework for Deconvolution to Reconstruct Multiple Fibers From Diffusion Weighted MRI,"Diffusion magnetic resonance imaging (MRI) is a relatively new imaging modality which is capable of measuring the diffusion of water molecules in biological systems noninvasively. The measurements from diffusion MRI provide unique clues for extracting orientation information of brain white matter fibers and can be potentially used to infer the brain connectivity in vivo using tractography techniques. Diffusion tensor imaging (DTI), currently the most widely used technique, fails to extract multiple fiber orientations in regions with complex microstructure. In order to overcome this limitation of DTI, a variety of reconstruction algorithms have been introduced in the recent past. One of the key ingredients in several model-based approaches is deconvolution operation which is presented in a unified deconvolution framework in this paper. Additionally, some important computational issues in solving the deconvolution problem that are not addressed adequately in previous studies are described in detail here. Further, we investigate several deconvolution schemes towards achieving stable, sparse, and accurate solutions. Experimental results on both simulations and real data are presented. The comparisons empirically suggest that nonnegative least squares method is the technique of choice for the multifiber reconstruction problem in the presence of intravoxel orientational heterogeneity.","Deconvolution,
Magnetic resonance imaging,
Diffusion tensor imaging,
Image reconstruction,
Data mining,
Biology computing,
Biological systems,
In vivo,
Microstructure,
Reconstruction algorithms"
A Proposal for the Genetic Lateral Tuning of Linguistic Fuzzy Systems and Its Interaction With Rule Selection,"Linguistic fuzzy modeling allows us to deal with the modeling of systems by building a linguistic model which is clearly interpretable by human beings. However, since the accuracy and the interpretability of the obtained model are contradictory properties, the necessity of improving the accuracy of the linguistic model arises when complex systems are modeled. To solve this problem, one of the research lines in recent years has led to the objective of giving more accuracy to linguistic fuzzy modeling without losing the interpretability to a high level. In this paper, a new postprocessing approach is proposed to perform an evolutionary lateral tuning of membership functions, with the main aim of obtaining linguistic models with higher levels of accuracy while maintaining good interpretability. To do so, we consider a new rule representation scheme base on the linguistic 2-tuples representation model which allows the lateral variation of the involved labels. Furthermore, the cooperation of the lateral tuning together with fuzzy rule reduction mechanisms is studied in this paper, presenting results on different real applications. The obtained results show the good performance of the proposed approach in high-dimensional problems and its ability to cooperate with methods to remove unnecessary rules.","Proposals,
Fuzzy systems,
Fuzzy logic,
Humans,
Knowledge based systems,
Genetic algorithms,
Modeling,
Computer science,
Artificial intelligence,
Fuzzy sets"
From Template to Image: Reconstructing Fingerprints from Minutiae Points,"Most fingerprint-based biometric systems store the minutiae template of a user in the database. It has been traditionally assumed that the minutiae template of a user does not reveal any information about the original fingerprint. In this paper, we challenge this notion and show that three levels of information about the parent fingerprint can be elicited from the minutiae template alone, viz., 1) the orientation field information, 2) the class or type information, and 3) the friction ridge structure. The orientation estimation algorithm determines the direction of local ridges using the evidence of minutiae triplets. The estimated orientation field, along with the given minutiae distribution, is then used to predict the class of the fingerprint. Finally, the ridge structure of the parent fingerprint is generated using streamlines that are based on the estimated orientation field. Line integral convolution is used to impart texture to the ensuing ridges, resulting in a ridge map resembling the parent fingerprint. The salient feature of this noniterative method to generate ridges is its ability to preserve the minutiae at specified locations in the reconstructed ridge map. Experiments using a commercial fingerprint matcher suggest that the reconstructed ridge structure bears close resemblance to the parent fingerprint",
Modeling and Simulation Study of the Propagation and Defense of Internet E-mail Worms,"As many people rely on e-mail communications for business and everyday life, Internet e-mail worms constitute one of the major security threats for our society. Unlike scanning worms such as Code Red or Slammer, e-mail worms spread over a logical network defined by e-mail address relationships, making traditional epidemic models invalid for modeling the propagation of e-mail worms. In addition, we show that the topological epidemic models presented by M. Boguna, et al. (2000) largely overestimate epidemic spreading speed in topological networks due to their implicit homogeneous mixing assumption. For this reason, we rely on simulations to study e-mail worm propagation in this paper. We present an e-mail worm simulation model that accounts for the behaviors of e-mail users, including e-mail checking time and the probability of opening an e-mail attachment. Our observations of e-mail lists suggest that an Internet e-mail network follows a heavy-tailed distribution in terms of node degrees, and we model it as a power-law network. To study the topological impact, we compare e-mail worm propagation on power-law topology with worm propagation on two other topologies: small-world topology and random-graph topology. The impact of the power-law topology on the spread of e-mail worms is mixed: E-mail worms spread more quickly on a power-law topology than on a small-world topology or a random-graph topology, but immunization defense is more effective on a power-law topology.","Internet,
Electronic mail,
Computer worms,
Network topology,
Computer viruses,
IP networks,
Computer security,
Business communication,
Societies,
Immune system"
Architecture-Based Software Reliability Analysis: Overview and Limitations,"With the growing size and complexity of software applications, research in the area of architecture-based software reliability analysis has gained prominence. The purpose of this paper is to provide an overview of the existing research in this area, critically examine its limitations, and suggest ways to address the identified limitations","Software reliability,
Application software,
Computer architecture,
Flow graphs,
Software architecture,
Cost function,
Parameter estimation,
Computer science,
Computational modeling,
Computer simulation"
Reducing the Calibration Effort for Probabilistic Indoor Location Estimation,"WLAN location estimation based on 802.11 signal strength is becoming increasingly prevalent in today's pervasive computing applications. Among the well-established location determination approaches, probabilistic techniques show good performance and, thus, become increasingly popular. For these techniques to achieve a high level of accuracy, however, a large number of training samples are usually required for calibration, which incurs a great amount of offline manual effort. In this paper, we aim to solve the problem by reducing both the sampling time and the number of locations sampled in constructing a radio map. We propose a novel learning algorithm that builds location-estimation systems based on a small fraction of the calibration data that traditional techniques require and a collection of user traces that can be cheaply obtained. When the number of sampled locations is reduced, an interpolation method is developed to effectively patch a radio map. Extensive experiments show that our proposed methods are effective in reducing the calibration effort. In particular, unlabeled user traces can be used to compensate for the effects of reducing the calibration effort and can even improve the system performance. Consequently, manual effort can be reduced substantially while a high level of accuracy is still achieved","Calibration,
Sampling methods,
Interpolation,
Wireless LAN,
Phase estimation,
Pervasive computing,
Manuals,
System performance,
Bayesian methods,
Hidden Markov models"
"Accurate, Dense, and Robust Multi-View Stereopsis","This paper proposes a novel algorithm for calibrated multi-view stereopsis that outputs a (quasi) dense set of rectangular patches covering the surfaces visible in the input images. This algorithm does not require any initialization in the form of a bounding volume, and it detects and discards automatically outliers and obstacles. It does not perform any smoothing across nearby features, yet is currently the top performer in terms of both coverage and accuracy for four of the six benchmark datasets presented in [20]. The keys to its performance are effective techniques for enforcing local photometric consistency and global visibility constraints. Stereopsis is implemented as a match, expand, and filter procedure, starting from a sparse set of matched keypoints, and repeatedly expanding these to nearby pixel correspondences before using visibility constraints to filter away false matches. A simple but effective method for turning the resulting patch model into a mesh appropriate for image-based modeling is also presented. The proposed approach is demonstrated on various datasets including objects with fine surface details, deep concavities, and thin structures, outdoor scenes observed from a restricted set of viewpoints, and ""crowded"" scenes where moving obstacles appear in different places in multiple images of a static structure of interest.","Robustness,
Layout,
Matched filters,
Photometry,
Image reconstruction,
Smoothing methods,
Turning,
Embedded computing,
Computer science,
Surface reconstruction"
Pose-Oblivious Shape Signature,"A 3D shape signature is a compact representation for some essence of a shape. Shape signatures are commonly utilized as a fast indexing mechanism for shape retrieval. Effective shape signatures capture some global geometric properties which are scale, translation, and rotation invariant. In this paper, we introduce an effective shape signature which is also pose-oblivious. This means that the signature is also insensitive to transformations which change the pose of a 3D shape such as skeletal articulations. Although some topology-based matching methods can be considered pose-oblivious as well, our new signature retains the simplicity and speed of signature indexing. Moreover, contrary to topology-based methods, the new signature is also insensitive to the topology change of the shape, allowing us to match similar shapes with different genus. Our shape signature is a 2D histogram which is a combination of the distribution of two scalar functions defined on the boundary surface of the 3D shape. The first is a definition of a novel function called the local-diameter function. This function measures the diameter of the 3D shape in the neighborhood of each vertex. The histogram of this function is an informative measure of the shape which is insensitive to pose changes. The second is the centricity function that measures the average geodesic distance from one vertex to all other vertices on the mesh. We evaluate and compare a number of methods for measuring the similarity between two signatures, and demonstrate the effectiveness of our pose-oblivious shape signature within a 3D search engine application for different databases containing hundreds of models","Shape measurement,
Level measurement,
Indexing,
Histograms,
Databases,
Radio access networks,
Topology,
Search engines,
Acceleration"
Location Privacy in Sensor Networks Against a Global Eavesdropper,"While many protocols for sensor network security provide confidentiality for the content of messages, contextual information usually remains exposed. Such information can be critical to the mission of the sensor network, such as the location of a target object in a monitoring application, and it is often important to protect this information as well as message content. There have been several recent studies on providing location privacy in sensor networks. However, these existing approaches assume a weak adversary model where the adversary sees only local network traffic. We first argue that a strong adversary model, the global eavesdropper, is often realistic in practice and can defeat existing techniques. We then formalize the location privacy issues under this strong adversary model and show how much communication overhead is needed for achieving a given level of privacy. We also propose two techniques that prevent the leakage of location information: periodic collection and source simulation. Periodic collection provides a high level of location privacy, while source simulation provides trade-offs between privacy, communication cost, and latency. Through analysis and simulation, we demonstrate that the proposed techniques are efficient and effective in protecting location information from the attacker.","Privacy,
Protection,
Protocols,
Information security,
Monitoring,
Telecommunication traffic,
Traffic control,
Costs,
Delay,
Information analysis"
Real-Time Plane-Sweeping Stereo with Multiple Sweeping Directions,"Recent research has focused on systems for obtaining automatic 3D reconstructions of urban environments from video acquired at street level. These systems record enormous amounts of video; therefore a key component is a stereo matcher which can process this data at speeds comparable to the recording frame rate. Furthermore, urban environments are unique in that they exhibit mostly planar surfaces. These surfaces, which are often imaged at oblique angles, pose a challenge for many window-based stereo matchers which suffer in the presence of slanted surfaces. We present a multi-view plane-sweep-based stereo algorithm which correctly handles slanted surfaces and runs in real-time using the graphics processing unit (GPU). Our algorithm consists of (1) identifying the scene's principle plane orientations, (2) estimating depth by performing a plane-sweep for each direction, (3) combining the results of each sweep. The latter can optionally be performed using graph cuts. Additionally, by incorporating priors on the locations of planes in the scene, we can increase the quality of the reconstruction and reduce computation time, especially for uniform textureless surfaces. We demonstrate our algorithm on a variety of scenes and show the improved accuracy obtained by accounting for slanted surfaces.","Tellurium,
Image reconstruction,
Layout,
Surface reconstruction,
Real time systems,
Stereo image processing,
Hybrid integrated circuits,
Electrostatic precipitators,
Remotely operated vehicles,
Tin"
Vessels as 4-D Curves: Global Minimal 4-D Paths to Extract 3-D Tubular Surfaces and Centerlines,"In this paper, we propose an innovative approach to the segmentation of tubular structures. This approach combines all of the benefits of minimal path techniques such as global minimizers, fast computation, and powerful incorporation of user input, while also having the capability to represent and detect vessel surfaces directly which so far has been a feature restricted to active contour and surface techniques. The key is to represent the trajectory of a tubular structure not as a 3-D curve but to go up a dimension and represent the entire structure as a 4-D curve. Then we are able to fully exploit minimal path techniques to obtain global minimizing trajectories between two user supplied endpoints in order to reconstruct tubular structures from noisy or low contrast 3-D data without the sensitivity to local minima inherent in most active surface techniques. In contrast to standard purely spatial 3-D minimal path techniques, however, we are able to represent a full tubular surface rather than just a curve which runs through its interior. Our representation also yields a natural notion of a tube's ""central curve."" We demonstrate and validate the utility of this approach on magnetic resonance (MR) angiography and computed tomography (CT) images of coronary arteries.",
Feature Mining for Image Classification,"The efficiency and robustness of a vision system is often largely determined by the quality of the image features available to it. In data mining, one typically works with immense volumes of raw data, which demands effective algorithms to explore the data space. In analogy to data mining, the space of meaningful features for image analysis is also quite vast. Recently, the challenges associated with these problem areas have become more tractable through progress made in machine learning and concerted research effort in manual feature design by domain experts. In this paper, we propose a feature mining paradigm for image classification and examine several feature mining strategies. We also derive a principled approach for dealing with features with varying computational demands. Our goal is to alleviate the burden of manual feature design, which is a key problem in computer vision and machine learning. We include an in-depth empirical study on three typical data sets and offer theoretical explanations for the performance of various feature mining strategies. As a final confirmation of our ideas, we show results of a system, that utilizing feature mining strategies matches or outperforms the best reported results on pedestrian classification (where considerable effort has been devoted to expert feature design).","Image classification,
Computer vision,
Feature extraction,
Data mining,
Machine learning,
Face detection,
Machine vision,
Space exploration,
Filters,
Detectors"
Nonlinear Diffusion in Laplacian Pyramid Domain for Ultrasonic Speckle Reduction,"A new speckle reduction method, i.e., Laplacian pyramid-based nonlinear diffusion (LPND), is proposed for medical ultrasound imaging. With this method, speckle is removed by nonlinear diffusion filtering of bandpass ultrasound images in Laplacian pyramid domain. For nonlinear diffusion in each pyramid layer, a gradient threshold is automatically determined by a variation of median absolute deviation (MAD) estimator. The performance of the proposed LPND method has been compared with that of other speckle reduction methods, including the recently proposed speckle reducing anisotropic diffusion (SRAD) and nonlinear coherent diffusion (NCD). In simulation and phantom studies, an average gain of 1.55 dB and 1.34 dB in contrast-to-noise ratio was obtained compared to SRAD and NCD, respectively. The visual comparison of despeckled in vivo ultrasound images from liver and carotid artery shows that the proposed LPND method could effectively preserve edges and detailed structures while thoroughly suppressing speckle. These preliminary results indicate that the proposed speckle reduction method could improve image quality and the visibility of small structures and fine details in medical ultrasound imaging","Laplace equations,
Speckle,
Ultrasonic imaging,
Biomedical imaging,
Band pass filters,
Filtering,
Anisotropic magnetoresistance,
Imaging phantoms,
Gain,
In vivo"
Statistical Properties of Jacobian Maps and the Realization of Unbiased Large-Deformation Nonlinear Image Registration,"Maps of local tissue compression or expansion are often computed by comparing magnetic resonance imaging (MRI) scans using nonlinear image registration. The resulting changes are commonly analyzed using tensor-based morphometry to make inferences about anatomical differences, often based on the Jacobian map, which estimates local tissue gain or loss. Here, we provide rigorous mathematical analyses of the Jacobian maps, and use them to motivate a new numerical method to construct unbiased nonlinear image registration. First, we argue that logarithmic transformation is crucial for analyzing Jacobian values representing morphometric differences. We then examine the statistical distributions of log-Jacobian maps by defining the Kullback-Leibler (KL) distance on material density functions arising in continuum-mechanical models. With this framework, unbiased image registration can be constructed by quantifying the symmetric KL-distance between the identity map and the resulting deformation. Implementation details, addressing the proposed unbiased registration as well as the minimization of symmetric image matching functionals, are then discussed and shown to be applicable to other registration methods, such as inverse consistent registration. In the results section, we test the proposed framework, as well as present an illustrative application mapping detailed 3-D brain changes in sequential magnetic resonance imaging scans of a patient diagnosed with semantic dementia. Using permutation tests, we show that the symmetrization of image registration statistically reduces skewness in the log-Jacobian map.","Jacobian matrices,
Image registration,
Magnetic resonance imaging,
Image coding,
Magnetic analysis,
Mathematical analysis,
Statistical distributions,
Density functional theory,
Minimization methods,
Image matching"
Nanoscale Communication with Brownian Motion,"In this paper, the problem of communicating using chemical messages propagating using Brownian motion, rather than electromagnetic messages propagating as waves in free space or along a wire, is considered. This problem is motivated by nanotechnological and biotechnological applications, where the energy cost of electromagnetic communication might be prohibitive. Models are given for communication using particles that propagate with Brownian motion, and achievable capacity results are given. Under conservative assumptions, it is shown that rates exceeding one bit per particle are achievable.","Brownian motion,
Transmitters,
Electromagnetic propagation,
Nanobioscience,
Costs,
Chemical engineering,
Electromagnetic scattering,
Power engineering and energy,
Genetic engineering,
Communication channels"
Understanding Voltage Variations in Chip Multiprocessors using a Distributed Power-Delivery Network,"Recent efforts to address microprocessor power dissipation through aggressive supply voltage scaling and power management require that designers be increasingly cognizant of power supply variations. These variations, primarily due to fast changes in supply current, can be attributed to architectural gating events that reduce power dissipation. In order to study this problem, the authors propose a fine-grain, parameterizable model for power-delivery networks that allows system designers to study localized, on-chip supply fluctuations in high-performance microprocessors. Using this model, the authors analyze voltage variations in the context of next-generation chip-multiprocessor (CMP) architectures using both real applications and synthetic current traces. They find that the activity of distinct cores in CMPs present several new design challenges when considering power supply noise, and they describe potentially problematic activity sequences that are unique to CMP architectures","Voltage,
Microprocessors,
Power dissipation,
Power supplies,
Power system modeling,
Energy management,
Power system management,
Current supplies,
System-on-a-chip,
Network-on-a-chip"
Multichannel Blind Source Separation Using Convolution Kernel Compensation,"This paper studies a novel decomposition technique, suitable for blind separation of linear mixtures of signals comprising finite-length symbols. The observed symbols are first modeled as channel responses in a multiple-input-multiple-output (MIMO) model, while the channel inputs are conceptually considered sparse positive pulse trains carrying the information about the symbol arising times. Our decomposition approach compensates channel responses and aims at reconstructing the input pulse trains directly. The algorithm is derived first for the overdetermined noiseless MIMO case. A generalized scheme is then provided for the underdetermined mixtures in noisy environments. Although blind, the proposed technique approaches Bayesian optimal linear minimum mean square error estimator and is, hence, significantly noise resistant. The results of simulation tests prove it can be applied to considerably underdetermined convolutive mixtures and even to the mixtures of moderately correlated input pulse trains, with their cross-correlation up to 10% of its maximum possible value.","Blind source separation,
Convolution,
Kernel,
Source separation,
Working environment noise,
Signal processing,
MIMO,
Maximum likelihood estimation,
Discrete wavelet transforms,
Computer science"
Service-Oriented Smart-Home Architecture Based on OSGi and Mobile-Agent Technology,"The architecture of a conventional smart home is usually server-centric and thus causes many problems. Mobile devices and dynamic services affect a dynamically changing environment, which can result in very difficult interaction. In addition, how to provide services efficiently and appropriately is always an important issue for a smart home. To solve the problems caused by traditional architectures, to deal with the dynamic environment, and to provide appropriate services, we propose a service-oriented architecture (SOA) for smart-home environments, based on Open Services Gateway Initiative (OSGi) and mobile-agent (MA) technology. This architecture is a peer-to-peer (P2P) model based on multiple OSGi platforms, in which service-oriented mechanisms are used for system components to interact with one another, and MA technology is applied to augment the interaction mechanisms","Smart homes,
Service oriented architecture,
Computer architecture,
Appropriate technology,
Home appliances,
Distributed computing,
Home computing,
Computer science,
Chaos,
Peer to peer computing"
Improving the Robustness of Location-Based Routing for Underwater Sensor Networks,"This paper investigates a fundamental networking problem in underwater sensor networks: robust and energy-efficient routing. We present an adaptive location-based routing protocol, called hop-by-hop vector-based forwarding (HH-VBF). It uses the notion of a ""routing vector"" (a vector from the source to the sink) acting as the axis of the ""routing pipe"", similar to the vector based forward (VBF) routing in the work of P. Xie, J.-H. Cui and L. Lao (VBF: Vector-Based Forwarding Protocol for Underwater Sensor Networks. Technical report, UCONN CSE Technical Report: UbiNet-TR05-03 (BECAT/CSE-TR-05-6), Feb. 2005). Unlike the original VBF approach, however, HH-VBF suggests the use of a routing vector for each individual forwarder in the network, instead of a single network-wide source-to-sink routing vector. By the creation of the hop-by-hop vectors, HH-VBF can overcome two major problems in VBF: (1) too small data delivery ratio for sparse networks; (2) too sensitive to ""routing pipe"" radius threshold. We conduct simulations to evaluate HH-VBF, and the results show that HH-VBF yields much better performance than VBF in sparse networks. In addition, HH-VBF is less sensitive to the routing pipe radius threshold. Furthermore, we also analyze the behavior of HH-VBF and show that assuming proper redundancy and feedback techniques, HH-VBF can facilitate the avoidance of any ""void"" areas in the network.","Robustness,
Routing protocols,
Computer science,
Telecommunication traffic,
Network topology,
Degradation,
Computational modeling,
Power engineering and energy,
Feedback,
Energy efficiency"
Recognition of Pornographic Web Pages by Classifying Texts and Images,"With the rapid development of the World Wide Web, people benefit more and more from the sharing of information. However, Web pages with obscene, harmful, or illegal content can be easily accessed. It is important to recognize such unsuitable, offensive, or pornographic Web pages. In this paper, a novel framework for recognizing pornographic Web pages is described. A C4.5 decision tree is used to divide Web pages, according to content representations, into continuous text pages, discrete text pages, and image pages. These three categories of Web pages are handled, respectively, by a continuous text classifier, a discrete text classifier, and an algorithm that fuses the results from the image classifier and the discrete text classifier. In the continuous text classifier, statistical and semantic features are used to recognize pornographic texts. In the discrete text classifier, the naive Bayes rule is used to calculate the probability that a discrete text is pornographic. In the image classifier, the object's contour-based features are extracted to recognize pornographic images. In the text and image fusion algorithm, the Bayes theory is used to combine the recognition results from images and texts. Experimental results demonstrate that the continuous text classifier outperforms the traditional keyword-statistics-based classifier, the contour-based image classifier outperforms the traditional skin-region-based image classifier, the results obtained by our fusion algorithm outperform those by either of the individual classifiers, and our framework can be adapted to different categories of Web pages","Image recognition,
Text recognition,
Web pages,
Text categorization,
Web sites,
Decision trees,
Fuses,
Probability,
Feature extraction,
Image fusion"
"Argus: Low-Cost, Comprehensive Error Detection in Simple Cores","We have developed Argus, a novel approach for providing low-cost, comprehensive error detection for simple cores. The key to Argus is that the operation of a von Neumann core consists of four fundamental tasks - control flow, dataflow, computation, and memory access - that can be checked separately. We prove that Argus can detect any error by observing whether any of these tasks are performed incorrectly. We describe a prototype implementation, Argus-1, based on a single-issue, 4-stage, in-order processor to illustrate the potential of our approach. Experiments show that Argus-1 detects transient and permanent errors in simple cores with much lower impact on performance (<4% average overhead) and chip area (<17% overhead) than previous techniques.","Data flow computing,
Costs,
Error correction,
Hardware,
Runtime,
Computer errors,
Energy consumption,
Control systems,
Microarchitecture,
Computer science"
Adaptive Low Power Listening for Wireless Sensor Networks,"Most sensor networks require application-specific network-wide performance guarantees, suggesting the need for global and flexible network optimization. The dynamic and nonuniform local states of individual nodes in sensor networks complicate global optimization. Here, we present a cross-layer framework for optimizing global power consumption and balancing the load in sensor networks through greedy local decisions. Our framework enables each node to use its local and neighborhood state information to adapt its routing and MAC layer behavior. The framework employs a flexible cost function at the routing layer and adaptive duty cycles at the MAC layer in order to adapt a node's behavior to its local state. We identify three state aspects that impact energy consumption: 1) number of descendants in the routing tree, 2) radio duty cycle, and 3) role. We conduct experiments on a test-bed of 14 mica2 sensor nodes to compare the state representations and to evaluate the framework's energy benefits. The experiments show that the degree of load balancing increases for expanded state representations. The experiments also reveal that all state representations in our framework reduce global power consumption in the range of one-third for a time-driven monitoring network and in the range of one-fifth for an event-driven target tracking network.","Wireless sensor networks,
Routing,
Energy consumption,
Computerized monitoring,
Design optimization,
Cost function,
Testing,
Load management,
Target tracking,
Media Access Protocol"
On Quantum and Classical BCH Codes,"Classical Bose-Chaudhuri-Hocquenghem (BCH) codes that contain their (Euclidean or Hermitian) dual codes can be used to construct quantum stabilizer codes; this correspondence studies the properties of such codes. It is shown that a BCH code of length n can contain its dual code only if its designed distance delta=O(radicn), and the converse is proved in the case of narrow-sense codes. Furthermore, the dimension of narrow-sense BCH codes with small design distance is completely determined, and - consequently - the bounds on their minimum distance are improved. These results make it possible to determine the parameters of quantum BCH codes in terms of their design parameters","Galois fields,
Information processing,
Sufficient conditions,
Engineering profession,
Computer science,
Quantum mechanics,
Information theory,
Code standards,
Upper bound"
Towards automatic personalised content creation for racing games,"Evolutionary algorithms are commonly used to create high-performing strategies or agents for computer games. In this paper, we instead choose to evolve the racing tracks in a car racing game. An evolvable track representation is devised, and a multiobjective evolutionary algorithm maximises the entertainment value of the track relative to a particular human player. This requires a way to create accurate models of players' driving styles, as well as a tentative definition of when a racing track is fun, both of which are provided. We believe this approach opens up interesting new research questions and is potentially applicable to commercial racing games.","Computational intelligence,
Evolutionary computation,
Technological innovation,
Competitive intelligence,
Humans,
Computer science,
Taxonomy,
Optimal control,
Computational modeling,
Neural networks"
Segmenting Articular Cartilage Automatically Using a Voxel Classification Approach,"We present a fully automatic method for articular cartilage segmentation from magnetic resonance imaging (MRI) which we use as the foundation of a quantitative cartilage assessment. We evaluate our method by comparisons to manual segmentations by a radiologist and by examining the interscan reproducibility of the volume and area estimates. Training and evaluation of the method is performed on a data set consisting of 139 scans of knees with a status ranging from healthy to severely osteoarthritic. This is, to our knowledge, the only fully automatic cartilage segmentation method that has good agreement with manual segmentations, an interscan reproducibility as good as that of a human expert, and enables the separation between healthy and osteoarthritic populations. While high-field scanners offer high-quality imaging from which the articular cartilage have been evaluated extensively using manual and automated image analysis techniques, low-field scanners on the other hand produce lower quality images but to a fraction of the cost of their high-field counterpart. For low-field MRI, there is no well-established accuracy validation for quantitative cartilage estimates, but we show that differences between healthy and osteoarthritic populations are statistically significant using our cartilage volume and surface area estimates, which suggests that low-field MRI analysis can become a useful, affordable tool in clinical studies","Magnetic resonance imaging,
Image segmentation,
Costs,
Knee,
Magnetic analysis,
Reproducibility of results,
Diseases,
Ligaments,
Lesions,
Performance evaluation"
Detecting Wormhole Attacks in Wireless Networks Using Connectivity Information,"We propose a novel algorithm for detecting worm-hole attacks in wireless multi-hop networks. The algorithm uses only connectivity information to look for forbidden substructures in the connectivity graph. The proposed approach is completely localized and, unlike many techniques proposed in literature, does not use any special hardware artifact or location information, making the technique universally applicable. The algorithm is independent of wireless communication models. However, knowledge of the model and node distribution helps estimate a parameter used in the algorithm. We present simulation results for three different communication models and two different node distributions, and show that the algorithm is able to detect wormhole attacks with a 100% detection and 0% false alarm probabilities whenever the network is connected with high probability. Even for very low density networks where chances of disconnection is very high, the detection probability remains very high.","Wireless networks,
Peer to peer computing,
Wireless sensor networks,
Telecommunication traffic,
Protocols,
Communications Society,
Computer science,
USA Councils,
Spread spectrum communication,
Hardware"
A Simple Model for Analyzing P2P Streaming Protocols,"P2P streaming tries to achieve scalability (like P2P file distribution) and at the same time meet real-time playback requirements. It is a challenging problem still not well understood. In this paper, we describe a simple stochastic model that can be used to compare different data-driven downloading strategies based on two performance metrics: continuity (probability of continuous playback), and startup latency (expected time to start playback). We first study two simple strategies: rarest first and greedy. The former is a well-known strategy for P2P file sharing that gives good scalability, whereas the latter an intuitively reasonable strategy to optimize continuity and startup latency from a single peer's viewpoint. Greedy, while achieving low startup latency, fares poorly in continuity by failing to maximize P2P sharing; whereas rarest first is the opposite. This highlights the trade-off between startup latency and continuity, and how system scalability improves continuity. Based on this insight, we propose a mixed strategy that can be used to achieve the best of both worlds. Our algorithm dynamically adapts to the peer population size to ensure scalability; at the same time, it reserves part of a peer's effort to the immediate playback requirements to ensure low startup latency.","Protocols,
Scalability,
Delay,
Peer to peer computing,
Streaming media,
Network servers,
Analytical models,
Computer science,
Stochastic processes,
Measurement"
Health Monitoring of Civil Infrastructures Using Wireless Sensor Networks,"A Wireless Sensor Network (WSN) for Structural Health Monitoring (SHM) is designed, implemented, deployed and tested on the 4200 ft long main span and the south tower of the Golden Gate Bridge (GGB). Ambient structural vibrations are reliably measured at a low cost and without interfering with the operation of the bridge. Requirements that SHM imposes on WSN are identified and new solutions to meet these requirements are proposed and implemented. In the GGB deployment, 64 nodes are distributed over the main span and the tower, collecting ambient vibrations synchronously at 1 kHz rate, with less than 10 mus jitter, and with an accuracy of 30 muG. The sampled data is collected reliably over a 46-hop network, with a bandwidth of 441 B/s at the 46th hop. The collected data agrees with theoretical models and previous studies of the bridge. The deployment is the largest WSN for SHM.","Wireless sensor networks,
Bridges,
Vibration measurement,
Costs,
X-ray detection,
X-ray detectors,
Computerized monitoring,
Design engineering,
Testing,
Poles and towers"
Biometrics from Brain Electrical Activity: A Machine Learning Approach,"The potential of brain electrical activity generated as a response to a visual stimulus is examined in the context of the identification of individuals. Specifically, a framework for the visual evoked potential (VEP)-based biometrics is established, whereby energy features of the gamma band within VEP signals were of particular interest. A rigorous analysis is conducted which unifies and extends results from our previous studies, in particular, with respect to 1) increased bandwidth, 2) spatial averaging, 3) more robust power spectrum features, and 4) improved classification accuracy. Simulation results on a large group of subject support the analysis","Biometrics,
Machine learning,
Electroencephalography,
Fingerprint recognition,
Brain modeling,
Humans,
Spatial databases,
Iris,
Forgery,
Image storage"
A Flexible Real-Time Locking Protocol for Multiprocessors,"Real-time scheduling algorithms for multiprocessor systems have been the subject of considerable recent interest. For such an algorithm to be truly useful in practice, support for semaphore-based locking must be provided. However, for many global scheduling algorithms, no such mechanisms have been proposed. Furthermore, in the partitioned case, most prior semaphore schemes are either inefficient or restrict critical sections considerably. In this paper, a new flexible multiprocessor locking scheme is presented that can be applied under both partitioning and global scheduling. This scheme allows unrestricted critical-section nesting, but has been designed to deal with the common case of short non-nested accesses efficiently.","Protocols,
Processor scheduling,
Scheduling algorithm,
Partitioning algorithms,
Job shop scheduling,
Real time systems,
Multicore processing,
Computer science,
Multiprocessing systems,
Manufacturing"
Using Boosted Features for the Detection of People in 2D Range Data,"This paper addresses the problem of detecting people in two dimensional range scans. Previous approaches have mostly used pre-defined features for the detection and tracking of people. We propose an approach that utilizes a supervised learning technique to create a classifier that facilitates the detection of people. In particular, our approach applies AdaBoost to train a strong classifier from simple features of groups of neighboring beams corresponding to legs in range data. Experimental results carried out with laser range data illustrate the robustness of our approach even in cluttered office environments","Computer vision,
Leg,
Supervised learning,
Robot sensing systems,
Humans,
Data mining,
Laser modes,
Robotics and automation,
Computer science,
Laser beams"
Tracking Myocardial Motion From Cine DENSE Images Using Spatiotemporal Phase Unwrapping and Temporal Fitting,"Displacement encoding with stimulated echoes (DENSE) encodes myocardial tissue displacement into the phase of the MR image. Cine DENSE allows for rapid quantification of myocardial displacement at multiple cardiac phases through the majority of the cardiac cycle. For practical sensitivities to motion, relatively high displacement encoding frequencies are used and phase wrapping typically occurs. In order to obtain absolute measures of displacement, a two-dimensional (2-D) quality-guided phase unwrapping algorithm was adapted to unwrap both spatially and temporally. Both a fully automated algorithm and a faster semi-automated algorithm are proposed. A method for computing the 2-D trajectories of discrete points in the myocardium as they move through the cardiac cycle is introduced. The error in individual displacement measurements is reduced by fitting a time series to sequential displacement measurements along each trajectory. This improvement is in turn reflected in strain maps, which are derived directly from the trajectories. These methods were validated both in vivo and on a rotating phantom. Further measurements were made to optimize the displacement encoding frequency and to estimate the baseline strain noise both on the phantom and in vivo. The fully automated phase unwrapping algorithm was successful for 767 out of 800 images (95.9%), and the semi-automated algorithm was successful for 786 out of 800 images (98.3%). The accuracy of the tracking algorithm for typical cardiac displacements on a rotating phantom is 0.24plusmn0.15mm. The optimal displacement encoding frequency is in the region of 0.1 cycles/mm, and, for 2 scans of 17-s duration, the strain noise after temporal fitting was estimated to be 2.5plusmn3.0% at end-diastole, 3.1plusmn3.1% at end-systole, and 5.3plusmn5.0% in mid-diastole. The improvement in intra-myocardial strain measurements due to temporal fitting is apparent in strain histograms, and also in identifying regions of dysfunctional myocardium in studies of patients with infarcts","Tracking,
Myocardium,
Spatiotemporal phenomena,
Displacement measurement,
Strain measurement,
Frequency estimation,
Encoding,
Imaging phantoms,
Capacitive sensors,
In vivo"
Variable-Range Transmission Power Control in Wireless Ad Hoc Networks,"In this paper, we investigate the impact of variable-range transmission power control on the physical and network connectivity, on network capacity, and on power savings in wireless multihop networks. First, using previous work by Steele (1988), we show that, for a path attenuation factor a = 2, the average range of links in a planar random network of A m2 having n nodes is ~aradicA/n1. We show that this average range is approximately half the range obtained when common-range transmission control is used. Combining this result and previous work by Gupta and Kumar (2000), we derive an expression for the average traffic carrying capacity of variable-range-based multihop networks. For a = 2, we show that this capacity remains constant even when more nodes are added to the network. Second, we derive a model that approximates the signaling overhead of a routing protocol as a function of the transmission range and node mobility for both route discovery and route maintenance. We show that there is an optimum setting for the transmission range, not necessarily the minimum, which maximizes the capacity available to nodes in the presence of node mobility. The results presented in this paper highlight the need to design future MAC and routing protocols for wireless ad hoc and sensor networks based, not on common-range which is prevalent today, but on variable-range power control","Power control,
Mobile ad hoc networks,
Communication system traffic control,
Routing protocols,
Media Access Protocol,
Interference,
Spread spectrum communication,
Traffic control,
Wireless sensor networks,
Wireless application protocol"
Falkon: a Fast and Light-weight tasK executiON framework,"To enable the rapid execution of many tasks on compute clusters, we have developed Falkon, a Fast and Light-weight tasK executiON framework. Falkon integrates (1) multi-level scheduling to separate resource acquisition (via, e.g., requests to batch schedulers) from task dispatch, and (2) a streamlined dispatcher. Falkon's integration of multi-level scheduling and streamlined dispatchers delivers performance not provided by any other system. We describe Falkon architecture and implementation, and present performance results for both microbenchmarks and applications. Microbenchmarks show that Falkon throughput (487 tasks/sec) and scalability (to 54,000 executors and 2,000,000 tasks processed in just 112 minutes) are one to two orders of magnitude better than other systems used in production Grids. Large-scale astronomy and medical applications executed under Falkon by the Swift parallel programming system achieve up to 90% reduction in end-to-end run time, relative to versions that execute tasks via separate scheduler submissions.","Processor scheduling,
Grid computing,
Government,
Computer science,
Laboratories,
Throughput,
Parallel programming,
Scalability,
Production systems,
Large-scale systems"
Spatiotemporal Imaging with Partially Separable Functions,"Spatiotemporal imaging has a wide range of applications from functional neuroimaging, cardiac imaging to metabolic cancer imaging. A long-standing practical problem lies in obtaining high spatiotemporal resolution because the amount of data required increases exponentially as the physical dimension increases. This paper describes a new way for spatiotemporal imaging using partially separable functions. This model admits highly sparse sampling of the data space, providing a novel, effective way to achieve high spatiotemporal resolution. Practical imaging data will also be presented to demonstrate the performance of the new method.","Spatiotemporal phenomena,
High-resolution imaging,
Neuroimaging,
Spatial resolution,
Image reconstruction,
Image resolution,
Sampling methods,
Coils,
Cancer,
Signal resolution"
Location Estimation via Support Vector Regression,"Location estimation using the global system for mobile communication (GSM) is an emerging application that infers the location of the mobile receiver from multiple signals measurements. While geometrical and signal propagation models have been deployed to tackle this estimation problem, the terrain factors and power fluctuations have confined the accuracy of such estimation. Using support vector regression, we investigate the missing value location estimation problem by providing theoretical and empirical analysis on existing and novel kernels. A novel synthetic experiment is designed to compare the performances of different location estimation approaches. The proposed support vector regression approach shows promising performances, especially in terrains with local variations in environmental factors",
Collaboration in Software Engineering: A Roadmap,"Software engineering projects are inherently cooperative, requiring many software engineers to coordinate their efforts to produce a large software system. Integral to this effort is developing shared understanding surrounding multiple artifacts, each artifact embodying its own model, over the entire development process. This focus on model- oriented collaboration embedded within a larger process is what distinguishes collaboration research in software engineering from broader collaboration research, which tends to address artifact-neutral coordination technologies and toolkits. This article first presents a list of goals for software engineering collaboration, then surveys existing collaboration support tools in software engineering. The survey covers both tools that focus on a single artifact or stage in the development process (requirements support tools, UML collaboration tools), and tools that support the representation and execution of an entire software process. Important collaboration standards are also described. Several possible future directions for collaboration in software engineering are presented, including tight integration between web and desktop development environments, broader participation by customers and end users in the entire development process, capturing argumentation surrounding design rationale, and use of massively multiplayer online (MMO) game technology as a collaboration medium. The article concludes by noting a problem in performing research on collaborative systems, that of assessing how well certain artifacts, models, and embedded processes work, and whether they are better than other approaches.","Collaborative software,
Software engineering,
Collaborative work,
Collaborative tools,
Computer science,
Software systems,
Online Communities/Technical Collaboration,
Humans,
Convergence,
Writing"
Online Data Gathering for Maximizing Network Lifetime in Sensor Networks,"Energy-constrained sensor networks have been deployed widely for monitoring and surveillance purposes. Data gathering in such networks is often a prevalent operation. Since sensors have significant power constraints (battery life), energy efficient methods must be employed for data gathering to prolong network lifetime. We consider an online data gathering problem in sensor networks, which is stated as follows: assume that there is a sequence of data gathering queries, which arrive one by one. To respond to each query as it arrives, the system builds a routing tree for it. Within the tree, the volume of the data transmitted by each internal node depends on not only the volume of sensed data by the node itself, but also the volume of data received from its children. The objective is to maximize the network lifetime without any knowledge of future query arrivals and generation rates. In other words, the objective is to maximize the number of data gathering queries answered until the first node in the network fails. For the problem of concern, in this paper, we first present a generic cost model of energy consumption for data gathering queries if a routing tree is used for the query evaluation. We then show the problem to be NP-complete and propose several heuristic algorithms for it. We finally conduct experiments by simulation to evaluate the performance of the proposed algorithms in terms of network lifetime delivered. The experimental results show that, among the proposed algorithms, one algorithm that takes into account both the residual energy and the volume of data at each sensor node significantly outperforms the others",
Combining Formal Concept Analysis with Information Retrieval for Concept Location in Source Code,"The paper addresses the problem of concept location in source code by presenting an approach which combines formal concept analysis (FCA) and latent semantic indexing (LSI). In the proposed approach, LSI is used to map the concepts expressed in queries written by the programmer to relevant parts of the source code, presented as a ranked list of search results. Given the ranked list of source code elements, our approach selects most relevant attributes from these documents and organizes the results in a concept lattice, generated via FCA. The approach is evaluated in a case study on concept location in the source code of eclipse, an industrial size integrated development environment. The results of the case study show that the proposed approach is effective in organizing different concepts and their relationships present in the subset of the search results. The proposed concept location method outperforms the simple ranking of the search results, reducing the programmers' effort.",
Interactive Visual Exploration of a Large Spatio-temporal Dataset: Reflections on a Geovisualization Mashup.,"Exploratory visual analysis is useful for the preliminary investigation of large structured, multifaceted spatio-temporal datasets. This process requires the selection and aggregation of records by time, space and attribute, the ability to transform data and the flexibility to apply appropriate visual encodings and interactions. We propose an approach inspired by geographical 'mashups' in which freely-available functionality and data are loosely but flexibly combined using de facto exchange standards. Our case study combines MySQL, PHP and the LandSerf GIS to allow Google Earth to be used for visual synthesis and interaction with encodings described in KML. This approach is applied to the exploration of a log of 1.42 million requests made of a mobile directory service. Novel combinations of interaction and visual encoding are developed including spatial 'tag clouds', 'tag maps', 'data dials' and multi-scale density surfaces. Four aspects of the approach are informally evaluated: the visual encodings employed, their success in the visual exploration of the dataset, the specific tools used and the 'mashup' approach. Preliminary findings will be beneficial to others considering using mashups for visualization. The specific techniques developed may be more widely applied to offer insights into the structure of multifarious spatio-temporal data of the type explored here.","Reflection,
Mashups,
Encoding,
Data visualization,
Geographic Information Systems,
Earth,
Application software,
Spatial resolution,
Data mining,
Filters"
Selecting Stars: The k Most Representative Skyline Operator,"Skyline computation has many applications including multi-criteria decision making. In this paper, we study the problem of selecting k skyline points so that the number of points, which are dominated by at least one of these k skyline points, is maximized. We first present an efficient dynamic programming based exact algorithm in a 2d-space. Then, we show that the problem is NP-hard when the dimensionality is 3 or more and it can be approximately solved by a polynomial time algorithm with the guaranteed approximation ratio 1-1/e. To speed-up the computation, an efficient, scalable, index-based randomized algorithm is developed by applying the FM probabilistic counting technique. A comprehensive performance evaluation demonstrates that our randomized technique is very efficient, highly accurate, and scalable.",
Objects in Action: An Approach for Combining Action Understanding and Object Perception,"Analysis of videos of human-object interactions involves understanding human movements, locating and recognizing objects and observing the effects of human movements on those objects. While each of these can be conducted independently, recognition improves when interactions between these elements are considered. Motivated by psychological studies of human perception, we present a Bayesian approach which unifies the inference processes involved in object classification and localization, action understanding and perception of object reaction. Traditional approaches for object classification and action understanding have relied on shape features and movement analysis respectively. By placing object classification and localization in a video interpretation framework, we can localize and classify objects which are either hard to localize due to clutter or hard to recognize due to lack of discriminative features. Similarly, by applying context on human movements from the objects on which these movements impinge and the effects of these movements, we can segment and recognize actions which are either too subtle to perceive or too hard to recognize using motion features alone.","Humans,
Object recognition,
Shape,
Psychology,
Spraying,
Videos,
Bayesian methods,
Neurons,
Robustness,
Computer science"
Thermal-Aware 3D IC Placement Via Transformation,"3D IC technologies can help to improve circuit performance and lower power consumption by reducing wirelength. Also, 3D IC technology can be used to realize heterogeneous system-on-chip design, by integrating different modules together with less interference with each other. In this paper, we propose a novel thermal-aware 3D cell placement approach, named T3Place, based on transforming a 2D placement with good wirelength to a 3D placement, with the objectives of half-perimeter wirelength, through-the-silicon (TS) via number and temperature. T3Place is composed of two steps, transformation from a 2D placement to a 3D placement and the refinement of the resulting 3D placement. We proposed and compared several different transformation techniques, including local stacking transformation (LST), folding-2, folding-4 and window-based stacking/folding transformation, and concluded that (i) LST can generate 3D placements with the least wirelength, (ii) the folding-based transformations result in 3D placements with the fewest TS vias, and (iii) the window-based stacking/folding transformations provide good TS via number and wirelength tradeoffs. For example, with four device layers, LST can reduce the wirelength by over 2times compared to the initial 2D placement, while window-based stacking/folding can provide over 10times variation in terms of the TS via number, thus adaptive to different manufacturing ability for TS via density. Moreover, we proposed a novel relaxed conflict-net (RCN) graph-based layer assignment method to further refine the 3D placements. Compared to LST results, thermal-aware RCN graph-based layer assignment algorithm (r = 10%) can further reduce the maximum on-chip temperature by 37%, with only 6% TS via number increase and 8% wirelength increase.","Three-dimensional integrated circuits,
Stacking,
Thermal conductivity,
Temperature,
Energy consumption,
System-on-a-chip,
Integrated circuit interconnections,
Radio frequency,
Packaging,
Fabrication"
A performance study of BitTorrent-like peer-to-peer systems,"This paper presents a performance study of BitTorrent-like P2P systems by modeling, based on extensive measurements and trace analysis. Existing studies on BitTorrent systems are single-torrent based and usually assume the process of request arrivals to a torrent is Poisson-like. However, in reality, most BitTorrent peers participate in multiple torrents and file popularity changes over time. Our study of representative BitTorrent traffic provides insights into the evolution of single-torrent systems and several new findings regarding the limitations of BitTorrent systems: (1) Due to the exponentially decreasing peer arrival rate in a torrent, the service availability of the corresponding file becomes poor quickly, and eventually it is hard to locate and download this file. (2) Client performance in the BitTorrent-like system is unstable, and fluctuates significantly with the changes of the number of online peers. (3) Existing systems could provide unfair services to peers, where a peer with a higher downloading speed tends to download more and upload less. Motivated by the analysis and modeling results, we have further proposed a graph based model to study interactions among multiple torrents. Our model quantitatively demonstrates that inter-torrent collaboration is much more effective than stimulating seeds to serve longer for addressing the service unavailability in BitTorrent systems. An architecture for inter-torrent collaboration under an exchange based instant incentive mechanism is also discussed and evaluated by simulations","Peer to peer computing,
Collaboration,
Internet,
Computer science,
Bandwidth,
Performance analysis,
Traffic control,
Availability,
Game theory,
Robustness"
Deformation Models for Image Recognition,"We present the application of different nonlinear image deformation models to the task of image recognition. The deformation models are especially suited for local changes as they often occur in the presence of image object variability. We show that, among the discussed models, there is one approach that combines simplicity of implementation, low-computational complexity, and highly competitive performance across various real-world image recognition tasks. We show experimentally that the model performs very well for four different handwritten digit recognition tasks and for the classification of medical images, thus showing high generalization capacity. In particular, an error rate of 0.54 percent on the MNIST benchmark is achieved, as well as the lowest reported error rate, specifically 12.6 percent, in the 2005 international ImageCLEF evaluation of medical image specifically categorization.",
Nearly Constant Approximation for Data Aggregation Scheduling in Wireless Sensor Networks,"Data aggregation is a fundamental yet time-consuming task in wireless sensor networks. We focus on the latency part of data aggregation. Previously, the data aggregation algorithm of least latency [1] has a latency bound of (Delta - 1)R, where Delta is the maximum degree and R is the network radius. Since both Delta and R could be of the same order of the network size, this algorithm can still have a rather high latency. In this paper, we designed an algorithm based on maximal independent sets which has an latency bound of 23R + Delta - 18. Here Delta contributes to an additive factor instead of a multiplicative one; thus our algorithm is nearly constant approximation and it has a significantly less latency bound than earlier algorithms especially when Delta is large.","Wireless sensor networks,
Delay,
Algorithm design and analysis,
Approximation algorithms,
Broadcasting,
Computer science,
Base stations,
Heuristic algorithms,
Scheduling,
Communications Society"
Real-Time SLAM Relocalisation,"Monocular SLAM has the potential to turn inexpensive cameras into powerful pose sensors for applications such as robotics and augmented reality. However, current implementations lack the robustness required to be useful outside laboratory conditions: blur, sudden motion and occlusion all cause tracking to fail and corrupt the map. Here we present a system which automatically detects and recovers from tracking failure while preserving map integrity. By extending recent advances in keypoint recognition the system can quickly resume tracking - i.e. within a single frame time of 33 ms - using any of the features previously stored in the map. Extensive tests show that the system can reliably generate maps for long sequences even in the presence of frequent tracking failure.","Simultaneous localization and mapping,
Robot vision systems,
Cameras,
Robot sensing systems,
Robotics and automation,
Augmented reality,
Robustness,
Laboratories,
Tracking,
Resumes"
Top-k Monitoring in Wireless Sensor Networks,"Top-k monitoring is important to many wireless sensor applications. This paper exploits the semantics of top-k query and proposes an energy-efficient monitoring approach called FILA. The basic idea is to install a filter at each sensor node to suppress unnecessary sensor updates. Filter setting and query reevaluation upon updates are two fundamental issues to the correctness and efficiency of the FILA approach. We develop a query reevaluation algorithm that is capable of handling concurrent sensor updates. In particular, we present optimization techniques to reduce the probing cost. We design a skewed filter setting scheme, which aims to balance energy consumption and prolong network lifetime. Moreover, two filter update strategies, namely, eager and lazy, are proposed to favor different application scenarios. We also extend the algorithms to several variants of top-k query, that is, order-insensitive, approximate, and value monitoring. The performance of the proposed FILA approach is extensively evaluated using real data traces. The results show that FILA substantially outperforms the existing TAG-based approach and range caching approach in terms of both network lifetime and energy consumption under various network configurations.",
Physiology-Based Face Recognition in the Thermal Infrared Spectrum,"The current dominant approaches to face recognition rely on facial characteristics that are on or over the skin. Some of these characteristics have low permanency can be altered, and their phenomenology varies significantly with environmental factors (e.g., lighting). Many methodologies have been developed to address these problems to various degrees. However, the current framework of face recognition research has a potential weakness due to its very nature. We present a novel framework for face recognition based on physiological information. The motivation behind this effort is to capitalize on the permanency of innate characteristics that are under the skin. To establish feasibility, we propose a specific methodology to capture facial physiological patterns using the bioheat information contained in thermal imagery. First, the algorithm delineates the human face from the background using the Bayesian framework. Then, it localizes the superficial blood vessel network using image morphology. The extracted vascular network produces contour shapes that are characteristic to each individual. The branching points of the skeletonized vascular network are referred to as thermal minutia points (TMPs) and constitute the feature database. To render the method robust to facial pose variations, we collect for each subject to be stored in the database five different pose images (center, midleft profile, left profile, midright profile, and right profile). During the classification stage, the algorithm first estimates the pose of the test image. Then, it matches the local and global TMP structures extracted from the test image with those of the corresponding pose images in the database. We have conducted experiments on a multipose database of thermal facial images collected in our laboratory, as well as on the time-gap database of the University of Notre Dame. The good experimental results show that the proposed methodology has merit, especially with respect to the problem of low permanence over time. More importantly, the results demonstrate the feasibility of the physiological framework in face recognition and open the way for further methodological and experimental research in the area","Face recognition,
Infrared spectra,
Image databases,
Skin,
Spatial databases,
Testing,
Environmental factors,
Humans,
Bayesian methods,
Blood vessels"
An Oblivious Watermarking for 3-D Polygonal Meshes Using Distribution of Vertex Norms,"Although it has been known that oblivious (or blind) watermarking schemes are less robust than nonoblivious ones, they are more useful for various applications where a host signal is not available in the watermark detection procedure. From a viewpoint of oblivious watermarking for a three-dimensional (3-D) polygonal mesh model, distortionless attacks, such as similarity transforms and vertex reordering, might be more serious than distortion attacks including adding noise, smoothing, simplification, remeshing, clipping, and so on. Clearly, it is required to develop an oblivious watermarking that is robust against distortionless as well as distortion attacks. In this paper, we propose two oblivious watermarking methods for 3-D polygonal mesh models, which modify the distribution of vertex norms according to the watermark bit to be embedded. One method is to shift the mean value of the distribution and another is to change its variance. Histogram mapping functions are introduced to modify the distribution. These mapping functions are devised to reduce the visibility of watermark as much as possible. Since the statistical features of vertex norms are invariant to the distortionless attacks, the proposed methods are robust against such attacks. In addition, our methods employ an oblivious watermark detection scheme, which can extract the watermark without referring to the cover mesh model. Through simulations, we demonstrate that the proposed approaches are remarkably robust against distortionless attacks. In addition, they are fairly robust against various distortion attacks","Watermarking,
Copyright protection,
Streaming media,
Computer science,
Cryptography,
Design automation,
Smoothing methods,
Noise robustness,
Histograms,
Web sites"
The Future of Empirical Methods in Software Engineering Research,"We present the vision that for all fields of software engineering (SE), empirical research methods should enable the development of scientific knowledge about how useful different SE technologies are for different kinds of actors, performing different kinds of activities, on different kinds of systems. It is part of the vision that such scientific knowledge will guide the development of new SE technology and is a major input to important SE decisions in industry. Major challenges to the pursuit of this vision are: more SE research should be based on the use of empirical methods; the quality, including relevance, of the studies using such methods should be increased; there should be more and better synthesis of empirical evidence; and more theories should be built and tested. Means to meet these challenges include (1) increased competence regarding how to apply and combine alternative empirical methods, (2) tighter links between academia and industry, (3) the development of common research agendas with a focus on empirical methods, and (4) more resources for empirical research.","Software engineering,
Laboratories,
Software systems,
Computer science,
Informatics,
Computer industry,
Software development management,
Information science,
Computer Society,
Project management"
A NUCA Substrate for Flexible CMP Cache Sharing,"We propose an organization for the on-chip memory system of a chip multiprocessor in which 16 processors share a 16-Mbyte pool of 64 level-2 (L2) cache banks. The L2 cache is organized as a nonuniform cache architecture (NUCA) array with a switched network embedded in it for high performance. We show that this organization can support a spectrum of degrees of sharing: unshared, in which each processor owns a private portion of the cache, thus reducing hit latency, and completely shared, in which every processor shares the entire cache, thus minimizing misses, and every point in between. We measure the optimal degree of sharing for different cache bank mapping policies and also evaluate a per-application cache partitioning strategy. We conclude that a static NUCA organization with sharing degrees of 2 or 4 works best across a suite of commercial and scientific parallel workloads. We demonstrate that migratory dynamic NUCA approaches improve performance significantly for a subset of the workloads at the cost of increased complexity, especially as per-application cache partitioning strategies are applied. We also evaluate the energy efficiency of each design point in terms of network traffic, bank accesses, and external memory accesses.","System-on-a-chip,
Costs,
Energy efficiency,
Telecommunication traffic,
Cache memory,
Memory architecture,
Wire,
Conductivity,
Delay effects,
Hardware"
Weighted Fourier Series Representation and Its Application to Quantifying the Amount of Gray Matter,"We present a novel weighted Fourier series (WFS) representation for cortical surfaces. The WFS representation is a data smoothing technique that provides the explicit smooth functional estimation of unknown cortical boundary as a linear combination of basis functions. The basic properties of the representation are investigated in connection with a self-adjoint partial differential equation and the traditional spherical harmonic (SPHARM) representation. To reduce steep computational requirements, a new iterative residual fitting (IRF) algorithm is developed. Its computational and numerical implementation issues are discussed in detail. The computer codes are also available at http://www.stat.wisc.edu/ ~mchung/softwares/weighted-SPHARM/weighted-SPHARM.html . As an illustration, the WFS is applied in quantifying the amount of gray matter in a group of high functioning autistic subjects. Within the WFS framework, cortical thickness and gray matter density are computed and compared","Fourier series,
Smoothing methods,
Iterative algorithms,
Brain,
Partial differential equations,
Autism,
Kernel,
Statistics,
Biomedical informatics,
Information science"
VIRE: Active RFID-based Localization Using Virtual Reference Elimination,"RFID technologies are gaining much attention as they are attractive solutions to many application domains. Localization based on active RFID technologies provides a much needed added-value to further expand the application domain. LANDMARC was the first attempt using active RFID for indoor location sensing with satisfactory results. However, the LANDMARC approach suffers from two drawbacks. First, it does not work well in a closed area with severe radio signal multi-path effects. Second, to further improve the localization accuracy, more reference tags are needed which is costly and may trigger the RF interference phenomenon. The proposed VIRE approach can overcome the above drawbacks without additional cost. Based on the concept of virtual reference tags, a proximity map is maintained by each reader. An elimination algorithm is used to eliminate those unlikely locations to reduce the estimation error. Our experimental results show that the new method consistently enhances the precision of indoor localization from 17 to 73 percent over the LANDMARC approach at different tag locations in different environments.","Active RFID tags,
Radio transmitters,
Radiofrequency identification,
Global Positioning System,
Target tracking,
Application software,
Costs,
Radio frequency,
Receivers,
Computer science"
Interactive Humanoid Robots for a Science Museum,"One objective of the Intelligent Robotics and Communication Laboratories is to develop an intelligent communication robot that supports people in an open everyday environment by interacting with them. A humanoid robot can help achieve this objective because its physical structure lets it interact through human-like body movements such as shaking hands, greeting, and pointing. Both adults and children are more likely to understand such interactions than interactions with an electronic interface such as a touch panel or buttons. To behave intelligently during an interaction, a robot requires many types of information about its environment and the people with whom it interacts. However, in open everyday environments, simple recognition functions such as identifying an individual are difficult because the presence and movement of a large number of people as well as unfavorable illumination and background conditions affect the robot's sensing ability. We integrated humanoid robots and ubiquitous sensors in an autonomous system to assist visitors at an Osaka Science Museum exhibit","Humanoid robots,
Robot sensing systems,
Robot kinematics,
Intelligent robots,
Human robot interaction,
RFID tags,
Intelligent sensors,
Robot vision systems,
Cameras,
Databases"
Scheduling Efficiency of Distributed Greedy Scheduling Algorithms in Wireless Networks,"We consider the problem of distributed scheduling in wireless networks subject to simple collision constraints. We define the efficiency of a distributed scheduling algorithm to be the largest number (fraction) such that the throughput under the distributed scheduling policy is at least equal to the efficiency multiplied by the maximum throughput achievable under a centralized policy. For a general interference model, we prove a lower bound on the efficiency of a distributed scheduling algorithm by first assuming that all of the traffic only uses one hop of the network. We also prove that the lower bound is tight in the sense that, for any fraction larger than the lower bound, we can find a topology and an arrival rate vector within the fraction of the capacity region such that the network is unstable under a greedy scheduling policy. We then extend our results to a more general multihop traffic scenario and show that similar scheduling efficiency results can be established by introducing prioritization or regulators to the basic greedy scheduling algorithm",
Comparison of PDE-Based Nonlinear Diffusion Approaches for Image Enhancement and Denoising in Optical Coherence Tomography,"A comparison between two nonlinear diffusion methods for denoising OCT images is performed. Specifically, we compare and contrast the performance of the traditional nonlinear Perona-Malik filter with a complex diffusion filter that has been recently introduced by Gilboa . The complex diffusion approach based on the generalization of the nonlinear scale space to the complex domain by combining the diffusion and the free Schrodinger equation is evaluated on synthetic images and also on representative OCT images at various noise levels. The performance improvement over the traditional nonlinear Perona-Malik filter is quantified in terms of noise suppression, image structural preservation and visual quality. An average signal-to-noise ratio (SNR) improvement of about 2.5 times and an average contrast to noise ratio (CNR) improvement of 49% was obtained while mean structure similarity (MSSIM) was practically not degraded after denoising. The nonlinear complex diffusion filtering can be applied with success to many OCT imaging applications. In summary, the numerical values of the image quality metrics along with the qualitative analysis results indicated the good feature preservation performance of the complex diffusion process, as desired for better diagnosis in medical imaging processing","Image enhancement,
Noise reduction,
Nonlinear optics,
Tomography,
Optical filters,
Signal to noise ratio,
Biomedical optical imaging,
Schrodinger equation,
Noise level,
Degradation"
Maximally Stable Colour Regions for Recognition and Matching,"This paper introduces a novel colour-based affine co-variant region detector. Our algorithm is an extension of the maximally stable extremal region (MSER) to colour. The extension to colour is done by looking at successive time-steps of an agglomerative clustering of image pixels. The selection of time-steps is stabilised against intensity scalings and image blur by modelling the distribution of edge magnitudes. The algorithm contains a novel edge significance measure based on a Poisson image noise model, which we show performs better than the commonly used Euclidean distance. We compare our algorithm to the original MSER detector and a competing colour-based blob feature detector, and show through a repeatability test that our detector performs better. We also extend the state of the art in feature repeatability tests, by using scenes consisting of two planes where one is piecewise transparent. This new test is able to evaluate how stable a feature is against changing backgrounds.",
Large Deformation Diffeomorphism and Momentum Based Hippocampal Shape Discrimination in Dementia of the Alzheimer type,"In large-deformation diffeomorphic metric mapping (LDDMM), the diffeomorphic matching of images are modeled as evolution in time, or a flow, of an associated smooth velocity vector field v controlling the evolution. The initial momentum parameterizes the whole geodesic and encodes the shape and form of the target image. Thus, methods such as principal component analysis (PCA) of the initial momentum leads to analysis of anatomical shape and form in target images without being restricted to small-deformation assumption in the analysis of linear displacements. We apply this approach to a study of dementia of the Alzheimer type (DAT). The left hippocampus in the DAT group shows significant shape abnormality while the right hippocampus shows similar pattern of abnormality. Further, PCA of the initial momentum leads to correct classification of 12 out of 18 DAT subjects and 22 out of 26 control subjects",
Nonrigid Coregistration of Diffusion Tensor Images Using a Viscous Fluid Model and Mutual Information,"In this paper, a nonrigid coregistration algorithm based on a viscous fluid model is proposed that has been optimized for diffusion tensor images (DTI), in which image correspondence is measured by the mutual information criterion. Several coregistration strategies are introduced and evaluated both on simulated data and on brain intersubject DTI data. Two tensor reorientation methods have been incorporated and quantitatively evaluated. Simulation as well as experimental results show that the proposed viscous fluid model can provide a high coregistration accuracy, although the tensor reorientation was observed to be highly sensitive to the local deformation field. Nevertheless, this coregistration method has demonstrated to significantly improve spatial alignment compared to affine image matching.","Tensile stress,
Mutual information,
Diffusion tensor imaging,
Brain modeling,
Diffusion processes,
Deformable models,
Magnetic cores,
Physics,
Biomedical imaging,
Image matching"
Revisiting the Foundations of Artificial Immune Systems for Data Mining,"This paper advocates a problem-oriented approach for the design of artificial immune systems (AIS) for data mining. By problem-oriented approach we mean that, in real-world data mining applications the design of an AIS should take into account the characteristics of the data to be mined together with the application domain: the components of the AIS - such as its representation, affinity function, and immune process - should be tailored for the data and the application. This is in contrast with the majority of the literature, where a very generic AIS algorithm for data mining is developed and there is little or no concern in tailoring the components of the AIS for the data to be mined or the application domain. To support this problem-oriented approach, we provide an extensive critical review of the current literature on AIS for data mining, focusing on the data mining tasks of classification and anomaly detection. We discuss several important lessons to be taken from the natural immune system to design new AIS that are considerably more adaptive than current AIS. Finally, we conclude this paper with a summary of seven limitations of current AIS for data mining and ten suggested research directions.",
Analyzing the Energy-Time Trade-Off in High-Performance Computing Applications,"Although users of high-performance computing are most interested in raw performance both energy and power consumption has become critical concerns. One approach to lowering energy and power is to use high-performance cluster nodes that have several power-performance states so that the energy-time trade-off can be dynamically adjusted. This paper analyzes the energy-time trade-off of a wide range of applications-serial and parallel-on a power-scalable cluster. We use a cluster of frequency and voltage-scalable AMD-64 nodes, each equipped with a power meter. We study the effects of memory and communication bottlenecks via direct measurement of time and energy. We also investigate metrics that can, at runtime, predict when each type of bottleneck occurs. Our results show that, for programs that have a memory or communication bottleneck, a power-scalable cluster can save significant energy with only a small time penalty. Furthermore, we find that, for some programs, it is possible to both consume less energy and execute in less time by increasing the number of nodes while reducing the frequency-voltage setting of each node","Computer applications,
Frequency,
Voltage,
Gears,
High performance computing,
Energy consumption,
Costs,
Microprocessors,
Energy measurement,
Time measurement"
"Problems with Precision: A Response to ""Comments on 'Data Mining Static Code Attributes to Learn Defect Predictors'""","Zhang and Zhang argue that predictors are useless unless they have high precison&recall. We have a different view, for two reasons. First, for SE data sets with large neg/pos ratios, it is often required to lower precision to achieve higher recall. Second, there are many domains where low precision detectors are useful.","Data mining,
Detectors,
Equations,
Predictive models,
Accuracy,
Software engineering,
NASA,
Testing,
Performance evaluation,
Project management"
"Conformal Geometry and Its Applications on 3D Shape Matching, Recognition, and Stitching","Three-dimensional shape matching is a fundamental issue in computer vision with many applications such as shape registration, 3D object recognition, and classification. However, shape matching with noise, occlusion, and clutter is a challenging problem. In this paper, we analyze a family of quasi-conformal maps including harmonic maps, conformal maps, and least-squares conformal maps with regards to 3D shape matching. As a result, we propose a novel and computationally efficient shape matching framework by using least-squares conformal maps. According to conformal geometry theory, each 3D surface with disk topology can be mapped to a 2D domain through a global optimization and the resulting map is a diffeomorphism, i.e., one-to-one and onto. This allows us to simplify the 3D shape-matching problem to a 2D image-matching problem, by comparing the resulting 2D parametric maps, which are stable, insensitive to resolution changes and robust to occlusion, and noise. Therefore, highly accurate and efficient 3D shape matching algorithms can be achieved by using the above three parametric maps. Finally, the robustness of least-squares conformal maps is evaluated and analyzed comprehensively in 3D shape matching with occlusion, noise, and resolution variation. In order to further demonstrate the performance of our proposed method, we also conduct a series of experiments on two computer vision applications, i.e., 3D face recognition and 3D nonrigid surface alignment and stitching.","Geometry,
Shape,
Application software,
Noise shaping,
Computer vision,
Noise robustness,
Object recognition,
Harmonic analysis,
Topology,
Image resolution"
Detailed Human Shape and Pose from Images,"Much of the research on video-based human motion capture assumes the body shape is known a priori and is represented coarsely (e.g. using cylinders or superquadrics to model limbs). These body models stand in sharp contrast to the richly detailed 3D body models used by the graphics community. Here we propose a method for recovering such models directly from images. Specifically, we represent the body using a recently proposed triangulated mesh model called SCAPE which employs a low-dimensional, but detailed, parametric model of shape and pose-dependent deformations that is learned from a database of range scans of human bodies. Previous work showed that the parameters of the SCAPE model could be estimated from marker-based motion capture data. Here we go further to estimate the parameters directly from image data. We define a cost function between image observations and a hypothesized mesh and formulate the problem as optimization over the body shape and pose parameters using stochastic search. Our results show that such rich generative models enable the automatic recovery of detailed human shape and pose from images.","Humans,
Shape,
Biological system modeling,
Graphics,
Deformable models,
Parametric statistics,
Image databases,
Motion estimation,
Parameter estimation,
Cost function"
Random telegraph noise in flash memories - model and technology scaling,"This paper presents the first statistical model of Vt fluctuation (ΔVtcell) in a floating-gate flash memory due to random telegraph noise. It considers current-path percolation, which generates a large-amplitude-noise tail, caused by dopant induced surface potential non-uniformity It concludes that the impact of scaling is weaker than the widely-accepted 1/LeffWeff trend. 3-σ ΔVtcell is estimated to increase by 1.8x rather than ≫10x from 90 nm to 20 nm technology nodes.","Telegraphy,
Flash memory,
Electron traps,
Semiconductor device noise,
Fluctuations,
Semiconductor process modeling,
Nonvolatile memory,
Noise level,
Threshold voltage,
Noise measurement"
Peer-to-Peer in Metric Space and Semantic Space,This paper first proposes three improved gossip mechanisms by mapping links into metric space and dynamically adapting the number of selected neighbors to disseminate messages. Experiments and comparisons show that these mechanisms can improve the performance of gossip in peer-to-peer (P2P) networks. This is the effect of mapping a network into a metric space that differentiates nodes and links according to linking characteristics and controlling local information flow with knowing such differences. A further study about query routing on P2P semantic link network shows that mapping a network into a semantic space can also improve the performance. An intrinsic rule is found by experimental comparisons and analysis: the performance of a P2P network can be improved by designing an appropriate mapping from the network into metric space or semantic space. A general framework for networking with metric space and semantic space is suggested,
A System for Real-Time Measurement of the Brachial Artery Diameter in B-Mode Ultrasound Images,"The measurement of the brachial artery diameter is frequently used in clinical studies for evaluating the flow-mediated dilation and, in conjunction with the blood pressure value, for assessing arterial stiffness. This paper presents a system for computing the brachial artery diameter in real-time by analyzing B-mode ultrasound images. The method is based on a robust edge detection algorithm which is used to automatically locate the two walls of the vessel. The measure of the diameter is obtained with subpixel precision and with a temporal resolution of 25 samples/s, so that the small dilations induced by the cardiac cycle can also be retrieved. The algorithm is implemented on a standalone video processing board which acquires the analog video signal from the ultrasound equipment. Results are shown in real-time on a graphical user interface. The system was tested both on synthetic ultrasound images and in clinical studies of flow-mediated dilation. Accuracy, robustness, and intra/inter observer variability of the method were evaluated","Real time systems,
Brachytherapy,
Arteries,
Ultrasonic imaging,
Pressure measurement,
Ultrasonic variables measurement,
Robustness,
Blood pressure,
Image analysis,
Image edge detection"
Matching and Merging of Statecharts Specifications,"Model Management addresses the problem of managing an evolving collection of models, by capturing the relationships between models and providing well-defined operators to manipulate them. In this paper, we describe two such operators for manipulating hierarchical Statecharts: Match, for finding correspondences between models, and Merge, for combining models with respect to known correspondences between them. Our Match operator is heuristic, making use of both static and behavioural properties of the models to improve the accuracy of matching. Our Merge operator preserves the hierarchical structure of the input models, and handles differences in behaviour through parameterization. In this way, we automatically construct merges that preserve the semantics of Statecharts models. We illustrate and evaluate our work by applying our operators to AT&T telecommunication features.",
TARA: Topology-Aware Resource Adaptation to Alleviate Congestion in Sensor Networks,"Network congestion can be alleviated either by reducing demand (traffic control) or by increasing capacity (resource control). Unlike in traditional wired or other wireless counterparts, sensor network deployments provide elastic resource availability for satisfying the fidelity level required by applications. In many cases, using traffic control can violate fidelity requirements. Hence, we propose the use of resource control: increasing capacity by enabling more nodes to become active during periods of congestion. However, a naive approach to increase resources without a careful consideration of the type of congestion, traffic pattern, and network topology make the situation worse. In this paper, we present TARA, a topology-aware resource adaptation strategy to alleviate congestion. The core of TARA is our capacity analysis model, which can be used to estimate capacity of various topologies. Detailed performance results show that TARA can achieve data delivery rate and energy consumption that is close to an ideal offline resource control algorithm.","Traffic control,
Wireless sensor networks,
Communication system traffic control,
Network topology,
Remote monitoring,
Telecommunication traffic,
Capacitive sensors,
Energy efficiency,
Sensor phenomena and characterization,
Availability"
A Review of Core Compact Models for Undoped Double-Gate SOI MOSFETs,"In this paper, we review the compact-modeling framework for undoped double-gate (DG) silicon-on-insulator (SOI) MOSFETs. The use of multiple gates has emerged as a new technology to possibly replace the conventional planar MOSFET when its feature size is scaled to the sub-50-nm regime. MOSFET technology has been the choice for mainstream digital circuits for very large scale integration as well as for other high-frequency applications in the low-gigahertz range. But the continuing scaling of MOSFET presents many challenges, and multiple-gate, particularly DG, SOI devices seem to be attractive alternatives as they can effectively reduce the short-channel effects and yield higher current drive. Core compact models, including the analysis for surface potential and drain-current, for both the symmetric and asymmetric DG SOI MOSFETs, are discussed and compared. Numerical simulations are also included in order to assess the validity of the models reviewed","silicon-on-insulator,
MOSFET,
semiconductor device models"
Negative Samples Analysis in Relevance Feedback,"Recently, relevance feedback (RF) in content-based image retrieval (CBIR) has been implemented as an online binary classifier to separate the positive samples from the negative samples, where both sets of samples are labeled by the user. In many applications, it is reasonable to assume that all the positive samples are alike and thus that the region of the feature space occupied by the positive samples can be described by a single hypersurface. However, for the negative samples, previous RF methods either treat each one of the negative samples as an isolated point or assume the whole negative set can be described by a single convex hypersurface. In this paper, we argue that these treatments of the negative samples are not sound. Our belief is all positive samples are included in a set and the negative samples split into a small number of subsets, each one of which has a simple distribution. Therefore, we first cluster the negative samples into several groups; for each such negative group, we build a marginal convex machine (MCM) subclassifier between it and the single positive group which results in a series of subclassifiers. These subclassifiers are then incorporated into a biased MCM (BMCM) for RF. Experiments were carried out to prove the advantages of BMCM-based RF over previous methods for RF","Negative feedback,
Radio frequency,
Support vector machines,
Image retrieval,
Kernel,
Support vector machine classification,
Content based retrieval,
Feedback loop,
Boosting,
Entropy"
Surface-Constrained Volumetric Brain Registration Using Harmonic Mappings,"In order to compare anatomical and functional brain imaging data across subjects, the images must first be registered to a common coordinate system in which anatomical features are aligned. Intensity-based volume registration methods can align subcortical structures well, but the variability in sulcal folding patterns typically results in misalignment of the cortical surface. Conversely, surface-based registration using sulcal features can produce excellent cortical alignment but the mapping between brains is restricted to the cortical surface. Here we describe a method for volumetric registration that also produces an accurate one-to-one point correspondence between cortical surfaces. This is achieved by first parameterizing and aligning the cortical surfaces using sulcal landmarks. We then use a constrained harmonic mapping to extend this surface correspondence to the entire cortical volume. Finally, this mapping is refined using an intensity-based warp. We demonstrate the utility of the method by applying it to T1-weighted magnetic resonance images (MRIs). We evaluate the performance of our proposed method relative to existing methods that use only intensity information; for this comparison we compute the intersubject alignment of expert-labeled subcortical structures after registration.","Surface morphology,
Image registration,
Polynomials,
Brain,
Image processing,
Topology,
Image converters,
Magnetic resonance,
Magnetic resonance imaging,
Anatomical structure"
Beyond bottom-up: Incorporating task-dependent influences into a computational model of spatial attention,"A critical function in both machine vision and biological vision systems is attentional selection of scene regions worthy of further analysis by higher-level processes such as object recognition. Here we present the first model of spatial attention that (1) can be applied to arbitrary static and dynamic image sequences with interactive tasks and (2) combines a general computational implementation of both bottom-up (BU) saliency and dynamic top-down (TD) task relevance; the claimed novelty lies in the combination of these elements and in the fully computational nature of the model. The BU component computes a saliency map from 12 low-level multi-scale visual features. The TD component computes a low-level signature of the entire image, and learns to associate different classes of signatures with the different gaze patterns recorded from human subjects performing a task of interest. We measured the ability of this model to predict the eye movements of people playing contemporary video games. We found that the TD model alone predicts where humans look about twice as well as does the BU model alone; in addition, a combined BU*TD model performs significantly better than either individual component. Qualitatively, the combined model predicts some easy-to-describe but hard-to-compute aspects of attentional selection, such as shifting attention leftward when approaching a left turn along a racing track. Thus, our study demonstrates the advantages of integrating BU factors derived from a saliency map and TD factors learned from image and task contexts in predicting where humans look while performing complex visually-guided behavior.","Computational modeling,
Predictive models,
Humans,
Biology computing,
Biological system modeling,
Machine vision,
Layout,
Object recognition,
Image sequences,
Games"
Tracking as Repeated Figure/Ground Segmentation,"Tracking over a long period of time is challenging as the appearance, shape and scale of the object in question may vary. We propose a paradigm of tracking by repeatedly segmenting figure from background. Accurate spatial support obtained in segmentation provides rich information about the track and enables reliable tracking of non-rigid objects without drifting. Figure/ground segmentation operates sequentially in each frame by utilizing both static image cues and temporal coherence cues, which include an appearance model of brightness (or color) and a spatial model propagating figure/ground masks through low-level region correspondence. A superpixel-based conditional random field linearly combines cues and loopy belief propagation is used to estimate marginal posteriors of figure vs background. We demonstrate our approach on long sequences of sports video, including figure skating and football.","Shape,
Image segmentation,
Coherence,
Layout,
Computer science,
Brightness,
Belief propagation,
Nonlinear optics,
Image motion analysis,
Maintenance"
On Lifetime-Based Node Failure and Stochastic Resilience of Decentralized Peer-to-Peer Networks,"To model P2P networks that are commonly faced with high rates of churn and random departure decisions by end-users, this paper investigates the resilience of random graphs to lifetime-based node failure and derives the expected delay before a user is forcefully isolated from the graph and the probability that this occurs within his/her lifetime. Using these metrics, we show that systems with heavy-tailed lifetime distributions are more resilient than those with light-tailed (e.g., exponential) distributions and that for a given average degree, k-regular graphs exhibit the highest level of fault tolerance. As a practical illustration of our results, each user in a system with n = 100 billion peers, 30-minute average lifetime, and 1-minute node-replacement delay can stay connected to the graph with probability 1 - 1/n using only 9 neighbors. This is in contrast to 37 neighbors required under previous modeling efforts. We finish the paper by observing that many P2P networks are almost surely (i.e., with probability 1 - o(1)) connected if they have no isolated nodes and derive a simple model for the probability that a P2P system partitions under churn.","Peer to peer computing,
Stochastic processes,
Resilience,
Routing,
Failure analysis,
Computer science,
Fault tolerant systems,
Mathematics,
Delay systems,
Fingers"
"Fast, Approximately Optimal Solutions for Single and Dynamic MRFs","A new efficient MRF optimization algorithm, called Fast-PD, is proposed, which generalizes a-expansion. One of its main advantages is that it offers a substantial speedup over that method, e.g. it can be at least 3-9 times faster than a-expansion. Its efficiency is a result of the fact that Fast-PD exploits information coming not only from the original MRF problem, but also from a dual problem. Furthermore, besides static MRFs, it can also be used for boosting the performance of dynamic MRFs, i.e. MRFs varying over time. On top of that, Fast-PD makes no compromise about the optimality of its solutions: it can compute exactly the same answer as a-expansion, but, unlike that method, it can also guarantee an almost optimal solution for a much wider class of NP-hard MRF problems. Results on static and dynamic MRFs demonstrate the algorithm's efficiency and power. E.g., Fast-PD has been able to compute disparity for stereoscopic sequences in real time, with the resulting disparity coinciding with that of a-expansion.","Boosting,
Computer vision,
Optimization methods,
Computer science,
Pervasive computing,
Costs,
Computational efficiency,
Inference algorithms"
Distributed Mobility Management for Target Tracking in Mobile Sensor Networks,"Mobility management is a major challenge in mobile ad hoc networks (MANETs) due in part to the dynamically changing network topologies. For mobile sensor networks that are deployed for surveillance applications, it is important to use a mobility management scheme that can empower nodes to make better decisions regarding their positions such that strategic tasks such as target tracking can benefit from node movement. In this paper, we describe a distributed mobility management scheme for mobile sensor networks. The proposed scheme considers node movement decisions as part of a distributed optimization problem which integrates mobility-enhanced improvement in the quality of target tracking data with the associated negative consequences of increased energy consumption due to locomotion, potential loss of network connectivity, and loss of sensing coverage.","Mobile radio mobility management,
Target tracking,
Mobile ad hoc networks,
Sensor phenomena and characterization,
Bayesian methods,
Mobile communication,
Mobile robots,
Network topology,
Surveillance,
Energy consumption"
Connected Dominating Sets in Wireless Networks with Different Transmission Ranges,"Since there is no fixed infrastructure or centralized management in wireless ad hoc networks, a Connected Dominating Set (CDS) has been proposed to serve as a virtual backbone. The CDS of a graph representing a network has a significant impact on the efficient design of routing protocols in wireless networks. This problem has been studied extensively in Unit Disk Graphs (UDG), in which all nodes have the same transmission ranges. However, in practice, the transmission ranges of all nodes are not necessarily equal. In this paper, we model a network as a disk graph and introduce the CDS problem in disk graphs. We present two efficient approximation algorithms to obtain a minimum CDS. The performance ratio of these algorithms is constant if the ratio of the maximum transmission range over the minimum transmission range in the network is bounded. These algorithms can be implemented as distributed algorithms. Furthermore, we show a size relationship between a maximal independent set and a CDS as well as a bound of the maximum number of independent neighbors of a node in disk graphs. The theoretical analysis and simulation results are also presented to verify our approaches.","Wireless networks,
Spine,
Mobile ad hoc networks,
Routing protocols,
Approximation algorithms,
Distributed algorithms,
Analytical models,
Spread spectrum communication,
Relays,
Network topology"
Scan-Based Movement-Assisted Sensor Deployment Methods in Wireless Sensor Networks,"The efficiency of sensor networks depends on the coverage of the monitoring area. Although, in general, a sufficient number of sensors are used to ensure a certain degree of redundancy in coverage, a good sensor deployment is still necessary to balance the workload of sensors. In a sensor network with locomotion facilities, sensors can move around to self-deploy. The movement-assisted sensor deployment deals with moving sensors from an initial unbalanced state to a balanced state. Therefore, various optimization problems can be defined to minimize different parameters, including total moving distance, total number of moves, communication/computation cost, and convergence rate. In this paper, we first propose a Hungarian-algorithm-based optimal solution, which is centralized. Then, a localized scan-based movement-assisted sensor deployment method (SMART) and several variations of it that use scan and dimension exchange to achieve a balanced state are proposed. An extended SMART is developed to address a unique problem called communication holes in sensor networks. Extensive simulations have been done to verify the effectiveness of the proposed scheme.",
Recognizing Human Activities from Silhouettes: Motion Subspace and Factorial Discriminative Graphical Model,"We describe a probabilistic framework for recognizing human activities in monocular video based on simple silhouette observations in this paper. The methodology combines kernel principal component analysis (KPCA) based feature extraction and factorial conditional random field (FCRF) based motion modeling. Silhouette data is represented more compactly by nonlinear dimensionality reduction that explores the underlying structure of the articulated action space and preserves explicit temporal orders in projection trajectories of motions. FCRF models temporal sequences in multiple interacting ways, thus increasing joint accuracy by information sharing, with the ideal advantages of discriminative models over generative ones (e.g., relaxing independence assumption between observations and the ability to effectively incorporate both overlapping features and long-range dependencies). The experimental results on two recent datasets have shown that the proposed framework can not only accurately recognize human activities with temporal, intra-and inter-person variations, but also is considerably robust to noise and other factors such as partial occlusion and irregularities in motion styles.","Humans,
Graphical models,
Hidden Markov models,
Feature extraction,
Motion analysis,
Nonlinear optics,
Image motion analysis,
Video sequences,
Systems engineering and theory,
Kernel"
Machine Learning With AIBO Robots in the Four-Legged League of RoboCup,"Robot learning is a growing area of research at the intersection of robotics and machine learning. The main contributions of this paper include a review of how machine learning has been used on Sony AIBO robots and at RoboCup, with a focus on the four-legged league during the years 1998-2004. The review shows that the application-oriented use of machine learning in the four-legged league was still conservative and restricted to a few well-known and easy-to-use methods such as standard decision trees, evolutionary hill climbing, and support vector machines. Method-oriented spin-off studies emerged more frequently and increasingly addressed new and advanced machine learning techniques. Further, the paper presents some details about the growing impact of machine learning in the software system developed by the authors' robot soccer team-the NUbots","Machine learning,
Robot vision systems,
Computer science,
Australia,
Software systems,
Legged locomotion,
Humans,
Computational modeling,
Hardware,
Intelligent robots"
Acoustic Beamforming for Speaker Diarization of Meetings,"When performing speaker diarization on recordings from meetings, multiple microphones of different qualities are usually available and distributed around the meeting room. Although several approaches have been proposed in recent years to take advantage of multiple microphones, they are either too computationally expensive and not easily scalable or they cannot outperform the simpler case of using the best single microphone. In this paper, the use of classic acoustic beamforming techniques is proposed together with several novel algorithms to create a complete frontend for speaker diarization in the meeting room domain. New techniques we are presenting include blind reference-channel selection, two-step time delay of arrival (TDOA) Viterbi postprocessing, and a dynamic output signal weighting algorithm, together with using such TDOA values in the diarization to complement the acoustic information. Tests on speaker diarization show a 25% relative improvement on the test set compared to using a single most centrally located microphone. Additional experimental results show improvements using these techniques in a speech recognition task.","Array signal processing,
Loudspeakers,
Acoustic testing,
Speech,
Ambient intelligence,
Computer science,
Signal processing,
Switches,
Microphone arrays,
Associate members"
Bootstrap - Inspired Techniques in Computation Intelligence,"This article is about the success story of a seemingly simple yet extremely powerful approach that has recently reached a celebrity status in statistical and engineering sciences. The hero of this story - bootstrap resampling - is relatively young, but the story itself is a familiar one within the scientific community: a mathematician or a statistician conceives and formulates a theory that is first developed by fellow mathematicians and then brought to fame by other professionals, typically engineers, who point to many applications that can benefit from just such an approach. Signal processing boasts some of the finest examples of such stories, such as the classic story of Fourier transforms or the more contemporary tale of wavelet transforms.","Competitive intelligence,
Computational intelligence,
Signal processing algorithms,
Signal processing,
Statistical distributions,
Data engineering,
Fourier transforms,
Power engineering computing,
Power engineering and energy,
Fellows"
ESOP-based Toffoli Gate Cascade Generation,An ESOP-based Toffoli gate cascade synthesis algorithm is presented. The algorithm is capable of generating a cascade of reversible gates for logic functions with large numbers of qubits. The algorithm is fast as it uses a simple cost metric heuristic during a recursive divide-and-conquer function to determine NOT and Toffoli gate placement.,
Haptic Feedback Enhances Force Skill Learning,"This paper explores the use of haptic feedback to teach an abstract motor skill that requires recalling a sequence of forces. Participants are guided along a trajectory and are asked to learn a sequence of one-dimensional forces via three paradigms: haptic training, visual training, or combined visuohaptic training. The extent of learning is measured by accuracy of force recall. We find that recall following visuohaptic training is significantly more accurate than recall following visual or haptic training alone, although haptic training alone is inferior to visual training alone. This suggests that in conjunction with visual feedback, haptic training may be an effective tool for teaching sensorimotor skills that have a force-sensitive component to them, such as surgery. We also present a dynamic programming paradigm to align and compare spatiotemporal haptic trajectories","Force feedback,
Haptic interfaces,
Surgery,
Shape,
Force measurement,
Education,
Spatiotemporal phenomena,
Virtual environment,
Computer science,
Laboratories"
Analysis of Parasitic PNP Bipolar Transistor Mitigation Using Well Contacts in 130 nm and 90 nm CMOS Technology,"Three-dimensional TCAD models are used in mixed- mode simulations to analyze the effectiveness of well contacts at mitigating parasitic PNP bipolar conduction due to a direct hit ion strike. 130 nm and 90 nm technology are simulated. Results show careful well contact design can improve mitigation. However, well contact effectiveness is seen to decrease from the 130 nm to the 90 nm simulations.","CMOS technology,
Bipolar transistors,
Analytical models,
Computational modeling,
Electrons,
Semiconductor device modeling,
Circuit simulation,
Design automation,
Single event upset,
Geometry"
Shape Descriptors for Maximally Stable Extremal Regions,"This paper introduces an affine invariant shape descriptor for maximally stable extremal regions (MSER). Affine invariant feature descriptors are normally computed by sampling the original grey-scale image in an invariant frame defined from each detected feature, but we instead use only the shape of the detected MSER itself. This has the advantage that features can be reliably matched regardless of the appearance of the surroundings of the actual region. The descriptor is computed using the scale invariant feature transform (SIFT), with the resampled MSER binary mask as input. We also show that the original MSER detector can be modified to achieve better scale invariance by detecting MSERs in a scale pyramid. We make extensive comparisons of the proposed feature against a SIFT descriptor computed on grey-scale patches, and also explore the possibility of grouping the shape descriptors into pairs to incorporate more context. While the descriptor does not perform as well on planar scenes, we demonstrate various categories of full 3D scenes where it outperforms the SIFT descriptor computed on grey-scale patches. The shape descriptor is also shown to be more robust to changes in illumination. We show that a system can achieve the best performance under a range of imaging conditions by matching both the texture and shape descriptors.","Shape,
Layout,
Lighting,
Object recognition,
Computer vision,
Computer science,
Image sampling,
Detectors,
Robustness,
Object detection"
Fast Local Rerouting for Handling Transient Link Failures,"Link failures are part of the day-to-day operation of a network due to many causes such as maintenance, faulty interfaces, and accidental fiber cuts. Commonly deployed link state routing protocols such as OSPF react to link failures through global link state advertisements and routing table recomputations causing significant forwarding discontinuity after a failure. Careful tuning of various parameters to accelerate routing convergence may cause instability when the majority of failures are transient. To enhance failure resiliency without jeopardizing routing stability, we propose a local rerouting based approach called failure insensitive routing. The proposed approach prepares for failures using interface-specific forwarding, and upon a failure, suppresses the link state advertisement and instead triggers local rerouting using a backwarding table. With this approach, when no more than one link failure notification is suppressed, a packet is guaranteed to be forwarded along a loop-free path to its destination if such a path exists. This paper demonstrates the feasibility, reliability, and stability of our approach","Routing protocols,
Computer science,
Acceleration,
Convergence,
Stability,
Business,
Internet telephony,
Availability,
Cultural differences"
A Study of Face Recognition as People Age,"In this paper we study face recognition across ages within a real passport photo verification task. First, we propose using the gradient orientation pyramid for this task. Discarding the gradient magnitude and utilizing hierarchical techniques, we found that the new descriptor yields a robust and discriminative representation. With the proposed descriptor, we model face verification as a two-class problem and use a support vector machine as a classifier. The approach is applied to two passport data sets containing more than 1,800 image pairs from each person with large age differences. Although simple, our approach outperforms previously tested Bayesian technique and other descriptors, including the intensity difference and gradient with magnitude. In addition, it works as well as two commercial systems. Second, for the first time, we empirically study how age differences affect recognition performance. Our experiments show that, although the aging process adds difficulty to the recognition task, it does not surpass illumination or expression as a confounding factor.","Face recognition,
Aging,
Robustness,
Support vector machines,
Testing,
Computer science,
Educational institutions,
Bayesian methods,
Lighting,
Jacobian matrices"
Data-Driven Grasp Synthesis Using Shape Matching and Task-Based Pruning,"Human grasps, especially whole-hand grasps, are difficult to animate because of the high number of degrees of freedom of the hand and the need for the hand to conform naturally to the object surface. Captured human motion data provides us with a rich source of examples of natural grasps. However, for each new object, we are faced with the problem of selecting the best grasp from the database and adapting it to that object. This paper presents a data-driven approach to grasp synthesis. We begin with a database of captured human grasps. To identify candidate grasps for a new object, we introduce a novel shape matching algorithm that matches hand shape to object shape by identifying collections of features having similar relative placements and surface normals. This step returns many grasp candidates, which are clustered and pruned by choosing the grasp best suited for the intended task. For pruning undesirable grasps, we develop an anatomically-based grasp quality measure specific to the human hand. Examples of grasp synthesis are shown for a variety of objects not present in the original database. This algorithm should be useful both as an animator tool for posing the hand and for automatic grasp synthesis in virtual environments.",
Software Engineering for Automotive Systems: A Roadmap,"The first pieces of software were introduced into cars in 1976. By 2010, premium class vehicles are expected to contain one gigabyte of on-board software. We present research challenges in the domain of automotive software engineering.","Software engineering,
Automotive engineering,
Software systems,
Computer science,
Systems engineering and theory,
Software testing,
Software architecture,
Computer architecture,
Software safety,
System testing"
A Concentric Morphology Model for the Detection of Masses in Mammography,"We propose a technique for the automated detection of malignant masses in screening mammography. The technique is based on the presence of concentric layers surrounding a focal area with suspicious morphological characteristics and low relative incidence in the breast region. Mammographic locations with high concentration of concentric layers with progressively lower average intensity are considered suspicious deviations from normal parenchyma. The multiple concentric layers (MCLs) technique was trained and tested using the craniocaudal views of 270 mammographic cases with biopsy proven malignant masses from the digital database of screening mammography. One-half of the available cases were used for optimizing the parameters of the detection algorithm. The remaining cases were used for testing. During testing, malignant masses were detected with 92%, 88%, and 81% sensitivity at 5.4, 2.4, and 0.6 false positive marks per image. Testing on 82 normal screening mammograms showed a false positive rate of 5.0, 1.7, and 0.2 marks per image at the previously reported operating points. Furthermore, additional evaluation on 135 benign cases produced a significantly lower detection rate for benign masses (61.6%, 58.3%, and 43.7% at 5.1, 2.8, and 1.2 false positives per image, respectively). Overall, MCL is a promising computer-assisted detection strategy for screening mammograms to identify malignant masses while maintaining the detection rate of benign masses considerably lower.","Morphology,
Mammography,
Testing,
Cancer detection,
Biopsy,
Benign tumors,
Biomedical engineering,
Computer science,
Breast cancer,
Design automation"
Beyond Local Appearance: Category Recognition from Pairwise Interactions of Simple Features,"We present a discriminative shape-based algorithm for object category localization and recognition. Our method learns object models in a weakly-supervised fashion, without requiring the specification of object locations nor pixel masks in the training data. We represent object models as cliques of fully-interconnected parts, exploiting only the pairwise geometric relationships between them. The use of pairwise relationships enables our algorithm to successfully overcome several problems that are common to previously-published methods. Even though our algorithm can easily incorporate local appearance information from richer features, we purposefully do not use them in order to demonstrate that simple geometric relationships can match (or exceed) the performance of state-of-the-art object recognition algorithms.","Shape,
Solid modeling,
Object recognition,
Humans,
Deformable models,
Image recognition,
Training data,
Animals,
Cognitive science,
Computer vision"
A Cross Layered MAC and Clustering Scheme for Efficient Broadcast in VANETs,"In this paper, we illustrate the design of a cross-layered MAC and clustering solution for supporting the fast propagation of broadcast messages in a vehicular ad hoc network (VANET). A distributed dynamic clustering algorithm is proposed to create a dynamic virtual backbone in the vehicular network. The vehicle-members of the backbone are responsible for implementing an efficient messages propagation. The backbone creation and maintenance are proactively performed aiming to balance the stability of backbone connections as well as the cost/efficiency trade-off and the hops-reduction when forwarding broadcast messages. A fast multi-hop MAC forwarding mechanism is defined to exploit the role of backbone vehicles, under a cross-layered approach. Simulation results show the effectiveness of the mutual support of proactive clustering and MAC protocols for efficient dissemination of broadcast messages in VANETs.","Broadcasting,
Spine,
Vehicle dynamics,
Ad hoc networks,
Clustering algorithms,
Heuristic algorithms,
Stability,
Costs,
Vehicles,
Media Access Protocol"
Selection and Fusion of Color Models for Image Feature Detection,"The choice of a color model is of great importance for many computer vision algorithms (e.g., feature detection, object recognition, and tracking) as the chosen color model induces the equivalence classes to the actual algorithms. As there are many color models available, the inherent difficulty is how to automatically select a single color model or, alternatively, a weighted subset of color models producing the best result for a particular task. The subsequent hurdle is how to obtain a proper fusion scheme for the algorithms so that the results are combined in an optimal setting. To achieve proper color model selection and fusion of feature detection algorithms, in this paper, we propose a method that exploits nonperfect correlation between color models or feature detection algorithms derived from the principles of diversification. As a consequence, a proper balance is obtained between repeatability and distinctiveness. The result is a weighting scheme which yields maximal feature discrimination. The method is verified experimentally for three different image feature detectors. The experimental results show that the fusion method provides feature detection results having a higher discriminative power than the standard weighting scheme. Further, it is experimentally shown that the color model selection scheme provides a proper balance between color invariance (repeatability) and discriminative power (distinctiveness)",
A Systematic Review of Theory Use in Software Engineering Experiments,"Empirically based theories are generally perceived as foundational to science. However, in many disciplines, the nature, role and even the necessity of theories remain matters for debate, particularly in young or practical disciplines such as software engineering. This article reports a systematic review of the explicit use of theory in a comprehensive set of 103 articles reporting experiments, from of a total of 5,453 articles published in major software engineering journals and conferences in the decade 1993-2002. Of the 103 articles, 24 use a total of 40 theories in various ways to explain the cause-effect relationship(s) under investigation. The majority of these use theory in the experimental design to justify research questions and hypotheses, some use theory to provide post hoc explanations of their results, and a few test or modify theory. A third of the theories are proposed by authors of the reviewed articles. The interdisciplinary nature of the theories used is greater than that of research in software engineering in general. We found that theory use and awareness of theoretical issues are present, but that theory-driven research is, as yet, not a major issue in empirical software engineering. Several articles comment explicitly on the lack of relevant theory. We call for an increased awareness of the potential benefits of involving theory, when feasible. To support software engineering researchers who wish to use theory, we show which of the reviewed articles on which topics use which theories for what purposes, as well as details of the theories' characteristics","Software engineering,
Design for experiments,
Testing,
Computer industry,
Programming"
Finding a Path Subject to Many Additive QoS Constraints,"A fundamental problem in quality-of-service (QoS) routing is to find a path between a source-destination node pair that satisfies two or more end-to-end QoS constraints. We model this problem using a graph with n vertices and m edges with K additive QoS parameters associated with each edge, for any constant Kges2. This problem is known to be NP-hard. Fully polynomial time approximation schemes (FPTAS) for the case of K=2 have been reported in the literature. We concentrate on the general case and make the following contributions. 1) We present a very simple (Km+nlogn) time K-approximation algorithm that can be used in hop-by-hop routing protocols. 2) We present an FPTAS for one optimization version of the QoS routing problem with a time complexity of O(m(n/epsi)K-1). 3) We present an FPTAS for another optimization version of the QoS routing problem with a time complexity of O(nlogn+m(H/epsi)K-1) when there exists an H-hop path satisfying all QoS constraints. When K is reduced to 2, our results compare favorably with existing algorithms. The results of this paper hold for both directed and undirected graphs. For ease of presentation, undirected graph is used",
Gaussian Mean-Shift Is an EM Algorithm,"The mean-shift algorithm, based on ideas proposed by Fukunaga and Hosteller, is a hill-climbing algorithm on the density defined by a finite mixture or a kernel density estimate. Mean-shift can be used as a nonparametric clustering method and has attracted recent attention in computer vision applications such as image segmentation or tracking. We show that, when the kernel is Gaussian, mean-shift is an expectation-maximization (EM) algorithm and, when the kernel is non-Gaussian, mean-shift is a generalized EM algorithm. This implies that mean-shift converges from almost any starting point and that, in general, its convergence is of linear order. For Gaussian mean-shift, we show: 1) the rate of linear convergence approaches 0 (superlinear convergence) for very narrow or very wide kernels, but is often close to 1 (thus, extremely slow) for intermediate widths and exactly 1 (sublinear convergence) for widths at which modes merge, 2) the iterates approach the mode along the local principal component of the data points from the inside of the convex hull of the data points, and 3) the convergence domains are nonconvex and can be disconnected and show fractal behavior. We suggest ways of accelerating mean-shift based on the EM interpretation",
Impact of Process Variations on Multicore Performance Symmetry,"Multi-core architectures introduce a new granularity at which process variations may occur, yielding asymmetry among cores that were designed - and that software expects to be symmetric in performance. The chief source of this phenomenon are highly correlated, ""systematic"" within-die variations such as optical imperfections yielding variations across the exposure field. Per-core voltages can be used to bring all cores to the same performance level, but this compensation strategy also affects power, chiefly due to leakage power. Boosting a core's frequency may therefore boost its leakage sufficiently to engage thermal throttling. This sets up a tradeoff between static performance asymmetry due to frequency variation versus dynamic performance asymmetry due to thermal throttling. This paper explores the potential magnitude of these effects",
Robust 3-D Modeling of Vasculature Imagery Using Superellipsoids,"This paper presents methods to model complex vasculature in three-dimensional (3-D) images using cylindroidal superellipsoids, along with robust estimation and detection algorithms for automated image analysis. This model offers an explicit, low-order parameterization, enabling joint estimation of boundary, centerlines, and local pose. It provides a geometric framework for directed vessel traversal, and extraction of topological information like branch point locations and connectivity. M-estimators provide robust region-based statistics that are used to drive the superellipsoid toward a vessel boundary. A robust likelihood ratio test is used to differentiate between noise, artifacts, and other complex unmodeled structures, thereby verifying the model estimate. The proposed methodology behaves well across scale-space, shows a high degree of insensitivity to adjacent structures and implicitly handles branching. When evaluated on synthetic imagery mimicking specific structural complexities in tumor microvasculature, it consistently produces ubvoxel accuracy estimates of centerlines and widths in the presence of closely-adjacent vessels, branch points, and noise. An edit-based validation demonstrated a precision level of 96.6% at a recall level of 95.4%. Overall, it is robust enough for large-scale application",
A Geometric Theory for Analysis and Synthesis of Sub-6 DoF Parallel Manipulators,"Mechanism synthesis is mostly dependent on the designer's experience and intuition and is difficult to automate. This paper aims to develop a rigorous and precise geometric theory for analysis and synthesis of sub-6 DoF (or lower mobility) parallel manipulators. Using Lie subgroups and submanifolds of the special Euclidean group SE(3), we first develop a unified framework for modelling commonly used primitive joints and task spaces. We provide a mathematically rigorous definition of the notion of motion type using conjugacy classes. Then, we introduce a new structure for subchains of parallel manipulators using the product of two subgroups of SE(3) and discuss its realization in terms of the primitive joints. We propose the notion of quotient manipulators that substantially enriches the topologies of serial manipulators. Finally, we present a general procedure for specifying the subchain structures given the desired motion type of a parallel manipulator. The parallel mechanism synthesis problem is thus solved using the realization techniques developed for serial manipulators. Generality of the theory is demonstrated by systematically generating a large class of feasible topologies for (parallel or serial) mechanisms with a desired motion type of either a Lie subgroup or a submanifold.","Topology,
Kinematics,
Mechatronics,
Design optimization,
Mechanical factors,
Computer science,
Humans,
Machine tools,
Robot programming,
Manipulators"
Scented Widgets: Improving Navigation Cues with Embedded Visualizations,"This paper presents scented widgets, graphical user interface controls enhanced with embedded visualizations that facilitate navigation in information spaces. We describe design guidelines for adding visual cues to common user interface widgets such as radio buttons, sliders, and combo boxes and contribute a general software framework for applying scented widgets within applications with minimal modifications to existing source code. We provide a number of example applications and describe a controlled experiment which finds that users exploring unfamiliar data make up to twice as many unique discoveries using widgets imbued with social navigation data. However, these differences equalize as familiarity with the data increases.","Data visualization,
Radio navigation,
User interfaces,
Data analysis,
Application software,
Switches,
Guidelines,
Costs,
Animal structures,
Collaboration"
Diffusion Basis Functions Decomposition for Estimating White Matter Intravoxel Fiber Geometry,"In this paper, we present a new formulation for recovering the fiber tract geometry within a voxel from diffusion weighted magnetic resonance imaging (MRI) data, in the presence of single or multiple neuronal fibers. To this end, we define a discrete set of diffusion basis functions. The intravoxel information is recovered at voxels containing fiber crossings or bifurcations via the use of a linear combination of the above mentioned basis functions. Then, the parametric representation of the intravoxel fiber geometry is a discrete mixture of Gaussians. Our synthetic experiments depict several advantages by using this discrete schema: the approach uses a small number of diffusion weighted images (23) and relatively small b values (1250 s/mm2 ), i.e., the intravoxel information can be inferred at a fraction of the acquisition time required for datasets involving a large number of diffusion gradient orientations. Moreover our method is robust in the presence of more than two fibers within a voxel, improving the state-of-the-art of such parametric models. We present two algorithmic solutions to our formulation: by solving a linear program or by minimizing a quadratic cost function (both with non-negativity constraints). Such minimizations are efficiently achieved with standard iterative deterministic algorithms. Finally, we present results of applying the algorithms to synthetic as well as real data.","Geometry,
Magnetic resonance imaging,
Iterative algorithms,
Tensile stress,
Nerve fibers,
Computer science,
Magnetic field measurement,
Bifurcation,
Gaussian processes,
Robustness"
A Novel Method for Detecting Cropped and Recompressed Image Block,"One of the most common practices in image tampering involves cropping a patch from a source and pasting it onto a target. In this paper, we present a novel method for the detection of such tampering operations in JPEG images. The lossy JPEG compression introduces inherent blocking artifacts into the image and our method exploits such artifacts to serve as a 'watermark' for the detection of image tampering. We develop the blocking artifact characteristics matrix (BACM) and show that, for the original JPEG images, the BACM exhibits regular symmetrical shape; for images that are cropped from another JPEG image and re-saved as JPEG images, the regular symmetrical property of the BACM is destroyed. We fully exploit this property of the BACM and derive representation features from the BACM to train a support vector machine (SVM) classifier for recognizing whether an image is an original JPEG image or it has been cropped from another JPEG image and re-saved as a JPEG image. We present experiment results to show the efficacy of our method.","Transform coding,
Digital images,
Watermarking,
Image coding,
Forgery,
Support vector machines,
Support vector machine classification,
Counterfeiting,
Digital cameras,
Quantization"
Alternating Minimization Algorithms for Transmission Tomography,"A family of alternating minimization algorithms for finding maximum-likelihood estimates of attenuation functions in transmission X-ray tomography is described. The model from which the algorithms are derived includes polyenergetic photon spectra, background events, and nonideal point spread functions. The maximum-likelihood image reconstruction problem is reformulated as a double minimization of the I-divergence. A novel application of the convex decomposition lemma results in an alternating minimization algorithm that monotonically decreases the objective function. Each step of the minimization is in closed form. The family of algorithms includes variations that use ordered subset techniques for increasing the speed of convergence. Simulations demonstrate the ability to correct the cupping artifact due to beam hardening and the ability to reduce streaking artifacts that arise from beam hardening and background events",
Topology-Controlled Volume Rendering,"Topology provides a foundation for the development of mathematically sound tools for processing and exploration of scalar fields. Existing topology-based methods can be used to identify interesting features in volumetric data sets, to find seed sets for accelerated isosurface extraction, or to treat individual connected components as distinct entities for isosurfacing or interval volume rendering. We describe a framework for direct volume rendering based on segmenting a volume into regions of equivalent contour topology and applying separate transfer functions to each region. Each region corresponds to a branch of a hierarchical contour tree decomposition, and a separate transfer function can be defined for it. The novel contributions of our work are: 1) a volume rendering framework and interface where a unique transfer function can be assigned to each subvolume corresponding to a branch of the contour tree, 2) a runtime method for adjusting data values to reflect contour tree simplifications, 3) an efficient way of mapping a spatial location into the contour tree to determine the applicable transfer function, and 4) an algorithm for hardware-accelerated direct volume rendering that visualizes the contour tree-based segmentation at interactive frame rates using graphics processing units (GPUs) that support loops and conditional branches in fragment programs",
Computational Framework for Simulating Fluorescence Microscope Images With Cell Populations,"Fluorescence microscopy combined with digital imaging constructs a basic platform for numerous biomedical studies in the field of cellular imaging. As the studies relying on analysis of digital images have become popular, the validation of image processing methods used in automated image cytometry has become an important topic. Especially, the need for efficient validation has arisen from emerging high-throughput microscopy systems where manual validation is impractical. We present a simulation platform for generating synthetic images of fluorescence-stained cell populations with realistic properties. Moreover, we show that the synthetic images enable the validation of analysis methods for automated image cytometry and comparison of their performance. Finally, we suggest additional usage scenarios for the simulator. The presented simulation framework, with several user-controllable parameters, forms a versatile tool for many kinds of validation tasks, and is freely available at http://www.cs.tut.fi/sgn/csb/simcep.",
Using Low-Power Modes for Energy Conservation in Ethernet LANs,"Most Ethernet interfaces available for deployment in switches and hosts today can operate in a variety of different low power modes. However, currently these modes have very limited usage models. They do not take advantage of periods of inactivity, when the links remain idle or under-utilized. In this study, we propose methods that allow for detection of such periods to obtain energy savings with little impact on loss or delay. We evaluate our methods on a wide range of real-time traffic traces collected at a high-speed backbone switch within our campus LAN. Our results show that Ethernet interfaces at both ends can be put in extremely low power modes anywhere from 40%-98% of the time observed. In addition, we found that approximately 37% of interfaces studied (on the same switch) can be put in low power modes simultaneously which opens the potential for further energy savings in the switching fabric within the switch.",
A Replicated Quantitative Analysis of Fault Distributions in Complex Software Systems,"To contribute to the body of empirical research on fault distributions during development of complex software systems, a replication of a study of Fenton and Ohlsson is conducted. The hypotheses from the original study are investigated using data taken from an environment that differs in terms of system size, project duration, and programming language. We have investigated four sets of hypotheses on data from three successive telecommunications projects: 1) the Pareto principle, that is, a small number of modules contain a majority of the faults (in the replication, the Pareto principle is confirmed), 2) fault persistence between test phases (a high fault incidence in function testing is shown to imply the same in system testing, as well as prerelease versus postrelease fault incidence), 3) the relation between number of faults and lines of code (the size relation from the original study could be neither confirmed nor disproved in the replication), and 4) fault density similarities across test phases and projects (in the replication study, fault densities are confirmed to be similar across projects). Through this replication study, we have contributed to what is known on fault distributions, which seem to be stable across environments.","Software systems,
System testing,
Software engineering,
Computer languages,
Quality management,
Telecommunication switching,
Conducting materials"
Addressing Legal Requirements in Requirements Engineering,"Legal texts, such as regulations and legislation, are playing an increasingly important role in requirements engineering and system development. Monitoring systems for requirements and policy compliance has been recognized in the requirements engineering community as a key area for research. Similarly, regulatory compliance is critical in systems that are governed by regulations and law, especially given that non-compliance can result in both financial and criminal penalties. Working with legal texts can be very challenging, however, because they contain numerous ambiguities, cross-references, domain-specific definitions, and acronyms, and are frequently amended via new regulations and case law. Requirements engineers and compliance auditors must be able to identify relevant regulations, extract requirements and other key concepts, and monitor compliance throughout the software lifecycle. This paper surveys research efforts over the past 50 years in handling legal texts for systems development. These efforts include the use of symbolic logic, logic programming, first-order temporal logic, deontic logic, defeasible logic, goal modeling, and semi-structured representations. This survey can aid requirements engineers and auditors to better specify, monitor, and test software systems for compliance.","Law,
Legal factors,
Logic programming,
Monitoring,
Government,
Information security,
Legislation,
Software systems,
Computer science,
Systems engineering and theory"
Mobility Limited Flip-Based Sensor Networks Deployment,"An important phase of sensor networks operation is deployment of sensors in the field of interest. Critical goals during sensor networks deployment include coverage, connectivity, load balancing, etc. A class of work has recently appeared, where mobility in sensors is leveraged to meet deployment objectives. In this paper, we study deployment of sensor networks using mobile sensors. The distinguishing feature of our work is that the sensors in our model have limited mobilities. More specifically, the mobility in the sensors we consider is restricted to a flip, where the distance of the flip is bounded. We call such sensors as flip-based sensors. Given an initial deployment of flip-based sensors in a field, our problem is to determine a movement plan for the sensors in order to maximize the sensor network coverage and minimize the number of flips. We propose a minimum-cost maximum-flow-based solution to this problem. We prove that our solution optimizes both the coverage and the number of flips. We also study the sensitivity of coverage and the number of flips to flip distance under different initial deployment distributions of sensors. We observe that increased flip distance achieves better coverage and reduces the number of flips required per unit increase in coverage. However, such improvements are constrained by initial deployment distributions of sensors due to the limitations on sensor mobility","Intelligent sensors,
Load management,
Combustion,
Propellers,
Airplanes,
Landmine detection,
Fuel storage,
Sparks,
Propulsion,
Steering systems"
Implications of radio fingerprinting on the security of sensor networks,"We demonstrate the feasibility of finger-printing the radio of wireless sensor nodes (Chipcon 1000 radio, 433MHz). We show that, with this type of devices, a receiver can create device radio finger-prints and subsequently identify origins of messages exchanged between the devices, even if message contents and device identifiers are hidden. We further analyze the implications of device fingerprinting on the security of sensor networking protocols, specifically, we propose two new mechanisms for the detection of wormholes in sensor networks.","Fingerprint recognition,
Protocols,
Wireless sensor networks,
Protection,
Message authentication,
Computer science,
Spread spectrum communication,
Network topology,
Cloning,
Computer security"
An Improved Dynamic User Authentication Scheme for Wireless Sensor Networks,"Over the last few years, many researchers have paid a lot of attention to the user authentication problem. However, to date, there has been relatively little research suited for wireless sensor networks. Recently, Wong et al. proposed a dynamic user authentication scheme for WSNs that allows legitimate users to query sensor data at every sensor node of the network. We show that Wong et al.'s scheme is vulnerable to the replay and forgery attacks and propose a lightweight dynamic user authentication scheme for WSNs. The proposed scheme not only retains all the advantages in Wong et al.'s scheme but also enhances its security by withstanding the security weaknesses and allows legitimate users to change their passwords freely. In comparison with the previous scheme, our proposed scheme possesses many advantages, including resistance of the replay and forgery attacks, reduction of user's password leakage risk, capability of changeable password, and better efficiency.","Authentication,
Wireless sensor networks,
Forgery,
Protection,
Temperature sensors,
Information security,
Computer science,
Resists,
Condition monitoring,
Humidity"
Remote activation of ICs for piracy prevention and digital right management,"We introduce a remote activation scheme that aims to protect integrated circuits (IC) intellectual property (IP) against piracy. Remote activation enables designers to lock each working IC and to then remotely enable it. The new method exploits inherent unclonable variability in modern manufacturing for unique identification (ID) and integrate the IDs into the circuit functionality. The objectives are realized by replication of a few states of the finite state machine (FSM) and adding control to the state transitions. On each chip, the added control signals are a function of the unique IDs and are thus unclonable. On standard benchmark circuits, the experimental results show that the novel activation method is stable, unclonable, attack-resilient, while having a low overhead and a unique key for each IC.","Virtual manufacturing,
Semiconductor device manufacture,
Hardware,
Watermarking,
Computer science,
Digital integrated circuits,
Intrusion detection,
Intellectual property,
Databases,
Companies"
Energy-Efficient Scheduling for Real-Time Systems on Dynamic Voltage Scaling (DVS) Platforms,"Energy-efficient designs have played import roles for hardware and software implementations for a decade. With the advanced technology of VLSI circuit designs, energy-efficiency can be achieved by adopting the dynamic voltage scaling (DVS) technique. In this paper, we survey the studies for energy-efficient scheduling in real-time systems on DVS platforms to cover both theoretical and practical issues.","Energy efficiency,
Dynamic scheduling,
Real time systems,
Dynamic voltage scaling,
Voltage control,
Energy consumption,
Processor scheduling,
Computer science,
Design engineering,
Power engineering and energy"
Separation of Singing Voice From Music Accompaniment for Monaural Recordings,"Separating singing voice from music accompaniment is very useful in many applications, such as lyrics recognition and alignment, singer identification, and music information retrieval. Although speech separation has been extensively studied for decades, singing voice separation has been little investigated. We propose a system to separate singing voice from music accompaniment for monaural recordings. Our system consists of three stages. The singing voice detection stage partitions and classifies an input into vocal and nonvocal portions. For vocal portions, the predominant pitch detection stage detects the pitch of the singing voice and then the separation stage uses the detected pitch to group the time-frequency segments of the singing voice. Quantitative results show that the system performs the separation task successfully","Humans,
Auditory system,
Music information retrieval,
Instruments,
Automatic speech recognition,
Computer science,
Speech recognition,
Time frequency analysis,
Laboratories,
Cognitive science"
Provenance for Visualizations: Reproducibility and Beyond,"The demand for the construction of complex visualizations is growing in many disciplines of science, as users are faced with ever increasing volumes of data to analyze. In this paper, the authors present VisTrails, an open source provenance-management system that provides infrastructure for data exploration and visualization. VisTrails transparently records detailed provenance of exploratory computational tasks and leverages this information beyond just the ability to reproduce and share results. In particular, it uses this information to simplify the process of exploring data through visualization.","Reproducibility of results,
Data visualization,
Collaboration,
Data analysis,
Explosions,
Assembly,
Grid computing,
Mesh generation,
Libraries,
Web services"
Segmenting Motions of Different Types by Unsupervised Manifold Clustering,"We propose a novel algorithm for segmenting multiple motions of different types from point correspondences in multiple affine or perspective views. Since point trajectories associated with different motions live in different manifolds, traditional approaches deal with only one manifold type: linear subspaces for affine views, and homographic, bilinear and trilinear varieties for two and three perspective views. As real motion sequences contain motions of different types, we cast motion segmentation as a problem of clustering manifolds of different types. Rather than explicitly modeling each manifold as a linear, bilinear or multilinear variety, we use nonlinear dimensionality reduction to learn a low-dimensional representation of the union of all manifolds. We show that for a union of separated manifolds, the LLE algorithm computes a matrix whose null space contains vectors giving the segmentation of the data. An analysis of the variance of these vectors allows us to distinguish them from other vectors in the null space. This leads to a new algorithm for clustering both linear and nonlinear manifolds. Although this algorithm is theoretically designed for separated manifolds, our experiments demonstrate its performance on real data where this assumption does not hold. We test our algorithm on the Hopkins 155 motion segmentation database and achieve an average classification error of 4.8%, which compares favorably against state-of-the art multiframe motion segmentation methods.","Clustering algorithms,
Motion segmentation,
Computer vision,
Vectors,
Null space,
Analysis of variance,
Data analysis,
Functional analysis,
Algorithm design and analysis,
Testing"
The Discipline of Embedded Systems Design,"The wall between computer science and electrical engineering has kept the potential of embedded systems at bay. It is time to build a new scientific foundation with embedded systems design as the cornerstone, which will ensure a systematic and even-handed integration of the two fields. The embedded systems design problem certainly raises technology questions, but more important, it requires building a new scientific foundation that will systematically and even-handedly integrate computation and physicality from the bottom up. Support for this foundation will require enriching computer science paradigms to encompass models and methods traditionally found in electrical engineering.","Embedded system,
Hardware,
Mathematical model,
Embedded computing,
Physics computing,
Software design,
Analytical models,
Power engineering computing,
Jitter,
Computational modeling"
Supervised Learning of Image Restoration with Convolutional Networks,"Convolutional networks have achieved a great deal of success in high-level vision problems such as object recognition. Here we show that they can also be used as a general method for low-level image processing. As an example of our approach, convolutional networks are trained using gradient learning to solve the problem of restoring noisy or degraded images. For our training data, we have used electron microscopic images of neural circuitry with ground truth restorations provided by human experts. On this dataset, Markov random field (MRF), conditional random field (CRF), and anisotropic diffusion algorithms perform about the same as simple thresholding, but superior performance is obtained with a convolutional network containing over 34,000 adjustable parameters. When restored by this convolutional network, the images are clean enough to be used for segmentation, whereas the other approaches fail in this respect. We do not believe that convolutional networks are fundamentally superior to MRFs as a representation for image processing algorithms. On the contrary, the two approaches are closely related. But in practice, it is possible to train complex convolutional networks, while even simple MRF models are hindered by problems with Bayesian learning and inference procedures. Our results suggest that high model complexity is the single most important factor for good performance, and this is possible with convolutional networks.","Supervised learning,
Image restoration,
Image processing,
Object recognition,
Circuit noise,
Degradation,
Training data,
Electron microscopy,
Humans,
Markov random fields"
OPTIMOL: automatic Online Picture collecTion via Incremental MOdel Learning,"A well-built dataset is a necessary starting point for advanced computer vision research. It plays a crucial role in evaluation and provides a continuous challenge to state-of-the-art algorithms. Dataset collection is, however, a tedious and time-consuming task. This paper presents a novel automatic dataset collecting and model learning approach that uses object recognition techniques in an incremental method. The goal of this work is to use the tremendous resources of the web to learn robust object category models in order to detect and search for objects in real-world cluttered scenes. It mimics the human learning process of iteratively accumulating model knowledge and image examples. We adapt a non-parametric graphical model and propose an incremental learning framework. Our algorithm is capable of automatically collecting much larger object category datasets for 22 randomly selected classes from the Caltech 101 dataset. Furthermore, we offer not only more images in each object category dataset, but also a robust object model and meaningful image annotation. Our experiments show that OPTIMOL is capable of collecting image datasets that are superior to Caltech 101 and LabelMe.","Robustness,
Search engines,
Iterative algorithms,
Computer vision,
Humans,
Airplanes,
Internet,
Training data,
Data engineering,
Computer science"
Towards Scalable Representations of Object Categories: Learning a Hierarchy of Parts,"This paper proposes a novel approach to constructing a hierarchical representation of visual input that aims to enable recognition and detection of a large number of object categories. Inspired by the principles of efficient indexing (bottom-up,), robust matching (top-down,), and ideas of compositionality, our approach learns a hierarchy of spatially flexible compositions, i.e. parts, in an unsupervised, statistics-driven manner. Starting with simple, frequent features, we learn the statistically most significant compositions (parts composed of parts), which consequently define the next layer. Parts are learned sequentially, layer after layer, optimally adjusting to the visual data. Lower layers are learned in a category-independent way to obtain complex, yet sharable visual building blocks, which is a crucial step towards a scalable representation. Higher layers of the hierarchy, on the other hand, are constructed by using specific categories, achieving a category representation with a small number of highly generalizable parts that gained their structural flexibility through composition within the hierarchy. Built in this way, new categories can be efficiently and continuously added to the system by adding a small number of parts only in the higher layers. The approach is demonstrated on a large collection of images and a variety of object categories. Detection results confirm the effectiveness and robustness of the learned parts.",
Modeling and Analysis of Real-Time Cooperative Systems Using Petri Nets,"The existing formal techniques are not suitable for elegantly modeling passing value indeterminacy and describing batch processing function in real-time cooperative systems. Moreover, the correct behavior of the systems depends on not only the logical correctness of the results obtained through running workflows but also the time of producing them before critical deadlines. For these purposes, this paper proposes an interorganizational logical workflow net (ILWN) for modeling and analyzing real-time cooperative systems based on time Petri nets, workflow techniques, and temporal logic. Through attaching logical expressions to some actions of an ILWN model, the size of the model is reduced. Thus, ILWNs can efficiently mitigate the state explosion problem to some extent. Also, this paper analyzes the soundness of a subclass of ILWNs: the or-restricted ILWNs. A rigorous analysis approach is given based on their static net structures only. The concepts and techniques proposed in this paper are illustrated with a seller-buyer example in electronic commerce.","Real time systems,
Cooperative systems,
Petri nets,
Computer science,
Logic,
Safety,
Laboratories,
Joining processes,
Explosions,
Electronic commerce"
Algebraic Constructions of Optimal Frequency-Hopping Sequences,"Frequency-hopping (FH) spread spectrum and direct-sequence spread spectrum are two main spread-coding technologies. Frequency-hopping sequences are needed in FH code-division multiple-access (CDMA) systems. In this correspondence, three classes of optimal frequency-hopping sequences are constructed with algebraic methods. The three classes are based on perfect nonlinear functions, power functions, and norm functions, respectively. Both individual optimal frequency-hopping sequences and optimal families of frequency-hopping sequences are presented.",
Frequent Closed Sequence Mining without Candidate Maintenance,"Previous studies have presented convincing arguments that a frequent pattern mining algorithm should not mine all frequent patterns but only the closed ones because the latter leads to not only a more compact yet complete result set but also better efficiency. However, most of the previously developed closed pattern mining algorithms work under the candidate maintenance-and- test paradigm, which is inherently costly in both runtime and space usage when the support threshold is low or the patterns become long. In this paper, we present BIDE, an efficient algorithm for mining frequent closed sequences without candidate maintenance. It adopts a novel sequence closure checking scheme called Bl-Directional Extension and prunes the search space more deeply compared to the previous algorithms by using the BackScan pruning method. A thorough performance study with both sparse and dense, real, and synthetic data sets has demonstrated that BIDE significantly outperforms the previous algorithm: It consumes an order(s) of magnitude less memory and can be more than an order of magnitude faster. It is also linearly scalable in terms of database size.","Data mining,
Bidirectional control,
Itemsets,
Runtime,
Spatial databases,
Application software,
Computer bugs,
Large-scale systems,
Open source software,
Association rules"
"Leveraging temporal, contextual and ordering constraints for recognizing complex activities in video","We present a scalable approach to recognizing and describing complex activities in video sequences. We are interested in long-term, sequential activities that may have several parallel streams of action. Our approach integrates temporal, contextual and ordering constraints with output from low-level visual detectors to recognize complex, long-term activities. We argue that a hierarchical, object-oriented design lends our solution to be scalable in that higher-level reasoning components are independent from the particular low-level detector implementation and that recognition of additional activities and actions can easily be added. Three major components to realize this design are: a dynamic Bayesian network structure for representing activities comprised of partially ordered sub-actions, an object-oriented action hierarchy for building arbitrarily complex action detectors and an approximate Viterbi-like algorithm for inferring the most likely observed sequence of actions. Additionally, this study proposes the Erlang distribution as a comprehensive model of idle time between actions and frequency of observing new actions. We show results for our approach on real video sequences containing complex activities.","Detectors,
Video sequences,
Computerized monitoring,
Bayesian methods,
Object oriented modeling,
State estimation,
Viterbi algorithm,
Computer science,
Streaming media,
Algorithm design and analysis"
Consensus Clusterings,"In this paper we address the problem of combining multiple clusterings without access to the underlying features of the data. This process is known in the literature as clustering ensembles, clustering aggregation, or consensus clustering. Consensus clustering yields a stable and robust final clustering that is in agreement with multiple clusterings. We find that an iterative EM-like method is remarkably effective for this problem. We present an iterative algorithm and its variations for finding clustering consensus. An extensive empirical study compares our proposed algorithms with eleven other consensus clustering methods on four data sets using three different clustering performance metrics. The experimental results show that the new ensemble clustering methods produce clusterings that are as good as, and often better than, these other methods.",
Random Linear Network Coding: A free cipher?,"We consider the level of information security provided by random linear network coding in network scenarios in which all nodes comply with the communication protocols yet are assumed to be potential eavesdroppers (i.e. ""nice but curious""). For this setup, which differs from wiretapping scenarios considered previously, we develop a natural algebraic security criterion, and prove several of its key properties. A preliminary analysis of the impact of network topology on the overall network coding security, in particular for complete directed acyclic graphs, is also included.",
A Hierarchical Modeling and Analysis for Grid Service Reliability,"Grid computing is a recently developed technology. Although the developmental tools and techniques for the grid have been extensively studied, grid reliability analysis is not easy because of its complexity. This paper is the first one that presents a hierarchical model for the grid service reliability analysis and evaluation. The hierarchical modeling is mapped to the physical and logical architecture of the grid service system and makes the evaluation and calculation tractable by identifying the independence among layers. Various types of failures are interleaved in the grid computing environment, such as blocking failures, time-out failures, matchmaking failures, network failures, program failures, and resource failures. This paper investigates all of them to achieve a complete picture about grid service reliability. Markov models, queuing theory, and graph theory are mainly used to model, evaluate, and analyze the grid service reliability. Numerical examples are illustrated",
Co-ranking Authors and Documents in a Heterogeneous Network,"Recent graph-theoretic approaches have demonstrated remarkable successes for ranking networked entities, but most of their applications are limited to homogeneous networks such as the network of citations between publications. This paper proposes a novel method for co-ranking authors and their publications using several networks: the social network connecting the authors, the citation network connecting the publications, as well as the authorship network that ties the previous two together. The new co-ranking framework is based on coupling two random walks, that separately rank authors and documents following the PageRankparadigm. As a result, improved rankings of documents and their authors depend on each other in a mutually reinforcing way, thus taking advantage of the additional information implicit in the heterogeneous network of authors and documents.",
Multiplexing for Optimal Lighting,"Imaging of objects under variable lighting directions is an important and frequent practice in computer vision, machine vision, and image-based rendering. Methods for such imaging have traditionally used only a single light source per acquired image. They may result in images that are too dark and noisy, e.g., due to the need to avoid saturation of highlights. We introduce an approach that can significantly improve the quality of such images, in which multiple light sources illuminate the object simultaneously from different directions. These illumination-multiplexed frames are then computationally demultiplexed. The approach is useful for imaging dim objects, as well as objects having a specular reflection component. We give the optimal scheme by which lighting should be multiplexed to obtain the highest quality output, for signal-independent noise. The scheme is based on Hadamard codes. The consequences of imperfections such as stray light, saturation, and noisy illumination sources are then studied. In addition, the paper analyzes the implications of shot noise, which is signal-dependent, to Hadamard multiplexing. The approach facilitates practical lighting setups having high directional resolution. This is shown by a setup we devise, which is flexible, scalable, and programmable. We used it to demonstrate the benefit of multiplexing in experiments.",
Local Probabilistic Models for Link Prediction,"One of the core tasks in social network analysis is to predict the formation of links (i.e. various types of relationships) over time. Previous research has generally represented the social network in the form of a graph and has leveraged topological and semantic measures of similarity between two nodes to evaluate the probability of link formation. Here we introduce a novel local probabilistic graphical model method that can scale to large graphs to estimate the joint co-occurrence probability of two nodes. Such a probability measure captures information that is not captured by either topological measures or measures of semantic similarity, which are the dominant measures used for link prediction. We demonstrate the effectiveness of the co-occurrence probability feature by using it both in isolation and in combination with other topological and semantic features for predicting co-authorship collaborations on real datasets.",
Effect of Well and Substrate Potential Modulation on Single Event Pulse Shape in Deep Submicron CMOS,"Simulations are used to characterize the single event transient current and voltage waveforms in deep submicron CMOS integrated circuits. Results indicate that the mechanism controlling the height and duration of the observed current plateau is the redistribution of the electrostatic potential in the substrate following a particle strike. Quantitative circuit and technology factors influencing the mechanism include restoring current, device sizing, and well and substrate doping.",
Normalization-Cooperated Gradient Feature Extraction for Handwritten Character Recognition,"The gradient direction histogram feature has shown superior performance in character recognition. To alleviate the effect of stroke direction distortion caused by shape normalization and provide higher recognition accuracies, we propose a new feature extraction approach, called normalization-cooperated gradient feature (NCGF) extraction, which maps the gradient direction elements of original image to direction planes without generating the normalized image and can be combined with various normalization methods. Experiments on handwritten Japanese and Chinese character databases show that, compared to normalization-based gradient feature, the NCGF reduces the recognition error rate by factors ranging from 8.63 percent to 14.97 percent with high confidence of significance when combined with pseudo-two-dimensional normalization.",
A Robust Fingerprint Indexing Scheme Using Minutia Neighborhood Structure and Low-Order Delaunay Triangles,"Fingerprint indexing is a key technique in automatic fingerprint identification systems (AFIS). However, handling fingerprint distortion is still a problem. This paper concentrates on a more accurate fingerprint indexing algorithm that efficiently retrieves the top N possible matching candidates from a huge database. To this end, we design a novel feature based on minutia neighborhood structure (we call this minutia detail and it contains richer minutia information) and a more stable triangulation algorithm (low-order Delaunay triangles, consisting of order 0 and 1 Delaunay triangles), which are both insensitive to fingerprint distortion. The indexing features include minutia detail and attributes of low-order Delaunay triangle (its handedness, angles, maximum edge, and related angles between orientation field and edges). Experiments on databases FVC2002 and FVC2004 show that the proposed algorithm considerably narrows down the search space in fingerprint databases and is stable for various fingerprints. We also compared it with other indexing approaches, and the results show our algorithm has better performance, especially on fingerprints with distortion.","Robustness,
Fingerprint recognition,
Indexing,
Spatial databases,
Character recognition,
Information retrieval,
Algorithm design and analysis,
Intelligent systems,
Computer science,
Delay"
Survey on High-Temperature Packaging Materials for SiC-Based Power Electronics Modules,"High temperature SiC devices need the materials for device packages also capable of working at higher temperature than those for Si devices. This paper presents a selection of materials that are potentially suitable for use in high temperature package assembly, including die attach, substrate, interconnections, encapsulation, case, heat spreader and heat sink. The temperature under consideration is up to 250degC, corresponding to the need of many applications, including automobiles and aircraft.",
Secured Flipped Scan-Chain Model for Crypto-Architecture,Scan chains are exploited to develop attacks on cryptographic hardware and steal intellectual properties from the chip. This paper proposes a secured strategy to test designs by inserting a certain number of inverters between randomly selected scan cells. The security of the scheme has been analyzed. Two detailed case studies of RC4 stream cipher and AES block cipher have been presented to show that the proposed strategy prevents existing scan-based attacks in the literature. The elegance of the scheme lies in its less hardware overhead.,
Floating Codes for Joint Information Storage in Write Asymmetric Memories,"Memories whose storage cells transit irreversibly between states have been common since the start of the data storage technology. In recent years, flash memories and other non-volatile memories based on floating-gate cells have become a very important family of such memories. We model them by the write asymmetric memory (WAM), a memory where each cell is in one of q states - state 0, 1, middotmiddotmiddot, q - 1 - and can only transit from a lower state to a higher state. Data stored in a WAM can be rewritten by shifting the cells to higher states. Since the state transition is irreversible, the number of times of rewriting is limited. When multiple variables are stored in a WAM, we study codes, which we call floating codes, that maximize the total number of times the variables can be written and rewritten. In this paper, we present several families of floating codes that either are optimal, or approach optimality as the codes get longer. We also present bounds to the performance of general floating codes. The results show that floating codes can integrate the rewriting capabilities of different variables to a surprisingly high degree.",
Toward Broadcast Reliability in Mobile Ad Hoc Networks with Double Coverage,"The broadcast operation, as a fundamental service in mobile ad hoc networks (MANETs), is prone to the broadcast storm problem if forwarding nodes are not carefully designated. The objective of reducing broadcast redundancy while still providing high delivery ratio under high transmission error rate is a major challenge in MANETs. In this paper, we propose a simple broadcast algorithm, called double-covered broadcast (DCB), which takes advantage of broadcast redundancy to improve the delivery ratio in an environment that has rather high transmission error rate. Among the 1-hop neighbors of the sender, only selected forwarding nodes retransmit the broadcast message. Forwarding nodes are selected in such a way that 1) the sender's 2-hop neighbors are covered and 2) the sender's 1-hop neighbors are either forwarding nodes or nonforwarding nodes covered by at least two forwarding neighbors. The retransmissions of the forwarding nodes are received by the sender as the confirmation of their reception of the packet. The nonforwarding 1-hop neighbors of the sender do not acknowledge the reception of the broadcast. If the sender does not detect all its forwarding nodes' retransmissions, it resends the packet until the maximum number of retries is reached. Simulation results show that the proposed broadcast algorithm provides good performance under a high transmission error rate environment",
Conditional Anomaly Detection,"When anomaly detection software is used as a data analysis tool, finding the hardest-to-detect anomalies is not the most critical task. Rather, it is often more important to make sure that those anomalies that are reported to the user are in fact interesting. If too many unremarkable data points are returned to the user labeled as candidate anomalies, the software can soon fall into disuse. One way to ensure that returned anomalies are useful is to make use of domain knowledge provided by the user. Often, the data in question includes a set of environmental attributes whose values a user would never consider to be directly indicative of an anomaly. However, such attributes cannot be ignored because they have a direct effect on the expected distribution of the result attributes whose values can indicate an anomalous observation. This paper describes a general purpose method called conditional anomaly detection for taking such differences among attributes into account, and proposes three different expectation-maximization algorithms for learning the model that is used in conditional anomaly detection. Experiments with more than 13 different data sets compare our algorithms with several other more standard methods for outlier or anomaly detection","Humans,
Data analysis,
Application software,
Data mining,
Costs,
Software tools,
Expectation-maximization algorithms,
Biomedical informatics,
Computer vision,
Computer security"
A Topological Approach to Hierarchical Segmentation using Mean Shift,"Mean shift is a popular method to segment images and videos. Pixels are represented by feature points, and the segmentation is driven by the point density in feature space. In this paper, we introduce the use of Morse theory to interpret mean shift as a topological decomposition of the feature space into density modes. This allows us to build on the watershed technique and design a new algorithm to compute mean-shift segmentations of images and videos. In addition, we introduce the use of topological persistence to create a segmentation hierarchy. We validated our method by clustering images using color cues. In this context, our technique runs faster than previous work, especially on videos and large images. We evaluated accuracy with a classical benchmark which shows results on par with existing low-level techniques, i.e. we do not sacrifice accuracy for speed.","Image segmentation,
Videos,
Clustering algorithms,
Kernel,
Space technology,
Computational efficiency,
Density functional theory,
Computer science,
Artificial intelligence,
Laboratories"
Representing and Querying Correlated Tuples in Probabilistic Databases,"Probabilistic databases have received considerable attention recently due to the need for storing uncertain data produced by many real world applications. The widespread use of probabilistic databases is hampered by two limitations: (1) current probabilistic databases make simplistic assumptions about the data (e.g., complete independence among tuples) that make it difficult to use them in applications that naturally produce correlated data, and (2) most probabilistic databases can only answer a restricted subset of the queries that can be expressed using traditional query languages. We address both these limitations by proposing a framework that can represent not only probabilistic tuples, but also correlations that may be present among them. Our proposed framework naturally lends itself to the possible world semantics thus preserving the precise query semantics extant in current probabilistic databases. We develop an efficient strategy for query evaluation over such probabilistic databases by casting the query processing problem as an inference problem in an appropriately constructed probabilistic graphical model. We present several optimizations specific to probabilistic databases that enable efficient query evaluation. We validate our approach by presenting an experimental evaluation that illustrates the effectiveness of our techniques at answering various queries using real and synthetic datasets.",
Learning With Weblogs: Enhancing Cognitive and Social Knowledge Construction,"This study investigated the impact of weblog use on individual learning in the context of university senior-level business education. As an emergent form of personal communication, weblogs enable people to publish their thoughts as webpages, and to share information and knowledge. Recognizing the potential impact of weblogs on knowledge expression and sharing, this research sought to empirically examine whether the continuous use of weblogs as online learning logs would affect student learning performance. The assumption was that effective use of weblogs promoted the constructivist models of learning by supporting both cognitive and social knowledge construction, and by reinforcing individual accountability in learning. Results from an Information Systems undergraduate course with 31 participants indicated that the performance of students' weblogs was a significant predictor of the learning outcome (while traditional coursework was not). Moreover, individuals' cognitive construction effort to build their own mental models and social construction effort to further enrich/expand knowledge resources appeared to be two key aspects of the constructivist learning with weblogs. Our results imply the potential benefit of using weblogs as a knowledge construction tool and a social learning medium","Information systems,
Context,
Cognitive science,
Education,
Knowledge acquisition,
Publishing,
Surges,
Watches,
Educational products,
Technology management"
Enhanced Perimeter Routing for Geographic Forwarding Protocols in Urban Vehicular Scenarios,"Geographic stateless routing schemes such as GPSR have been widely adopted to routing in vehicular ad hoc networks (VANET). However, due to the particular urban topology and the non-uniform distribution of cars, the greedy routing mode often fails and needs a recovery strategy such as GPSR's perimeter mode to deliver data successfully to the destination. It has been shown that the cost of planarization, the non-uniform distribution of cars, and radio obstacles make GPSR's perimeter mode inefficient in urban configurations. Some enhancements have been proposed such as GPCR, which uses the concept of junction nodes to control the next road segments that packets should follow. However, the concept of junction nodes itself is problematic and hard to maintain in a dynamic urban environment. In this paper, we describe GpsrJ+, a solution that further improves the packet delivery ratio of GPCR with minimal modification by predicting on which road segment its neighboring junction node will forward packets to. GpsrJ+ differs from GPCR as decisions about which road segment to turn does not need to be made by junction nodes. Moreover, GpsrJ+ does not need an expensive planarization strategy since it uses the natural planar feature of urban maps. Consequently, GpsrJ+ reduces the hop count used in the perimeter mode by as much as 200% compared to GPSR. It therefore allows geographic routing schemes to return to the greedy mode faster.","Routing protocols,
Roads,
Planarization,
Vehicle dynamics,
Vehicle driving,
Ad hoc networks,
Safety,
Intelligent transportation systems,
Mobile ad hoc networks,
Vehicles"
Uncertainty Visualization in Medical Volume Rendering Using Probabilistic Animation,"Direct volume rendering has proved to be an effective visualization method for medical data sets and has reached wide-spread clinical use. The diagnostic exploration, in essence, corresponds to a tissue classification task, which is often complex and time-consuming. Moreover, a major problem is the lack of information on the uncertainty of the classification, which can have dramatic consequences for the diagnosis. In this paper this problem is addressed by proposing animation methods to convey uncertainty in the rendering. The foundation is a probabilistic Transfer Function model which allows for direct user interaction with the classification. The rendering is animated by sampling the probability domain over time, which results in varying appearance for uncertain regions. A particularly promising application of this technique is a ""sensitivity lens"" applied to focus regions in the data set. The methods have been evaluated by radiologists in a study simulating the clinical task of stenosis assessment, in which the animation technique is shown to outperform traditional rendering in terms of assessment accuracy.",
"Appearance Characterization of Linear Lambertian Objects, Generalized Photometric Stereo, and Illumination-Invariant Face Recognition","Traditional photometric stereo algorithms employ a Lambertian reflectance model with a varying albedo field and involve the appearance of only one object. In this paper, we generalize photometric stereo algorithms to handle all appearances of all objects in a class, in particular the human face class, by making use of the linear Lambertian property. A linear Lambertian object is one which is linearly spanned by a set of basis objects and has a Lambertian surface. The linear property leads to a rank constraint and, consequently, a factorization of an observation matrix that consists of exemplar images of different objects (e.g., faces of different subjects) under different, unknown illuminations. Integrability and symmetry constraints are used to fully recover the subspace bases using a novel linearized algorithm that takes the varying albedo field into account. The effectiveness of the linear Lambertian property is further investigated by using it for the problem of illumination-invariant face recognition using just one image. Attached shadows are incorporated in the model by a careful treatment of the inherent nonlinearity in Lambert's law. This enables us to extend our algorithm to perform face recognition in the presence of multiple illumination sources. Experimental results using standard data sets are presented","Photometry,
Face recognition,
Stereo vision,
Lighting,
Reflectivity,
Humans,
Jacobian matrices,
Automation,
Subspace constraints,
Face detection"
"Incremental Learning of Tasks From User Demonstrations, Past Experiences, and Vocal Comments","Since many years the robotics community is envisioning robot assistants sharing the same environment with humans. It became obvious that they have to interact with humans and should adapt to individual user needs. Especially the high variety of tasks robot assistants will be facing requires a highly adaptive and user-friendly programming interface. One possible solution to this programming problem is the learning-by-demonstration paradigm, where the robot is supposed to observe the execution of a task, acquire task knowledge, and reproduce it. In this paper, a system to record, interpret, and reason over demonstrations of household tasks is presented. The focus is on the model-based representation of manipulation tasks, which serves as a basis for incremental reasoning over the acquired task knowledge. The aim of the reasoning is to condense and interconnect the data, resulting in more general task knowledge. A measure for the assessment of information content of task features is introduced. This measure for the relevance of certain features relies both on general background knowledge as well as task-specific knowledge gathered from the user demonstrations. Beside the autonomous information estimation of features, speech comments during the execution, pointing out the relevance of features are considered as well. The results of the incremental growth of the task knowledge when more task demonstrations become available and their fusion with relevance information gained from speech comments is demonstrated within the task of laying a table","Humans,
Robot programming,
Humanoid robots,
Robot sensing systems,
Speech,
Educational robots,
Knowledge transfer,
Service robots,
Face,
Computer science"
"Context-Aware Middleware for Anytime, Anywhere Social Networks","Anytime, anywhere social computing requires several support mechanisms and tools, including location and proximity systems, expressive representation models of physical place and user characteristics, and effective social-matching algorithms. Anytime, anywhere social-network computing also requires shared and interoperable vocabularies for modeling location and entity characteristics. Current solutions tend to address only a subset of these issues. We believe that the success of anytime, anywhere social computing depends on middleware solutions that separate social-network management concerns from application requirements. Our middleware solution, the socially aware and mobile architecture (SAMOA), integrates a set of common management facilities for personalizing location-dependent social networks, and for propagating social networks' visibility up to the application level.","social sciences computing,
middleware,
mobile computing,
open systems"
Seven League Boots: A New Metaphor for Augmented Locomotion through Moderately Large Scale Immersive Virtual Environments,"When an immersive virtual environment represents a space that is larger than the available space within which a user can travel by directly walking, it becomes necessary to consider alternative methods for traveling through that space. The traditional solution is to require the user to travel 'indirectly', using a device that changes his viewpoint in the environment without actually requiring him to move - for example, a joystick. However, other solutions involving variations on direct walking are also possible. In this paper, we present a new metaphor for natural, augmented direct locomotion through moderately large-scale immersive virtual environments (IVEs) presented via head mounted display systems, which we call seven league boots. The key characteristic of this method is that it involves determining a user's intended direction of travel and then augmenting only the component of his or her motion that is aligned with that direction. After reviewing previously proposed methods for enabling intuitive locomotion through large IVEs, we begin by describing the technical implementation details of our novel method, discussing the various alternative options that we explored and parameters that we varied in an attempt to attain optimal performance. We then present the results of a pilot observer experiment that we conducted in an attempt to obtain objective, qualitative insight into the relative strengths and weaknesses of our new method, in comparison to the three most commonly used alternative locomotion methods: flying, via use of a wand; normal walking, with a uniform gain applied to the output of the tracker; and normal walking without gain, but with the location and orientation of the larger virtual environment periodically adjusted relative to position of the participant in the real environment. In this study we found, among other things, that for travel down a long, straight virtual hallway, participants overwhelmingly preferred the seven league boots method to the other methods, overall",
Point-Based Rigid-Body Registration Using an Unscented Kalman Filter,"We present and validate a novel registration algorithm mapping two data sets, generated from a rigid object, in the presence of Gaussian noise. The proposed method is based on the unscented Kalman filter (UKF) algorithm that is generally employed for analyzing nonlinear systems corrupted by additive Gaussian noise. First, we employ our proposed registration algorithm to fit two randomly generated data sets in the presence of isotropic Gaussian noise, when the corresponding points between the two data sets are assumed to be known. Then, we extend the registration method to the case where the data (with known correspondences) is stimulated by anisotropic Gaussian noise. The new registration method not only reliably converges to the correct registration solution, but it also estimates the variance, as a confidence measure, for each of the estimated registration transformation parameters. Furthermore, we employ the proposed registration algorithm for rigid-body, point-based registration where corresponding points between two registering data sets are unknown. The algorithm is tested on point data sets which are garnered from a pelvic cadaver and a scaphoid bone phantom by means of computed tomography (CT) and tracked free-hand ultrasound imaging. The collected 3-D points in the ultrasound frame are registered to the 3-D meshes in the CT frame by using the proposed and the standard iterative closest points (ICP) registration algorithms. Experimental results demonstrate that our proposed method significantly outperforms the ICP registration algorithm in the presence of additive Gaussian noise. It is also shown that the proposed registration algorithm is more robust than the ICP registration algorithm in terms of outliers in data sets and initial misalignment between the two data sets.",
Efficient Top-k Query Evaluation on Probabilistic Data,"Modern enterprise applications are forced to deal with unreliable, inconsistent and imprecise information. Probabilistic databases can model such data naturally, but SQL query evaluation on probabilistic databases is difficult: previous approaches have either restricted the SQL queries, or computed approximate probabilities, or did not scale, and it was shown recently that precise query evaluation is theoretically hard. In this paper we describe a novel approach, which computes and ranks efficiently the top-k answers to a SQL query on a probabilistic database. The restriction to top-k answers is natural, since imprecisions in the data often lead to a large number of answers of low quality, and users are interested only in the answers with the highest probabilities. The idea in our algorithm is to run in parallel several Monte-Carlo simulations, one for each candidate answer, and approximate each probability only to the extent needed to compute correctly the top-k answers.",
Time-optimal paths for a Dubins airplane,"We consider finding a time-optimal trajectory for an airplane from some starting point and orientation to some final point and orientation. Our model extends the Dubins car by L.E. Dubins (1957) to have altitude, which leads to Dubins airplane. We assume that the system has independent bounded control over the altitude velocity as well as the turning rate in the plane. Through the use of the Pontryagin Maximum Principle, we characterize the time-optimal trajectories for the system. They are composed of turns with minimum radius, straight line segments, and pieces of planar elastica. One motivation for determining these elementary pieces is for use as motion primitives in modern planning and control algorithms that consider obstacles.",
The feasibility of information dissemination in vehicular ad-hoc networks,"In this paper we consider information dissemination in vehicular ad-hoc networks (VANETs) in city scenarios. Information dissemination is an important building block of many proposed VANET applications. These applications need a certain dissemination performance to work satisfactorily. This is critical during the rollout of VANETs, when only few cars participate. After analytical considerations, we focus on simulations using a detailed model of a whole city. We assess the dissemination performance depending on the amount of equipped vehicles on the road. For few equipped vehicles, we show that dissemination speed and coverage will not be sufficient. Therefore, we propose to use specialized, but simple and cheap infrastructure, stationary supporting units (SSUs). If a small number of SSUs is installed in a city and connected via some backbone network, the dissemination performance improves dramatically, especially during the VANET rollout phase. Thus, SSUs allow for a faster and earlier rollout of working, dissemination-based VANET applications","Ad hoc networks,
Cities and towns,
Protocols,
Road vehicles,
Computer science,
Application software,
Road transportation,
3G mobile communication,
Satellites,
Stress"
Minimum-Latency Broadcast Scheduling in Wireless Ad Hoc Networks,"A wide range of applications for wireless ad hoc networks are time-critical and impose stringent requirement on the communication latency. This paper studies the problem Minimum-Latency Broadcast Scheduling (MLBS) in wireless ad hoc networks represented by unit-disk graphs. This problem is NP-hard. A trivial lower bound on the minimum broadcast latency is the radius R of the network with respect to the source of the broadcast, which is the maximum distance of all the nodes from the source of the broadcast. The previously best-known approximation algorithm for MLBS produces a broadcast schedule with latency at most 648 R. In this paper, we present three progressively improved approximation algorithms for MLBS. They produce broadcast schedules with latency at most 24 R -23, 16 R -15, and R + O (log R) respectively.","Mobile ad hoc networks,
Delay,
Approximation algorithms,
Processor scheduling,
Computer science,
Peer to peer computing,
Scheduling algorithm,
Time factors,
Radio broadcasting,
Communications Society"
Slotted Scheduled Tag Access in Multi-Reader RFID Systems,"Radio frequency identification (RFID) is a technology where a reader device can ""sense"" the presence of a close-by object by reading a tag device attached to the object. To improve coverage, multiple RFID readers can be deployed in the given region. In this paper, we consider the problem of slotted scheduled access of RFID tags in a multiple reader environment. In particular, we develop centralized algorithms in a slotted time model to read all the tags using near-optimal number of time slots. We consider two scenarios -one wherein the tag distribution in the physical space is unknown, and the other where tag distribution is known or can be estimated a priori. For each of these scenarios, we consider two cases depending on whether a single channel or multiple channels are available. All the above version of the problem are NP-hard. We design approximation algorithms for the single channel and heuristic algorithms for the multiple channel cases. Through extensive simulations, we show that for the single channel case, our heuristics perform close to the approximation algorithms. In general, our simulations show that our algorithms significantly outperform colorwave, an existing algorithm for similar problems.","Radiofrequency identification,
Approximation algorithms,
Throughput,
Scheduling algorithm,
Intrusion detection,
Processor scheduling,
RFID tags,
Spatial resolution,
Access protocols,
Batteries"
SET: Detecting node clones in sensor networks,"Sensor nodes that are deployed in hostile environments are vulnerable to capture and compromise. An adversary may obtain private information from these sensors, clone and intelligently deploy them in the network to launch a variety of insider attacks. This attack process is broadly termed as a clone attack. Currently, the defenses against clone attacks are not only very few, but also suffer from selective interruption of detection and high overhead (computation and memory). In this paper, we propose a new effective and efficient scheme, called SET, to detect such clone attacks. The key idea of SET is to detect clones by computing set operations (intersection and union) of exclusive subsets in the network. First, SET securely forms exclusive unit subsets among one-hop neighbors in the network in a distributed way. This secure subset formation also provides the authentication of nodes’ subset membership. SET then employs a tree structure to compute non-overlapped set operations and integrates interleaved authentication to prevent unauthorized falsification of subset information during forwarding. Randomization is used to further make the exclusive subset and tree formation unpredictable to an adversary. We show the reliability and resilience of SET by analyzing the probability that an adversary may effectively obstruct the set operations. Performance analysis and simulations also demonstrate that the proposed scheme is more efficient than existing schemes from both communication and memory cost standpoints.","Cloning,
Intelligent sensors,
Authentication,
Intelligent networks,
Computer networks,
Tree data structures,
Resilience,
Performance analysis,
Analytical models,
Costs"
Information Discriminant Analysis: Feature Extraction with an Information-Theoretic Objective,"Using elementary information-theoretic tools, we develop a novel technique for linear transformation from the space of observations into a low-dimensional (feature) subspace for the purpose of classification. The technique is based on a numerical optimization of an information-theoretic objective function, which can be computed analytically. The advantages of the proposed method over several other techniques are discussed and the conditions under which the method reduces to linear discriminant analysis are given. We show that the novel objective function enjoys many of the properties of the mutual information and the Bayes error and we give sufficient conditions for the method to be Bayes-optimal. Since the objective function is maximized numerically, we show how the calculations can be accelerated to yield feasible solutions. The performance of the method compares favorably to other linear discriminant-based feature extraction methods on a number of simulated and real-world data sets.","Information analysis,
Feature extraction,
Linear discriminant analysis,
Mutual information,
Data mining,
Vectors,
Principal component analysis,
Sufficient conditions,
Acceleration,
Information theory"
Mode-seeking by Medoidshifts,"We present a nonparametric mode-seeking algorithm, called medoidshift, based on approximating the local gradient using a weighted estimate of medoids. Like meanshift, medoidshift clustering automatically computes the number of clusters and the data does not have to be linearly separable. Unlike meanshift, the proposed algorithm does not require the definition of a mean. This property allows medoidshift to find modes even when only a distance measure between samples is defined. In this sense, the relationship between the medoidshift algorithm and the meanshift algorithm is similar to the relationship between the k-medoids and the k-means algorithms. We show that medoidshifts can also be used for incremental clustering of growing datasets by recycling previous computations. We present experimental results using medoidshift for image segmentation, incremental clustering for shot segmentation and clustering on nonlinearly separable data.","Clustering algorithms,
Image segmentation,
Computer vision,
Application software,
Partitioning algorithms,
Robots,
Computer science,
Lakes,
Robotics and automation,
Recycling"
Two Tier Secure Routing Protocol for Heterogeneous Sensor Networks,"Research on sensor network routing focused on efficiency and effectiveness of data dissemination. Few of them considered security issues during the design time of a routing protocol. Furthermore, previous research on sensor networks mainly considered homogeneous sensor networks where all sensor nodes have the same capabilities. It has been shown that homogeneous ad hoc networks have poor fundamental performance limits and scalability. To achieve better performance, we adopt a heterogeneous sensor network (HSN) model. In this paper, we present a secure and efficient routing protocol for HSNs - two tier secure routing (TTSR). TTSR takes advantage of powerful high-end sensors in an HSN. Our security analysis demonstrates that TTSR can defend typical attacks on sensor routing. Our performance evaluation shows that TTSR has higher delivery ratio, lower end-to-end delay and energy consumption than a popular sensor network routing protocol.","Routing protocols,
National security,
Computer science,
Ad hoc networks,
Manufacturing,
Energy storage,
Data security,
Scalability,
Energy consumption,
Terrorism"
Constructing a Wireless Sensor Network to Fully Cover Critical Grids by Deploying Minimum Sensors on Grid Points Is NP-Complete,This paper proves that deploying sensors on grid points to construct a wireless sensor network that fully covers critical grids using minimum sensors (critical-grid coverage problem) and that fully covers a maximum total weight of grids using a given number of sensors (weighted-grid coverage problem) are each NP-complete,"computational complexity,
geometry,
wireless sensor networks"
Unsupervised Learning of Gaussian Mixtures Based on Variational Component Splitting,"In this paper, we present an incremental method for model selection and learning of Gaussian mixtures based on the recently proposed variational Bayes approach. The method adds components to the mixture using a Bayesian splitting test procedure: a component is split into two components and then variational update equations are applied only to the parameters of the two components. As a result, either both components are retained in the model or one of them is found to be redundant and is eliminated from the model. In our approach, the model selection problem is treated locally, in a region of the data space, so we can set more informative priors based on the local data distribution. A modified Bayesian mixture model is presented to implement this approach, along with a learning algorithm that iteratively applies a splitting test on each mixture component. Experimental results and comparisons with two other techniques testify for the adequacy of the proposed approach","Unsupervised learning,
Testing,
Bayesian methods,
Clustering algorithms,
Covariance matrix,
Computer science education,
Educational programs,
Stability,
Optimization methods,
Equations"
Verification and validation of simulation models,"In this paper we discuss verification and validation of simulation models. Four different approaches to deciding model validity are described; two different paradigms that relate verification and validation to the model development process are presented; various validation techniques are defined; conceptual model validity, model verification, operational validity, and data validity are discussed; a way to document results is given; a recommended procedure for model validation is presented; and model accreditation is briefly discussed.","Accreditation,
Computational modeling,
Random variables,
Testing,
Computer simulation,
Educational institutions,
Computer science,
Decision making,
Application software,
Costs"
Scheduling Data-IntensiveWorkflows onto Storage-Constrained Distributed Resources,"In this paper we examine the issue of optimizing disk usage and of scheduling large-scale scientific workflows onto distributed resources where the workflows are data- intensive, requiring large amounts of data storage, and where the resources have limited storage resources. Our approach is two-fold: we minimize the amount of space a workflow requires during execution by removing data files at runtime when they are no longer required and we schedule the workflows in a way that assures that the amount of data required and generated by the workflow fits onto the individual resources. For a workflow used by gravitational- wave physicists, we were able to improve the amount of storage required by the workflow by up to 57 %. We also designed an algorithm that can not only find feasible solutions for workflow task assignment to resources in disk- space constrained environments, but can also improve the overall workflow performance.","Large-scale systems,
Scheduling,
Engines,
Computer science,
Laboratories,
Marine technology,
Information technology,
Memory,
Runtime,
Algorithm design and analysis"
Modeling View and Posture Manifolds for Tracking,"In this paper we consider modeling data lying on multiple continuous manifolds. In particular, we model the shape manifold of a person performing a motion observed from different view points along a view circle at fixed camera height. We introduce a model that ties together the body configuration (kinematics) manifold and the visual manifold (observations) in a way that facilitates tracking the 3D configuration with continuous relative view variability. The model exploits the low dimensionality nature of both the body configuration manifold and the view manifold where each of them are represented separately.","Cameras,
Biological system modeling,
Humans,
Shape,
Kinematics,
Solid modeling,
Tracking,
Computer science,
Search problems,
Lighting"
Characterizing Driver Behavior on Signalized Intersection Approaches at the Onset of a Yellow-Phase Trigger,"This paper involves a field test on 60 test participants to characterize driver behavior (perception-reaction time (PRT) and stopping/running decisions) at the onset of a yellow phase. Driver behavior is analyzed for five trigger distances that are measured from the vehicle position at the start of the yellow indication to the stop bar. This paper demonstrates that the 1.0-s 85th-percentile PRT that is recommended in traffic-signal-design procedures is valid and consistent with the field observations. Furthermore, this paper clearly shows that brake PRTs are impacted by the vehicle's time to intersection (TTI) at the onset of a yellow-indication introduction. This paper also demonstrates that either a lognormal or a beta distribution is sufficient to model the stochastic nature of the brake PRT. In terms of stopping decisions, this paper demonstrates that the probability of stopping varies from 100% at a TTI of 5.5 s to 9% at a TTI of 1.6 s. This paper also indicates a decrease in the probability of stopping for male drivers when compared with female drivers. Furthermore, this study suggests that drivers 65 years of age and older are significantly less likely to clear the intersection at short yellow-indication trigger distances when compared with other age groups. The dilemma zone for the less than 40 year old group is found to range from 3.9 to 1.85 s, whereas the dilemma zone for the greater than 70 year old group is found to range from 3.2 to 1.5 s.",
Software Development Environments for Scientific and Engineering Software: A Series of Case Studies,"The need for high performance computing applications for computational science and engineering projects is growing rapidly, yet there have been few detailed studies of the software engineering process used for these applications. The DARPA High Productivity Computing Systems Program has sponsored a series of case studies of representative computational science and engineering projects to identify the steps involved in developing such applications (i.e. the life cycle, the workflows, technical challenges, and organizational challenges). Secondary goals were to characterize tool usage and identify enhancements that would increase the programmers' productivity. Finally, these studies were designed to develop a set of lessons learned that can be transferred to the general computational science and engineering community to improve the software engineering process used for their applications. Nine lessons learned from five representative projects are presented, along with their software engineering implications, to provide insight into the software development environments in this domain.",
Matching by Linear Programming and Successive Convexification,"We present a novel convex programming scheme to solve matching problems, focusing on the challenging problem of matching in a large search range and with cluttered background. Matching is formulated as metric labeling with L1 regularization terms, for which we propose a novel linear programming relaxation method and an efficient successive convexification implementation. The unique feature of the proposed relaxation scheme is that a much smaller set of basis labels is used to represent the original label space. This greatly reduces the size of the searching space. A successive convexification scheme solves the labeling problem in a coarse to fine manner. Importantly, the original cost function is reconvexified at each stage, in the new focus region only, and the focus region is updated so as to refine the searching result. This makes the method well-suited for large label set matching. Experiments demonstrate successful applications of the proposed matching scheme in object detection, motion estimation, and tracking","Linear programming,
Labeling,
Motion estimation,
Application software,
Tracking,
Polynomials,
Belief propagation,
Annealing,
Relaxation methods,
Cost function"
Feature-Preserving MRI Denoising: A Nonparametric Empirical Bayes Approach,"This paper presents a novel method for Bayesian denoising of magnetic resonance (MR) images that bootstraps itself by inferring the prior, i.e., the uncorrupted-image statistics, from the corrupted input data and the knowledge of the Rician noise model. The proposed method relies on principles from empirical Bayes (EB) estimation. It models the prior in a nonparametric Markov random field (MRF) framework and estimates this prior by optimizing an information-theoretic metric using the expectation-maximization algorithm. The generality and power of nonparametric modeling, coupled with the EB approach for prior estimation, avoids imposing ill-fitting prior models for denoising. The results demonstrate that, unlike typical denoising methods, the proposed method preserves most of the important features in brain MR images. Furthermore, this paper presents a novel Bayesian-inference algorithm on MRFs, namely iterated conditional entropy reduction (ICER). This paper also extends the application of the proposed method for denoising diffusion-weighted MR images. Validation results and quantitative comparisons with the state of the art in MR-image denoising clearly depict the advantages of the proposed method.","Magnetic resonance imaging,
Noise reduction,
Bayesian methods,
Magnetic resonance,
Statistics,
Rician channels,
Magnetic noise,
Markov random fields,
Expectation-maximization algorithms,
Entropy"
Locality-Aware and Churn-Resilient Load-Balancing Algorithms in Structured Peer-to-Peer Networks,"Structured peer-to-peer overlay networks, like distributed hash tables (DHTs), map data items to the network based on a consistent hashing function. Such mapping for data distribution has an inherent load balance problem. Data redistribution algorithms based on randomized matching of heavily loaded nodes with light ones can deal with the dynamics of DHTs. However, they are unable to consider the proximity of the nodes simultaneously. There are other methods that rely on auxiliary networks to facilitate locality-aware load redistribution. Due to the cost of network construction and maintenance, the locality-aware algorithms can hardly work for DHTs with churn. This paper presents a locality-aware randomized load-balancing algorithm to deal with both the proximity and network churn at the same time. We introduce a factor of randomness in the probing of lightly loaded nodes in a range of proximity. We further improve the efficiency by allowing the probing of multiple candidates (d-way) at a time. Simulation results show the superiority of the locality-aware two-way randomized algorithm in comparison with other random or locality-aware algorithms. In DHTs with churn, it performs no worse than the best chum-resilient algorithm. It takes advantage of node capacity heterogeneity and achieves good load balance effectively even in a skewed distribution of items","Peer to peer computing,
Load management,
Costs,
Computer Society,
Resource management,
Intrusion detection,
Robustness"
Estimating 3-D Respiratory Motion From Orbiting Views by Tomographic Image Registration,"Respiratory motion remains a significant source of errors in treatment planning for the thorax and upper abdomen. Recently, we proposed a method to estimate two-dimensional (2-D) object motion from a sequence of slowly rotating X-ray projection views, which we called deformation from orbiting views (DOVs). In this method, we model the motion as a time varying deformation of a static prior of the anatomy. We then optimize the parameters of the motion model by maximizing the similarity between the modeled and actual projection views. This paper extends the method to full three-dimensional (3-D) motion and cone-beam projection views. We address several practical issues for using a cone-beam computed tomography (CBCT) scanner that is integrated in a radiotherapy system, such as the effects of Compton scatter and the limited gantry rotation for one breathing cycle. We also present simulation and phantom results to illustrate the performance of this method","Motion estimation,
Image registration,
Thorax,
Abdomen,
Two dimensional displays,
X-ray imaging,
Deformable models,
Anatomy,
Computed tomography,
X-ray scattering"
Dynamic Ethernet Link Shutdown for Energy Conservation on Ethernet Links,"Recent studies of network traffic utilization on campus networks have shown that data networks remain heavily under-utilized. Yet currently there is little attempt to save energy on network interfaces by using the low power modes available in Ethernet transceivers during periods of inactivity or low utilization. In this paper we design and evaluate a dynamic ethernet link shutdown (DELS) algorithm that utilizes current technology leading to significant benefits in energy savings with little noticeable impact on packet loss or delay. The algorithm uses buffer occupancy, the behavior of previous packet arrival times and a configurable maximum bounded delay to make sleeping decisions. The scheme was evaluated using simulations with inputs generated using a synthetic traffic generator for smooth and bursty traffic. The results show that the percentage of total time that a link can be shut down can be anywhere from 80% to 60% for traffic loads up to 5%.","Ethernet networks,
Energy conservation,
Telecommunication traffic,
Switches,
Delay,
Computer science,
Transceivers,
Traffic control,
Batteries,
Protocols"
Analysis of Retinal Vasculature Using a Multiresolution Hermite Model,"This paper presents a vascular representation and segmentation algorithm based on a multiresolution Hermite model (MHM). A two-dimensional Hermite function intensity model is developed which models blood vessel profiles in a quad-tree structure over a range of spatial resolutions. The use of a multiresolution representation simplifies the image modeling and allows for a robust analysis by combining information across scales. Estimation over scale also reduces the overall computational complexity. As well as using MHM for vessel labelling, the local image modeling can accurately represent vessel directions, widths, amplitudes, and branch points which readily enable the global topology to be inferred. An expectation-maximization (EM) type of optimization scheme is used to estimate local model parameters and an information theoretic test is then applied to select the most appropriate scale/feature model for each region of the image. In the final stage, Bayesian stochastic inference is employed for linking the local features to obtain a description of the global vascular structure. After a detailed description and analysis of MHM, experimental results on two standard retinal databases are given that demonstrate its comparative performance. These show MHM to perform comparably with other retinal vessel labelling methods","Retina,
Spatial resolution,
Labeling,
Image segmentation,
Blood vessels,
Biomedical imaging,
Image resolution,
Robustness,
Image analysis,
Information analysis"
Emotion Classification Using Web Blog Corpora,"In this paper, we investigate the emotion classification of web blog corpora using support vector machine (SVM) and conditional random field (CRF) machine learning techniques. The emotion classifiers are trained at the sentence level and applied to the document level. Our methods also determine an emotion category by taking the context of a sentence into account. Experiments show that CRF classifiers outperform SVM classifiers. When applying emotion classification to a blog at the document level, the emotion of the last sentence in a document plays an important role in determining the overall emotion.",
Integrated Hybrid-Simulation of Electric Power and Communications Systems,"The modern power grid is strongly integrated with its communication network. While a power system primarily consists of elements that are modeled by continuous equations, a communication system has discrete event dynamics. We model the integrated operation of these two systems with a hybrid modeling and simulation technique. Systematically combining continuous and discrete event system models is necessary for correctly simulating critical system behaviors. This paper discusses an approach based on the discrete event system specification (DEVS) that characterizes the interaction of the two systems formally to preserve simulation correctness. We demonstrate the implementation of our integrated hybrid simulation technique with detailed generator and network models in a wide-area cooperative automatic load-control scenario.","Power system modeling,
Hybrid power systems,
Power system dynamics,
Power system simulation,
Discrete event systems,
Discrete event simulation,
Power systems,
Power grids,
Communication networks,
Equations"
A Hierarchical Algorithm for MR Brain Image Parcellation,"We introduce an algorithm for segmenting brain magnetic resonance (MR) images into anatomical compartments such as the major tissue classes and neuro-anatomical structures of the gray matter. The algorithm is guided by prior information represented within a tree structure. The tree mirrors the hierarchy of anatomical structures and the subtrees correspond to limited segmentation problems. The solution to each problem is estimated via a conventional classifier. Our algorithm can be adapted to a wide range of segmentation problems by modifying the tree structure or replacing the classifier. We evaluate the performance of our new segmentation approach by revisiting a previously published statistical group comparison between first-episode schizophrenia patients, first-episode affective psychosis patients, and comparison subjects. The original study is based on 50 MR volumes in which an expert identified the brain tissue classes as well as the superior temporal gyrus, amygdala, and hippocampus. We generate analogous segmentations using our new method and repeat the statistical group comparison. The results of our analysis are similar to the original findings, except for one structure (the left superior temporal gyrus) in which a trend-level statistical significance (p = 0.07) was observed instead of statistical significance.","Brain,
Biomedical imaging,
Laboratories,
Hospitals,
Neuroscience,
Image segmentation,
Surgery,
Computer science,
Artificial intelligence,
Tree data structures"
Localized Outlying and Boundary Data Detection in Sensor Networks,"This paper targets the identification of outlying sensors (that is, outlying reading sensors) and the detection of the reach of events in sensor networks. Typical applications include the detection of the transportation front line of some vegetation or animalcule's growth over a certain geographical region. We propose and analyze two novel algorithms for outlying sensor identification and event boundary detection. These algorithms are purely localized and, thus, scale well to large sensor networks. Their computational overhead is low, since only simple numerical operations are involved. Simulation results indicate that these algorithms can clearly detect the event boundary and can identify outlying sensors with a high accuracy and a low false alarm rate when as many as 20 percent sensors report outlying readings. Our work is exploratory in that the proposed algorithms can accept any kind of scalar values as inputs-a dramatic improvement over existing work, which takes only 0/1 decision predicates. Therefore, our algorithms are generic. They can be applied as long as ""events"" can be modeled by numerical numbers. Though designed for sensor networks, our algorithms can be applied to the outlier detection and regional data analysis in spatial data mining.","Sensor phenomena and characterization,
Event detection,
Algorithm design and analysis,
Vegetation,
Face detection,
Transportation,
Computational modeling,
Discrete event simulation,
Biological system modeling,
Numerical models"
Distributed Scrum: Agile Project Management with Outsourced Development Teams,"Agile project management with Scrum derives from best business practices in companies like Fuji-Xerox, Honda, Canon, and Toyota. Toyota routinely achieves four times the productivity and 12 times the quality of competitors. Can Scrum do the same for globally distributed teams? Two agile companies, SirsiDynix and StarSoft development laboratories achieved comparable performance developing a Java application with over 1,000,000 lines of code. During 2005, a distributed team of 56 Scrum developers working from Provo, Utah; Waterloo, Canada; and St. Petersburg, Russia, delivered 671,688 lines of production Java code. At 15.3 function points per developer/month, this is the most productive Java project ever documented. SirsiDynix best practices are similar to those observed on distributed Scrum teams at IDX Systems, radically different than those promoted by PMBOK, and counterintuitive to practices advocated by the Scrum Alliance. This paper analyzes and recommends best practices for globally distributed agile teams","Project management,
Programming,
Object oriented modeling,
Java,
Computer architecture,
Uncertainty,
Companies,
Productivity,
Best practices,
Software systems"
An Adaptive Shared/Private NUCA Cache Partitioning Scheme for Chip Multiprocessors,"The significant speed-gap between processor and memory and the limited chip memory bandwidth make last-level cache performance crucial for future chip multiprocessors. To use the capacity of shared last-level caches efficiently and to allow for a short access time, proposed non-uniform cache architectures (NUCAs) are organized into per-core partitions. If a core runs out of cache space, blocks are typically relocated to nearby partitions, thus managing the cache as a shared cache. This uncontrolled sharing of all resources may unfortunately result in pollution that degrades performance. We propose a novel non-uniform cache architecture in which the amount of cache space that can be shared among the cores is controlled dynamically. The adaptive scheme estimates, continuously, the effect of increasing/decreasing the shared partition size on the overall performance. We show that our scheme outperforms a private and shared cache organization as well as a hybrid NUCA organization in which blocks in a local partition can spill over to neighbor core partitions","Pollution,
Bandwidth,
Delay,
Degradation,
Microprocessors,
Size control"
Practical Scheduling of Bag-of-Tasks Applications on Grids with Dynamic Resilience,"Over the past decade, the grid has emerged as an attractive platform to tackle various large-scale problems, especially in science and engineering. One primary issue associated with the efficient and effective utilization of heterogeneous resources in a grid is scheduling. Grid scheduling involves a number of challenging issues, mainly due to the dynamic nature of the grid. There are only a handful of scheduling schemes for grid environments that realistically deal with this dynamic nature that have been proposed in the literature. In this paper, two novel scheduling algorithms, called the shared-input-data-based listing (SIL) algorithm and the multiple queues with duplication (MQD) algorithm for bag-of-tasks (BoT) applications in grid environments are proposed. The SIL algorithm targets scheduling data-intensive BoT (DBoT) applications, whereas the MQD algorithm deals with scheduling computationally intensive BoT (CBoT) applications. Their common and primary forte is that they make scheduling decisions without fully accurate performance prediction information. Another point to note is that both scheduling algorithms adopt task duplication as an attempt to reduce serious schedule increases. Our evaluation study employs a number of experiments with various simulation settings. The results show the practicability and competitiveness of our algorithms when compared to existing methods","scheduling,
grid computing"
Real-Time Vessel Segmentation and Tracking for Ultrasound Imaging Applications,"A method for vessel segmentation and tracking in ultrasound images using Kalman filters is presented. A modified Star-Kalman algorithm is used to determine vessel contours and ellipse parameters using an extended Kalman filter with an elliptical model. The parameters can be used to easily calculate the transverse vessel area which is of clinical use. A temporal Kalman filter is used for tracking the vessel center over several frames, using location measurements from a handheld sensorized ultrasound probe. The segmentation and tracking have been implemented in real-time and validated using simulated ultrasound data with known features and real data, for which expert segmentation was performed. Results indicate that mean errors between segmented contours and expert tracings are on the order of 1%-2% of the maximum feature dimension, and that the transverse cross-sectional vessel area as computed from estimated ellipse parameters a, b as determined by our algorithm is within 10% of that determined by experts. The location of the vessel center was tracked accurately for a range of speeds from 1.4 to 11.2 mm/s.","Ultrasonic imaging,
Image segmentation,
Ultrasonic variables measurement,
Biomedical imaging,
Medical diagnostic imaging,
Veins,
Standardization,
Probes,
Computational modeling,
Parameter estimation"
Face Recognition by Regularized Discriminant Analysis,"When the feature dimension is larger than the number of samples the small sample-size problem occurs. There is great concern about it within the face recognition community. We point out that optimizing the Fisher index in linear discriminant analysis does not necessarily give the best performance for a face recognition system. We propose a new regularization scheme. The proposed method is evaluated using the Olivetti research laboratory database, the Yale database, and the Feret database.",
Rayleigh-Maximum-Likelihood Filtering for Speckle Reduction of Ultrasound Images,"Speckle is a multiplicative noise that degrades ultrasound images. Recent advancements in ultrasound instrumentation and portable ultrasound devices necessitate the need for more robust despeckling techniques, for both routine clinical practice and teleconsultation. Methods previously proposed for speckle reduction suffer from two major limitations: 1) noise attenuation is not sufficient, especially in the smooth and background areas; 2) existing methods do not sufficiently preserve or enhance edges-they only inhibit smoothing near edges. In this paper, we propose a novel technique that is capable of reducing the speckle more effectively than previous methods and jointly enhancing the edge information, rather than just inhibiting smoothing. The proposed method utilizes the Rayleigh distribution to model the speckle and adopts the robust maximum-likelihood estimation approach. The resulting estimator is statistically analyzed through first and second moment derivations. A tuning parameter that naturally evolves in the estimation equation is analyzed, and an adaptive method utilizing the instantaneous coefficient of variation is proposed to adjust this parameter. To further tailor performance, a weighted version of the proposed estimator is introduced to exploit varying statistics of input samples. Finally, the proposed method is evaluated and compared to well-accepted methods through simulations utilizing synthetic and real ultrasound data","Filtering,
Speckle,
Ultrasonic imaging,
Smoothing methods,
Degradation,
Instruments,
Noise robustness,
Noise reduction,
Background noise,
Attenuation"
Interactive Tensor Field Design and Visualization on Surfaces,"Designing tensor fields in the plane and on surfaces is a necessary task in many graphics applications, such as painterly rendering, pen-and-ink sketching of smooth surfaces, and anisotropic remeshing. In this article, we present an interactive design system that allows a user to create a wide variety of symmetric tensor fields over 3D surfaces either from scratch or by modifying a meaningful input tensor field such as the curvature tensor. Our system converts each user specification into a basis tensor field and combines them with the input field to make an initial tensor field. However, such a field often contains unwanted degenerate points which cannot always be eliminated due to topological constraints of the underlying surface. To reduce the artifacts caused by these degenerate points, our system allows the user to move a degenerate point or to cancel a pair of degenerate points that have opposite tensor indices. These operations provide control over the number and location of the degenerate points in the field. We observe that a tensor field can be locally converted into a vector field so that there is a one-to-one correspondence between the set of degenerate points in the tensor field and the set of singularities in the vector field. This conversion allows us to effectively perform degenerate point pair cancellation and movement by using similar operations for vector fields. In addition, we adapt the image-based flow visualization technique to tensor fields, therefore allowing interactive display of tensor fields on surfaces. We demonstrate the capabilities of our tensor field design system with painterly rendering, pen-and-ink sketching of surfaces, and anisotropic remeshing",
Diversity and Multiplexing Tradeoff in General Fading Channels,"The optimal tradeoff between diversity gain and multiplexing gain for multiple-inputmultiple-output (MIMO) channels has been studied recently under the independent and identically distributed (i.i.d.) Rayleigh-fading assumption. In this correspondence, this result is extended and the optimal tradeoff performance is derived for generalized fading channel conditions, including different fading types, nonidentical fading distributions, spatial correlation, and nonzero channel means. Our results include many known models as special cases and shed light on the effects of different channel parameters on the optimal tradeoff performance","Fading,
Information rates,
Steganography,
Conferences,
Cryptography,
Lattices,
Computational complexity,
Data mining,
Gray-scale,
Computer science"
Cortical Surface Shape Analysis Based on Spherical Wavelets,"In vivo quantification of neuroanatomical shape variations is possible due to recent advances in medical imaging and has proven useful in the study of neuropathology and neurodevelopment. In this paper, we apply a spherical wavelet transformation to extract shape features of cortical surfaces reconstructed from magnetic resonance images (MRIs) of a set of subjects. The spherical wavelet transformation can characterize the underlying functions in a local fashion in both space and frequency, in contrast to spherical harmonics that have a global basis set. We perform principal component analysis (PCA) on these wavelet shape features to study patterns of shape variation within normal population from coarse to fine resolution. In addition, we study the development of cortical folding in newborns using the Gompertz model in the wavelet domain, which allows us to characterize the order of development of large-scale and finer folding patterns independently. Given a limited amount of training data, we use a regularization framework to estimate the parameters of the Gompertz model to improve the prediction performance on new data. We develop an efficient method to estimate this regularized Gompertz model based on the Broyden-Fletcher-Goldfarb-Shannon (BFGS) approximation. Promising results are presented using both PCA and the folding development model in the wavelet domain. The cortical folding development model provides quantitative anatomic information regarding macroscopic cortical folding development and may be of potential use as a biomarker for early diagnosis of neurologic deficits in newborns","Surface waves,
Shape,
Wavelet analysis,
Principal component analysis,
Surface reconstruction,
Pediatrics,
Wavelet domain,
Magnetic analysis,
In vivo,
Biomedical imaging"
Maximizing Lifetime of Sensor Surveillance Systems,"This paper addresses the maximal lifetime scheduling problem in sensor surveillance systems. Given a set of sensors and targets in an area, a sensor can watch only one target at a time, our task is to schedule sensors to watch targets and forward the sensed data to the base station, such that the lifetime of the surveillance system is maximized, where the lifetime is the duration that all targets are watched and all active sensors are connected to the base station. We propose an optimal solution to find the target-watching schedule for sensors that achieves the maximal lifetime. Our solution consists of three steps: 1) computing the maximal lifetime of the surveillance system and a workload matrix by using the linear programming technique; 2) decomposing the workload matrix into a sequence of schedule matrices that can achieve the maximal lifetime; and 3) determining the sensor surveillance trees based on the above obtained schedule matrices, which specify the active sensors and the routes to pass sensed data to the base station. This is the first time in the literature that the problem of maximizing lifetime of sensor surveillance systems has been formulated and the optimal solution has been found","Sensor systems,
Surveillance,
Watches,
Base stations,
Processor scheduling,
Matrix decomposition,
Monitoring,
Computer science,
Linear programming,
Collaboration"
K-Means+ID3: A Novel Method for Supervised Anomaly Detection by Cascading K-Means Clustering and ID3 Decision Tree Learning Methods,"In this paper, we present ""k-means+ID3"", a method to cascade k-means clustering and the ID3 decision tree learning methods for classifying anomalous and normal activities in a computer network, an active electronic circuit, and a mechanical mass-beam system. The k-means clustering method first partitions the training instances into k clusters using Euclidean distance similarity. On each cluster, representing a density region of normal or anomaly instances, we build an ID3 decision tree. The decision tree on each cluster refines the decision boundaries by learning the subgroups within the cluster. To obtain a final decision on classification, the decisions of the k-means and ID3 methods are combined using two rules: 1) the nearest-neighbor rule and 2) the nearest-consensus rule. We perform experiments on three data sets: 1) network anomaly data (NAD), 2) Duffing equation data (DED), and 3) mechanical system data (MSD), which contain measurements from three distinct application domains of computer networks, an electronic circuit implementing a forced Duffing equation, and a mechanical system, respectively. Results show that the detection accuracy of the k-means+ID3 method is as high as 96.24 percent at a false-positive-rate of 0.03 percent on NAD; the total accuracy is as high as 80.01 percent on MSD and 79.9 percent on DED","Decision trees,
Learning systems,
Computer networks,
Electronic circuits,
Equations,
Mechanical systems,
Classification tree analysis,
Clustering methods,
Euclidean distance,
Performance evaluation"
Visual Assessment of Clustering Tendency for Rectangular Dissimilarity Matrices,"We have an m times n matrix D, and assume that its entries correspond to pair wise dissimilarities between m row objects Or and n column objects Oc, which, taken together (as a union), comprise a set O of N = m + n objects. This paper develops a new visual approach that applies to four different cluster assessment problems associated with O. The problems are the assessment of cluster tendency: PI) amongst the row objects Or; P2) amongst the column objects Oc; P3) amongst the union of the row and column objects Or U Oc; and P4) amongst the union of the row and column objects that contain at least one object of each type (co-clusters). The basis of the method is to regard D as a subset of known values that is part of a larger, unknown N times N dissimilarity matrix, and then impute the missing values from D. This results in estimates for three square matrices (Dr, Dc, DrUc) that can be visually assessed for clustering tendency using the previous VAT or sVAT algorithms. The output from assessment of DrUc ultimately leads to a rectangular coVAT image which exhibits clustering tendencies in D. Five examples are given to illustrate the new method. Two important points: i) because VAT is scalable by sVAT to data sets of arbitrary size, and because coVAT depends explicitly (and only) on VAT, this new approach is immediately scalable to, say, the scoVAT model, which works for even very large (unloadable) data sets without alteration; and ii) VAT, sVAT and coVAT are autonomous, parameter free models - no ""hidden values"" are needed to make them work.","Frequency estimation,
Clustering algorithms,
Data visualization,
Length measurement,
Taxonomy,
Bioinformatics,
Computer science,
Fuzzy logic,
Data analysis,
Gene expression"
An Identity-Free and On-Demand Routing Scheme against Anonymity Threats in Mobile Ad Hoc Networks,Introducing node mobility into the network also introduces new anonymity threats. This important change of the concept of anonymity has recently attracted attentions in mobile wireless security research. This paper presents identity-free routing and on- demand routing as two design principles of anonymous routing in mobile ad hoc networks. We devise ANODR (ANonymous On-Demand Routing) as the needed anonymous routing scheme that is compliant with the design principles. Our security analysis and simulation study verify the effectiveness and efficiency of ANODR.,"Routing,
Mobile ad hoc networks,
Privacy,
Reconnaissance,
Wireless communication,
Unmanned aerial vehicles,
Mobile communication,
Protection,
Communication system security,
Ad hoc networks"
Differential evolution for high-dimensional function optimization,"Most reported studies on differential evolution (DE) are obtained using low-dimensional problems, e.g., smaller than 100, which are relatively small for many real-world problems. In this paper we propose two new efficient DE variants, named DECC-I and DECC-II, for high-dimensional optimization (up to 1000 dimensions). The two algorithms are based on a cooperative coevolution framework incorporated with several novel strategies. The new strategies are mainly focus on problem decomposition and subcomponents cooperation. Experimental results have shown that these algorithms have superior performance on a set of widely used benchmark functions.","Genetic mutations,
Genetic programming,
Computer science,
Chromium,
Scalability,
Evolutionary computation,
Computer applications,
Application software,
Computer architecture,
Genetic algorithms"
LRED: A Robust and Responsive AQM Algorithm Using Packet Loss Ratio Measurement,"Active queue management (AQM) is an effective means to enhance congestion control, and to achieve trade-off between link utilization and delay. The de facto standard, random early detection (RED), and many of its variants employ queue length as a congestion indicator to trigger packet dropping. Despite their simplicity, these approaches often suffer from unstable behaviors in a dynamic network. Adaptive parameter settings, though might solve the problem, remain difficult in such a complex system. Recent proposals based on analytical TCP control and AQM models suggest the use of both queue length and traffic input rate as congestion indicators, which effectively enhances stability. Their response time generally increases however, leading to frequent buffer overflow and emptiness. In this paper, we propose a novel AQM algorithm that achieves fast response time and yet good robustness. The algorithm, called Loss Ratio-based RED (LRED), measures the latest packet loss ratio, and uses it as a complement to queue length for adaptively adjusting the packet drop probability. We develop an analytical model for LRED, which demonstrates that LRED is responsive even if the number of TCP flows and their persisting times vary significantly. It also provides a general guideline for the parameter settings in LRED. The performance of LRED is further examined under various simulated network environments, and compared to existing AQM algorithms. Our simulation results show that, with comparable complexities, LRED achieves shorter response time and higher robustness. More importantly, it trades off the goodput with queue length better than existing algorithms, enabling flexible system configurations","Robustness,
Loss measurement,
Delay effects,
Adaptive systems,
Proposals,
Queueing analysis,
Communication system traffic control,
Traffic control,
Stability analysis,
Buffer overflow"
Effective Fault Localization using Code Coverage,"Localizing a bug in a program can be a complex and time- consuming process. In this paper we propose a code coverage-based fault localization method to prioritize suspicious code in terms of its likelihood of containing program bugs. Code with a higher risk should be examined before that with a lower risk, as the former is more suspicious (i.e., more likely to contain program bugs) than the latter. We also answer a very important question: how can each additional test case that executes the program successfully help locate program bugs? We propose that with respect to a piece of code, the aid introduced by the first successful test that executes it in computing its likelihood of containing a bug is larger than or equal to that of the second successful test that executes it, which is larger than or equal to that of the third successful test that executes it, etc. A tool, chiDebug, was implemented to automate the computation of the risk of the code and the subsequent prioritization of suspicious code for locating program bugs. A case study using the Siemens suite was also conducted. Data collected from our study support the proposal described above. They also indicate that our method (in particular Heuristics III (c), (d), and (e)) can effectively reduce the search domain for locating program bugs.","Computer bugs,
Proposals,
Computer science,
Automatic testing,
Data visualization,
Software testing,
Software debugging,
Software design,
Java,
Application software"
Android as a telecommunication medium with a human-like presence,"In this research, we realize human telepresence by developing a remote-controlled android system called Geminoid HI-1. Experimental results confirm that participants felt stronger presence of the operator when he talked through the android than when he appeared on a video monitor in a video conference system. In addition, participants talked with the robot naturally and evaluated its human likeness as equal to a man on a video monitor. At this paper's conclusion, we will discuss a remote-control system for telepresence that uses a human-like android robot as a new telecommunication medium.","Abstracts,
Communications technology,
Teleoperators,
Synchronization,
Cameras"
Visual Interactive Systems for End-User Development: A Model-Based Design Methodology,"This paper is about the development of systems whose end users are professional people working in a specific domain (e.g., medicine, geology, mechanical engineering); they are expert in that domain, but not necessarily expert in nor even conversant with computer science. In several work organizations, end users need to tailor their software systems to better adapt them to their requirements and even to create or modify software artifacts. These are end-user development activities and are the focus of this paper. A model of the interaction between users and systems, which also takes into account their reciprocal coevolution during system usage, is discussed. This model is used to define a methodology aimed at designing software environments that allow end users to become designers of their own tools. The methodology is illustrated by discussing two experimental cases.","Interactive systems,
Design methodology,
Geology,
Software systems,
Humans,
Computer science,
Software tools,
Mechanical engineering,
Software design,
User centered design"
Time Series Forecasting for Dynamic Environments: The DyFor Genetic Program Model,"Several studies have applied genetic programming (GP) to the task of forecasting with favorable results. However, these studies, like those applying other techniques, have assumed a static environment, making them unsuitable for many real-world time series which are generated by varying processes. This study investigates the development of a new ldquodynamicrdquo GP model that is specifically tailored for forecasting in nonstatic environments. This dynamic forecasting genetic program (DyFor GP) model incorporates features that allow it to adapt to changing environments automatically as well as retain knowledge learned from previously encountered environments. The DyFor GP model is tested for forecasting efficacy on both simulated and actual time series including the U.S. Gross Domestic Product and Consumer Price Index Inflation. Results show that the performance of the DyFor GP model improves upon that of benchmark models for all experiments. These findings highlight the DyFor GP's potential as an adaptive, nonlinear model for real-world forecasting applications and suggest further investigations.",
A Cognitive Model of Improvisation in Emergency Management,"An enduring characteristic of emergencies is the need for near-simultaneous development and deployment of new management procedures. This need can arise with the onset of highly novel problems and the need to act quickly-factors that reduce opportunities for extensive planning in managing the emergency. As a result, decision makers in emergencies must be prepared to improvise. By understanding the cognitive processes in improvisation, organizations can better learn how to plan for, manage, and learn from improvised action. To help create this understanding, this paper reviews and synthesizes prior results on improvisation in the art of jazz, exploring how these results may be applied to improvisation in emergency management. A theory of improvisation in emergency management is then developed and expressed as a cognitive model. The modelpsilas implementation in computer-executable code is then reviewed, along with an illustration of how the model improvises in an emergency situation. Finally, implications of this model and opportunities for future research are presented.","Disaster management,
Art,
Time factors,
Risk management,
Decision making,
Technology planning,
Terrorism,
Telecommunication services,
Cities and towns,
Information systems"
Learning 3-D Scene Structure from a Single Still Image,"We consider the problem of estimating detailed 3D structure from a single still image of an unstructured environment. Our goal is to create 3D models which are both quantitatively accurate as well as visually pleasing. For each small homogeneous patch in the image, we use a Markov random field (MRF) to infer a set of ""plane parameters"" that capture both the 3D location and 3D orientation of the patch. The MRF, trained via supervised learning, models both image depth cues as well as the relationships between different parts of the image. Inference in our model is tractable, and requires only solving a convex optimization problem. Other than assuming that the environment is made up of a number of small planes, our model makes no explicit assumptions about the structure of the scene; this enables the algorithm to capture much more detailed 3D structure than does prior art (such as Saxena et ah, 2005, Delage et ah, 2005, and Hoiem et el, 2005), and also give a much richer experience in the 3D flythroughs created using image-based rendering, even for scenes with significant non-vertical structure. Using this approach, we have created qualitatively correct 3D models for 64.9% of 588 images downloaded from the Internet, as compared to Hoiem et al.'s performance of 33.1%. Further, our models are quantitatively more accurate than either Saxena et al. or Hoiem et al.","Layout,
Markov random fields,
Supervised learning,
Sun,
Computer science,
Inference algorithms,
Art,
Rendering (computer graphics),
Internet,
Humans"
Configurable Proof Obligations in the Frog Toolkit,"In model based formal methods, incompatible tools for different techniques is the norm. However, greater applicability to industrial scale systems increasingly requires combining the strengths of different techniques, in line with the verification grand challenge. The Frog tool embodies a construct-based specification syntax, and its meta-language Frog-CCL allows the generic configuration of both a constructs syntax and its proof obligations. For a specific system, Frog generates the system's verification conditions mechanically from the generic ones. Relationships between systems such as refinement and retrenchment can be configured. An example retrenchment between two simple systems illustrates the technique.",
Consistent Temporal Variations in Many Outdoor Scenes,"This paper details an empirical study of large image sets taken by static cameras. These images have consistent correlations over the entire image and over time scales of days to months. Simple second-order statistics of such image sets show vastly more structure than exists in generic natural images or video from moving cameras. Using a slight variant to PCA, we can decompose all cameras into comparable components and annotate images with respect to surface orientation, weather, and seasonal change. Experiments are based on a data set from 538 cameras across the United States which have collected more than 17 million images over the the last 6 months.","Layout,
Cameras,
Principal component analysis,
Higher order statistics,
Independent component analysis,
Jacobian matrices,
Video sharing,
Color,
Computer science,
Statistical distributions"
Information Sharing as a Coordination Mechanism for Reducing the Bullwhip Effect in a Supply Chain,"The bullwhip effect is an amplification of the variability of the orders placed by companies in a supply chain. This variability reduces the efficiency of supply chains, since it incurs costs due to higher inventory levels and supply chain agility reduction. Eliminating the bullwhip effect is surely simple; every company just has to order following the market demand, i.e., each company should use a lot-for-lot type of ordering policy. However, many reasons, such as inventory management, lot-sizing, and market, supply, or operation uncertainties, motivate companies not to use this strategy. Therefore, the bullwhip effect cannot be totally eliminated. However, it can be reduced by information sharing, which is the form of collaboration considered in this paper. More precisely, we study how to separate demand into original demand and adjustments. We describe two principles explaining how to use the shared information to reduce the amplification of order variability induced by lead times, which we propose as a cause of the effect. Simulations confirm the value of these two principles with regard to costs and customer service levels","Supply chains,
Costs,
Computer science,
Cities and towns,
Customer service,
Multiagent systems,
Supply chain management,
Fluctuations,
Software engineering,
Inventory management"
"High-frequency Acoustic Recording Package (HARP) for broad-band, long-term marine mammal monitoring","Advancements in low-power and high-data-capacity consumer computer technology during the past decade have been adapted to autonomously record sounds from marine mammals over long periods. Acoustic monitoring has advantages over traditional visual surveys including greater detection ranges, continuous long-term monitoring in remote locations under various weather conditions and independent of daylight, and lower cost. However, until recently, the technology required to autonomously record whale sounds over long durations has been limited to low-frequency (< 1000 Hz) baleen whales. The need for a broader-band, higher-data capacity system capable of autonomously recording toothed whales and other marine mammals for long periods has prompted the development of a High-frequency acoustic recording package (HARP) capable of sample rates up to 200 kHz. Currently, HARPs accumulate data at a rate of almost 2 TB per instrument deployment which creates challenges for processing these large data sets. One method we employ to address some of these challenges is a spectral averaging algorithm in which the data are compressed and viewed as long duration spectrograms. These spectrograms provide the ability to view large amounts of data quickly for events of interest, and they provide a link for quickly accessing the short time-scale data for more detailed analysis. HARPs are currently in use worldwide to acoustically monitor marine mammals for behavioral and ecological long-term studies. The HARP design is described and data analysis strategies along with software tools are discussed using examples of broad-band recorded data.","Packaging,
Computerized monitoring,
Remote monitoring,
Whales,
Marine technology,
Condition monitoring,
Spectrogram,
Acoustic signal detection,
Costs,
Instruments"
Density-Induced Support Vector Data Description,"The purpose of data description is to give a compact description of the target data that represents most of its characteristics. In a support vector data description (SVDD), the compact description of target data is given in a hyperspherical model, which is determined by a small portion of data called support vectors. Despite the usefulness of the conventional SVDD, however, it may not identify the optimal solution of target description especially when the support vectors do not have the overall characteristics of the target data. To address the issue in SVDD methodology, we propose a new SVDD by introducing new distance measurements based on the notion of a relative density degree for each data point in order to reflect the distribution of a given data set. Moreover, for a real application, we extend the proposed method for the protein localization prediction problem which is a multiclass and multilabel problem. Experiments with various real data sets show promising results","Object detection,
Distance measurement,
Proteins,
Clustering methods,
Support vector machines,
Support vector machine classification,
Laboratories,
Systems biology,
Computer science,
Training data"
SA-REST and (S)mashups : Adding Semantics to RESTful Services,"The evolution of the Web 2.0 phenomenon has led to the increased adoption of the RESTful services paradigm. RESTful services often take the form of RSS/Atom feeds and AJAX based light weight services. The XML based messaging paradigm of RESTful services has made it possible to compose various services together. Such compositions of RESTful services is widely referred to as Mashups. In this paper, we outline the limitations in current approaches to creating mashups. We address these limitations by proposing a framework called as SA-REST. SA-REST adds semantics to RESTful services. Our proposed framework builds upon the original ideas in WSDL-S, our W3C submission, which was subsequently adapted for Semantic Annotation of WSDL (SAWSDL), now a W3C proposed recommendation. We demonstrate use of microformats for semantic annotation of RESTful services and then the use of such semanti- cally enabled services with better support for interoperability for creating dynamic mashups called SMashups.","Mashups,
Computer science,
Feeds,
XML,
Web services,
Information filtering,
Information filters,
Standards development,
Mediation,
Scalability"
Content-Based Image Annotation Refinement,"Automatic image annotation has been an active research topic due to its great importance in image retrieval and management. However, results of the state-of-the-art image annotation methods are often unsatisfactory. Despite continuous efforts in inventing new annotation algorithms, it would be advantageous to develop a dedicated approach that could refine imprecise annotations. In this paper, a novel approach to automatically refining the original annotations of images is proposed. For a query image, an existing image annotation method is first employed to obtain a set of candidate annotations. Then, the candidate annotations are re-ranked and only the top ones are reserved as the final annotations. By formulating the annotation refinement process as a Markov process and defining the candidate annotations as the states of a Markov chain, a content-based image annotation refinement (CIAR) algorithm is proposed to re-rank the candidate annotations. It leverages both corpus information and the content feature of a query image. Experimental results on a typical Corel dataset show not only the validity of the refinement, but also the superiority of the proposed algorithm over existing ones.","Snow,
Hidden Markov models,
Noise figure,
Asia,
Image retrieval,
Content based retrieval,
Content management,
Support vector machines,
Support vector machine classification,
Linear discriminant analysis"
Referenceless MR Thermometry for Monitoring Thermal Ablation in the Prostate,"Referenceless proton resonance frequency (PRF) shift thermometry provides a means to measure temperature changes during minimally invasive thermotherapy that is inherently robust to motion and tissue displacement. However, if the referenceless method is used to determine temperature changes during prostate ablation, phase gaps between water and fat in image regions used to determine the background phase can confound the phase estimation. We demonstrate an extension to referenceless thermometry which eliminates this problem by allowing background phase estimation in the presence of phase discontinuities between aqueous and fatty tissue. In this method, images are acquired with a multi-echo sequence and binary water and fat maps are generated from a Dixon reconstruction. For the background phase estimation, water and fat regions are treated separately and the phase offset between the two tissue types is determined. The method is demonstrated feasibile in phantoms and during in vivo thermal ablation of canine prostate.","Monitoring,
Phase estimation,
Temperature measurement,
Protons,
Resonance,
Resonant frequency,
Frequency measurement,
Displacement measurement,
Motion measurement,
Minimally invasive surgery"
Visualizing Changes of Hierarchical Data using Treemaps,"While the treemap is a popular method for visualizing hierarchical data, it is often difficult for users to track layout and attribute changes when the data evolve over time. When viewing the treemaps side by side or back and forth, there exist several problems that can prevent viewers from performing effective comparisons. Those problems include abrupt layout changes, a lack of prominent visual patterns to represent layouts, and a lack of direct contrast to highlight differences. In this paper, we present strategies to visualize changes of hierarchical data using treemaps. A new treemap layout algorithm is presented to reduce abrupt layout changes and produce consistent visual patterns. Techniques are proposed to effectively visualize the difference and contrast between two treemap snapshots in terms of the map items' colors, sizes, and positions. Experimental data show that our algorithm can achieve a good balance in maintaining a treemap's stability, continuity, readability, and average aspect ratio. A software tool is created to compare treemaps and generate the visualizations. User studies show that the users can better understand the changes in the hierarchy and layout, and more quickly notice the color and size differences using our method.","Data visualization,
Displays,
File systems,
Switches,
Computer science,
Data engineering,
Stability,
Software tools,
Data analysis,
Spirals"
Optimal Embeddings of Paths with Various Lengths in Twisted Cubes,"Twisted cubes are variants of hypercubes. In this paper, we study the optimal embeddings of paths of all possible lengths between two arbitrary distinct nodes in twisted cubes. We use TQn to denote the n-dimensional twisted cube and use dist(TQn, u, v) to denote the distance between two nodes u and v in TQn, where n ges l is an odd integer. The original contributions of this paper are as follows: 1) We prove that a path of length l can be embedded between u and v with dilation 1 for any two distinct nodes u and v and any integer l with dist(TQn, u, v) + 2 les l les 2n - 1 (n ges 3) and 2) we find that there exist two nodes u and v such that no path of length dist(TQn, u, v) + l can be embedded between u and v with dilation 1 (n ges 3). The special cases for the nonexistence and existence of embeddings of paths between nodes u and v and with length dist(TQn, u, v) + 1 are also discussed. The embeddings discussed in this paper are optimal in the sense that they have dilation 1","Multiprocessor interconnection networks,
Hypercubes,
Parallel processing,
Measurement,
Delay,
Tree graphs,
Binary trees"
Marking Estimation of Petri Nets With Silent Transitions,"In this paper, we deal with the problem of estimating the marking of a labeled Petri net system based on the observation of transitions labels. In particular, we assume that a certain number of transitions are labeled with the empty string , while unique labels taken from a given alphabet are assigned to each of the other transitions. Transitions labeled with the empty string are called silent because their firing cannot be observed. Under some technical assumptions on the structure of the unobservable subnet, we formally prove that the set of markings consistent with the observed word can be represented by a linear system with a fixed structure that does not depend on the length of the observed word.","Petri nets,
Doped fiber amplifiers,
Automata,
Corona,
Linear systems,
Control systems,
Computer science,
Discrete event systems,
Context modeling,
Observers"
Multicast Algorithms for Multi-Channel Wireless Mesh Networks,"Multicast is a key technology that provides efficient data communication among a set of nodes for wireless multi-hop networks. In sensor networks and MANETs, multicast algorithms are designed to be energy efficient and to achieve optimal route discovery among mobile nodes, respectively. However, in wireless mesh networks, which are required to provide high quality service to end users as the ""last-mile"" of the Internet, throughput maximization conflicting with scarce bandwidth has the paramount priority. We propose a level channel assignment (LCA) algorithm and a multi-channel multicast (MCM) algorithm to optimize throughput for multi-channel and multi-interface mesh networks. The algorithms first build a multicast structure by minimizing the number of relay nodes and hop count distances between the source and destinations, and use dedicated channel assignment strategies to improve the network capacity by reducing interference. We also illustrate that the use of partially overlapping channels can further improve the throughput. Simulations show that our algorithms greatly outperform the single-channel multicast algorithm. We observe that MCM achieves better throughput and shorter delay while LCA can be realized in distributed manner.","Multicast algorithms,
Wireless mesh networks,
Throughput,
Data communication,
Wireless sensor networks,
Spread spectrum communication,
Algorithm design and analysis,
Energy efficiency,
IP networks,
Web and internet services"
"How Do You Quickly Choreograph Inter-Vehicular Communications? A Fast Vehicle-to-Vehicle Multi-Hop Broadcast Algorithm, Explained",,"Broadcasting,
Protocols,
Roads,
Mobile communication,
Computer science,
Proposals,
Delay effects,
Vehicle safety,
Spread spectrum communication,
Moon"
Predicting Eclipse Bug Lifetimes,"In non-trivial software development projects planning and allocation of resources is an important and difficult task. Estimation of work time to fix a bug is commonly used to support this process. This research explores the viability of using data mining tools to predict the time to fix a bug given only the basic information known at the beginning of a bug's lifetime. To address this question, a historical portion of the Eclipse Bugzilla database is used for modeling and predicting bug lifetimes. A bug history transformation process is described and several data mining models are built and tested. Interesting behaviours derived from the models are documented. The models can correctly predict up to 34.9% of the bugs into a discretized log scaled lifetime class.","Computer bugs,
History,
Data mining,
Databases,
Predictive models,
Testing,
Open source software,
XML,
Computer science,
Programming"
Correspondence Mapping Induced State and Action Metrics for Robotic Imitation,"This paper addresses the problem of body mapping in robotic imitation where the demonstrator and imitator may not share the same embodiment [degrees of freedom (DOFs), body morphology, constraints, affordances, and so on]. Body mappings are formalized using a unified (linear) approach via correspondence matrices, which allow one to capture partial, mirror symmetric, one-to-one, one-to-many, many-to-one, and many-to-many associations between various DOFs across dissimilar embodiments. We show how metrics for matching state and action aspects of behavior can be mathematically determined by such correspondence mappings, which may serve to guide a robotic imitator. The approach is illustrated and validated in a number of simulated 3-D robotic examples, using agents described by simple kinematic models and different types of correspondence mappings","Mirrors,
Robot programming,
Humans,
Computer science,
Psychology,
Robot kinematics,
Morphology,
Symmetric matrices,
Education,
Pediatrics"
High performance virtual machine migration with RDMA over modern interconnects,"One of the most useful features provided by virtual machine (VM) technologies is the ability to migrate running OS instances across distinct physical nodes. As a basis for many administration tools in modern clusters and data-centers, VM migration is desired to be extremely efficient to reduce both migration time and performance impact on hosted applications. Currently, most VM environments use the Socket interface and the TCP/IP protocol to transfer VM migration traffic. In this paper, we propose a high performance VM migration design by using RDMA (Remote Direct Memory Access). RDMA is a feature provided by many modern high speed interconnects that are currently being widely deployed in data-centers and clusters. By taking advantage of the low software overhead and the one-sided nature of RDMA, our design significantly improves the efficiency of VM migration. We also contribute a set of micro-benchmarks and application-level benchmark evaluations aimed at evaluating important metrics of VM migration. The evaluations using our prototype implementation over Xen and InfiniBand show that RDMA can drastically reduce the migration overhead: up to 80% on total migration time and up to 77% on application observed downtime.","Bandwidth,
Protocols,
Software,
IP networks,
Memory management,
Hardware,
Servers"
Cached k-d tree search for ICP algorithms,"The ICP (iterative closest point) algorithm is the de facto standard for geometric alignment of three-dimensional models when an initial relative pose estimate is available. The basis of ICP is the search for closest points. Since the development of ICP, k-d trees have been used to accelerate the search. This paper presents a novel search procedure, namely cached k-d trees, exploiting iterative behavior of the ICP algorithm. It results in a significant speedup of about 50% as we show in an evaluation using different data sets.","Iterative closest point algorithm,
Cache memory,
Iterative algorithms,
Clouds,
Solid modeling,
Robotics and automation,
Neodymium,
Quaternions,
Computer science,
Knowledge based systems"
Geolocating Static Cameras,"A key problem in widely distributed camera networks is locating the cameras. This paper considers three scenarios for camera localization: localizing a camera in an unknown environment, adding a new camera in a region with many other cameras, and localizing a camera by finding correlations with satellite imagery. We find that simple summary statistics (the time course of principal component coefficients) are sufficient to geolocate cameras without determining correspondences between cameras or explicitly reasoning about weather in the scene. We present results from a database of images from 538 cameras collected over the course of a year. We find that for cameras that remain stationary and for which we have accurate image times- tamps, we can localize most cameras to within 50 miles of the known location. In addition, we demonstrate the use of a distributed camera network in the construction a map of weather conditions.","Cameras,
Satellites,
Layout,
Calibration,
Sun,
Jacobian matrices,
Computer science,
Statistical distributions,
Image databases,
Surveillance"
A Distributed Connectivity Restoration Algorithm in Wireless Sensor and Actor Networks,"There has been an increased interest in applications of wireless sensor and actor networks (WSANs) in recent years. In such applications, a set of mobile actor nodes are deployed in addition to sensors in order to collect sensors' data and perform specific tasks in response to detected events/objects. In most scenarios actors have to respond collectively which requires an inter-actor coordination. Therefore, maintaining a connected inter-actor network is crucial to the effectiveness of WSANs. However, WSANs often operate unattended in harsh environments where actors can easily fail or get damaged. Due to such failures an actor will be unable to communicate with its neighbors which may lead to partitioning the inter-actor network. In this paper we present DARA; a Distributed Actor Recovery Algorithm, which opts to efficiently restore the connectivity of the inter-actor network that has been affected by the failure of an actor. The idea is to identify the least set of actors that should be repositioned in order to establish connectivity among disjoint network partitions. DARA strives to localize the scope of the recovery process and minimize the movement overhead imposed on the involved actors. The effectiveness of DARA is validated through simulation experiments.","Wireless sensor networks,
Event detection,
Fires,
Computer networks,
Computer science,
Application software,
Partitioning algorithms,
Optimized production technology,
Collaboration,
Distributed computing"
Cluster-based Medium Access Scheme for VANETs,"In recent years research on active safety systems with the main focus on environment sensing has been done. Therefore vehicles are equipped with sensors which record information about the nearby traffic situation and possible hazards. For anticipatory driving additional information outside of the range of the sensors' area needs to be collected. This can be done by introducing data transfer between vehicles, which allows to gather data of a larger area and therefore improves the anticipatory capabilities of the system. One main aspect in the data exchange between vehicles is the organization of the access to the medium. It has to be able to cope with all the problems of mobile ad-hoc networks. Especially the hidden station problem is an important factor in vehicular ad-hoc networks (VANETs). But also the high density of nodes, high dynamics and a limited data rate require a fair and efficient access to the medium. In this paper we present a medium access scheme for vehicular ad-hoc networks which is based on clustering of the vehicles. Thus the effect of hidden stations can be minimized, which leads to a more reliable data transfer than IEEE 802.11 based systems can provide.","Ad hoc networks,
Telecommunication network reliability,
Roads,
Intelligent transportation systems,
Hazards,
Vehicle dynamics,
Wireless sensor networks,
Telecommunication traffic,
Unicast,
Broadcasting"
Fast GPU-Based CT Reconstruction using the Common Unified Device Architecture (CUDA),"The Common Unified Device Architecture (CUDA) is a fundamentally new programming approach making use of the unified shader design of the most current Graphics Processing Units (CPUs) from NVIDIA. The programming interface allows to implement an algorithm using standard C language and a few extensions without any knowledge about graphics programming using OpenGL, DirectX, and shading languages. We apply this revolutionary new technology to the FDK method, which solves the three-dimensional reconstruction task in cone-beam CT. The computational complexity of this algorithm prohibits its use for many medical applications without hardware acceleration. Today's CPUs with their high level of parallelism are cost-efficient processors for performing the FDK reconstruction according to medical requirements. In this paper, we present an innovative implementation of the most time-consuming parts of the FDK algorithm: filtering and back-projection. We also explain the required transformations to parallelize the algorithm for the CUDA architecture. Our implementation approach further allows to do an on-the-fly- reconstruction, which means that the reconstruction is completed right after the end of data acquisition. This enables us to present the reconstructed volume to the physician in real-time, immediately after the last projection image has been acquired by the scanning device. Finally, we compare our results to our highly optimized FDK implementation on the Cell Broadband Engine Architecture (CBEA), both with respect to reconstruction speed and implementation effort.","Image reconstruction,
Computed tomography,
Graphics,
Computational complexity,
Medical services,
Biomedical equipment,
Hardware,
Acceleration,
Parallel processing,
Biomedical imaging"
Online trajectory generation in an amphibious snake robot using a lamprey-like central pattern generator model,"This article presents a control architecture for controlling the locomotion of an amphibious snake/lamprey robot capable of swimming and serpentine locomotion. The control architecture is based on a central pattern generator (CPG) model inspired from the neural circuits controlling locomotion in the lamprey's spinal cord. The CPG model is implemented as a system of coupled nonlinear oscillators on board of the robot. The CPG generates coordinated travelling waves in real time while being interactively modulated by a human-operator. Interesting aspects of the CPG model include (1) that it exhibits limit cycle behavior (i.e. it produces stable rhythmic patterns that are robust against perturbations), (2) that the limit cycle behavior has a closed-form solution which provides explicit control over relevant characteristics such as frequency, amplitude and wavelength of the travelling waves, and (3) that the control parameters of the CPG can be continuously and interactively modulated by a human operator to offer high maneuverability. We demonstrate how the CPG allows one to easily adjust the speed and direction of locomotion both in water and on ground while ensuring that continuous and smooth setpoints are sent to the robot's actuated joints.","Robot kinematics,
Centralized control,
Limit-cycles,
Spinal cord,
Coupling circuits,
Oscillators,
Robust control,
Closed-form solution,
Frequency,
Amplitude modulation"
Do Gradations of Time Zone Separation Make a Difference in Performance? A First Laboratory Study,"We often hear that global software engineering teams are affected by time differences. While there is considerable research on the difficulties of distance, culture and other dimensions, there has been little research that isolated the impact of just time differences. The research question that guides us is whether there are gradual differences across time zones that impact team performance. In this study we conducted a laboratory experiment with 42 dyadic teams. The teams were randomly assigned into 4 time zone overlap conditions: full overlap, 2/3 overlap, 1/3 overlap and no overlap. Using a fictional map task, we found that participants' perceptions of process are unrelated to actual objective performance measures of speed and accuracy. Consistent with our expectations, we found that a small time separation has no effect on accuracy, but that more time separation has a significant effect on accuracy. Also consistent with our expectations, we found that a small amount of time separation has a significant effect on production speed. However, contrary to our expectations, we found that further increases in partial overlap have less significant effects on speed, and when there is no overlap speed actually increases, albeit not significantly - a ""U-shaped"" effect.","Laboratories,
Delay effects,
Software engineering,
Collaborative work,
Velocity measurement,
Production,
International collaboration,
Time measurement,
Particle measurements,
Availability"
Failure Resilience for Device Drivers,"Studies have shown that device drivers and extensions contain 3-7 times more bugs than other operating system code and thus are more likely to fail. Therefore, we present a failure-resilient operating system design that can recover from dead drivers and other critical components - primarily through monitoring and replacing malfunctioning components on the fly - transparent to applications and without user intervention. This paper focuses on the post-mortem recovery procedure. We explain the working of our defect detection mechanism, the policy-driven recovery procedure, and post-restart reintegration of the components. Furthermore, we discuss the concrete steps taken to recover from network, block device, and character device driver failures. Finally, we evaluate our design using performance measurements, software fault-injection experiments, and an analysis of the reengineering effort.","Resilience,
Operating systems,
Computer crashes,
Network servers,
Computer bugs,
Condition monitoring,
Error correction codes,
File servers,
Computer science,
Concrete"
"Characterizing Network Traffic in a Cluster-based, Multi-tier Data Center","With the increasing use of various Web-based services, design of high performance, scalable and dependable data centers has become a critical issue. Recent studies show that a clustered, multi-tier architecture is a cost-effective approach to design such servers. Since these servers are highly distributed and complex, understanding the workloads driving them is crucial for the success of the ongoing research to improve them. In view of this, there has been a significant amount of work to characterize the workloads of Web-based services. However, all of the previous studies focus on a high level view of these servers, and analyze request-based or session-based characteristics of the workloads. In this paper, we focus on the characteristics of the network behavior within a clustered, multi-tiered data center. Using a real implementation of a clustered three-tier data center, we analyze the arrival rate and inter-arrival time distribution of the requests to individual server nodes, the network traffic between tiers, and the average size of messages exchanged between tiers. The main results of this study are; (1) in most cases, the request inter-arrival rates follow log-normal distribution, and self-similarity exists when the data center is heavily loaded, (2) message sizes can be modeled by the log-normal distribution, and (3) service times fit reasonably well with the Pareto distribution and show heavy tailed behavior at heavy loads.","Telecommunication traffic,
Delay,
Network servers,
Large-scale systems,
Hardware,
Log-normal distribution,
Web server,
Computer science,
Data engineering,
Design engineering"
Temperature-aware processor frequency assignment for MPSoCs using convex optimization,"The increasing processing capability of Multi-Processor Systems-on-Chips (MPSoCs) is leading to an increase in chip power dissipation, which in turn leads to significant increase in chip temperature. An important challenge facing the MPSoC designers is to achieve the highest performance system operation that satisfies the temperature and power consumption constraints. The frequency of operation of the different processors and the application workload assignment play a critical role in determining the performance, power consumption and temperature profile of the MPSoC. In this paper, we propose novel convex optimization based methods that solve this important problem of temperature-aware processor frequency assignment, such that the total system performance is maximized and the temperature and power constraints are met. We perform experiments on several realistic SoC benchmarks using a cycle-accurate FPGA-based thermal emulation platform, which show that the systems designed using our methods meet the temperature and power consumption requirements at all time instances, while achieving maximum performance.","Mathematical model,
Optimization,
Power demand,
Steady-state,
Time frequency analysis,
Thermal conductivity,
System-on-a-chip"
Boosting Coded Dynamic Features for Facial Action Units and Facial Expression Recognition,"It is well known that how to extract dynamical features is a key issue for video based face analysis. In this paper, we present a novel approach of facial action units (AU) and expression recognition based on coded dynamical features. In order to capture the dynamical characteristics of facial events, we design the dynamical haar-like features to represent the temporal variations of facial events. Inspired by the binary pattern coding, we further encode the dynamic haar-like features into binary pattern features, which are useful to construct weak classifiers for boosting learning. Finally the Adaboost is performed to learn a set of discriminating coded dynamic features for facial active units and expression recognition. Experiments on the CMU expression database and our own facial AU database show its encouraging performance.",
Experiments with Underwater Robot Localization and Tracking,"This paper describes a novel experiment in which two very different methods of underwater robot localization are compared. The first method is based on a geometric approach in which a mobile node moves within a field of static nodes, and all nodes are capable of estimating the range to their neighbours acoustically. The second method uses visual odometry, from stereo cameras, by integrating scaled optical flow. The fundamental algorithmic principles of each localization technique is described. We also present experimental results comparing acoustic localization with GPS for surface operation, and a comparison of acoustic and visual methods for underwater operation.","Robot localization,
Underwater tracking,
Underwater acoustics,
Acoustic sensors,
Global Positioning System,
Sea surface,
Ocean temperature,
Underwater vehicles,
Acoustic measurements,
Sea measurements"
Noninvasive ECG as a Tool for Predicting Termination of Paroxysmal Atrial Fibrillation,"Atrial fibrillation (AF) is the most common cardiac arrhythmia and entails an increased risk of thromboembolic events. Prediction of the termination of an AF episode, based on noninvasive techniques, can benefit patients, doctors and health systems. The method described in this paper is based on two-lead surface electrocardiograms (ECGs): 1-min ECG recordings of AF episodes including N-type (not terminating within an hour after the end of the record), S-type (terminating 1 min after the end of the record) and T-type (terminating immediately after the end of the record). These records are organised into three learning sets (N, S and T) and two test sets (A and B). Starting from these ECGs, the atrial and ventricular activities were separated using beat classification and class averaged beat subtraction, followed by the evaluation of seven parameters representing atrial or ventricular activity. Stepwise discriminant analysis selected the set including dominant atrial frequency (DAF, index of atrial activity) and average HR (HRmean, index of ventricular activity) as optimal for discrimination between N/T-type episodes. The linear classifier, estimated on the 20 cases of the N and T learning sets, provided a performance of 90% on the 30 cases of a test set for the N/T-type discrimination. The same classifier led to correct classification in 89% of the 46 cases for N/S-type discrimination. The method has shown good results and seems to be suitable for clinical application, although a larger dataset would be very useful for improvement and validation of the algorithms and the development of an earlier predictor of paroxysmal AF spontaneous termination time.","Electrocardiography,
Atrial fibrillation,
Rhythm,
Noninvasive treatment,
Testing,
Signal processing algorithms,
Computer science,
Hospitals,
Frequency,
Biomedical signal processing"
CMAC: An Energy Efficient MAC Layer Protocol Using Convergent Packet Forwarding for Wireless Sensor Networks,"Low duty cycle operation is critical to conserve energy in wireless sensor networks. Traditional wake-up scheduling approaches either require periodic synchronization messages or incur high packet delivery latency due to the lack of any synchronization. In this paper, we present the design of a new low duty-cycle MAC layer protocol called Convergent MAC (CMAC). CMAC avoids synchronization overhead while supporting low latency. By using zero communication when there is no traffic, CMAC allows operation at very low duty cycles. When carrying traffic, CMAC first uses any cast to wake up forwarding nodes, and then converges from route-suboptimal any cast with unsynchronized duty cycling to route-optimal unicast with synchronized scheduling. To validate our design and provide a usable module for the community, we implement CMAC in TinyOS and evaluate it on the Kansei testbed consisting of 105 XSM nodes. The results show that CMAC at 1% duty cycle significantly outperforms BMAC at 1% in terms of latency, throughput and energy efficiency. We also compare CMAC with other protocols using simulations. The results show for 1% duty cycle, CMAC exhibits similar throughput and latency as CSMA/CA using much less energy, and outperforms SMAC and GeRaF in all aspects.","Energy efficiency,
Media Access Protocol,
Wireless application protocol,
Wireless sensor networks,
Delay,
Throughput,
Multiaccess communication,
Unicast,
Testing,
Peer to peer computing"
Local Ensemble Kernel Learning for Object Category Recognition,"This paper describes a local ensemble kernel learning technique to recognize/classify objects from a large number of diverse categories. Due to the possibly large intraclass feature variations, using only a single unified kernel-based classifier may not satisfactorily solve the problem. Our approach is to carry out the recognition task with adaptive ensemble kernel machines, each of which is derived from proper localization and regularization. Specifically, for each training sample, we learn a distinct ensemble kernel constructed in a way to give good classification performance for data falling within the corresponding neighborhood. We achieve this effect by aligning each ensemble kernel with a locally adapted target kernel, followed by smoothing out the discrepancies among kernels of nearby data. Our experimental results on various image databases manifest that the technique to optimize local ensemble kernels is effective and consistent for object recognition.","Kernel,
Object recognition,
Humans,
Face,
Visual system,
Machine learning,
Testing,
Training data,
Image retrieval,
Biological system modeling"
TCAM Razor: A Systematic Approach Towards Minimizing Packet Classifiers in TCAMs,"Packet classification is the core mechanism that enables many networking services on the Internet such as firewall packet filtering and traffic accounting. Using ternary content addressable memories (TCAMs) to perform high-speed packet classification has become the de facto standard in industry. TCAMs classify packets in constant time by comparing a packet with all classification rules of ternary encoding in parallel. Despite their high speed, TCAMs suffer from the well-known range expansion problem. As packet classification rules usually have fields specified as ranges, converting such rules to TCAM-compatible rules may result in an explosive increase in the number of rules. This is not a problem if TCAMs have large capacities. Unfortunately, TCAMs have very limited capacity, and more rules means more power consumption and more heat generation for TCAMs. Even worse, the number of rules in packet classifiers have been increasing rapidly with the growing number of services deployed on the internet. To address the range expansion problem of TCAMs, we consider the following problem: given a packet classifier, how can we generate another semantically equivalent packet classifier that requires the least number of TCAM entries? In this paper, we propose a systematic approach, the TCAM Razor, that is effective, efficient, and practical. In terms of effectiveness, our TCAM Razor prototype achieves a total compression ratio of 3.9%, which is significantly better than the previously published best result of 54%. In terms of efficiency, our TCAM Razor prototype runs in seconds, even for large packet classifiers. Finally, in terms of practicality, our TCAM Razor approach can be easily deployed as it does not require any modification to existing packet classification systems, unlike many previous range expansion solutions.","Web and internet services,
Prototypes,
IP networks,
Information filtering,
Information filters,
Telecommunication traffic,
Associative memory,
Encoding,
Explosives,
Energy consumption"
Discovery of Periodic Patterns in Spatiotemporal Sequences,"In many applications that track and analyze spatiotemporal data, movements obey periodic patterns; the objects follow the same routes (approximately) over regular time intervals. For example, people wake up at the same time and follow more or less the same route to their work everyday. The discovery of hidden periodic patterns in spatiotemporal data could unveil important information to the data analyst. Existing approaches for discovering periodic patterns focus on symbol sequences. However, these methods cannot directly be applied to a spatiotemporal sequence because of the fuzziness of spatial locations in the sequence. In this paper, we define the problem of mining periodic patterns in spatiotemporal data and propose an effective and efficient algorithm for retrieving maximal periodic patterns. In addition, we study two interesting variants of the problem. The first is the retrieval of periodic patterns that are frequent only during a continuous subinterval of the whole history. The second problem is the discovery of periodic patterns, whose instances may be shifted or distorted. We demonstrate how our mining technique can be adapted for these variants. Finally, we present a comprehensive experimental evaluation, where we show the effectiveness and efficiency of the proposed techniques","Spatiotemporal phenomena,
Data analysis,
Pattern analysis,
History,
Computer Society,
Application software,
Tracking,
Information analysis,
Information retrieval"
Global Optimization for Shape Fitting,"We propose a global optimization framework for 3D shape reconstruction from sparse noisy 3D measurements frequently encountered in range scanning, sparse feature-based stereo, and shape-from-X. In contrast to earlier local or banded optimization methods for shape fitting, we compute global optimum in the whole volume removing dependence on initial guess and sensitivity to numerous local minima. Our global method is based on two main ideas. First, we suggest a new regularization functional with a data alignment term that maximizes the number of (weakly-oriented) data points contained by a surface while allowing for some measurement errors. Second, we propose a touch-expand algorithm for finding a minimum cut on a huge 3D grid using an automatically adjusted band. This overcomes prohibitively high memory cost of graph cuts when computing globally optimal surfaces at high-resolution. Our results for sparse or incomplete 3D data from laser scanning and passive multi-view stereo are robust to noise, outliers, missing parts, and varying sampling density.","Surface fitting,
Surface reconstruction,
Shape measurement,
Noise shaping,
Optimization methods,
Mathematics,
Computer science,
Mechanical variables measurement,
Measurement errors,
Cost function"
Feature Selection and Combination Criteria for Improving Accuracy in Protein Structure Prediction,"The classification of protein structures is essential for their function determination in bioinformatics. At present, a reasonably high rate of prediction accuracy has been achieved in classifying proteins into four classes in the SCOP database according to their primary amino acid sequences. However, for further classification into fine-grained folding categories, especially when the number of possible folding patterns as those defined in the SCOP database is large, it is still quite a challenge. In our previous work, we have proposed a two-level classification strategy called hierarchical learning architecture (HLA) using neural networks and two indirect coding features to differentiate proteins according to their classes and folding patterns, which achieved an accuracy rate of 65.5%. In this paper, we use a combinatorial fusion technique to facilitate feature selection and combination for improving predictive accuracy in protein structure classification. When applying various criteria in combinatorial fusion to the protein fold prediction approach using neural networks with HLA and the radial basis function network (RBFN), the resulting classification has an overall prediction accuracy rate of 87% for four classes and 69.6% for 27 folding categories. These rates are significantly higher than the accuracy rate of 56.5% previously obtained by Ding and Dubchak. Our results demonstrate that data fusion is a viable method for feature selection and combination in the prediction and classification of protein structure.","Proteins,
Accuracy,
Neural networks,
Amino acids,
Control engineering,
Computer science,
Bioinformatics,
Spatial databases,
Radial basis function networks,
Data analysis"
Stack Trace Analysis for Large Scale Debugging,"We present the Stack Trace Analysis Tool (STAT) to aid in debugging extreme-scale applications. STAT can reduce problem exploration spaces from thousands of processes to a few by sampling stack traces to form process equivalence classes, groups of processes exhibiting similar behavior. We can then use full-featured debuggers on representatives from these behavior classes for root cause analysis. STAT scalably collects stack traces over a sampling period to assemble a profile of the application's behavior. STAT routines process the samples to form a call graph prefix tree that encodes common behavior classes over the program's process space and time. STAT leverages MRNet, an infrastructure for tool control and data analyses, to overcome scalability barriers faced by heavy-weight debuggers. We present STAT's design and an evaluation that shows STAT gathers informative process traces from thousands of processes with sub-second latencies, a significant improvement over existing tools. Our case studies of production codes verify that STAT supports the quick identification of errors that were previously difficult to locate.",
A Multi-Resolution Dynamic Model for Face Aging Simulation,"In this paper we present a dynamic model for simulating face aging process. We adopt a high resolution grammatical face model [1] and augment it with age and hair features. This model represents all face images by a multi-layer and-or graph and integrates three most prominent aspects related to aging changes: global appearance changes in hair style and shape, deformations and aging effects of facial components, and wrinkles appearance at various facial zones. Then face aging is modeled as a dynamic Markov process on this graph representation which is learned from a large dataset. Given an input image, we firstly compute the graph representation, and then sample the graph structures over various age groups according to the learned dynamic model. Finally we generate new face images with the sampled graphs. Our approach has three novel aspects: (1) the aging model is learned from a dataset of 50,000 adult faces at different ages; (2) we explicitly model the uncertainty in face aging andean sample multiple plausible aged faces for an input image; and (3) we conduct a simple human experiment to validate the simulated aging process.","Aging,
Face detection,
Computational modeling,
Shape,
Computer vision,
Hair,
Face recognition,
Computer simulation,
Humans,
Computer graphics"
Spatio-Temporal Context for Robust Multitarget Tracking,"In multitarget tracking, the main challenge is to maintain the correct identity of targets even under occlusions or when differences between the targets are small. The paper proposes a new approach to this problem by incorporating the context information. The context of a target in an image sequence has two components: the spatial context including the local background and nearby targets and the temporal context including all appearances of the targets that have been seen previously. The paper considers both aspects. We propose a new model for multitarget tracking based on the classification of each target against its spatial context. The tracker searches a region similar to the target while avoiding nearby targets. The temporal context is included by integrating the entire history of target appearance based on probabilistic principal component analysis (PPCA). We have developed a new incremental scheme that can learn the full set of PPCA parameters accurately online. The experiments show robust tracking performance under the condition of severe clutter, occlusions, and pose changes",
Efficient Computation of Morse-Smale Complexes for Three-dimensional Scalar Functions,"The Morse-Smale complex is an efficient representation of the gradient behavior of a scalar function, and critical points paired by the complex identify topological features and their importance. We present an algorithm that constructs the Morse-Smale complex in a series of sweeps through the data, identifying various components of the complex in a consistent manner. All components of the complex, both geometric and topological, are computed, providing a complete decomposition of the domain. Efficiency is maintained by representing the geometry of the complex in terms of point sets.","Isosurfaces,
Topology,
Computer science,
Tree graphs,
Surface topography,
Data analysis,
Data visualization,
Data structures,
Geometry,
Computer vision"
Minimal Shape and Intensity Cost Path Segmentation,"A new generic model-based segmentation algorithm is presented, which can be trained from examples akin to the active shape model (ASM) approach in order to acquire knowledge about the shape to be segmented and about the gray-level appearance of the object in the image. Whereas ASM alternates between shape and intensity information during search, the proposed approach optimizes for shape and intensity characteristics simultaneously. Local gray-level appearance information at the landmark points extracted from feature images is used to automatically detect a number of plausible candidate locations for each landmark. The shape information is described by multiple landmark-specific statistical models that capture local dependencies between adjacent landmarks on the shape. The shape and intensity models are combined in a single cost function that is optimized noniteratively using dynamic programming, without the need for initialization. The algorithm was validated for segmentation of anatomical structures in chest and hand radiographs. In each experiment, the presented method had a significant higher performance when compared to the ASM schemes. As the method is highly effective, optimally suited for pathological cases and easy to implement, it is highly useful for many medical image segmentation tasks.","Image segmentation,
Active shape model,
Data mining,
Feature extraction,
Cost function,
Dynamic programming,
Anatomical structure,
Radiography,
Pathology,
Biomedical imaging"
Distributed Cooperative Active Sensing Using Consensus Filters,"We consider the problem of multiple mobile sensor agents tracking the position of one or more moving targets. In our formulation, each agent maintains a target estimate, and each agent moves so as to maximize the expected information from its sensor, relative to the current uncertainty in the estimate. The novelty of our approach is that each agent need only communicate with one-hop neighbors in a communication network, resulting in a fully distributed and scalable algorithm, yet the performance of the system approximates that of a centralized optimal solution to the same problem. We provide two fully distributed algorithms based on one-time measurements and a Kalman filter approach, and we validate the algorithms with simulations.","Target tracking,
Sensor fusion,
Motion control,
Communication system control,
Motion estimation,
Uncertainty,
Communication networks,
Fuses,
Covariance matrix,
Mechanical sensors"
Graph Database Indexing Using Structured Graph Decomposition,"We introduce a novel method of indexing graph databases in order to facilitate subgraph isomorphism and similarity queries. The index is comprised of two major data structures. The primary structure is a directed acyclic graph which contains a node for each of the unique, induced subgraphs of the database graphs. The secondary structure is a hash table which cross-indexes each subgraph for fast isomorphic lookup. In order to create a hash key independent of isomorphism, we utilize a code-based canonical representation of adjacency matrices, which we have further refined to improve computation speed. We validate the concept by demonstrating its effectiveness in answering queries for two practical datasets. Our experiments show that for subgraph isomorphism queries, our method outperforms existing methods by more than an order of magnitude.","Databases,
Indexing,
Pattern matching,
Proteins,
Chemicals,
Testing,
Computer science,
Tree graphs,
Drugs,
Data engineering"
"Autonomous Automobile Trajectory Tracking for Off-Road Driving: Controller Design, Experimental Validation and Racing","This paper presents a nonlinear control law for an automobile to autonomously track a trajectory, provided in real-time, on rapidly varying, off-road terrain. Existing methods can suffer from a lack of global stability, a lack of tracking accuracy, or a dependence on smooth road surfaces, any one of which could lead to the loss of the vehicle in autonomous off-road driving. This work treats automobile trajectory tracking in a new manner, by considering the orientation of the front wheels - not the vehicle's body - with respect to the desired trajectory, enabling collocated control of the system. A steering control law is designed using the kinematic equations of motion, for which global asymptotic stability is proven. This control law is then augmented to handle the dynamics of pneumatic tires and of the servo-actuated steering wheel. To control vehicle speed, the brake and throttle are actuated by a switching proportional integral (PI) controller. The complete control system consumes a negligible fraction of a computer's resources. It was implemented on a Volkswagen Touareg, ""Stanley"", the Stanford Racing Team's entry in the DARPA Grand Challenge 2005, a 132 mi autonomous off-road race. Experimental results from Stanley demonstrate the ability of the controller to track trajectories between obstacles, over steep and wavy terrain, through deep mud puddles, and along cliff edges, with a typical root mean square (RMS) crosstrack error of under 0.1 m. In the DARPA National Qualification Event 2005, Stanley was the only vehicle out of 40 competitors to not hit an obstacle or miss a gate, and in the DARPA Grand Challenge 2005 Stanley had the fastest course completion time.","Automobiles,
Trajectory,
Remotely operated vehicles,
Wheels,
Control systems,
Pi control,
Proportional control,
Stability,
Roads,
Surface treatment"
"Gradual Release: Unifying Declassification, Encryption and Key Release Policies","Information security has a challenge to address: enabling information-flow controls with expressive information release (or declassification) policies. Existing approaches tend to address some aspects of information release, exposing the other aspects for possible attacks. It is striking that these approaches fall into two mostly separate categories: revelation-based (as in information purchase, aggregate computation, moves in a game, etc.) and encryption-based declassification (as in sending encrypted secrets over an untrusted network, storing passwords, etc.). This paper introduces gradual release, a policy that unifies declassification, encryption, and key release policies. We model an attacker's knowledge by the sets of possible secret inputs as functions of publicly observable outputs. The essence of gradual release is that this knowledge must remain constant between releases. Gradual release turns out to be a powerful foundation for release policies, which we demonstrate by formally connecting revelation-based and encryption-based declassification. Furthermore, we show that gradual release can be provably enforced by security types and effects.","Cryptography,
Information security,
Joining processes,
Aggregates,
Computer networks,
Data security,
Computer science,
Power system security,
Protocols"
Tracking Large Variable Numbers of Objects in Clutter,"We propose statistical data association techniques/or visual tracking of enormously large numbers of objects. We do not assume any prior knowledge about the numbers involved, and the objects may appear or disappear anywhere in the image frame and at any time in the sequence. Our approach combines the techniques of multitarget track initiation, recursive Bayesian tracking, clutter modeling, event analysis, and multiple hypothesis filtering. The original multiple hypothesis filter addresses an NP-hard problem and is thus not practical. We propose two cluster-based data association approaches that are linear in the number of detections and tracked objects. We applied the method to track wildlife in infrared video. We have successfully tracked hundreds of thousands of bats which were flying at high speeds and in dense formations.","Wildlife,
Computer science,
Biology,
Sequences,
Bayesian methods,
Filtering,
Filters,
NP-hard problem,
Object detection,
Signal to noise ratio"
A Hybrid Particle Swarm Algorithm with Cauchy Mutation,"Particle swarm optimization (PSO) has shown its fast search speed in many complicated optimization and search problems. However, PSO could often easily fall into local optima because the particles could quickly get closer to the best particle. At such situations, the best particle could hardly be improved. This paper proposes a new hybrid PSO (HPSO) to solve this problem by adding a Cauchy mutation on the best particle so that the mutated best particle could lead all the rest of particles to the better positions. Experimental results on many well-known benchmark optimization problems have shown that HPSO could successfully deal with those difficult multimodal functions while maintaining fast search speed on those simple unimodal functions in the function optimization","Particle swarm optimization,
Genetic mutations,
Evolutionary computation,
Computer science,
Geology,
Search problems,
Random number generation,
Velocity control,
Genetic programming,
Testing"
Contextual Anatomic Mimesis Hybrid In-Situ Visualization Method for Improving Multi-Sensory Depth Perception in Medical Augmented Reality,"The need to improve medical diagnosis and reduce invasive surgery is dependent upon seeing into a living human system. The use of diverse types of medical imaging and endoscopic instruments has provided significant breakthroughs, but not without limiting the surgeon's natural, intuitive and direct 3D perception into the human body. This paper presents a method for the use of augmented reality (AR) for the convergence of improved perception of 3D medical imaging data (mimesis) in context to the patient's own anatomy (in-situ) incorporating the physician's intuitive multi- sensory interaction and integrating direct manipulation with endoscopic instruments. Transparency of the video images recorded by the color cameras of a video see-through, stereoscopic head- mounted-display (HMD) is adjusted according to the position and line of sight of the observer, the shape of the patient's skin and the location of the instrument. The modified video image of the real scene is then blended with the previously rendered virtual anatomy. The effectiveness has been demonstrated in a series of experiments at the Chirurgische Klinik in Munich, Germany with cadaver and in-vivo studies. The results can be applied for designing medical AR training and educational applications.","Visualization,
Augmented reality,
Biomedical imaging,
Instruments,
Medical diagnostic imaging,
Humans,
Anatomy,
Medical diagnosis,
Surgery,
Surges"
A Web Service Recommender System Using Enhanced Syntactical Matching,"Service-oriented computing (SOC) enables organizations and individual users to discover openly-accessible capabilities realized as services over the Internet. However, service registries can potentially be very large preventing organizations from discovering services in real-time. In fact, consumers may not be aware of the services that can be of most benefit to them. In our work, we introduce a web service recommender system that proactively discovers and manages web services. This paper focuses on the underlying search and ranking algorithms that enable the recommendations. As an innovation, we have analyzed real, fully-operational web services currently available on the Internet and, as a result, have discovered insights into how real web service messages are defined. Using these general naming tendencies coupled with enhanced syntactical methods, we are able to aggregate services by their messages and accurately suggest candidate services to users as a part of daily routines.","Web services,
Recommender systems,
Web and internet services,
Ontologies,
Service oriented architecture,
Computer science,
Technological innovation,
Aggregates,
Resource description framework,
Humans"
3D Model based Object Class Detection in An Arbitrary View,"In this paper, a novel object class detection method based on 3D object modeling is presented. Instead of using a complicated mechanism for relating multiple 2D training views, the proposed method establishes spatial connections between these views by mapping them directly to the surface of 3D model. The 3D shape of an object is reconstructed by using a homographic framework from a set of model views around the object and is represented by a volume consisting of binary slices. Features are computed in each 2D model view and mapped to the 3D shape model using the same homographic framework. To generalize the model for object class detection, features from supplemental views are also considered. A codebook is constructed from all of these features and then a 3D feature model is built. Given a 2D test image, correspondences between the 3D feature model and the testing view are identified by matching the detected features. Based on the 3D locations of the corresponding features, several hypotheses of viewing planes can be made. The one with the highest confidence is then used to detect the object using feature location matching. Performance of the proposed method has been evaluated by using the PASCAL VOC challenge dataset and promising results are demonstrated.","Object detection,
Shape,
Image reconstruction,
Computer vision,
Cameras,
Surface reconstruction,
Testing,
Computer science,
Solid modeling,
Reconstruction algorithms"
Mobility Reduces Uncertainty in MANETs,"Evaluating and quantifying trust stimulates collaboration in mobile ad hoc networks (MANETs). Many existing reputation systems sharply divide the trust value into right or wrong, thus ignoring another core dimension of trust: uncertainty. As uncertainty deeply impacts a node's anticipation of others' behavior and decisions during interaction, we include uncertainty in the reputation system. Specifically, we use an uncertainty metric to directly reflect a node's confidence in the sufficiency of its past experience, and study how the collection of trust information may affect uncertainty in nodes' opinions. Higher uncertainty leads to higher transaction cost and reduced acceptance of communication and cooperation. After defining a way to reveal and compute the uncertainty in trust opinions, we exploit mobility, one of the important characteristics of MANETs, to efficiently reduce uncertainty and to speed up trust convergence. A two-level mobility assisted uncertainty reduction scheme (MAURS) that offers controllable trade-off between time and cost to achieve a convergence objective of trust is also provided. Extensive analytical and simulation results are presented to support our proposal.","Uncertainty,
Costs,
Mobile ad hoc networks,
Convergence,
Collaboration,
Peer to peer computing,
Communications Society,
Computer science,
Communication system control,
Analytical models"
Multiscale 3-D Shape Representation and Segmentation Using Spherical Wavelets,"This paper presents a novel multiscale shape representation and segmentation algorithm based on the spherical wavelet transform. This work is motivated by the need to compactly and accurately encode variations at multiple scales in the shape representation in order to drive the segmentation and shape analysis of deep brain structures, such as the caudate nucleus or the hippocampus. Our proposed shape representation can be optimized to compactly encode shape variations in a population at the needed scale and spatial locations, enabling the construction of more descriptive, nonglobal, nonuniform shape probability priors to be included in the segmentation and shape analysis framework. In particular, this representation addresses the shortcomings of techniques that learn a global shape prior at a single scale of analysis and cannot represent fine, local variations in a population of shapes in the presence of a limited dataset. Specifically, our technique defines a multiscale parametric model of surfaces belonging to the same population using a compact set of spherical wavelets targeted to that population. We further refine the shape representation by separating into groups wavelet coefficients that describe independent global and/or local biological variations in the population, using spectral graph partitioning. We then learn a prior probability distribution induced over each group to explicitly encode these variations at different scales and spatial locations. Based on this representation, we derive a parametric active surface evolution using the multiscale prior coefficients as parameters for our optimization procedure to naturally include the prior for segmentation. Additionally, the optimization method can be applied in a coarse-to-fine manner. We apply our algorithm to two different brain structures, the caudate nucleus and the hippocampus, of interest in the study of schizophrenia. We show: 1) a reconstruction task of a test set to validate the expressiveness of our multiscale prior and 2) a segmentation task. In the reconstruction task, our results show that for a given training set size, our algorithm significantly improves the approximation of shapes in a testing set over the Point Distribution Model, which tends to oversmooth data. In the segmentation task, our validation shows our algorithm is computationally efficient and outperforms the Active Shape Model algorithm, by capturing finer shape details","Brain,
Hippocampus,
Testing,
Active shape model,
Wavelet transforms,
Parametric statistics,
Surface waves,
Wavelet coefficients,
Biological information theory,
Probability distribution"
Driving into Intelligent Spaces with Pervasive Communications,"Recent advances in digital modulation and transmission, signal processing, wireless access protocols, and IC technology have spawned many intelligent devices and objects. A good example are intelligent transportation spaces (ITSP) which have shown many potential benefits including driving safety, transport efficiency, and comfort that accrue from increased traffic information, reduced driving loads, and improved route management. Although wireline communications might enable some ITSP requirements, the inherent mobility, flexibility and scalability of wireless communications clearly make them critical. Enabling technologies in this area include an array of wireless solutions such as ISM bands, DSRC bands, UWB radio, and MAC protocols","Space technology,
Digital modulation,
Digital signal processing,
Access protocols,
Digital integrated circuits,
Intelligent transportation systems,
Safety,
Scalability,
Wireless communication,
Media Access Protocol"
Efficient Fault Identification of Diagnosable Systems under the Comparison Model,"Diagnosis by comparison is a realistic approach to the fault diagnosis of massive multicomputers. This paper addresses the fault identification of diagnosable multicomputer systems under the MM* comparison model. We find that the fault location task can be reduced to that under the classical PMC* model. On this basis, we present an O(n times Delta3 times delta) time diagnosis algorithm for an n-node MM* diagnosable system, where Delta and delta denote the maximum and minimum degrees of a node, respectively. The proposed algorithm is much more efficient than the fastest known diagnosis algorithm (which consumes O(n5) time) because realistic massive multicomputers are sparsely interconnected and, hence, Delta,delta Lt n.","Fault diagnosis,
Algorithm design and analysis,
Fault location,
Classification algorithms,
Program processors,
Availability,
Complexity theory"
Soft Sensing and Optimal Power Control for Cognitive Radio,"We consider a cognitive radio system where the secondary transmitter varies its transmit power based on all the information available from the spectrum sensor. The operation of the secondary user is governed by its peak transmit power constraint and an average interference constraint at the primary receiver. Without restricting the sensing scheme (total received energy, or correlation etc), we characterize the power adaptation strategies that maximize the secondary user's SNR and capacity. We show that, in general, the capacity optimal power adaptation requires decreasing the secondary transmit power from the peak power to zero in a continuous fashion as the probability of the primary user being present increases. We find that power control that maximizes the SNR is binary, i.e., if there is any transmission, it takes place only at the peak power level. Numerical results for common spectrum sensing schemes show that the SNR and capacity maximizing schemes can be significantly different.","Power control,
Cognitive radio,
Interference constraints,
Radio transmitters,
Receivers,
Radiofrequency interference,
Computer science,
Sensor phenomena and characterization,
Sensor systems,
Capacity planning"
Performance and Reliability of Tree-Structured Grid Services Considering Data Dependence and Failure Correlation,"Grid computing is a newly emerging technology aimed at large-scale resource sharing and global-area collaboration. It is the next step in the evolution of parallel and distributed computing. Due to the largeness and complexity of the grid system, its performance and reliability are difficult to model, analyze, and evaluate. This paper presents a model that relaxes some assumptions made in prior research on distributed systems that were inappropriate for grid computing. The paper proposes a virtual tree-structured model of the grid service. This model simplifies the physical structure of a grid service, allows service performance (execution time) to be efficiently evaluated, and takes into account data dependence and failure correlation. Based on the model, an algorithm for evaluating the grid service time distribution and the service reliability indices is suggested. The algorithm is based on Graph theory and probability theory. Illustrative examples and a real case study of the BioGrid are presented.",
955-fps Real-time Shape Measurement of a Moving/Deforming Object using High-speed Vision for Numerous-point Analysis,This paper describes real-time shape measurement using a newly developed high-speed vision system. Our proposed measurement system can observe a moving/deforming object at high frame rate and can acquire data in real-time. This is realized by using two-dimensional pattern projection and a high-speed vision system with a massively parallel co-processor for numerous-point analysis. We detail our proposed shape measurement system and present some results of evaluation experiments. The experimental results show the advantages of our system compared with conventional approaches.,"Shape measurement,
Real time systems,
Delay,
Throughput,
Machine vision,
Time measurement,
Robot control,
Control systems,
Robotics and automation,
Information analysis"
A New Steganographic Method for Data Hiding in Microsoft Word Documents by a Change Tracking Technique,"A new steganographic method for data hiding in Microsoft Word documents by a change tracking technique is proposed. The data embedding is disguised such that the stegodocument appears to be the product of a collaborative writing effort. Text segments in the document are degenerated, mimicking to be the work of an author with inferior writing skills, with the secret message embedded in the choices of degenerations. The degenerations are then revised with the changes being tracked, making it appear as if a cautious author is correcting the mistakes. The change tracking information contained in the stegodocument allows the original cover, the degenerated document, and, hence, the secret message to be recovered. The extra change tracking information added during message embedding is vital in a normal collaboration scenario, and so hinders ignorant removals by skeptics. Experiments demonstrate the feasibility of the proposed method","Steganography,
Data encapsulation,
Collaboration,
Collaborative work,
Writing,
Computer science,
Communication system security,
Data security,
Information security,
Protection"
Using Distributed Source Coding to Secure Fingerprint Biometrics,"We describe a method to encode fingerprint biometrics securely for use, e.g., in encryption or access control. The system is secure because the stored data does not suffice to recreate the original fingerprint biometric. Therefore, a breach in database security does not lead to the loss of biometric data. At the same time the stored data suffices to validate a probe fingerprint. Our approach is based on the use of distributed source coding techniques implemented with graph-based codes. We present a statistical model of the relationship between the enrollment biometric and the (noisy) biometric measurement taking during authentication. We describe how to validate or reject a candidate biometric probe given the probe and the stored encoded data. We report the effectiveness of our method as tested on a database consisting of 579 data sets, each containing roughly 15 measurements of a single finger. We thereby demonstrate a working secure biometric system for fingerprints.",
Dynamic Cascades for Face Detection,"In this paper, we propose a novel method, called ""dynamic cascade"", for training an efficient face detector on massive data sets. There are three key contributions. The first is a new cascade algorithm called ""dynamic cascade "", which can train cascade classifiers on massive data sets and only requires a small number of training parameters. The second is the introduction of a new kind of weak classifier, called ""Bayesian stump"", for training boost classifiers. It produces more stable boost classifiers with fewer features. Moreover, we propose a strategy for using our dynamic cascade algorithm with multiple sets of features to further improve the detection performance without significant increase in the detector's computational cost. Experimental results show that all the new techniques effectively improve the detection performance. Finally, we provide the first large standard data set for face detection, so that future researches on the topic can be compared on the same training and testing set.","Face detection,
Detectors,
Bayesian methods,
Robustness,
Heuristic algorithms,
Computational efficiency,
Helium,
Sun,
Asia,
Testing"
Illustrative Design Space Studies with Microarchitectural Regression Models,"We apply a scalable approach for practical, comprehensive design space evaluation and optimization. This approach combines design space sampling and statistical inference to identify trends from a sparse simulation of the space. The computational efficiency of sampling and inference enables new capabilities in design space exploration. We illustrate these capabilities using performance and power models for three studies of a 260,000 point design space: (1) Pareto frontier analysis, (2) pipeline depth analysis, and (3) multiprocessor heterogeneity analysis. For each study, we provide an assessment of predictive error and sensitivity of observed trends to such error. We construct Pareto frontiers and find predictions for Pareto optima are no less accurate than those for the broader design space. We reproduce and enhance prior pipeline depth studies, demonstrating constrained sensitivity studies may not generalize when many other design parameters are held at constant values. Lastly, we identify efficient heterogeneous core designs by clustering per benchmark optimal architectures. Collectively, these studies motivate the application of techniques in statistical inference for more effective use of modern simulator infrastructure","Microarchitecture,
Computational modeling,
Sampling methods,
Predictive models,
Costs,
Space exploration,
Pareto analysis,
Pipelines,
Design optimization,
Performance analysis"
Optimal Transmission Range for Wireless Ad Hoc Networks Based on Energy Efficiency,"The transmission range that achieves the most economical use of energy in wireless ad hoc networks is studied for uniformly distributed network nodes. By assuming the existence of forwarding neighbors and the knowledge of their locations, the average per-hop packet progress for a transmission range that is universal for all nodes is derived. This progress is then used to identify the optimal per-hop transmission range that gives the maximal energy efficiency. Equipped with this analytical result, the relation between the most energy-economical transmission range and the node density, as well as the path-loss exponent, is numerically investigated. It is observed that when the path-loss exponent is high (such as four), the optimal transmission ranges are almost identical over the range of node densities that we studied. However, when the path-loss exponent is only two, the optimal transmission range decreases noticeably as the node density increases. Simulation results also confirm the optimality of the per-hop transmission range that we found analytically.","Mobile ad hoc networks,
Energy efficiency,
Energy consumption,
Ad hoc networks,
Routing protocols,
Computer science,
Batteries,
Data processing,
Network topology,
Power generation economics"
Posterior Crlb Based Sensor Selection for Target Tracking in Sensor Networks,"The objective in sensor collaboration for target tracking is to dynamically select a subset of sensors over time to optimize tracking performance in terms of mean square error (MSE). In this paper, we apply the Monte Carlo method to compute the expected posterior Cramer-Rao lower bound (CRLB) in a nonlinear, possibly non-Gaussian, dynamic system. The joint recursive one-step-ahead CRLB on the state vector is introduced as the criterion for sensor selection. The proposed approach is validated by simulation results. In the experiments, a particle filter is used to track a single target moving according to a white noise acceleration model through a two-dimensional field where bearing-only sensors are randomly distributed. Simulation results demonstrate the improved tracking performance of the proposed method compared to other existing methods in terms of tracking accuracy.","Target tracking,
Particle filters,
Mean square error methods,
Nonlinear dynamical systems,
Computational modeling,
White noise,
Acceleration,
State estimation,
Covariance matrix,
Noise measurement"
A Lattice-Based MRF Model for Dynamic Near-Regular Texture Tracking,"A near-regular texture (NRT) is a geometric and photometric deformation from its regular origin - a congruent wallpaper pattern formed by 2D translations of a single tile. A dynamic NRT is an NRT under motion. Although NRTs are pervasive in man-made and natural environments, effective computational algorithms for NRTs are few. This paper addresses specific computational challenges in modeling and tracking dynamic NRTs, including ambiguous correspondences, occlusions, and drastic illumination and appearance variations. We propose a lattice-based Markov-random-field (MRF) model for dynamic NRTs in a 3D spatiotemporal space. Our model consists of a global lattice structure that characterizes the topological constraint among multiple textons and an image observation model that handles local geometry and appearance variations. Based on the proposed MRF model, we develop a tracking algorithm that utilizes belief propagation and particle filtering to effectively handle the special challenges of the dynamic NRT tracking without any assumption on the motion types or lighting conditions. We provide quantitative evaluations of the proposed method against existing tracking algorithms and demonstrate its applications in video editing","Solid modeling,
Particle tracking,
Photometry,
Pervasive computing,
Computational modeling,
Lighting,
Spatiotemporal phenomena,
Geometry"
Assessing Carbon Nanotube Bundle Interconnect for Future FPGA Architectures,"Field programmable gate arrays (FPGAs) are important hardware platforms in various applications due to increasing design complexity and mask costs. However, as CMOS process technology continues to scale, standard copper interconnect becomes a major bottleneck for FPGA performance. This paper proposed utilizing bundles of single-walled carbon nanotubes (SWCNT) as wires in the FPGA interconnect fabric and compare their performance to standard copper interconnect in future process technologies. To leverage the performance advantages of nanotube-based interconnect, several important aspects of the FPGA routing architecture were explored including the segmentation distribution and the internal population of the wires. The results demonstrate that FPGAs utilizing SWCNT bundle interconnect can achieve a 19% improvement in average area delay product over the best performing architecture for standard copper interconnect in 22 nm process technology","Carbon nanotubes,
Field programmable gate arrays,
Copper,
CMOS technology,
Wires,
Hardware,
Costs,
CMOS process,
Fabrics,
Routing"
On the Path Coverage Properties of Random Sensor Networks,"In a sensor network, the points in the operational area that are suitably sensed are a two-dimensional spatial coverage process. For randomly deployed sensor networks, typically, the network coverage of two-dimensional areas is analyzed. However, in many sensor network applications, e.g., tracking of moving objects, the sensing process on paths, rather than in areas, is of interest. With such an application in mind, we analyze the coverage process induced on a one-dimensional path by a sensor network that is modeled as a two-dimensional Boolean model. In the analysis, the sensor locations form a spatial Poisson process of density lambda and the sensing regions are circles of i.i.d. random radii. We first obtain a strong law for the fraction of a path that is k-sensed, i.e., sensed by (ges k) sensors. Asymptotic path-sensing results are obtained under the same limiting regimes as those required for asymptotic coverage by a two-dimensional Boolean model. Interestingly, the asymptotic fraction of the area that is 1-sensed is the same as the fraction of a path that is 1-sensed. For k = 1, we also obtain a central limit theorem that shows that the asymptotics converge at the rate of Theta(lambda1/2) for k = 1. For finite networks, the expectation and variance of the fraction of the path that is k-sensed is obtained. The asymptotics and the finite network results are then used to obtain the critical sensor density to k-sense a fraction alphak of an arbitrary path with very high probability is also obtained. Through simulations, we then analyze the robustness of the model when the sensor deployment is nonhomogeneous and when the paths are not rectilinear. Other path coverage measures like breach, support, ""length to first sense,"" and sensing continuity measures like holes and clumps are also characterized. Finally, we discuss some generalizations of the results like characterization of the coverage process of m-dimensional ""straight line paths"" by n-dimensional, n > m, sensor networks",
A Ten-year Review of Granular Computing,"The year 2007 marks the 10th anniversary of the introduction of granular computing research. We have experienced the emergence and growth of granular computing research in the past ten years. It is essential to explore and review the progress made in the field of granular computing. We use two popular databases, ISI's Web of Science and IEEE Digital Library to conduct our research. We study the current status, the trends and the future direction of granular computing and identify prolific authors, impact authors, and the most impact papers in the past decade.","Problem-solving,
Databases,
Intersymbol interference,
Software libraries,
Computer science,
Fuzzy sets,
Natural languages,
Information processing,
Size measurement,
Computer applications"
Adaptive Route Optimization in Hierarchical Mobile IPv6 Networks,"By introducing a mobility anchor point (MAP), Hierarchical Mobile IPv6 (HMIP6) reduces the signaling overhead and handoff latency associated with Mobile IPv6. However, if a mobile node (MN)'s session activity is high and its mobility is relatively low, HMIPv6 may degrade end-to-end data throughput due to the additional packet tunneling at the MAP. In this paper, we propose an adaptive route optimization (ARO) scheme to improve the throughput performance in HMIPv6 networks. Depending on the measured session-to-mobility ratio (SMR), ARO chooses one of the two different route optimization algorithms adaptively. Specifically, an MN informs a correspondent node (CN) of its on-link care-of address (LCoA) if the CN's SMR is greater than a predefined threshold. If the SMR is equal to or lower than the threshold, the CN is informed with the MN's regional CoA (RCoA). We analyze the performance of ARO in terms of balancing the signaling overhead reduction and the data throughput improvement. We also derive the optimal SMR threshold explicitly to achieve such a balance. Analytical and simulation results demonstrate that ARO is a viable scheme for deployment in HMIPv6 networks.","Throughput,
Mobile radio mobility management,
Delay,
Performance analysis,
Degradation,
Tunneling,
Signal analysis,
Analytical models,
Adaptive systems,
Next generation networking"
Road Surface and Obstacle Detection Based on Elevation Maps from Dense Stereo,"A new approach for the detection of the road surface and obstacles is presented. The 3D data from dense stereo is transformed into a rectangular elevation map. A quadratic road surface model is first fitted, by a RANSAC approach, to the region in front of the ego vehicle. This primary solution is then refined by a region growing-like process, driven by the 3D resolution and uncertainty model of the stereo sensor. An optimal global solution for the road surface is obtained. The road surface is used for a rough discrimination between road and above-road points. Above-road points are grouped based on vicinity and false areas are rejected. Each above-road area is classified into obstacles (cars, pedestrians etc.) or traffic isles (road-parallel patches) by using criteria related to the density of the 3D points. The proposed real-time algorithm was evaluated in an urban scenario and can be used in complex applications, from ego-pose estimation to path planning.","Uncertainty,
Image reconstruction,
Surface reconstruction,
Image edge detection,
Intelligent transportation systems,
Rough surfaces,
Surface roughness,
Surface fitting,
Road transportation,
USA Councils"
Dynamic Fault Tree Analysis Using Input/Output Interactive Markov Chains,"Dynamic fault trees (DFT) extend standard fault trees by allowing the modeling of complex system components' behaviors and interactions. Being a high level model and easy to use, DFT are experiencing a growing success among reliability engineers. Unfortunately, a number of issues still remains when using DFT. Briefly, these issues are (1) a lack of formality (syntax and semantics), (2) limitations in modular analysis and thus vulnerability to the state-space explosion problem, and (3) lack in modular model-building. We use the input/output interactiveMarkov chain (I/O-IMC) formalism to analyse DFT. I/O-IMC have a precise semantics and are an extension of continuous-time Markov chains with input and output actions. In this paper, using the I/OI-MC framework, we address and resolve issues (2) and (3) mentioned above. We also show, through some examples, how one can readily extend the DFT modeling capabilities using the I/O-IMC framework.","Fault trees,
Explosions,
Stochastic processes,
Performance analysis,
Formal specifications,
Computer science,
Reliability engineering,
Measurement standards,
Signal processing,
Algebra"
Assessing the Comprehension of UML Class Diagrams via Eye Tracking,"Eye-tracking equipment is used to assess how well a subject comprehends UML class diagrams. The results of a study are presented in which eye movements are captured in a non-obtrusive manner as users performed various comprehension tasks on UML class diagrams. The goal of the study is to identify specific characteristics of UML class diagrams, such as layout, color, and stereotype usage that are most effective for supporting a given task. Results indicate subjects have a variation in the eye movements (i.e., how the subjects navigate the diagram) depending on their UML expertise and software-design ability to solve the given task. Layouts with additional semantic information about the design were found to be most effective and the use of class stereotypes seems to play a substantial role in comprehension of these diagrams.","Unified modeling language,
Navigation,
Software design,
Usability,
Software maintenance,
Computer science,
Feedback,
Video recording,
Software systems,
Data visualization"
Recognition and understanding of meetings the AMI and AMIDA projects,"The AMI and AMIDA projects are concerned with the recognition and interpretation of multiparty meetings. Within these projects we have: developed an infrastructure for recording meetings using multiple microphones and cameras; released a 100 hour annotated corpus of meetings; developed techniques for the recognition and interpretation of meetings based primarily on speech recognition and computer vision; and developed an evaluation framework at both component and system levels. In this paper we present an overview of these projects, with an emphasis on speech recognition and content extraction.","Ambient intelligence,
Speech recognition,
Microphones,
Cameras,
Instruments,
Layout,
Signal analysis,
Speech analysis,
Humans,
Automatic speech recognition"
Learning Computer Networking on Open Paravirtual Laboratories,"Learning practical information communication technology skills such as network configuration and security planning requires hands-on experience with a number of different devices which may be unavailable or too costly to provide, especially for institutions under tight budget constraints. This paper describes how a specific open software technology, paravirtualization, can be used to set up open source virtual networking labs (VNLs) easily and at virtually no cost. The paper highlights how paravirtual labs can be adopted jointly by partner organizations, e.g., when the institution hosting the virtual lab provides hands-on training and students' skill evaluation as a service to partner institutions overseas. A practical VNL implementation, the open virtual lab (OVL), is used to describe the added value that open source VNLs can give to e-Learning frameworks, achieving a level of students' performance comparable or better than the one obtained when students directly interact with physical networking equipment.","Open source software,
E-learning,
Virtual reality"
"Reconfigured Scan Forest for Test Application Cost, Test Data Volume, and Test Power Reduction","A new scan architecture called reconfigured scan forest is proposed for cost-effective scan testing. Multiple scan flip-flops can be grouped based on structural analysis that avoids new untestable faults due to new reconvergent fanouts. The proposed new scan architecture allows only a few scan flip-flops to be connected to the XOR trees. The size of the XOR trees can be greatly reduced compared with the original scan forest; therefore, area overhead and routing complexity can be greatly reduced. It is shown that test application cost, test data volume, and test power with the proposed scan forest architecture can be greatly reduced compared with the conventional full scan design with a single scan chain and several recent scan testing methods","logic testing,
boundary scan testing,
flip-flops"
Exploiting Object Hierarchy: Combining Models from Different Category Levels,"We investigated the computational properties of natural object hierarchy in the context of constellation object class models, and its utility for object class recognition. We first observed an interesting computational property of the object hierarchy: comparing the recognition rate when using models of objects at different levels, the higher more inclusive levels (e.g., closed-frame vehicles or vehicles) exhibit higher recall but lower precision when compared with the class specific level (e.g., bus). These inherent differences suggest that combining object classifiers from different hierarchical levels into a single classifier may improve classification, as it appears like these models capture different aspects of the object. We describe a method to combine these classifiers, and analyze the conditions under which improvement can be guaranteed. When given a small sample of a new object class, we describe a method to transfer knowledge across the tree hierarchy, between related objects. Finally, we describe extensive experiments using object hierarchies obtained from publicly available datasets, and show that the combined classifiers significantly improve recognition results.","Classification tree analysis,
Boosting,
Training data,
Context modeling,
Vehicles,
Humans,
Object recognition,
Computer science,
Databases,
Cognition"
Challenges and Approaches in Large-Scale P2P Media Streaming,"Large-scale multimedia streaming over the Internet requires an enormous amount of server and network resources. Traditional client-server approaches allocate a dedicated stream from the server for each client request, which is expensive and doesn't scale well. By using end hosts' huge bandwidth and computational capacity, peer-to-peer technologies shed new light on media streaming applications' development. Yet, locating supplying peers and content delivery path maintenance are two major challenges in this area","Streaming media,
Large-scale systems,
Network servers,
Web server,
Bandwidth,
Internet,
Peer to peer computing,
Video sharing,
IP networks,
YouTube"
Scalable and Modular Algorithms for Floating-Point Matrix Multiplication on Reconfigurable Computing Systems,"The abundant hardware resources on current reconfigurable computing systems provide new opportunities for high-performance parallel implementations of scientific computations. In this paper, we study designs for floating-point matrix multiplication, a fundamental kernel in a number of scientific applications, on reconfigurable computing systems. We first analyze design trade-offs in implementing this kernel. These trade-offs are caused by the inherent parallelism of matrix multiplication and the resource constraints, including the number of configurable slices, the size of on-chip memory, and the available memory bandwidth. We propose three parameterized algorithms which can be tuned according to the problem size and the available hardware resources. Our algorithms employ linear array architecture with simple control logic. This architecture effectively utilizes the available resources and reduces routing complexity. The processing elements (PEs) used in our algorithms are modular so that it is easy to embed floating-point units into them. Experimental results on a Xilinx Virtex-ll Pro XC2VP100 show that our algorithms achieve good scalability and high sustained GFLOPS performance. We also implement our algorithms on Cray XD1. XD1 is a high-end reconfigurable computing system that employs both general-purpose processors and reconfigurable devices. Our algorithms achieve a sustained performance of 2.06 GFLOPS on a single node of XD1","Concurrent computing,
Hardware,
Bandwidth,
Routing,
Field programmable gate arrays,
Kernel,
Parallel processing,
Logic arrays,
Parallel algorithms,
Scientific computing"
Finite-Time Distributed Consensus in Graphs with Time-Invariant Topologies,"We present a method for achieving consensus in distributed systems in a finite number of time-steps. Our scheme involves a linear iteration where, at each time-step, each node updates its value to be a weighted average of its own previous value and those of its neighbors. If D denotes the degree of the minimal polynomial of the weight matrix associated with the linear iteration, we show that each node can immediately calculate the consensus value as a linear combination of its own past values over at most D time-steps. We also show that each node can determine the coefficients for this linear combination in a decentralized manner. The proposed scheme has the potential to significantly reduce the time and communication required to reach consensus in distributed systems.","Topology,
Control systems,
Protocols,
Convergence,
Cities and towns,
Polynomials,
Voting,
Computer science,
Sensor fusion,
Sensor systems"
Scale-Dependent 3D Geometric Features,"Three-dimensional geometric data play fundamental roles in many computer vision applications. However, their scale-dependent nature, i.e. the relative variation in the spatial extents of local geometric structures, is often overlooked. In this paper we present a comprehensive framework for exploiting this 3D geometric scale variability. Specifically, we focus on detecting scale-dependent geometric features on triangular mesh models of arbitrary topology. The key idea of our approach is to analyze the geometric scale variability of a given 3D model in the scale-space of a dense and regular 2D representation of its surface geometry encoded by the surface normals. We derive novel corner and edge detectors, as well as an automatic scale selection method, that acts upon this representation to detect salient geometric features and determine their intrinsic scales. We evaluate the effectiveness and robustness of our method on a number of models of different topology. The results show that the resulting scale-dependent geometric feature set provides a reliable basis for constructing a rich but concise representation of the geometric structure at hand.","Solid modeling,
Computer vision,
Geometry,
Topology,
Kernel,
Image edge detection,
Detectors,
Robustness,
Application software,
Feature extraction"
Network Coding Performance for Reliable Multicast,"The capacity gain of network coding has been extensively studied in wired and wireless networks. Recently, it has been shown that network coding improves network reliability by reducing the number of packet retransmissions in lossy networks. However, the extent of the reliability benefit of network coding is not known. This paper quantifies the reliability gain of network coding for reliable multicasting in a wireless network where network coding is the most promising. We define the expected number of transmissions per packet as the performance metric for reliability and derive analytical expressions characterizing the performance of network coding. For a tree-based multicast, we derive expressions for the expected number of transmissions at the source of the multicast and inside the multicast tree. We also analyze the performance of error control mechanisms based on rateless codes and automatic repeat request (ARQ). We then use the analytical expressions to study the impact of multicast group size on the performance of different error control schemes. Our numerical results show that network coding significantly reduces the number of retransmissions in lossy networks compared to end-to-end ARQ scheme, however, rateless coding and link-by-link ARQ are able to achieve performance results comparable to that of network coding. Interestingly, link-by-link ARQ can outperform rateless coding depending on the network size and loss probability. We conjecture that network coding achieves a logarithmic reliability gain with respect to multicast group size compared to a simple ARQ scheme.","Network coding,
Automatic repeat request,
Error correction,
Wireless networks,
Computer network reliability,
Performance analysis,
Broadcasting,
Forward error correction,
Neodymium,
Propagation losses"
Orthogonal Grid Constructions of Copulas,"We establish a general framework for constructing copulas that can be regarded as a patchwork-like assembly of arbitrary copulas, with nonoverlapping rectangles as patches. We derive a family of construction methods that require the choice of a single background copula. When this background copula is the greatest copula , we retrieve the well-known ordinal sum construction, while in the case of the smallest copula , we obtain a construction method that is dual to the ordinal sum construction. Also nonsingular background copulas lead to suitable construction methods.","Mathematics,
Assembly,
Boundary conditions,
Fuzzy sets,
Biometrics,
Process control,
Computer science,
Joining processes"
Tracing Traffic through Intermediate Hosts that Repacketize Flows,"Tracing interactive traffic that traverses stepping stones (i.e., intermediate hosts) is challenging, as the packet headers, lengths, and contents can all be changed by the stepping stones. The traffic timing has therefore been studied as a means of tracing traffic. One such technique uses traffic timing as a side channel into which a watermark, or identifying tag, can be embedded to aid with tracing. The effectiveness of such techniques is greatly reduced when repacketization of the traffic occurs at the stepping stones. Repacketization is a natural effect of many applications, including SSH, and therefore poses a serious challenge for traffic tracing. This paper presents a new method of embedding a watermark in traffic timing, for purposes of tracing the traffic in the presence of repacketization. This method uses an invariant characteristic of two traffic flows which are part of the same stepping stone chain, namely, elapsed time of the flows. The duration of each flow is sliced into short fixed-length intervals. Packet timing is adjusted to manipulate the packet count in specific intervals, for purposes of embedding the watermark. A statistical analysis of the method, with no assumptions or limitations concerning the distribution of packet times, proves the effectiveness of the method given a sufficient number of packets, despite natural and/or deliberate repacketization and perturbation of the traffic timing by an adversary. The method has been implemented and tested on a large number of synthetically-generated SSH traffic flows. The results demonstrate that 100% detection rates and less than 1% false positive rates are achievable under conditions of 2 seconds of maximum timing perturbation and 12% repacketization rate, using fewer than 1000 packets.","Timing,
Watermarking,
Traffic control,
USA Councils,
Delay,
Communications Society,
Computer science,
Software engineering,
TCPIP,
Protocols"
Greedy Virtual Coordinates for Geographic Routing,"We present a new approach for generating virtual coordinates that produces usable coordinates quickly and improves the routing performance of existing geographic routing algorithms. Starting from a set of initial coordinates derived from a set of elected perimeter nodes, greedy embedding spring coordinates (GSpring) detects possible dead ends and uses a modified spring relaxation algorithm to incrementally adjust virtual coordinates to increase the convexity of voids in the virtual routing topology. This reduces the probability that packets will end up in dead ends during greedy forwarding. The coordinates derived by GSpring achieve routing stretch that is up to 50% lower than that for NoGeo, the best existing algorithm for deriving virtual Euclidean coordinates for geographic routing. For realistic network topologies with obstacles, GSpring coordinates achieves from between 10 to 15% better routing stretch than actual physical coordinates.","Routing,
Network topology,
Springs,
Computer science,
Wireless networks,
Packet switching,
Switches,
Artificial intelligence,
Laboratories,
Global Positioning System"
Mechatronics - A unifying interdisciplinary and intelligent engineering science paradigm,,"Mechatronics,
Design engineering,
Knowledge engineering,
Automotive engineering,
Fuses,
Technological innovation,
Systems engineering and theory,
Adders,
Consumer electronics,
Computer science education"
DR-Prolog: A System for Defeasible Reasoning with Rules and Ontologies on the Semantic Web,"Nonmonotonic rule systems are expected to play an important role in the layered development of the semantic Web. Defeasible reasoning is a direction in nonmonotonic reasoning that is based on the use of rules that may be defeated by other rules. It is a simple, but often more efficient approach than other nonmonotonic rule systems for reasoning with incomplete and inconsistent information. This paper reports on the implementation of a system for defeasible reasoning on the Web. The system 1) is syntactically compatible with RuleML, 2) features strict and defeasible rules, priorities, and two kinds of negation, 3) is based on a translation to logic programming with declarative semantics, 4) is flexible and adaptable to different intuitions within defeasible reasoning, and 5) can reason with rules, RDF, RDF Schema, and (parts of) OWL ontologies","Ontologies,
Semantic Web,
OWL,
Resource description framework,
Computer science,
Logic programming,
Microstrip,
XML,
Employment"
A Probabilistic Coverage Protocol for Wireless Sensor Networks,"We propose a new probabilistic coverage protocol (denoted by PCP) that considers probabilistic sensing models. PCP is fairly general and can be used with different sensing models. In particular, PCP requires the computation of a single parameter from the adopted sensing model, while everything else remains the same. We show how this parameter can be derived in general, and we actually do the calculations for two example sensing models: (i) the probabilistic exponential sensing model, and (ii) the commonly-used deterministic disk sensing model. The first model is chosen because it is conservative in terms of estimating sensing capacity, and it has been used before in another probabilistic coverage protocol, which enables us to conduct a fair comparison. Because it is conservative, the exponential sensing model can be used as a first approximation for many other sensing models. The second model is chosen to show that our protocol can easily function as a deterministic coverage protocol. In this case, we compare our protocol against two recent deterministic protocols that were shown to outperform others in the literature. Our comparisons indicate that our protocol outperforms all other protocols in several aspects, including number of activated sensors and total energy consumed. We also demonstrate the robustness of our protocol against random node failures, node location inaccuracy, and imperfect time synchronization.",
Reputation Management Survey,"Electronic markets, distributed peer-to-peer applications and other forms of online collaboration are all based on mutual trust, which enables transacting peers to overcome the uncertainty and risk inherent in the environment. Reputation systems provide essential input for computational trust as predictions on future behaviour based on the past actions of a peer In order to analyze the maturity of current reputation systems, we compare eleven reputation systems within a taxonomy of the credibility aspects of a reputation system. The taxonomy covers three topics: 1) the creation and content of a recommendation, 2) the selection and use of recommenders, and 3) the interpretation and reasoning applied to the gathered information. Although we find it possible to form a trusted reputation management network over an open network environment, there are still many regulatory and technical obstacles to address. This survey reveals various good mechanisms and methods used, but the area still requires both a) formation of standard mechanisms and metrics for reputation system collaboration and b) standard metainformation of right granularity for evaluating the credibility of reputation information provided","Peer to peer computing,
Taxonomy,
Online Communities/Technical Collaboration,
Environmental management,
Consumer electronics,
Uncertainty,
Costs,
Risk management,
Computer science,
Informatics"
Centralized Scheduling and Channel Assignment in Multi-Channel Single-Transceiver WiMax Mesh Network,"The IEEE 802.16a standard defines WiMax mesh network, using the base station (BS) as a coordinator for the centralized scheduling. This paper proposes a centralized scheduling algorithm for WiMax mesh networks. In our scheme, each node has one transceiver and can be tuned between multiple channels, intending to eliminate the secondary interference for reducing the length of scheduling. We first study the problem when sufficient channels are supported, then extend our solution to the case with insufficient number of channels. Both the scheduling algorithm and the channel assignment strategies are included. The simulation results show that the multi-channel single-transceiver MAC can reduce the length of scheduling substantially as compared with the single channel system, and double channel may provide a performance similar to the multiple channels.","WiMAX,
Mesh networks,
Scheduling algorithm,
Communications Society,
Processor scheduling,
Computer science,
Physical layer,
Telecommunication traffic,
Time division multiple access,
Data communication"
"Optimal AUV path planning for extended missions in complex, fast-flowing estuarine environments","This paper addresses the problems of automatically planning autonomous underwater vehicle (AUV) paths which best exploit complex current data, from computational estuarine model forecasts, while also avoiding obstacles. In particular we examine the possibilities for a novel type of AUV mission deployment in fast flowing tidal river regions which experience bi-directional current flow. These environments are interesting in that, by choosing an appropriate path in space and time, an AUV may both bypass adverse currents which are too fast to be overcome by the vehicle's motors and also exploit favorable currents to achieve far greater speeds than the motors could otherwise provide, while substantially saving energy. The AUV can ""ride"" currents both up and down the river, enabling extended monitoring of otherwise energy-exhausting, fast flow environments. The paper discusses suitable path parameterizations, cost functions and optimization techniques which enable optimal AUV paths to be efficiently generated. These paths take maximum advantage of the river currents in order to minimize energy expenditure, journey time and other cost parameters. The resulting path planner can automatically suggest useful alternative mission start and end times and locations to those specified by the user. Examples are presented for navigation in a simple simulation of the fast flowing Hudson River waters around Manhattan.",
Continuous Evaluation of Monochromatic and Bichromatic Reverse Nearest Neighbors,"This paper presents a novel algorithm for Incremental and General Evaluation of continuous Reverse Nearest neighbor queries (IGERN, for short). The IGERN algorithm is general as it is applicable for both the monochromatic and bichromatic reverse nearest neighbor queries. The incremental aspect of IGERN is achieved through determining only a small set of objects to be monitored. While previous algorithms for monochromatic queries rely mainly on monitoring six pie regions, IGERN takes a radical approach by monitoring only a single region around the query object. The IGERN algorithm clearly outperforms the state-of-the-art algorithms in monochromatic queries. In addition, the IGERN algorithm presents the first attempt for continuous evaluation of bichromatic reverse nearest neighbor queries. The computational complexity of IGERN is presented in comparison to the state-of-the-art algorithms in the monochromatic case and to the use of Voronoi diagrams for the bichromatic case. In addition, the correctness of IGERN in both the monochromatic and bichromatic cases are proved. Extensive experimental analysis shows that IGERN is efficient, is scalable, and outperforms previous techniques for continuous reverse nearest neighbor queries.","Nearest neighbor searches,
Recurrent neural networks,
Monitoring,
Strategic planning,
Virtual reality,
Computer science,
Information science,
Educational institutions,
Computational complexity,
Query processing"
Intraclass Retrieval of Nonrigid 3D Objects: Application to Face Recognition,"As the size of the available collections of 3D objects grows, database transactions become essential for their management with the key operation being retrieval (query). Large collections are also precategorized into classes so that a single class contains objects of the same type (e.g., human faces, cars, four-legged animals). It is shown that general object retrieval methods are inadequate for intraclass retrieval tasks. We advocate that such intraclass problems require a specialized method that can exploit the basic class characteristics in order to achieve higher accuracy. A novel 3D object retrieval method is presented which uses a parameterized annotated model of the shape of the class objects, incorporating its main characteristics. The annotated subdivision-based model is fitted onto objects of the class using a deformable model framework, converted to a geometry image and transformed into the wavelet domain. Object retrieval takes place in the wavelet domain. The method does not require user interaction, achieves high accuracy, is efficient for use with large databases, and is suitable for nonrigid object classes. We apply our method to the face recognition domain, one of the most challenging intraclass retrieval tasks. We used the Face Recognition Grand Challenge v2 database, yielding an average verification rate of 95.2 percent at to 10-3 false accept rate. The latest results of our work can be found at http://www.cbl.uh.edu/UR8D/","Face recognition,
Deformable models,
Wavelet domain,
Transaction databases,
Information retrieval,
Humans,
Animals,
Shape,
Solid modeling,
Image converters"
A hybrid evolutionary approach to the university course timetabling problem,"Combinations of evolutionary based approaches with local search have provided very good results for a variety of scheduling problems. This paper describes the development of such an algorithm for university course timetabling. This problem is concerned with the assignment of lectures to specific timeslots and rooms. For a solution to be feasible, a number of hard constraints must be satisfied. The quality of the solution is measured in terms of a penalty value which represents the degree to which various soft constraints are satisfied. This hybrid evolutionary approach is tested over established datasets and compared against state-of-the-art techniques from the literature. The results obtained confirm that the approach is able to produce solutions to the course timetabling problem which exhibit some of the lowest penalty values in the literature on these benchmark problems. It is therefore concluded that the hybrid evolutionary approach represents a particularly effective methodology for producing high quality solutions to the university course timetabling problem.","Evolutionary computation,
Random number generation,
Genetic mutations"
Temporal Change Analysis for Characterization of Mass Lesions in Mammography,"In this paper, we present a fully automated computer-aided diagnosis (CAD) program to detect temporal changes in mammographic masses between two consecutive screening rounds. The goal of this work was to improve the characterization of mass lesions by adding information about the tumor behavior over time. Towards this goal we previously developed a regional registration technique that finds for each mass lesion on the current view a location on the prior view where the mass was most likely to develop. For the task of interval change analysis, we designed two kinds of temporal features: difference features and similarity features. Difference features indicate the (relative) change in feature values determined on prior and current views. These features may be especially useful for lesions that are visible on both views. Similarity features measure whether two regions are comparable in appearance and may be useful for lesions that are visible on the prior view as well as for newly developing lesions. We evaluated the classification performance with and without the use of temporal features on a dataset consisting of 465 temporal mammogram pairs, 238 benign, and 227 malignant. We used cross validation to partition the dataset into a training set and a test set. The training set was used to train a support vector machine classifier and the test set to evaluate the classifier. The average Az value (area under the receiver operating characteristic curve) for classifying each lesion was 0.74 without temporal features and 0.77 with the use of temporal features. The improvement obtained by adding temporal features was statistically significant (P = 0.005). In particular, similarity features contributed to this improvement. Furthermore, we found that the improvement was comparable for masses that were visible and for masses that were not visible on the prior view. These results show that the use of temporal features is an effective approach to improve the characterization of masses.","Lesions,
Mammography,
Cancer,
Shape,
Neoplasms,
Testing,
Benign tumors,
Computer aided diagnosis,
Support vector machines,
Support vector machine classification"
"WireVis: Visualization of Categorical, Time-Varying Data From Financial Transactions","Large financial institutions such as Bank of America handle hundreds of thousands of wire transactions per day. Although most transactions are legitimate, these institutions have legal and financial obligations in discovering those that are suspicious. With the methods of fraudulent activities ever changing, searching on predefined patterns is often insufficient in detecting previously undiscovered methods. In this paper, we present a set of coordinated visualizations based on identifying specific keywords within the wire transactions. The different views used in our system depict relationships among keywords and accounts over time. Furthermore, we introduce a search-by-example technique which extracts accounts that show similar transaction patterns. In collaboration with the Anti-Money Laundering division at Bank of America, we demonstrate that using our tool, investigators are able to detect accounts and transactions that exhibit suspicious behaviors.","Data visualization,
Wire,
Law,
Legal factors,
Data mining,
Collaborative tools,
Computer graphics,
Financial management,
Risk management,
Risk analysis"
Approximation algorithm for the temperature-aware scheduling problem,"The paper addresses the problem of performance optimization for a set of periodic tasks with discrete voltage/frequency states under thermal constraints. We prove that the problem is NP-hard, and present a pseudo-polynomial optimal algorithm and a fully polynomial time approximation technique (FPTAS) for the problem. The FPTAS technique is able to generate solutions in polynomial time that are guaranteed to be within a designer specified quality bound (QB) (say within 1% of the optimal). We evaluate our techniques by experimentation with multimedia and synthetic benchmarks mapped on the 70 nm CMOS technology processor. The experimental results demonstrate our techniques are able to match optimal solutions when QB is set at 5%, can generate solutions that arc quite close to optimal (< 5%) even when QB is set at higher values (50%), and executes in few seconds (with QB > 25%) for large task sets with 120 nodes (while the optimal solution takes several hundred seconds). We also analyze the effect of different thermal parameters, such as the initial temperature, the final temperature and the thermal resistance.","Approximation algorithms,
Scheduling algorithm,
Thermal resistance,
Polynomials,
CMOS technology,
Temperature,
Optimization,
Voltage,
Frequency,
CMOS process"
An Empirical Study of Test Case Filtering Techniques Based on Exercising Information Flows,"Some software defects trigger failures only when certain local or nonlocal program interactions occur. Such interactions are modeled by the closely related concepts of information flows, program dependences, and program slices. The latter concepts underlie a 78 variety of proposed test data adequacy criteria, and they form a potentially important basis for filtering existing test cases. We report the results of an empirical study of several test case filtering techniques that are based on exercising information flows. Both coverage-based and profile-distribution-based filtering techniques are considered. They are compared to filtering techniques based on exercising simpler program elements, such as basic blocks, branches, function calls, and call pairs, with respect to their effectiveness for revealing defects.","Information filtering,
Information filters,
Automatic testing,
Software testing,
Computer Society,
Instruments,
Computer science,
Joining processes,
Timing"
Bits through ARQs: Spectrum Sharing with a Primary Packet System,"We study a problem motivated by cognitive radio in which the primary is a packet system that employs ARQ feedback. A secondary system is allowed to transmit in the same frequency band provided it ensures that the primary attains a specified target rate. That is, the secondary has a certain ""interference budget"". The crux of the problem is that the secondary does not know how much interference it creates on the primary and therefore is ignorant of its interference budget. Absent this knowledge, we propose a scheme in which the secondary eavesdrops on the primary's ARQ and uses this knowledge to stay within its interference budget. Under certain assumptions, we show there exists an optimal rate-interference budget (RIB) tradeoff. We compare how far fixed strategies are from this RIB function as we vary the interference budget. Further, we exhibit a strategy that is optimal beyond a threshold interference budget and within 1 bit per primary packet elsewhere.","Cognitive radio,
Automatic repeat request,
Radio transmitters,
Statistics,
Feedback,
Receivers,
Interference channels,
Frequency,
Radio spectrum management,
FCC"
Parallel PSO using MapReduce,"In optimization problems involving large amounts of data, such as web content, commercial transaction information, or bioinformatics data, individual function evaluations may take minutes or even hours. particle swarm optimization (PSO) must be parallelized for such functions. However, large-scale parallel programs must communicate efficiently, balance work across all processors, and address problems such as failed nodes. We present mapreduce particle swarm optimization (MRPSO), a PSO implementation based on the mapreduce parallel programming model. We describe MapReduce and show how PSO can be naturally expressed in this model, without explicitly addressing any of the details of parallelization. We present a benchmark function for evaluating MRPSO and note that MRPSO is not appropriate for optimizing easily evaluated functions. We demonstrate that MRPSO scales to 256 processors on moderately difficult problems and tolerates node failures.","Particle swarm optimization,
Large-scale systems,
Load management,
Parallel programming,
Fault tolerance,
Concurrent computing,
Space exploration,
Bioinformatics,
Robustness,
Computer networks"
TEXEMS: Texture Exemplars for Defect Detection on Random Textured Surfaces,"We present an approach to detecting and localizing defects in random color textures which requires only a few defect free samples for unsupervised training. It is assumed that each image is generated by a superposition of various-size image patches with added variations at each pixel position. These image patches and their corresponding variances are referred to here as textural exemplars or texems. Mixture models are applied to obtain the texems using multiscale analysis to reduce the computational costs. Novelty detection on color texture surfaces is performed by examining the same-source similarity based on the data likelihood in multiscale, followed by logical processes to combine the defect candidates to localize defects. The proposed method is compared against a Gabor filter bank-based novelty detection method. Also, we compare different texem generalization schemes for defect detection in terms of accuracy and efficiency.","Surface texture,
Gabor filters,
Inspection,
Textiles,
Filter bank,
Ceramics,
Displays,
Markov random fields"
Data Dissemination with Ring-Based Index for Wireless Sensor Networks,"In wireless sensor networks, sensor nodes are capable of not only measuring real world phenomena, but also storing, processing, and transferring these measurements. Many techniques have been proposed for disseminating sensing data. However, most of them are not efficient in the scenarios where a huge amount of sensing data are generated, but only a small portion of them are queried. In this paper, we first propose an index-based data dissemination scheme to address the problem. With this scheme, sensing data are collected, processed, and stored at the nodes close to the detecting nodes, and the location information of these storing nodes is pushed to some index nodes, which act as the rendezvous points for sinks and sources. To address the issues of fault tolerance and load balance, we extend the scheme with an adaptive ring-based index (ARI) technique in which the index nodes for one event type form a ring surrounding the location which is determined by the event type, and the ring can be dynamically reconfigured. Considering that frequently updating or querying index nodes may cause high overhead, we also propose a lazy index updating (LIU) mechanism and a lazy index querying (LIQ) mechanism to reduce the overhead. Analysis and simulations are conducted to evaluate the performance of the proposed scheme. The results show that the proposed scheme outperforms the external storage-based scheme, the DCS scheme, and the local storage-based schemes with flood-response style. The results also show that using ARI can tolerate clustering failures and achieve load balance and using LIU (LIQ) can further improve the system performance. is pushed to some index nodes,","Wireless sensor networks,
Sensor phenomena and characterization,
Distributed control,
Event detection,
Floods,
Fault tolerance,
Performance analysis,
Analytical models,
System performance,
Production"
Social Network Extraction of Academic Researchers,"This paper addresses the issue of extraction of an academic researcher social network. By researcher social network extraction, we are aimed at finding, extracting, and fusing the 'semantic '-based profiling information of a researcher from the Web. Previously, social network extraction was often undertaken separately in an ad-hoc fashion. This paper first gives a formalization of the entire problem. Specifically, it identifies the 'relevant documents' from the Web by a classifier. It then proposes a unified approach to perform the researcher profiling using conditional random fields (CRF). It integrates publications from the existing bibliography datasets. In the integration, it proposes a constraints-based probabilistic model to name disambiguation. Experimental results on an online system show that the unified approach to researcher profiling significantly outperforms the baseline methods of using rule learning or classification. Experimental results also indicate that our method to name disambiguation performs better than the baseline method using unsupervised learning. The methods have been applied to expert finding. Experiments show that the accuracy of expert finding can be significantly improved by using the proposed methods.",
Diffusion Tensor Analysis With Invariant Gradients and Rotation Tangents,"Guided by empirically established connections between clinically important tissue properties and diffusion tensor parameters, we introduce a framework for decomposing variations in diffusion tensors into changes in shape and orientation. Tensor shape and orientation both have three degrees-of-freedom, spanned by invariant gradients and rotation tangents, respectively. As an initial demonstration of the framework, we create a tunable measure of tensor difference that can selectively respond to shape and orientation. Second, to analyze the spatial gradient in a tensor volume (a third-order tensor), our framework generates edge strength measures that can discriminate between different neuroanatomical boundaries, as well as creating a novel detector of white matter tracts that are adjacent yet distinctly oriented. Finally, we apply the framework to decompose the fourth-order diffusion covariance tensor into individual and aggregate measures of shape and orientation covariance, including a direct approximation for the variance of tensor invariants such as fractional anisotropy.","Tensile stress,
Diffusion tensor imaging,
Shape measurement,
Image edge detection,
Anisotropic magnetoresistance,
Magnetic resonance imaging,
Biomedical imaging,
Biological tissues,
Muscles,
Image processing"
Constructing PCA Baseline Algorithms to Reevaluate ICA-Based Face-Recognition Performance,"The literature on independent component analysis (ICA)-based face recognition generally evaluates its performance using standard principal component analysis (PCA) within two architectures, ICA Architecture I and ICA Architecture II. In this correspondence, we analyze these two ICA architectures and find that ICA Architecture I involves a vertically centered PCA process (PCA I), while ICA Architecture II involves a whitened horizontally centered PCA process (PCA II). Thus, it makes sense to use these two PCA versions as baselines to reevaluate the performance of ICA-based face-recognition systems. Experiments on the FERET, AR, and AT&T face-image databases showed no significant differences between ICA Architecture I (II) and PCA I (II), although ICA Architecture I (or II) may, in some cases, significantly outperform standard PCA. It can be concluded that the performance of ICA strongly depends on the PCA process that it involves. Pure ICA projection has only a trivial effect on performance in face recognition.",
Cluster-Based Forwarding for Reliable End-to-End Delivery in Wireless Sensor Networks,"Providing efficient and reliable communication in wireless sensor networks is a challenging problem. To recover from corrupted packets, previous approaches have tried to use retransmissions and FEC mechanisms. The energy efficiency of these mechanisms, however, is very sensitive to unreliable links. In this paper, we present cluster-based forwarding, where each node forms a cluster such that any node in the next-hop's cluster can take forwarding responsibility. This architecture, designed specifically for wireless sensor networks, achieves better energy-efficiency by reducing retransmissions. Cluster-based forwarding is not a routing protocol. Rather, it is designed as an extension layer that can augment existing routing protocols. Using simulations, we demonstrate that cluster-based forwarding is effective in improving both end-to-end energy efficiency and latency of current routing protocols.","Wireless sensor networks,
Routing protocols,
Mobile ad hoc networks,
Forward error correction,
Energy efficiency,
Telecommunication network reliability,
Wireless application protocol,
Access protocols,
Computer network reliability,
Computer science"
On the relationship between the algebraic connectivity and graph's robustness to node and link failures,"We study the algebraic connectivity in relation to the graph's robustness to node and link failures. Graph's robustness is quantified with the node and the link connectivity, two topological metrics that give the number of nodes and links that have to be removed in order to disconnect a graph. The algebraic connectivity, i.e. the second smallest eigenvalue of the Laplacian matrix, is a spectral property of a graph, which is an important parameter in the analysis of various robustness-related problems. In this paper we study the relationship between the proposed metrics in three well-known complex network models: the random graph of Erdos-Renyi, the small-world graph of Watts-Strogatz and the scale-free graph of Barabasi-Albert. From (Fielder, 1973) it is known that the algebraic connectivity is a lower bound on both the node and the link connectivity. Through extensive simulations with the three complex network models, we show that the algebraic connectivity is not trivially connected to graph's robustness to node and link failures. Furthermore, we show that the tightness of this lower bound is very dependent on the considered complex network model.","Robustness,
Laplace equations,
Eigenvalues and eigenfunctions,
Complex networks,
Network topology,
Upper bound,
Mathematics,
Computer science,
Predictive models,
Graph theory"
Best Practices Involving Teamwork in the Classroom: Results From a Survey of 6435 Engineering Student Respondents,"A teamwork survey was conducted at Oakland University, Rochester, MI, in 533 engineering and computer science courses over a two-year period. Of the 6435 student respondents, 4349 (68%) reported working in teams. Relative to the students who only worked individually, the students who worked in teams were significantly more likely to agree that the course had achieved its stated learning objectives (p < 0.001). Regression analysis showed that roughly one-quarter of the variance in belief about whether the objectives were met could be explained by four factors: 1) student satisfaction with the team experience; 2) the presence of instructor guidance related to teamwork; 3) the presence of slackers on teams; and 4) team size. Pearson product-moment correlations revealed statistically significant associations between agreement that the course objectives had been fulfilled and the use of student teams and between satisfaction with teams and the occurrences of instructor guidance on teamwork skills. These and other results suggest that assigning work to student teams can lead to learning benefits and student satisfaction, provided that the instructor pays attention to how the teams and the assignments are set up.",
About Penetration Testing,"Students generally learn red teaming, sometimes called penetration testing or ethical hacking, as ""breaking into your own system to see how hard it is to do so"". Contrary to this simplistic view, a penetration test requires a detailed analysis of the threats and potential attackers in order to be most valuable. Using the results of penetration testing requires proper interpretation. Neither testers nor sponsors should assert that the penetration test has found all possible flaws, or that the failure to find flaws means that the system is secure. All types of testing can show only the presence of flaws and never the absence of them. The best that testers can say is that the specific flaws they looked for and failed to find aren't present: this can give some idea of the overall security of the system's design and implementation.","System testing,
Permission,
Information security,
Computer crime,
Vehicles,
Degradation,
Protection,
Privacy"
Arm-Training with T-WREX After Chronic Stroke: Preliminary Results of a Randomized Controlled Trial,"This study presents preliminary results of a randomized controlled trial comparing a novel passive arm orthosis training system, the Therapy Wilmington Robotic Exoskeleton (T-WREX), with conventional self-directed upper extremity exercises. Chronic stroke survivors (n = 23) with moderate to severe upper limb hemiparesis trained three times per week for eight weeks with minimal supervision from an occupational therapist. Both groups demonstrated significant improvements in arm movement ability according to the Fugl-Meyer (3.7 point mean improvement in T-WREX group, p = 0.001, and 2.7 point improvement in control group, p = 0.003). Individuals who completed T-WREX training also demonstrated significant gains in self-rated quality of arm movement on the Motor Activity Log (p=0.05), and showed a trend towards greater gains on all clinical measures, although this trend was not significant at the current study size. Post-treatment surveys revealed a subjective preference for T-WREX training over conventional gravity-supported exercises. These preliminary results suggest that the T-WREX is a safe device feasible for clinical use, and effective in enhancing upper extremity motor recovery and patient motivation. Next steps are discussed.","Medical treatment,
Extremities,
Rehabilitation robotics,
Control systems,
Aerospace engineering,
Gravity,
Robots,
Exoskeletons,
Current measurement,
Gain measurement"
Axis-Based Virtual Coordinate Assignment Protocol and Delivery-Guaranteed Routing Protocol in Wireless Sensor Networks,"In this paper, we propose a method of constructing a virtual coordinate system (ABVCap) in wireless sensor networks where location information is not available. A routing protocol based on ABVCap virtual coordinates is also introduced. Our routing protocol guarantees packet delivery and does not require computing and storing of the global topological features. Using simulations, we evaluate the performance of the proposed routing protocol (ABVCap routing), the greedy routing protocol based on VCap virtual coordinates (VCap routing), the greedy routing protocol based on physical coordinates (Euclidean routing), greedy perimeter stateless routing (GPSR routing), and geometric spanner routing (GSR routing). The simulations show that our method guarantees packet delivery while ensuring moderate routing path length overhead costs.","Routing protocols,
Wireless application protocol,
Wireless sensor networks,
Chemical technology,
Global Positioning System,
Iterative algorithms,
Communications Society,
Computer science,
Electronic mail,
Computational modeling"
Single Pass Fuzzy C Means,"Recently several algorithms for clustering large data sets or streaming data sets have been proposed. Most of them address the crisp case of clustering, which cannot be easily generalized to the fuzzy case. In this paper, we propose a simple single pass (through the data) fuzzy c means algorithm that neither uses any complicated data structure nor any complicated data compression techniques, yet produces data partitions comparable to fuzzy c means. We also show our simple single pass fuzzy c means clustering algorithm when compared to fuzzy c means produces excellent speed-ups in clustering and thus can be used even if the data can be fully loaded in memory. Experimental results using five real data sets are provided.","Clustering algorithms,
Sampling methods,
Data structures,
Fuzzy sets,
Partitioning algorithms,
Image sampling,
Data compression,
Statistics,
Data analysis,
Intrusion detection"
A Particle Swarm Optimization Algorithm with Differential Evolution,"Differential evolution (DE) is a simple evolutionary algorithm that has shown superior performance in the global continuous optimization. It mainly utilizes the differential information to guide its further search. But the differential information also results in instability of performance. Particle swarm optimization (PSO) has been developing rapidly and has been applied widely since it is introduced, as it can converge quickly. But PSO easily got stuck in local optima because it easily loses the diversity of swarm. This paper proposes a combination of DE and PSO (termed DEPSO) that makes up their disadvantages. DEPSO combines the differential information obtained by DE with the memory information extracted by PSO to create the promising solutions. Finally, DEPSO is tested to solve several benchmark optimization problems. The experimental results show the effectiveness of DEPSO algorithm for the multimodal function, and also verify that DEPSO can perform better than other algorithms (DE, CPSO) in solving the benchmark problems.",
Image Matching via Saliency Region Correspondences,"We introduce the notion of co-saliency for image matching. Our matching algorithm combines the discriminative power of feature correspondences with the descriptive power of matching segments. Co-saliency matching score favors correspondences that are consistent with 'soft' image segmentation as well as with local point feature matching. We express the matching model via a joint image graph (JIG) whose edge weights represent intra-as well as inter-image relations. The dominant spectral components of this graph lead to simultaneous pixel-wise alignment of the images and saliency-based synchronization of 'soft' image segmentation. The co-saliency score function, which characterizes these spectral components, can be directly used as a similarity metric as well as a positive feedback for updating and establishing new point correspondences. We present experiments showing the extraction of matching regions and pointwise correspondences, and the utility of the global image similarity in the context of place recognition.","Image matching,
Image segmentation,
Pixel,
Object recognition,
Feature extraction,
Layout,
Coherence,
Manifolds,
Information science,
Feedback"
Numerical Function Generators Using LUT Cascades,"This paper proposes an architecture and a synthesis method for high-speed computation of fixed-point numerical functions such as trigonometric, logarithmic, sigmoidal, square root, and combinations of these functions. Our architecture is based on the lookup table (LUT) cascade, which results in a significant reduction in circuit complexity compared to traditional approaches. This is suitable for automatic synthesis and we show a synthesis method that converts a Matlab-like specification into an LUT cascade design. Experimental results show the efficiency of our approach as implemented on a field-programmable gate array (FPGA)","table lookup,
fixed point arithmetic,
mathematics computing"
Event-Condition-Action Systems for Reconfigurable Logic Control,"The contribution of this paper is the introduction of the event-condition-action (ECA) paradigm for the design of modular logic controllers that are reconfigurable. ECA rules have been used extensively to specify the behavior of active database and expert systems and are recognized as a highly reconfigurable tool to design reactive behavior. This paper develops a method to design modular logic controllers whose dynamics are governed by ECA rules, with the ultimate goal of producing reconfigurable control. Modularity, integrability, and diagnosability measures that have in the past been used to measure the reconfigurability of manufacturing systems are used to assess the reconfigurability of the developed controllers. For the modularity measure, criteria found in computer science to evaluate the modularity of object-oriented programs are adapted to evaluate the modularity of modular logic controllers. The results of this paper are that reconfigurability is highly dependent on the level of modularity of the logic control system, and that not all ""modular"" structures are reconfigurable. There are approaches, such as the one shown in this paper using ECA rules, that can greatly increase the modularity, integrability, and diagnosability of the logic control system, thus increasing its reconfigurability. Note to Practitioners-This paper has been motivated by the problem of designing reconfigurable modular logic controllers. Reconfiguration is important in manufacturing, but it has also been an issue in the software design domain. There are software systems that currently exist, such as active data bases or expert systems with very powerful reconfiguration capabilities enabled by event-condition-action (ECA) rules. This paper applies the ECA concept to the design of modular logic controllers. This paper begins by describing what an ECA logic system is and then focuses on how ECA logic systems can be implemented with modular control approaches. To this end, two designs are considered. First, modular finite state machines are used to construct ECA logic systems, and a theoretical framework is built using this approach. Three qualitative measures for reconfigurability (modularity, integrability, and diagnosability) are presented and the controllers are evaluated using these measures. Second, an implementation using the IEC 61499 function block standard is presented as it is a widely understood and accepted standard for modular control applications. Future work entails theoretical analysis using modular verification techniques that exploit a controller structure","Reconfigurable logic,
Control systems,
Logic design,
IEC standards,
Databases,
Diagnostic expert systems,
Design methodology,
Manufacturing systems,
Computer science,
Software design"
Shape Variation-Based Frieze Pattern for Robust Gait Recognition,"Gait is an attractive biometric for vision-based human identification. Previous work on existing public data sets has shown that shape cues yield improved recognition rates compared to pure motion cues. However, shape cues are fragile to gross appearance variations of an individual, for example, walking while carrying a ball or a backpack. We introduce a novel, spatiotemporal shape variation-based frieze pattern (SVB frieze pattern) representation for gait, which captures motion information over time. The SVB frieze pattern represents normalized frame difference over gait cycles. Rows/columns of the vertical/horizontal SVB frieze pattern contain motion variation information augmented by key frame information with body shape. A temporal symmetry map of gait patterns is also constructed and combined with vertical/horizontal SVB frieze patterns for measuring the dissimilarity between gait sequences. Experimental results show that our algorithm improves gait recognition performance on sequences with and without gross differences in silhouette shape. We demonstrate superior performance of this computational framework over previous algorithms using shape cues alone on both CMU MoBo and UoS HumanID gait databases.","Shape,
Robustness,
Pattern recognition,
Humans,
Image databases,
Biometrics,
Legged locomotion,
Testing,
Spatiotemporal phenomena,
Hidden Markov models"
Valence-arousal evaluation using physiological signals in an emotion recall paradigm,"The work presented in this paper aims at assessing human emotions using peripheral as well as electroencephalographic (EEG) physiological signals. Three specific areas of the valence-arousal emotional space are defined, corresponding to negatively excited, positively excited, and calm-neutral states. An acquisition protocol based on the recall of past emotional events has been designed to acquire data from both peripheral and EEG signals. Pattern classification is used to distinguish between the three areas of the valence-arousal space. The performance of two classifiers has been evaluated on different features sets: peripheral data, EEG data, and EEG data with prior feature selection. Comparison of results obtained using either peripheral or EEG signals confirms the interest of using EEG's to assess valence and arousal in emotion recall conditions.","Electroencephalography,
Emotion recognition,
Speech analysis,
Nervous system,
Humans,
Target recognition,
Turning,
Head,
Cameras,
Central nervous system"
Autonomy and Common Ground in Human-Robot Interaction: A Field Study,"The use of robots, especially autonomous mobile robots, to support work is expected to increase over the next few decades. However, little empirical research examines how users form mental models of robots, how they collaborate with them, and what factors contribute to the success or failure of human-robot collaboration. A two-year observational study of a collaborative human-robot system suggests that the factors disrupting the creation of common ground for interactive communication change at different levels of robot autonomy. Our observations of users collaborating with the remote robot showed differences in how the users reached common ground with the robot in terms of an accurate, shared understanding of the robot's context, planning, and actions - a process called grounding. We focus on how the types and levels of robot autonomy affect grounding. We also examine the challenges a highly autonomous system presents to people's ability to maintain a shared mental model of the robot","Collaboration,
Robot kinematics,
Human robot interaction,
Mobile robots,
Collaborative work,
Robot sensing systems,
Cognitive science,
Grounding,
Ground support,
Microorganisms"
Rear Vehicle Detection and Tracking for Lane Change Assist,"A monocular vision based rear vehicle detection and tracking system is presented for Lane Change Assist (LCA), which does not need road boundary and lane information. Our algorithm extracts regions of interest (ROI) using the shadow underneath a vehicle, and accurately localizes vehicle regions in ROI by vehicle features such as symmetry, edge and shadow underneath vehicles. The algorithm realizes vehicle verification by combining knowledge-based and learning-based methods. During vehicle tracking, templates are dynamically created on-line, tracking window is adaptively adjusted with motion estimation, and confidence is determined for tracked vehicle. The algorithm was tested under various traffic scenes at different daytime, the result illustrated good performance.","Vehicle detection,
Vehicles,
Tracking,
Roads,
Change detection algorithms,
Data mining,
Learning systems,
Motion estimation,
Testing,
Layout"
Characterizing Overlay Multicast Networks and Their Costs,"Overlay networks among cooperating hosts have recently emerged as a viable solution to several challenging problems, including multicasting, routing, content distribution, and peer-to-peer services. Application-level overlays, however, incur a performance penalty over router-level solutions. This paper quantifies and explains this performance penalty for overlay multicast trees via: 1) Internet experimental data; 2) simulations; and 3) theoretical models. We compare a number of overlay multicast protocols with respect to overlay tree structure, and underlying network characteristics. Experimental data and simulations illustrate that the mean number of hops and mean per-hop delay between parent and child hosts in overlay trees generally decrease as the level of the host in the overlay tree increases. Overlay multicast routing strategies, overlay host distribution, and Internet topology characteristics are identified as three primary causes of the observed phenomenon. We show that this phenomenon yields overlay tree cost savings: Our results reveal that the normalized cost L(n)/U(n) is propn0.9 for small n, where L(n) is the total number of hops in all overlay links, U(n) is the average number of hops on the source to receiver unicast paths, and n is the number of members in the overlay multicast session. This can be compared to an IP multicast cost proportional to n0.6 to n0.8","Costs,
Routing,
Internet,
Delay,
Tree data structures,
Network topology,
Unicast,
Computer science,
Bandwidth,
Analytical models"
Optimal subchannel assignment in a two-hop OFDM relay,The paper studies subchannel assignment in a two-hop OFDM relay system in which the transmitting nodes (source and relay) have access to channel information and interference-related information. We show that with L-superadditive relay (performance) functions a simple ranking of subchannels leads to the optimal assignment with a very low computational complexity. Numerical results quantify the benefit of subchannel assignment in a frequency-selective channel.,
A Linear-Time Two-Scan Labeling Algorithm,"This paper presents a fast linear-time two-scan algorithm for labeling connected components in binary images. In the first scan, provisional labels are assigned to object pixels in the same way as do most conventional labeling algorithms. To improve efficiency, we use corresponding equivalent label sets and a representative label table for resolving label equivalences. When the first scan is finished, all provisional labels belonging to each connected component in a given image are combined in the corresponding equivalent label set, and they are assigned a unique representative label with the representative label table. During the second scan, by use of the completed representative label table, all provisional labels belonging to each connected component are replaced by their representative label. Our algorithm is very simple in principle, and is easy to implement. Experimental results demonstrated that the efficiency of our algorithm is superior to that of other labeling algorithms.","Labeling,
Pixel,
Pattern recognition,
Chaos,
Radiology,
Biology,
Image processing,
Computer vision,
Noise reduction,
Interpolation"
Topologically Clean Distance Fields,"Analysis of the results obtained from material simulations is important in the physical sciences. Our research was motivated by the need to investigate the properties of a simulated porous solid as it is hit by a projectile. This paper describes two techniques for the generation of distance fields containing a minimal number of topological features, and we use them to identify features of the material. We focus on distance fields defined on a volumetric domain considering the distance to a given surface embedded within the domain. Topological features of the field are characterized by its critical points. Our first method begins with a distance field that is computed using a standard approach, and simplifies this field using ideas from Morse theory. We present a procedure for identifying and extracting a feature set through analysis of the MS complex, and apply it to find the invariants in the clean distance field. Our second method proceeds by advancing a front, beginning at the surface, and locally controlling the creation of new critical points. We demonstrate the value of topologically clean distance fields for the analysis of filament structures in porous solids. Our methods produce a curved skeleton representation of the filaments that helps material scientists to perform a detailed qualitative and quantitative analysis of pores, and hence infer important material properties. Furthermore, we provide a set of criteria for finding the ""difference"" between two skeletal structures, and use this to examine how the structure of the porous solid changes over several timesteps in the simulation of the particle impact.","Laboratories,
Solid modeling,
Computer science,
Projectiles,
Materials science and technology,
Data analysis,
Data visualization,
Scientific computing,
Analytical models,
Feature extraction"
Manipulation Planning Among Movable Obstacles,"This paper presents the resolve spatial constraints (RSC) algorithm for manipulation planning in a domain with movable obstacles. Empirically we show that our algorithm quickly generates plans for simulated articulated robots in a highly nonlinear search space of exponential dimension. RSC is a reverse-time search that samples future robot actions and constrains the space of prior object displacements. To optimize the efficiency of RSC, we identify methods for sampling object surfaces and generating connecting paths between grasps and placements. In addition to experimental analysis of RSC, this paper looks into object placements and task-space motion constraints among other unique features of the three dimensional manipulation planning domain.","Orbital robotics,
Robot kinematics,
Motion planning,
Robotics and automation,
Robotic assembly,
Motion analysis,
Object detection,
Manipulators,
USA Councils,
Computer science"
An MSI Micromechanical Differential Disk-Array Filter,"A medium-scale integrated (MSI) vibrating micromechanical filter circuit that utilizes 128 radial-mode disk and mechanical link elements to achieve low motional resistance while suppressing unwanted modes and feedthrough signals has been demonstrated with a 0.06%-bandwidth insertion loss less than 2.5 dB at 163 MHz. The ability to attain an insertion loss this small for such a tiny percent bandwidth on chip is unprecedented and is made possible here by the availability of Q's >10,000 provided by capacitively transduced resonators. In particular, the MSI mechanical circuit is able to harness the high Q of capacitively transduced resonators while overcoming their impedance deficiencies via strategic mechanical circuit design methodologies, such as the novel use of wavelength-optimized resonator coupling to effect a differential mode of operation that substantially improves the stopband rejection of the filter response while also suppressing unwanted modes.",Micromechanical devices
Comparison of Large Margin Training to Other Discriminative Methods for Phonetic Recognition by Hidden Markov Models,"In this paper we compare three frameworks for discriminative training of continuous-density hidden Markov models (CD-HMMs). Specifically, we compare two popular frameworks, based on conditional maximum likelihood (CML) and minimum classification error (MCE), to a new framework based on margin maximization. Unlike CML and MCE, our formulation of large margin training explicitly penalizes incorrect decodings by an amount proportional to the number of mislabeled hidden states. It also leads to a convex optimization over the parameter space of CD-HMMs, thus avoiding the problem of spurious local minima. We used discriminatively trained CD-HMMs from all three frameworks to build phonetic recognizers on the TIMIT speech corpus. The different recognizers employed exactly the same acoustic front end and hidden state space, thus enabling us to isolate the effect of different cost functions, parameterizations, and numerical optimizations. Experimentally, we find that our framework for large margin training yields significantly lower error rates than both CML and MCE training.","Hidden Markov models,
Parameter estimation,
Maximum likelihood estimation,
Speech recognition,
Error analysis,
Automatic speech recognition,
Computer science,
Cepstral analysis,
Computer errors,
Maximum likelihood decoding"
On the dynamic resource availability in grids,"Currently deployed grids gather together thousands of computational and storage resources for the benefit of a large community of scientists. However, the large scale, the wide geographical spread, and at times the decision of the rightful resource owners to commit the capacity elsewhere, raises serious resource availability issues. Little is known about the characteristics of the grid resource availability, and of the impact of resource unavailability on the performance of grids. In this work, we make first steps in addressing this twofold lack of information. First, we analyze a long-term availability trace and assess the resource availability characteristics of Grid'5000, an experimental grid environment of over 2,500 processors. The average utilization for the studied trace is increased by almost 5%, when availability is considered. Based on the results of the analysis, we further propose a model for grid resource availability. Our analysis and modeling results show that grid computational resources become unavailable at a high rate, negatively affecting the ability of grids to execute long jobs. Second, through trace-based simulation, we show evidence that resource availability can have a severe impact on the performance of the grid systems. The results of this step show evidence that the performance of a grid system can rise when availability is taken into consideration, and that human administration of availability change information results in 10-15 times more job failures than for an automated monitoring solution, even for a lowly utilized system.","Availability,
Grid computing,
Peer to peer computing,
Large-scale systems,
Computer networks,
Mathematics,
Computer science,
Computational modeling,
Humans,
Computerized monitoring"
Analytical Model for BitTorrent-Based Live Video Streaming,,
Powered Ankle-Foot Prosthesis for the Improvement of Amputee Ambulation,"This paper presents the mechanical design, control scheme, and clinical evaluation of a novel, motorized ankle-foot prosthesis, called MIT Powered Ankle-Foot Prosthesis. Unlike a conventional passive-elastic ankle-foot prosthesis, this prosthesis can provide active mechanical power during the stance period of walking. The basic architecture of the prosthesis is a unidirectional spring, configured in parallel with a force-controllable actuator with series elasticity. With this architecture, the ankle-foot prosthesis matches the size and weight of the human ankle, and is also capable of delivering high mechanical power and torque observed in normal human walking. We also propose a biomimetic control scheme that allows the prosthesis to mimic the normal human ankle behavior during walking. To evaluate the performance of the prosthesis, we measured the rate of oxygen consumption of three unilateral transtibial amputees walking at self-selected speeds to estimate the metabolic walking economy. We find that the powered prosthesis improves amputee metabolic economy from 7% to 20% compared to the conventional passive-elastic prostheses (Flex-Foot Ceterus and Freedom Innovations Sierra), even though the powered system is twofold heavier than the conventional devices. This result highlights the benefit of performing net positive work at the ankle joint to amputee ambulation and also suggests a new direction for further advancement of an ankle-foot prosthesis.","Prosthetics,
Legged locomotion,
Humans,
Springs,
Actuators,
Elasticity,
Torque,
Biomimetics,
Velocity measurement,
Technological innovation"
Dynamic Channel Assignment in IEEE 802.11 Networks,"We design a dynamic channel assignment algorithm for IEEE 802.11 wireless networks. Our algorithm assigns channels dynamically in a way that minimizes channel interference generated by neighboring access points (APs) on a reference access point, resulting in higher throughput. We implement and simulate our algorithm using two versions (I: pick rand and II: pick first) and different number of APs (4, 9, 16, and 25). Analysis of our algorithm shows an improvement by a factor of 4 (by lowering the total interference on an AP by 6 dBm on average) over default settings of having all APs use the same channel. As the number of APs is increased in a given service area, dynamic channel assignment becomes crucial; otherwise overlapping channel interference becomes a limiting factor.","Interference,
Frequency,
Throughput,
Heuristic algorithms,
Algorithm design and analysis,
Resource management,
Wireless LAN,
Computer science,
Wireless networks,
Degradation"
Data Consistency Based Rigid Motion Artifact Reduction in Fan-Beam CT,"It is well known that a rigid in-plane motion can be decomposed into a translation and a rotation around an origin. Based on our previous work, we first extend the Helgason-Ludwig consistency condition (HLCC) to cover a general rigid motion in fan-beam geometry. Then, we model the general motion by several parameters, and develop an iterative scheme for estimation of the in-plane motion parameters. This scheme determines the motion parameters by numerically minimizing an objective function constructed based on the HLCC. After the motion parameters are estimated, image reconstruction can be performed to compensate for the motion effects. Finally, we implement the algorithm and evaluate its performance in numerical simulations","Computed tomography,
Motion estimation,
Parameter estimation,
Image reconstruction,
Geometry,
Head,
Radiology,
Cities and towns,
Iterative algorithms,
Numerical simulation"
Choosing Between Open- and Closed-Loop Experiments in Linear System Identification,"This correspondence shows that open-loop experiments are optimal for a broad class of systems when the system input is constrained. In addition, we show that, for a general class of systems, when the output power is constrained, closed-loop experiments are optimal. Both results use a strong notion of optimality and use expressions for estimation accuracy which are nonasymptotic in model order but asymptotic in data length.","Linear systems,
Power generation,
Power system modeling,
Statistics,
Design engineering,
Cost function,
Delay,
Computer science,
Australia,
White noise"
A Flexible Content Adaptation System Using a Rule-Based Approach,"Content adaptation is an important technique for mobile devices. Existing content adaptation systems have been developed with specific adaptation goals. In this paper, we present an extensible content adaptation system, Xadaptor. We take a rule-based approach to facilitate extensible, systematic, and adaptive content adaptation. It integrates adaptation mechanisms for various content types and organizes them into the rule base. Rules are invoked based on the individual client information. We classify HTML page objects into structure, content, and pointer objects. Existing content adaptation techniques mainly focus on content objects and do not consider adaptation for structure and pointer objects. In Xadaptor, novel adaptation techniques for the structure object HTML table have been developed. We use fuzzy logic to model the adaptation quality and guide the adaptation decision. To demonstrate the feasibility of our approach, we have implemented a prototype system. Experimental studies show that Xadaptor is capable of on-the-fly content adaptation and is easily extensible","Computer displays,
Streaming media,
Adaptive systems,
HTML,
Mobile computing,
IP networks,
Computer networks,
Transcoding,
Fuzzy logic"
A Comparison of Static Architecture Compliance Checking Approaches,"The software architecture is one of the most important artifacts created in the lifecycle of a software system. It enables, facilitates, hampers, or interferes directly the achievement of business goals, functional and quality requirements. One instrument to determine how adequate the architecture is for its intended usage is architecture compliance checking. This paper compares three static architecture compliance checking approaches (reflexion models, relation conformance rules, and component access rules) by assessing their applicability in 13 distinct dimensions. The results give guidance on when to use which approach.","Computer architecture,
Software architecture,
Software systems,
Computer languages,
Packaging,
Software engineering,
Instruments,
Computer science,
Connectors,
Java"
"Concept and Design of A Fully Autonomous Sewer Pipe Inspection Mobile Robot ""KANTARO""","In current conventional method, the sewer pipe inspection is undertaken using a cable-tethered robot with an on-board video camera system, completely, tele-operated by human operator. All commercial sewer inspection robots have platforms with poor mobility functions so that those robots are only capable to move into the straight pipes. Inspecting the sewage pipes using the state of the arts inspection methods by the current robots is costly, mostly human cost, and not fast enough to check and inspect the amount of sewage pipes will grow stronger than it has actually happened, specially in Japan. In order to realize inexpensive and effective inspection system, an autonomous pipe inspection method should be introduced to improve the inspection efficiency by reducing the time and manpower in the inspection process. The development of a fully autonomous pipe inspection system, requires design and development of an un-tethered mobile robot equipped with the required sensors using for autonomous pipe assessment and damage detection, and capability of navigating, completely, autonomously inside of sewer networks including different types of pipe-bends such as curves and junctions. KANTARO, presented in this paper, is the prototype of a passive-active intelligent, fully autonomous, un-tethered robot which has an intelligent modular architecture in its sensor and mechanism. KANTARO prototype robot, including a novel passive-active intelligent moving mechanism, can move into the straight pipe and pass various kinds of pipe bends without need to any intelligence of the controller or sensor reading. In order to realize a fully autonomous inspection robot, we also developed a small and intelligent 2D laser scanner for detecting of the navigational landmarks, independently with the main computer system, and fusion with a fish eye camera to assess the pipe state and fault detection.","Inspection,
Mobile robots,
Intelligent sensors,
Intelligent robots,
Robot sensing systems,
Robot vision systems,
Cameras,
Humans,
Navigation,
Prototypes"
Structural Analysis of fMRI Data Revisited: Improving the Sensitivity and Reliability of fMRI Group Studies,"Group studies of functional magnetic resonance imaging datasets are usually based on the computation of the mean signal across subjects at each voxel (random effects analyses), assuming that all subjects have been set in the same anatomical space (normalization). Although this approach allows for a correct specificity (rate of false detections), it is not very efficient for three reasons: i) its underlying hypotheses, perfect coregistration of the individual datasets and normality of the measured signal at the group level are frequently violated; ii) the group size is small in general, so that asymptotic approximations on the parameters distributions do not hold; iii) the large size of the images requires some conservative strategies to control the false detection rate, at the risk of increasing the number of false negatives. Given that it is still very challenging to build generative or parametric models of intersubject variability, we rely on a rule based, bottom-up approach: we present a set of procedures that detect structures of interest from each subject's data, then search for correspondences across subjects and outline the most reproducible activation regions in the group studied. This framework enables a strict control on the number of false detections. It is shown here that this analysis demonstrates increased validity and improves both the sensitivity and reliability of group analyses compared with standard methods. Moreover, it directly provides information on the spatial position correspondence or variability of the activated regions across subjects, which is difficult to obtain in standard voxel-based analyses.","Data analysis,
Magnetic analysis,
Magnetic resonance imaging,
Image analysis,
Signal analysis,
Neuroimaging,
Size measurement,
Size control,
Parametric statistics,
Information analysis"
On the Access Pricing and Network Scaling Issues of Wireless Mesh Networks,"Distributed wireless mesh network technology is ready for public deployment in the near future. However, without an incentive system, one should not assume that private self-interested wireless nodes would participate in such a public network and cooperate in the packet forwarding service. This paper studies the use of pricing as an incentive mechanism for stimulating participation and collaboration in public wireless mesh networks. Our focus is on the ""economic behavior"" of the network nodes-the pricing and purchasing strategies of the access point, wireless relaying nodes, and clients. We use a ""game-theoretic approach"" to analyze their interactions from one-hop to multihop networks and when the network has an unlimited or limited channel capacity. The important results that we show are that the access point and relaying wireless nodes will adopt a simple yet optimal fixed-rate pricing strategy in a multihop network with an unlimited capacity. However, the access price grows quickly with the hop distance between a client and the access point, which may limit the ""scalability"" of the wireless mesh network. In case where the network has limited capacity, the optimal strategy for the access point is to vary the access charge and even interrupt service to connecting clients. To this end, we focus on the access point adopting a non-self-enforcing but more practical ""fixed-rate noninterrupted service"" model and propose an algorithm based on the Markovian decision theory to devise the optimal pricing strategy. Results show that the scalability of a network with limited capacity is upper bounded by one with an unlimited capacity. We believe that this work will shed light on the deployment and pricing issues of distributed public wireless mesh networks.","Wireless communication,
Pricing,
Ad hoc networks,
Biological system modeling,
Games,
Communication system security,
Internet,
Markov processes"
pDCS: Security and Privacy Support for Data-Centric Sensor Networks,"The demand for efficient data dissemination/access techniques to find the relevant data from within a sensor network has led to the development of data-centric sensor networks (DCS), where the sensor data as contrast to sensor nodes are named based on attributes such as event type or geographic location. However, saving data inside a network also creates security problems due to the lack of tamper-resistance of the sensor nodes and the unattended nature of the sensor network. For example, an attacker may simply locate and compromise the node storing the event of his interest. To address these security problems, we present pDCS, a privacy-enhanced DCS network which offers different levels of data privacy based on different cryptographic keys. In addition, we propose several query optimization techniques based on Euclidean Steiner Tree and Keyed Bloom Filter to minimize the query overhead while providing certain query privacy. Finally, detailed analysis and simulations show that the Keyed Bloom Filter scheme can significantly reduce the message overhead with the same level of query delay and maintain a very high level of query privacy.","Data security,
Data privacy,
Animals,
Sensor phenomena and characterization,
Distributed control,
Peer to peer computing,
Filters,
Wireless sensor networks,
Computer science,
Monitoring"
Maximizing Non-Monotone Submodular Functions,"Submodular maximization generalizes many important problems including Max Cut in directed/undirected graphs and hypergraphs, certain constraint satisfaction problems and maximum facility location problems. Unlike the problem of minimizing submodular functions, the problem of maximizing submodular functions is NP-hard.","Computer science,
Mathematics,
Polynomials,
Greedy algorithms,
Algorithm design and analysis,
Approximation algorithms"
"Egocentric depth judgments in optical, see-through augmented reality","A fundamental problem in optical, see-through augmented reality (AR) is characterizing how it affects the perception of spatial layout and depth. This problem is important because AR system developers need to both place graphics in arbitrary spatial relationships with real-world objects, and to know that users will perceive them in the same relationships. Furthermore, AR makes possible enhanced perceptual techniques that have no real-world equivalent, such as x-ray vision, where AR users are supposed to perceive graphics as being located behind opaque surfaces. This paper reviews and discusses protocols for measuring egocentric depth judgments in both virtual and augmented environments, and discusses the well-known problem of depth underestimation in virtual environments. It then describes two experiments that measured egocentric depth judgments in AR. Experiment I used a perceptual matching protocol to measure AR depth judgments at medium and far-field distances of 5 to 45 meters. The experiment studied the effects of upper versus lower visual field location, the x-ray vision condition, and practice on the task. The experimental findings include evidence for a switch in bias, from underestimating to overestimating the distance of AR-presented graphics, at ~ 23 meters, as well as a quantification of how much more difficult the x-ray vision condition makes the task. Experiment II used blind walking and verbal report protocols to measure AR depth judgments at distances of 3 to 7 meters. The experiment examined real-world objects, real-world objects seen through the AR display, virtual objects, and combined real and virtual objects. The results give evidence that the egocentric depth of AR objects is underestimated at these distances, but to a lesser degree than has previously been found for most virtual reality environments. The results are consistent with previous studies that have implicated a restricted field-of-view, combined with an inability for observers to scan the ground plane in a near-to-far direction, as explanations for the observed depth underestimation.","Observers,
Legged locomotion,
Augmented reality,
Visualization,
Optical variables measurement"
Using GUI Run-Time State as Feedback to Generate Test Cases,"This paper presents a new automated model-driven technique to generate test cases by using feedback from the execution of a ""seed test suite"" on an application under test (AUT). The test cases in the seed suite are designed to be generated automatically and executed very quickly. During their execution, feedback obtained from the AUT's run-time state is used to generate new, ""improved"" test cases. The new test cases subsequently become part of the seed suite. This ""anytime technique"" continues iteratively, generating and executing additional test cases until resources are exhausted or testing goals have been met. The feedback-based technique is demonstrated for automated testing of graphical user interfaces (GUIs). An existing abstract model of the GUI is used to automatically generate the seed test suite. It is executed; during its execution, state changes in the GUI pinpoint important relationships between GUI events, which evolve the model and help to generate new test cases. Together with a reverse- engineering algorithm used to obtain the initial model and seed suite, the feedback-based technique yields a fully automatic, end-to-end GUI testing process. A feasibility study on four large fielded open-source software (OSS) applications demonstrates that this process is able to significantly improve existing techniques and help identify/report serious problems in the OSS. In response, these problems have been fixed by the developers of the OSS in subsequent versions.",
Implementation and performance analysis of non-blocking collective operations for MPI,"Collective operations and non-blocking point-to-point operations have always been part of MPI. Although non-blocking collective operations are an obvious extension to MPI, there have been no comprehensive studies of this functionality. In this paper we present LibNBC, a portable high-performance library for implementing non-blocking collective MPI communication operations. LibNBC provides non-blocking versions of all MPI collective operations, is layered on top of MPI-1, and is portable to nearly all parallel architectures. To measure the performance characteristics of our implementation, we also present a microbenchmark for measuring both latency and overlap of computation and communication. Experimental results demonstrate that the blocking performance of the collective operations in our library is comparable to that of collective operations in other high-performance MPI implementations. Our library introduces a very low overhead between the application and the underlying MPI and thus, in conjunction with the potential to overlap communication with computation, offers the potential for optimizing real-world applications.","Performance analysis,
Libraries,
Open systems,
Laboratories,
Parallel programming,
Concurrent computing,
Computer architecture,
Permission,
Parallel processing,
Computer science"
The Challenges of Building Advanced Mechatronic Systems,"Mechatronics is an engineering discipline integrating the fields of mechanical engineering, electrical engineering and computer science. While the word ""mechatronics"" already has a long history, it is only the last ten years that we see their application all around us. Cars, CD players, washing machines, railways are all examples of mechatronic systems. The main characteristic (and driving force) of recent advances is the progressively tighter coupling of mechanic and electronic components with software. This makes software engineering (together with network technology) the main computer science discipline involved in mechatronics. In this paper we survey current developments and discuss future trends in mechatronics, in particular from a software engineering point of view. The future of mechatronics will specifically see a move towards a high degree of adaptibility and self-organisation. This poses new challenges on software engineering, especially on modelling, code generation and analysis. We exemplify existing as well as future strands by a collaborative research and development project of a mechatronic rail system from the University of Paderborn.","Mechatronics,
Software engineering,
Computer science,
Railway engineering,
Mechanical engineering,
Electrical engineering,
History,
Application software,
Washing machines,
Rail transportation"
Lexicographic Maxmin Fairness for Data Collection in Wireless Sensor Networks,"The ad hoc deployment of a sensor network causes unpredictable patterns of connectivity and varied node density, resulting in uneven bandwidth provisioning on the forwarding paths. When congestion happens, some sensors may have to reduce their data rates. It is an interesting but difficult problem to determine which sensors must reduce rates and how much they should reduce. This paper attempts to answer a fundamental question about congestion resolution: What are the maximum rates at which the individual sensors can produce data without causing congestion in the network and unfairness among the peers? We define the maxmin optimal rate assignment problem in a sensor network, where all possible forwarding paths are considered. We provide an iterative linear programming solution, which finds the maxmin optimal rate assignment and a forwarding schedule that implements the assignment in a low-rate sensor network. We prove that there is one and only one such assignment for a given configuration of the sensor network. We also study the variants of the maxmin fairness problem in sensor networks.","Wireless sensor networks,
Peer to peer computing,
Bandwidth,
Base stations,
Linear programming,
Monitoring,
Military computing,
Physical layer,
Sensor fusion"
Targeted Prostate Biopsy Using Statistical Image Analysis,"In this paper, a method for maximizing the probability of prostate cancer detection via biopsy is presented, by combining image analysis and optimization techniques. This method consists of three major steps. First, a statistical atlas of the spatial distribution of prostate cancer is constructed from histological images obtained from radical prostatectomy specimen. Second, a probabilistic optimization framework is employed to optimize the biopsy strategy, so that the probability of cancer detection is maximized under needle placement uncertainties. Finally, the optimized biopsy strategy generated in the atlas space is mapped to a specific patient space using an automated segmentation and elastic registration method. Cross-validation experiments showed that the predictive power of the optimized biopsy strategy for cancer detection reached the 94%-96% levels for 6-7 biopsy cores, which is significantly better than standard random-systematic biopsy protocols, thereby encouraging further investigation of optimized biopsy strategies in prospective clinical studies.",
Fast Human Pose Estimation using Appearance and Motion via Multi-Dimensional Boosting Regression,"We address the problem of estimating human pose in video sequences, where rough location has been determined. We exploit both appearance and motion information by defining suitable features of an image and its temporal neighbors, and learning a regression map to the parameters of a model of the human body using boosting techniques. Our algorithm can be viewed as a fast initialization step for human body trackers, or as a tracker itself. We extend gradient boosting techniques to learn a multi-dimensional map from (rotated and scaled) Haar features to the entire set of joint angles representing the full body pose. We test our approach by learning a map from image patches to body joint angles from synchronized video and motion capture walking data. We show how our technique enables learning an efficient real-time pose estimator, validated on publicly available datasets.","Humans,
Motion estimation,
Boosting,
Biological system modeling,
Legged locomotion,
Tracking,
Video sequences,
Joints,
Computer science,
Testing"
Recognizing Upper Body Postures using Textile Strain Sensors,"In this paper we present a garment prototype using strain sensors to recognize upper body postures. A novel thermoplastic elastomer strain sensor was used for measuring strain in the clothing. This sensor has a linear resistance response to strain, a small hysteresis and can be fully integrated into textile. A study was conducted with eight participants wearing the garment and performing a total of 27 upper body postures. A Naive Bayes classification was applied to identify the different postures. Nearly a complete recognition rate of 97% was achieved when the classification was adapted to the individual participant. A classification rate of 84% was achieved for an all-user classification and 65% for an independent user. These results show the feasibility to recognize postures with our setup, even in an unseen user setting. Furthermore, we used the garment prototype in a gym experiment to explore its potential for rehabilitation and fitness training. Intensity, speed and number of repetitions could be obtained from the garment sensor data.",
Aneka: Next-Generation Enterprise Grid Platform for e-Science and e-Business Applications,"In this paper, we present the design of Aneka, a .NET based service-oriented platform for desktop grid computing that provides: (i) a configurable service container hosting pluggable services for discovering, scheduling and balancing various types of workloads and (ii) a flexible and extensible framework/API supporting various programming models including threading, batch processing, MPI and dataflow. Users and developers can easily use different programming models and the services provided by the container to run their applications over desktop Grids managed by Aneka. We present the implementation of both the essential and advanced services within the platform. We evaluate the system with applications using the grid task and dataflow models on top of the infrastructure and conclude with some future directions of the current system.","Grid computing,
Object oriented modeling,
Containers,
Personal communication networks,
Application software,
Processor scheduling,
Data security,
Peer to peer computing,
Authentication,
Resource management"
Efficient Construction of Pipelined Multibit-Trie Router-Tables,"Efficient algorithms to construct multibit tries suitable for pipelined router-table applications are developed. We first enhance the 1-phase algorithm of Basu and Narlikar, obtaining a 1-phase algorithm that is 2.5 to 3 times as fast. Next, we develop 2-phase algorithms that not only guarantee to minimize the maximum per-stage memory but also guarantee to use the least total memory subject to the former constraint. Our 2-phase algorithms not only generate better pipelined trees than those generated by the 1-phase algorithm, but they also take much less time. A node pull-up scheme that guarantees no increase in maximum per-stage memory as well as a partitioning heuristic that generates pipelined multibit tries requiring less maximum per-stage memory than required by the tries obtained using the 1-phase and 2-phase algorithms are also proposed",
Estimating Optimal Parameters for MRF Stereo from a Single Image Pair,"This paper presents a novel approach for estimating the parameters for MRF-based stereo algorithms. This approach is based on a new formulation of stereo as a maximum a posterior (MAP) problem in which both a disparity map and MRF parameters are estimated from the stereo pair itself. We present an iterative algorithm for the MAP estimation that alternates between estimating the parameters while fixing the disparity map and estimating the disparity map while fixing the parameters. The estimated parameters include robust truncation thresholds for both data and neighborhood terms, as well as a regularization weight. The regularization weight can be either a constant for the whole image or spatially-varying, depending on local intensity gradients. In the latter case, the weights for intensity gradients are also estimated. Our approach works as a wrapper for existing stereo algorithms based on graph cuts or belief propagation, automatically tuning their parameters to improve performance without requiring the stereo code to be modified. Experiments demonstrate that our approach moves a baseline belief propagation stereo algorithm up six slots in the Middlebury rankings",
Using Register Lifetime Predictions to Protect Register Files against Soft Errors,"To increase the resistance of register files to soft errors, this paper presents the ParShield architecture. ParShield is based on two observations: (i) the data in a register is only useful for a small fraction of the register's lifetime, and (ii) not all registers are equally vulnerable. ParShield selectively protects registers by generating, storing, and checking the ECCs of only the most vulnerable registers while they contain useful data. In addition, it stores a parity bit for all the registers, re-using the ECC circuitry for parity generation and checking. ParShield has no SDC AVF and a small average DUE AVF of 0.040 and 0.010 for the integer and floating-point register files, respectively. ParShield consumes on average only 81% and 78% of the power of a design with full ECC for the SPECint and SPECfp applications, respectively. Finally, ParShield has no performance impact and little area requirements.","Protection,
Registers,
Error correction codes,
Computer errors,
Circuits,
Error correction,
Application software,
Computer science,
Computer architecture,
Voltage"
Vessel Axis Tracking Using Topology Constrained Surface Evolution,"An approach to 3-D vessel axis tracking based on surface evolution is presented. The main idea is to guide the evolution of the surface by analyzing its skeleton topology during evolution, and imposing shape constraints on the topology. For example, the intermediate topology can be processed such that it represents a single vessel segment, a bifurcation, or a more complex vascular topology. The evolving surface is then reinitialized with the newly found topology. Reinitialization is a crucial step since it creates probing behavior of the evolving front, encourages the segmentation process to extract the vascular structure of interest and reduces the risk on leaking of the curve into the background. The method was evaluated in two computed tomography angiography applications: 1) extracting the internal carotid arteries including the region in which they traverse through the skull base, which is challenging due to the proximity of bone structures and overlap in intensity values; 2) extracting the carotid bifurcations including many cases in which they are severely stenosed and contain calcifications. The vessel axis was found in 90% (18/20 internal carotids in ten patients) and 70% (14/20 carotid bifurcations in a different set of ten patients) of the cases","Topology,
Bifurcation,
Shape,
Biomedical imaging,
Computed tomography,
Angiography,
Skull,
Pathology,
Solid modeling,
Image segmentation"
Gaussian Processes and Reinforcement Learning for Identification and Control of an Autonomous Blimp,"Blimps are a promising platform for aerial robotics and have been studied extensively for this purpose. Unlike other aerial vehicles, blimps are relatively safe and also possess the ability to loiter for long periods. These advantages, however, have been difficult to exploit because blimp dynamics are complex and inherently non-linear. The classical approach to system modeling represents the system as an ordinary differential equation (ODE) based on Newtonian principles. A more recent modeling approach is based on representing state transitions as a Gaussian process (GP). In this paper, we present a general technique for system identification that combines these two modeling approaches into a single formulation. This is done by training a Gaussian process on the residual between the non-linear model and ground truth training data. The result is a GP-enhanced model that provides an estimate of uncertainty in addition to giving better state predictions than either ODE or GP alone. We show how the GP-enhanced model can be used in conjunction with reinforcement learning to generate a blimp controller that is superior to those learned with ODE or GP models alone.",
Kernel-based visual servoing,"Traditionally, visual servoing is separated into tracking and control subsystems. This separation, though convenient, is not necessarily well justified. When tracking and control strategies are designed independently, it is not clear how to optimize them to achieve a certain task. In this work, we propose a framework in which spatial sampling kernels - borrowed from the tracking and registration literature - are used to design feedback controllers for visual servoing. The use of spatial sampling kernels provides natural hooks for Lyapunov theory, thus unifying tracking and control and providing a framework for optimizing a particular servoing task. As a first step, we develop kernel-based visual servos for a subset of relative motions between camera and target scene. The subset of motions we consider are 2D translation, scale, and roll of the target relative to the camera. Our approach provides formal guarantees on the convergence/stability of visual servoing algorithms under putatively generic conditions.",
Agent-Based Approach to Handle Business Complexity in U.S. Wholesale Power Trading,"This study documents the practicality of an agent-based approach by examining how two groups of agents handle business complexity related to power trading. Three important findings are identified in this research. First, the proposed approach can estimate fluctuations of electricity prices as well as other well-known methods such as neural networks and genetic algorithms. Second, multiple learning capabilities incorporated in adaptive agents do not have an advantage over limited learning capabilities in predicting the market price of electricity. Finally, a theoretical extension of multiple learning capabilities may have potential for developing the agent-based approach for power trading",
Multilevel Huffman Coding: An Efficient Test-Data Compression Method for IP Cores,"A new test-data compression method suitable for cores of unknown structure is introduced in this paper. The proposed method encodes the test data provided by the core vendor using a new, very effective compression scheme based on multilevel Huffman coding. Each Huffman codeword corresponds to three different kinds of information, and thus, significant compression improvements compared to the already known techniques are achieved. A simple architecture is proposed for decoding the compressed data on chip. Its hardware overhead is very low and comparable to that of the most efficient methods in the literature. Moreover, the major part of the decompressor can be shared among different cores, which reduces the hardware overhead of the proposed architecture considerably. Additionally, the proposed technique offers increased probability of detection of unmodeled faults since the majority of the unknown values of the test sets are replaced by pseudorandom data generated by a linear feedback shift register",
Utility-based QoS Brokering in Service Oriented Architectures,"Quality of service (QoS) is an important consideration in the dynamic service selection in the context of service oriented architectures. This paper extends previous work on QoS brokering for SOAs by designing, implementing, and experimentally evaluating a service selection QoS broker that maximizes a utility function for service consumers. Utility functions allow stakeholders to ascribe a value to the usefulness of a system as a function of several attributes such as response time, throughput, and availability. This work assumes that consumers of services provide to a QoS broker their utility functions and their cost constraints on the requested services. Service providers register with the broker by providing service demands for each of the resources used by the services provided and cost functions for each of the services. Consumers request services from the QoS broker, which selects a service provider that maximizes the consumer's utility function subject to its cost constraint. The QoS broker uses analytic queuing models to predict the QoS values of the various services that could be selected under varying workload conditions. The broker and services were implemented using a J2EE/Weblogic platform and experiments were conducted to evaluate the broker's efficacy. Results showed that the broker adequately adapts its selection of service providers according to cost constraints.","Service oriented architecture,
Quality of service,
Cost function,
Availability,
Queueing analysis,
Predictive models,
Constraint optimization,
Runtime,
Performance analysis,
Computer science"
Power Management of Aggregate Electric Water Heater Loads by Voltage Control,"This paper discusses a voltage-control strategy for power management of aggregate residential electric water heater (EWH) load. It is shown that by proper control of EWH operating voltage, the aggregate EWH power demand during peak-load hours can be reduced without necessarily sacrificing customers' comfort level.","Water heating,
Energy management,
Aggregates,
Voltage control,
Power demand,
Load flow control,
Power system dynamics,
Temperature control,
Control systems,
Power system management"
A Generalized Fuzzy Clustering Regularization Model With Optimality Tests and Model Complexity Analysis,"In this paper, we propose a generalized fuzzy clustering regularization (GFCR) model and then study its theoretical properties. GFCR unifies several fuzzy clustering algorithms, such as fuzzy c-means (FCM), maximum entropy clustering (MEC), fuzzy clustering based on Fermi-Dirac entropy, and fuzzy bidirectional associative clustering network, etc. The proposed GFCR becomes an alternative model of the generalized FCM (GFCM) that was recently proposed by Yu and Yang. To advance theoretical study, we have the following three considerations. 1) We give an optimality test to monitor if GFCR converges to a local minimum. 2) We relate the GFCR optimality tests to Occam's razor principle, and then analyze the model complexity for fuzzy clustering algorithms. 3) We offer a general theoretical method to evaluate the performance of fuzzy clustering algorithms. Finally, some numerical experiments are used to demonstrate the validity of our theoretical results and complexity analysis.","Testing,
Clustering algorithms,
Partitioning algorithms,
Entropy,
Data structures,
Computer science education,
Educational programs,
Monitoring,
Performance analysis,
Algorithm design and analysis"
Energy-Efficient Localized Topology Control Algorithms in IEEE 802.15.4-Based Sensor Networks,"Sensor networks have emerged as a promising technology with various applications, where power efficiency is one of the critical requirements. The recent IEEE 802.15.4 standard offers a promising platform for wireless sensor networks. Since each node can act as a coordinator or a device in the IEEE 802.15.4 standard, 802.15.4-based sensor networks have various possible network topologies. To reduce power consumption, in this paper, we try to construct network topologies with a small number of coordinators while still maintaining network connectivity. By reducing the number of coordinators, the average duty cycle is reduced and the battery life is prolonged. Three topology control algorithms are proposed in this paper. Self-pruning (SP) is the simplest one with O(1) running time and provides the shortest path to the sink node. Ordinal pruning (OP) can significantly improve SP in terms of power saving with O(n) running time. Layered pruning (LP) is a trade off between the first two pruning algorithms with O(radicn) running time and has a slightly higher power consumption than OP. Furthermore, all three algorithms are independent of the physical radio propagation characteristics. Extensive simulations have been performed to verify the effectiveness of the proposed topology control schemes","Energy efficiency,
Network topology,
Energy consumption,
Sensor phenomena and characterization,
Wireless communication,
Monitoring,
Wireless sensor networks,
Batteries,
Communication system control,
Radio propagation"
Grasping POMDPs,"We provide a method for planning under uncertainty for robotic manipulation by partitioning the configuration space into a set of regions that are closed under compliant motions. These regions can be treated as states in a partially observable Markov decision process (POMDP), which can be solved to yield optimal control policies under uncertainty. We demonstrate the approach on simple grasping problems, showing that it can construct highly robust, efficiently executable solutions","Uncertainty,
Orbital robotics,
Motion planning,
Robotics and automation,
Robot sensing systems,
Feedback,
Optimal control,
Robustness,
Robot vision systems,
Shape"
Measurement and Classification of Out-of-Sequence Packets in a Tier-1 IP Backbone,"We present a classification methodology and a measurement study for out-of-sequence packets in TCP connections going over the Sprint IP backbone. Out-of-sequence packets can result from many events including loss, looping, reordering, or duplication in the network. It is important to quantify and understand the causes of such out-of-sequence packets since it is an indicator of the performance of a TCP connection, and the quality of its end-end path. Our study is based on passively observed packets from a point inside a large backbone network-as opposed to actively sending and measuring end-end probe traffic at the sender or receiver. A new methodology is thus required to infer the causes of a connection's out-of-sequence packets using only measurements taken in the ""middle"" of the connection's end-end path. We describe techniques that classify observed out-of-sequence behavior based only on the previously and subsequently-observed packets within a connection and knowledge of how TCP behaves. We analyze numerous several-hour packet-level traces from a set of OC-12 and OC-48 links for tens of millions connections generated in nearly 7600 unique ASes. We show that using our techniques, it is possible to classify almost all out-of-sequence packets in our traces and that we can quantify the uncertainty in our classification. Our measurements show a relatively consistent rate of out-of-sequence packets of approximately 4%. We observe that a majority of out-of-sequence packets are retransmissions, with a smaller percentage resulting from in-network reordering",
Optimizing File Availability in Peer-to-Peer Content Distribution,"A fundamental paradigm in peer-to-peer (P2P) content distribution is that of a large community of intermittently-connected nodes that cooperate to share files. Because nodes are intermittently connected, the P2P community must replicate and replace files as a function of their popularity to achieve satisfactory performance. In this paper, we develop an analytical optimization theory for benchmarking the performance of replication/replacement algorithms, including algorithms that employ erasure codes. We also consider a content management algorithm, the Top-K Most Frequently Requested algorithm, and show that in most cases this algorithm converges to an optimal replica profile. Finally, we present two approaches for achieving an evenly balanced load over all the peers in the community.",
Towards robotic self-reassembly after explosion,"This paper introduces a new challenge problem: designing robotic systems to recover after disassembly from high-energy events and a first implemented solution of a simplified problem. It uses vision-based localization for self- reassembly. The control architecture for the various states of the robot, from fully-assembled to the modes for sequential docking, are explained and inter-module communication details for the robotic system are described.","Explosions,
Intelligent robots,
Self-assembly,
Robustness,
USA Councils,
Explosives,
Earthquakes,
Protection,
Injuries"
MetaAware: Identifying Metamorphic Malware,"Detection of malicious software (malware) by the use of static signatures is often criticized for being overly simplistic. Available methods of obfuscating code (so-called metamorphic malware) will invalidate the use of a fixed signature, without changing the harmful effects of the software. This paper presents a new approach for recognizing metamorphic malware. The method uses fully automated static analysis of executables to summarize and compare program semantics, based primarily on the pattern of library or system functions which are called. The proposed method has been prototyped and evaluated using randomized benchmark programs, instances of known malware program variants, and utility software available in multiple releases. The results demonstrate three important capabilities of the proposed method: (a) it does well at identifying metamorphic variants of common malware; (h) it distinguishes easily between programs that are not related; and, (c) it can identify and detect program variations, or code reuse. Such variations can be due to insertion of malware (such as viruses) into the executable of a host program. We argue that this method of metamorphic code detection will be difficult for malware writers to bypass.","Protection,
Pattern matching,
Viruses (medical),
Character generation,
Computer security,
Application software,
Laboratories,
Computer science,
Pattern analysis,
Software libraries"
A Genetic Algorithm Based Architecture for Evolving Type-2 Fuzzy Logic Controllers for Real World Autonomous Mobile Robots,"The type-2 Fuzzy Logic Controller (FLC) has started to emerge as a promising control mechanism for autonomous mobile robots navigating in real world environments. This is because such robots need control mechanisms such as type-2 FLCs which can handle the large amounts of uncertainties present in real world environments. However, manually designing and tuning the type-2 Membership Functions (MFs) for an interval type-2 FLC to give a good response is a difficult task. This paper will present a Genetic Algorithm (GA) based architecture to evolve the type-2 MFs of interval type-2 FLCs for mobile robots that will navigate in real world environments. The GA based system converges after a small number of iterations to type-2 MFs which give a very good performance. We have performed a series of real world experiments in which the evolved type-2 FLCs controlled a real robot in an outdoor arena. The evolved type-2 FLCs dealt with the uncertainties present in the real world to give a very good performance that has outperformed their type-1 counterparts as well as the manually designed type-2 FLCs.",
Distributed Placement of Service Facilities in Large-Scale Networks,"The effectiveness of service provisioning in large-scale networks is highly dependent on the number and location of service facilities deployed at various hosts. The classical, centralized approach to determining the latter would amount to formulating and solving the uncapacitated k-median (UKM) problem (if the requested number of facilities is fixed), or the uncapacitated facility location (UFL) problem (if the number of facilities is also to be optimized). Clearly, such centralized approaches require knowledge of global topological and demand information, and thus do not scale and are not practical for large networks. The key question posed and answered in this paper is the following: ""How can we determine in a distributed and scalable manner the number and location of service facilities?"" We propose an innovative approach in which topology and demand information is limited to neighborhoods, or balls of small radius around selected facilities, whereas demand information is captured implicitly for the remaining (remote) clients outside these neighborhoods, by mapping them to clients on the edge of the neighborhood; the ball radius regulates the trade-off between scalability and performance. We develop a scalable, distributed approach that answers our key question through an iterative re-optimization of the location and the number of facilities within such balls. We show that even for small values of the radius (1 or 2), our distributed approach achieves performance under various synthetic and real Internet topologies that is comparable to that of optimal, centralized approaches requiring full topology and demand information.",
Scan chain design for three-dimensional integrated circuits (3D ICs),"Scan chains are widely used to improve the testability of IC designs. In traditional 2D IC designs, various design techniques on the construction of scan chains have been proposed to facilitate DFT (Design-For-Test). Recently, three-dimensional (3D) technologies have been proposed as a promising solution to continue technology scaling. In this paper, we study the scan chain construction for 3D ICs, examining the impact of 3D technologies on scan chain ordering. Three different 3D scan chain design approaches (namely, VIA3D, MAP3D, and OPT3D) are proposed and compared, with the experimental results for ISCAS89 benchmark circuits. The advantages as well as disadvantages for each approach are discussed. The results show that both MAP3D and VIA3D approaches require no changes of 2D scan chain algorithms, but OPT3D can achieve the best wire length reduction for the scan chain design. The average scan chain wire length of six ISCAS89 benchmarks obtained from OPT3D has 46.0% reduction compared to the 2D scan chain design. To the best of our knowledge, this is the first study on scan chain design for 3D integrated circuits.","integrated circuit testing,
design for testability,
integrated circuit design"
Steerable Random Fields,"In contrast to traditional Markov random field (MRF) models, we develop a steerable random field (SRF) in which the field potentials are defined in terms of filter responses that are steered to the local image structure. In particular, we use the structure tensor to obtain derivative responses that are either aligned with, or orthogonal to, the predominant local image structure, and analyze the statistics of these steered filter responses in natural images. Clique potentials are defined over steered filter responses using a Gaussian scale mixture model and are learned from training data. The SRF model connects random field models with anisotropic regularization and provides a statistical motivation for the latter. We demonstrate that steering the random field to the local image structure improves image denoising and inpainting performance compared with traditional pairwise MRFs.","Nonlinear filters,
Anisotropic magnetoresistance,
Image restoration,
Statistics,
Markov random fields,
Tensile stress,
Training data,
Image denoising,
History,
Pixel"
Coupling Metrics for Predicting Maintainability in Service-Oriented Designs,"Service-oriented computing (SOC) is emerging as a promising paradigm for developing distributed enterprise applications. Although some initial concepts of SOC have been investigated in the research literature, and related technologies are in the process of adoption by an increasing number of enterprises, the ability to measure the structural attributes of service-oriented designs thus predicting the quality of the final software product does not currently exist. Therefore, this paper proposes a set of metrics for quantifying the structural coupling of design artefacts in service-oriented systems. The metrics, which are validated against previously established properties of coupling, are intended to predict the quality characteristic of maintainability of service-oriented software. This is expected to benefit both research and industrial communities as existing object-oriented and procedural metrics are not readily applicable to the implementation of service-oriented systems.",
Open-Vocabulary Spoken Utterance Retrieval using Confusion Networks,"This paper presents a novel approach to open-vocabulary spoken utterance retrieval using confusion networks. If out-of-vocabulary (OOV) words are present in queries and the corpus, word-based indexing will not be sufficient. For this problem, we apply phone confusion networks and combine them with word confusion networks. With this approach, we can generate a more compact index table that enables robust keyword matching compared with typical lattice-based methods. In the retrieval experiments with speech recordings in MIT lecture corpus, our method using phone confusion networks outperformed lattice-based methods especially for OOV queries.","Lattices,
Automatic speech recognition,
Indexing,
Robustness,
Laboratories,
Error analysis,
Glass,
Computer science,
Artificial intelligence,
Audio recording"
Outlier Detection and Handling for Robust 3-D Active Shape Models Search,"This paper presents a new outlier handling method for volumetric segmentation with three-dimensional (3-D) active shape models. The method is based on a shape metric that is invariant to scaling, rotation and translation by using the ratio of interlandmark distances as a local shape dissimilarity measure. Tolerance intervals for the descriptors are calculated from the training samples and used as a statistical tolerance model to infer the validity of the feature points. A replacement point is then suggested for each outlier based on the tolerance model and the position of the valid points. A geometrically weighted fitness measure is introduced for feature point detection, which limits the presence of outliers and improves the convergence of the proposed segmentation framework. The algorithm is immune to the extremity of the outliers and can handle a highly significant presence of erroneous feature points. The practical value of the technique is validated with 3-D magnetic resonance (MR) segmentation tasks of the carotid artery and myocardial borders of the left ventricle","Robustness,
Active shape model,
Shape measurement,
Rotation measurement,
Weight measurement,
Computer vision,
Convergence,
Extremities,
Magnetic resonance,
Carotid arteries"
Opposition-based particle swarm algorithm with cauchy mutation,"Particle swarm optimization (PSO) has shown its fast search speed in many complicated optimization and search problems. However, PSO could often easily fall into local optima. This paper presents an Opposition-based PSO (OPSO) to accelerate the convergence of PSO and avoid premature convergence. The proposed method employs opposition-based learning for each particle and applies a dynamic Cauchy mutation on the best particle. Experimental results on many well- known benchmark optimization problems have shown that OPSO could successfully deal with those difficult multimodal functions while maintaining fast search speed on those simple unimodal functions in the function optimization.","Particle swarm optimization,
Genetic mutations,
Evolutionary computation,
Testing"
Requirements in the wild: How small companies do it,"Small companies form a large part of the software industry, but have mostly been overlooked by the requirements engineering research community. We know very little about the techniques these companies use to elicit and track requirements and about their contexts of operations. This paper presents preliminary results from an ongoing exploratory case study of requirements management in seven small companies, which found that (a) successful small companies exhibit a huge diversity of requirements practices that work well enough for their contexts; (b) these companies display strong cultural cohesion; (c) the principal of the company tends to retain control of the requirements processes long after other tasks have been delegated; and (d) the evidence rejects the simplistic view of a current ""software crisis"", as requirements errors for these companies, though problematic, are rarely catastrophic. We develop a number of hypotheses to explain these findings.",
SoftMAC: Layer 2.5 Collaborative MAC for Multimedia Support in Multihop Wireless Networks,"In this paper, we present the challenges in supporting multimedia, in particular, VoIP services over multihop wireless networks using commercial IEEE 802.11 MAC DCF hardware, and propose a novel software solution, called Layer 2.5 SoftMAC. Our proposed SoftMAC resides between the IEEE 802.11 MAC layer and the IP layer to coordinate the real-time (RT) multimedia and best-effort (BE) data packet transmission among neighboring nodes in a multihop wireless network. To effectively ensure acceptable VoIP services, channel busy time and collision rate need to be well controlled below appropriate levels. Targeted at this, our SoftMAC architecture employs three key mechanisms: 1) distributed admission control for regulating the load of RT traffic, 2) rate control for minimizing the impact of BT traffic on RT one, and 3) nonpreemptive priority queuing for providing high priority service to VoIP traffic. To evaluate the efficacy of these mechanisms, extensive simulations are conducted using the network simulator NS2. We also implement our proposed SoftMAC as a Windows network driver interlace specification (NDIS) driver and build a multihop wireless network testbed with 32 wireless nodes equipped with IEEE 802.11 a/b/g combo cards. Our evaluation and testing results demonstrate the effectiveness of our proposed software solution. Our proposed collaborative SoftMAC framework can also provide good support for A/V streaming in home networks where the network consists of hybrid WLAN (wireless LAN) and Ethernet",
Topology Repair of Solid Models Using Skeletons,"We present a method for repairing topological errors on solid models in the form of small surface handles, which often arise from surface reconstruction algorithms. We utilize a skeleton representation that offers a new mechanism for identifying and measuring handles. Our method presents two unique advantages over previous approaches. First, handle removal is guaranteed not to introduce invalid geometry or additional handles. Second, by using an adaptive grid structure, our method is capable of processing huge models efficiently at high resolutions.","Topology,
Solid modeling,
Skeleton,
Surface reconstruction,
Reconstruction algorithms,
Geometry,
Surface morphology,
Robustness,
Morphological operations,
Computer science"
Distributed detection of a nuclear radioactive source using fusion of correlated decisions,A distributed detection method is developed for the detection of a nuclear radioactive source using a small number of radiation counters. Local one bit decisions are made at each sensor over a period of time and a fusion center makes the global decision. A novel test for the fusion of correlated decisions is derived using the theory of copulas and optimal sensor thresholds are obtained using the Normal copula function. The performance of the derived fusion rule is compared with that of the Chair-Varshney rule. An increase in detection performance is observed. A method to estimate the correlation between the sensor observations using only the vector of sensor decisions is also proposed.,"Radiation detectors,
Sensor fusion,
Radioactive materials,
Sensor phenomena and characterization,
Weapons,
Computer science,
Mathematics,
Laboratories,
Testing,
Explosives"
Robust Real-Time Visual SLAM Using Scale Prediction and Exemplar Based Feature Description,Two major limitations of real-time visual SLAM algorithms are the restricted range of views over which they can operate and their lack of robustness when faced with erratic camera motion or severe visual occlusion. In this paper we describe a visual SLAM algorithm which addresses both of these problems. The key component is a novel feature description method which is both fast and capable of repeat-able correspondence matching over a wide range of viewing angles and scales. This is achieved in real-time by using a SIFT-like spatial gradient descriptor in conjunction with efficient scale prediction and exemplar based feature representation. Results are presented illustrating robust realtime SLAM operation within an office environment.,"Robustness,
Simultaneous localization and mapping,
Cameras,
Matched filters,
Robot vision systems,
Filtering,
Computer science,
Layout,
Wearable computers,
Stochastic processes"
Differential evolution for multiobjective optimization with self adaptation,"This paper presents performance assessment of differential evolution for multiobjective optimization with self adaptation algorithm, which uses the self adaptation mechanism from evolution strategies to adapt F and CR parameters of the candidate creation in DE. Results for several runs on CEC2007 special session test functions are presented and assessed with different performance metrics. Based on these metrics, algorithm strengths and weaknesses are discussed.","Chromium,
Measurement,
Evolutionary computation,
Genetic mutations,
Computer science,
Automatic testing,
Simulated annealing,
Performance evaluation,
Computer architecture,
Ground penetrating radar"
Efficient Least Squares Multimodal Registration With a Globally Exhaustive Alignment Search,"There are many image registration situations in which the initial misalignment of the two images is large. These registration problems, often involving comparison of the two images only within a region of interest (ROI), are difficult to solve. Most intensity-based registration methods perform local optimization of their cost function and often miss the global optimum when the initial misregistration is large. The registration of multimodal images makes the problem even more difficult since it limits the choice of available cost functions. We have developed an efficient method, capable of multimodal rigid-body registration within an ROI, that performs an exhaustive search over all integer translations, and a local search over rotations. The method uses the fast Fourier transform to efficiently compute the sum of squared differences cost function for all possible integer pixel shifts, and for each shift models the relationship between the intensities of the two images using linear regression. Test cases involving medical imaging, remote sensing and forensic science applications show that the method consistently brings the two images into close registration so that a local optimization method should have no trouble fine-tuning the solution.","Least squares methods,
Cost function,
Optimization methods,
Image registration,
Fast Fourier transforms,
Pixel,
Linear regression,
Medical tests,
Biomedical imaging,
Remote sensing"
Image Resolution Enhancement using Inter-Subband Correlation in Wavelet Domain,"Wavelet-based resolution enhancement methods improve the image resolution by estimating the high-frequency band information. In this paper, we propose a new resolution enhancement method using inter-subband correlation in which the sampling phase in DWT is considered. Interpolation filters are designed by analyzing correlations between subbands having different sampling phases in the lower level, and applied to the correlated subbands in the higher level. The filters are estimated under the assumption that correlations between two subbands in the higher level are similar to that in the lower level in DWT. The experimental results show that our proposed method outperforms the conventional interpolation methods including the other wavelet-based methods with respect to peak signal-to-noise-ratio (PSNR) as well as the subjective quality.","Image resolution,
Wavelet domain,
Frequency estimation,
Discrete wavelet transforms,
Interpolation,
Wavelet transforms,
Image sampling,
Filter bank,
Decoding,
Hidden Markov models"
Opportunistic MAC Protocols for Cognitive Radio Based Wireless Networks,"Recently, cognitive radio technology has attracted more and more attention since it is a novel and effective approach to improve the utilization of the precious radio spectrum. We propose MAC protocols for the cognitive radio based wireless networks. Specifically, the cognitive MAC protocols allow secondary users to identify and use the available frequency spectrum in a way that constrains the level of interference to the primary users. In our schemes, each secondary user is equipped with two transceivers. One of the transceivers is tuned to a dedicated control channel, while the other is used as a cognitive radio that can periodically sense and dynamically use an identified available channels. Our proposed schemes integrate the spectrum sensing at the PHY layer and packet scheduling at the MAC layer. Our schemes smoothly coordinate the two transceivers of the secondary users to enable them to collaboratively sense and dynamically utilize the available frequency spectrum.","Media Access Protocol,
Wireless application protocol,
Cognitive radio,
Wireless networks,
Transceivers,
Radiofrequency identification,
Frequency,
Interference constraints,
Radio control,
Physical layer"
"A Cooperative, Self-Configuring High-Availability Solution for Stream Processing","We present a collaborative, self-configuring high availability (HA) approach for stream processing that enables low-latency failure recovery while incurring small run-time overhead. Our approach relies on a novel fine-grained checkpointing model that allows query fragments at each server to be backed up at multiple other servers and recovered collectively (in parallel) when there is a failure. In this paper, we first address the problem of determining the appropriate query fragments at each server. We then discuss, for each fragment, which server to use as its backup as well as the proper checkpoint schedule. We also introduce and analyze operator-specific delta-checkpointing techniques to reduce the overall HA cost. Finally, we quantify the benefits of our approach using results from our prototype implementation and a detailed simulator.","Checkpointing,
Delay,
Patient monitoring,
Costs,
Computer science,
Collaboration,
Availability,
Runtime,
Virtual prototyping,
Safety"
Visualizing Design Patterns in Their Applications and Compositions,"Design patterns are generic design solutions that can be applied and composed in different applications where pattern-related information is generally implicit in the Unified Modeling Language (UML) diagrams of the applications. It is unclear in which pattern instances each modeling element, such as class, attribute, and operation, participates. It is hard for a designer to find the design patterns used in an application design. Consequently, the benefits of design patterns are compromised because designers cannot communicate with each other in terms of the design patterns they used and their design decisions and trade-offs. In this paper, we present a UML profile that defines new stereotypes, tagged values, and constraints for tracing design patterns in UML diagrams. These new stereotypes and tagged values are attached to a modeling element to explicitly represent the role the modeling element plays in a design pattern so that the user can identify the pattern in a UML diagram. Based on this profile, we also develop a Web service (tool) for explicitly visualizing design patterns in UML diagrams. With this service, users are able to visualize design patterns in their applications and compositions because pattern-related information can be dynamically displayed. A real-world case study and a comparative experiment with existing approaches are conducted to evaluate our approach.","Visualization,
Unified modeling language,
Object oriented modeling,
Software systems,
Application software,
Software design,
Web services,
Service oriented architecture,
Natural languages,
Production facilities"
Survey of Improving K-Nearest-Neighbor for Classification,"KNN (k-nearest-neighbor) has been widely used as an effective classification model. In this paper, we summarize three main shortcomings confronting KNN and single out three main methods for overcoming its three shortcomings. Keeping to these methods, we try our best to survey some improved algorithms and experimentally tested their effectiveness. Besides, we discuss some directions for future study on KNN.",
Constructing k-Connected m-Dominating Sets in Wireless Sensor Networks,"A k-Connected m-Dominating Set (kmCDS) working as a virtual backbone in a wireless sensor network is necessary for fault tolerance and routing flexibility. In order to construct a kmCDS with the minimum size, some approximation algorithms have been proposed in the literature. However, all of those algorithms only consider some special cases where k = 1,2 or k = m. In this paper, we propose one centralized heuristic algorithm CGA and one distributed algorithms, DDA which is deterministic, to construct a kmCDS for general k and m. Simulation results are also presented to evaluate our algorithms and the results show that our algorithms have better performances than the exiting other algorithms.",
Multi-Robot Area Patrol under Frequency Constraints,"This paper discusses the problem of generating patrol paths for a team of mobile robots inside a designated target area. Patrolling requires an area to be visited repeatedly by the robot(s) in order to monitor its current state. First, we present frequency optimization criteria used for evaluation of patrol algorithms. We then present a patrol algorithm that guarantees maximal uniform frequency, i.e., each point in the target area is covered at the same optimal frequency. This solution is based on finding a circular path that visits all points in the area, while taking into account terrain directionality and velocity constraints. Robots are positioned uniformly along this path, using a second algorithm. Moreover, the solution is guaranteed to be robust in the sense that uniform frequency of the patrol is achieved as long as at least one robot works properly.","Frequency,
Robot sensing systems,
Robustness,
Surveillance,
Mobile robots,
Humans,
Costs,
Monitoring,
Robotics and automation,
Computer science"
Toward a Theory of Shape from Specular Flow,"The image of a curved, specular (mirror-like) surface is a distorted reflection of the environment. The goal of our work is to develop a framework for recovering general shape from such distortions when the environment is neither calibrated nor known. To achieve this goal we consider far-field illumination, where the object-environment distance is relatively large, and we examine the dense specular flow that is induced on the image plane through relative object-environment motion. We show that under these very practical conditions the observed specular flow can be related to surface shape through a pair of coupled nonlinear partial differential equations. Importantly, this relationship depends only on the environment's relative motion and not its content. We examine the qualitative properties of these equations, present analytic methods for recovery of the shape in several special cases, and empirically validate our results using captured data. We also discuss the relevance to both computer vision and human perception.",
Improving hypervolume-based multiobjective evolutionary algorithms by using objective reduction methods,"Hypervolume based multiobjective evolutionary algorithms (MOEA) nowadays seem to be the first choice when handling multiobjective optimization problems with many, i.e., at least three objectives. Experimental studies have shown that hypervolume-based search algorithms as SMS-EMOA can outperform established algorithms like NSGA-II and SPEA2. One problem remains with most of the hypervolume based algorithms: the best known algorithm for computing the hypervolume needs time exponentially in the number of objectives. To save computation time during hypervolume computation which can be better spent in the generation of more solutions, we propose a general approach how objective reduction techniques can be incorporated into hypervolume based algorithms. Different objective reduction strategies are developed and then compared in an experimental study on two test problems with up to nine objectives. The study indicates that the (temporary) omission of objectives can improve hypervolume based MOEAs drastically in terms of the achieved hypervolume indicator values.",
Adaptive locomotion of a snake like robot based on curvature derivatives,"This paper presents locomotion control for a robotic snake that adaptively travels over rugged terrain using obstacles as supports. This kind of locomotion can typically be found in snakes in nature. The snake robot dealt with in this paper has an articulated structure in which all joints are actively driven. For locomotion on a flat terrain, joint torques distributed according to the curvature derivative of the body curve has been proved to be optimal under the condition that there is no lateral slippage at every part of the body. In this paper, the same method is applied to locomotion using two kinds of environmental supports; a narrow corridor surrounded by smoothly curved walls and a peg.","Tactile sensors,
Propulsion,
Intelligent robots,
Mobile robots,
Friction,
Prototypes,
Servomechanisms,
Robot sensing systems,
USA Councils"
RFID-Based Exploration for Large Robot Teams,"To coordinate a team of robots for exploration is a challenging problem, particularly in large areas as for example the devastated area after a disaster. This problem can generally be decomposed into task assignment and multi-robot path planning. In this paper, we address both problems jointly. This is possible because we reduce significantly the size of the search space by utilizing RFID tags as coordination points. The exploration approach consists of two parts: a stand-alone distributed local search and a global monitoring process which can be used to restart the local search in more convenient locations. Our results show that the local exploration works for large robot teams, particularly if there are limited computational resources. Experiments with the global approach showed that the number of conflicts can be reduced, and that the global coordination mechanism increases significantly the explored area.","Robot kinematics,
Path planning,
RFID tags,
Orbital robotics,
Robotics and automation,
Radiofrequency identification,
Monitoring,
State-space methods,
Grid computing,
Computer science"
Adaptive DWT-SVD Domain Image Watermarking Using Human Visual Model,"As digital watermarking has become an important tool for copyright protection, various watermarking schemes have been proposed in literature. Among them both discrete wavelet transform (DWT) and singular value decomposition (SVD) are commonly used. In a DWT-based watermarking scheme, the host image is decomposed into four frequency bands, and DWT coefficients in each band are modified to hide watermark information. Modification in all frequencies enables watermarking schemes using DWT robust to a wide range of attacks. However, as most transform methods, DWT decomposes images in terms of a standard basis set which is not necessarily optimal for a given image. By contrast with DWT, SVD offers a tailor-made basis for a given image which packs maximum signal energy into as few coefficients as possible. SVD is used in image processing also for its properties of stability, proportion invariance and rotation invariance. In this paper we propose a hybrid DWT-SVD domain watermarking scheme considering human visual properties. After decomposing the host image into four subbands, we apply SVD to each subband and embed singular values of the watermark into them. The embedding strength is determined by a human visual model proposed in A.S. Lewis and G. Knowles, (1992) and improved in M. Bertran et al., (2001). Our scheme has advantages of robustness for its embedding data into all frequencies and large capacity for using SVD. In addition, the use of human visual model guarantees the imperceptibility of the watermark.",
Symmetric Data Attachment Terms for Large Deformation Image Registration,"Nonrigid medical image registration between images that are linked by an invertible transformation is an inherently symmetric problem. The transformation that registers the image pair should ideally be the inverse of the transformation that registers the pair with the order of images interchanged. This property is referred to as symmetry in registration or inverse consistent registration. However, in practical estimation, the available registration algorithms have tended to produce inverse inconsistent transformations when the template and target images are interchanged. In this paper, we propose two novel cost functions in the large deformation diffeomorphic framework that are inverse consistent. These cost functions have symmetric data-attachment terms; in the first, the matching error is measured at all points along the flow between template and target, and in the second, matching is enforced only at the midpoint of the flow between the template and target. We have implemented these cost functions and present experimental results to validate their inverse consistent property and registration accuracy.","Image registration,
Cost function,
Biomedical imaging,
Image color analysis,
Image motion analysis,
Biomedical engineering,
Fluid flow measurement,
Computer applications,
Anatomy,
Back"
A TCAM-Based Parallel Architecture for High-Speed Packet Forwarding,"A partitioned TCAM-based search engine is presented that increases the packet forwarding rate multiple times over traditional TCAMs. The model works for IPv4 and IPv6 packet forwarding. Unlike the previous art, the improvement is achieved regardless of the incoming traffic pattern. Employing small and private memories that dynamically store popular route prefixes inside the ASIC and taking advantage of the inherent characteristics of Internet traffic to exploit parallelism make this improvement possible. Using four TCAM chips, an embodiment of the proposed model delivered more than six times the throughput of a conventional configuration with equal storage capacity and equal clock rate. Power consumption is also reduced in the new system. Other parameters such as storage density and table update performance are not adversely affected",
Non-Linear Index Coding Outperforming the Linear Optimum,"The following source coding problem was introduced by Birk and Kol: a sender holds a word x epsi {0,1}n, and wishes to broadcast a codeword to n receivers, R1,..., Rnmiddot. The receiver Ri is interested in x;, and has prior side information comprising some subset of the n bits. This corresponds to a directed graph G on n vertices, where ij is an edge iff Ri knows the bit xj . An index code for G is an encoding scheme which enables each Ri to always reconstruct Xj, given his side information. The minimal word length of an index code was studied by Bar-Yossef Birk, Jay ram and Kol. Thev introduced a graph parameter, minrk2(G), which completely characterizes the length of an optimal linear index code for G. The authors of (Z. Bar-Yossef, 2006) showed that in various cases linear codes attain the optimal word length, and conjectured that linear index coding is in fact always optimal. In this work, we disprove the main conjecture of (Z. Bar-Yossef, 2006) in the following strong sense: for any epsiv > 0 and sufficiently large n, there is an n-vertex graph G so that evety linear index code for G requires codewords of length at least n1-epsiv and yet a non-linear index code for G has a word length of nepsiv. This is achieved by an explicit construction, which extends Alon's variant of the celebrated Ramsey construction of Frankl and Wilson.",
Integrating Global and Local Structures: A Least Squares Framework for Dimensionality Reduction,"Linear discriminant analysis (LDA) is a popular statistical approach for dimensionality reduction. LDA captures the global geometric structure of the data by simultaneously maximizing the between-class distance and minimizing the within-class distance. However, local geometric structure has recently been shown to be effective for dimensionality reduction. In this paper, a novel dimensionality reduction algorithm is proposed, which integrates both global and local structures. The main contributions of this paper include: (1) We present a least squares formulation for dimensionality reduction, which facilities the integration of global and local structures; (2) We design an efficient model selection scheme for the optimal integration, which balances the tradeoff between the global and local structures; and (3) We present a detailed theoretical analysis on the intrinsic relationship between the proposed framework and LDA. Our extensive experimental studies on benchmark data sets show that the proposed integration framework is competitive with traditional dimensionality reduction algorithms, which use global or local structure only.",
Electrical characterization of trough silicon via (TSV) depending on structural and material parameters based on 3D full wave simulation,"In this paper, we show the electrical characteristics of TSV (through silicon via) depending on structural parameters such as TSV pitch, TSV height, TSV size and thickness of SiO2 for DC leakage blocking between TSV and silicon substrate, and material parameter of silicon substrate such as silicon resistivity in case of single silicon substrate. And we also show X-talk characteristics of two TSVs depending on distance of two signal TSVs and different locations of two signal TSVs and two ground TSVs in array type arrangement of TSV. Additionally, we show the electrical characteristics of TSV depending on number of stacked TSVs. All electrical characterizations on this paper are obtained using commercial 3-D full wave simulator and spice type circuit simulator such as HFSS of Ansoft Corporation and ADS of Agilent Corporation, respectively.",
A Human Action Recognition System for Embedded Computer Vision Application,"In this paper, we propose a human action recognition system suitable for embedded computer vision applications in security systems, human-computer interaction and intelligent environments. Our system is suitable for embedded computer vision application based on three reasons. Firstly, the system was based on a linear support vector machine (SVM) classifier where classification progress can be implemented easily and quickly in embedded hardware. Secondly, we use compacted motion features easily obtained from videos. We address the limitations of the well known motion history image (MHI) and propose a new hierarchical motion history histogram (HMHH) feature to represent the motion information. HMHH not only provides rich motion information, but also remains computationally inexpensive. Finally, we combine MHI and HMHH together and extract a low dimension feature vector to be used in the SVM classifiers. Experimental results show that our system achieves significant improvement on the recognition performance.","Humans,
Computer vision,
Application software,
Support vector machines,
Support vector machine classification,
History,
Computer security,
Machine intelligence,
Hardware,
Videos"
A Constant Approximation Algorithm for Interference Aware Broadcast in Wireless Networks,"Broadcast protocols play a vital role in multihop wireless networks. Due to the broadcast nature of radio signals, a node's interference range can be larger than its transmission range, i.e., it can interfere with other node's reception even if the latter is not within its transmission range. To design an efficient broadcast protocol, both the collision and the interference among multiple transmissions must be addressed. However, most of the previous works on wireless broadcast protocols either treated interference in the same way as collision or did not consider interference at all. In this paper, we study a more general model in which interference is distinguished from collision, and propose a simple and yet efficient interference and collision free broadcast protocol. Our objective is to minimize the makespan, i.e., the earliest time such that every node receives the message. By exploiting the geometry property of the nodes that interfere with each other, we show that our algorithm is a constant approximation algorithm, it guarantees to deliver the message to all nodes within a small constant factor of the optimal makespan. We apply our algorithm under both the unit disk graph model and the more realistic radio irregularity model. The experimental results show that our algorithm consistently outperforms the previous algorithms.",
How well do multi-objective evolutionary algorithms scale to large problems,"In spite of large amount of research work in multi- objective evolutionary algorithms, most have evaluated their algorithms on problems with only two to four objectives. Little has been done to understand the performance of the multi- objective evolutionary algorithms on problems with a larger number of objectives. It is unclear whether the conclusions drawn from the experiments on problems with a small number of objectives could be generalised to those with a large number of objectives. In fact, some of our preliminary work [1] has indicated that such generalisation may not be possible. This paper first presents a comprehensive set of experimental studies, which show that the performance of multi-objective evolutionary algorithms, such as NSGA-II and SPEA2, deteriorates substantially as the number of objectives increases. NSGA-II, for example, did not even converge for problems with six or more objectives. This paper analyses why this happens and proposes several new methods to improve the convergence of NSGA-II for problems with a large number of objectives. The proposed methods categorise members of an archive into small groups (non-dominated solutions with or without domination), using dominance relationship between the new and existing members in the archive. New removal strategies are introduced. Our experimental results show that the proposed methods clearly outperform NSGA-II in terms of convergence.",
Adaptive enhancement and noise reduction in very low light-level video,"A general methodology for noise reduction and contrast enhancement in very noisy image data with low dynamic range is presented. Video footage recorded in very dim light is especially targeted. Smoothing kernels that automatically adapt to the local spatio-temporal intensity structure in the image sequences are constructed in order to preserve and enhance fine spatial detail and prevent motion blur. In color image data, the chromaticity is restored and demosaicing of raw RGB input data is performed simultaneously with the noise reduction. The method is very general, contains few user-defined parameters and has been developed for efficient parallel computation using a GPU. The technique has been applied to image sequences with various degrees of darkness and noise levels, and results from some of these tests, and comparisons to other methods, are presented. The present work has been inspired by research on vision in nocturnal animals, particularly the spatial and temporal visual summation that allows these animals to see in dim light.",
New Perspectives on the Sources of White Matter DTI Signal,"A minimalist numerical model of white matter is presented, the objective of which is to help provide a biological basis for improved diffusion tensor imaging (DTI) analysis. Water diffuses, relaxes, and exchanges in three compartments-intracellular, extracellular, and myelin sheath. Exchange between compartments is defined so as to depend on the diffusion coefficients and the compartment sizes. Based on the model, it is proposed that an additive ldquobaseline tensorrdquo that correlates with intraaxonal water volume be included in the computation. Anisotropy and tortuosity calculated from such analysis may correspond better to tract ultrastructure than if calculated without the baseline. According to the model, reduced extracellular volume causes increased baseline and reduced apparent diffusion. Depending on the pulse sequence, reduced permeability can cause an increase in both the baseline and apparent diffusion.",
Generalized Kasami Sequences: The Large Set,"In this correspondence, new binary sequence families Fk of period 2n-1 are constructed for even n and any k with gcd(k,n)=2 if n/2 is odd or gcd(k,n)=1 if n/2 is even. The distribution of their correlation values is completely determined. These families have maximum correlation 2n/2+1 and family size 23n/2 + 2n/2 for odd n/2 or 23n/2+2n/2-1 for even n/2. The proposed families include the large set of Kasami sequences, where the k is taken as k=n/2+1.","Binary sequences,
Gold,
Multiaccess communication,
Autocorrelation,
Spread spectrum communication,
Satellites,
Information theory,
Mathematics,
Computer science"
A Supervised Learning Approach to Monaural Segregation of Reverberant Speech,"Room reverberation degrades speech signals and poses a major challenge to current monaural speech segregation systems. Previous research relies on inverse filtering as a front-end for partially restoring the harmonicity of the reverberant signal. We show that the inverse filtering approach is sensitive to different room configurations, hence undesirable in general reverberation conditions. We propose a supervised learning approach to map a set of harmonic features into a pitch based grouping cue for each time-frequency (T-F) unit. We use a speech segregation method to estimate an ideal binary T-F mask which retains the reverberant mixture in a local T-F unit if and only if the energy of target is stronger than interference energy. Results show that our approach improves the segregation performance considerably.",
Voltage Island Generation under Performance Requirement for SoC Designs,"Using multiple supply voltages on a SoC design is an efficient way to achieve low power. However, it may lead to a complex power network and a huge number of level shifters if we just set the cores to operate at their respective lowest voltage levels. We present two formulations for the voltage level assignment problem. The first is exact but takes longer time to compute a solution. The second can be solved much faster with virtually no loss on optimality. In addition, we propose a modification to the traditional floorplanning framework. Unlike previous works (Jingcao Hu et al., 2004) and (Hung et al., 2005), we can optimize the total power consumption, the level shifter overhead, and the power network complexity without compromising the wirelength and the chip area. In the experiments, we obtained 17- 53% power savings with voltage island generation.",
S3PAS: A Scalable Shoulder-Surfing Resistant Textual-Graphical Password Authentication Scheme,"The vulnerabilities of the textual password have been well known. Users tend to pick short passwords or passwords that are easy to remember, which makes the passwords vulnerable for attackers to break. Furthermore, textual password is vulnerable to shoulder-surfing, hidden-camera and spyware attacks. Graphical password schemes have been proposed as a possible alternative to text-based scheme. However, they are mostly vulnerable to shoulder-surfing. In this paper, we propose a Scalable Shoulder-Surfing Resistant Textual-Graphical Password Authentication Scheme (S3PAS). S3PAS seamlessly integrates both graphical and textual password schemes and provides nearly perfect resistant to shoulder-surfing, hidden-camera and spyware attacks. It can replace or coexist with conventional textual password systems without changing existing user password profiles. Moreover, it is immune to brute-force attacks through dynamic and volatile session passwords. S3PAS shows significant potential bridging the gap between conventional textual password and graphical password. Further enhancements of S3PAS scheme are proposed and briefly discussed. Theoretical analysis of the security level using S3PAS is also investigated.",
Multiplicative Updates for Nonnegative Quadratic Programming,"Many problems in neural computation and statistical learning involve optimizations with nonnegativity constraints. In this article, we study convex problems in quadratic programming where the optimization is confined to an axis-aligned region in the nonnegative orthant. For these problems, we derive multiplicative updates that improve the value of the objective function at each iteration and converge monotonically to the global minimum. The updates have a simple closed form and do not involve any heuristics or free parameters that must be tuned to ensure convergence. Despite their simplicity, they differ strikingly in form from other multiplicative updates used in machine learning. We provide complete proofs of convergence for these updates and describe their application to problems in signal processing and pattern recognition.",
New Insights into Image Processing of Cortical Blood Flow Monitors Using Laser Speckle Imaging,"Laser speckle imaging has increasingly become a viable technique for real-time medical imaging. However, the computational intricacies and the viewing experience involved limit its usefulness for real-time monitors such as those intended for neurosurgical applications. In this paper, we propose a new technique, tLASCA, which processes statistics primarily in the temporal direction using the laser speckle contrast analysis (LASCA) equation, proposed by Briers and Webster. This technique is thoroughly compared with the existing techniques for signal processing of laser speckle images, including, the spatial-based sLASCA and the temporal-based modified laser speckle imaging (mLSI) techniques. sLASCA is an improvement of the basic LASCA technique. In sLASCA, the derived contrasts are further averaged over a predetermined number of raw speckle images. mLSI, on the other hand, is the technique in which temporal statistics are processed using the equation described by Ohtsubo and Asakura. tLASCA preserves the original image resolution similar to mLSI. lLASCA outperforms sLASCA (window size M = 5) with faster convergence of A' values (5.32 versus 20.56 s), shorter per-frame processing time (0.34 versus 2.51 s), and better subjective and objective quality evaluations of contrast images. tLASCA also outperforms mLSI with faster convergence of K values (5.32 s) compared to N values (10.44 s), shorter per-frame processing time (0.34 versus 0.91 s), smaller intensity fluctuations among frames (8%-10% versus 15%-35%), and better subjective and objective quality evaluations of contrast images. As laser speckle imaging becomes an important tool for real-time monitoring of blood flows and vascular perfusion, tLASCA is proven to be the technique of choice.",
Noise Optimization of Charge Amplifiers With MOS Input Transistors Operating in Moderate Inversion Region for Short Peaking Times,"The noise of a fast charge sensitive amplifier (CSA) with an input MOS transistor operating in the moderate inversion region is discussed. The MOS transistor operation in the moderate inversion region becomes especially important in multichannel readout systems where limited power dissipation is required. The ENC of a CSA followed by a fast shaper is usually dominated by the voltage noise of the input MOS transistor. We carried out noise minimization for such a CSA, searching for an optimum input transistor width. The analyses were made using a simplified EKV model and were compared to HSPICE simulations using a BSIM3v3 model. We considered several CMOS technology generations with minimum transistor gate length ranging from 0.13 mum to 0.8 mum. We studied the sensitivity of ENC to the input transistor width, and propose a simple formula to estimate the optimum transistor width, which is valid in a wide current density range.","MOSFETs,
CMOS technology,
Voltage,
Power dissipation,
Multi-stage noise shaping,
Semiconductor device modeling,
Analytical models,
Current density,
Performance analysis,
Computer science"
Path-Sensitive Inference of Function Precedence Protocols,"Function precedence protocols define ordering relations among function calls in a program. In some instances, precedence protocols are well-understood (e.g., a call to pthread_mutex_init must always be present on all program paths before a call to pthread_mutex_lock). Oftentimes, however, these protocols are neither well- documented, nor easily derived. As a result, protocol violations can lead to subtle errors that are difficult to identify and correct. In this paper, we present CHRONICLER, a tool that applies scalable inter-procedural path-sensitive static analysis to automatically infer accurate function precedence protocols. Chronicler computes precedence relations based on a program's control-flow structure, integrates these relations into a repository, and analyzes them using sequence mining techniques to generate a collection of feasible precedence protocols. Deviations from these protocols found in the program are tagged as violations, and represent potential sources of bugs. We demonstrate CHRONICLER's effectiveness by deriving protocols for a collection of benchmarks ranging in size from 66 K to 2 M lines of code. Our results not only confirm the existence of bugs in these programs due to precedence protocol violations, but also highlight the importance of path sensitivity on accuracy and scalability.","Protocols,
Computer bugs,
Computer science,
Error correction,
Automatic generation control,
Scalability,
Programming,
Data structures,
Libraries,
Sockets"
Distributed Particle Filter for Target Tracking,"This paper investigates target tracking using a distributed particle filter over sensor networks. Gaussian mixture model is adopted to approximate the posterior distribution of weighted particles in this distributed particle filter. The parameters of Gaussian mixture model are exchanged between neighbor sensor nodes. Each node can obtain the Gaussian mixture model representing particle's posterior distribution through the parameter exchange. With the posterior distribution, the distributed particle filter can draw particles from it, predicted particles and observations, update particle weights, and re-sample particles based the predicted weights. The parameter exchange is key to implement the distributed operation. It is implemented by using an average consensus filter. Through this consensus filter, each sensor node can gradually diffuse its local statistics of weighted particles over the entire network and asymptotically obtain the estimated global statistics. The parameters of Gaussian mixture model can be calculated by using the estimated global statistics. Because the average consensus filter only requires that each sensor node communicate with its neighbors, the proposed distributed particle filter is scalable and robust. Simulations of tracking tasks in a sensor network with 100 sensor nodes are given.","Particle filters,
Target tracking,
Statistical distributions,
Statistics,
Robustness,
Robotics and automation,
Computational efficiency,
Message passing,
Parameter estimation,
Monitoring"
Intelligent Gateways Placement for Reduced Data Latency in Wireless Sensor Networks,"Many applications of wireless sensor networks (WSNs) have emerged over the last few years. In such networks resource-constrained sensor nodes are deployed to probe their surroundings and send the collected data to one or multiple gateways for processing. Therefore, proper placement of sensors and gateways in the area of interest facilities communications and allows for better operation of the network. While sensors in many WSN applications are randomly deployed, gateways often can be placed in a controlled manner. In this paper, we introduce two novel algorithms for proper positioning of gateways in WSNs. We employ genetic algorithms for selecting the best spot for placing each gateway so that sensors' data can be delivered to a gateway with the least latency. The two algorithms differ in their complexity and the quality of the solution that they can achieve and thus enable a design level trade-off. Validation results confirm the effectiveness of both algorithms in reducing the data delivery delay and their positive impact on other performance metrics such as the network lifetime.",
Random Sparse Linear Systems Observed Via Arbitrary Channels: A Decoupling Principle,"This paper studies the problem of estimating the vector input to a sparse linear transformation based on the observation of the output vector through a bank of arbitrary independent channels. The linear transformation is drawn randomly from an ensemble with mild regularity conditions. The central result is a decoupling principle in the large-system limit. That is, the optimal estimation of each individual symbol in the input vector is asymptotically equivalent to estimating the same symbol through a scalar additive Gaussian channel, where the aggregate effect of the interfering symbols is tantamount to a degradation in the signal-to-noise ratio. The degradation is determined from a recursive formula related to the score function of the conditional probability distribution of the noisy channel. A sufficient condition is provided for belief propagation (BP) to asymptotically produce the a posteriori probability distribution of each input symbol given the output. This paper extends the authors' previous decoupling result for Gaussian channels to arbitrary channels, which was based on an earlier work of Montanari and Tse. Moreover, a rigorous justification is provided for the generalization of some results obtained via statical physics methods.",
Dogged Learning for Robots,"Ubiquitous robots need the ability to adapt their behaviour to the changing situations and demands they will encounter during their lifetimes. In particular, non-technical users must be able to modify a robot's behaviour to enable it to perform new, previously unknown tasks. Learning from demonstration is a viable means to transfer a desired control policy onto a robot and mixed-initiative control provides a method for smooth transitioning between learning and acting. We present a learning system (dogged learning) that combines learning from demonstration and mixed initiative control to enable lifelong learning for unknown tasks. We have implemented dogged learning on a Sony Aibo and successfully taught it behaviours such as mimicry and ball seeking",
Gammatone Features and Feature Combination for Large Vocabulary Speech Recognition,"In this work, an acoustic feature set based on a gammatone filterbank is introduced for large vocabulary speech recognition. The gammatone features presented here lead to competitive results on the EPPS English task, and considerable improvements were obtained by subsequent combination to a number of standard acoustic features, i.e. MFCC, PLP, MF-PLP, and VTLN plus voicedness. Best results were obtained when combining gammatone features to all other features using weighted ROVER, resulting in a relative improvement of about 12% in word error rate compared to the best single feature system. We also found that ROVER gives better results for feature combination than both log-linear model combination and LDA.","Vocabulary,
Speech recognition,
IIR filters,
Frequency,
Filter bank,
Humans,
Biology,
Feature extraction,
Cepstral analysis,
Computer science"
Distributed average consensus with stochastic communication failures,We consider a distributed average consensus algorithm over a network in which communication links fail with independent probability. Convergence in such stochastic networks is defined in terms of the variance of deviation from average. We characterize the decay factor of the variance in terms of the eigenvalues of a Lyapunov-like matrix recursion. We give expressions for the decay factors in the asymptotic limits of small failure probability and large networks. We also present a simulation-free method for computing the decay factor for any particular graph instance and use this method to study the behavior of various network examples as a function of link failure probability.,
A Geometric Method for Automatic Extraction of Sulcal Fundi,"Sulcal fundi are 3-D curves that lie in the depths of the cerebral cortex and, in addition to their intrinsic value in brain research, are often used as landmarks for downstream computations in brain imaging. In this paper, we present a geometric algorithm that automatically extracts the sulcal fundi from magnetic resonance images and represents them as spline curves lying on the extracted triangular mesh representing the cortical surface. The input to our algorithm is a triangular mesh representation of an extracted cortical surface as computed by one of several available software packages for performing automated and semi-automated cortical surface extraction. Given this input we first compute a geometric depth measure for each triangle on the cortical surface mesh, and based on this information we extract sulcal regions by checking for connected regions exceeding a depth threshold. We then identify endpoints of each region and delineate the fundus by thinning the connected region while keeping the endpoints fixed. The curves, thus, defined are regularized using weighted splines on the surface mesh to yield high-quality representations of the sulcal fundi. We present the geometric framework and validate it with real data from human brains. Comparisons with expert-labeled sulcal fundi are part of this validation process","Brain,
Magnetic resonance imaging,
Spline,
Data mining,
Humans,
Cerebral cortex,
Magnetic resonance,
Software algorithms,
Software packages,
Joining processes"
Predicting Parking Lot Occupancy in Vehicular Ad Hoc Networks,"The search for free parking places is a promising application for vehicular ad hoc networks (VANETs). In order to guide drivers to a free parking place at their destination, it is necessary to estimate the occupancy state of the parking lots within the destination area at time of arrival. In this paper, we present a model to predict parking lot occupancy based on information exchanged among vehicles. In particular, our model takes the age of received parking lot information and the time needed to arrive at a certain parking lot into account and estimates the future parking situation at time of arrival. It is based on queueing theory and uses a continuous-time homogeneous Markov model. We have evaluated the model in a simulation study based on a detailed model of the city of Brunswick, Germany.",
ShadowCuts: Photometric Stereo with Shadows,"We present an algorithm for performing Lambertian photometric stereo in the presence of shadows. The algorithm has three novel features. First, a fast graph cuts based method is used to estimate per pixel light source visibility. Second, it allows images to be acquired with multiple illuminants, and there can be fewer images than light sources. This leads to better surface coverage and improves the reconstruction accuracy by enhancing the signal to noise ratio and the condition number of the light source matrix. The ability to use fewer images than light sources means that the imaging effort grows sublinearly with the number of light sources. Finally, the recovered shadow maps are combined with shading information to perform constrained surface normal integration. This reduces the low frequency bias inherent to the normal integration process and ensures that the recovered surface is consistent with the shadowing configuration The algorithm works with as few as four light sources and four images. We report results for light source visibility detection and high quality surface reconstructions for synthetic and real datasets.","Photometry,
Light sources,
Surface reconstruction,
Shadow mapping,
Image reconstruction,
Stereo vision,
Signal to noise ratio,
Pixel,
Computer science,
Layout"
Visualization Criticism - The Missing Link Between Information Visualization and Art,"Classifications of visualization are often based on technical criteria, and leave out artistic ways of visualizing information. Understanding the differences between information visualization and other forms of visual communication provides important insights into the way the field works, though, and also shows the path to new approaches. We propose a classification of several types of information visualization based on aesthetic criteria. The notions of artistic and pragmatic visualization are introduced, and their properties discussed. Finally, the idea of visualization criticism is proposed, and its rules are laid out. Visualization criticism bridges the gap between design, art, and technical/pragmatic information visualization. It guides the view away from implementation details and single mouse clicks to the meaning of a visualization.",
Flexible Object Models for Category-Level 3D Object Recognition,"Today's category-level object recognition systems largely focus on fronto-parallel views of objects with characteristic texture patterns. To overcome these limitations, we propose a novel framework for visual object recognition where object classes are represented by assemblies of partial surface models (PSMs) obeying loose local geometric constraints. The PSMs themselves are formed of dense, locally rigid assemblies of image features. Since our model only enforces local geometric consistency, both at the level of model parts and at the level of individual features within the parts, it is robust to viewpoint changes and intra-class variability. The proposed approach has been implemented, and it outperforms the state-of-the-art algorithms for object detection and localization recently compared in [14] on the Pascal 2005 VOC Challenge Cars Test 1 data.",
Automatic Correction of Level Set Based Subvoxel Precise Centerlines for Virtual Colonoscopy Using the Colon Outer Wall,"Virtual colonoscopy (VC) is becoming a more prevalent method to detect and diagnose colorectal cancer. An essential component of using VC to detect cancerous polyps, especially in conjunction with computer-aided diagnosis, is the accurate calculation of the centerline of the colon. While the colon is often modeled as a simple cylinder, the amount of colonic distention may vary between patients and within the same patient often causing loops and multiple disconnected segments to be present in the colon segmentation. These variations have caused previous centerline algorithms to fail to capture a complete and accurate centerline for all colons. We have developed an automatic method to determine from a computed tomography (CT) VC a subvoxel precise centerline that is accurate even in cases of over-distended or under-distended colons. In this algorithm, the loops in the colon caused by over-distention are detected and removed when the centerline calculation is performed. Also, a newly developed method for the detection and segmentation of the outer wall of the colon is used to connect collapsed portions of the colon where the lumen segmentation fails to produce a continuous centerline. These two methods allow for a complete and accurate centerline to be calculated in uniformly distended colons as well as in colons containing segments which are over-distended and/or under-distended. We have demonstrated successfully the effectiveness of our algorithm on 50 cases, 25 of which resulted in erroneous solutions by previous centerline algorithms due to variability in the colon distention.","Level set,
Virtual colonoscopy,
Colon,
Colonography,
Computed tomography,
Colonic polyps,
Cancer detection,
Computer aided diagnosis,
Radio navigation,
Smoothing methods"
Distributed Sparse Random Projections for Refinable Approximation,"Consider a large-scale wireless sensor network measuring compressible data, where n distributed data values can be well-approximated using only k <g n coefficients of some known transform. We address the problem of recovering an approximation of the n data values by querying any L sensors, so that the reconstruction error is comparable to the optimal fc-term approximation. To solve this problem, we present a novel distributed algorithm based on sparse random projections, which requires no global coordination or knowledge. The key idea is that the sparsity of the random projections greatly reduces the communication cost of pre-processing the data. Our algorithm allows the collector to choose the number of sensors to query according to the desired approximation error. The reconstruction quality depends only on the number of sensors queried, enabling robust refinable approximation.",
Feature-based head pose estimation from images,"Estimating the head pose is an important capability of a robot when interacting with humans since the head pose usually indicates the focus of attention. In this paper, we present a novel approach to estimate the head pose from monocular images. Our approach proceeds in three stages. First, a face detector roughly classifies the pose as frontal, left, or right profile. Then, classifiers trained with AdaBoost using Haar-like features, detect distinctive facial features such as the nose tip and the eyes. Based on the positions of these features, a neural network finally estimates the three continuous rotation angles we use to model the head pose. Since we have a compact representation of the face using only few distinctive features, our approach is computationally highly efficient. As we show in experiments with standard databases as well as with real-time image data, our system locates the distinctive features with a high accuracy and provides robust estimates of the head pose.","Head,
Face detection,
Human robot interaction,
Focusing,
Detectors,
Computer vision,
Facial features,
Nose,
Eyes,
Neural networks"
A Robust Spanning Tree Topology for Data Collection and Dissemination in Distributed Environments,"Large-scale distributed applications are subject to frequent disruptions due to resource contention and failure. Such disruptions are inherently unpredictable and, therefore, robustness is a desirable property for the distributed operating environment. In this work, we describe and evaluate a robust topology for applications that operate on a spanning tree overlay network. Unlike previous work that is adaptive or reactive in nature, we take a proactive approach to robustness. The topology itself is able to simultaneously withstand disturbances and exhibit good performance. We present both centralized and distributed algorithms to construct the topology, and then demonstrate its effectiveness through analysis and simulation of two classes of distributed applications: Data collection in sensor networks and data dissemination in divisible load scheduling. The results show that our robust spanning trees achieve a desirable trade-off for two opposing metrics where traditional forms of spanning trees do not. In particular, the trees generated by our algorithms exhibit both resilience to data loss and low power consumption for sensor networks. When used as the overlay network for divisible load scheduling, they display both robustness to link congestion and low values for the makespan of the schedule",
Automatic Composition of SemanticWeb Services,"Service-oriented computing is gaining wider acceptance. For Web services to become practical, an infrastructure needs to be supported that allows users and applications to discover, deploy, compose and synthesize services automatically. For this automation to be effective, formal semantic descriptions of Web services should be available. In this paper we formally define the Web service discovery and composition problem and present an approach for automatic service discovery and composition based on semantic description of Web services. We also report on an implementation of a semantics-based automated service discovery and composition engine that we have developed. This engine employs a multi-step narrowing algorithm and is efficiently implemented using the constraint logic programming technology. The salient features of our engine are its scalability, i.e., its ability to handle very large service repositories, and its extremely efficient processing times for discovery and composition queries. We evaluate our engine for automated discovery and composition on repositories of different sizes and present the results.","Web services,
Search engines,
Semantic Web,
Automation,
Logic programming,
Context-aware services,
Computer science,
Application software,
Scalability,
Internet"
Adapting SVM Classifiers to Data with Shifted Distributions,"Many data mining applications can benefit from adapt- ing existing classifiers to new data with shifted distribu- tions. In this paper, we present Adaptive Support Vector Machine (Adapt-SVM) as an efficient model for adapting a SVM classifier trained from one dataset to a new dataset where only limited labeled examples are available. By in- troducing a new regularizer into SVM's objective function, Adapt-SVM aims to minimize both the classification error over the training examples, and the discrepancy between the adapted and original classifier. We also propose a selective sampling strategy based on the loss minimization principle to seed the most informative examples for classifier adap- tation. Experiments on an artificial classification task and on a benchmark video classification task shows that Adapt- SVM outperforms several baseline methods in terms of ac- curacy and/or efficiency.",
Sprinkler: A Reliable and Energy Efficient Data Dissemination Service for Extreme Scale Wireless Networks of Embedded Devices,"We present Sprinkler, a reliable data dissemination service for wireless embedded devices which are constrained in energy, processing speed, and memory. Sprinkler embeds a virtual grid over the network whereby it can locally compute a connected dominating set of the devices to avoid redundant transmissions and a transmission schedule to avoid collisions. Sprinkler transmits O(1) times the optimum number of packets in O(1) of the optimum latency; its time complexity is O(1). Sprinkler is tolerant to fail-stop and state corruption faults. Thus, Sprinkler is suitable for resource-constrained wireless embedded devices. We evaluate the performance of Sprinkler in terms of the number of packet transmissions and the latency, both in an outdoor and indoor environment. The outdoor evaluation is based on data from project ExScal, which deployed 203 extreme scale stargazer (XSS). Our indoor evaluation is based on an implementation in the Kansei testbed, which houses 210 XSSs whose transmission power is controllable to even low ranges. We compare Sprinkler with the existing reliable data dissemination services, analytically or using simulations also. Our evaluations show that Sprinkler is not only energy efficient as compared to existing schemes, but also has less latency. Further, the energy consumption of nodes and the latency grows linearly as a function of newly added nodes as the network grows larger.","Energy efficiency,
Wireless networks,
Delay,
Computer networks,
Embedded computing,
Grid computing,
Processor scheduling,
Indoor environments,
Testing,
Data analysis"
Detecting Fault Modules Applying Feature Selection to Classifiers,"At present, automated data collection tools allow us to collect large amounts of information, not without associated problems. This paper, we apply feature selection to several software engineering databases selecting attributes with the final aim that project managers can have a better global vision of the data they manage. In this paper, we make use of attribute selection techniques in different datasets publicly available (PROMISE repository), and different data mining algorithms for classification to defect faulty modules. The results show that in general, smaller datasets with less attributes maintain or improve the prediction capability with less attributes than the original datasets.",
Audio Information Retrieval using Semantic Similarity,"We improve upon query-by-example for content-based audio information retrieval by ranking items in a database based on semantic similarity, rather than acoustic similarity, to a query example. The retrieval system is based on semantic concept models that are learned from a training data set containing both audio examples and their text captions. Using the concept models, the audio tracks are mapped into a semantic feature space, where each dimension indicates the strength of the semantic concept. Audio retrieval is then based on ranking the database tracks by their similarity to the query in the semantic space. We experiment with both semantic- and acoustic-based retrieval systems on a sound effects database and show that the semantic-based system improves retrieval both quantitatively and qualitatively.","Information retrieval,
Music information retrieval,
Spatial databases,
Audio databases,
Image retrieval,
Content based retrieval,
Acoustical engineering,
Data engineering,
Birds,
Drives"
Gateway Placement for Throughput Optimization in Wireless Mesh Networks,"In this paper, we address the problem of gateway placement for throughput optimization in multi-hop wireless mesh networks. Assume that each mesh node in the mesh network has a traffic demand. Given the number of gateways need to be deployed (denoted by k) and the interference model in the network, we study where to place exactly k gateways in the mesh network such that the total throughput is maximized while it also ensures a certain fairness among all mesh nodes. We propose a novel grid-based gateway deployment method using a cross-layer throughput optimization. Our proposed method can also be extended to work with multi-channel and multi-radio mesh networks. Simulation result demonstrates that our method can effectively exploit the resources available and perform much better than random and fixed deployment methods.","Throughput,
Wireless mesh networks,
Mesh networks,
Spine,
Spread spectrum communication,
IP networks,
Peer to peer computing,
Computer science,
USA Councils,
Telecommunication traffic"
On The Generalization of Error-Correcting WOM Codes,"WOM (write once memory) codes are codes for efficiently storing and updating data in a memory whose state transition is irreversible. Storage media that can be classified as WOM includes flash memories, optical disks and punch cards. Error-correcting WOM codes can correct errors besides its regular data updating capability. They are increasingly important for electronic memories using MLCs (multi-level cells), where the stored data are prone to errors. In this paper, we study error-correcting WOM codes that generalize the classic models. In particular, we study codes for jointly storing and updating multiple variables - instead of one variable - in WOMs with multi-level cells. The error-correcting codes we study here are also a natural extension of the recently proposed floating codes. We analyze the performance of the generalized error- correcting WOM codes and present several bounds. The number of valid states for a code is an important measure of its complexity. We present three optimal codes for storing two binary variables in n q-ary cells, where n = 1,2,3, respectively. We prove that among all the codes with the minimum number of valid states, the three codes maximize the total number of times the variables can be updated.",
Parallel hierarchical visualization of large time-varying 3D vector fields,"We present the design of a scalable parallel pathline construction method for visualizing large time-varying 3D vector fields. A 4D (i.e., time and the 3D spatial domain) representation of the vector field is introduced to make a time-accurate depiction of the flow field. This representation also allows us to obtain pathlines through streamline tracing in the 4D space. Furthermore, a hierarchical representation of the 4D vector field, constructed by clustering the 4D field, makes possible interactive visualization of the flow field at different levels of abstraction. Based on this hierarchical representation, a data partitioning scheme is designed to achieve high parallel efficiency. We demonstrate the performance of parallel pathline visualization using data sets obtained from terascale flow simulations. This new capability will enable scientists to study their time-varying vector fields at the resolution and interactivity previously unavailable to them.","Data visualization,
Large-scale systems,
Concurrent computing,
Computer science,
Permission,
Chaos,
Supercomputers,
Analytical models,
Graphics"
HCRL: A Hop-Count-Ratio based Localization in Wireless Sensor Networks,"Determining the positions of nodes is essential in many applications and geographic routing protocols of Wireless Sensor Networks. Since localization is a fundamental component of sensor networks, the cost for localization itself should be minimized. In this paper, we focus on developing a localization algorithm which provides both low-cost and accuracy. Considering these requirements, we propose a novel range-free localization technique, called HCRL, which uses only the ratios of anchor-to-node hop-counts. HCRL satisfies low-cost with a single flooding from a small number of anchor nodes, and subdivides one-hop into several sub-hops by transmission power control to improve localization accuracy. Unlike previous work, we have conducted real experiments, which were made possible by using an external antenna with an omni-directional radiation pattern. The experimental results show that the performance of HCRL is superior to the conventional DV-Hop scheme with a small transmission overhead.",
A Scalable Overlay Multicast Architecture for Large-Scale Applications,"In this paper, we propose a two-tier overlay multicast architecture (TOMA) to provide scalable and efficient multicast support for various group communication applications. In TOMA, multicast service overlay network (MSON) is advocated as the backbone service domain, while end users in access domains form a number of small clusters, in which an application-layer multicast protocol is used for the communication between the clustered end users. TOMA is able to provide efficient resource utilization with less control overhead, especially for large-scale applications. It also alleviates the state scalability problem and simplifies multicast tree construction and maintenance when there are large numbers of groups in the network. To help MSON providers efficiently plan backbone service overlay, we suggest several provisioning algorithms to locate proxies, select overlay links, and allocate link bandwidth. Extensive simulation studies demonstrate the promising performance of TOMA","Large-scale systems,
Bandwidth,
Spine,
Multicast protocols,
Scalability,
Video on demand,
Videoconference,
Web and internet services,
Resource management,
Communication system control"
Autonomous Vision-based Landing and Terrain Mapping Using an MPC-controlled Unmanned Rotorcraft,"In this paper, we present a vision-based terrain mapping and analysis system, and a model predictive control (MPC)-based flight control system, for autonomous landing of a helicopter-based unmanned aerial vehicle (UAV) in unknown terrain. The vision system is centered around Geyer et al.'s recursive multi-frame planar parallax algorithm (2006), which accurately estimates 3D structure using geo-referenced images from a single camera, as well as a modular and efficient mapping and terrain analysis module. The vision system determines the best trajectory to cover large areas of terrain or to perform closer inspection of potential landing sites, and the flight control system guides the vehicle through the requested flight pattern by tracking the reference trajectory as computed by a real-time MPC-based optimization. This trajectory layer, which uses a constrained system model, provides an abstraction between the vision system and the vehicle. Both vision and flight control results are given from flight tests with an electric UAV.",
Non-Threshold based Event Detection for 3D Environment Monitoring in Sensor Networks,"Event detection is a crucial task for wireless sensor network applications, especially environment monitoring. Existing approaches for event detection are mainly based on some predefined threshold values, and thus are often inaccurate and incapable of capturing complex events. For example, in coal mine monitoring scenarios, gas leakage or water osmosis can hardly be described by the overrun of specified attribute thresholds, but some complex pattern in the full-scale view of the environmental data. To address this issue, we propose a non-threshold based approach for the real 3D sensor monitoring environment. We employ energy-efficient methods to collect a time series of data maps from the sensor network and detect complex events through matching the gathered data to spatio-temporal data patterns. Finally, we conduct trace driven simulations to prove the efficacy and efficiency of this approach on detecting events of complex phenomena from real-life records.","Event detection,
Wireless sensor networks,
Sensor phenomena and characterization,
Computerized monitoring,
Vehicle detection,
Energy efficiency,
Pattern matching,
Safety,
Computer science,
Application software"
Effects of electrodes with varying thickness on energy trapping in thickness-shear quartz resonators,We study the possibility of using electrodes of varying thickness for strong energy trapping in quartz thickness-shear resonators as an alternative for contoured resonators. A theoretical analysis is performed. Results show that nonuniform electrodes can produce strong trapping of thickness-shear modes,"Electrodes,
Vibrations,
Equations,
Crystalline materials,
Resonant frequency,
Laboratories,
Civil engineering,
Materials science and technology,
Performance analysis,
Piezoelectric materials"
Fast Planar-Oriented Ripple Search Algorithm for Hyperspace VQ Codebook,"This paper presents a fast codebook search method for improving the quantization complexity of full-search vector quantization (VQ). The proposed method is built on the planar Voronoi diagram to label a ripple search domain. Then, the appropriate codeword can easily be found just by searching the local region instead of global exploration. In order to take a step further and obtain the close result full-search VQ would, we equip the proposed method with a duplication mechanism that helps to bring down the possible quantizing distortion to its lowest level. According to the experimental results, the proposed method is indeed capable of providing better outcome at a faster quantization speed than the existing partial-search methods. Moreover, the proposed method only requires a little extra storage for duplication",
Volume Parameterization for Design Automation of Customized Free-Form Products,"This paper addresses the problem of volume parameterization that serves as the geometric kernel for design automation of customized free-form products. The purpose of volume parameterization is to establish a mapping between the spaces that are near to two reference free-form models, so that the shape of a product presented in free-form surfaces can be transferred from the space around one reference model to another reference model. The mapping is expected to keep the spatial relationship between the product model and reference models as much as possible. We separate the mapping into rigid body transformation and elastic warping. The rigid body transformation is determined by anchor points defined on the reference models using a least-squares fitting approach. The elastic warping function is more difficult to obtain, especially when the meshes of the reference objects are inconsistent. A three-stage approach is conducted. First, a coarse-level warping function is computed based on the anchor points. In the second phase, the topology consistency is maintained through a surface fitting process. Finally, the mapping of volume parameterization is established on the surface fitting result. Compared to previous methods, the approach presented here is more efficient. Also, benefitting from the separation of rigid body transformation and elastic warping, the transient shape of a transferred product does not give unexpected distortion. At the end of this paper, various industry applications of our approach in design automation are demonstrated. Note to Practitioners-The motivation of this research is to develop a geometric solution for the design automation of customized free-form objects, which can greatly improve the efficiency of design processes in various industries involving customized products (e.g., garment design, toy design, jewel design, shoe design, and glasses design, etc.). The products in the above industries are usually composed of a very complex geometry shape (represented by free-form surfaces), and is not driven by a parameter table but a reference object with free-form shapes (e.g., mannequin, toy, wrist, foot, and head models). After carefully designing a product around one particular reference model, it is desirable to have an automated tool for ""grading"" this product to other shape-changed reference objects while retaining the original spatial relationship between the product and reference models. This is called the design automation of a customized free-form object. Current commercial 3-D/2-D computer-aided design (CAD) systems, developed for the design automation of models with regular shape, cannot support the design automation in this manner. The approach in this paper develops efficient techniques for constraining and reconstructing a product represented by free-form surfaces around reference objects with different shapes, so that this design automation problem can be fundamentally solved. Although the approach has not been integrated into commercial CAD systems, the results based on our preliminary implementation are encouraging-the spatial relationship between reference models and the customized products is well preserved",
Compensation of Some Time Dependent Deformations in Tomography,This work concerns 2D+t dynamic tomography. We show that a much larger class of deformations than the affine transforms can be compensated analytically within filtered back projection algorithms in 2D parallel beam and fan beam dynamic tomography. We present numerical experiments on the Shepp and Logan phantom showing that nonaffine deformations can be compensated. A generalization to 3D cone beam tomography is proposed,
Reverse Fast Broadcasting (RFB) for Video-on-Demand Applications,"A popular video can be broadcast by partitioning the video into segments, which are broadcast on several channels simultaneously and periodically. This method allows multiple users to share channels, leading to higher bandwidth utilization. Previous studies mainly focus on reducing viewers' waiting time. This work studies another important issue, namely client buffer. When the term ""client buffer"" is used, it means the worst-case buffer requirements (i.e., the maximum value). savings. A reverse fast broadcasting (RFB) scheme is proposed to alleviate the buffer problem. There are two specific properties for RFB. First, this scheme arranges the segments in descending order of their numeric indices on each channel at the server end. Second, RFB requires a client to receive segments as late as possible, such that the client buffers the smallest number of segments. RFB has the same waiting time as the fast broadcasting (FB) scheme, but just needs a half of buffer spaces required by FB. RFB also requires smaller client buffers than the pyramid broadcasting (PB), skyscraper broadcasting (SkB), greedy disk-conserving broadcasting (GDB), BroadCatch, and recursive frequency-splitting schemes. Moreover, if we apply the same changes to the PB, SkB and GDB schemes, their buffer requirements decrease by 25% to 75%",
Left Ventricular Deformation Recovery From Cine MRI Using an Incompressible Model,"This paper presents a method for 3D deformation recovery of the left ventricular (LV) wall from anatomical cine magnetic resonance imaging (MRI). The method is based on a de- formable model that is incompressible, a desired property since the myocardium has been shown to be nearly incompressible. The LV wall needs to be segmented in an initial frame after which the method automatically determines the deformation everywhere in the LV wall throughout the cardiac cycle. Two studies were conducted to validate the method. In the first study, the deformation recovered from a 3D anatomical cine MRI of a healthy volunteer was compared against the manual segmentation of the LV wall and against the corresponding 3D tagged cine MRI. The average volume agreement between the model and the manual segmentation had a false positive rate of 3%, false negative rate of 3%, and true positive rate of 93%. The average distance between the model and manually determined intersections of perpendicular tag planes was 1.6 mm (1.1 pixel). Another set of 3D anatomical and tagged MRI scans was taken of the same volunteer four months later. The method was applied to the second set and the recovered deformation was very similar to the one obtained from the first set. In the second study, the method was applied to 3D anatomical cine MRI scans of three patients with ventricular dyssynchrony and three age-matched healthy volunteers. The LV wall deformations recovered for the three normals agreed well and the recovered strains were similar to those reported by other researchers for normal subjects. Strains and displacements of the three patients were clearly smaller than those of the three normals indicating reduced cardiac function. The deformation recovered for the three normals and the three patients was validated against manual segmentation and corresponding tag cine MRI scans and the agreement was similar to that of the first validation study.","Magnetic resonance imaging,
Deformable models,
Myocardium,
Heart,
Image segmentation,
Capacitive sensors,
Cardiovascular diseases,
Magnetic analysis,
Cardiac disease,
Costs"
Target-oriented scheduling in directional sensor networks,"Unlike convectional omni-directional sensors that always have an omni-angle of sensing range, directional sensors may have a limited angle of sensing range due to technical constraints or cost considerations. A directional sensor network consists of a number of directional sensors, which can switch to several directions to extend their sensing ability to cover all the targets in a given area. Power conservation is still an important issue in such directional sensor networks. In this paper, we address the multiple directional cover sets problem (MDCS) of organizing the directions of sensors into a group of non-disjoint cover sets to extend the network lifetime. One cover set, in which the directions cover all the targets, is activated at one time. We prove the MDCS to be NP-complete and propose three heuristic algorithms for the MDCS. Simulation results are also presented to demonstrate the performance of these algorithms.","Sensor phenomena and characterization,
Switches,
Infrared sensors,
Computer science,
Costs,
Communications Society,
Scheduling,
Organizing,
Heuristic algorithms,
Monitoring"
Analytic Carrier-Based Charge and Capacitance Model for Long-Channel Undoped Surrounding-Gate MOSFETs,"Three terminal charges and nine intrinsic capacitances associated to the gate, source, and drain terminals of long-channel undoped surrounding-gate (SRG) MOSFETs are derived physically from an exact analytical solution of the channel current-continuity principle and channel charge-partition scheme in this paper. Although requiring lengthy and complex mathematical expressions, all explicit solutions for the capacitances can be obtained analytically. The validity of the analytical solutions is confirmed by comparing model predictions with simulation data obtained using the 3-D numerical solvers. The explicit expressions to the terminal charges and transcapacitance not only lead to a clearer understanding of SRG MOSFET device physics but also provide a better infrastructure to develop a complete carrier-based model for the SRG-MOSFET-based circuit simulation",
Computing the Types of the Relationships Between Autonomous Systems,"We investigate the problem of computing the types of the relationships between Internet Autonomous Systems. We refer to the model introduced by Gao [IEEE/ACM Transactions on Networking, 9(6):733-645, 2001] and Subramanian (IEEE Infocom, 2002) that bases the discovery of such relationships on the analysis of the AS paths extracted from the BGP routing tables. We characterize the time complexity of the above problem, showing both NP-completeness results and efficient algorithms for solving specific cases. Motivated by the hardness of the general problem, we propose approximation algorithms and heuristics based on a novel paradigm and show their effectiveness against publicly available data sets. The experiments provide evidence that our algorithms perform significantly better than state-of-the-art heuristics",
Cooperative Relay Service in a Wireless LAN,"As a family of wireless local area network (WLAN) protocols between physical layer and higher layer protocols, IEEE 802.11 has to accommodate the features and requirements of both ends. However, current practice has addressed the problems of these two layers separately and is far from satisfactory. On one end, due to varying channel conditions, WLANs have to provide multiple physical channel rates to support various signal qualities. A low channel rate station not only suffers low throughput, but also significantly degrades the throughput of other stations. On the other end, the power saving mechanism of 802.11 is ineffective in TCP-based communications, in which the wireless network interface (WNI) has to stay awake to quickly acknowledge senders, and hence, the energy is wasted on channel listening during idle awake time. In this paper, considering the needs of both ends, we utilize the idle communication power of the WNI to provide a Cooperative Relay Service (CRS) for WLANs with multiple channel rates. We characterize energy efficiency as energy per bit, instead of energy per second. In CRS, a high channel rate station relays data frames as a proxy between its neighboring stations with low channel rates and the Access Point, improving their throughput and energy efficiency. Different from traditional relaying approaches, CRS compensates a proxy for the energy consumed in data forwarding. The proxy obtains additional channel access time from its clients, leading to the increase of its own throughput without compromising its energy efficiency. Extensive experiments are conducted through a prototype implementation and ns-2 simulations to evaluate our proposed CRS. The experimental results show that CRS achieves significant performance improvements for both low and high channel rate stations","Relays,
Wireless LAN,
Throughput,
Degradation,
Energy efficiency,
Energy consumption,
Switches,
Mobile communication,
Batteries,
Computer science"
MBAL: A Mobile Beacon-Assisted Localization Scheme for Wireless Sensor Networks,"Localization is one of the critical issues on wireless sensor networks. Localization schemes are classified into range-based and range-free according to the method of whether to use range information. In this paper, we propose a novel range-based localization scheme which involves a movement strategy of mobile beacon, called mobile beacon-assisted localization (MBAL). Contrary to many research activities which have been carried out to design localization schemes using mobile beacons only based on random movement method, we consider totally a new scheme providing movement path selection with a low computational complexity. A new range check technique is also adopted into the MBAL as a useful solution to the position-ambiguity problem of bilateration in order to improve the performance of the proposed localization scheme. Simulation results verify that the MBAL impressively achieves energy efficiency because of its mobile beacon based approach using the proposed movement strategy and range check technique.",
Energy-Efficient Routing in Linear Wireless Sensor Networks,"Wireless sensor networks are used for structure monitoring and border surveillance. Typical applications, such as sensors embedded in the outer surface of a pipeline or mounted along the supporting structure of a bridge, feature a linear sensor arrangement. Economical power use of sensor nodes is essential for long-lasting operation. In this paper, we present MERR (minimum energy relay routing), a novel approach to energy-efficient data routing to a single control center in a linear sensor topology. Based on an optimal transmission distance, relay paths are established that aim for minimizing the total power consumption. We study MERR by both stochastic analysis and simulation, comparing it to other possible approaches and a theoretically optimal protocol. We find that MERR consumes 80% less power than conventional approaches and performs close to the theoretical optimum for practicable sensor networks.","Energy efficiency,
Routing,
Wireless sensor networks,
Relays,
Monitoring,
Surveillance,
Pipelines,
Bridges,
Power generation economics,
Topology"
A Robust Quadruped Walking Gait for Traversing Rough Terrain,"Legged locomotion excels when terrains become too rough for wheeled systems or open-loop walking pattern generators to succeed, i.e., when accurate foot placement is of primary importance in successfully reaching the task goal. In this paper we address the scenario where the rough terrain is traversed with a static walking gait, and where for every foot placement of a leg, the location of the foot placement was selected irregularly by a planning algorithm. Our goal is to adjust a smooth walking pattern generator with the selection of every foot placement such that the COG of the robot follows a stable trajectory characterized by a stability margin relative to the current support triangle. We propose a novel parameterization of the COG trajectory based on the current position, velocity, and acceleration of the four legs of the robot. This COG trajectory has guaranteed continuous velocity and acceleration profiles, which leads to continuous velocity and acceleration profiles of the leg movement, which is ideally suited for advanced model-based controllers. Pitch, yaw, and ground clearance of the robot are easily adjusted automatically under any terrain situation. We evaluate our gait generation technique on the Little-Dog quadruped robot when traversing complex rocky and sloped terrains.","Robustness,
Legged locomotion,
Foot,
Leg,
Acceleration,
Robotics and automation,
Character generation,
Stability,
Automatic control,
Velocity control"
A Survey on Digital Camera Image Forensic Methods,"There are two main interests in digital camera image forensics, namely source identification and forgery detection. In this paper, we first briefly provide an introduction to the major processing stages inside a digital camera and then review several methods for source digital camera identification and forgery detection. Existing methods for source identification explore the various processing stages inside a digital camera to derive the clues for distinguishing the source cameras while forgery detection checks for inconsistencies in image quality or for presence of certain characteristics as evidence of tampering.","Digital cameras,
Forensics,
Lenses,
Forgery,
Sensor arrays,
Image sensors,
Color,
Electronics packaging,
Digital filters,
Interpolation"
Zero-Forcing Based Two-phase Relaying,"In cellular mobile communication systems, the link performance can be remarkably improved by deploying relays between the base station and the mobile station. In this paper we propose an efficient duplexing scheme so that both spatial and temporal gain by adding relays can be increased. Using the proposed relaying scheme, the conventional system of four-phase relaying can be simplified to a two-phase relaying system and the additional resource consumption due to relays can be minimized. We show the capacity gain for cell edge users with numerical results.","Relays,
Base stations,
Decoding,
Mobile communication,
Downlink,
Communications Society,
Computer science,
Mobile computing,
Multiaccess communication,
Analytical models"
Indexing Spatio-Temporal Trajectories with Efficient Polynomial Approximations,"Complex queries on trajectory data are increasingly common in applications involving moving objects. MBR or grid-cell approximations on trajectories perform suboptimally since they do not capture the smoothness and lack of internal area of trajectories. We describe a parametric space indexing method for historical trajectory data, approximating a sequence of movement functions with single continuous polynomial. Our approach works well, yielding much finer approximation quality than MBRs. We present the PA-tree, a parametric index that uses this method, and show through extensive experiments that PA-trees have excellent performance for offline and online spatio-temporal range queries. Compared to MVR-trees, PA-trees are an order of magnitude faster to construct and incur I/O cost for spatio-temporal range queries lower by a factor of 2-4. SETI is faster than our method for index construction and timestamp queries, but incurs twice the I/O cost for time interval queries, which are much more expensive and are the bottleneck in online processing. Therefore, the PA-tree is an excellent choice for both offline and online processing of historical trajectories",
Polyp Detection in Colonoscopy Video using Elliptical Shape Feature,"Early detection of polyps and cancers is one of the most important goals of colonoscopy. Computer-based analysis of video files using texture features, as has been proposed for polyps of the stomach and colon, has two major limitations: this method uses a fixed size analysis window and relies heavily on a training set of images for accuracy. To overcome these limitations, we propose a new technique focusing on shape instead of texture in this paper. The proposed polyp region detection method is based on the elliptical shape that is common for nearly all small colon polyps.","Colonic polyps,
Colonoscopy,
Shape,
Image segmentation,
Image analysis,
Image texture analysis,
Computer science,
Cancer detection,
Colon,
Image edge detection"
Interactive Level-of-Detail Selection Using Image-Based Quality Metric for Large Volume Visualization,"For large volume visualization, an image-based quality metric is difficult to incorporate for level-of-detail selection and rendering without sacrificing the interactivity. This is because it is usually time-consuming to update view-dependent information as well as to adjust to transfer function changes. In this paper, we introduce an image-based level-of-detail selection algorithm for interactive visualization of large volumetric data. The design of our quality metric is based on an efficient way to evaluate the contribution of multiresolution data blocks to the final image. To ensure real-time update of the quality metric and interactive level-of-detail decisions, we propose a summary table scheme in response to runtime transfer function changes and a GPU-based solution for visibility estimation. Experimental results on large scientific and medical data sets demonstrate the effectiveness and efficiency of our algorithm",
FreeSim - a free real-time freeway traffic simulator,"In this paper we describe FreeSim, which is a fully-customizable macroscopic and microscopic freeflow traffic simulator. FreeSim allows for multiple freeway systems to be easily represented and loaded into the simulator as a graph data structure with edge weights determined by the current speeds. Traffic and graph algorithms can be created and executed for the entire freeway system or for individual vehicles, and the traffic data used by the simulator can be user-generated or be converted from real-time data gathered by a transportation organization. The vehicles in FreeSim can communicate with the system monitoring the traffic on the freeways, which makes FreeSim ideal for ITS simulation. FreeSim is licensed under the GNU General Public License, and the source code is available for download from http://www.freewaysimulator.com.",
Hypergraph-based Dynamic Load Balancing for Adaptive Scientific Computations,"Adaptive scientific computations require that periodic repartitioning (load balancing) occur dynamically to maintain load balance. Hypergraph partitioning is a successful model for minimizing communication volume in scientific computations, and partitioning software for the static case is widely available. In this paper, we present a new hypergraph model for the dynamic case, where we minimize the sum of communication in the application plus the migration cost to move data, thereby reducing total execution time. The new model can be solved using hypergraph partitioning with faced vertices. We describe an implementation of a parallel multilevel repartitioning algorithm within the Zoltan load-balancing toolkit, which to our knowledge is the first code for dynamic load balancing based on hypergraph partitioning. Finally, we present experimental results that demonstrate the effectiveness of our approach on a Linux cluster with up to 64 processors. Our new algorithm compares favorably to the widely used ParMETIS partitioning software in terms of quality, and would have reduced total execution time in most of our test cases.","Load management,
Costs,
Biomedical computing,
Partitioning algorithms,
US Department of Energy,
Laboratories,
Clustering algorithms,
Computational modeling,
Contracts,
Biomedical informatics"
Innovating Collaborative Content Creation: The Role of Altruism and Wiki Technology,"Wikipedia demonstrates the feasibility and success of an innovative form of content creation, namely openly shared, collaborative writing. This research sought to understand the success of Wikipedia as a collaborative model, considering both technology and participant motivations. The research finds that while participants have both individualistic and collaborative motives, collaborative (altruistic) motives dominate. The collaboration model differs from that of open source software development, which is less inclusive with respect to participation, and more ""selfish"" with respect to contributor motives. The success of the Wikipedia model appears to be related to wiki technology and the ""wiki way"" of collaboration",
Single Event Upsets in a 130 nm Hardened Latch Design Due to Charge Sharing,"Critical charge to represent a logic HIGH is steadily decreasing with decreasing technology feature size. Many methods have been developed to increase critical charge requirement for storage elements, thereby reducing the soft error rates. Design-based approaches have been proposed that use four storage nodes instead of two nodes to retain data. Such designs are considered single event upset (SEU) immune at low energy ion hits for all practical purposes because a single ion hit at a storage node does not cause an upset. However, such designs are vulnerable to ion hits that result in multiple nodes collecting charges. For deep sub-micron technologies, the proximity of circuit nodes results in charge collection at multiple nodes when a single ion strikes a node. Researchers first observed the effect of such charge sharing in SRAM designs. In this paper, circuit and 3D technology computer aided design (TCAD) mixed-mode simulations are used to characterize charge sharing between sensitive pairs of devices and the resulting upsets in a hardened storage cell. The simulation results were verified with experimental data showing upsets due to charge sharing in a hardened cell when exposed to low energy ions",
Moment Invariants for the Analysis of 2D Flow Fields,"We present a novel approach for analyzing two-dimensional (2D) flow field data based on the idea of invariant moments. Moment invariants have traditionally been used in computer vision applications, and we have adapted them for the purpose of interactive exploration of flow field data. The new class of moment invariants we have developed allows us to extract and visualize 2D flow patterns, invariant under translation, scaling, and rotation. With our approach one can study arbitrary flow patterns by searching a given 2D flow data set for any type of pattern as specified by a user. Further, our approach supports the computation of moments at multiple scales, facilitating fast pattern extraction and recognition. This can be done for critical point classification, but also for patterns with greater complexity. This multi-scale moment representation is also valuable for the comparative visualization of flow field data. The specific novel contributions of the work presented are the mathematical derivation of the new class of moment invariants, their analysis regarding critical point features, the efficient computation of a novel feature space representation, and based upon this the development of a fast pattern recognition algorithm for complex flow structures.","Data visualization,
Pattern recognition,
Data mining,
Computer vision,
Space technology,
Feature extraction,
Data analysis,
Application software,
Pattern analysis,
Algorithm design and analysis"
Limiting Sybil Attacks in Structured P2P Networks,"One practical limitation of structured peer-to-peer (P2P) networks is that they are frequently subject to Sybil attacks: malicious parties can compromise the network by generating and controlling large numbers of shadow identities. In this paper, we propose an admission control system that mitigates Sybil attacks by adaptively constructing a hierarchy of cooperative peers. The admission control system vets joining nodes via client puzzles. A node wishing to join the network is serially challenged by the nodes from a leaf to the root of the hierarchy. Nodes completing the puzzles of all nodes in the chain are provided a cryptographic proof of the vetted identity. We evaluate our solution and show that an adversary must perform days or weeks of effort to obtain even a small percentage of nodes in small P2P networks, and that this effort increases linearly with the size of the network. We further show that we can place a ceiling on the number of IDs any adversary may obtain by requiring periodic reassertion of the IDs continued validity.","Peer to peer computing,
Intrusion detection,
Admission control,
Authentication,
Public key cryptography,
Communications Society,
Computer science,
Performance evaluation,
Robustness,
Content based retrieval"
A Cost-Sensitive Model for Preemptive Intrusion Response Systems,"The proliferation of complex and fast-spreading intrusions not only requires advances in intrusion detection mechanisms but also demands development of sophisticated and automated intrusion response systems. In this paper we present a novel cost-sensitive model for intrusion response that incorporates preemptive deployment of the response actions. Specifically, our technique relies on comparing the cost of deploying a response against the cost of damage caused by an ""'un-attended"" intrusion and decides to preemptively deploy a response with maximum benefit. Our technique further allows adaptation of responses to the changing environment through evaluation of success and failure of previously triggered responses. We demonstrate the advantages of the approach and evaluate it using a damage reduction metric.","Intrusion detection,
Monitoring,
Pattern matching,
Computer science,
Event detection,
Safety,
Cost function,
Delay,
Specification languages,
Utility theory"
Minimum Variance FIR Smoothers for Discrete-Time State Space Models,We propose a fixed-lag finite-impulse-response (FIR) smoother for a discrete-time state space model. The proposed FIR smoother estimates the state at the fixed-lag time using measured output samples on the recent finite time horizon so that the variance of the estimation error is minimized. The minimum variance FIR (MVFIR) smoother is unbiased and independent of any a priori information of the state on the horizon. A numerical example shows that the proposed MVFIR smoother has better performance than the fixed-lag Kalman smoother based on the infinite impulse response structure when transitory modeling uncertainties exist.,"Finite impulse response filter,
State-space methods,
IIR filters,
State estimation,
Time measurement,
Estimation error,
Computer science,
Kalman filters,
Uncertainty,
Estimation theory"
Unmanned ground vehicle swarm formation control using potential fields,"A novel technique is presented for organizing swarms of robots into formation utilizing artificial potential fields generated from normal and sigmoid functions. These functions construct the surface swarm members travel on, controlling the overall swarm geometry and the individual member spacing. Limiting functions are defined to provide tighter swarm control by modifying and adjusting a set of control variables forcing the swarm to behave according to set constraints, formation and member spacing. The swarm function and limiting functions are combined to control swarm formation, orientation, and swarm movement as a whole. Parameters are chosen based on desired formation as well as user defined constraints. This approach compared to others, is simple, computationally efficient, scales well to different swarm sizes, to heterogeneous systems, and to both centralized and decentralized swarm models. Simulation results are presented for a swarm of four and ten particles following circle, ellipse and wedge formations. Experimental results are also included with four unmanned ground vehicles (UGV).","Land vehicles,
Computational geometry,
Protection,
Robot sensing systems,
Computer science,
Automotive engineering,
Organizing,
Computational modeling,
Velocity control,
Guidelines"
Surface-Level Gateway Deployment for Underwater Sensor Networks,"The performance of underwater sensor networks (UWSNs) is greatly limited by the low bandwidth and high propagation delay of acoustic communications. Deploying multiple surface-level radio-capable gateways can enhance UWSN performance from many aspects. In this paper, we mainly focus on the surface gateway deployment, which is modelled as an optimization problem. Integer Linear Programming (ILP) is used for solving variations of the deployment optimization problem. The tradeoff between the number of surface gateways and the expected delay and energy consumption is analyzed. We conduct simulations to evaluate the benefits of surface gateway optimization and investigate the effect of acoustic channel capacity and the underwater sensor node deployment pattern. Our results show the significant advantages of surface gateway optimization. The results also provide useful guidelines for real network deployment.","Acoustic sensors,
Underwater acoustics,
Bandwidth,
Propagation delay,
Underwater communication,
Integer linear programming,
Energy consumption,
Channel capacity,
Capacitive sensors,
Guidelines"
Graph based Metrics for Intrusion Response Measures in Computer Networks,"This contribution presents a graph based approach for modelling the effects of both attacks against computer networks and response measures as reactions against the attacks. Certain properties of the model graphs are utilized to quantify different response metrics which are well-kown from the pragmatic view of network security officers. Using these metrics, it is possible to (1) quantify practically relevant properties of a response measure after its application, and (2) estimate these properties for all available response measures prior to their application. The latter case is the basis for the selection of an appropriate reaction to a given attack. Our graph-based model is similar to those used in software reliability analysis and was designed for a scalable granularity in representing properties of the network and its components to be protected. Different examples show the applicability of the model and the resulting metric values.","Computer networks,
Costs,
Risk analysis,
Application software,
Workstations,
Data security,
Information processing,
Ergonomics,
Computer science,
Software reliability"
"A framework for rapid system-level exploration, synthesis, and programming of multimedia MP-SoCs","In this paper, we present the Daedalus framework, which allows for traversing the path from sequential application specification to a working MP-SoC prototype in FPGA technology with the (parallelized) application mapped onto it in only a matter of hours. During this traversal, which offers a high degree of automation, guidance is provided by Daedalus' integrated system-level design space exploration environment. We show that Daedalus offers remarkable potentials for quickly experimenting with different MP-SoC architectures and exploring system-level design options during the very early stages of design. Using a case study with a Motion-JPEG encoder application, we illustrate Daedalus' design steps and demonstrate its efficiency.","Computer architecture,
Computational modeling,
Program processors,
IP networks,
Field programmable gate arrays,
XML,
Space exploration"
A RSSI-Based DV-Hop Algorithm for Wireless Sensor Networks,"Wireless sensor network (WSN) are widely used in many different scenarios. The localization information is crucial for the operation of WSN. There are mainly two types of localization algorithms. The Range-based localization algorithm has strict requirements on hardware, thus is expensive to be implemented in practice. The Range-free localization algorithm reduces the hardware cost. However, it can only achieve high accuracy in ideal scenarios. In this paper, we locate unknown nodes by incorporating the advantages of these two types of methods and propose a new algorithm named the RSSI-based DV-hop algorithm (RDV-hop). Our algorithm improves the localization accuracy compared with previous algorithms, which has been demonstrated by the simulating results.",
Mesh-Based Depth Coding for 3D Video using Hierarchical Decomposition of Depth Maps,"In this paper, we present a new coding scheme for depth maps using a hierarchical decomposition. After we decompose a depth map into three disjoint images and a layer descriptor according to the region of edges, we merge the disjoint images of each depth map into an image. Then, the merged images and the layer descriptor are coded by H.264/AVC. Unlike previous mesh-based depth coding methods, we compress the irregular depth information using a conventional 2D video coder. Experimental results show that our scheme improves compression efficiency of mesh-based depth coding.","Video compression,
Layout,
Image coding,
Automatic voltage control,
Rendering (computer graphics),
Color,
Mesh generation,
Surface texture,
Video coding,
Erbium"
Improving Student Performance Using Self-Assessment Tests,"Testing is the most generic and perhaps most widely used mechanism for student assessment. Most tests are based on the classical test theory, which says that a student's score is the sum of the scores obtained in all questions plus some kind of error. The most relevant is that the student test result depends heavily on the individual's learning preferences or abilities and also on the actual test's format. According to this theory, tests aren't necessarily useful in intelligent educational systems, which require accurately obtaining the student's knowledge state to guide the learning process. Yet the Web has created a new generation of intelligent systems-adaptive hypermedia systems which offer new types of instructional interaction. Educational AHSs adapt the learning process on the basis of the student's learning preferences, knowledge, and availability. One such Web-based tool is Siette (the system of intelligent evaluation using rests), which infers student knowledge using adaptive testing.",
On the Prevalence of Sensor Faults in Real-World Deployments,"Various sensor network measurement studies have reported instances of transient faults in sensor readings. In this work, we seek to answer a simple question: How often are such faults observed in real deployments? To do this, we first explore and characterize three qualitatively different classes of fault detection methods. Rule-based methods leverage domain knowledge to develop heuristic rules for detecting and identifying faults. Estimation methods predict ""normal"" sensor behavior by leveraging sensor correlations, flagging anomalous sensor readings as faults. Finally, learning-based methods are trained to statistically identify classes of faults. We find that these three classes of methods sit at different points on the accuracy/robustness spectrum. Rule-based methods can be highly accurate, but their accuracy depends critically on the choice of parameters. Learning methods can be cumbersome, but can accurately detect and classify faults. Estimation methods are accurate, but cannot classify faults. We apply these techniques to four real-world sensor data sets and find that the prevalence of faults as well as their type varies with data sets. All three methods are qualitatively consistent in identifying sensor faults in real world data sets, lending credence to our observations. Our work is a first-step towards automated on-line fault detection and classification.","Fault detection,
Sensor phenomena and characterization,
Fault diagnosis,
Learning systems,
Robustness,
Computer science,
Wireless sensor networks,
Turning,
Instruments,
Hardware"
Algorithm-Hardware Codesign of Fast Parallel Round-Robin Arbiters,"As a basic building block of a switch scheduler, a fast and fair arbiter is critical to the efficiency of the scheduler, which is the key to the performance of a high-speed switch or router. In this paper, we propose a parallel round-robin arbiter (PRRA) based on a simple binary search algorithm, which is specially designed for hardware implementation. We prove that our PRRA achieves round-robin fairness under all input patterns. We further propose an improved (IPRRA) design that reduces the timing of PRRA significantly. Simulation results with TSMC .18mum standard cell library show that PRRA and IPRRA can meet the timing requirement of a terabit 256 times 256 switch. Both PRRA and IPRRA are much faster and simpler than the programmable priority encoder (PPE), a well-known round-robin arbiter design. We also introduce an additional design which combines PRRA and IPRRA and provides trade-offs in gate delay, wire delay, and circuit area. With the binary tree structure and high performance, our designs are scalable for large N and useful for implementing schedulers for high-speed switches and routers","Round robin,
Switches,
Timing,
Delay,
Algorithm design and analysis,
Hardware,
Libraries,
Wire,
Circuits,
Binary trees"
"Identifying, Assigning, and Quantifying Crosscutting Concerns","Crosscutting concerns degrade software quality. Before we can modularize the crosscutting concerns in our programs to increase software quality, we must first be able to find them. Unfortunately, accurately locating the code related to a concern is difficult, and without proper metrics, determining how much the concern is crosscutting is impossible. We propose a systematic methodology for identifying which code is related to which concern, and a suite of metrics for quantifying the amount of crosscutting code. Our concern identification and assignment guidelines resolve some of the ambiguity issues encountered by other researchers. We applied this approach to systematically identify all the requirement concerns in a 13,531 line program. We found that 95% of the concerns were crosscutting - indicating a significant potential for improving modularity - and that our metrics were better able to determine which concerns would benefit the most from reengineering.",
A Texture-Based Neural Network Classifier for Biometric Identification using Ocular Surface Vasculature,"In an earlier work we had explored the possibility of utilizing the vascular pattern of the sclera, episclera, and conjunctiva as a biometric indicator. These blood vessels, which can be observed on the white part of the human eye, demonstrate rich and seemingly unique details in visible light, and can be easily imaged using commercially available digital cameras. In this work we discuss a new method to represent and match the textural intricacies of this vascular structure using wavelet-derived features in conjunction with neural network classifiers. Our experimental results, based on the evidence of 50 subjects, indicate the potential of the proposed scheme to characterize the individuality of the ocular surface vascular patterns and further confirm our assertion that these patterns are indeed unique across individuals.","Neural networks,
Biometrics,
Surface texture,
Iris,
Humans,
Retina,
Feature extraction,
Computer science,
Cities and towns,
Optical imaging"
Band Selection of Hyperspectral Images for Automatic Detection of Poultry Skin Tumors,"This paper presents a spectral band selection method for feature dimensionality reduction in hyperspectral image analysis for detecting skin tumors on poultry carcasses. A hyperspectral image contains spatial information measured as a sequence of individual wavelength across broad spectral bands. Despite the useful information for skin tumor detection, real-time processing of hyperspectral images is often a challenging task due to the large amount of data. Band selection finds a subset of significant spectral bands in terms of information content for dimensionality reduction. This paper presents a band selection method of hyperspectral images based on the recursive divergence for the automatic detection of poultry carcasses. For this, we derive a set of recursive equations for the fast calculation of divergence with an additional band to overcome the computational restrictions in real-time processing. A support vector machine is used as a classifier for tumor detection. From our experiments, the proposed band selection method shows high detection accuracy with low false positive rates compared to the canonical analysis at a small number of spectral bands. Also, compared with the enumeration approach of 93.75% detection rate, our proposed recursive divergence approach gives 90.6% detection rate, which is within the industry-accepted accuracy of 90-95%, while achieving the computational saving for real-time processing.",
Innovative Simulations of Heavy Ion Cross Sections in 130 nm CMOS SRAM,A simulation tool to predict the heavy ion cross section is proposed. A 20% average error between experimental and simulated results is shown for a SRAM in a commercial 130 nm CMOS technology. Input parameters are obtained by device or circuit simulations and no fitting parameters or empirical calibration with previous radiation testings is needed.,"Random access memory,
Circuit testing,
Circuit simulation,
CMOS technology,
Predictive models,
Calibration,
Passivation,
Computer aided manufacturing,
Design automation"
An LDA-based Community Structure Discovery Approach for Large-Scale Social Networks,"Community discovery has drawn significant research interests among researchers from many disciplines for its increasing application in multiple, disparate areas, including computer science, biology, social science and so on. This paper describes an LDA(latent Dirichlet Allocation)-based hierarchical Bayesian algorithm, namely SSN-LDA (simple social network LDA). In SSN-LDA, communities are modeled as latent variables in the graphical model and defined as distributions over the social actor space. The advantage of SSN-LDA is that it only requires topological information as input. This model is evaluated on two research collaborative networkst: CtteSeer and NanoSCI. The experimental results demonstrate that this approach is promising for discovering community structures in large-scale networks.","Large-scale systems,
Social network services,
Application software,
Computer science,
Biology,
Bayesian methods,
Linear discriminant analysis,
Biological system modeling,
Graphical models,
Collaborative work"
Improving software practice through education: Challenges and future trends,"We argue that the software engineering (SE) community could have a significant impact on the future of the discipline by focusing its efforts on improving the education of software engineers. There are some bright spots such as the various projects to codify knowledge, and the development of undergraduate SE programs. However, there remain several key challenges, each of which is addressed in this paper: The challenges are 1) making programs attractive to students, 2) focusing education appropriately, 3) communicating industrial reality more effectively, 4) defining curricula that are forward-looking, 5) providing education for existing practitioners, 6) making SE education more evidence- based, 7) ensuring that SE educators have the necessary background, and 8) raising the prestige and quality of SE educational research. For each challenge, we provide action items and open research questions.","Software engineering,
Educational programs,
Computer science education,
Computer science,
Educational institutions,
Councils,
Educational technology,
Medical diagnostic imaging,
Software tools,
Knowledge management"
Enhanced Intrusion Detection System for Discovering Malicious Nodes in Mobile Ad Hoc Networks,"As mobile wireless ad hoc networks have different characteristics from wired networks and even from standard wireless networks, there are new challenges related to security issues that need to be addressed. Many intrusion detection systems have been proposed and most of them are tightly related to routing protocols, such as Watchdog/Pathrater and Routeguard. These solutions include two parts: intrusion detection (Watchdog) and response (Pathrater and Routeguard). Watchdog resides in each node and is based on overhearing. Through overhearing, each node can detect the malicious action of its neighbors and report other nodes. However, if the node that is overhearing and reporting itself is malicious, then it can cause serious impact on network performance. In this paper, we overcome the weakness of Watchdog and introduce our intrusion detection system called ExWatchdog. The main feature of the proposed system is its ability to discover malicious nodes which can partition the network by falsely reporting other nodes as misbehaving and then proceeds to protect the network. Simulation results show that our system decrease the overhead greatly, though it does not increase the throughput obviously.","Intrusion detection,
Mobile ad hoc networks,
Peer to peer computing,
Routing protocols,
Ad hoc networks,
Wireless networks,
Authentication,
Communications Society,
Computer networks,
Information science"
"Digital Relationships in the ""MySpace"" Generation: Results From a Qualitative Study","A qualitative study was conducted to explore how subjects use social networking sites and instant messenger to engage in interpersonal relationships. The results were used to develop a preliminary framework that models how attitudes towards privacy and impression management, when mediated by technology, translate into social interactions. This paper begins with a review of relevant literature, then describes the experimental design, summarizes the results, introduces the framework, and finishes with a discussion of conclusions and implications for future research. This paper describes the collection and analysis of qualitative data, and its use to inform a preliminary theoretical framework that can support future research into the design of systems that support social interactions","MySpace,
Computer mediated communication,
Social network services,
Communications technology,
Information processing,
Context modeling,
Privacy,
Technology management,
Social implications of technology,
Design for experiments"
Anonymous Data Collection in Sensor Networks,"Sensor networks involving human participants will require privacy protection before wide deployment is feasible. This paper proposes and evaluates a set of protocols that enable anonymous data collection in a sensor network. Sensor nodes, instead of transmitting their actual data, transmit a sample of the data complement to a basestation. The basestation then uses the negative samples to reconstruct a histogram of the original sensor readings. These protocols, collectively defined as a negative survey, are computationally simple and do not increase communication overhead. Thus, the negative survey can be implemented efficiently on existing sensor network platforms. We analyze the accuracy of the negative survey under a variety of conditions and define a range of parameter values for which it is practical. We also describe an example traffic monitoring application that uses the negative survey to classify traffic behavior. We demonstrate that for reasonable traffic scenarios, the system accurately classifies traffic behavior without revealing private information.",
Lurking in the Shadows: Identifying Systemic Threats to Kernel Data,"The integrity of kernel code and data is fundamental to the integrity of the computer system. Tampering with the kernel data is an attractive venue for rootkit writers since malicious modifications in the kernel are harder to identify compared to their user-level counterparts. So far however, the pattern followed for tampering is limited to hiding malicious objects in user-space. This involves manipulating a subset of kernel data structures that are related to intercepting user requests or affecting the user's view of the system. Hence, defense techniques are built around detecting such hiding behavior. The contribution of this paper is to demonstrate a new class of stealthy attacks that only exist in kernel space and do not employ any hiding techniques traditionally used by rootkits. These attacks are stealthy because the damage done to the system is not apparent to the user or intrusion detection systems installed on the system and are symbolic of a more systemic problem present throughout the kernel. Our goal in building these attack prototypes was to show that such attacks are not only realistic, but worse; they cannot be detected by the current generation of kernel integrity monitors, without prior knowledge of the attack signature.","Kernel,
Data structures,
Intrusion detection,
Control systems,
File systems,
Detectors,
Computer architecture,
Monitoring,
Computer science,
Prototypes"
Mining Generalized Associations of Semantic Relations from Textual Web Content,"Traditional text mining techniques transform free text into flat bags of words representation, which does not preserve sufficient semantics for the purpose of knowledge discovery. In this paper, we present a two-step procedure to mine generalized associations of semantic relations conveyed by the textual content of Web documents. First, RDF (resource description framework) metadata representing semantic relations are extracted from raw text using a myriad of natural language processing techniques. The relation extraction process also creates a term taxonomy in the form of a sense hierarchy inferred from WordNet. Then, a novel generalized association pattern mining algorithm (GP-Close) is applied to discover the underlying relation association patterns on RDF metadata. For pruning the large number of redundant overgeneralized patterns in relation pattern search space, the GP-Close algorithm adopts the notion of generalization closure for systematic overgeneralization reduction. The efficacy of our approach is demonstrated through empirical experiments conducted on an online database of terrorist activities","Data mining,
Resource description framework,
Text mining,
Web sites,
Natural language processing,
Taxonomy,
Databases,
Association rules,
Explosives,
Information resources"
A Comparison Framework for Breathing Motion Estimation Methods From 4-D Imaging,"Motion estimation is an important issue in radiation therapy of moving organs. In particular, motion estimates from 4-D imaging can be used to compute the distribution of an absorbed dose during the therapeutic irradiation. We propose a strategy and criteria incorporating spatiotemporal information to evaluate the accuracy of model-based methods capturing breathing motion from 4-D CT images. This evaluation relies on the identification and tracking of landmarks on the 4-D CT images by medical experts. Three different experts selected more than 500 landmarks within 4-D CT images of lungs for three patients. Landmark tracking was performed at four instants of the expiration phase. Two metrics are proposed to evaluate the tracking performance of motion-estimation models. The first metric cumulates over the four instants the errors on landmark location. The second metric integrates the error over a time interval according to an a priori breathing model for the landmark spatiotemporal trajectory. This latter metric better takes into account the dynamics of the motion. A second aim of this paper is to estimate the impact of considering several phases of the respiratory cycle as compared to using only the extreme phases (end-inspiration and end-expiration). The accuracy of three motion estimation models (two image registration-based methods and a biomechanical method) is compared through the proposed metrics and statistical tools. This paper points out the interest of taking into account more frames for reliably tracking the respiratory motion.",
Approaching Ideal NoC Latency with Pre-Configured Routes,"In multi-core ASICs, processors and other compute engines need to communicate with memory blocks and other cores with latency as close as possible to the ideal of a direct buffered wire. However, current state of the art networks-on-chip (NoCs) suffer, at best, latency of one clock cycle per hop. We investigate the design of a NoC that offers close to the ideal latency in some preferred, run-time configurable paths. Processors and other compute engines may perform network reconfiguration to guarantee low latency over different sets of paths as needed. Flits in non-preferred paths are given lower priority than flits in preferred ones, and suffer a delay of one clock cycle per hop when there is no contention. To achieve our goal, we use the ""mad-postman"" technique: every incoming flit is eagerly (i.e. speculatively) forwarded to the input's preferred output, if any. This is accomplished with the mere delay of a single pre-enabled tri-state driver. We later check if that decision was correct, and if not, we forward the flit to the proper output. Incorrectly forwarded flits are classified as dead and eliminated in later hops. We use a 2D mesh topology tailored for processor-memory communication, and a modified version of XY routing that remains deadlock-free. Performance gains are significant and can be proven greatly useful in other application domains as well","Network-on-a-chip,
Delay,
Engines,
Clocks,
Wire,
Runtime,
Computer networks,
Topology,
Routing,
System recovery"
Grid-based Coordinated Routing in Wireless Sensor Networks,,"Wireless sensor networks,
Routing protocols,
Energy efficiency,
Quality of service,
Communication networks,
Base stations,
Computer science,
Floods,
Energy consumption,
Sensor systems"
Fault Prediction using Early Lifecycle Data,"The prediction of fault-prone modules in a software project has been the topic of many studies. In this paper, we investigate whether metrics available early in the development lifecycle can be used to identify fault-prone software modules. More precisely, we build predictive models using the metrics that characterize textual requirements. We compare the performance of requirements-based models against the performance of code-based models and models that combine requirement and code metrics. Using a range of modeling techniques and the data from three NASA projects, our study indicates that the early lifecycle metrics can play an important role in project management, either by pointing to the need for increased quality monitoring during the development or by using the models to assign verification and validation activities.","Predictive models,
Fault diagnosis,
NASA,
Project management,
Monitoring,
Software tools,
Software reliability,
Data engineering,
Reliability engineering,
Computer science"
Focus Area Extraction by Blind Deconvolution for Defining Regions of Interest,"We present an automatic focus area estimation method, working with a single image without a priori information about the image, the camera, or the scene. It produces relative focus maps by localized blind deconvolution and a new residual error-based classification. Evaluation and comparison is performed and applicability is shown through image indexing",
Reputation-Based System for Encouraging the Cooperation of Nodes in Mobile Ad Hoc Networks,"In a mobile ad hoc network, node cooperation in packet forwarding is required in order for the network to function properly. However, some selfish nodes might intend not to forward packets in order to save resources for their own use. To discourage such behavior, we propose a reputation-based system to detect selfish nodes and respond to them by showing that being cooperative will benefit them more than being selfish. In this paper, besides cooperative nodes and selfish nodes, we introduce a new type of node called a suspicious node. These suspicious nodes will be further investigated and if they tend to behave selfishly, we could take some actions against them like we do to selfish nodes to encourage them to be cooperative. We introduce the use of a state model to decide what we should do or respond to nodes in each state. In addition, we introduce the use of a timing period to control when the reputation should be updated.","Mobile ad hoc networks,
Peer to peer computing,
Monitoring,
Timing,
Routing protocols,
Transmitting antennas,
Communications Society,
Computer science,
Degradation,
Power generation economics"
Stabilization with disturbance attenuation over a Gaussian channel,"We propose a linear control and communication scheme for the purposes of stabilization and disturbance attenuation when a discrete Gaussian channel is present in the feedback loop. Specifically, the channel input is amplified by a constant gain before transmission and the channel output is processed through a linear time invariant filter to produce the control signal. We show how the gain and filter may be chosen to minimize the variance of the plant output. For an order one plant, our scheme achieves the theoretical minimum taken over a much broader class of compensators.","Attenuation,
Gaussian channels,
Communication system control,
Linear feedback control systems,
Hydrogen,
Feedback loop,
Nonlinear filters,
State feedback,
Estimation error,
Signal processing"
Efficient gather and scatter operations on graphics processors,"Gather and scatter are two fundamental data-parallel operations, where a large number of data items are read (gathered) from or are written (scattered) to given locations. In this paper, we study these two operations on graphics processing units (GPUs). With superior computing power and high memory bandwidth, GPUs have become a commodity multiprocessor platform for general-purpose high-performance computing. However, due to the random access nature of gather and scatter, a naive implementation of the two operations suffers from a low utilization of the memory bandwidth and consequently a long, unhidden memory latency. Additionally, the architectural details of the GPUs, in particular, the memory hierarchy design, are unclear to the programmers. Therefore, we design multi-pass gather and scatter operations to improve their data access locality, and develop a performance model to help understand and optimize these two operations. We have evaluated our algorithms in sorting, hashing, and the sparse matrix-vector multiplication in comparison with their optimized CPU counterparts. Our results show that these optimizations yield 2--4X improvement on the GPU bandwidth utilization and 30--50% improvement on the response time. Overall, our optimized GPU implementations are 2--7X faster than their optimized CPU counterparts.","Scattering,
Graphics,
Robustness,
Delay,
Computer architecture,
Computer science,
Data engineering,
Large-scale systems,
Aggregates,
Bandwidth"
E-Learning 2.0 = e-Learning 1.0 + Web 2.0?,"You"" has been elected as person of the year by the Time Magazine, Web 2.0 is the most quoted article of Wikipedia in 2006 and e-Learning 2.0 is the buzzword of today. This article likes to point out what are the advantages and disadvantages of this hype. Is e-Learning 1.0 a thing of the past or still necessary for the learning future. The question whether Web 2.0 will change the education of tomorrow or not will be answered in a very critical way. The summary of this article pointed out that there is considerably more than using new applications and bring them together with the experiences of e-Learning 1.0. Due to the fact that the importance of e-Learning 2.0 is growing very fast it can be summarized that a lot of more research work must be done in future","Electronic learning,
Computer science education,
Animation,
Web sites,
Internet,
Educational institutions,
Wikipedia,
Computer networks,
IP networks,
Discussion forums"
Progress in Multimodality Imaging: Truly Simultaneous Ultrasound and Magnetic Resonance Imaging,"Multimodality medical imaging takes advantage of the strengths of different imaging modalities to provide a more complete picture of the anatomy under investigation. Many complementary modalities have been combined to form such systems and some are gaining use clinically. One combination that has not been developed, in large part due to technical difficulties, is a combined magnetic resonance (MR) and ultrasound (US) imaging system. Such a system offers the potential to combine the strengths of these modalities in a wide range of diagnostic and therapeutic applications. The goal of this study was to evaluate the feasibility of performing simultaneous multimodality US and MR imaging. An US imaging system capable of operation in a clinical MR imager was developed, and methods to perform simultaneous imaging were investigated. Simultaneous imaging was feasible without any mutual interference by either filtering the transmitted and received US signal, or by synchronizing data acquisition between the two imaging systems. Spatial registration between the two modalities was achieved by using a reference phantom with implanted glass beads in orthogonal planes. Excellent agreement was observed between spatial measurements of an object made with both modalities, and the feasibility of using this system in vivo was demonstrated in a rabbit model. Simultaneous US and MR imaging is achievable, and can provide complementary information about an object under investigation. This demonstration of technical feasibility and the development of a prototype system open up the potential to investigate the promising clinical applications of this combined technology.","Ultrasonic imaging,
Magnetic resonance imaging,
Biomedical imaging,
Anatomy,
Magnetic resonance,
Performance evaluation,
Interference,
Filtering,
Data acquisition,
Imaging phantoms"
Opportunistic Mobile Sensor Data Collection with SCAR,"Sensors are now embedded in all sorts of devices (such as phones and PDAs) and attached to many moving things such as robots, vehicles and animals. The collection of data from these mobile sensors presents challenges related to the variability of the topology of the sensor network and the need to limit communication (for energy or bandwidth saving). Fortunately, the data collected, despite considerable, is often delay tolerant and its delivery to the sinks is, in most cases, not time critical. We have devised SCAR, a context aware opportunistic routing protocol which allows efficient routing of sensor data to sinks, through selection of best paths by prediction over movement patterns and current battery level of nodes. In this paper we present the implementation of the protocol in Contiki and validate the approach through the use of the COOJA simulator with mobility traces provided by the ZebraNet Project. We compare the performance with respect to random choice based dissemination.","Personal digital assistants,
Robot sensing systems,
Vehicles,
Animals,
Network topology,
Mobile communication,
Bandwidth,
Delay effects,
Context awareness,
Routing protocols"
Classification Based on Cortical Folding Patterns,"We describe here a classification system based on automatically identified cortical sulci. Multivariate recognition methods are required for the detection of complex brain patterns with a spatial distribution. However, such methods may face the well-known issue of the curse of dimensionality-the risk of overfitting the training dataset in high-dimensional space. We overcame this problem, using a classifier pipeline with one- or two-stage of descriptor selection based on machine-learning methods, followed by a support vector machine classifier or linear discriminant analysis. We compared alternative designs of the pipeline on two different datasets built from the same database corresponding to 151 brains. The first dataset dealt with cortex asymmetry and the second dealt with the effect of the subject's sex. Our system successfully (98%) distinguished between the left and right hemispheres on the basis of sulcal shape (size, depth, etc.). The sex of the subject could be determined with a success rate of 85%. These results highlight the attractiveness of multivariate recognition models combined with appropriate descriptor selection. The sulci selected by the pipeline are consistent with previous whole-brain studies on sex effects and hemispheric asymmetries","Pattern recognition,
Pipelines,
Linear discriminant analysis,
Psychology,
Diseases,
Biomedical imaging,
Hospitals,
Face detection,
Support vector machines,
Support vector machine classification"
Recognizing Groceries in situ Using in vitro Training Data,"The problem of using pictures of objects captured under ideal imaging conditions (here referred to as in vitro) to recognize objects in natural environments (in situ) is an emerging area of interest in computer vision and pattern recognition. Examples of tasks in this vein include assistive vision systems for the blind and object recognition for mobile robots; the proliferation of image databases on the web is bound to lead to more examples in the near future. Despite its importance, there is still a need for a freely available database to facilitate study of this kind of training/testing dichotomy. In this work one of our contributions is a new multimedia database of 120 grocery products, GroZi-120. For every product, two different recordings are available: in vitro images extracted from the web, and in situ images extracted from camcorder video collected inside a grocery store. As an additional contribution, we present the results of applying three commonly used object recognition/detection algorithms (color histogram matching, SIFT matching, and boosted Haar-like features) to the dataset. Finally, we analyze the successes and failures of these algorithms against product type and imaging conditions, both in terms of recognition rate and localization accuracy, in order to suggest ways forward for further research in this domain.","In vitro,
Training data,
Pattern recognition,
Image recognition,
Object recognition,
Image databases,
Computer vision,
Veins,
Machine vision,
Mobile robots"
Using Genetic Algorithms to Aid Test-Data Generation for Data-Flow Coverage,"This paper presents an automatic test-data generation technique that uses a genetic algorithm (GA) to generate test data that satisfy data-flow coverage criteria. The technique applies the concepts of dominance relations between nodes to define a new multi-objective fitness function to evaluate the generated test data. The paper also presents the results of a set of empirical studies conducted on a set of programs that evaluate the effectiveness of our technique compared to the random-testing technique. The studies show the effective of our technique in achieving coverage of the test requirements, and in reducing the size of test suites, the search time, and the number of iterations required to satisfy the data-flow criteria.","Genetic algorithms,
Automatic testing,
Software testing,
Educational institutions,
Automatic control,
Software engineering,
Computer science,
Genetic mutations"
Towards Comparison of Deadband Sampling Types,"The deadband sampling can be used in networked control systems to reduce the number of triggered events. The purpose of this paper is to define the influence of different factors on the efficiency of deadband sampling. Different deadband criteria, control algorithms and configurations of closed control loops using deadband samplings are compared on the basis of extensive simulations. The control loop performance and the number of triggered events are evaluated simultaneously.","Sampling methods,
Delay,
Networked control systems,
Automatic control,
Automation,
Computer science,
Electronic mail,
Protocols,
Jitter,
Degradation"
Two-Tier Multiple Query Optimization for Sensor Networks,"When there are multiple queries posed to the resource-constrained wireless sensor network, it is critical to process them efficiently. In this paper, we propose a two-tier multiple query optimization (TTMQO) scheme. The first tier, called base station optimization, adopts a cost-based approach to rewrite a set of queries into an optimized set that shares the commonality and eliminates the redundancy among the queries in the original set. The optimized queries are then injected into the wireless sensor network. In the second tier, called in-network optimization, our scheme efficiently delivers query results by taking advantage of the broadcast nature of the radio channel and sharing the sensor readings among similar queries over time and space at a finer granularity. Our experimental results indicate that our proposed TTMQO scheme offers significant improvements over the traditional single query optimization technique.",
A Fast Fully 4-D Incremental Gradient Reconstruction Algorithm for List Mode PET Data,"We describe a fast and globally convergent fully four-dimensional incremental gradient (4DIG) algorithm to estimate the continuous-time tracer density from list mode positron emission tomography (PET) data. Detection of 511-keV photon pairs produced by positron-electron annihilation is modeled as an inhomogeneous Poisson process whose rate function is parameterized using cubic B-splines. The rate functions are estimated by minimizing the cost function formed by the sum of the negative log-likelihood of arrival times, spatial and temporal roughness penalties, and a negativity penalty. We first derive a computable bound for the norm of the optimal temporal basis function coefficients. Based on this bound we then construct and prove convergence of an incremental gradient algorithm. Fully 4-D simulations demonstrate the substantially faster convergence behavior of the 4DIG algorithm relative to preconditioned conjugate gradient. Four-dimensional reconstructions of real data are also included to illustrate the performance of this method","Reconstruction algorithms,
Positron emission tomography,
Image reconstruction,
Spatial resolution,
Signal processing,
Image processing,
Convergence,
Heuristic algorithms,
Image resolution,
Maximum likelihood estimation"
Fully Automatic Liver Segmentation through Graph-Cut Technique,"The accurate knowledge of the liver structure including blood vessels topography, liver surface and lesion localizations is usually required in treatments like liver ablations and radiotherapy. In this paper, we propose an approach for automatic segmentation of liver complex geometries. It consists of applying a graph-cut method initialized by an adaptive threshold. The algorithm has been tested on 10 datasets (CT and MR). A parametric comparison with the results obtained by previous algorithms based on active contour is also carried out and discussed. Main limitations of active contour approaches result to be overcome and segmentation is improved. Feasibility to routinely use graph-cut approach for automatic liver segmentation is also demonstrated.","Computed tomography,
Active contours,
Cancer,
Image segmentation,
Surface topography,
Liver neoplasms,
Biomedical imaging,
Physiology,
Surgery,
Surface morphology"
Learning Visual Representations using Images with Captions,"Current methods for learning visual categories work well when a large amount of labeled data is available, but can run into severe difficulties when the number of labeled examples is small. When labeled data is scarce it may be beneficial to use unlabeled data to learn an image representation that is low-dimensional, but nevertheless captures the information required to discriminate between image categories. This paper describes a method for learning representations from large quantities of unlabeled images which have associated captions; the goal is to improve learning in future image classification problems. Experiments show that our method significantly outperforms (1) a fully-supervised baseline model, (2) a model that ignores the captions and learns a visual representation by performing PCA on the unlabeled images alone and (3) a model that uses the output of word classifiers trained using captions and unlabeled data. Our current work concentrates on captions as the source of meta-data, but more generally other types of meta-data could be used.",
A Taxonomy Learning Method and Its Application to Characterize a Scientific Web Community,"The need to extract and manage domain-specific taxonomies has become increasingly relevant in recent years. A taxonomy is a form of business intelligence used to integrate information, reduce semantic heterogeneity, describe emergent communities and interest groups, and facilitate communication between information systems. We present a semiautomated strategy to extract domain-specific taxonomies from Web documents and its application to model a network of excellence in the emerging research field of enterprise interoperability",
Performance Evaluation for Three-Dimensional Networks-On-Chip,"Three dimensional (3D) integrated circuits (ICs) are capable of achieving better performance, functionality, and packaging density compared to more traditional planar ICs. On the other hand, networks-on-chip (NoCs) are an enabling solution for integrating large numbers of embedded cores in a single die. 3D NoC architectures combine the benefits of these two new domains to offer an unprecedented performance gain. In this paper, the authors develop a consistent and meaningful evaluation methodology to evaluate the performance of a variety of 3D NoC architectures compared to existing 2D counterparts. The authors demonstrate that the 3D NoCs are capable of achieving higher throughput, lower latency, and lower energy dissipation at the cost of small silicon area overhead.",
TTM: An Efficient Mechanism to Detect Wormhole Attacks in Wireless Ad-hoc Networks,,"Time to market,
Ad hoc networks,
Mobile ad hoc networks,
Routing protocols,
Data engineering,
Computer networks,
Computer science,
Cryptography,
Proposals,
Hardware"
Modeling and Analysis of Self-Heating in FinFET Devices for Improved Circuit and EOS/ESD Performance,"A rigorous analytical thermal model has been formulated for the analysis of self-heating effects in FinFETs, under both steady-state and transient stress conditions. 3-D self-consistent electrothermal simulations, calibrated with experimentally measured electrical characteristics, were used to understand the nature of self- heating in FinFETs and calibrate the proposed model. The accuracy of the model has been demonstrated for a wide range of multi-fin devices, by comparing against finite element simulations. The model has been applied to carry out a detailed sensitivity analysis of self-heating with respect to various FinFET parameters and structures which are critical for improving circuit performance and EOS/ESD reliability. The transient model has been used to estimate the thermal time constants of these devices and predict the sensitivity of power-to-failure to various device parameters, for both long and short pulse ESD situations.",
Variable Interactions in Query-Driven Visualization,"Our ability to generate ever-larger, increasingly-complex data, has established the need for scalable methods that identify, and provide insight into, important variable trends and interactions. Query-driven methods are among the small subset of techniques that are able to address both large and highly complex datasets. This paper presents a new method that increases the utility of query-driven techniques by visually conveying statistical information about the trends that exist between variables in a query. In this method, correlation fields, created between pairs of variables, are used with the cumulative distribution functions of variables expressed in a users query. This integrated use of cumulative distribution functions and correlation fields visually reveals, with respect to the solution space of the query, statistically important interactions between any three variables, and allows for trends between these variables to be readily identified. We demonstrate our method by analyzing interactions between variables in two flame-front simulations.","Data visualization,
Distribution functions,
Fires,
Histograms,
Analytical models,
Throughput,
Large-scale systems,
Performance analysis,
Combustion,
Chemicals"
Optimal Lattice-Reduction Aided Successive Interference Cancellation for MIMO Systems,"In this letter, we investigated the optimal minimum-mean-squared-error (MMSE) based successive interference cancellation (SIC) strategy designed for lattice-reduction aided multiple-input multiple-output (MIMO) detectors. For the sake of generating the MMSE-based MIMO symbol estimate at each SIC detection stage, we model the so-called effective symbols generated with the aid of lattice-reduction as joint Gaussian distributed random variables. However, after lattice-reduction, the effective symbols become correlated and exhibit a non-zero mean. Hence, we derive the optimal MMSE SIC detector, which updates the mean and variance of the effective symbols at each SIC detection stage. As a result, the proposed detector achieves a better performance compared to its counterpart dispensing with updating the mean and variance, and performs close to the maximum likelihood detector.",
Multi-class object tracking algorithm that handles fragmentation and grouping,"We propose a framework for detecting and tracking multiple interacting objects, while explicitly handling the dual problems of fragmentation (an object may be broken into several blobs) and grouping (multiple objects may appear as a single blob). We use foreground blobs obtained by background subtraction from a stationary camera as measurements. The main challenge is to associate blob measurements with objects, given the fragment-object-group ambiguity when the number of objects is variable and unknown, and object-class-specific models are not available. We first track foreground blobs till they merge or split. We then build an inference graph representing merge-split relations between the tracked blobs. Using this graph and a generic object model based on spatial connectedness and coherent motion, we label the tracked blobs as whole objects, fragments of objects or groups of interacting objects. The outputs of our algorithm are entire tracks of objects, which may include corresponding tracks from groups during interactions. Experimental results on multiple video sequences are shown.",
Analytical Synthesis of Generalized Multi-band Microwave Filters,An analytical procedure is presented for the synthesis of generalized multi-band filters. The procedure is applicable to both symmetric and asymmetric frequency responses and effective for any number of passbands and stopbands. An optimum equal-ripple performance is obtained in each passband and stopband. Prescribed real or complex transmission zeros can also be employed in the multi-band filtering function.,"Microwave filters,
Passband,
Resonator filters,
Frequency,
Transmission line matrix methods,
Filtering theory,
Transfer functions,
Network synthesis,
Poles and zeros,
Dual band"
Characterizing and Classifying Desktop Grid,"Desktop Grid has recently received the strong attraction for executing high throughput applications as CPU, storage and network capacities improve and become cheaper. Desktop Grid is different from Grid in many respects, but there is no general survey or taxonomy for desktop Grid. Therefore, we propose a new comprehensive taxonomy and survey of desktop Grid in order to characterize and categorize desktop Grid.","Taxonomy,
Grid computing,
Certification,
Network servers,
Computer networks,
Processor scheduling,
Computer science,
Throughput,
High performance computing,
Internet"
Toward an Unified Representation for Imitation of Human Motion on Humanoids,"In this paper, we present a framework for perception, visualization, reproduction and recognition of human motion. On the perception side, various human motion capture systems exist, all of them having in common to calculate a sequence of configuration vectors for the human model in the core of the system. These human models may be 2D or 3D kinematic models, or on a lower level, 2D or 3D positions of markers. However, for appropriate visualization in terms of a 3D animation, and for reproduction on an actual robot, the acquired motion must be mapped to the target 3D kinematic model. On the understanding side, various action and activity recognition systems exist, which assume input of different kinds. However, given human motion capture data in terms of a high-dimensional 3D kinematic model, it is possible to transform the configurations into the appropriate representation which is specific to the recognition module. We will propose a complete architecture, allowing the replacement of any perception, visualization, reproduction module, or target platform. In the core of our architecture, we define a reference 3D kinematic model, which we intend to become a common standard in the robotics community, to allow sharing different software modules and having common benchmarks.",
Hole Avoiding in Advance Routing in Wireless Sensor Networks,"Energy consumption is a major issue when designing routing protocols in wireless sensor networks. We propose a novel hole avoiding in advance routing protocol (HAIR) to address this issue. In the proposed protocol, a data packet can avoid meeting a ""hole"" in advance instead of bypassing a hole when it meets the hole as existing hole avoiding re-routing protocols (HARR) do. We prove that the proposed protocol can always find a routing path between a given source node and the sink if such a routing path does exist in the network. Simulation studies show that the proposed HAIR protocol constructs routing paths with shorter routing distance and less energy consumption in comparison with the existing HARR protocols. The proposed protocol can be used in large-scale wireless sensor networks.",
A Variational Approach to Problems in Calibration of Multiple Cameras,"This paper addresses the problem of calibrating camera parameters using variational methods. One problem addressed is the severe lens distortion in low-cost cameras. For many computer vision algorithms aiming at reconstructing reliable representations of 3D scenes, the camera distortion effects will lead to inaccurate 3D reconstructions and geometrical measurements if not accounted for. A second problem is the color calibration problem caused by variations in camera responses that result in different color measurements and affects the algorithms that depend on these measurements. We also address the extrinsic camera calibration that estimates relative poses and orientations of multiple cameras in the system and the intrinsic camera calibration that estimates focal lengths and the skew parameters of the cameras. To address these calibration problems, we present multiview stereo techniques based on variational methods that utilize partial and ordinary differential equations. Our approach can also be considered as a coordinated refinement of camera calibration parameters. To reduce computational complexity of such algorithms, we utilize prior knowledge on the calibration object, making a piecewise smooth surface assumption, and evolve the pose, orientation, and scale parameters of such a 3D model object without requiring a 2D feature extraction from camera views. We derive the evolution equations for the distortion coefficients, the color calibration parameters, the extrinsic and intrinsic parameters of the cameras, and present experimental results.","Calibration,
Cameras,
Distortion measurement,
Lenses,
Computer vision,
Layout,
Stereo vision,
Differential equations,
Computational complexity,
Feature extraction"
Visual Simulation of Heat Shimmering and Mirage,"We provide a physically-based framework for simulating the natural phenomena related to heat interaction between objects and the surrounding air. We introduce a heat transfer model between the heat source objects and the ambient flow environment, which includes conduction, convection, and radiation. The heat distribution of the objects is represented by a novel temperature texture. We simulate the thermal flow dynamics that models the air flow interacting with the heat by a hybrid thermal lattice Boltzmann model (HTLBM). The computational approach couples a multiple-relaxation-time LBM (MRTLBM) with a finite difference discretization of a standard advection-diffusion equation for temperature. In heat shimmering and mirage, the changes in the index of refraction of the surrounding air are attributed to temperature variation. A nonlinear ray tracing method is used for rendering. Interactive performance is achieved by accelerating the computation of both the MRTLBM and the heat transfer, as well as the rendering on contemporary graphics hardware (GPU)","Heat transfer,
Rendering (computer graphics),
Temperature distribution,
Computational modeling,
Lattice Boltzmann methods,
Finite difference methods,
Difference equations,
Nonlinear equations,
Ray tracing,
Acceleration"
An 81.6 GOPS Object Recognition Processor Based on NoC and Visual Image Processing Memory,An 81.6 GOPS object recognition processor is developed by using NoC and visual image processing (VIP) memory. SIFT (scale invariant feature transform) object recognition requires huge computing power and data transactions among tasks. The chip integrates 10 SIMD PEs for data/task level parallelism while the NoC facilitates inter-PE communications. The VIP memory searches local maximum pixel inside a 3times3 window in a single cycle providing 65.6 GOPS. The proposed processor achieves 15.9 fps SIFT feature extraction at 200 MHz.,
Retransmission or Redundancy: Transmission Reliability in Wireless Sensor Networks,"As an application-driven network, wireless sensor network generally requires high data reliability to maintain detection and response capabilities. Although two approaches, which are retransmission and redundancy, have been proposed to enhance data reliability, the theoretical work is required to evaluate their impact on transmission reliability and energy efficiency. In this paper, we offer a comprehensive theoretical study on the packet arrival probability and average energy consumption for both approaches. Our analysis indicates that when loss probability remains low or moderate, erasure coding, a scheme based on redundancy, is more reliable and energy efficient than retransmission. However, the performance of erasure coding would largely deteriorate under high packet loss condition. We also demonstrate that its resistance capability against packet loss weakens as hop number increases. Furthermore, with the increase in redundancy, erasure coding has to sacrifice the advantage of energy efficiency for reliability.","Redundancy,
Wireless sensor networks,
Reliability theory,
Energy efficiency,
Computer network reliability,
Energy consumption,
Telecommunication network reliability,
Maintenance,
Defense industry,
Computer science"
Robust 3D Face Recognition Using Learned Visual Codebook,"In this paper, we propose a novel learned visual code-book (LVC) for 3D face recognition. In our method, we first extract intrinsic discriminative information embedded in 3D faces using Gabor filters, then K-means clustering is adopted to learn the centers from the filter response vectors. We construct LVC by these learned centers. Finally we represent 3D faces based on LVC and achieve recognition using a nearest neighbor (NN) classifier. The novelty of this paper comes from 1) We first apply textons based methods into 3D face recognition; 2) We encompass the efficiency of Gabor features for face recognition and the robustness of texton strategy for texture classification simultaneously. Our experiments are based on two challenging databases, CASIA 3D face database and FRGC2.0 3D face database. Experimental results show LVC performs better than many commonly used methods.","Robustness,
Face recognition,
Spatial databases,
Data mining,
Gabor filters,
Principal component analysis,
Neural networks,
Flowcharts,
Tensile stress,
Histograms"
Analysis of Feature Space for Monitoring Persons with Parkinson's Disease With Application to a Wireless Wearable Sensor System,"We present work to develop a wireless wearable sensor system for monitoring patients with Parkinson's disease (PD) in their homes. For monitoring outside the laboratory, a wearable system must not only record data, but also efficiently process data on-board. This manuscript details the analysis of data collected using tethered wearable sensors. Optimal window length for feature extraction and feature ranking were calculated, based on their ability to capture motor fluctuations in persons with PD. Results from this study will be employed to develop a software platform for the wireless system, to efficiently process on-board data.",
Multiobjective Genetic Fuzzy Systems: Review and Future Research Directions,Evolutionary algorithms have been successfully used in many studies to design accurate and interpretable fuzzy systems under the name of genetic fuzzy systems. Recently evolutionary multiobjective algorithms have been used for interpretability-accuracy tradeoff analysis of fuzzy systems. We first review a wide range of related studies to multiobjective genetic fuzzy systems. Then we illustrate multiobjective design of fuzzy systems through computational experiments on some benchmark data sets. Finally we point out promising future research directions.,"Genetics,
Fuzzy systems,
Evolutionary computation,
Neural networks,
Fuzzy neural networks,
Algorithm design and analysis,
Training data,
Partitioning algorithms,
Computer science,
Intelligent systems"
Weighted Local Variance-Based Edge Detection and Its Application to Vascular Segmentation in Magnetic Resonance Angiography,"Accurate detection of vessel boundaries is particularly important for a precise extraction of vasculatures in magnetic resonance angiography (MRA). In this paper, we propose the use of weighted local variance (WLV)-based edge detection scheme for vessel boundary detection in MRA. The proposed method is robust against changes of intensity contrast of edges and capable of giving high detection responses on low contrast edges. These robustness and capabilities are essential for detecting the boundaries of vessels in low contrast regions of images, which can contain intensity inhomogeneity, such as bias field, interferences induced from other tissues, or fluctuation of the speed related vessel intensity. The performance of the WLV-based edge detection scheme is studied and shown to be able to return strong and consistent detection responses on low contrast edges in the experiments. The proposed edge detection scheme can be embedded naturally in the active contour models for vascular segmentation. The WLV-based vascular segmentation method is tested using MRA image volumes. It is experimentally shown that the WLV-based edge detection approach can achieve high-quality segmentation of vasculatures in MRA images.","Image edge detection,
Magnetic resonance,
Angiography,
Biomedical imaging,
Image segmentation,
Active contours,
Blood vessels,
Eigenvalues and eigenfunctions,
Robustness,
Computer science"
Balancing Push and Pull for Efficient Information Discovery in Large-Scale Sensor Networks,"In this paper, we investigate efficient strategies for supporting on-demand information dissemination and gathering in large-scale wireless sensor networks. In particular, we propose a ""comb-needle"" discovery support model resembling an ancient method: use a comb to help find a needle in sand or a haystack. The model combines push and pull for information dissemination and gathering. The push component features data duplication in a linear neighborhood of each node. The pull component features a dynamic formation of an on-demand routing structure resembling a comb. The comb-needle model enables us to investigate the cost of a spectrum of push and pull combinations for supporting query and discovery in large-scale sensor networks. Our result shows that the optimal routing structure depends on the frequency of query occurrence and the spatial-temporal frequency of related events in the network. The benefit of balancing push and pull for information discovery is demonstrated",
Automated Generation of Context-Aware Tests,"The incorporation of context-awareness capabilities into pervasive applications allows them to leverage contextual information to provide additional services while maintaining an acceptable quality of service. These added capabilities, however, introduce a distinct input space that can affect the behavior of these applications at any point during their execution, making their validation quite challenging. In this paper, we introduce an approach to improve the test suite of a context-aware application by identifying context-aware program points where context changes may affect the application's behavior, and by systematically manipulating the context data fed into the application to increase its exposure to potentially valuable context variations. Preliminary results indicate that the approach is more powerful than existing testing approaches used on this type of application.",
Ferry: A P2P-Based Architecture for Content-Based Publish/Subscribe Services,"We propose Ferry, an architecture that extensively yet wisely exploits the underlying distributed hash table (DHT) overlay structure to build an efficient and scalable platform for content-based publish/subscribe (pub/sub) services. Ferry aims to host any and many content-based pubservices: Any pubsub service with a unique scheme can run on top of Ferry, and multiple pub/sub services can coexist on top of Ferry. For each pub/sub service, Ferry does not need to maintain or dynamically generate any dissemination tree. Instead, it exploits the embedded trees in the underlying DHT to deliver events, thereby imposing little overhead. Ferry can support a pub/sub scheme with a large number of event attributes. To deal with a skewed distribution of subscriptions and events, Ferry uses one-hop subscription push and attribute partitioning to balance load","Subscriptions,
Scalability,
Content management,
Large-scale systems,
Filtering,
Matched filters,
Data structures,
Query processing,
Routing,
Network servers"
Middleware Vertical Handoff Manager: A Neural Network-Based Solution,"Major research challenges in the next generation of wireless networks include the provisioning of worldwide seamless mobility across heterogeneous wireless networks, the improvement of end-to-end quality of service (QoS), supporting high data rates over wide area and enabling users to specify their personal preferences. The integration and interoperability of this multitude of available networks will lead to the emergence of the fourth generation (4G) of wireless technologies. 4G wireless technologies have the potential to provide these features and many more, which at the end will change the way we use mobile devices and provide a wide variety of new applications. However, such technology does not come without its challenges. One of these challenges is the user's ability to control and manage handoffs across heterogeneous wireless networks. This paper proposes a solution to this problem using artificial neural networks (ANNs). The proposed method is capable of distinguishing the best existing wireless network that matches predefined user preferences set on a mobile device when performing a vertical handoff. The overall performance of the proposed method shows 87.0 % success rate in finding the best available wireless network. To test for the robustness and effectiveness of the neural network algorithm, some of the features were removed from the training set and results showed a significant impact on the overall performance of the system. Hence, managing vertical handoffs through user preferences can be significantly affected with the selection of features used to provide the closest match of the available wireless networks.","Middleware,
Neural networks,
Wireless networks,
Quality of service,
Roaming,
Communications Society,
Computer network management,
Quality management,
Computer networks,
Information science"
NASA World Wind: Opensource GIS for Mission Operations,"This paper describes NASA World Wind, its technical architecture and performance, and its emerging use for mission operations. World Wind is a geographic information system that provides graphical access to terabytes of imagery and elevation models for planets and other celestial objects including satellite and other data of the Earth, Moon, Mars, Venus, and Jupiter; as well as astronomical data made available through the Sloan Digital Sky Survey. World Wind is also a customizable system that can be integrated as part of other applications. World Wind is not only an application in which add-ons can be integrated, but is also being developed as a plugin that can be integrated with other applications. This paper also describes the significant contributions of the international opensource community in making World Wind what it is today. Contributions have involved the following: 1) lead development of add-ons, several of which have been integrated as part of the core system available for direct download via sourceforge, 2) lead provider of high-resolution data sets, 3) lead help desk support through Internet relay chat for end-users and developers, and 4) significant technical contributions to the core system including bug identification, tracking and resolution as well as ideas for new features and source code modifications.","NASA,
Geographic Information Systems,
Planets,
Satellites,
Earth,
Moon,
Mars,
Venus,
Jupiter,
Internet"
Network-aware P2P file sharing over the wireless mobile networks,"With the coming wireless mobile networks era and the popular use of P2P applications, how to improve the resource retrieval and discovery for P2P file sharing applications in wireless mobile networks becomes a critical issue. In this paper, we propose a novel network-aware P2P file architecture and related control schemes that can provide continuous resource retrieval and discovery for mobile users over the wireless network environment. The proposed architecture divides a P2P file sharing network into multiple network-aware clusters, in which peers are assigned to a network-aware cluster using a network prefix division. Accordingly, there are two designs for supporting mobile peers to retrieve files in wireless mobile networks. First, a novel file discovery control scheme named mobility-aware file discovery control (MAFDC) scheme is devised to obtain fresh status of shared peers and find the new resource providing peers in wireless mobile networks. Second, a resource provider selection algorithm is devised to enable a mobile peer to select new resource providing peers for continuous file retrieval.","Peer to peer computing,
Wireless networks,
Music information retrieval,
Network topology,
Computer science,
Data communication,
Video sharing,
File servers,
Network servers,
Bandwidth"
Near-Optimal Node Clustering in Wireless sensor Networks for Environment Monitoring,"Wireless sensor networks (WSNs) for environment monitoring consist of a large number of low-cost battery-powered sensors nodes, densely deployed throughout a remote or inaccessible physical space. ""Energy conservation"" has been identified as the key challenge in the design and operation of these networks. At the same time, clustering of sensor nodes has been widely recognized as the most promising approach in dealing with the given challenge. In our earlier work, we examine the actual energy-conservation effectiveness of node clustering in WSNs, and we prove that only clustering schemes that position their resultant clusters within the isoclusters1 of the monitored phenomenon are guaranteed to reduce the nodes' energy consumption and extend the network lifetime. A thorough review of the known literature on WSNs shows that the existing WSN clustering algorithms commonly do not satisfy the above requirement, i.e. they do not consider the similarity of sensed data as an important clustering criterion. Therefore, the utilization of these algorithms cannot be considered truly effective in dealing with the WSN energy conservation challenge. In this paper, we propose a novel WSN clustering algorithm - local negotiated clustering algorithm (LNCA). To our knowledge, LNCA is the first clustering algorithm that employs the similarity of nodes' readings as the main criterion in cluster formation. As such, LNCA is highly effective in minimizing in-network data-reporting traffic and, accordingly, in reducing the energy usage of individual sensor nodes. Our simulation results show clear performance supremacy of LNCA over two popular WSN clustering algorithms: low-energy adaptive clustering hierarchy (LEACH) and weight clustering algorithm (WCA).","Wireless sensor networks,
Clustering algorithms,
Computerized monitoring,
Remote monitoring,
Computer science,
Power engineering and energy,
Energy conservation,
Intelligent networks,
Energy consumption,
Traffic control"
Multi-View Stereo via Graph Cuts on the Dual of an Adaptive Tetrahedral Mesh,"We formulate multi-view 3D shape reconstruction as the computation of a minimum cut on the dual graph of a semi- regular, multi-resolution, tetrahedral mesh. Our method does not assume that the surface lies within a finite band around the visual hull or any other base surface. Instead, it uses photo-consistency to guide the adaptive subdivision of a coarse mesh of the bounding volume. This generates a multi-resolution volumetric mesh that is densely tesselated in the parts likely to contain the unknown surface. The graph-cut on the dual graph of this tetrahedral mesh produces a minimum cut corresponding to a triangulated surface that minimizes a global surface cost functional. Our method makes no assumptions about topology and can recover deep concavities when enough cameras observe them. Our formulation also allows silhouette constraints to be enforced during the graph-cut step to counter its inherent bias for producing minimal surfaces. Local shape refinement via surface deformation is used to recover details in the reconstructed surface. Reconstructions of the Multi- View Stereo Evaluation benchmark datasets and other real datasets show the effectiveness of our method.","Surface reconstruction,
Image reconstruction,
Shape,
Stereo image processing,
Topology,
Cost function,
Refining,
Robustness,
Computer science,
Mesh generation"
Dual RSA and Its Security Analysis,"We present new variants of an RSA whose key generation algorithms output two distinct RSA key pairs having the same public and private exponents. This family of variants, called dual RSA, can be used in scenarios that require two instances of RSA with the advantage of reducing the storage requirements for the keys. Two applications for dual RSA, blind signatures and authentication/secrecy, are proposed. In addition, we also provide the security analysis of dual RSA. Compared to normal RSA, the security boundary should be raised when applying dual RSA to the types of small-d, small-e, and rebalanced-RSA.","Security,
Public key cryptography,
Elliptic curve cryptography,
Sun,
Elliptic curves,
Computer science,
Authentication,
Lattices,
Computational efficiency,
Councils"
Enable Efficient Compound Image Compression in H.264/AVC Intra Coding,"This paper presents an efficient compound image compression approach based on H.264/AVC intra coding. The text blocks are distinguished from the picture blocks and compressed with a new coding mode. In particular, the text blocks are represented by base colors and index map in spatial domain. A color quantization algorithm optimized for compression is designed to generate this representation. As for the entropy coding of text blocks, a structure-aware context-based arithmetic coder is developed. The mode selection algorithm based on rate distortion optimization is used to select the text blocks along with H.264/AVC intra modes, which can adapt to the targeted bit-rate. Experimental results show that the proposed scheme can achieve 2.8dB gain on average for compound images compared with H.264/AVC intra coding.",
Capturing a Convex Object With Three Discs,"This paper addresses the problem of capturing an arbitrary convex object P in the plane with three congruent disc-shaped robots. Given two stationary robots in contact with P, we characterize the set of positions of a third robot, the so-called capture region, that prevent P from escaping to infinity via continuous rigid motion. We show that the computation of the capture region reduces to a visibility problem. We present two algorithms for solving this problem, and for computing the capture region when P is a polygon and the robots are points (zero-radius discs). The first algorithm is exact and has polynomial time complexity. The second one uses simple hidden surface removal techniques from computer graphics to output an arbitrarily accurate approximation of the capture region; it has been implemented, and examples are presented.","Robotics and automation,
Robot sensing systems,
Computer science,
H infinity control,
Kinematics,
Orbital robotics,
Polynomials,
Computer graphics,
Manipulators,
Mobile robots"
Reaction Mass Pendulum (RMP): An explicit model for centroidal angular momentum of humanoid robots,"A number of conceptually simple but behavior-rich ""inverted pendulum"" humanoid models have greatly enhanced the understanding and analytical insight of humanoid dynamics. However, these models do not incorporate the robot's angular momentum properties, a critical component of its dynamics. We introduce the reaction mass pendulum (RMP) model, a 3D generalization of the better-known reaction wheel pendulum. The RMP model augments the existing models by compactly capturing the robot's centroidal momenta through its composite rigid body (CRB) inertia. This model provides additional analytical insights into legged robot dynamics, especially for motions involving dominant rotation, and leads to a simpler class of control laws. In this paper we show how a humanoid robot of general geometry and dynamics can be mapped into its equivalent RMP model. A movement is subsequently mapped to the time evolution of the RMP. We also show how an ""inertia shaping"" control law can be designed based on the RMP.",
An Exploratory Study of Web Services on the Internet,"Web services technology has received much attention in the last few years, and a lot of research efforts have been devoted to utilizing services on the Internet to fulfill consumers' requirements. However, little research has been done on the current status of web services on the Internet, which has a great impact on current research. Enlightened by this situation, we made an exploratory study of the current status of web services on the Internet. Our study mainly focused on the investigation of four aspects, including the number, complexity, quality of description and the function diversity of available web services on the Internet. A web services investigation system is built up to harvest web services from the Internet and calculate the statistical results. The investigation results are reported in this paper, and, based on our study, the development trend of web services technology is also discussed in this paper.",
Face Re-Lighting from a Single Image under Harsh Lighting Conditions,"In this paper, we present a new method to change the illumination condition of a face image, with unknown face geometry and albedo information. This problem is particularly difficult when there is only one single image of the subject available and it was taken under a harsh lighting condition. Recent research demonstrates that the set of images of a convex Lambertian object obtained under a wide variety of lighting conditions can be approximated accurately by a low-dimensional linear subspace using spherical harmonic representation. However, the approximation error can be large under harsh lighting conditions thus making it difficult to recover albedo information. In order to address this problem, we propose a subregion based framework that uses a Markov Random Field to model the statistical distribution and spatial coherence of face texture, which makes our approach not only robust to harsh lighting conditions, but insensitive to partial occlusions as well. The performance of our framework is demonstrated through various experimental results, including the improvement to the face recognition rate under harsh lighting conditions.",
A Runtime Analysis of Evolutionary Algorithms for Constrained Optimization Problems,"Although there are many evolutionary algorithms (EAs) for solving constrained optimization problems, there are few rigorous theoretical analyses. This paper presents a time complexity analysis of EAs for solving constrained optimization. It is shown when the penalty coefficient is chosen properly, direct comparison between pairs of solutions using penalty fitness function is equivalent to that using the criteria ldquosuperiority of feasible pointrdquo or ldquosuperiority of objective function value.rdquo This paper analyzes the role of penalty coefficients in EAs in terms of time complexity. The results show that in some examples, EAs benefit greatly from higher penalty coefficients, while in other examples, EAs benefit from lower penalty coefficients. This paper also investigates the runtime of EAs for solving the 0-1 knapsack problem and the results indicate that the mean first hitting times ranges from a polynomial-time to an exponential time when different penalty coefficients are used.","Runtime,
Algorithm design and analysis,
Evolutionary computation,
Constraint optimization,
Convergence,
Constraint theory,
Polynomials,
Stochastic processes,
Computer science"
Age-based packet arbitration in large-radix k-ary n-cubes,"As applications scale to increasingly large processor counts, the interconnection network is frequently the limiting factor in application performance. In order to achieve application scalability, the interconnect must maintain high bandwidth while minimizing variation in packet latency. As the offered load in the network increases with growing problem sizes and processor counts, so does the expected maximum packet latency in the network, directly impacting performance of applications with any synchronized communication. Age-based packet arbitration reduces the variance in packet latency as well as average latency. This paper describes the Cray XT router packet aging algorithm which allows globally fair arbitration by incorporating ""age"" in the packet output arbitration. We describe the parameters of the aging algorithm and how to arrive at appropriate settings. We show that an efficient aging algorithm reduces both the average packet latency and the variance in packet latency on communication-intensive benchmarks.","Prefetching,
Drain avalanche hot carrier injection,
History,
Delay,
Government,
Bridges,
Performance gain,
Cache memory,
Sun,
Computer science"
A Complete Methodology for Generating Multi-Robot Task Solutions using ASyMTRe-D and Market-Based Task Allocation,"This paper presents an approach that enables heterogeneous robots to automatically form groups as needed to generate both strongly-cooperative and weakly-cooperative multi-robot task solutions in the same application. The fundamental contribution of this work is the layering of our low-level coalition formation algorithm for generating strongly-cooperative task solutions, with high-level, traditional task allocation methods for weakly-cooperative task solutions. At the low level, coalitions that generate strongly-cooperative multi-robot task solutions are formed using our ASyMTRe-D approach that maps environmental sensors and perceptual and motor schemas to the required flow of information in the robot team, automatically reconfiguring the connections of schemas within and across robots to form efficient solutions. At the high level, a traditional task allocation approach is used to enable individual robots and/or coalitions to compete for weakly-cooperative task assignments through task allocation. We introduce the site clearing task to motivate the work, and then formalize the problem. We then present the approach of layering ASyMTRe-D with task allocation. We validate the approach on a team of robots with the site clearing task. We believe the resulting approach is a flexible system that can handle a broad range of realistic multi-robot applications beyond what is possible using other existing approaches.",
Space-Efficient TCAM-Based Classification Using Gray Coding,"Ternary content-addressable memories (TCAMs) are increasingly used for high-speed packet classification. TCAMs compare packet headers against all rules in a classification database in parallel and thus provide high throughput unparalleled by software-based solutions. TCAMs are not well-suited, however, for representing rules that contain range fields. Such rules have to be represented by multiple TCAM entries. The resulting range expansion can dramatically reduce TCAM utilization. The majority of real-life database ranges are short. We present a novel algorithm called short range gray encoding (SRGE) for the efficient representation of short range rules. SRGE encodes range borders as binary reflected gray codes and then represents the resulting range by a minimal set of ternary strings. SRGE is database independent and does not use TCAM extra bits. For the small number of ranges whose expansion is not significantly reduced by SRGE, we use dependent encoding that exploits the extra bits available on today's TCAMs. Our comparative analysis establishes that this hybrid scheme utilizes TCAM more efficiently than previously published solutions. The SRGE algorithm has worst-case expansion ratio of 2W-4, where W is the range-field length . We prove that any TCAM encoding scheme has worst-case expansion ratio W or more.","Databases,
Encoding,
Computer science,
Throughput,
Streaming media,
Pattern matching,
Communications Society,
Reflective binary codes,
Internet,
Routing"
Globally Optimal Image Segmentation with an Elastic Shape Prior,"So far global optimization techniques have been developed independently for the tasks of shape matching and image segmentation. In this paper we show that both tasks can in fact be solved simultaneously using global optimization. By computing cycles of minimal ratio in a large graph spanned by the product of the input image and a shape template, we are able to compute globally optimal segmentations of the image which are similar to a familiar shape and located in places of strong gradient. The presented approach is translation-invariant and robust to local and global scaling and rotation of the given shape. We show how it can be extended to incorporate invariance to similarity transformations. The particular structure of the graph allows for run-time and memory efficient implementations. Highly parallel implementations on graphics cards allow to produce globally optimal solutions in a few seconds only.","Image segmentation,
Shape measurement,
Runtime,
Robustness,
Computer science,
Graphics,
Statistics,
Level set,
Background noise,
Noise shaping"
Degrees of Freedom of Wireless Networks - What a Difference Delay Makes,"We explore the impact of propagation delay on the degrees of freedom of wireless interference networks. For K > 2 user interference channel we show through an example that propagation delays can increase the degrees of freedom by up to a factor of K/2. We provide an example of node placement for a 4 user interference network such that the propagation delays for line-of-sight communication allow perfect interference alignment. We show that even if nodes are randomly placed, one can almost surely achieve sufficient interference alignment to approach the upperbound on the degrees of freedom by choosing the basic symbol duration small enough. An analogy with deterministic channel models is pointed out as an interesting mechanism to translate propagation delay based interference alignment schemes to delay-free Gaussian channel models.","Wireless networks,
Propagation delay,
Interference channels,
Transmitters,
Power system modeling,
Additive noise,
Decoding,
Computer science,
Delay estimation,
Signal to noise ratio"
Random Walks on Sensor Networks,"We consider the mobile data gathering problem in large-scale wireless sensor networks with static sensor nodes and a mobile patrol node. Based on the assumptions that (a) the sensor positions are unknown and (b) the network may not be entirely connected, we formulate the problem as one of random walks in random geometric graphs and derive analytical bounds for the node coverage, i.e. the number of queried sensor nodes within a given time frame. Based on this metric, we propose an algorithm that improves the data gathering performance by generating constrained random walks, in which the probability mass function at each step reflects the available side information (e.g. the memory of past visited sites).","Wireless sensor networks,
Sensor phenomena and characterization,
Mobile robots,
Robot sensing systems,
Mobile computing,
Large-scale systems,
Navigation,
Radio access networks,
Telecommunications,
Computer science"
Improvements on Sensor Noise Based Source Camera Identification,"In a novel method for identifying the source camera of a digital image is proposed. The method is based on first extracting imaging sensor's pattern noise from many images and later verifying its presence in a given image through a correlative procedure. In this paper, we investigate the performance of this method in a more realistic setting and provide results concerning its detection performance. To improve the applicability of the method as a forensic tool, we propose an enhancement over it by also verifying that class properties of the image in question are in agreement with those of the camera. For this purpose, we identify and compare characteristics due to demosaicing operation. Our results show that the enhanced method offers a significant improvement in the performance.",
Efficient Hierarchical-PCA Dimension Reduction for Hyperspectral Imagery,"Hyperspectral systems have improved significantly through recent advancements in sensor technology, which have made possible to acquire data with several hundred channels. These advances provide the possible benefit of not only collecting more detailed information than previously possible, but also of producing more accurate data. Some of the major challenges in handling such large data sets are removing redundant information and assuring the continued relevance of vital information to the application at hand. For example, conventional methods for land use and land cover classifications may not be applicable, due to the large data volumes used to characterize hyperspectral cubes. Therefore, these conventional methods may require a preprocessing step, namely dimension reduction. Dimension reduction can be seen as a transformation from a high order dimension to a low order dimension in order to conquer the so- called ""curse of the dimensionality,"" which eliminates data redundancy. Principal Component Analysis (PCA) is one such data reduction technique, which is often used when analyzing remotely sensed data. In computing the principal components, the eigenvalues of the covariance matrix of the 3-D image must be computed. Since this is a global operation it requires high computational resources and requires whole image to be stored which increases memory requirements. This paper reports an hierarchical algorithm, which can effectively reduce the hyperspectral data to intrinsic dimensionality. In the hierarchical PCA, we break the image into various parts and then perform PCA on each part separately and then combine the results. The classification results over the original and the resulting reduced data have been compared. The results show that reduced data obtained by hierarchical PCA can compare favorably to the results obtained from original data.",
Real-time Object Classification in Video Surveillance Based on Appearance Learning,"Classifying moving objects to semantically meaningful categories is important for automatic visual surveillance. However, this is a challenging problem due to the factors related to the limited object size, large intra-class variations of objects in a same class owing to different viewing angles and lighting, and real-time performance requirement in real-world applications. This paper describes an appearance-based method to achieve real-time and robust objects classification in diverse camera viewing angles. A new descriptor, i.e., the multi-block local binary pattern (MB-LBP), is proposed to capture the large-scale structures in object appearances. Based on MB-LBP features, an adaBoost algorithm is introduced to select a subset of discriminative features as well as construct the strong two-class classifier. To deal with the non-metric feature value of MB-LBP features, a multi-branch regression tree is developed as the weak classifiers of the boosting. Finally, the error correcting output code (ECOC) is introduced to achieve robust multi-class classification performance. Experimental results show that our approach can achieve real-time and robust object classification in diverse scenes.",
RAMP Blue: A Message-Passing Manycore System in FPGAs,"We are developing a set of reusable design blocks and several prototype systems for emulation of multi-core architectures in FPGAs. RAMP Blue is the first of these prototypes and was designed to emulate a distributed-memory message-passing architecture. The system consists of 768-1008 MicroBlaze cores in 64-84 Virtex-II Pro 70 FPGAs on 16-21 BEE2 boards, surpassing the milestone of 1000 cores in a standard 42U rack. An architecture based on point-to-point channels and switches using a combination of custom and generic hardware provides the functionality. Virtual-cut-through dimensional routing on one of two hybrid topologies with virtual channels provides the connectivity. A control network with a tree topology provides management and debugging capabilities. A software infrastructure consisting of GCC, uClinux and UPC allows running off-the-shelf applications and scientific benchmarks. Initial performance is encouraging for emulation purposes. In this paper we report on the design and implementation of RAMP Blue and discuss our experiences and lessons learned.",
Literature Fingerprinting: A New Method for Visual Literary Analysis,"In computer-based literary analysis different types of features are used to characterize a text. Usually, only a single feature value or vector is calculated for the whole text. In this paper, we combine automatic literature analysis methods with an effective visualization technique to analyze the behavior of the feature values across the text. For an interactive visual analysis, we calculate a sequence of feature values per text and present them to the user as a characteristic fingerprint. The feature values may be calculated on different hierarchy levels, allowing the analysis to be done on different resolution levels. A case study shows several successful applications of our new method to known literature problems and demonstrates the advantage of our new visual literature fingerprinting.","Fingerprint recognition,
Visualization,
Lifting equipment,
Vocabulary,
Application software,
Information analysis,
Visual analytics,
Computer applications,
Art,
Iron"
Hand Gesture Recognition Research Based on Surface EMG Sensors and 2D-accelerometers,"For realizing multi-DOF interfaces in wearable computer system, accelerometers and surface EMG sensors are used synchronously to detect hand movement information for multiple hand gesture recognition. Experiments were designed to collect gesture data with both sensing techniques to compare their performance in the recognition of various wrist and finger gestures. Recognition tests were run using different subsets of information: accelerometer and sEMG data separately and combined sensor data. Experimental results show that the combination of sEMG sensors and accelerometers achieved 5-10% improvement in the recognition accuracies for hand gestures when compared to that obtained using sEMG sensors solely.",
On the Memory Access Patterns of Supercomputer Applications: Benchmark Selection and Its Implications,"This paper compares the system performance evaluation cooperative (SPEC) Integer and Floating-Point suites to a set of real-world applications for high-performance computing at Sandia National Laboratories. These applications focus on the high-end scientific and engineering domains; however, the techniques presented in this paper are applicable to any application domain. The applications are compared in terms of three memory properties: 1) temporal locality (or reuse over time), 2) spatial locality (or the use of data ""near"" data that has already been accessed), and 3) data intensiveness (or the number of unique bytes the application accesses). The results show that real-world applications exhibit significantly less spatial locality, often exhibit less temporal locality, and have much larger data sets than the SPEC benchmark suite. They further quantitatively demonstrate the memory properties of real supercomputing applications.",
Multicore Surprises: Lessons Learned from Optimizing Sweep3D on the Cell Broadband Engine,"The Cell Broadband Engine (BE) processor provides the potential to achieve an impressive level of performance for scientific applications. This level of performance can be reached by exploiting several dimensions of parallelism, such as thread-level parallelism using several synergistic processing elements, data streaming parallelism, vector parallelism in the form of 128-bit SIMD operations, and pipeline parallelism by issuing multiple instructions in the same clock cycle. In our exploration to achieve the optimum level of performance for Sweep3D, we have enjoyed many pleasant surprises, such as a very high floating point performance, reaching 64% of the theoretical peak in double precision, and an over all performance speedup ranging from 4.5 times when compared with ""heavy iron"" processors, up to over 20 times with conventional processors.","Multicore processing,
Engines,
Parallel processing,
Yarn,
Laboratories,
Concurrent computing,
High performance computing,
Energy consumption,
Buildings,
Computer architecture"
Signal Sets From Functions With Optimum Nonlinearity,"Signal sets with the best correlation property are desirable in code-division multiple-access (CDMA) systems. In this paper, the construction of Wootters and Fields for mutually unbiased bases is extended into a generic construction of signal sets using planar functions. Then, specific classes of planar functions and almost bent functions are employed to obtain (q2+q,q) signal sets. The signal sets derived from planar functions are optimal with respect to the Levenstein bound, and those obtained from almost bent functions nearly meet the Levenstein bound. The signal sets constructed in this paper could have a very small alphabet size, and have applications in synchronous DS-CDMA systems, where the number of users is greater than the signal space dimension or the spreading factor",
Indexing Uncertain Categorical Data,"Uncertainty in categorical data is commonplace in many applications, including data cleaning, database integration, and biological annotation. In such domains, the correct value of an attribute is often unknown, but may be selected from a reasonable number of alternatives. Current database management systems do not provide a convenient means for representing or manipulating this type of uncertainty. In this paper we extend traditional systems to explicitly handle uncertainty in data values. We propose two index structures for efficiently searching uncertain categorical data, one based on the R-tree and another based on an inverted index structure. Using these structures, we provide a detailed description of the probabilistic equality queries they support. Experimental results using real and synthetic datasets demonstrate how these index structures can effectively improve the performance of queries through the use of internal probabilistic information.",
Architecture-Level Soft Error Analysis: Examining the Limits of Common Assumptions,"This paper concerns the validity of a widely used method for estimating the architecture-level mean time to failure (MTTF) due to soft errors. The method first calculates the failure rate for an architecture-level component as the product of its raw error rate and an architecture vulnerability factor (AVF). Next, the method calculates the system failure rate as the sum of the failure rates (SOFR) of all components, and the system MTTF as the reciprocal of this failure rate. Both steps make significant assumptions. We investigate the validity of the AVF+SOFR method across a large design space, using both mathematical and experimental techniques with real program traces from SPEC 2000 benchmarks and synthesized traces to simulate longer real-world workloads. We show that AVF+SOFR is valid for most of the realistic cases under current raw error rates. However, for some realistic combinations of large systems, long-running workloads with large phases, and/or large raw error rates, the MTTF calculated using AVF+SOFR shows significant-discrepancies from that using first principles. We also show that SoftArch, a previously proposed alternative method that does not make the AVF+SOFR assumptions, does not exhibit the above discrepancies.","Error analysis,
Rivers,
Computer science,
Computer errors,
Single event upset,
Neutrons,
Cosmic rays,
Alpha particles,
Integrated circuit packaging,
Logic devices"
Sania: Syntactic and Semantic Analysis for Automated Testing against SQL Injection,"With the recent rapid increase in interactive Web applications that employ back-end database services, an SQL injection attack has become one of the most serious security threats. The SQL injection attack allows an attacker to access the underlying database, execute arbitrary commands at intent, and receive a dynamically generated output, such as HTML Web pages. In this paper, we present our technique, Sania, for detecting SQL injection vulnerabilities in Web applications during the development and debugging phases. Sania intercepts the SQL queries between a Web application and a database, and automatically generates elaborate attacks according to the syntax and semantics of the potentially vulnerable spots in the SQL queries. In addition, Sania compares the parse trees of the intended SQL query and those resulting after an attack to assess the safety of these spots. We evaluated our technique using real-world Web applications and found that our solution is efficient in comparison with a popular Web application vulnerabilities scanner. We also found vulnerability in a product that was just about to be released.","Automatic testing,
Computer security,
Application software,
Debugging,
Relational databases,
Authentication,
Information analysis,
Computer science,
Data security,
HTML"
On Parameter Tuning for FAST TCP,"This paper studies the stability of FAST TCP using a continuous time model of a single-link single-source network. A sufficient condition on asymptotical stability of FAST TCP congestion window is obtained, which relates all the relevant parameters in FAST TCP and decouples the key parameter a from others. A guideline on FAST TCP parameter setting is thus provided. The ns2 simulations validate the theoretical results","Guidelines,
Propagation delay,
Sufficient conditions,
Asymptotic stability,
Continuous time systems,
Protocols,
Performance gain,
Australia,
Computer science,
Network topology"
A Declarative Approach to Enhancing the Reliability of BPEL Processes,"Currently, BPEL is the de-facto standard for the Web service composition. Because Web services are autonomous and loosely coupled, BPEL processes are susceptible to a wide variety of faults. However, BPEL only provides limited constructs for handling faults, which makes fault handling a time-consuming and error-prone task. In this paper, we propose a declarative approach to enhancing the reliability of BPEL processes. Our solution specifies fault handling logic through a set of event- condition-action (ECA) rules which build on an extensible set of fault-tolerant patterns. These ECA rules are integrated with normal business logic before deployment to generate a fault-tolerant BPEL process. We also develop a GUI tool to assist designers to specify ECA rules. Experiments show our approach is feasible.","Web services,
Fault tolerance,
Logic design,
Computer science,
Guidelines,
Engines,
Graphical user interfaces,
Middleware,
Runtime,
Taxonomy"
Efficient Non-Planar Routing around Dead Ends in Sparse Topologies using Random Forwarding,"Geographic forwarding in wireless sensor networks (WSN) has long suffered from the problem of bypassing ""dead ends,"" i.e., those areas in the network where no node can be found in the direction of the data collection point (the sink). Solutions have been proposed to this problem, that rely on geometric techniques leading to the planarization of the network topology graph. In this paper, a novel method alternative to planarization is proposed, termed ALBA-R, that successfully routes packets to the sink transparently to dead ends. ALBA-R combines nodal duty cycles (awake/asleep schedules), channel access and geographic routing in a cross-layer fashion. Dead ends are dealt with by enhancing geographic routing with a mechanism that is distributed, localized and capable of routing packets around connectivity holes. An extensive set of simulations is provided, that demonstrates that ALBA-R is scalable, generates negligible overhead, and outperforms similar solutions with respect to all the metrics of interest investigated, especially in sparse topologies, notoriously the toughest benchmark for geographic routing protocols.",
Adaptive Distance Metric Learning for Clustering,"A good distance metric is crucial for unsupervised learning from high-dimensional data. To learn a metric without any constraint or class label information, most unsupervised metric learning algorithms appeal to projecting observed data onto a low-dimensional manifold, where geometric relationships such as local or global pairwise distances are preserved. However, the projection may not necessarily improve the separability of the data, which is the desirable outcome of clustering. In this paper, we propose a novel unsupervised adaptive metric learning algorithm, called AML, which performs clustering and distance metric learning simultaneously. AML projects the data onto a low-dimensional manifold, where the separability of the data is maximized. We show that the joint clustering and distance metric learning can be formulated as a trace maximization problem, which can be solved via an iterative procedure in the EM framework. Experimental results on a collection of benchmark data sets demonstrated the effectiveness of the proposed algorithm.",
Adaptive Trajectory Tracking Control of Skid-Steered Mobile Robots,"Skid-steered mobile robots have been widely used for terrain exploration and navigation. In this paper, we present an adaptive trajectory control design for a skid-steered wheeled mobile robot. Kinematic and dynamic modeling of the robot is first presented. A pseudo-static friction model is used to capture the interaction between the wheels and the ground. An adaptive control algorithm is designed to simultaneously estimate the wheel/ground contact friction information and control the mobile robot to follow a desired trajectory. A Lyapunov-based convergence analysis of the controller and the estimation of the friction model parameter are presented. Simulation and preliminary experimental results based on a four-wheel robot prototype are demonstrated for the effectiveness and efficiency of the proposed modeling and control scheme",
Localization in Wireless Sensor Networks,"A fundamental problem in wireless sensor networks is localization - the determination of the geographical locations of sensors. Most existing localization algorithms were designed to work well either in networks of static sensors or networks in which all sensors are mobile. In this paper, we propose two localization algorithms, MSL and MSL*, that work well when any number of sensors are static or mobile. MSL and MSL* are range-free algorithms - they do not require that sensors are equipped with hardware to measure signal strengths, angles of arrival of signals or distances to other sensors. We present simulation results to demonstrate that MSL and MSL* outperform existing algorithms in terms of localization error in very different mobility conditions. MSL* outperforms MSL in most scenarios, but incurs a higher communication cost. MSL outperforms MSL* when there is significant irregularity in the radio range. We also point out some problems with a well known lower bound for the error in any range-free localization algorithm in static sensor networks.","Wireless sensor networks,
Hardware,
Algorithm design and analysis,
Costs,
Wireless communication,
Goniometers,
Monte Carlo methods,
Batteries,
Monitoring,
Cellular phones"
Real-time Path Planning for Virtual Agents in Dynamic Environments,"We present a novel approach for real-time path planning of multiple virtual agents in complex dynamic scenes. We introduce a new data structure, Multi-agent Navigation Graph (MaNG), which is constructed from the first- and second-order Voronoi diagrams. The MaNG is used to perform route planning and proximity computations for each agent in real time. We compute the MaNG using graphics hardware and present culling techniques to accelerate the computation. We also address undersampling issues for accurate computation. Our algorithm is used for real-time multi-agent planning in pursuit-evasion and crowd simulation scenarios consisting of hundreds of moving agents, each with a distinct goal","Path planning,
Computational modeling,
Data structures,
Virtual environment,
Layout,
Navigation,
Collision avoidance,
Graphics,
Hardware,
Acceleration"
Achievable information rates for molecular communication with distinct molecules,"In molecular communication, messages are conveyed from a transmitter to a receiver by releasing a pattern of molecules at the transmitter, and allowing those molecules to propagate through a fluid medium towards a receiver. In this paper, achievable information rates are estimated for a molecular communication system when information is encoded using a set of distinct molecules, and when the molecules propagate across the medium via Brownian motion. Results are provided which indicate large gains in information rate over the case where the released molecules are indistinguishable from each other.","Receivers,
Transmitters,
Information rates,
Approximation methods,
Microorganisms,
Nanobioscience,
Probability density function"
Collage CAPTCHA,"Nowadays, many daily human activities such as education, commerce, talks, etc. are carried out through the Internet. In cases such as the registering in websites, some hackers write programs to make automatic false enrolments which waste the resources of the website while this may even stop the entire website from working. Therefore, it is necessary to tell apart human users from computer programs which is known as CAPTCHA (completely automated p[ublic turing test to tell computers and human apart). CAPTCHA methods are mainly based on the weak points of OCR (optical character recognition) systems while using them are undesirable to human users. In this paper a method has been presented for telling the human users and computer softwares apart on the basis of choice of an object shown on the screen. In this method some pictures are chosen randomly and after effecting some changes such as rotating, all of them are shown on the screen. Then we ask the user to choose a specific object. If the user chooses the right object we can guess that the user has been a human being not computer software. The main advantage of this method is its simplicity because the user does not have to type anything. This method has been implemented by the Java programming language.",
Real-time Body Tracking Using a Gaussian Process Latent Variable Model,"In this paper, we present a tracking framework for capturing articulated human motions in real-time, without the need for attaching markers onto the subject's body. This is achieved by first obtaining a low dimensional representation of the training motion data, using a nonlinear dimensionality reduction technique called back-constrained GPLVM. A prior dynamics model is then learnt from this low dimensional representation by partitioning the motion sequences into elementary movements using an unsupervised EM clustering algorithm. The temporal dependencies between these elementary movements are efficiently captured by a Variable Length Markov Model. The learnt dynamics model is used to bias the propagation of candidate pose feature vectors in the low dimensional space. By combining this with an efficient volumetric reconstruction algorithm, our framework can quickly evaluate each candidate pose against image evidence captured from multiple views. We present results that show our system can accurately track complex structured activities such as ballet dancing in real-time.",
BSIM-MG: A Versatile Multi-Gate FET Model for Mixed-Signal Design,"A novel surface-potential based multi-gate FET (MG-FET) compact model has been developed for mixed-signal design applications. For the first time, a MG-FET model captures the effect of finite body doping on the electrical behavior of MG-FETs. A unique field penetration length model has been developed to model the short channel effects in MG-FETs. A multitude of physical effects such as poly-depletion effect and quantum-mechanical effect (QME) have been incorporated. The expressions for terminal currents and charges are co-continuous making the model suitable for mixed-signal design. The model has been verified extensively with TCAD and experimental data.",
Analysis of the Reliability of a Nationwide Short Message Service,"SMS has been arguably the most popular wireless data service for cellular networks. Due to its ubiquitous availability and universal support by mobile handsets and cellular carriers, it is also being considered for emergency notification and other mission-critical applications. Despite its increased popularity, the reliability of SMS service in real-world operational networks has received little study so far. In this work, we investigate the reliability of SMS by analyzing traces collected from a nationwide cellular network over a period of three weeks. Although the SMS service incorporates a number of reliability mechanisms such as delivery acknowledgement and multiple retries, our study shows that its reliability is not as good as we expected. For example the message delivery failure ratio is as high as 5.1% during normal operation conditions. We also analyze the performance of the service under stressful conditions, and in particular during a ""flash-crowd"" event that occurred in New Year's Eve of 2005. Two important factors that adversely affect reliability of SMS are also examined: bulk message delivery that may induce network-wide congestion, and the topological structure of the social network formed by SMS users, which may facilitate quick propagation of viruses or other malware.",
Procrastination determination for periodic real-time tasks in leakage-aware dynamic voltage scaling systems.,"Many computing systems have adopted the dynamic voltage scaling (DVS) technique to reduce energy consumption by slowing down operation speed. However, The longer a job executes, the more energy in leakage current the processor consumes for the job. To reduce the power/energy consumption from the leakage current, a processor can enter the dormant mode. Existing research results for leakage-aware DVS scheduling perform procrastination of real-time jobs greedily so that the idle time can be aggregated as long as possible to turn off the processor. This paper proposes algorithms for the procrastination determination of periodic real-time tasks in uniprocessor systems. Instead of greedy procrastination, the procrastination procedures are applied only when the evaluated energy consumption is less than not procrastination. Evaluation results show that our proposed algorithms could derive energy-efficient solutions and outperform existing algorithms. Keywords: Energy-aware systems. .Scheduling, Leakage-aware scheduling. Dynamic voltage scaling. Job procrastination.",
RARE: An Energy-Efficient Target Tracking Protocol for Wireless Sensor Networks,"Energy efficiency for target tracking in wireless sensor networks is very important and can be improved by reducing the number of nodes involved in communications. We propose two algorithms, RARE-area and RARE-node to reduce the number of nodes participating in tracking and so increase energy efficiency. The RARE-area algorithm ensures that only nodes that receive a given quality of data participate in tracking and the RARE-node algorithm ensures that any nodes with redundant information do not participate in tracking. Simulation studies show significant energy savings are obtained with implementation of either the RARE-area algorithm alone or both RARE-area and RARE-node algorithms together.",
Graph Based Discriminative Learning for Robust and Efficient Object Tracking,"Object tracking is viewed as a two-class 'one-versus-rest' classification problem, in which the sample distribution of the target is approximately Gaussian while the background samples are often multimodal. Based on these special properties, we propose a graph embedding based discriminative learning method, in which the topology structures of graphs are carefully designed to reflect the properties of the sample distributions. This method can simultaneously learn the subspace of the target and its local discriminative structure against the background. Moreover, a heuristic negative sample selection scheme is adopted to make the classification more effective. In tracking procedure, the graph based learning is embedded into a Bayesian inference framework cascaded with hierarchical motion estimation, which significantly improves the accuracy and efficiency of the localization. Furthermore, an incremental updating technique for the graphs is developed to capture the changes in both appearance and illumination. Experimental results demonstrate that, compared with two state-of-the-art methods, the proposed tracking algorithm is more efficient and effective, especially in dynamically changing and clutter scenes.","Robustness,
Target tracking,
Lighting,
Inference algorithms,
Learning systems,
Bayesian methods,
Motion estimation,
Brightness,
Covariance matrix,
Laboratories"
C2AP: Coverage-aware and Connectivity-constrained Actor Positioning in Wireless Sensor and Actor Networks,"In addition to the miniaturized sensor nodes, wireless sensor and actor networks (WSANs) employ significantly more capable actor nodes that can perform application specific actions to deal with events detected and reported by the sensors. Since these actions can be taken at any spot within the monitored area, the actors should be carefully placed in order to provide maximal coverage. Moreover, the actors often coordinate among themselves in order to arbitrate tasks and thus inter-actor connectivity is usually a requirement. In this paper, we propose a distributed actor positioning algorithm that maximizes the coverage of actors without violating the connectivity requirement. The approach applies repelling forces between neighboring actors, similar to molecular particles in Physics, in order to spread them in the region. However, the movement of each actor is restricted in order to maintain the connectivity of the inter-actor network. The performance of the approach is validated through simulations.",
Learning to predict gender from iris images,"This paper employs machine learning techniques to develop models that predict gender based on the iris texture features. While there is a large body of research that explores biometrics as a means of verifying identity, there has been very little work done to determine if biometric measures can be used to determine specific human attributes. If it is possible to discover such attributes, they would be useful in situations where a biometric system fails to identify an individual that has not been enrolled, yet still needs to be identified. The iris was selected as the biometric to analyze for two major reasons: (1) quality methods have already been developed to segment and encode an iris image, (2) current iris encoding methods are conducive to selecting and extracting attributes from an iris texture and creating a meaningful feature vector.","Iris,
Biometrics,
Machine learning,
Biological system modeling,
Predictive models,
Humans,
Image analysis,
Image texture analysis,
Image segmentation,
Image coding"
A genetic algorithms approach to modeling the performance of memory-bound computations,"Benchmarks that measure memory bandwidth, such as STREAM, Apex-MAPS and MultiMAPS, are increasingly popular due to the ""Von Neumann"" bottleneck of modern processors which causes many calculations to be memory-bound. We present a scheme for predicting the performance of HPC applications based on the results of such benchmarks. A Genetic Algorithm approach is used to ""learn"" bandwidth as a function of cache hit rates per machine with MultiMAPS as the fitness test. The specific results are 56 individual performance predictions including 3 full-scale parallel applications run on 5 different modern HPC architectures, with various CPU counts and inputs, predicted within 10% average difference with respect to independently verified runtimes.",
Computing the Stopping Distance of a Tanner Graph Is NP-Hard,Two decision problems related to the computation f stopping sets in Tanner graphs are shown to be NP-complete. It follows as a consequence that there exists no polynomial time algorithm for computing the stopping distance of a Tanner graph unless P = NP.,"Parity check codes,
Iterative decoding,
Iterative algorithms,
Performance analysis,
NP-complete problem,
Polynomials,
Research and development,
Computer science,
Automation,
Linear code"
Fracturing Rigid Materials,"We propose a novel approach to fracturing (and denting) brittle materials. To avoid the computational burden imposed by the stringent time step restrictions of explicit methods or with solving nonlinear systems of equations for implicit methods, we treat the material as a fully rigid body in the limit of infinite stiffness. In addition to a triangulated surface mesh and level set volume for collisions, each rigid body is outfitted with a tetrahedral mesh upon which finite element analysis can be carried out to provide a stress map for fracture criteria. We demonstrate that the commonly used stress criteria can lead to arbitrary fracture (especially for stiff materials) and instead propose the notion of a time averaged stress directly into the FEM analysis. When objects fracture, the virtual node algorithm provides new triangle and tetrahedral meshes in a straightforward and robust fashion. Although each new rigid body can be rasterized to obtain a new level set, small shards can be difficult to accurately resolve. Therefore, we propose a novel collision handling technique for treating both rigid bodies and rigid body thin shells represented by only a triangle mesh",
Extending stability beyond CPU millennium: a micron-scale atomistic simulation of Kelvin-Helmholtz instability,"We report the computational advances that have enabled the first micron-scale simulation of a Kelvin-Helmholtz (KH) instability using molecular dynamics (MD). The advances are in three key areas for massively parallel computation such as on BlueGene/L (BG/L): fault tolerance, application kernel optimization, and highly efficient parallel I/O. In particular, we have developed novel capabilities for handling hardware parity errors and improving the speed of interatomic force calculations, while achieving near optimal I/O speeds on BG/L, allowing us to achieve excellent scalability and improve overall application performance. As a result we have successfully conducted a 2-billion atom KH simulation amounting to 2.8 CPU-millennia of run time, including a single, continuous simulation run in excess of 1.5 CPU-millennia. We have also conducted 9-billion and 62.5-billion atom KH simulations. The current optimized ddcMD code is benchmarked at 115.1 TFlop/s in our scaling study and 103.9 TFlop/s in a sustained science run, with additional improvements ongoing. These improvements enabled us to run the first MD simulations of micron-scale systems developing the KH instability.","Stability,
Processor scheduling,
Scheduling algorithm,
Information science,
Licenses,
Hardware,
Software algorithms,
Permission,
Computer science,
Concurrent computing"
Personality and Emotion-Based High-Level Control of Affective Story Characters,"Human emotional behavior, personality, and body language are the essential elements in the recognition of a believable synthetic story character. This paper presents an approach using story scripts and action descriptions in a form similar to the content description of storyboards to predict specific personality and emotional states. By adopting the Abridged Big Five Circumplex (AB5C) Model of personality from the study of psychology as a basis for a computational model, we construct a hierarchical fuzzy rule-based system to facilitate the personality and emotion control of the body language of a dynamic story character. The story character can consistently perform specific postures and gestures based on his/her personality type. Story designers can devise a story context in the form of our story interface which predictably motivates personality and emotion values to drive the appropriate movements of the story characters. Our system takes advantage of relevant knowledge described by psychologists and researchers of storytelling, nonverbal communication, and human movement. Our ultimate goal is to facilitate the high-level control of a synthetic character",
Composite Event Detection in Wireless Sensor Networks,"Sensor networks can be used for event alarming applications. To date, in most of the proposed schemes, the raw or aggregated sensed data is periodically sent to a data consuming center. However, with this scheme, the occurrence of an emergency event such as a fire is hardly reported in a timely manner which is a strict requirement for event alarming applications. In sensor networks, it is also highly desired to conserve energy so that the network lifetime can be maximized. Furthermore, to ensure the quality of surveillance, some applications require that if an event occurs, it needs to be detected by at least k sensors where k is a user-defined parameter. In this work, we examine the timely energy-efficient k-watching event detection problem (TEKWEO). A topology-and-routing-supported algorithm is proposed which constructs a set of detection sets that satisfy the short notification time, energy conservation, and tunable quality of surveillance requirements for event alarming applications. Simulation results are shown to validate the proposed algorithm.",
A Proactive Tree Recovery Mechanism for Resilient Overlay Multicast,"Overlay multicast constructs a multicast delivery tree among end hosts. Unlike traditional IP multicast, the non-leaf nodes in the tree are normal end hosts, which are potentially more susceptible to failures than routers and may leave the multicast group voluntarily. In these cases, all downstream nodes are affected. Thus, an important problem for making overlay multicast more dependable is how to recover from node departures in order to minimize the disruption of service to those affected nodes. In this paper, we propose a proactive tree recovery mechanism to make the overlay multicast resilient to these failures and unexpected events. Rather than letting downstream nodes try to find a new parent after a node departure, each non-leaf node precalculates a parent-to-be for each of its children. When this non-leaf node is gone, all its children can find their respective new parents immediately. The salient feature of the approach is that rescue plans for multiple non-leaf nodes can work together for their respective children when they fail or leave at the same time. Extensive simulations demonstrate that our proactive approach can recover from node departures much faster than reactive methods, while the quality of trees restored and the cost of recovery are reasonable",
A study on the design issues of Memetic Algorithm,"Over the recent years, there has been increasing research activities made on improving the efficacy of memetic algorithm (MA) for solving complex optimization problems. Particularly, these efforts have revealed the success of MA on a wide range of real world problems. MAs not only converge to high quality solutions, but also search more efficiently than their conventional counterparts. Despite the success and surge in interests on MAs, there is still plenty of scope for furthering our understanding on how and why synergy between population- based and individual learning searchers would lead to successful Memetic Algorithms. In this paper we outline several important design issues of Memetic Algorithms and present a systematic study on each. In particular, we conduct extensive experimental studies on the impact of each individual design issue and their relative impacts on memetic search performances by means of three commonly used synthetic problems. From the empirical studies obtained, we attempt to reveal the behaviors of several MA variants to enhance our understandings on MAs.",
Predicting Labor Cost through IT Management Complexity Metrics,"We propose a model for relating IT management complexity metrics to key business-level performance metrics like time and labor cost. In particular, we address the problem of quantifying and predicting the value that automation and IT service management process transformation will yield before their actual deployment. Our approach looks at this problem from a different, new perspective by regarding complexity as a surrogate for potential labor cost and human-error-induced problems: It consists in (1) assessing and evaluating the complexity of IT management processes and procedures, (2) separately measuring business-level performance metrics (3) relating the collected complexity metrics to business-level performance metrics by means of a quantitative model, and (4) validating the model through a field study. Algorithms are presented for selecting a subset of the complexity metrics to use as explanatory variables in a quantitative model and for constructing the quantitative model itself. Besides improving decision making for deploying automation technologies and transforming IT service management processes, our quantitative IT management complexity model can help service providers and outsourcers predict the amount of human effort and skills that will be needed to provide a given service, thus allowing them to more effectively evaluate costs and benefits of automation technologies and IT management process transformations.","Costs,
Automation,
Technology management,
Measurement,
Quality management,
Predictive models,
Productivity,
Humans,
Financial management,
Computer science"
Towards a Real-Time Bayesian Imitation System for a Humanoid Robot,"Imitation learning, or programming by demonstration (PbD), holds the promise of allowing robots to acquire skills from humans with domain-specific knowledge, who nonetheless are inexperienced at programming robots. We have prototyped a real-time, closed-loop system for teaching a humanoid robot to interact with objects in its environment. The system uses nonparametric Bayesian inference to determine an optimal action given a configuration of objects in the world and a desired future configuration. We describe our prototype implementation, show imitation of simple motor acts on a humanoid robot, and discuss extensions to the system",
On Context-Specific Substitutability of Web Services,"Web service substitution refers to the problem of identifying a service that can replace another service in the context of a composition with a specified functionality. Existing solutions to this problem rely on detecting the functional and behavioral equivalence of a particular service to be replaced and candidate services that could replace it. We introduce the notion of context-specific substitutability, where context refers to the overall functionality of the composition that is required to be maintained after replacement of its constituents. Using the context information, we investigate two variants of the substitution problem, namely environment-independent and environment- dependent, where environment refers to the constituents of a composition and show how the substitutability criteria can be relaxed within this model. We provide a logical formulation of the resulting criteria based on model checking techniques as well as prove the soundness and completeness of the proposed approach.",
Automatic Relocalisation for a Single-Camera Simultaneous Localisation and Mapping System,"We describe a fast method to relocalise a monocular visual SLAM (simultaneous localisation and mapping) system after tracking failure. The monocular SLAM system stores the 3D locations of visual landmarks, together with a local image patch. When the system becomes lost, candidate matches are obtained using correlation, then the pose of the camera is solved via an efficient implementation of RANSAC using a three-point-pose algorithm. We demonstrate the usefulness of this method within visual SLAM: (i) we show tracking can reliably resume after tracking failure due to occlusions, motion blur or unmodelled rapid motions; (ii) we show how the method can be used as an adjunct for a proposal distribution in a particle filter framework; (iii) during successful tracking we use idle cycles to test if the current map overlaps with a previously-built map, and we provide a solution to aligning the two maps by splicing the camera trajectories in a consistent and optimal way.",
Document Transformation for Multi-label Feature Selection in Text Categorization,"Feature selection on multi-label documents for automatic text categorization is an under-explored research area. This paper presents a systematic document transformation framework, whereby the multi-label documents are transformed into single-label documents before applying standard feature selection algorithms, to solve the multi-label feature selection problem. Under this framework, we undertake a comparative study on four intuitive document transformation approaches and propose a novel approach called entropy-based label assignment (ELA), which assigns the labels weights to a multi-label document based on label entropy. Three standard feature selection algorithms are utilized for evaluating the document transformation approaches in order to verify its impact on multi-class text categorization problems. Using a SVM classifier and two multi-label evaluation benchmark text collections, we show that the choice of document transformation approaches can significantly influence the performance of multi-class categorization and that our proposed document transformation approach ELA can achieve better performance than all other approaches.","Text categorization,
Entropy,
Data mining,
Asia,
Computer science,
Support vector machines,
Support vector machine classification,
Algorithm design and analysis,
Explosives,
Web sites"
Detection of HTTP-GET flood Attack Based on Analysis of Page Access Behavior,"Recently, there are many denial-of-service (DoS) attacks by computer viruses or botnet. DoS attacks to Web services are called HTTP-GET flood attack and threats of them increase day by day. In this type of attacks, malicious clients send a large number of HTTP-GET requests to the target Web server automatically. Since these HTTP-GET requests have legitimate formats and are sent via normal TCP connections, an intrusion detection system (IDS) can not detect them. In this paper, we propose HTTP-GET flood detection techniques based on analysis of page access behavior. We propose two detection algorithms, one is focusing on a browsing order of pages and the other is focusing on a correlation with browsing time to page information size. We implement detection techniques and evaluate attack detection rates, i.e., false positive and false negative. The results show that our techniques can detect the HTTP-GET flood attack effectively.",
Hierarchical Ensemble of Global and Local Classifiers for Face Recognition,"In the literature of psychophysics and neurophysiology, many studies have shown that both global and local features are crucial for face representation and recognition. This paper proposes a novel face recognition method which combines both global and local discriminative features. In this method, global features are extracted from whole face images by Fourier transform and local features are extracted from some spatially partitioned image patches by Gabor wavelet transform. After this, multiple classifiers are obtained by applying Fisher Discriminant Analysis on global Fourier features and local patches of Gabor features. All these classifiers are combined to form a hierarchical ensemble by sum rule. We evaluated the proposed method using Face Recognition Grand Challenge (FRGC) experimental protocols and database known as the largest data sets available. Experimental results on FRGC version 2.0 data set have shown that the proposed method achieves a verification rate of 86%, while the best reported was 76%.","Face recognition,
Feature extraction,
Principal component analysis,
Psychology,
Neurophysiology,
Fourier transforms,
Lighting,
Face detection,
Linear discriminant analysis,
Discrete Fourier transforms"
Transform Coding for Hardware-accelerated Volume Rendering,"Hardware-accelerated volume rendering using the GPU is now the standard approach for real-time volume rendering, although limited graphics memory can present a problem when rendering large volume data sets. Volumetric compression in which the decompression is coupled to rendering has been shown to be an effective solution to this problem; however, most existing techniques were developed in the context of software volume rendering, and all but the simplest approaches are prohibitive in a real-time hardware-accelerated volume rendering context. In this paper we present a novel block-based transform coding scheme designed specifically with real-time volume rendering in mind, such that the decompression is fast without sacrificing compression quality. This is made possible by consolidating the inverse transform with dequantization in such a way as to allow most of the reprojection to be precomputed. Furthermore, we take advantage of the freedom afforded by offline compression in order to optimize the encoding as much as possible while hiding this complexity from the decoder. In this context we develop a new block classification scheme which allows us to preserve perceptually important features in the compression. The result of this work is an asymmetric transform coding scheme that allows very large volumes to be compressed and then decompressed in real-time while rendering on the GPU.","Transform coding,
Rendering (computer graphics),
Graphics,
Hardware,
Pipelines,
Data visualization,
Encoding,
Decoding,
Resource management,
Production"
Achievable Rate Region of a Two Phase Bidirectional Relay Channel,"In this work, the capacity region of the broadcast channel in a two phase bidirectional relay communication scenario is proved. Thereby, each receiving node has perfect knowledge about the message intended for the other node. The capacity region can be achieved using an auxiliary random variable taking two values, i.e., by the principle of time-sharing. The resulting achievable rate region of the two-phase bidirectional relaying includes the region which can be achieved by network coding applying XOR on the decoded messages at the relay node.",
Performance of a Simple Tuned Fuzzy Controller and a PID Controller on a DC Motor,"We are presenting the usefulness of an innovative method called simple tuning algorithm (STA) for tuning fuzzy controllers, it has only one variable to adjust to achieve the tuning goal, this in counterpart to other methods like the proportional integral derivative (PID) controller wish has three variables to adjust for the same goal. Comparative examples of the STA and the PID methods are presented in a speed control of a real DC gear motor application. The PID controller was tuned using the Ziegler-Nichols. In base of the obtained quantitative and qualitative measures and observations, we are concluding that the fuzzy controller performance outperformed the PID controller; moreover, the tuning process using the STA method was easier than using the Ziegler-Nichols method",
Joint Affinity Propagation for Multiple View Segmentation,"A joint segmentation is a simultaneous segmentation of registered 2D images and 3D points reconstructed from the multiple view images. It is fundamental in structuring the data for subsequent modeling applications. In this paper, we treat this joint segmentation as a weighted graph labeling problem. First, we construct a 3D graph for the joint 3D and 2D points using a joint similarity measure. Then, we propose a hierarchical sparse affinity propagation algorithm to automatically and jointly segment 2D images and group 3D points. Third, a semi-supervised affinity propagation algorithm is proposed to refine the automatic results with the user assistance. Finally, intensive experiments demonstrate the effectiveness of the proposed approaches.",
NoC Topologies Exploration based on Mapping and Simulation Models,"NoC architectures are considered the next generation of communication infrastructure for future systems-on- chip. Selection of the network architecture and mapping of IP nodes onto the NoC topology are two important research topics. In this paper we compare well known NoC interconnect systems, specifically, Ring, 2d-Mesh, Spidergon and unbuffered Crossbar using theoretical uniform traffic based on the request/reply paradigm as well as a realistic traffic based on a Mpeg4 application. The IP mapping is computed by the SCOTCH partitioning tool opportunely modified to maximize selected embedding quality criteria under multiple topological constraints.","Network-on-a-chip,
Topology,
Routing,
Computer architecture,
System recovery,
Telecommunication traffic,
Traffic control,
Clocks,
Computational modeling,
Computer science"
A Comparison of PMD-Cameras and Stereo-Vision for the Task of Surface Reconstruction using Patchlets,"Recently real-time active 3D range cameras based on time-of-flight technology (PMD) have become available. Those cameras can be considered as a competing technique for stereo-vision based surface reconstruction. Since those systems directly yield accurate 3d measurements, they can be used for benchmarking vision based approaches, especially in highly dynamic environments. Therefore, a comparative study of the two approaches is relevant. In this work the achievable accuracy of the two techniques, PMD and stereo, is compared on the basis of patch-let estimation. As patchlet we define an oriented small planar 3d patch with associated surface normal. Least-squares estimation schemes for estimating patchlets from PMD range images as well as from a pair of stereo images are derived. It is shown, how the achivable accuracy can be estimated for both systems. Experiments under optimal conditions for both systems are performed and the achievable accuracies are compared. It has been found that the PMD system outperformed the stereo system in terms of achievable accuracy for distance measurements, while the estimation of normal direction is comparable for both systems.",
Nadi Tarangini: A Pulse Based Diagnostic System,"Ayurveda is a traditional medicine and natural healing system in India. Nadi-Nidan (pulse-based diagnosis) is a prominent method in Ayurveda, and is known to dictate all the salient features of a human body. In this paper, we provide details of our procedure for obtaining the complete spectrum of the nadi pulses as a time series. The system Nadi Tarangini contains a diaphragm element equipped with strain gauge, a transmitter cum amplifier, and a digitizer for quantifying analog signal. The system acquires the data with 16-bit accuracy with practically no external electronic or interfering noise. Prior systems for obtaining the nadi pulses have been few and far between, when compared to systems such as ECG. The waveforms obtained with our system have been compared with these other similar equipment developed earlier, and is shown to contain more details. The pulse waveform is also shown to have the desirable variations with respect to age of patients, and the pressure applied at the sensing element. The system is being evaluated by Ayurvedic practitioners as a computer-aided diagnostic tool.",
Opportunistic Networks for Emergency Applications and Their Standard Implementation Framework,"We present a novel paradigm of opportunistic networks or oppnets in the context of emergency preparedness and response (EPR). Oppnets constitute the category of ad hoc networks where diverse systems, not employed originally as nodes of an oppnet, join it dynamically in order to perform certain tasks they have been called to participate in. After describing the oppnets and their operation, we discuss the oppnet virtual machine (OVM) - a standard implementation framework for oppnet applications. Oppnets can significantly improve effectiveness and efficiency of EPR one of the six mission areas within the national strategy for homeland security. They can also improve other diverse applications, including agriculture, environment, healthcare, manufacturing, surveillance, and transportation. Oppnets should create new application niches as yet hard to imagine. To the best of our knowledge we have been the first to work on oppnets.",
Aura 3D Textures,"This paper presents a new technique, called aura 3D textures, for generating solid textures based on input examples. Our method is fully automatic and requires no user interactions in the process. Given an input texture sample, our method first creates its aura matrix representations and then generates a solid texture by sampling the aura matrices of the input sample constrained in multiple view directions. Once the solid texture is generated, any given object can be textured by the solid texture. We evaluate the results of our method based on extensive user studies. Based on the evaluation results using human subjects, we conclude that our algorithm can generate faithful results of both stochastic and structural textures with an average successful rate of 76.4 percent. Our experimental results also show that the new method outperforms Wei and Levoy's method and is comparable to that proposed by Jagnow et al. (2004)",
Exact Fault-Sensitive Feasibility Analysis of Real-Time Tasks,"In this paper, we consider the problem of checking the feasibility of a set of n real-time tasks while provisioning for timely recovery from (at most) k transient faults. We extend the well-known processor demand approach to take into account the extra overhead that may be induced by potential recovery operations under earliest-deadline-first scheduling. We develop a necessary and sufficient test using a dynamic programming technique. An improvement upon the previous solutions is to address and efficiently solve the case where the recovery blocks associated with a given task do not necessarily have the same execution time. We also provide an online version of the algorithm that does not require a priori knowledge of release times. The online algorithm runs in O(m ldr k2) time, where m is the number of ready tasks. We extend the framework to periodic execution settings: We derive a sufficient condition that can be checked efficiently for the feasibility of periodic tasks in the presence of faults. Finally, we analyze the case where the recovery blocks are to be executed nonpreemptively and we formally show that the problem becomes intractable under that assumption.","Real time systems,
Transient analysis,
Circuit faults,
Timing,
Processor scheduling,
Redundancy,
Fault tolerance"
Reputation-based Trust in Wireless Sensor Networks,"Wireless sensor networks (WSNs) are becoming more and more common. WSNs have many existing and envisioned applications due to their ease of deployment, particularly in remote areas. But the security of WSNs is still an issue. Some existing approaches mainly rely on cryptography to ensure data authentication and integrity. These approaches only address part of the problem of security in WSNs. However, these approaches are not sufficient for the unique characteristics and novel misbehaviors encountered in WSNs. Recently, the use of reputation systems has become an important mechanism in WSNs. In this paper we propose a reputation-based trust which borrows tools from probability, statistics and mathematics analysis. We have suggested a new term certainty used in trust system and we argued that the positive or negative outcomes for a certain event is not enough information to make a decision in WSNs. We build up a reputation space and trust space in WSNs, and define a transformation from reputation space to trust space. Finally, we discuss some important properties of them and point out some open problems in reputation system in WSNs.","Wireless sensor networks,
Cryptography,
Sensor phenomena and characterization,
Mathematics,
Authentication,
Hardware,
Data security,
Statistical analysis,
Educational institutions,
Computer science"
Software Design and Architecture The once and future focus of software engineering,"The design of software has been a focus of software engineering research since the field's beginning. This paper explores key aspects of this research focus and shows why design will remain a principal focus. The intrinsic elements of software design, both process and product, are discussed: concept formation, use of experience, and means for representation, reasoning, and directing the design activity. Design is presented as being an activity engaged by a wide range of stakeholders, acting throughout most of a system's lifecycle, making a set of key choices which constitute the application's architecture. Directions for design research are outlined, including: (a) drawing lessons, inspiration, and techniques from design fields outside of computer science, (b) emphasizing the design of application ""character"" (functionality and style) as well as the application's structure, and (c) expanding the notion of software to encompass the design of additional kinds of intangible complex artifacts.","Software design,
Computer architecture,
Software engineering,
Computer science,
Application software,
Informatics,
Programming,
Floors,
Software architecture,
Service awards"
An Effective Illustrative Visualization Framework Based on Photic Extremum Lines (PELs),"Conveying shape using feature lines is an important visualization tool in visual computing. The existing feature lines (e.g., ridges, valleys, silhouettes, suggestive contours, etc.) are solely determined by local geometry properties (e.g., normals and curvatures) as well as the view position. This paper is strongly inspired by the observation in human vision and perception that a sudden change in the luminance plays a critical role to faithfully represent and recover the 3D information. In particular, we adopt the edge detection techniques in image processing for 3D shape visualization and present photic extremum lines (PELs) which emphasize significant variations of illumination over 3D surfaces. Comparing with the existing feature lines, PELs are more flexible and offer users more freedom to achieve desirable visualization effects. In addition, the user can easily control the shape visualization by changing the light position, the number of light sources, and choosing various light models. We compare PELs with the existing approaches and demonstrate that PEL is a flexible and effective tool to illustrate 3D surface and volume for visual computing.",
Workflow Composition of Service Level Agreements,"Service-oriented architectures enable an environment where businesses can expose services for use by their collaborators and their peer organizations. In such an environment, organizations may have long-standing cooperative agreements representing the existing services that they share. Furthermore, they may have service level agreements (SLAs) that assure the quality of service standards of the existing services. In an ad-hoc workflow scenario, a business may need to perform real-time composition of the existing services in response to consumer requests. In this work, we suggest that, in parallel to traditional Web service composition, the business must also compose the corresponding SLAs of the chosen services to generate a guaranteed service level to the consumer. In this paper, we introduce a process for composing SLAs associated with a workflow of Web services. This process results in the discovery of the best composite (i.e. workflow) capability while optimizing the underlying service-level attributes.","Web services,
Quality of service,
Business communication,
Computer science,
Service oriented architecture,
Collaborative work,
Markup languages,
XML,
Data models,
Dynamic programming"
Analysis and diagnostics of the brain-stem ultrasound images,"The medical sonography is an ultrasound-based technique used to visualize internal organs. It is based on detecting the reflections of the waves emitted by a probe. The waves may have various frequencies. Generally, the higher the frequency is, the better and more detailed output images we can get. However, if we want to obtain the brain-stem ultrasound images suitable for the Parkinson disease diagnostics, we can only use the low-frequency probes due to the presence of skull. For this reason, the output images will necessarily be of low quality. In spite of that, these images are essential for the Parkinson disease diagnostics and treatment because, except outer symptoms, they provide the only way how to determine the existence and seriousness of the disease. The low image quality may even lead to the different or inaccurate diagnosis of same image from different medical doctors. Our objective is to create a tool that should help to minimize the physician's subjectivity in the final diagnosis and should provide more exact information about the processed images.","Image analysis,
Ultrasonic imaging,
Parkinson's disease,
Biomedical imaging,
Medical diagnostic imaging,
Probes,
Frequency,
Ultrasonography,
Visualization,
Reflection"
A Queueing-Theoretic Foundation of Available Bandwidth Estimation: Single-Hop Analysis,"Most existing available-bandwidth measurement techniques are justified using a constant-rate fluid cross-traffic model. To achieve a better understanding of the performance of current bandwidth measurement techniques in general traffic conditions, this paper presents a queueing-theoretic foundation of single-hop packet-train bandwidth estimation under bursty arrivals of discrete cross-traffic packets. We analyze the statistical mean of the packet-train output dispersion and its mathematical relationship to the input dispersion, which we call the probing-response curve. This analysis allows us to prove that the single-hop response curve in bursty cross-traffic deviates from that obtained under fluid cross traffic of the same average intensity and to demonstrate that this may lead to significant measurement bias in certain estimation techniques based on fluid models. We conclude the paper by showing, both analytically and experimentally, that the response-curve deviation vanishes as the packet-train length or probing packet size increases, where the vanishing rate is decided by the burstiness of cross-traffic.",
Motivation and Learning Progress Through Educational Games,"In engineering, we are often faced with the problem of how to teach complex theoretical material to students who are mainly interested in solving practical problems. This gap between the application-oriented expectation of the learner and the theory-focused material chosen by the lecturer may end up causing high barriers for the learning performance of the students. This becomes obvious if we recall that motivation is a cornerstone for good learning. One way to close this gap between theory and practice is educational games. In this paper, we show how educational games can exemplarily help to motivate and teach undergraduate university students in a basic automatic control course. The success of this approach can, for example, be seen from the course evaluations and feedback from our students.","Game theory,
Automatic control,
Feedback,
Computer science education,
Control engineering education,
Electronic learning,
Information technology,
Remote laboratories,
Physics education,
Mathematics"
Character Recognition using Spiking Neural Networks,"A spiking neural network model is used to identify characters in a character set. The network is a two layered structure consisting of integrate-and-fire and active dendrite neurons. There are both excitatory and inhibitory connections in the network. Spike time dependent plasticity (STDP) is used for training. The winner take all mechanism is enforced by the lateral inhibitory connections. It is found that most of the characters are recognized in a character set consisting of 48 characters. The network is trained successfully with increased resolution of the characters. Also, addition of uniform random noise does not decrease its recognition capability.","Character recognition,
Neural networks,
Neurons,
Biological information theory,
Delay,
Mobile robots,
Artificial neural networks,
Temporal lobe,
Biology computing,
Navigation"
Detecting Patch Submission and Acceptance in OSS Projects,"The success of open source software (OSS) is completely dependent on the work of volunteers who contribute their time and talents. The submission of patches is the major way that participants outside of the core group of developers make contributions. We argue that the process of patch submission and acceptance into the codebase is an important piece of the open source puzzle and that the use of patch-related data can be helpful in understanding how OSS projects work. We present our methods in identifying the submission and acceptance of patches and give results and evaluation in applying these methods to the Apache webserver, Python interpreter, Postgres SQL database, and (with limitations) MySQL database projects. In addition, we present valuable ways in which this data has been and can be used.","Open source software,
Documentation,
Birds,
Computer science,
Face detection,
Writing,
Computer bugs,
Information analysis,
Database systems"
Tamper Detection Based on Regularity of Wavelet Transform Coefficients,"Powerful digital media editing tools make producing good quality forgeries very easy for almost anyone. Therefore, proving the authenticity and integrity of digital media becomes increasingly important. In this work, we propose a simple method to detect image tampering operations that involve sharpness/blurriness adjustment. Our approach is based on the assumption that if a digital image undergoes a copy-paste type of forgery, average sharpness/blurriness value of the forged region is expected to be different as compared to the non-tampered parts of the image. The method of estimating sharpness/blurriness value of an image is based on the regularity properties of wavelet transform coefficients which involves measuring the decay of wavelet transform coefficients across scales. Our preliminary results show that the estimated sharpness/blurriness scores can be used to identify tampered areas of the image.","Wavelet transforms,
Digital images,
Watermarking,
Robustness,
Forgery,
Authentication,
Image processing,
Law,
Statistics,
Cameras"
A Scientometric Study of the Perceived Quality of Business and Technical Communication Journals,"In this paper we present, from an academic perspective, the perceived quality ratings of business and technical communication journals. Through a survey of academic experts, we asked respondents to rate the top overall journals, business communication journals, technical communication journals, and the top journals from a technology perspective. In addition, we asked respondents to list the journals that they read most frequently. We analyzed the results by breaking down the rankings into world regions and academic departments. The top-three overall journals for all regions are Journal of Business and Technical Communication, Journal of Business Communication, and IEEE Transactions on Professional Communication. Importantly, differences by world region and academic department type were found in all these rankings. These results can support researchers worldwide by helping them target their publishing efforts to journals that have the best fit with their business and technical communication discipline, world region, and academic home.","Business communication,
Professional communication,
Publishing,
Incentive schemes,
Communications technology,
Rhetoric,
Books,
Computer science,
Quality control"
Market-based grid resource allocation using a stable continuous double auction,"A market-based grid resource allocation mechanism is presented and evaluated. It takes into account the architectural features and special requirements of computational grids while ensuring economic efficiency, even when the underlying resources are being used by self-interested and uncooperative participants. A novel stable continuous double auction (SCDA), based on the more conventional continuous double auction (CDA), is proposed for Grid resource allocation. It alleviates the unnecessarily volatile behaviour of the CDA, while maintaining other beneficial features. Experimental results show that the SCDA is superior to the CDA in terms of both economic efficiency and scheduling efficiency. The SCDA delivers continuous matching, high efficiency and low cost, allied with low price volatility and low bidding complexity. Its ability to deliver immediate allocation and its stable prices facilitate co-allocation of resources and it also enables incremental evolution towards a full grid resource market. Effective market-based Grid resource allocation is thus shown to be feasible.","Resource management,
Environmental economics,
Economic forecasting,
Costs,
Centralized control,
Computational efficiency,
Computer science,
Grid computing,
Delay effects,
Robustness"
Modeling and Documenting the Evolution of Architectural Design Decisions,"All software systems are built as a result of a set of design decisions that are made during the architecting phase. At present, there is still a lack of appropriate notations, methods and tools for recording and exploiting these architectural design decisions. In addition, the need for maintaining and evolving the decisions made in the past turns critical for the success of the evolution of the system. In this research paper we extend a previous work to detail those issues related to the evolution of architectural design decisions.","Decision support systems,
Computer science,
Design engineering,
Systems engineering and theory,
Telematics,
Software systems,
Software engineering,
Decision making,
Computer architecture,
Costs"
A Variability Modeling Method for Adaptable Services in Service-Oriented Computing,"Publish-discover-compose paradigm of service-oriented computing (SOC) presents a challenge on service applicability. Services are not just for predefined clients, rather for potentially many unknown clients. Hence, published services should be highly adaptable to various service clients and contexts. For that, service variability must carefully be modeled by considering the unique computing paradigm and requirements of SOC such as dynamic discovery and composition of services. Current SOC approaches to modeling services largely focus on defining business processes and service components without considering service variability in sufficient details. In this paper, we first compare the variability on conventional applications and the variability on SOC. Then, we identify four types of variability on services. For the types of service variability, we present a method to model service variability and design adaptable services. Using our proposed framework, we believe the applicability and reusability of such services can be greatly increased.",
Improving Quality of Service and Assuring Fairness in WLAN Access Networks,"As public deployment of wireless local area networks (WLANs) has increased and various applications with different service requirements have emerged, fairness and quality of service (QoS) are two imperative issues in allocating wireless channels. This study proposes a fair QoS agent (FQA) to simultaneously provide per-class QoS enhancement and per-station fair channel sharing in WLAN access networks. FQA implements two additional components above the 802.11 MAC: a dual service differentiator and a service level manager. The former is intended to improve QoS for different service classes by differentiating service with appropriate scheduling and queue management algorithms, while the latter is to assure fair channel sharing by estimating the fair share for each station and dynamically adjusting the service levels of packets. FQA assures (weighted) fairness among stations in terms of channel access time without decreasing channel utilization. Furthermore, it can provide quantitative service assurance in terms of queuing delay and packet loss rate. FQA neither resorts to any complex fair scheduling algorithm nor requires maintaining per-station queues. Since the FQA algorithm is an add-on scheme above the 802.11 MAC, it does not require any modification of the standard MAC protocol. Extensive ns-2 simulations confirm the effectiveness of the FQA algorithm with respect to the per class QoS enhancement and per-station fair channel sharing","Quality of service,
Wireless LAN,
Scheduling algorithm,
Media Access Protocol,
Delay,
Streaming media,
Peer to peer computing,
Dynamic scheduling,
Internet"
Dynamic Dependency Monitoring to Secure Information Flow,"Although static systems for information flow security are well-studied, few works address run-time information flow monitoring. Run-time information flow control offers distinct advantages in precision and in the ability to support dynamically defined policies. To this end, we here develop a new run-time information flow system based on the runtime tracking of indirect dependencies between program points. Our system tracks both direct and indirect information flows, and noninterference results are proved.",
An FEC-based Reliable Data Transport Protocol for Underwater Sensor Networks,"In this paper, we investigate the reliable data transport problem in underwater sensor networks. Underwater sensor networks are significantly different from terrestrial sensor networks in two aspects: acoustic channels are used for communication and most sensor nodes are mobile due to water current. These distinctions feature underwater sensor networks with low bandwidth capacity, large propagation delay, high error probability, half-duplex channels, and highly dynamic topology, which pose many new challenges for reliable data transport in underwater sensor networks. In this paper, we propose a protocol, called segmented data reliable transport (SDRT), to achieve reliable data transfer in underwater sensor networks. SDRT is essentially a hybrid approach of ARQ and FEC. It adopts efficient erasure codes (so-called SVT codes in this paper), transferring encoded packets block by block and hop by hop. Compared with other existing reliable data transport approaches for underwater networks, SDRT can reduce the total number of transmitted packets, improve channel utilization, and simplify protocol management. In addition, we develop a mathematic model to estimate the expected number of packets actually needed. Based on this model, we can set the block size appropriately for SDRT, as helps to address the node mobility issue. We conduct simulations to evaluate our model and SDRT. The results show that our model can closely predict the number of packets actually needed, and SDRT is energy efficient and can achieve high channel utilization.","Transport protocols,
Acoustic sensors,
Telecommunication network reliability,
Capacitive sensors,
Underwater acoustics,
Mobile communication,
Underwater communication,
Bandwidth,
Capacity planning,
Propagation delay"
"SNMP Traffic Analysis: Approaches, Tools, and First Results","The simple network management protocol (SNMP) is widely deployed to monitor, control, and configure network elements. Even though the SNMP technology is well documented and understood, it remains relatively unclear how SNMP is used in practice and what the typical SNMP usage patterns are. This paper discusses how to perform large-scale SNMP traffic measurements in order to develop a better understanding of how SNMP is used in production networks. The tools described in this paper have been applied to networks ranging from large national research networks to relatively small faculty networks. The goal of the research is to provide feedback to SNMP protocol developers within the IETF, researchers working within the context of the IRTF-NMRG, as well as other researchers interested in network management in general. We believe that the results are also valuable for operators and vendors who want to optimize their management interactions or understand the traffic generated by their management software.","Protocols,
Performance evaluation,
Computer science,
Production,
Telecommunication traffic,
Jacobian matrices,
Computer network management,
Computerized monitoring,
Large-scale systems,
Communication system traffic control"
Improved Video Registration using Non-Distinctive Local Image Features,"The task of registering video frames with a static model is a common problem in many computer vision domains. The standard approach to registration involves finding point correspondences between the video and the model and using those correspondences to numerically determine registration transforms. Current methods locate video-to-model point correspondences by assembling a set of reference images to represent the model and then detecting and matching invariant local image features between the video frames and the set of reference images. These methods work well when all video frames can be guaranteed to contain a sufficient number of distinctive visual features. However, as we demonstrate, these methods are prone to severe misregistration errors in domains where many video frames lack distinctive image features. To overcome these errors, we introduce a concept of local distinctiveness which allows us to find model matches for nearly all video features, regardless of their distinctiveness on a global scale. We present results from the American football domain-where many video frames lack distinctive image features-which show a drastic improvement in registration accuracy over current methods. In addition, we introduce a simple, empirical stability test that allows our method to be fully automated. Finally, we present a registration dataset from the American football domain we hope can be used as a benchmarking tool for registration methods.","Computer vision,
Robustness,
Assembly,
Computer science,
Stability,
Automatic testing,
Benchmark testing,
Cameras,
Video sequences,
Robot localization"
Classification of Heterogeneous Fuzzy Data by Choquet Integral With Fuzzy-Valued Integrand,"As a fuzzification of the Choquet integral, the defuzzified choquet integral with fuzzy-valued integrand (DCIFI) takes a fuzzy-valued integrand and gives a crisp-valued integration result. In this paper, the DCIFI acts as a projection to project high-dimensional heterogeneous fuzzy data to one-dimensional crisp data to handle the classification problems involving different data forms, such as crisp data, interval values, fuzzy numbers, and linguistic variables, simultaneously. The nonadditivity of the signed fuzzy measure applied in the DCIFI can represent the interaction among the measurements of features towards the discrimination of classes. Values of the signed fuzzy measure in the DCIFI are considered to be unknown parameters which should be learned before the classifier is used to classify new data. We have implemented a genetic algorithm (GA)-based adaptive classifier-learning algorithm to optimally learn the signed fuzzy measure values and the classified boundaries simultaneously. The performance of our algorithm has been tested both on synthetic and real data. The experimental results are satisfactory and outperform those of existing methods, such as the fuzzy decision trees and the fuzzy-neuro networks.","Data mining,
Fuzzy sets,
Extraterrestrial measurements,
Genetic algorithms,
Testing,
Decision trees,
Data engineering,
Mathematics,
Computer science,
Performance evaluation"
Time to Contact Relative to a Planar Surface,"We show how to determine the time to contact from time varying images using only accumulated sums of suitable products of image brightness derivatives. There is no need for feature or object detection, tracking of features, estimation of optical flow, or any ""higher level"" processing. This so-called ""direct"" method for determining the time to contact is based on analysis of the motion field resulting from rigid body motion under perspective projection and the constant brightness assumption. The method has essentially no latency, since it can be based on analysis of just two frames of a video sequence, and does not require a calibrated camera. An implementation of the method is demonstrated on synthetic image sequences and stop motion sequences - where the ground truth is accurately know - as well as on video sequences taken by a camera mounted on moving vehicles.","Brightness,
Computer vision,
Image motion analysis,
Motion analysis,
Video sequences,
Cameras,
Object detection,
Delay,
Image sequence analysis,
Image sequences"
Automatic Segmentation of the Caudate Nucleus From Human Brain MR Images,"We describe a knowledge-driven algorithm to automatically delineate the caudate nucleus (CN) region of the human brain from a magnetic resonance (MR) image. Since the lateral ventricles (LVs) are good landmarks for positioning the CN, the algorithm first extracts the LVs, and automatically localizes the CN from this information guided by anatomic knowledge of the structure. The face validity of the algorithm was tested with 55 high-resolution T1-weighted magnetic resonance imaging (MRI) datasets, and segmentation results were overlaid onto the original image data for visual inspection. We further evaluated the algorithm by comparing automated segmentation results to a ""gold standard"" established by human experts for these 55 MR datasets. Quantitative comparison showed a high intraclass correlation between the algorithm and expert as well as high spatial overlap between the regions-of-interest (ROIs) generated from the two methods. The mean spatial overlap plusmn standard deviation (defined by the intersection of the 2 ROIs divided by the union of the 2 ROIs) was equal to 0.873 plusmn 0.0234. The algorithm has been incorporated into a public domain software program written in Java and, thus, has the potential to be of broad benefit to neuroimaging investigators interested in basal ganglia anatomy and function","Image segmentation,
Humans,
Magnetic resonance imaging,
Magnetic resonance,
Data mining,
Testing,
Inspection,
Gold,
Software algorithms,
Java"
Recent developments on silicon photomultipliers produced at FBK-irst,"In this contribution, new developments on the silicon photomultipliers (SiPMs) fabricated at FBK-irst (Trento, Italy) are reported. With respect to the first series of devices produced in 2005/2006, there have been major improvements on both the the layout and the technology. Concerning the first aspect we fabricated SiPMs with increased fill factor and with different geometries (square/circular devices, arrays and matrices of SiPMs) to meet the requirements of different applications. Concerning the technology, we identified a process technique able to reduce significantly the dark count rate. In this paper we will describe the main electro-optical characteristics of these devices.","Silicon,
Photomultipliers,
Nuclear and plasma sciences,
Geometry,
Laboratories,
Physics,
Testing,
Biomedical optical imaging,
Optical devices,
Optical crosstalk"
Using Color Compatibility for Assessing Image Realism,"Why does placing an object from one photograph into another often make the colors of that object suddenly look wrong? One possibility is that humans prefer distributions of colors that are often found in nature; that is, we find pleasing these color combinations that we see often. Another possibility is that humans simply prefer colors to be consistent within an image, regardless of what they are. In this paper, we explore some of these issues by studying the color statistics of a large dataset of natural images, and by looking at differences in color distribution in realistic and unrealistic images. We apply our findings to two problems: 1) classifying composite images into realistic vs. non- realistic, and 2) recoloring image regions for realistic compositing.",
Eliciting honest value information in a batch-queue environment,"Markets and auctions have been proposed as mechanisms lor efficiently and fairly allocating resources in a number of different computational settings. Economic approaches to resource allocation in batch-controlled systems, however, have proved difficult due to the fact that, unlike reservation systems, every resource allocation decision made by the scheduler affects the turnaround time of all jobs in the queue. Economists refer to this characteristic as an ""externality"", where a transaction affects more than just the immediate resource consumer and producer. The problem is particularly acute for computational grid systems where organizations wish to engage in service-level agreements but are not at liberty to abandon completely the use of space-sharing and batch scheduling as the local control policies. Grid administrators desire the ability to make these agreements based on anticipated user demand, but eliciting truthful reportage of job importance and priority has proved difficult due to the externalities present when resources are batch controlled. In this paper we propose and evaluate the application of the Expected Externality Mechanism as an approach to solving this problem that is based on economic principles. In particular, this mechanism provides incentives for users to reveal information honestly about job importance and priority in an environment where batch-scheduler resource allocation decisions introduce ""externalities"" that affect all users. Our tests indicate that the mechanism meets its theoretical predictions in practice and can be implemented in a computationally tractable manner.","Resource management,
Environmental economics,
Economic forecasting,
Processor scheduling,
Grid computing,
Computer science,
Control systems,
Testing,
Hardware,
Bridges"
A Min-Min Max-Min selective algorihtm for grid task scheduling,"Today, the high cost of supercomputers in the one hand and the need for large-scale computational resources on the other hand, has led to use network of computational resources known as Grid. Numerous research groups in universities, research labs, and industries around the world are now working on a type of Grid called Computational Grids that enable aggregation of distributed resources for solving large-scale data intensive problems in science, engineering, and commerce. Several institutions and universities have started research and teaching programs on Grid computing as part of their parallel and distributed computing curriculum. To better use tremendous capabilities of this distributed system, effective and efficient scheduling algorithms are needed. In this paper, we introduce a new scheduling algorithm based on two conventional scheduling algorithms, Min-Min and Max-Min, to use their cons and at the same time, cover their pros. It selects between the two algorithms based on standard deviation of the expected completion time of tasks on resources. We evaluate our scheduling heuristic, the Selective algorithm, within a grid simulator called GridSim. We also compared our approach to its two basic heuristics. The experimental results show that the new heuristic can lead to significant performance gain for a variety of scenarios.",
Interactive sound rendering in complex and dynamic scenes using frustum tracing,"We present a new approach for real-time sound rendering in complex, virtual scenes with dynamic sources and objects. Our approach combines the efficiency of interactive ray tracing with the accuracy of tracing a volumetric representation. We use a four-sided convex frustum and perform clipping and intersection tests using ray packet tracing. A simple and efficient formulation is used to compute secondary frusta and perform hierarchical traversal. We demonstrate the performance of our algorithm in an interactive system for complex environments and architectural models with tens or hundreds of thousands of triangles. Our algorithm can perform real-time simulation and rendering on a high-end PC.","Layout,
Rendering (computer graphics),
Ray tracing,
Computational modeling,
Hardware,
Acceleration,
Data visualization,
Optical reflection,
Acoustic reflection,
Acoustic beams"
Utilizing OFDM Guard Interval for Spectrum Sensing,"Spectrum sensing is crucial for dynamic spectrum management systems. In this paper, we propose a scheme that utilizes the guard interval of OFDM symbol at the transmitter for spectrum sensing. The cyclic prefix is not inserted in the guard interval at the transmitter, whereas the circulant convolution is secured at the OFDM receiver through the proposed mechanism. Simulation results show that the scheme can be implemented with no impact on the BER under various channel conditions, and detection of incumbent DTV signal is possible in the OFDM guard interval. In addition, we develop enhancements to the circulant convolution preserving mechanism for handling the transceiver imperfections in practice.",
Residual-Based Measurement of Peer and Link Lifetimes in Gnutella Networks,"Existing methods of measuring lifetimes in P2P systems usually rely on the so-called create-based method (CBM), which divides a given observation window into two halves and samples users ""created"" in the first half every Delta time units until they die or the observation period ends. Despite its frequent use, this approach has no rigorous accuracy or overhead analysis in the literature. To shed more light on its performance, we flrst derive a model for CBM and show that small window size or large Delta may lead to highly inaccurate lifetime distributions. We then show that create-based sampling exhibits an inherent tradeoff between overhead and accuracy, which does not allow any fundamental improvement to the method. Instead, we propose a completely different approach for sampling user dynamics that keeps track of only residual lifetimes of peers and uses a simple renewal-process model to recover the actual lifetimes from the observed residuals. Our analysis indicates that for reasonably large systems, the proposed method can reduce bandwidth consumption by several orders of magnitude compared to prior approaches while simultaneously achieving higher accuracy. We finish the paper by implementing a two-tier Gnutella network crawler equipped with the proposed sampling method and obtain the distribution of ultrapeer lifetimes in a network of 6.4 million users and 60 million links. Our experimental results show that ultrapeer lifetimes are Pareto with shape a alpha ap 1.1; however, link lifetimes exhibit much lighter tails with alpha ap 1.9.","Sampling methods,
Time measurement,
Bandwidth,
Crawlers,
Streaming media,
Routing,
Communications Society,
Computer science,
USA Councils,
Shape"
Active Learning from Data Streams,"In this paper, we address a new research problem on active learning from data streams where data volumes grow continuously and labeling all data is considered expensive and impractical. The objective is to label a small portion of stream data from which a model is derived to predict newly arrived instances as accurate as possible. In order to tackle the challenges raised by data streams' dynamic nature, we propose a classifier ensembling based active learning framework which selectively labels instances from data streams to build an accurate classifier. A minimal variance principle is introduced to guide instance labeling from data streams. In addition, a weight updating rule is derived to ensure that our instance labeling process can adaptively adjust to dynamic drifting concepts in the data. Experimental results on synthetic and real-world data demonstrate the performances of the proposed efforts in comparison with other simple approaches.","Labeling,
Predictive models,
Data mining,
USA Councils,
Accuracy,
Uncertainty,
Computer science,
Data engineering,
Decision making,
Association rules"
Scene Parsing Using Region-Based Generative Models,"Semantic scene classification is a challenging problem in computer vision. In contrast to the common approach of using low-level features computed from the whole scene, we propose ""scene parsing"" utilizing semantic object detectors (e.g., sky, foliage, and pavement) and region-based scene-configuration models. Because semantic detectors are faulty in practice, it is critical to develop a region-based generative model of outdoor scenes based on characteristic objects in the scene and spatial relationships between them. Since a fully connected scene configuration model is intractable, we chose to model pairwise relationships between regions and estimate scene probabilities using loopy belief propagation on a factor graph. We demonstrate the promise of this approach on a set of over 2000 outdoor photographs, comparing it with existing discriminative approaches and those using low-level features",
Legible Cities: Focus-Dependent Multi-Resolution Visualization of Urban Relationships,"Numerous systems have been developed to display large collections of data for urban contexts; however, most have focused on layering of single dimensions of data and manual calculations to understand relationships within the urban environment. Furthermore, these systems often limit the user's perspectives on the data, thereby diminishing the user's spatial understanding of the viewing region. In this paper, we introduce a highly interactive urban visualization tool that provides intuitive understanding of the urban data. Our system utilizes an aggregation method that combines buildings and city blocks into legible clusters, thus providing continuous levels of abstraction while preserving the user's mental model of the city. In conjunction with a 3D view of the urban model, a separate but integrated information visualization view displays multiple disparate dimensions of the urban data, allowing the user to understand the urban environment both spatially and cognitively in one glance. For our evaluation, expert users from various backgrounds viewed a real city model with census data and confirmed that our system allowed them to gain more intuitive and deeper understanding of the urban model from different perspectives and levels of abstraction than existing commercial urban visualization systems.",
Severless Search and Authentication Protocols for RFID,"With the increasing popularity of RFID applications, different authentication schemes have been proposed to provide security and privacy protection to users. Most recent RFID protocols use a central database to store the RFID tag data. An RFID reader first queries the RFID tag and returns the reply to the database. After authentication, the database returns the tag data to the reader. In this paper, we proposed a more flexible authentication protocol that provides comparable protection without the need for a central database. We also suggest a protocol for secure search for RFID tags. We believe that as RFID applications become widespread, the ability to search for RFID tags will be increasingly useful","Authentication,
Radiofrequency identification,
Databases,
RFID tags,
Data security,
Privacy,
Protection,
Access protocols,
Computer science,
Educational institutions"
Embodiment and Human-Robot Interaction: A Task-Based Perspective,"In this work, we further test the hypothesis that physical embodiment has a measurable effect on performance and impression of social interactions. Support for this hypothesis would suggest fundamental differences between virtual agents and robots from a social standpoint and would have significant implications for human-robot interaction. We have refined our task-based metrics to give a measurement, not only of the participant's immediate impressions of a coach for a task, but also of the participant's performance in a given task. We measure task performance and participants' impression of a robot's social abilities in a structured task based on the Towers of Hanoi puzzle. Our experiment compares aspects of embodiment by evaluating: (1) the difference between a physical robot and a simulated one; and (2) the effect of physical presence through a co-located robot versus a remote, tele-present robot. With a participant pool (n=21) of roboticists and non- roboticists, we were able to show that participants felt that an embodied robot w as more appealing and perceptive of the world than non-embodied robots. A larger pool of participants (n=32) also demonstrated that the embodied robot was seen as most helpful, watchful, and enjoyable when compared to a remote tele-present robot and a simulated robot.","Rehabilitation robotics,
Human robot interaction,
Particle measurements,
Laboratories,
Embedded system,
Computer science,
USA Councils,
System testing,
Poles and towers,
Computational modeling"
Approximation Algorithms for Partial-Information Based Stochastic Control with Markovian Rewards,"We consider a variant of the classic multi-armed bandit problem (MAB), which we call feedback MAB, where the reward obtained by playing each of n independent arms varies according to an underlying on/off Markov process with known parameters. The evolution of the Markov chain happens irrespective of whether the arm is played, and furthermore, the exact state of the Markov chain is only revealed to the player when the arm is played and the reward observed. At most one arm (or in general, M arms) can be played any time step. The goal is to design a policy for playing the arms in order to maximize the infinite horizon time average expected reward. This problem is an instance of a partially observable Markov decision process (POMDP), and a special case of the notoriously intractable ""restless bandit"" problem. Unlike the stochastic MAB problem, the feedback MAB problem does not admit to greedy index-based optimal policies. Vie state of the system at any time step encodes the beliefs about the states of different arms, and the policy decisions change these beliefs - this aspect complicates the design and analysis of simple algorithms. We design a constant factor approximation to the feedback MAB problem by solving and rounding a natural LP relaxation to this problem. As far as we are aware, this is the first approximation algorithm for a POMDP problem.","Approximation algorithms,
Stochastic processes,
Arm,
Transmitters,
Computer science,
Infinite horizon,
Algorithm design and analysis,
Feedback,
Markov processes"
A Combinatorial Approach to Measuring Anonymity,"In this paper we define a new metric for quantifying the degree of anonymity collectively afforded to users of an anonymous communication system. We show how our metric, based on the permanent of a matrix, can be useful in evaluating the amount of information needed by an observer to reveal the communication pattern as a whole. We also show how our model can be extended to include probabilistic information learned by an attacker about possible sender-recipient relationships. Our work is intended to serve as a complementary tool to existing information-theoretic metrics, which typically consider the anonymity of the system from the perspective of a single user or message.","Computer science,
Particle measurements,
Size measurement,
Privacy,
Computational complexity,
Tin,
Q measurement"
Projector Calibration using Arbitrary Planes and Calibrated Camera,"In this paper, an easy calibration method for projector is proposed. The calibration handled in this paper is projective relation between 3D space and 2D pattern, and is not correction of trapezoid distortion in projected pattern. In projector-camera systems, especially for 3D measurement, such calibration is the basis of process. The projection from projector can be modeled as inverse projection of the pinhole camera, which is generally considered as perspective projection. In the existing systems, some special objects or devices are often used to calibrate projector, so that 3D-2D projection map can be measured for typical camera calibration methods. The proposed method utilizes projective geometry between camera and projector, so that it requires only pre-calibrated camera and a plane. It is easy to practice, easy to calculate, and reasonably accurate.",
Integrating Diagnostic B-Mode Ultrasonography Into CT-Based Radiation Treatment Planning,"This paper presents methods and a clinical procedure for integrating B-mode ultrasound images tagged with position information with a planning computed tomography (CT) scan for radiotherapy. A workflow is described that allows the integration of these modalities into the clinic. A surface mapping approach provides a preregistration of the ultrasound image borders onto the patient's skin. Successively, a set of individual ultrasound images from a freehand sweep is chosen by the physician. These images are automatically registered with the planning CT scan using novel intensity-based methods. We put a particular focus on deriving an appropriate similarity measure based on the physical properties and artifacts of ultrasound. A combination of a weighted mutual information term, edge correlation, clamping to the skin surface, and occlusion detection is able to assess the alignment of structures in ultrasound images and information reconstructed from the CT data. We demonstrate the practicality of our methods on five patients with head and neck tumors and cervical lymph node metastases and provide a detailed report on the conducted experiments, including the setup, calibration, acquisition, and verification of our algorithms. The mean target registration error on nine data sets is 3.9 mm. Thus, the additional information about intranodal architecture and fulfillment of malignancy criteria derived from a high-resolution ultrasonography of lymph nodes can be localized and visualized in the CT scan coordinate space and is made available for further radiation treatment planning.","Ultrasonic imaging,
Computed tomography,
Skin,
Image edge detection,
Lymph nodes,
Particle measurements,
Ultrasonic variables measurement,
Mutual information,
Clamps,
Surface reconstruction"
The Effects of Active Queue Management and Explicit Congestion Notification on Web Performance,"We present an empirical study of the effects of active queue management (AQM) and explicit congestion notification (ECN) on the distribution of response times experienced by users browsing the Web. Three prominent AQM designs are considered: the proportional integral (PI) controller, the random exponential marking (REM) controller, and adaptive random early detection (ARED). The effects of these AQM designs were studied with and without ECN. Our primary measure of performance is the end-to-end response time for HTTP request-response exchanges. Our major results are as follows. If ECN is not supported, ARED operating in byte-mode was the best performing design, providing better response time performance than drop-tail queueing at offered loads above 90% of link capacity. However, ARED operating in packet-mode (with or without ECN) was the worst performing design, performing worse than drop-tail queueing. ECN support is beneficial to PI and REM. With ECN, PI and REM were the best performing designs, providing significant improvement over ARED operating in byte-mode. In the case of REM, the benefit of ECN was dramatic. Without ECN, response time performance with REM was worse than drop-tail queueing at all loads considered. ECN was not beneficial to ARED. Under current ECN implementation guidelines, ECN had no effect on ARED performance. However, ARED performance with ECN improved significantly after reversing a guideline that was intended to police unresponsive flows. Overall, the best ARED performance was achieved without ECN. Whether or not the improvement in response times with AQM is significant, depends heavily on the range of round-trip times (RTTs) experienced by flows. As the variation in flows' RTT increases, the impact of AQM and ECN on response-time performance is reduced.We conclude that AQM can improve application and network performance for Web or Web-like workloads. In particular, it appears likely that with AQM and ECN, provider links may be operated at near saturation levels without significant degradation in user-perceived performance.","Delay,
Pi control,
Proportional control,
Guidelines,
Computer science,
TCPIP,
Programmable control,
Adaptive control,
Time measurement,
Degradation"
Particle Swarm Optimization in High-Dimensional Bounded Search Spaces,"When applying particle swarm optimization (PSO) to real world optimization problems, often boundary constraints have to be taken into account. In this paper, we show that the bound handling mechanism essentially influences the swarm behavior, especially in high-dimensional search spaces. In our theoretical analysis, we prove that all particles are initialized very close to the boundary with overwhelming probability, and that the global guide is expected to leave the search space in every forth dimension. Afterwards, we investigate the initialization process when optimizing the sphere function, a widely used benchmark, in more detail in order to provide a first step towards explaining previously observed phenomena. Moreover, we present a broad experimental study of commonly applied bound handling mechanisms on a variety of benchmark functions which is useful for choosing an appropriate strategy in real world applications. Finally, we derive some guidelines for the practical application of the PSO algorithm in high-dimensional bounded search spaces","Particle swarm optimization,
Constraint optimization,
Guidelines,
Computer science,
Counting circuits,
Topology,
Equations,
Upper bound,
Proposals"
GP-UKF: Unscented kalman filters with Gaussian process prediction and observation models,"This paper considers the use of non-parametric system models for sequential state estimation. In particular, motion and observation models are learned from training examples using Gaussian process (GP) regression. The state estimator is an unscented Kalman filter (UKF). The resulting GP-UKF algorithm has a number of advantages over standard (parametric) UKFs. These include the ability to estimate the state of arbitrary nonlinear systems, improved tracking quality compared to a parametric UKF, and graceful degradation with increased model uncertainty. These advantages stem from the fact that GPs consider both the noise in the system and the uncertainty in the model. If an approximate parametric model is available, it can be incorporated into the GP; resulting in further performance improvements. In experiments, we show how the GP-UKF algorithm can be applied to the problem of tracking an autonomous micro-blimp.",
A Multichip Neuromorphic System for Spike-Based Visual Information Processing,"We present a multichip, mixed-signal VLSI system for spike-based vision processing. The system consists of an 80 × 60 pixel neuromorphic retina and a 4800 neuron silicon cortex with 4,194,304 synapses. Its functionality is illustrated with experimental data on multiple components of an attention-based hierarchical model of cortical object recognition, including feature coding, salience detection, and foveation. This model exploits arbitrary and reconfigurable connectivity between cells in the multichip architecture, achieved by asynchronously routing neural spike events within and between chips according to a memory-based look-up table. Synaptic parameters, including conductance and reversal potential, are also stored in memory and are used to dynamically configure synapse circuits within the silicon neurons.",
Variable-Number Variable-Band Selection for Feature Characterization in Hyperspectral Signatures,"This paper presents a novel band selection-based feature characterization technique for a hyperspectral signature, which is referred to as variable-number variable-band selection (VNVBS). Since a hyperspectral signature can be uniquely characterized by its spectral profile, its feature characterization can be achieved by selecting appropriate bands from the original set of spectral bands, and the number of bands to be selected is totally determined by its original spectral shape. As a result, two hyperspectral signatures may require different sets of bands for spectral feature characterization. Therefore, the proposed VNVBS allows one to select a different number of variable bands in accordance with the hyperspectral signature to be processed. In order for the VNVBS to select an appropriate subset of bands for a hyperspectral signature, a new band prioritization criterion (BPC), which is referred to as orthogonal subspace projector based BPC, is derived. It assigns a different priority score to each spectral band of a hyperspectral signature such that various features can be captured by the VNVBS. Accordingly, the VNVBS can be interpreted as a spectral feature extraction technique for hyperspectral signature characterization. Finally, experiments using two data sets are conducted to demonstrate that the VNVBS can improve the performance of the hyperspectral signature characterization.",
An Implementation and Evaluation of Client-Side File Caching for MPI-IO,"Client-side file caching has long been recognized as a file system enhancement to reduce the amount of data transfer between application processes and I/O servers. However, caching also introduces cache coherence problems when a file is simultaneously accessed by multiple processes. Existing coherence controls tend to treat the client processes independently and ignore the aggregate I/O access pattern. This causes a serious performance degradation for parallel I/O applications. In this paper we discuss our new implementation and present an extended performance evaluation on GPFS and Lustre parallel file systems. In addition to comparing our methods to traditional approaches, we examine the performance of MPI-IO caching under direct I/O mode to bypass the underlying file system cache. We also investigate the performance impact of two file domain partitioning methods to MPI collective I/O operations: one which creates a balanced workload and the other which aligns accesses to the file system stripe size. In our experiments, alignment results in better performance by reducing file lock contention. When the cache page size is set to a multiple of the stripe size, MPI-IO caching inherits the same advantage and produces significantly improved I/O bandwidth.",
Discovering Temporal Communities from Social Network Documents,"This paper studies the discovery of communities from social network documents produced over time, addressing the discovery of temporal trends in community memberships. We first formulate static community discovery at a single time period as a tripartite graph partitioning problem. Then we propose to discover the temporal communities by threading the statically derived communities in different time periods using a new constrained partitioning algorithm, which partitions graphs based on topology as well as prior information regarding vertex membership. We evaluate the proposed approach on synthetic datasets and a real-world dataset prepared from the CiteSeer.",
Iterative Image Reconstruction Using Inverse Fourier Rebinning for Fully 3-D PET,"We describe a fast forward and back projector pair based on inverse Fourier rebinning for use in iterative image reconstruction for fully 3-D positron emission tomography (PET). The projector pair is used as part of a factored system matrix that takes into account detector-pair response by using shift-variant sinogram blur kernels, thereby combining the computational advantages of Fourier rebinning with iterative reconstruction using accurate system models. The forward projector consists of a 2-D projector, which maps 3-D images into 2-D direct sinograms, followed by exact inverse rebinning which maps the 2-D into fully 3-D sinograms. The back projector is implemented as the transpose of the forward projector and differs from the true exact rebinning operator in the sense that it does not require reprojection to compute missing lines of response (LORs). We compensate for two types of inaccuracies that arise in a cylindrical PET scanner when using inverse Fourier rebinning: 1) nonuniform radial sampling and 2) nonconstant oblique angles in the radial direction in a single oblique sinogram. We examine the effects of these corrections on sinogram accuracy and reconstructed image quality. We evaluate performance of the new projector pair for maximum a posteriori (MAP) reconstruction of simulated and in vivo data. The new projector results in only a small loss in resolution towards the edge of the field-of-view when compared to the fully 3-D geometric projector and requires an order of magnitude less computation","Image reconstruction,
Positron emission tomography,
Two dimensional displays,
Iterative methods,
Solid modeling,
Image quality,
Computational efficiency,
Detectors,
Fast Fourier transforms,
Kernel"
System identification using quantized data,"In this paper we consider the problem of identification of linear systems using quantized data. We argue that, where possible, it is desirable to not utilize ""naively"" quantized data but instead it is preferable to choose the quantization mechanism carefully. In particular, we show that using a generalized noise shaping coder improves the accuracy of the estimates. We examine the accuracy of estimates for both naive and coded quantizers.","System identification,
Quantization,
Noise shaping,
Signal processing,
Communication channels,
Communication system control,
Additive noise,
Control systems,
USA Councils,
Linear systems"
Landing a Helicopter on a Moving Target,We present the design of an optimal trajectory controller for landing a helicopter on a moving target. The trajectory planner is based on the variational Hamiltonian and Euler-Lagrange equations. We use a kinematic model of the helicopter to derive an optimal controller that is able to track an arbitrarily moving target and then land on it. Simulations are shown to verify the performance of the optimal trajectory controller. Data from real flight trials is presented to validate the inputs obtained from the trajectory planner to track a desired trajectory. We present initial trials in simulation for landing the helicopter autonomously on a moving target.,"Helicopters,
Trajectory,
Target tracking,
Optimal control,
Remotely operated vehicles,
Equations,
Kinematics,
Mobile robots,
Land use planning,
Control systems"
Toward Optimal Data Aggregation in Random Wireless Sensor Networks,"Data gathering is one of the most important services provided by wireless sensor networks (WSNs). Since the predominant traffic pattern in data gathering services is many-to-one communication, it is critical to understand the limitations of many-to-one information flows and devise efficient data aggregation protocols to support prolonged operations in WSNs. In this paper, we provide a theoretical characterization of data aggregation processes under different communication modalities in WSNs. We demonstrate that data aggregation rates of Theta(log(n)/n) and Theta(1) are optimal when operating in fading environments with power path-loss exponents that satisfy 2 < alpha < 4 and alpha > 4, respectively. Furthermore, the optimal rate can be achieved using a generalization of cooperative beam-forming called cooperative time-reversal communication. In contrast, the non-cooperative multihop relay strategies widely adopted in literature are shown to be suboptimal in the low-to-medium attenuation regime (for 2 < alpha < 4).","Wireless sensor networks,
Relays,
Protocols,
Telecommunication traffic,
Unicast,
Wireless networks,
Upper bound,
Topology,
Communications Society,
Computer science"
Compact Nano-Electro-Mechanical Non-Volatile Memory (NEMory) for 3D Integration,"A new electro-mechanical non-volatile memory (NVM) cell design is proposed and demonstrated for the first time. The fabricated cells operate with relatively low program/erase voltages and large sensing margin. Because only dielectric and metal layers are required, this cell design is suitable for post-CMOS fabrication. As the cell area is reduced, low operating voltages can be maintained by scaling the vertical dimensions of the cell. Nanometer-scale electro-mechanical memory technology is therefore attractive for high-density embedded memory applications.","Nonvolatile memory,
Fabrication,
Electrodes,
Hysteresis,
Circuits,
Flash memory,
Costs,
Dielectrics,
Low voltage,
Tunneling"
Human Pose Estimation using Motion Exemplars,"We present a motion exemplar approach for finding body configuration in monocular videos. A motion correlation technique is employed to measure the motion similarity at various space-time locations between the input video and stored video templates. These observations are used to predict the conditional state, distributions of exemplars and joint positions. Exemplar sequence selection and joint position estimation are then solved with approximate inference using Gibbs sampling and gradient ascent. The presented approach is able to find joint positions accurately for people with textured clothing. Results are presented on a dataset containing slow, fast and incline walk videos of various people from different view angles. The results demonstrate an overall improvement compared to previous methods.","Humans,
Motion estimation,
Clothing,
Kinematics,
State estimation,
Birth disorders,
Board of Directors,
Motion detection,
Tracking,
Image sequences"
Stereovision Based Vehicle Tracking in Urban Traffic Environments,"This paper presents an algorithm for tracking the cuboids generated from grouping the 3D points obtained through stereovision. The solution described in the paper takes into consideration the particularities of the scenario and of the sensor, and brings considerable improvement in all the phases of tracking: initialization, prediction, measurement and update. The corner of the cuboid becomes the central working concept, thus improving the handling of partially occluded objects, of objects partially out of the field of view, and of objects whose measurement is fragmented by the sensor inaccuracies. After association at corner level, multiple measurements or validated parts of a measurement form a virtual object, the meta measurement, which is used for track update. The size of a vehicle is tracked using a histogram voting method. The resulted algorithm shows robustness and accuracy in the crowded urban scenario.","Sensor phenomena and characterization,
Intelligent transportation systems,
Intelligent vehicles,
USA Councils,
Particle measurements,
Phase measurement,
Histograms,
Voting,
Robustness,
Kalman filters"
Probe Minimization by Schedule Optimization: Supporting Top-K Queries with Expensive Predicates,"This paper addresses the problem of evaluating ranked top-k queries with expensive predicates. As major DBMSs now all support expensive user-defined predicates for Boolean queries, we believe such support for ranked queries can be even more important: first, ranked queries often need to model user-specific concepts of preference, relevance, or similarity, which call for dynamic user-defined functions. Second, middleware systems must incorporate external predicates for integrating autonomous sources typically accessible only by per-object queries. Third, ranked queries often accompany Boolean ranking conditions, which may turn predicates into expensive ones, as the index structure on the predicate built on the base table may be no longer effective in retrieving the filtered objects in order. Fourth, fuzzy joins are inherently expensive, as they are essentially user-defined operations that dynamically associate multiple relations. These predicates, being dynamically defined or externally accessed, cannot rely on index mechanisms to provide zero-time sorted output, and must instead require per-object probe to evaluate. To enable probe minimization, we develop the problem as cost-based optimization of searching over potential probe schedules. In particular, we decouple probe scheduling into object and predicate scheduling problems and develop an analytical object scheduling optimization and a dynamic predicate scheduling optimization, which combined together form a cost-effective probe schedule","Probes,
Optimal scheduling,
Dynamic scheduling,
Information retrieval,
Image retrieval,
Computer science,
Middleware,
Query processing,
Distributed information systems,
Database systems"
Modeling and Managing Thermal Profiles of Rack-mounted Servers with ThermoStat,"High power densities and the implications of high operating temperatures on the failure rates of components are key driving factors of temperature-aware computing. Computer architects and system software designers need to understand the thermal consequences of their proposals, and develop techniques to lower operating temperatures to reduce both transient and permanent component failures. Tools for understanding temperature ramifications of designs have been mainly restricted to industry for studying packaging and cooling mechanisms, with little access to such toolsets for academic researchers. Developing such tools is an arduous task since it usually requires cross-cutting areas of expertise spanning architecture, systems software, thermodynamics, and cooling systems. Recognizing the need for such tools, there has been work on modeling temperatures of processors at the micro-architectural level which can be easily understood and employed by computer architects for processor designs. However, there is a dearth of such tools in the academic/research community for undertaking architectural/systems studies beyond a processor - a server box, rack or even a machine room. This paper presents a detailed 3-dimensional computational fluid dynamics based thermal modeling tool, called ThermoStat, for rack-mounted server systems. Using this tool, we model a 20 (each with dual Xeon processors) node rack-mounted server system, and validate it with over 30 temperature sensor measurements at different points in the servers/rack. We conduct several experiments with this tool to show how different load conditions affect the thermal profile, and also illustrate how this tool can help design dynamic thermal management techniques","Thermal management,
Thermostats,
Temperature,
System software,
Cooling,
Thermal conductivity,
Thermal loading,
Power system modeling,
Power system management,
Software design"
Business and IT Alignment with SEAM for Enterprise Architecture,"To align an IT system with an organization's needs, it is necessary to understand the organization 's position within its environment as well as its internal configuration. In SEAM for enterprise architecture the organization is considered as a hierarchy of systems that span from business down to IT. The alignment process addresses the complete hierarchy. We illustrate the use of SEAM for enterprise architecture with an example in which a new hiring process and an IT system are developed. With this approach it is possible to train new engineers in the design of business and IT alignment. It is also possible to scope projects in a way that integrate both business and IT strategies. This enables the consideration of IT developments in an enterprise-wide context.","Companies,
Computer architecture,
Application software,
Computer science,
Programming,
Software systems,
Distributed computing,
Business communication,
Gas insulated transmission lines,
Design engineering"
A Bayesian network based Qos assessment model for web services,"Quality of service (QoS) plays a key role in Web services. In an open and volatile environment, a provider may not deliver the QoS it declared. Hence, it's necessary to provide a QoS assessment model to determine the likely behavior of a provider. Although many researches have been done to develop models and techniques to assist users in QoS assessment, most of them ignore various QoS requirements of users, which are great important to evaluate a provider adopting the policy based on service differentiation. In this paper, we propose an approach, called Bayesian network based QoS assessment model, to QoS assessment. Through online learning, it supports to update the corresponding Bayesian network dynamically. The salient feature of this model is that it can correctly predict the provider's capability in various combinations of users' QoS requirements, especially to the provider with different service levels. Experimental results show that the proposed QoS assessment model is effective.","Bayesian methods,
Web services,
Quality of service,
Predictive models,
Monitoring,
Computer science,
Software quality,
Resource management,
Computer networks,
Pattern recognition"
COPACC: An Architecture of Cooperative Proxy-Client Caching System for On-Demand Media Streaming,"Proxy caching is a key technique to reduce transmission cost for on-demand multimedia streaming. The effectiveness of current caching schemes, however, is limited by the insufficient storage space and weak cooperation among proxies and their clients, particularly considering the high bandwidth demands from media objects. In this paper, we propose COPACC, a cooperative proxy-and-client caching system that addresses the above deficiencies. This innovative approach combines the advantages of both proxy caching and peer-to-peer client communications. It leverages the client-side caching to amplify the aggregated cache space and rely on dedicated proxies to effectively coordinate the communications. We propose a comprehensive suite of distributed protocols to facilitate the interactions among different network entities in COPACC. It also realizes a smart and cost-effective cache indexing, searching, and verifying scheme. Furthermore, we develop an efficient cache allocation algorithm for distributing video segments among the proxies and clients. The algorithm not only minimizes the aggregated transmission cost of the whole system, but also accommodates heterogeneous computation and storage constraints of proxies and clients. We have extensively evaluated the performance of COPACC under various network and end-system configurations. The results demonstrate that it achieves remarkably lower transmission cost as compared to pure proxy-based caching with limited storage space. On the other hand, it is much more robust than a pure peer-to-peer communication system in the presence of node failures. Meanwhile, its computation and control overheads are both kept in low levels","Streaming media,
Peer to peer computing,
Costs,
Network servers,
Web server,
Web and internet services,
Bandwidth,
Protocols,
Indexing,
Robustness"
Guaranteed-Delivery Geographic Routing Under Uncertain Node Locations,"Geographic routing protocols like GOAFR or GPSR rely on exact location information at the nodes, as when the greedy routing phase gets stuck at a local minimum, they require, as a fallback, a planar subgraph whose identification, in all existing methods, depends on exact node positions. In practice, however, location information at the network nodes is hardly precise; be it because the employed location hardware, such as GPS, exhibits an inherent measurement imprecision, or because the localization protocols which estimate positions of the network nodes cannot do so without errors. In this paper we propose a novel naming and routing scheme that can handle the uncertainty in location information. It is based on a macroscopic variant of geographic greedy routing, as well as a macroscopic planarization of the communication graph. If an upper bound on the deviation from true node locations is available, our routing protocol guarantees delivery of messages. Due to its macroscopic view, our routing scheme also produces shorter and more load-balanced paths than common geographic routing schemes, in particular in sparsely connected networks or in the presence of obstacles.","Peer to peer computing,
Wireless sensor networks,
Routing protocols,
Global Positioning System,
Planarization,
Communications Society,
Computer science,
Hardware,
Position measurement,
Upper bound"
Evaluation of the Low Frame Error Rate Performance of LDPC Codes Using Importance Sampling,"We present an importance sampling method for the evaluation of the low frame error rate (FER) performance of LDPC codes under iterative decoding. It relies on a combinatorial characterization of absorbing sets, which are the dominant cause of decoder failure in the low FER region. The biased density in the importance sampling scheme is a mean-shifted version of the original Gaussian density, which is suitably centered between a codeword and a dominant absorbing set. This choice of biased density yields an unbiased estimator for the FER with a variance lower by several orders of magnitude than the standard Monte Carlo estimator. Using this importance sampling scheme in software, we obtain good agreement with the experimental results obtained from a fast hardware emulator of the decoder.","Error analysis,
Parity check codes,
Monte Carlo methods,
Iterative decoding,
Hardware,
Maximum likelihood decoding,
Message passing,
Yield estimation,
Signal to noise ratio,
Bit error rate"
Using Online Traffic Statistical Matching for Optimizing Packet Filtering Performance,"Packet classification plays a critical role in many of the current networking technologies, and efficient yet lightweight packet classification techniques are highly crucial for their successful deployment. Most of the current packet classification techniques exploit the characteristics of classification policies, without considering the traffic behavior in optimizing their search data structures. In this paper, we present novel techniques that utilize traffic characteristics coupled with careful analysis of the policy to obtain adaptive methods that can accommodate varying traffic statistics while maintaining a high throughput. The first technique uses segmentation of the traffic space to achieve disjoint subsets of traffic properties and build bounded depth Huffman trees using the statistics collected for these segments. The second technique simplifies the structure maintenance by keeping the segments ordered in a most-recently-used (MRU) list instead of a tree. The techniques are evaluated and their performance are compared. Moreover, attacks targeting the firewall performance are discussed and corresponding protection schemes are presented.","Matched filters,
Telecommunication traffic,
Information filtering,
Information filters,
Intrusion detection,
Internet,
Communications Society,
Computer science,
USA Councils,
Data structures"
DENGRAPH: A Density-based Community Detection Algorithm,Detecting densely connected subgroups in graphs such as communities in social networks is of interest in many research fields. Several methods have been developed to find communities but most of them have a high time complexity and are thus not applicable for large networks. Inspired by the clustering algorithm incremental DBSCAN we propose a density-based graph clustering algorithm DENGRAPH that is designed to deal with large dynamic datasets with noise and present first experimental results.,"Detection algorithms,
Clustering algorithms,
Social network services,
Partitioning algorithms,
Algorithm design and analysis,
Intelligent networks,
Computer science,
Communities,
Iterative algorithms,
World Wide Web"
Lattice-Based Volumetric Global Illumination,"We describe a novel volumetric global illumination framework based on the face-centered cubic (FCC) lattice. An FCC lattice has important advantages over a Cartesian lattice. It has higher packing density in the frequency domain, which translates to better sampling efficiency. Furthermore, it has the maximal possible kissing number (equivalent to the number of nearest neighbors of each site), which provides optimal 3D angular discretization among all lattices. We employ a new two-pass (illumination and rendering) global illumination scheme on an FCC lattice. This scheme exploits the angular discretization to greatly simplify the computation in multiple scattering and to minimize illumination information storage. The GPU has been utilized to further accelerate the rendering stage. We demonstrate our new framework with participating media and volume rendering with multiple scattering, where both are significantly faster than traditional techniques with comparable quality.","Lighting,
Lattices,
Rendering (computer graphics),
Optical scattering,
FCC,
Light scattering,
Particle scattering,
Visualization,
Vectors,
Sampling methods"
A New Validation Method for X-ray Mammogram Registration Algorithms Using a Projection Model of Breast X-ray Compression,"Establishing spatial correspondence between features visible in X-ray mammograms obtained at different times has great potential to aid assessment and quantitation of change in the breast indicative of malignancy. The literature contains numerous non- rigid registration algorithms developed for this purpose, but existing approaches are flawed by the assumption of inappropriate 2-D transformation models and quantitative estimation of registration accuracy is limited. In this paper, we describe a novel validation method which simulates plausible mammographic compressions of the breast using a magnetic resonance imaging (MRI) derived finite element model. By projecting the resulting known 3-D displacements into 2-D and generating pseudo-mammograms from these same compressed magnetic resonance (MR) volumes, we can generate convincing images with known 2-D displacements with which to validate a registration algorithm. We illustrate this approach by computing the accuracy for two conventional nonrigid 2-D registration algorithms applied to mammographic test images generated from three patient MR datasets. We show that the accuracy of these algorithms is close to the best achievable using a 2-D one-to-one correspondence model but that new algorithms incorporating more representative transformation models are required to achieve sufficiently accurate registrations for this application.",
Global Optimization through Searching Rotation Space and Optimal Estimation of the Essential Matrix,"This paper extends the set of problems for which a global solution can be found using modern optimization methods. In particular, the method is applied to estimation of the essential matrix, giving the first guaranteed optimal algorithm for estimating the relative pose under a geometric cost function, in this case, the L-infinity cost function. Convex optimization techniques has been shown to provide optimal solutions to many of the common problems in structure from motion. However, they do not apply to problems involving rotations. In this paper, we introduce a search method that allows such problems to be solved optimally. Apart from the essential matrix, the algorithm is applied to the camera pose problem, providing an optimal algorithm.","Cameras,
Optimization methods,
Australia,
Coordinate measuring machines,
Cost function,
Noise measurement,
Image reconstruction,
Search methods,
Minimax techniques,
Geometry"
An architecture for exploiting multi-core processors to parallelize network intrusion prevention,"It is becoming increasingly difficult to implement effective systems for preventing network attacks, due to the combination of (1) the rising sophistication of attacks requiring more complex analysis to detect, (2) the relentless growth in the volume of network traffic that we must analyze, and, critically, (3) the failure in recent years for uniprocessor performance to sustain the exponential gains that for so many years CPUs enjoyed (ldquoMoorepsilas Lawrdquo). For commodity hardware, tomorrowpsilas performance gains will instead come from multicore architectures in which a whole set of CPUs executes concurrently. Taking advantage of the full power of multi-core processors for network intrusion prevention requires an indepth approach. In this work we frame an architecture customized for parallel execution of network attack analysis. At the lowest layer of the architecture is an ldquoActive Network Interfacerdquo (ANI), a custom device based on an inexpensive FPGA platform. The ANI provides the inline interface to the network, reading in packets and forwarding them after they are approved. It also serves as the front-end for dispatching copies of the packets to a set of analysis threads. The analysis itself is structured as an event-based system, which allows us to find many opportunities for concurrent execution, since events introduce a natural, decoupled asynchrony into the flow of analysis while still maintaining good cache locality. Finally, by associating events with the packets that ultimately stimulated them, we can determine when all analysis for a given packet has completed, and thus that it is safe to forward the pending packet - providing none of the analysis elements previously signaled that the packet should instead be discarded.","Multicore processing,
Failure analysis,
Performance analysis,
Performance gain,
Signal analysis,
Telecommunication traffic,
Hardware,
Field programmable gate arrays,
Dispatching,
Yarn"
Learning Multiscale Representations of Natural Scenes Using Dirichlet Processes,"We develop nonparametric Bayesian models for multiscale representations of images depicting natural scene categories. Individual features or wavelet coefficients are marginally described by Dirichlet process (DP) mixtures, yielding the heavy-tailed marginal distributions characteristic of natural images. Dependencies between features are then captured with a hidden Markov tree, and Markov chain Monte Carlo methods used to learn models whose latent state space grows in complexity as more images are observed. By truncating the potentially infinite set of hidden states, we are able to exploit efficient belief propagation methods when learning these hierarchical Dirichlet process hidden Markov trees (HDP-HMTs) from data. We show that our generative models capture interesting qualitative structure in natural scenes, and more accurately categorize novel images than models which ignore spatial relationships among features.","Layout,
Hidden Markov models,
Wavelet coefficients,
Statistics,
Computer science,
Tree graphs,
Image representation,
Statistical distributions,
Bayesian methods,
State-space methods"
"Semantic Web Services, Part 1","Semantic Web services (SWS) has been a vigorous technology research area for about six years. A great deal of innovative work has been done, and a great deal remains. Several large research initiatives have been producing substantial bodies of technology, which are gradually maturing. SOA vendors are looking seriously at semantic technologies and have made initial commitments to supporting selected approaches. In the world of standards, numerous activities have reflected the strong interest in this work. Perhaps the most visible of these is Sawsdl (Weerawarana, 2005). Sawsdl recently achieved Recommendation status at the World Wide Web Consortium. Sawsdl's completion provides a fitting opportunity to reflect on the state of the art and practice in SWS - past, present, and future. This two-part installment of Trends & Controversies discusses what has been accomplished in SWS, what value SWS can ultimately provide, and where we can go from here to reap these technologies' benefits.","Semantic Web,
Computer science,
Resource description framework,
Service oriented architecture,
History,
OWL,
Energy management,
Web services,
MySpace,
Humans"
Automatic Fall Incident Detection in Compressed Video for Intelligent Homecare,"This paper presents a compressed-domain fall incident detection scheme for intelligent homecare applications. First, a compressed-domain object segmentation scheme is performed to extract moving objects based on global motion estimation and local motion clustering. After detecting the moving objects, three compressed-domain features of each object are then extracted for identifying and locating fall incidents. The proposed system can differentiate fall-down from squatting by taking into account the event duration. Our experiments show that the proposed method can correctly detect fall incidents in real time.","Video compression,
Senior citizens,
Event detection,
Surveillance,
Cameras,
Computer vision,
Computerized monitoring,
Smoke detectors,
Injuries,
Application software"
Distributed Healthcare: Simultaneous Assessment of Multiple Individuals,"Employing pervasive computing technologies can help enable continuous patient monitoring and assessment in various settings outside of hospitals, lowering healthcare costs and allowing earlier detection of problems","Medical services,
Patient monitoring,
Biomedical monitoring,
Legged locomotion,
Costs,
Senior citizens,
Condition monitoring,
Pervasive computing,
Security,
Aging"
Autonomously Flying VTOL-Robots: Modeling and Control,"In this paper an approach for control of autonomously flying robots with vertical take off and landing capabilities (VTOL) is presented. After reviewing that the motion description for different VTOL-robots is very similar, the general control scheme for VTOL-robots is presented. This scheme is based on linearisation and decoupling using inversion of the system model blocks. To compensate the model uncertainties and disturbances two additional parts are included into the controller: a reduced state observer based on a robot motion model as well as a disturbances observer and compensator for orientation control. The presented approach was applied to two different VTOL-robots: a helicopter and a quad-rotor. In real flight experiments it was verified that the presented general but simple controller provides sufficient performance for a wide range of practical applications.",
Achievable rate region of CSMA schedulers in wireless networks with primary interference constraints,"We consider Carrier Sense Multiple Access (CSMA) schedulers for wireless networks. For networks where all nodes are within transmission range of each other, it has been shown that such schedulers achieve the network capacity in the limiting region of large networks with a small sensing delay. However the design and analysis of CSMA schedulers for general networks has been an open problem due to the complexity of the interaction among coupled interference constraints. For networks with primary interference constraints, we introduce a tractable analysis of such CSMA schedulers based on a fixed point approximation. We then use the approximation to characterize the achievable rate region of static CSMA schedulers. We show that the approximation is asymptotically accurate for the limiting regime of large networks with a small sensing delay, and that in this case the achievable rate region of CSMA converges to the capacity region.","Multiaccess communication,
Wireless networks,
Interference constraints,
USA Councils,
Throughput,
Couplings,
Processor scheduling,
Computer science,
Subcontracting,
Access protocols"
Skin Detail Analysis for Face Recognition,"This paper presents a novel framework to localize in a photograph prominent irregularities in facial skin, in particular nevi (moles, birthmarks). Their characteristic configuration over a face is used to encode the person's identity independent of pose and illumination. This approach extends conventional recognition methods, which usually disregard such small scale variations and thereby miss potentially highly discriminative features. Our system detects potential nevi with a very sensitive multi scale template matching procedure. The candidate points are filtered according to their discriminative potential, using two complementary methods. One is a novel skin segmentation scheme based on gray scale texture analysis that we developed to perform outlier detection in the face. Unlike most other skin detection/segmentation methods it does not require color input. The second is a local saliency measure to express a point's uniqueness and confidence taking the neighborhood's texture characteristics into account. We experimentally evaluate the suitability of the detected features for identification under different poses and illumination on a subset of the FERET face database.","Skin,
Face recognition,
Face detection,
Lighting,
Computer vision,
Spatial databases,
Principal component analysis,
Facial features,
Computer science,
Performance analysis"
High-Level Power Estimation and Low-Power Design Space Exploration for FPGAs,"In this paper, we present a simultaneous resource allocation and binding algorithm for FPGA power minimization. To fully validate our methodology and result, our work targets a real FPGA architecture - Altera Stratix FPGA, which includes generic logic elements, DSP cores, and memories, etc. We design a high-level power estimator for this architecture and evaluate its estimation accuracy against a commercial gate-level power estimator - Quartus II PowerPlay Analyzer. During the synthesis stage, we pay special attention to interconnections and multiplexers. We concentrate on resource allocation and binding tasks because they are the key steps to determine the interconnections. We use a novel approach to explore the design space. Experimental results show that our high-level power estimator is 8.7% away from PowerPlay Analyzer. Meanwhile, we are able to achieve a significant amount of power reduction (32%) with better circuit speed (16%) compared to a traditional resource allocation and binding algorithm.",
Chip-interleaved self-encoded multiple access with iterative detection in fading channels,"We propose to apply chip interleaving and iterative detection to self-encoded multiple access (SEMA) communications. In SEMA, the spreading code is obtained from user bit information itself without using a pseudo noise code generator. The proposed scheme exploits the inherent diversity in self encoded spread spectrum signals. Chip interleaving not only increases the diversity gain, but also enhances the performance of iterative detection. We employ user-mask and interference cancellation to decouple self-encoded multiuser signals. This paper describes the proposed scheme and analyzes its performance. The analytical and simulation results show that the proposed system can achieve a 3 dB power gain and possess a diversity gain that can yield a significant performance improvement in both Rayleigh and multipath fading channels.","Receivers,
Bit error rate,
Signal to noise ratio,
Interference,
Rayleigh channels,
Detectors"
"Initial conditions, generalized functions, and the laplace transform troubles at the origin","The unilateral Laplace transform is widely used to analyze signals, linear models, and control systems, and is consequently taught to most engineering undergraduates. In our courses at MIT in electrical engineering and computer science, mathematics, and mechanical engineering, we have found some significant pitfalls associated with teaching students to understand and apply the Laplace transform. We have independently concluded that one reason students find the Laplace transform difficult is that there is significant confusion present in many of the standard textbook presentations of this subject, in all three of our disciplines","Laplace equations,
Differential equations,
Signal analysis,
Control system synthesis,
Electrical engineering,
Computer science,
Mathematics,
Mechanical engineering,
Education,
Transient analysis"
ASRPAKE: An Anonymous Secure Routing Protocol with Authenticated Key Exchange for Wireless Ad Hoc Networks,"In this paper, we present a novel anonymous secure routing protocol for mobile ad hoc networks (MANETs). The proposed protocol not only provides anonymity from all the intermediate nodes, but also integrates the authenticated key exchange mechanisms into the routing algorithm design. Furthermore, a new attack on anonymous services, called snare attack, is introduced, where a compromised node lures a very important node (VIN) into communicating with him and traces back to the VIN by following the route path. An adversary can then snare the VIN and launch decapitation strike on the VIN. Finally, we present a novel DECOY mechanism as a countermeasure to enhance anonymity of VINs and defeat snare attack.","Routing protocols,
Mobile ad hoc networks,
Peer to peer computing,
Algorithm design and analysis,
Mobile communication,
Elliptic curve cryptography,
Communications Society,
Computer science,
Communication system security,
Military communication"
Texture-Preserving Shadow Removal in Color Images Containing Curved Surfaces,"Several approaches to shadow removal in color images have been introduced in recent years. Yet these methods fail in removing shadows that are cast on curved surfaces, as well as retaining the original texture of the image in shadow boundaries, known as penumbra regions. In this paper, we propose a novel approach which effectively removes shadows from curved surfaces while retaining the textural information in the penumbra, yielding high quality shadow-free images. Our approach aims at finding scale factors to cancel the effect of shadows, including penumbra regions where illumination changes gradually. Due to the fact that surface geometry is also taken into account when computing the scale factors, our method can handle a wider range of shadow images than current state-of-the-art methods, as demonstrated by several examples.","Color,
Surface texture,
Lighting,
Computer science,
Computational geometry,
Image segmentation,
Pixel,
Testing,
Integral equations,
Layout"
Dots and Incipients: Extended Features for Partial Fingerprint Matching,"There are fundamental differences in the way fingerprints are compared by forensic examiners and current automatic systems. For example, while automatic systems focus mainly on the quantitative measures of fingerprint minutiae (ridge ending and bifurcation points), forensic examiners often analyze details of intrinsic ridge characteristics and relational information. This process, known as qualitative friction ridge analysis [1], includes examination of ridge shape, pores, dots, incipient ridges, etc. This explains the challenges that current automatic systems face in processing partial fingerprints, mostly seen in latents. The forensics and automatic fingerprint identification systems (AFIS) communities have been active in standardizing the definition of extended feature set, as well as quantifying the relevance and reliability of these features for automatic matching systems. CDEFFS (committee to define an extended feature set) has proposed a working draft on possible definitions and representations of extended features [2]. However, benefits of utilizing these extended features in automatic systems are not yet known. While fingerprint matching technology is quite mature for matching tenprints [3], matching partial fingerprints, especially latents, still needs a lot of improvement. We propose an algorithm to extract two major level 3 feature types, dots and incipients, based on local phase symmetry and demonstrate their effectiveness in partial print matching. Since dots and incipients can be easily encoded by forensic examiners, we believe the results of this research will have benefits to next generation identification (NGI) systems.",
Distributed Algorithms for Secure Multipath Routing in Attack-Resistant Networks,"To proactively defend against intruders from readily jeopardizing single-path data sessions, we propose a distributed secure multipath solution to route data across multiple paths so that intruders require much more resources to mount successful attacks. Our work exhibits several important properties that include: (1) routing decisions are made locally by network nodes without the centralized information of the entire network topology; (2) routing decisions minimize throughput loss under a single-link attack with respect to different session models; and (3) routing decisions address multiple link attacks via lexicographic optimization. We devise two algorithms termed the Bound-Control algorithm and the Lex-Control algorithm, both of which provide provably optimal solutions. Experiments show that the Bound-Control algorithm is more effective to prevent the worst-case single-link attack when compared to the single-path approach, and that the Lex-Control algorithm further enhances the Bound-Control algorithm by countering severe single-link attacks and various types of multi-link attacks. Moreover, the Lex-Control algorithm offers prominent protection after only a few execution rounds, implying that we can sacrifice minimal routing protection for significantly improved algorithm performance. Finally, we examine the applicability of our proposed algorithms in a specialized defensive network architecture called the attack-resistant network and analyze how the algorithms address resiliency and security in different network settings.","Distributed algorithms,
Peer to peer computing,
Protection,
Data security,
Network topology,
Resilience,
Throughput,
Algorithm design and analysis,
Routing protocols,
Computer science"
A Study of a Transactional Parallel Routing Algorithm,"Transactional memory proposes an alternative synchronization primitive to traditional locks. Its promise is to simplify the software development of multi-threaded applications while at the same time delivering the performance of parallel applications using (complex and error prone) fine grain locking. This study reports our experience implementing a realistic application using transactional memory (TM). The application is Lee's routing algorithm and was selected for its abundance of parallelism but difficulty of expressing it with locks. Each route between a source and a destination point in a grid can be considered a unit of parallelism. Starting from this simple approach, we evaluate the exploitable parallelism of a transactional parallel implementation and explore how it can be adapted to deliver better performance. The adaptations do not introduce locks nor alter the essence of the implemented algorithm, but deliver up to 20 times more parallelism. The adaptations are derived from understanding the application itself and TM. The evaluation simulates an abstracted TM system and, thus, the results are independent of specific software or hardware TM implemented, and describe properties of the application.","Routing,
Application software,
Parallel processing,
Hardware,
Parallel programming,
Printed circuits,
Computer science,
Computer errors,
Large-scale systems,
Multicore processing"
Revisiting Negative Selection Algorithms,"This paper reviews the progress of negative selection algorithms, an anomaly/change detection approach in Artificial Immune Systems (AIS). Following its initial model, we try to identify the fundamental characteristics of this family of algorithms and summarize their diversities. There exist various elements in this method, including data representation, coverage estimate, affinity measure, and matching rules, which are discussed for different variations. The various negative selection algorithms are categorized by different criteria as well. The relationship and possible combinations with other AIS or other machine learning methods are discussed. Prospective development and applicability of negative selection algorithms and their influence on related areas are then speculated based on the discussion.","machine learning,
Artificial immune systems,
negative selection algorithms"
"On Conflict-Avoiding Codes of Length
n=4m
for Three Active Users","New improved upper and lower bounds on the maximum size of a symmetric or arbitrary conflict-avoiding code of length n = 4 m for three active users are proved. Furthermore, direct constructions for optimal conflict-avoiding codes of length n = 4 m and m equiv 2 (mod 4) for three active users are provided.","Protocols,
Optical feedback,
Combinatorial mathematics,
Information science,
Binary sequences,
Optical receivers,
Optical design,
Australia,
Graph theory,
Computer science"
Experimental Measurement of the Capacity for VoIP Traffic in IEEE 802.11 WLANs,"We measured the capacity for VoIP traffic in an 802.11b test-bed and compared it with the theoretical capacity and our simulation results. We identified factors that have been commonly overlooked in past studies but affect experiments and simulations. We found that in many papers the capacity for VoIP traffic has been measured via simulations or experiments without considering those factors, showing different capacity in each paper. After these corrections, simulations and experiments yielded a capacity estimate of 15 calls for 64 kb/s CBR VoIP traffic with 20 ms packetization interval and 34 calls to 36 calls for VBR VoIP traffic with 0.39 activity ratio.",
Probabilistic Heuristics for Disseminating Information in Networks,"We study the problem of disseminating a piece of information through all the nodes of a network, given that it is known originally only to a single node. In the absence of any structural knowledge on the network, other than the nodes' neighborhoods, this problem is traditionally solved by flooding all the network's edges. We analyze a recently introduced probabilistic algorithm for flooding and give an alternative probabilistic heuristic that can lead to some cost-effective improvements, like better trade-offs between the message and time complexities involved. We analyze the two algorithms, both mathematically and by means of simulations, always within a random-graph framework and considering relevant node-degree distributions","Peer to peer computing,
Floods,
Algorithm design and analysis,
Bandwidth,
Analytical models,
Bidirectional control,
Distributed computing,
Delay effects,
Systems engineering and theory,
Computer science"
Topology Control and Channel Assignment in Multi-Radio Multi-Channel Wireless Mesh Networks,"The aggregate capacity of wireless mesh networks can be improved significantly by equipping each node with multiple interfaces and by using multiple channels in order to reduce the effect of interference. Efficient channel assignment is required to ensure the optimal use of the limited channels in the radio spectrum. In this paper, a cluster-based multipath topology control and channel assignment scheme (CoMTaC), is proposed, which explicitly creates a separation between the channel assignment and topology control functions, thus minimizing flow disruptions. A cluster-based approach is employed to ensure basic network connectivity. Intrinsic support for broadcasting with minimal overheads is also provided. CoMTaC also takes advantage of the inherent multiple paths that exist in a typical WMN by constructing a spanner of the network graph and using the additional node interfaces. The second phase of CoMTaC proposes a dynamic distributed channel assignment algorithm, which employs a novel interference estimation mechanism based on the average link-layer queue length within the interference domain. Partially overlapping channels are also included in the channel assignment process to enhance the network capacity. Extensive simulation based experiments have been conducted to test various parameters and the effectiveness of the proposed scheme. The experimental results show that the proposed scheme outperforms existing dynamic channel assignment schemes by a minimum of a factor of 2.","Network topology,
Wireless mesh networks,
Interference,
Broadcasting,
Telecommunication traffic,
Routing protocols,
Radio control,
Throughput,
Costs,
Computer science"
Unbalanced Expanders and Randomness Extractors from Parvaresh-Vardy Codes,"We give an improved explicit construction of highly unbalanced bipartite expander graphs with expansion arbitrarily close to the degree (which is polylogarithmic in the number of vertices). Both the degree and the number of right-hand vertices are polynomially close to optimal, whereas the previous constructions of Ta-Shma, Umans, and Zuckerman (STOC ""01) required at least one of these to be quasipolynomial in the optimal. Our expanders have a short and self-contained description and analysis, based on the ideas underlying the recent list-decodable error-correcting codes of Parvaresh and Vardy (FOCS ""05). Our expanders can be interpreted as near-optimal ""randomness condensers,"" that reduce the task of extracting randomness from sources of arbitrary min-entropy rate to extracting randomness from sources of min-entropy rate arbitrarily close to 1, which is a much easier task. Using this connection, we obtain a new construction of randomness extractors that is optimal up to constant factors, while being much simpler than the previous construction of Lu et al. (STOC ""03) and improving upon it when the error parameter is small (e.g. 1/poly(n)).","Graph theory,
Computer science,
Polynomials,
Error correction codes,
Bipartite graph,
Data structures,
Decoding,
Random number generation,
Application software,
Computational complexity"
An Organizational Evolutionary Algorithm for Numerical Optimization,"Taking inspiration from the interacting process among organizations in human societies, this correspondence designs a kind of structured population and corresponding evolutionary operators to form a novel algorithm, organizational evolutionary algorithm (OEA), for solving both unconstrained and constrained optimization problems. In OEA, a population consists of organizations, and an organization consists of individuals. All evolutionary operators are designed to simulate the interaction among organizations. In experiments, 15 unconstrained functions, 13 constrained functions, and 4 engineering design problems are used to validate the performance of OEA, and thorough comparisons are made between the OEA and the existing approaches. The results show that the OEA obtains good performances in both the solution quality and the computational cost. Moreover, for the constrained problems, the good performances are obtained by only incorporating two simple constraints handling techniques into the OEA. Furthermore, systematic analyses have been made on all parameters of the OEA. The results show that the OEA is quite robust and easy to use.",
Performance of Limited Feedback Schemes for Downlink OFDMA with Finite Coherence Time,"We consider the capacity of a downlink orthogonal frequency division multiple access (OFDMA) system with limited feedback rate RF per sub-channel and finite coherence time T. The feedback is used to relay channel state information (CSI) from K users to the base station. The order-optimal capacity growth with Rayleigh fading sub-channels is ominus(N log log K) as N and K increase with fixed ratio, where N is the number of sub-channels. However, to achieve this, previous work requires a feedback rate per subchannel that scales linearly with the system size. Here we explicitly include the feedback overhead when calculating the sum capacity, and study the tradeoff between feedback rate and sum capacity. We propose two limited feedback schemes, one based on sequential transmissions across users and the other based on random access, in which the each feedback bit requests the use of a sub-channel group containing multiple subchannels. With fixed RFT, the sum capacity for both schemes with optimized sub-channel groups increases as ominus(N). If RFT grows faster than log K, then both schemes can achieve the order- optimal capacity growth. We also show that when RFT is small, the random access scheme performs better than the sequential transmission scheme, whereas the reverse is true for large RFT.","Downlink,
Base stations,
State feedback,
Frequency conversion,
Radio frequency,
Channel state information,
Rayleigh channels,
Coherence,
Relays,
Frequency diversity"
"(t, k) - Diagnosis for Matching Composition Networks under the MM* Model","(t, k)-diagnosis, which is a generalization of sequential diagnosis, requires at least k faulty processors identified and repaired in each iteration provided there are at most t faulty processors, where tgesk. In this paper, a (t, k)-diagnosis algorithm under the MM* model is proposed for matching composition networks, which include many well-known interconnection networks, such as hypercubes, crossed cubes, twisted cubes, and Mobius cubes. It is shown that a matching composition network of n dimensions is (Omega((2n*log n)/n), n)-diagnosable","multiprocessor interconnection networks,
fault diagnosis,
fault tolerant computing"
Rogue Access Point Detection by Analyzing Network Traffic Characteristics,"One of the most challenging network security concerns for network administrators is the presence of rogue access points. Rogue access points, if undetected, can be an open door to sensitive information on the network. Many data raiders have taken advantage of the undetected rogue access points in enterprises to not only get free Internet access, but also to view confidential information. Most of the current solutions to detect rouge access points are not automated and are dependent on a specific wireless technology. In this paper, we present a rogue access point detection approach. The approach is an automated solution which can be installed on any router at the edge of a network. The main premise of our approach is to distinguish authorized WLAN hosts from unauthorized WLAN hosts connected to rogue access points by analyzing traffic characteristics at the edge of a network. Simulation results verify the effectiveness of our approach in detecting rogue access points in a heterogeneous network comprised of wireless and wired subnets.","Telecommunication traffic,
Communication system security,
Wireless LAN,
Wireless networks,
Information security,
Traffic control,
Frequency,
Personnel,
Wireless sensor networks,
Monitoring"
Learning Motion Correlation for Tracking Articulated Human Body with a Rao-Blackwellised Particle Filter,"Inference in 3D articulated human body tracking is challenging due to the high dimensionality and nonlinearity of the parameter-space. We propose a particle filter with Rao-Blackwellisation which marginalizes part of the state variables by exploiting the correlation between the right-side and the left-side joint Euler angles. The correlation is naturally induced by the symmetric and repetitive patterns in specific human activities. A novel algorithm is proposed to learn the correlation from the training data using partial least square regression. The learned correlation is then used as motion prior in designing the Rao-Blackwellised particle filter, which estimates only one group of state variables using the Monte Carlo method, leaving the other group being exactly computed through an analytical filter that utilizes the learned motion correlation. We evaluate the effectiveness of the motion correlation for 3D articulated human body tracking. The accuracy of the proposed 3D tracker is quantitatively assessed based on the distance between the true and the estimated marker positions. Extensive experiments with multi-camera walking sequences from the HumanEva-I/II data set show that (i) the proposed tracker achieves significantly lower estimation error than both the annealed particle filter and the standard particle filter; and (ii) the learned motion correlation generalizes well to motion performed by subjects other than the training subject.","Particle tracking,
Humans,
Particle filters,
Motion analysis,
Joints,
Training data,
Least squares methods,
Motion estimation,
State estimation,
Legged locomotion"
The State and Parameter Estimation of an Li-Ion Battery Using a New OCV-SOC Concept,"The open circuit voltage (OCV) is widely used to estimate the state of charge (SOC) in many SOC estimation algorithms. But the relationship between the OCV and SOC can not be exactly same for all batteries. Because the conventional OCV-SOC differs between batteries, there is a problem in that the OCV-SOC data should be measured to accurately estimate the SOC in different batteries. Therefore, the conventional OCV- SOC should be modified. In this paper, a new OCV-SOC that is independent of the battery conditions is proposed. Thus, problems resulting from the defects of the EKF can be avoided by preventing the OCV-SOC data from varying. In this paper, the SOC and battery capacity are estimated using the dual EKF with the proposed OCV-SOC.","Parameter estimation,
Voltage,
Filters,
State estimation,
Battery charge measurement,
Hybrid electric vehicles,
Circuit testing,
Computer science,
Recursive estimation,
Temperature"
3D Haar-Like Features for Pedestrian Detection,"One basic observation for pedestrian detection in video sequences is that both appearance and motion information are important to model the moving people. Based on this observation, we propose a new kind of features, 3D Haar-like (3DHaar) features. Motivated by the success of Haar-like features in image based face detection and differential-frame based pedestrian detection, we naturally extend this feature by defining seven types of volume filters in 3D space, instead of using rectangle filter in 2D space. The advantage is that it can not only represent pedestrian's appearance, but also capture the motion information. To validate the effectiveness of the proposed method, we combine the 3DHaar with support vector machine (SVM) for pedestrian detection. Our experiments demonstrate the 3DHaar are more effective for video based pedestrian detection.","Computer vision,
Video sequences,
Support vector machines,
Motion detection,
Humans,
Lighting,
Face detection,
Filters,
Biological system modeling,
Support vector machine classification"
Localization With Limited Sensing,"Localization is a fundamental problem for many kinds of mobile robots. Sensor systems of varying ability have been proposed and successfully used to solve the problem. This paper probes the lower limits of this range by describing three extremely simple robot models and addresses the active localization problem for each. The robot, whose configuration is composed of its position and orientation, moves in a fully-known, simply connected polygonal environment. We pose the localization task as a planning problem in the robot's information space, which encapsulates the uncertainty in the robot's configuration. We consider robots equipped with: 1) angular and linear odometers; 2) a compass and contact sensor and; 3) an angular odometer and contact sensor. We present localization algorithms for models 1 and 2 and show that no algorithm exists for model 3. An implementation with simulation examples is presented.","Robot sensing systems,
Orbital robotics,
Mobile robots,
Sensor systems,
Uncertainty,
Probes,
Detectors,
Computer science,
Rotation measurement"
Feedback linearization and simultaneous stiffness-position control of robots with antagonistic actuated joints,"In this paper, the dynamic model of a robot with antagonistic actuated joints is presented, and the problem of full linearization via static state feedback is analyzed. The use of transmission elements with nonlinear relation between the displacement and the actuated force allows to control both the position and the stiffness of each joint. The main advantage of this actuation modality is that the achieved stiffness becomes a mechanical characteristic of the system and it is not the result of an immediate control action as in the classical impedance control scheme (Davison, 2003). Different examples of implementation of this kind of devices are known in literature, even if limited to one single joint (Kjita et al., 2003; Laumond and Kineocam, 2006; Mansard and Chaumette, 2004 and 2006) and the application of antagonistic actuated kinematic chains in the field of robotic hand design is under investigation (Stasse et al., 2006). After a brief review of the dependence of the properties of antagonistic actuation on the transmission elements characteristics, a scheme for simultaneous stiffness-position control of the linearized system is presented. Finally, simulation results of a two-link antagonistic actuated arm are reported and discussed.","Linear feedback control systems,
Robot control,
Service robots,
Orbital robotics,
Robot sensing systems,
State feedback,
Control systems,
Electrical equipment industry,
Actuators,
Humans"
The Projection Explorer: A Flexible Tool for Projection-based Multidimensional Visualization,"Multidimensional projections map data points, defined in a high-dimensional data space, into a 1D, 2D or 3D representation space. Such a mapping may be typically achieved with dimensional reduction, clustering, or force directed point placement. Projections can be displayed and navigated by data analysts by means of visual representations, which may vary from points on a plane to graphs, surfaces or volumes. Typically, projections strive to preserve distance relationships amongst data points, as defined in the original space. Information loss is inevitable and the projection approach defines the extent to which the distance preserving goal is attained. We introduce PEx-the projection explorer - a visualization tool for mapping and exploration of high-dimensional data via projections. A set of examples - on both structured (table) and unstructured (text) data - illustrate how projection based visualizations, coupled with appropriate exploration tools, offer a flexible set-up for multidimensional data exploration. The projections in PEx handle relatively large data sets at a computational cost adequate to user interaction.","Multidimensional systems,
Data visualization,
Data analysis,
Principal component analysis,
Navigation,
Computational efficiency,
Eigenvalues and eigenfunctions,
Computer graphics,
Image processing,
Cognitive science"
A Static Load-Balancing Scheme for Parallel XML Parsing on Multicore CPUs,"A number of techniques to improve the parsing performance of XML have been developed. Generally, however, these techniques have limited impact on the construction of a DOM tree, which can be a significant bottleneck. Meanwhile, the trend in hardware technology is toward an increasing number of cores per CPU. As we have shown in previous work, these cores can be used to parse XML in parallel, resulting in significant speedups. In this paper, we introduce a new static partitioning and load-balancing mechanism. By using a static, global approach, we reduce synchronization and load-balancing overhead, thus improving performance over dynamic schemes for a large class of XML documents. Our approach leverages libxm12 without modification, which reduces development effort and shows that our approach is applicable to real-world, production parsers. Our scheme works well with Sun's Niagara class of CMT architectures, and shows that multiple hardware threads can be effectively used for XML parsing.","XML,
Multicore processing,
Hardware,
Yarn,
Memory management,
Computer science,
Tree data structures,
Clocks,
Concurrent computing,
Production"
Unequal Growth Codes: Intermediate Performance and Unequal Error Protection for Video Streaming,"We investigate the design of fountain codes with good intermediate performance and built-in unequal error protection for low-delay video multicast. In particular, we design novel short-blocklength fountain codes for media streaming applications to multiple heterogeneous receivers and analyze their performance. Our theoretical contribution is the generalization of the growth code analysis for unequal error protection to suit the characteristics of video data. Simulation results show that the proposed method can effectively increase the number of decodable packets over a very wide range of packet drop rates and provide smooth and graceful video quality degradation for users with various channel conditions. The proposed scheme also enjoys the important benefits of much lower decoder complexity and simpler system architecture compared to traditional MDS erasure coding based solutions.","Error correction codes,
Streaming media,
Decoding,
Degradation,
Automatic repeat request,
Forward error correction,
Bandwidth,
Computer errors,
Video compression,
Video codecs"
Fuzzy Logic Control System for Autonomous Sailboats,Sailing experts can explain basic sailing skills by rules about how to steer sails and rudder according to direction of target and wind. This paper describes how to transform the sailor's knowledge into Mamdani type fuzzy inference systems. The proposed system controls two actuators - rudder and sails - even during tack and jibe. In combination with an automatic weather routeing system the sailboat is able to reach any target completely autonomously. Experiments on a demonstration sailboat have shown excellent results. Detailed log data analysis shows manoeuvres carried out as expected by sailing experts.,
A Delay Composition Theorem for Real-Time Pipelines,"Uniprocessor schedulability theory made great strides, in part, due to the simplicity of composing the delay of a job from the execution times of higher-priority jobs that preempt it. In this paper, we bound the end-to-end delay of a job in a multistage pipeline as a function of higher-priority job execution times on different stages. We show that the end-to-end delay is bounded by that of a single virtual ""bottleneck"" stage plus a small additive component. This contribution effectively transforms the pipeline into a single stage system. The wealth of schedulability analysis techniques derived for uniprocessors can then be applied to decide the schedulability of the pipeline. The transformation does not require imposing artitifical per-stage deadlines, but rather models the pipeline as a whole and uses the end-to-end deadlines directly in the single-stage analysis. It also does not make assumptions on job arrival patterns or periodicity and thus can be applied to periodic and aperiodic tasks alike. We show through simulations that this approach outperforms previous pipeline schedulability tests except for very short pipelines or when deadlines are sufficiently large. The reason lies in the way we account for execution overlap among stages. We discuss how previous approaches account for overlap and point out interesting differences that lead to different performance advantages in different cases. We hope that the pipeline delay composition rule, derived in this paper, may be a step towards a general schedulability analysis foundation for large distributed systems.","Pipelines,
Processor scheduling,
Real time systems,
Dynamic scheduling,
Testing,
Distributed computing,
Delay estimation,
Computer science,
Data processing,
Throughput"
Relative Performance of Scheduling Algorithms in Grid Environments,"Effective scheduling is critical for the performance of an application launched onto the Grid environment. Finding effective scheduling algorithms for this problem is a challenging research area. Many scheduling algorithms have been proposed, studied and compared on heterogeneous parallel computers but there are few studies comparing the performance of scheduling algorithms in Grid environments. The Grid is unique because of the drastic cost differences between inter-cluster and the intra-cluster data transfers. In this paper, we compare several scheduling algorithms that represent two classes of schedulers used for Grid computing. We analyze the results to explain how different resource environments and workflow application structures affect the performance of these algorithms. Based on our experiments, we introduce a new measurement called effective aggregated computing power (EACP) that could drastically improve the performance of some schedulers.","Scheduling algorithm,
Processor scheduling,
Grid computing,
Concurrent computing,
Computer science,
Application software,
Costs,
Performance analysis,
Algorithm design and analysis,
Power measurement"
Load prediction using hybrid model for computational grid,"Due to the dynamic nature of grid environments, schedule algorithms always need assistance of a long-time-ahead load prediction to make decisions on how to use grid resources efficiently. In this paper, we present and evaluate a new hybrid model, which predicts the n-step-ahead load status by using interval values. This model integrates autoregressive (AR) model with confidence interval estimations to forecast the future load of a system. Meanwhile, two filtering technologies from signal processing field are also introduced into this model to eliminate data noise and enhance prediction accuracy. The results of experiments conducted on a real grid environment demonstrate that this new model is more capable of predicting n-step-ahead load in a computational grid than previous works. The proposed hybrid model performs well on prediction advance time for up to 50 minutes, with significant less prediction errors than conventional AR model. It also achieves an interval length acceptable for task scheduler.",
A Steganographic Method with High Embedding Capacity by Improving Exploiting Modification Direction,"We propose an effective data hiding method based on the EMD embedding scheme proposed by Zhang et al. in order to enhance the embedding capacity. The main idea of our proposed scheme is to convert secret binary messages into a sequence of digits in an 8-ary notational system. Each secret digit is embedded into two cover pixels by modifying the least significant bits (LSBs). As the experimental results show, the embedding bit-rate of our scheme is 1.5 times better than the EMD embedding scheme. In a word, we propose a high-quality and high-capacity data hiding method that can ensure the stego-image quality and security.","Steganography,
Pixel,
Data encapsulation,
Computer science,
Gray-scale,
PSNR,
Image converters,
Information management,
Chaos,
Electronic mail"
The Titech large vocabulary WFST speech recognition system,"In this paper we present evaluations on the large vocabulary speech decoder we are currently developing at Tokyo Institute of Technology. Our goal is to build a fast, scalable, flexible decoder to operate on weighted finite state transducer (WFST) search spaces. Even though the development of the decoder is still in its infancy we have already implemented a impressive feature set and are achieving good accuracy and speed on a large vocabulary spontaneous speech task. We have developed a technique to allow parts of the decoder to be run on the graphics processor, this can lead to a very significant speed up.","Vocabulary,
Speech recognition,
Decoding,
Space technology,
Graphics,
Natural languages,
Transducers,
Computer science,
Speech analysis,
Parallel programming"
A Statistical Parts-Based Model of Anatomical Variability,"In this paper, we present a statistical parts-based model (PBM) of appearance, applied to the problem of modeling intersubject anatomical variability in magnetic resonance (MR) brain images. In contrast to global image models such as the active appearance model (AAM), the PBM consists of a collection of localized image regions, referred to as parts, whose appearance, geometry and occurrence frequency are quantified statistically. The parts-based approach explicitly addresses the case where one-to-one correspondence does not exist between all subjects in a population due to anatomical differences, as model parts are not required to appear in all subjects. The model is constructed through a fully automatic machine learning algorithm, identifying image patterns that appear with statistical regularity in a large collection of subject images. Parts are represented by generic scale-invariant features, and the model can, therefore, be applied to a wide variety of image domains. Experimentation based on 2-D MR slices shows that a PBM learned from a set of 102 subjects can be robustly fit to 50 new subjects with accuracy comparable to 3 human raters. Additionally, it is shown that unlike global models such as the AAM, PBM fitting is stable in the presence of unexpected, local perturbation",
Routing Bandwidth-Guaranteed Paths in MPLS Traffic Engineering: A Multiple Race Track Learning Approach,"This paper presents an efficient adaptive online routing algorithm for the computation of bandwidth-guaranteed paths in multiprotocol label switching witching (MPLS)-based networks by using a learning scheme that computes an optimal ordering of routes. The contribution of this work is twofold. The first is that we propose a new class of solutions other than those available in the literature, incorporating the family of stochastic random races (RR) algorithms. The most popular previously proposed MPLS-based traffic engineering (TE) solutions attempt to find a superior path to route an incoming setup request. Our algorithm, on the other hand, tries to learn an optimal ordering of the paths through which requests can be routed according to the rank of the paths in the order learned by the algorithm. The second contribution of our work is that we have proposed a routing algorithm that has a performance superior to the important algorithms in the literature. Our conclusions are based on three important performance criteria: 1) the rejection ratio, 2) the percentage of accepted bandwidth, and 3) the average route computation time per request. Although some of the previously proposed algorithms were designed to achieve low rejection and high throughput of route requests, they are unreasonably slow. Our algorithm, on the other hand, in general attempts to reject the least number of requests, achieves the highest throughput, and computes routes in the fastest possible time when compared to the algorithms that we used as benchmarks for comparison.","Routing,
Multiprotocol label switching,
Bandwidth,
Routing protocols,
Heuristic algorithms,
Learning automata,
Quality of service"
EnviroStore: A Cooperative Storage System for Disconnected Operation in Sensor Networks,"This paper presents a new cooperative storage system for sensor networks geared for disconnected operation (where sensor nodes do not have a connected path to a basestation). The goal of the system is to maximize its data storage capacity by appropriately distributing storage utilization and opportunistically offloading data to external devices when possible. The system is motivated by the observation that a large category of sensor network applications, such as environmental data logging, does not require real-time data access. Such networks generally operate in a disconnected mode. Rather than focusing on multihop routing to a basestation, an important concern becomes (i) to maximize the effective storage capacity of the disconnected sensor network such that it accommodates the most data, and (ii) to take the best advantage of data upload opportunities when they become available to relieve network storage. The storage system described in this paper achieves the above goals, leading to significant improvements in the amount of data collected compared to non-cooperative storage. It is implemented in nesC for TinyOS and evaluated in TOSSIM through various application scenarios.","Sensor systems,
Flash memory,
Computer science,
Sensor systems and applications,
Capacitive sensors,
Springs,
Energy consumption,
Communications Society,
Peer to peer computing,
Real time systems"
Fault Tolerance Analysis of NoC Architectures,"The paper presents an approach for analyzing and improving fault tolerance aspects in NoC architectures. This is a necessary step to be taken in order to implement reliable systems in future nanoscale technologies. Several NoC architectures and the router structures as well as the network interface needed for them are presented and compared for their fault tolerance, area and performance. The results indicate that a network structure built from simple 3-port routers provides better fault tolerance than a structure based on more complex multiport routers, and that the area overhead can be kept moderate","Fault tolerance,
Network-on-a-chip,
Network interfaces,
Circuit faults,
Clocks,
Throughput,
Delay,
Power system reliability,
Routing,
Information analysis"
Carbon Nanotube/Copper Composites for Via Filling and Thermal Management,"With excellent current carrying capacity and extremely high thermal conductivity, carbon nanotube (CNT) has been proposed for interconnect and thermal interface material (TIM) applications. In this paper, we present a method of fabricating aligned CNT/copper composites on the silicon substrates and in the silicon dioxide vias. Electrical measurement of the CNT/copper composite vias demonstrates much lower electrical resistance than that of vias with CNT only. Thermal characterization shows the thermal resistance decreased by increasing copper loading into the CNT films. The electroplated copper fills the voids between the neighboring nanotubes. The improvement of the electrical and thermal conductance is resulted from the decreased porosity of the as-grown CNTs. The copper filling increases the contact area between the one-dimensional nanotube and the three-dimensional electrode or heat collector. This mechanically more robust material can sustain more rigorous electrical or thermal stressing cycling. Our results make CNT a step closer to the practical application of CNTs for the on-chip interconnections and thermal management.","Thermal management,
Carbon nanotubes,
Copper,
Filling,
Thermal conductivity,
Thermal resistance,
Conducting materials,
Electric resistance,
Thermal loading,
Organic materials"
Ninja on a Plane: Automatic Discovery of Physical Planes for Augmented Reality Using Visual SLAM,"Most work in visual augmented reality (AR) employs predefined markers or models that simplify the algorithms needed for sensor positioning and augmentation but at the cost of imposing restrictions on the areas of operation and on interactivity. This paper presents a simple game in which an AR agent has to navigate using real planar surfaces on objects that are dynamically added to an unprepared environment. An extended Kalman filter (EKF) simultaneous localisation and mapping (SLAM) framework with automatic plane discovery is used to enable the player to interactively build a structured map of the game environment using a single, agile camera. By using SLAM, we are able to achieve real-time interactivity and maintain rigorous estimates of the system's uncertainty, which enables the effects of high quality estimates to be propagated to other features (points and planes) even if they are outside the camera's current field of view.","Augmented reality,
Simultaneous localization and mapping,
Cameras,
Sensor systems,
Real time systems,
Uncertainty,
Layout,
Surface fitting,
Robustness,
Interactive systems"
Online Competitions: An Open Space to Improve the Learning Process,"Experimentation and practical work, which are usually accomplished in a laboratory, are the basics of technological fields. Laboratory activities enable students to acquire methodologies, work habits, knowledge, and experience of equipment operation, in conditions as near as possible to their future professional activities. The evolution of communication and information technologies opens new possibilities in educational methods. This article describes a project that aims to facilitate the use of real robots in an educational laboratory via Web, allowing users to learn different robotics aspects while performing a competition. Students can remotely program several robots to participate in games to accomplish a set of goals in a remote stadium (the RoboStadium). To facilitate the use of robots, the online robot stadium provides a set of training resources. Having these resources, previous knowledge on robotics is not required to use the system. Since robotics is a multidisciplinary field (mechanics, electronics, control, mathematics, computers, etc.), students of different degrees can take advantage of the presented system. Researchers of two Spanish universities are participating in this project, which provides robotics telelaboratories via Web.","Laboratories,
Educational robots,
Educational institutions,
Computer science education,
Educational programs,
Space technology,
Professional activities,
Control engineering computing,
Computer science,
Learning systems"
Distributed pattern matching: a key to flexible and efficient P2P search,"Flexibility and efficiency are the prime requirements for any P2P search mechanism. Existing P2P systems do not provide satisfactory solution for achieving these two conflicting goals. Unstructured search protocols (as adopted in Gnutella and FastTrack) provide search flexibility but exhibit poor performance characteristics. Structured search techniques (mostly Distributed Hash Table (DHT)-based), on the other hand, can efficiently route queries but support exact-match semantic only. In this paper we have defined Distributed Pattern Matching (DPM) problem and have presented a novel P2P architecture, named Distributed Pattern Matching System (DPMS), as a solution. Possible application areas of DPM include P2P search, service discovery and P2P databases. In DPMS, advertised patterns are replicated and aggregated by the peers, organized in a lattice-like hierarchy. Replication Improves availability and resilience to peer failure, and aggregation reduces storage overhead. An advertised pattern can be discovered using any subset of its 1-bits. Search complexity in DPMS is logarithmic to the total number of peers in the system. Advertisement overhead and guarantee on search completeness is comparable to that of DHT-based systems. We have presented mathematical analysis and simulation results to demonstrate the effectiveness of DPMS.","Pattern matching,
Filters,
Mathematical analysis,
Analytical models,
Computer science,
Protocols,
Databases,
Resilience,
Computational modeling"
A Homographic Framework for the Fusion of Multi-view Silhouettes,"This paper presents a purely image-based approach to fusing foreground silhouette information from multiple arbitrary views. Our approach does not require 3D constructs like camera calibration to carve out 3D voxels or project visual cones in 3D space. Using planar homographies and foreground likelihood information from a set of arbitrary views, we show that visual hull intersection can be performed in the image plane without requiring to go in 3D space. This process delivers a 2D grid of object occupancy likelihoods representing a cross-sectional slice of the object. Subsequent slices of the object are obtained by extending the process to planes parallel to a reference plane in a direction along the body of the object. We show that homographies of these new planes between views can be computed in the framework of plane to plane homologies using the homography induced by a reference plane and the vanishing point of the reference direction. Occupancy grids are stacked on top of each other, creating a three dimensional data structure that encapsulates the object shape and location. Object structure is finally segmented out by minimizing an energy functional over the surface of the object in a level sets formulation. We show the application of our method on complicated object shapes as well as cluttered environments containing multiple objects.","Calibration,
Cameras,
Layout,
Shape,
Object recognition,
Image reconstruction,
Computer science,
Data structures,
Level set,
Fusion power generation"
Multi-hop-based Monte Carlo Localization for Mobile Sensor Networks,"Many low-cost localization techniques have been proposed for wireless sensor networks. However, few consider the mobility of networked sensors. In this paper, we propose an effective and practical localization technique especially designed for mobile sensor networks. Our system is based on the sequential Monte Carlo method, but dissimilar to other conventional localization schemes, our algorithm covers a large sensor field with very few anchor nodes by information flooding. The algorithm works without knowledge of the maximum transmission range, and covers some of the problems caused by the flooding beacons. We discuss factors to implement the algorithm in the real world and present several solutions. Our mechanism is implemented in a real environment, and its feasibility is validated by experiments. The simulation results show that our algorithm outperforms conventional Monte Carlo localization schemes by decreasing estimation errors by up to 50%, and the overhead of the algorithm could be minimized by appropriately adjusting the system parameters.","Monte Carlo methods,
Peer to peer computing,
Wireless sensor networks,
Mobile computing,
Sensor systems,
Floods,
Sensor systems and applications,
Global Positioning System,
Costs,
Computer science"
Multithreaded SAT Solving,"This paper describes the multithreaded MiraXT SAT solver which was designed to take advantage of current and future shared memory multiprocessor systems. The paper highlights design and implementation details that allow the multiple threads to run and cooperate efficiently. Results show that in single threaded mode, MiraXT compares well to other state of the art solvers on industrial problems. In threaded mode, it provides cutting edge performance, as speedup is obtained on both SAT and UNSAT instances.","Yarn,
Business continuity,
Multiprocessing systems,
Message passing,
Computer science,
Hardware,
Manufacturing,
Multicore processing,
Packaging,
Sun"
Augmented Reality-based factory planning - an application tailored to industrial needs,"Over the past years, a variety of augmented reality (AR)-based applications were created, to support industrial processes. Although these first demonstrator applications or prototypes cover all parts of the industrial product process (design, planning and production, service and maintenance), only a few of them actually turned into established and applied solutions. In the automotive industry, one successful field for AR-based applications is factory design and planning, metaio and Volkswagen Group Research have concerted their research work to continuously enhance a first prototype in order to create a serviceable and accepted software tool, which is tailored to the needs of AR-based factory planning processes. This paper presents the developing process of this application, the requirements analysis and the realization as a complete system. A concrete example in industry describes the usage of the system for factory planning and an outlook on future work in this area closes the document.","Production facilities,
Process planning,
Application software,
Augmented reality,
Prototypes,
Process design,
Production planning,
Automotive engineering,
Computer industry,
Software prototyping"
Peer-to-Peer Multimedia Streaming Using BitTorrent,"We propose a peer-to-peer multimedia streaming solution based on the BitTorrent content-distribution protocol. Our proposal includes two modifications that allow it to deliver multimedia data on time. First, we replace the rarest-first chunk downloading policy of BitTorrent by a policy requiring peers to download first the chunks that they will watch in the near future. Second, we introduce a new randomized tit-for-tat peer selection policy that gives free tries to a larger number of peers and lets them participate sooner in the media distribution. Our simulations indicate that both changes are required to achieve a good streaming quality.","Streaming media,
Peer to peer computing,
Protocols,
Internet,
Large-scale systems,
Bandwidth,
Network servers,
Computer science,
Proposals,
Watches"
The Information Bottleneck Revisited or How to Choose a Good Distortion Measure,It is well-known that the information bottleneck method and rate distortion theory are related. Here it is described how the information bottleneck can be considered as rate distortion theory for a family of probability measures where information divergence is used as distortion measure. It is shown that the information bottleneck method has some properties that are not shared with rate distortion theory based on any other divergence measure. In this sense the information bottleneck method is unique.,"Distortion measurement,
Rate distortion theory,
Rate-distortion,
Computer science,
Loss measurement,
Information retrieval,
Time measurement,
Random variables,
Kernel"
Phase Based Modelling of Dynamic Textures,"This paper presents a model of spatiotemporal variations in a dynamic texture (DT) sequence. Most recent work on DT modelling represents images in a DT sequence as the responses of a linear dynamical system (LDS) to noise. Despite its merits, this model has limitations because it attempts to model temporal variations in pixel intensities which do not take advantage of global motion coherence. We propose a model that relates texture dynamics to the variation of the Fourier phase, which captures the relationships among the motions of all pixels (i.e. global motion) within the texture, as well as the appearance of the texture. Unlike LDS, our model does not require segmentation or cropping during the training stage, which allows it to handle DT sequences containing a static background. We test the performance of this model on recognition and synthesis of DT's. Experiments with a dataset that we have compiled demonstrate that our phase based model outperforms LDS.","Spatiotemporal phenomena,
Optical noise,
Fires,
Clouds,
Motion analysis,
Optical computing,
Image motion analysis,
Motion estimation,
Physics computing,
Coherence"
Fast Predictions of Variance Images for Fan-Beam Transmission Tomography With Quadratic Regularization,"Accurate predictions of image variances can be useful for reconstruction algorithm analysis and for the design of regularization methods. Computing the predicted variance at every pixel using matrix-based approximations is impractical. Even most recently adopted methods that are based on local discrete Fourier approximations are impractical since they would require a forward and backprojection and two fast Fourier transform (FFT) calculations for every pixel, particularly for shift-variant systems like fan-beam tomography. This paper describes new ""analytical"" approaches to predicting the approximate variance maps of 2-D images that are reconstructed by penalized-likelihood estimation with quadratic regularization in fan-beam geometries. The simplest of the proposed analytical approaches requires computation equivalent to one backprojection and some summations, so it is computationally practical even for the data sizes in X-ray computed tomography (CT). Simulation results show that it gives accurate predictions of the variance maps. The parallel-beam geometry is a simple special case of the fan-beam analysis. The analysis is also applicable to 2-D positron emission tomography (PET)","Analysis of variance,
Image analysis,
Algorithm design and analysis,
Geometry,
X-ray imaging,
Computed tomography,
Positron emission tomography,
Reconstruction algorithms,
Design methodology,
Fast Fourier transforms"
Binary Matrix Factorization with Applications,"An interesting problem in nonnegative matrix factorization (NMF) is to factorize the matrix X which is of some specific class, for example, binary matrix. In this paper, we extend the standard NMF to binary matrix factorization (BMF for short): given a binary matrix X, we want to factorize X into two binary matrices W, H (thus conserving the most important integer property of the objective matrix X) satisfying X ap WH. Two algorithms are studied and compared. These methods rely on a fundamental boundedness property of NMF which we propose and prove. This new property also provides a natural normalization scheme that eliminates the bias of factor matrices. Experiments on both synthetic and real world datasets are conducted to show the competency and effectiveness of BMF.","Matrix decomposition,
Data mining,
Computer science,
USA Councils,
Clustering algorithms,
Application software,
Data analysis,
DNA,
Proteins,
Machine learning"
PABC: Power-Aware Buffer Cache Management for Low Power Consumption,"Power consumed by memory systems becomes a serious issue as the size of the memory installed increases. With various low power modes that can be applied to each memory unit, the operating system can reduce the number of active memory units by collocating active pages onto a few memory units. This paper presents a memory management scheme based on this observation, which differs from other approaches in that all of the memory space is considered, while previous methods deal only with pages mapped to user address spaces. The buffer cache usually takes more than half of the total memory and the pages access patterns are different from those in user address spaces. Based on an analysis of buffer cache behavior and its interaction with the user space, our scheme achieves up to 63 percent more power reduction. Migrating a page to a different memory unit increases memory latencies, but it is shown to reduce the power consumed by an additional 4.4 percent","cache storage,
operating systems (computers),
power aware computing"
Modeling path capacity in multi-hop IEEE 802.11 networks for QoS services,"QoS provisioning in multi-hop IEEE 802.11 networks is very challenging due to the interference nature of wireless medium and the contention-based behavior among neighboring nodes. In such networks, one of the key questions for QoS support is: given a specific topology and traffic condition, how much bandwidth can be utilized along a path in the network without violating QoS demand of existing traffic? Considering that in general QoS-sensitive traffic has the well-controlled sending rate, one key observation is that the network unsaturated condition should be considered. Another observation is that, not only the interaction between the new traffic and the existing ones that can be sensed (by the new one), but also the interaction between the new traffic and the traffic that is hidden but can have influence upon the new one should be studied. Based upon the above observations, we propose an analytical model for multi-hop IEEE 802.11 networks to calculate how much bandwidth can be utilized along a path without violating the QoS requirements of existing traffic. A notion, ""free channel time"", which is the time allowed for a wireless link to transmit data, is introduced to analyze the path capacity. Simulation results demonstrate that our proposed analytical model can accurately predict the path capacity under various network conditions without breaking QoS demands of all existing traffic","Spread spectrum communication,
Telecommunication traffic,
Traffic control,
Interference,
Communication system traffic control,
Streaming media,
Analytical models,
Computer science,
Bandwidth,
Wireless mesh networks"
Focal waveform of a prolate-spheroidal impulse-radiating antenna,This paper develops some analytical approximations for the transient focal waveform produced at the second focus of a prolate-spheroidal reflector due to a pulse TEM wave launched from the first focus. This is extended to consider the spot size of the peak field near the second focus.,"Apertures,
Electric fields,
Conductors,
Geometry,
Wires,
Reflector antennas"
"Joint Consideration of Fault-Tolerance, Energy-Efficiency and Performance in On-Chip Networks","High reliability against noise, low energy consumption and high performance are key objectives in the design of on-chip networks. Recently some researchers have considered the various trade-offs between two of these objectives. However, as we will argue later, the three design objectives should be considered jointly and simultaneously. The first aim of this paper is to analyze the impact of various error-control schemes on the simultaneous trade-off between reliability, performance and energy when voltage swing varies. We provide a detailed comparative analysis of the error-control schemes using analytical models and SPICE simulations. The second aim of this paper is to analyze the impact of noise power and time constraint on the effectiveness of error-control schemes, which have not been addressed in previous studies","Fault tolerance,
Energy efficiency,
Network-on-a-chip,
Analytical models,
Energy consumption,
Performance analysis,
Voltage,
Error analysis,
SPICE,
Time factors"
Many-to-Many Aggregation for Sensor Networks,"Wireless sensor networks have enormous potential to aid data collection in a number of areas, such as environmental and wildlife research. In this paper, we address the challenges of supporting many-to-many aggregation in a sensor network. An application of many-to-many aggregation is in-network control of sensors. For expensive sensing tasks such as sap flux measurements and camera repositioning, we use low-cost information obtained at multiple other nodes in the network to control such tasks, e.g., decreasing sampling rates when readings are predictable or unimportant, while increasing sampling rates when there are interesting activities. In general, there is a many-to-many relationship between sources (nodes providing control inputs) and destinations (nodes requiring control outputs). We present a method for implementing many-to-many aggregation in a sensor network that minimizes the communication cost by optimally balancing a combination of multicast and in-network aggregation. Our optimization technique is efficient in finding the initial solution and handling dynamic updates.","Communication system control,
Sampling methods,
Intelligent sensors,
Wireless sensor networks,
Cameras,
Temperature sensors,
Aggregates,
Wildlife,
Cost function,
Fluid flow measurement"
HealthLine: Speech-based access to health information by low-literate users,"Health information access by low-literate community health workers is a pressing need of community health programs across the developing world. We present results from a needs assessment we conducted to understand the health information access practices and needs of various types of health workers in Pakistan. We also present a prototype for speech-based health information access, as well as discuss our experiences from a pilot study involving its use by community health workers in a rural health center.","Prototypes,
Computer science,
Electronic mail,
Pediatrics,
Costs,
Natural languages,
Medical services,
Telephony,
Pressing,
Government"
Statistical Disk Cluster Classification for File Carving,"File carving is the process of recovering files from a disk without the help of a file system. In forensics, it is a helpful tool in finding hidden or recently removed disk content. Known signatures in file headers and footers are especially useful in carving such files out, that is, from header until footer. However, this approach assumes that file clusters remain in order. In case of file fragmentation, file clusters can be disconnected and the order can even be disrupted such that straighforward carving will fail. In this paper, we focus on methods for classifying clusters into file types by using the statistics of the clusters. By not exploiting the possible embedded signatures, we generate evidence from a different source that can be integrated later on. We propose a set of characteristic features and use statistical pattern recognition to learn a supervised classification model for a range of relevant file types. We exploit the statistics of a restricted number of neighboring clusters (context) to improve classification performance. In the experiments we show that the proposed features indeed enable the differentation of clusters into file types. Moreover, for some file types the incorporation of cluster context improves the recognition performance significantly.","File systems,
Forensics,
Statistics,
Computer security,
Information security,
Intelligent systems,
Computer science,
Biometrics,
Pattern recognition,
Out of order"
A Purpose-Based Access Control Model,"Achieving privacy preservation in a data-sharing computing environment is a challenging problem. The requirements for a privacy preserving data access policy should be formally specified in order to be able to establish consistency between the privacy policy and its purported implementation in practice. Previous work has shown that when specifying a privacy policy, the notion of purpose should be used as the basis for access control. A privacy policy should ensure that data can only be used for its intended purpose, and the access purpose should be compliant with the data's intended purpose. This paper presents a mechanism to specify privacy policy using VDM. The entities in the purpose-based access control model are specified, the invariants corresponding to the privacy requirements in privacy policy are specified, and the operations in the model and their proof obligations are defined and investigated.","Access control,
Data privacy,
Protection,
Data security,
Authorization,
Computer security,
Information security,
Computer science,
Information technology,
Information systems"
Accounting for Cache-Related Preemption Delay in Dynamic Priority Schedulability Analysis,"Recently there has been considerable interest in incorporating timing effects of micro architectural features of processors (e.g. caches and pipelines) into the schedulability analysis of tasks running on them. Following this line of work, in this paper the authors show how to account for the effects of cache-related preemption delay (CRPD) in the standard schedulability tests for dynamic priority schedulers like EDF. Even if the memory space of tasks is disjoint, their memory blocks usually map into a shared cache. As a result, task preemption may introduce additional cache misses which are encountered when the preempted task resumes execution; the delay due to these additional misses is called CRPD. Previous work on accounting for CRPD was restricted to only static priority schedulers and periodic task models. Our work extends these results to dynamic priority schedulers and more general task models (e.g. sporadic, generalized multiframe and recurring real-time). The authors show that the schedulability tests are useful through extensive experiments using synthetic task sets, as well as through a detailed case study","Dynamic scheduling,
Testing,
Processor scheduling,
Delay effects,
Timing,
Vehicle dynamics,
Microarchitecture,
Pipelines,
Resumes,
Performance analysis"
Simulating Future Changes in Arctic and Subarctic Vegetation,"The arctic is a sensitive system undergoing dramatic changes related to recent warming trends. Vegetation dynamics - increases in the quantity of green vegetation and a northward migration of trees into the arctic tundra - are a component of this change. Although field studies over long time periods can be logistically problematic, simulation modeling provides a means for projecting changes in arctic and subarctic vegetation caused by environmental variations.","Arctic,
Vegetation mapping,
Productivity,
Biomass,
Soil,
Computational modeling,
Atmospheric modeling,
Ecosystems,
Sea ice,
Satellites"
Ordered CSMA: a collision-free MAC protocol for underwater acoustic networks,"Since underwater acoustic (UWA) networks have the nature of long propagation delay, low bit rates and error-prone acoustic communication, protocols designed for underwater acoustic networks are significantly different from that of terrestrial radio networks. Limited by these nature of UWA channels, conventional medium access control (MAC) protocols of radio packet network ether have low efficiency or are not able to apply to underwater acoustic networks. It is necessary to develop an efficient MAC protocol for underwater acoustic networks. In this paper, a collision-free MAC protocol for UWA networks called Ordered Carrier Sense Multiple Access (Ordered CSMA) is proposed and analyzed. Ordered CSMA combines the concepts of round-robin scheduling and CSMA. In Ordered CSMA, each station transmits data frame in a fixed order. More specifically, each station transmits immediately after the data frame transmission of last station in the order, instead of waiting for a period of maximum propagation delay. To achieve this, each station is constantly sensing the carrier and listens to all received frames. Due to the characteristics of collision free and high channel utilization, Ordered CSMA shows a great MAC efficiency improvement in our simulations, compared to previous works.","Multiaccess communication,
Media Access Protocol,
Underwater acoustics,
Access protocols,
Time division multiple access,
Propagation delay,
Radio network,
Underwater communication,
Optical attenuators,
Optical scattering"
Process-Independent Resistor Temperature-Coefficients using Series/Parallel and Parallel/Series Composite Resistors,"This paper introduces series/parallel and parallel/series composite resistor topologies. These topologies allow one to design a temperature coefficient that is insensitive to process variations, unlike the traditional simple series and simple parallel composite resistors. Formulas and normalized circuits are provided. Four zero-temperature coefficient resistors are simulated, and methods for picking the best topology are discussed.","Resistors,
Temperature dependence,
Circuit topology,
Moon,
Computer science,
Circuit simulation,
Taylor series,
CMOS process,
Forward contracts"
Texture Anisotropy of the Brain's White Matter as Revealed by Anatomical MRI,"The purpose of this work was to study specific texture properties of the brain's white matter (WM) based on conventional high-resolution T 1-weighted magnetic resonance imaging (MRI) datasets. Quantitative parameters anisotropy and laminarity were derived from 3-D texture analysis. Differences in WM texture associated with gender were evaluated on an age-matched sample of 210 young healthy subjects (mean age 24.8, SD 3.97 years, 103males and 107 females). Changes of WM texture with age were studied using 112 MRI-T1 datasets of healthy subjects aged 16 to 70years (57 males and 55 females). Both texture measures indicated a ""more regular"" WM structure in females (p<10-6). An age-related deterioration of WM structure manifests itself as a remarkable decline of both parameters (p<10-6) that is more prominent in females (p<10-6 ) than in males (p=0.02). Texture analysis of anatomical MRI-T1 brain datasets provides quantitative information about macroscopic WM characteristics and helps discriminating between normal and pathological aging","Anisotropic magnetoresistance,
Magnetic resonance imaging,
Aging,
Pathology,
Image texture analysis,
Magnetic analysis,
Image analysis,
Computer vision,
Magnetic properties,
Information analysis"
DRESR: Dynamic Routing in Enterprise Service Bus,"Enterprise service bus (ESB) provides the infrastructure services for message exchange and routing in service-oriented architecture (SOA). Dynamic routing is necessary to support dynamic service selection and composition. However, current ESB software can only support fixed routing path by static configuration files and/or hard-coded rules. This paper proposes DRESR (dynamic reconfigurable ESB service routing) framework to enable the dynamic routing path construction and message routing. DRESER defines the mechanisms for routing path abstraction, instantiation, and (re)configuration. It supports the specification of service selection preferences and facilitates the service selection based on runtime testing results.","Service oriented architecture,
Runtime environment,
Testing,
Topology,
Computer science,
Navigation,
Routing protocols,
Collaboration,
Delay,
Availability"
Optimizing the Internet Gateway Deployment in a Wireless Mesh Network,"In a wireless mesh network (WMN), mesh routers (MRs) are interconnected by wireless links and form a wireless backbone to provide ubiquitous high-speed Internet connectivity for mobile clients. The wireless backbone is tightly integrated with the Internet by nodes called as Internet gateways (IGWs). In this paper, we address the IGW deployment problem which is shown to be NP-hard. We first formulate it as a linear program (LP) issue, then develop two heuristic algorithms for the purpose of cost-efficient IGW deployment. We further compare our algorithms with a leading approach by extensive simulations. Our analysis shows the effectiveness of the proposed algorithms.","IP networks,
Wireless mesh networks,
Internet,
Spine,
Throughput,
Mobile computing,
Protocols,
Telecommunication traffic,
Tree graphs,
Spread spectrum communication"
"Isotropy, Reciprocity and the Generalized Bas-Relief Ambiguity","A set of images of a Lambertian surface under varying lighting directions defines its shape up to a three-parameter generalized bas-relief (GBR) ambiguity. In this paper, we examine this ambiguity in the context of surfaces having an additive non-Lambertian reflectance component, and we show that the GBR ambiguity is resolved by any non-Lambertian reflectance function that is isotropic and spatially invariant. The key observation is that each point on a curved surface under directional illumination is a member of a family of points that are in isotropic or reciprocal configurations. We show that the GBR can be resolved in closed form by identifying members of these families in two or more images. Based on this idea, we present an algorithm for recovering full Euclidean geometry from a set of uncalibrated photometric stereo images, and we evaluate it empirically on a number of examples.","Reflectivity,
Surface reconstruction,
Stereo vision,
Shape,
Spatial resolution,
Lighting,
Photometry,
Computer vision,
Stereo image processing,
Image reconstruction"
A Secure Group Agreement (SGA) Protocol for Peer-to-Peer Applications,"The lack of a trusted central authority poses a unique security challenge to peer-to-peer networks. It must be assumed that some fraction of all peers in a network are corrupt and may collude to try to derive an advantage. Nonetheless, in some circumstances it is necessary to select a subset of the peer-to-peer network in such a way that all members of the selected group can be confident that most group members are honest. We propose a secure protocol for the selection of a subset of peers from the network without a trusted authority. Our protocol ensures, with any desired probability, that the percentage of corrupt members in the subset is no greater than a selected limit (up to the total percentage of corrupt peers). We then discuss the use of this protocol in the context of a peer-to-peer game.","Protocols,
Peer to peer computing,
Voting,
Internet,
Application software,
Computer science,
Software engineering,
Computer security,
National security,
IP networks"
Detecting Malicious Behavior in Cooperative Diversity,"We consider a cooperative diversity scheme where a relay cooperatively enhances communication between a source and destination. In cooperative diversity, due to lack of a mechanism to ensure relay's adherence to the cooperation strategy, the receiver is often assumed to be passive. In this paper, we consider a smart destination which examines relay's signal prior to applying diversity combining. This is attributed to the assumption that relay may not conform to the cooperation strategies at all times and may behave maliciously. Based on this assumption, we develop a statistical detection technique to mitigate malicious relay behavior in decode-and-forward cooperation strategy. The detection technique statistically compares the signals received from the two diversity branches to determine the relay's behavior. As the uncertainty in the direct path is only due to the channel, correlation of received signals from the source and relay provides a basis to characterize relay behavior. We show, both by analysis and simulation, that a malicious relay reduces the correlation between the received signals in the diversity branch. Finally, we investigate bit-error rate and outage behavior performance in the presence of a smart destination.","Relays,
Diversity reception,
Decoding,
Protocols,
Communication system security,
Uncertainty,
Bit error rate,
Fading,
Antenna arrays,
Quality of service"
2D Barcode and Augmented Reality Supported English Learning System,"This study aims to construct a 2D barcode handheld augmented reality supported learning system called HELLO (handheld english language learning organization), to improve students' English level. The HELLO integrates the 2D barcodes, the Internet, augmented reality, mobile computing and database technologies. The proposed system consists of two subsystems: an English learning management system and a mobile learning tools system. A four-week pilot study and questionnaire survey were conducted in college to evaluate effects of proposed learning system and student learning attitudes. Furthermore, the evaluation results indicate that 2D barcodes and augmented reality technology are useful for English learning.","Augmented reality,
Learning systems,
Mobile computing,
Games,
Natural languages,
Educational institutions,
Electronic learning,
Radiofrequency identification,
Application software,
Internet"
Intrusion Detection Model Based On Particle Swarm Optimization and Support Vector Machine,"Advance in information and communication technologies, force us to keep most of the information electronically, consequently, the security of information has become a fundamental issue. The traditional intrusion detection systems look for unusual or suspicious activity, such as patterns of network traffic that are likely indicators of unauthorized activity. However, normal operation often produces traffic that matches likely ""attack signature"", resulting in false alarms. One main drawback is the inability of detecting new attacks which do not have known signatures. In this paper particle swarm optimization (PSO) is used to implement a feature selection, and support vector machine (SVMs) with the one-versus-rest method serve as a fitness function of PSO for classification problems from the literature. Experimental result shows that our method allows us to recognize not only known attacks but also to detect suspicious activity that may be the result of a new, unknown attack. Our method simplifies features effectively and obtains a higher classification accuracy compared to other methods","Intrusion detection,
Particle swarm optimization,
Support vector machines,
Information security,
Support vector machine classification,
Telecommunication traffic,
Information systems,
Computer security,
Computer networks,
Data mining"
'Good' Organisational Reasons for 'Bad' Software Testing: An Ethnographic Study of Testing in a Small Software Company,"In this paper we report on an ethnographic study of a small software house to discuss the practical work of software testing. Through use of two rich descriptions, we discuss that 'rigour' in systems integration testing necessarily has to be organisationally defined. Getting requirements 'right', defining 'good' test scenarios and ensuring 'proper' test coverage are activities that need to be pragmatically achieved taking account of organisational realities and constraints such as: the dynamics of customer relationships; using limited effort in an effective way; timing software releases; and creating a market. We discuss how these organisational realities shape (1) requirements testing; (2) test coverage; (3) test automation; and (4) test scenario design.",
The PBD-Closure of Constant-Composition Codes,"We show an interesting pairwise balanced design (PBD)-closure result for the set of lengths of constant-composition codes whose distance and size meet certain conditions. A consequence of this PBD-closure result is that the size of optimal constant-composition codes can be determined for infinite families of parameter sets from just a single example of an optimal code. As an application, the sizes of several infinite families of optimal constant-composition codes are derived. In particular, the problem of determining the size of optimal constant-composition codes having distance four and weight three is solved for all lengths sufficiently large. This problem was previously unresolved for odd lengths, except for lengths seven and eleven.",
Open-ZB: an open-source implementation of the IEEE 802.15.4/ZigBee protocol stack on TinyOS,"The IEEE 802.15.4/ZigBee protocols are gaining increasing interests in both research and industrial communities as candidate technologies for Wireless Sensor Network (WSN) applications. In this paper, we present an open-source implementation of the IEEE 802.15.4/ZigBee protocol stack under the TinyOS operating system for the MICAz and TelosB motes. This work has been driven by the need for an open- source implementation of the IEEE 802.15.4/ZigBee protocols, filling a gap between some newly released complex C implementations and black-box implementations from different manufacturers. In addition, we share our experience on the challenging problems that we have faced during the implementation of the protocol stack. We strongly believe that this open-source implementation will potentiate research works on the IEEE 802.15.4/ZigBee protocols, allowing their demonstration and validation through experimentation.","Open source software,
ZigBee,
Wireless sensor networks,
Wireless application protocol,
Access protocols,
Operating systems,
Standards development,
Computational modeling,
Computer science,
Computer industry"
A Measurement of a large-scale Peer-to-Peer Live Video Streaming System,"Peer-to-peer (P2P) technologies have found much success in applications like file distributions, and its adoption in live video streaming has recently attracted significant attentions. With the emerge of commercial P2P streaming systems that are orders of magnitude larger than the academic systems, understanding its basic principles and limitations are important in the design of future systems. Coolstreaming represented one of the earliest large-scale live streaming trials in the Internet. In this paper, we discuss the fundamental components of the system. By leveraging the recent results obtained from live event broadcast, we develop some basis to demonstrate that a random partnership selection has the potentially to scale. Specifically, first, we examine the overlay topology. Second, using a combination of real traces and analysis, we present the highly skewed distribution of peer contribution; a small fraction of peers contribute most of the upload capacity. Third, we discuss the main limitations and the implications on the scalability.",
Reversible data hiding scheme using two steganographic images,,"Data encapsulation,
Steganography,
Image coding,
Data mining,
Robustness,
Discrete wavelet transforms,
Computer science,
Decoding,
Pixel,
Discrete cosine transforms"
Achievable Rates for Network Coding on the Exchange Channel,"Network coding, where relay nodes combine the information received from multiple links rather than simply replicating and forwarding the received packets, has shown the promise of significantly improving system performance. In very recent works, multiple researchers have presented methods for increasing system throughput by employing network coding inspired methods to mix packets at the physical layer: physical-layer network coding (PNC). A common example used to validate much of this work is that of two sources exchanging information through a single intervening relay - a situation that we denote the ""exchange channel"". In this paper, achievable rates of various schemes on the exchange channel are considered. Achievable rates for traditional multi-hop routing approaches, network coding approaches, and various PNC approaches are considered. A new method of PNC inspired by Tomlinson-Harashima precoding (THP), where a modulo operation is used to control the power at the relay, is introduced, and shown to have a slight advantage over analogous schemes at high signal-to-noise ratios (SNRs).","Network coding,
Relays,
Wireless networks,
Physical layer,
Routing,
US Government,
Computer networks,
Educational institutions,
Computer science,
System performance"
Providing Anonymity in Wireless Sensor Networks,"Sensor networks are often used to monitor sensitive information from the environment or track sensitive objects' movements. Anonymity has become an important problem in sensor networks, and has been widely researched in wireless ad hoc and wired networks. The limited capacity and resources of current sensor networks have brought new challenges to anonymity research. In this paper, two efficient methods are proposed based on using a one-way hash chain to dynamically change the identity of sensor nodes in order to provide anonymity, and their anonymity properties are analyzed and compared.","Wireless sensor networks,
Protection,
Base stations,
Telecommunication traffic,
Cryptography,
Educational institutions,
Routing protocols,
Monitoring,
Tracking,
Capacitive sensors"
"Grid-Oriented Storage: A Single-Image, Cross-Domain, High-Bandwidth Architecture","This paper describes the grid-oriented storage (GOS) architecture and its implementations. A GOS-specific file system (GOS-FS), the single-purpose intent of a GOS OS, and secure interfaces via grid security infrastructure (GSI) motivate and enable this new architecture. As an FTP server, GOS with a slimmed OS, with a total volume of around 150 MB, outperforms the standard GridFTP by 20-40 percent. As a file server, GOS-FS acts as a network/grid interface, enabling a user to perform searches and access resources without downloading them locally. In the real-world tests between Cambridge and Beijing, where the transfer distance is 10,000 km, the multistreamed GOS-FS file opening/saving resulted in a remarkable performance increase of about 2-25 times, compared to the single-streamed network file system (NFSv4). GOS is expected to be a variant of or successor to the well-used network-attached storage (NAS) and/or storage area network (SAN) products in the grid era","grid computing,
network operating systems,
storage management"
Free- and Open-Source Software for a Course on Network Management: Authoring and Enactment of Scripts Based on Collaborative Learning Strategies,"This paper describes a computer-supported collaborative learning (CSCL) case study in engineering education carried out within the context of a network management course. The case study shows that the use of two computing tools developed by the authors and based on free-and open-source software (FOSS) provide significant educational benefits over traditional engineering pedagogical approaches in terms of both concepts and engineering competencies acquisition. The Collage authoring tool guides and supports the course teacher in the process of authoring computer-interpretable representations (using the IMS learning design standard notation) of effective collaborative pedagogical designs. Besides, the Gridcole system supports the enactment of that design by guiding the students throughout the prescribed sequence of learning activities. The paper introduces the goals and context of the case study, elaborates on how Collage and Gridcole were employed, describes the applied evaluation methodology, and discusses the most significant findings derived from the case study.","Open source software,
Collaborative learning,
Learning systems"
Multicast Routing with Delay and Delay Variation Constraints for Collaborative Applications on Overlay Networks,"Computer supported collaborative applications on overlay networks are gaining popularity among users who are geographically dispersed. Examples of these kinds of applications include video-conferencing, distributed database replication, and online games. This type of application requires a multicasting subnetwork, using which messages should arrive at the destinations within a specified delay bound. These applications also require that destinations receive the message from the source at approximately the same time. The problem of finding a multicasting subnetwork with delay and delay-variation bound has been proved to be an NP complete problem in the literature and heuristics have been proposed for this problem. In this paper, we provide an efficient heuristic to obtain a multicast subnetwork on an overlay network, given a source and a set of destinations that is within a specified maximum delay and a specified maximum variation in the delays from a source to the destinations. The time-complexity of our algorithm is O(|E|+nk log(|E|/n)+m2k), where n and |E| are the number of nodes and edges in the network, respectively, k is the number of shortest paths determined, and m is the number of destinations. We have shown that our algorithm is significantly better in terms of time-complexity than existing algorithms for the same problem. Our extensive empirical studies indicate that our heuristic uses significantly less runtime in comparison with the best-known heuristics while achieving the tightest delay variation for a given end-to-end delay bound","Routing,
Collaboration,
Delay,
Application software,
Computer applications,
Computer networks,
Distributed databases,
Games,
Multicast algorithms,
Runtime"
Design a Neural Network for Features Selection in Non-intrusive Monitoring of Industrial Electrical Loads,"This paper proposes to compare the performance of neural network classifiers between back propagation (BP) and learning vector quantization (LVQ) for pattern analyses of features selection in a non-intrusive load monitoring (NILM) system. Load recognition for identifying loads being connected and disconnected is applied to a NILM by using a neural network, especially for industrial electrical loads, even though some loads are activated at the nearly same time. In order to accurately decompose the aggregate load into its components, a feature-based model for describing the signatures of individual appliances and load combinations is used. The model will suggest the certain signatures which can be detected for all loads in order to indicate the activities of the separate components. To verify the performance of the model for the features selection, the data sets of the electrical loads and the load recognition techniques apply an electromagnetic transient program (EMTP) and a neural network, respectively. The effectiveness and computation equipment of load recognition are analyzed and compared by using the back propagation classifier and the learning vector quantization classifier. To obtain a maximum recognition accuracy rate, the calculation of the turn-on transient energy signature employs a window of samples, At, to adaptively segment a transient representative of a class of loads. Experiments performed with a variety of model data sets which reveal the back propagation classifier is superior to the learning quantization classifier in the effectiveness and computation equipment of load recognition.","Neural networks,
Monitoring,
Vector quantization,
EMTP,
Pattern analysis,
Aggregates,
Home appliances,
Electromagnetic modeling,
Electromagnetic propagation,
Accuracy"
Cognitive Technology for Ultra-Wideband/WiMax Coexistence,"Cognitive radios have been advanced as a technology for the opportunistic use of under-utilized spectrum wherein secondary devices sense the presence of the primary user and use the spectrum only if it is deemed empty. The distinguishing aspect of cognitive radios is the ability to sense the primary user and modify their transmission parameters to avoid interference to the primary. In this paper we explore the use of cognitive technology to enable the operation of ultra-wideband (UWB) devices in WiMax bands. In this particular example UWB devices must incorporate cognitive technology to detect and avoid (DAA) WiMax devices in certain regulatory domains. We start by discussing various options for detection and avoidance. We then describe the obstacles faced in achieving robust detection and avoidance with an on-chip implementation of basic DAA functionality. This implementation is based on the energy detector and can reliably detect WiMax uplink transmissions. Finally, we present empirical results for the operation of a single cognitive technology enabled UWB device with a WiMax system. This interaction also highlights the problem of dealing with listen before speak primaries where secondary transmission could interfere by denying the primary access to the medium.","Ultra wideband technology,
WiMAX,
Interference,
Cognitive radio,
Face detection,
Robustness,
Europe,
Radio spectrum management,
Regulators,
Computer science"
Unsupervised Graph-basedWord Sense Disambiguation Using Measures of Word Semantic Similarity,"This paper describes an unsupervised graph-based method for word sense disambiguation, and presents comparative evaluations using several measures of word semantic similarity and several algorithms for graph centrality. The results indicate that the right combination of similarity metrics and graph centrality algorithms can lead to a performance competing with the state-of-the-art in unsupervised word sense disambiguation, as measured on standard data sets.","Natural languages,
Computer science,
Measurement standards,
Humans,
Production facilities,
Encoding,
Labeling"
An Entropy-Based Weighted Clustering Algorithm and Its Optimization for Ad Hoc Networks,"As a newly-proposed weighing-based clustering algorithm, WCA has improved performance compared with other previous clustering algorithms. But the high mobility of nodes will lead to high frequency of re-affiliation which will increase the network overhead. To solve this problem, we propose an entropy- based WCA (EWCA) which can enhance the stability of the network. Meanwhile, in order better to facilitate the optimal operation of the MAC protocol and to further stabilize the network structure, this paper applies tabu search onto EWCA to choose a near optimal dominant set. Consequently, less clusterheads are required to manage the network. Simulation study indicates that the revised algorithm (EWCA-TS) has improved performance with respect to the original WCA, especially on the number of clusters and the re-affiliation frequency.","Clustering algorithms,
Ad hoc networks,
Peer to peer computing,
Frequency,
Nominations and elections,
Approximation algorithms,
Computer science,
Stability,
Media Access Protocol,
Wireless networks"
Fast Matching of Planar Shapes in Sub-cubic Runtime,"The matching of planar shapes can be cast as a problem of finding the shortest path through a graph spanned by the two shapes, where the nodes of the graph encode the local similarity of respective points on each contour. While this problem can be solved using dynamic time warping, the complete search over the initial correspondence leads to cubic runtime in the number of sample points. In this paper, we cast the shape matching problem as one of finding the shortest circular path on a torus. We propose an algorithm to determine this shortest cycle which has provably sub-cubic runtime. Numerical experiments demonstrate that the proposed algorithm provides faster shape matching than previous methods. As an application, we show that it allows to efficiently compute a clustering of a shape data base.","Shape,
Runtime,
Clustering algorithms,
Computer science,
Image analysis,
Information retrieval,
Image retrieval,
Internet,
Dynamic programming,
Speech recognition"
An Evaluation of Server Consolidation Workloads for Multi-Core Designs,"While chip multiprocessors with ten or more cores will be feasible within a few years, the search for applications that fully exploit their attributes continues. In the meantime, one sure-fire application for such machines will be to serve as consolidation platforms for sets of workloads that previously occupied multiple discrete systems. Such server consolidation scenarios will simplify system administration and lead to savings in power, cost, and physical infrastructure. This paper studies the behavior of server consolidation workloads, focusing particularly on sharing of caches across a variety of configurations. Noteworthy interactions emerge within a workload, and notably across workloads, when multiple server workloads are scheduled on the same chip. These workloads present an interesting design point and will help designers better evaluate trade-offs as we push forward into the many-core era.","Costs,
Application software,
Yarn,
Space technology,
Interference,
Communication system control,
Design engineering,
Computer science,
Job shop scheduling,
Sun"
An Effective Design of Deadlock-Free Routing Algorithms Based on 2D Turn Model for Irregular Networks,"System area networks (SANs), which usually accept arbitrary topologies, have been used to connect hosts in PC clusters. Although deadlock-free routing is often employed for low-latency communications using wormhole or virtual cut-through switching, the interconnection adaptivity introduces difficulties in establishing deadlock-free paths. An up*/down* routing algorithm, which has been widely used to avoid deadlocks in irregular networks, tends to make unbalanced paths as it employs a one-dimensional directed graph. The current study introduces a two-dimensional directed graph on which adaptive routings called left-up first turn (L-turn) routings and right-down last turn (R-turn) routings are proposed to make the paths as uniformly distributed as possible. This scheme guarantees deadlock-freedom because it uses the turn model approach, and the extra degree of freedom in the two-dimensional graph helps to ensure that the prohibited turns are well-distributed. Simulation results show that better throughput and latency results from uniformly distributing the prohibited turns by which the traffic would be more distributed toward the leaf nodes. The L-turn routings, which meet this condition, improve throughput by up to 100 percent compared with two up*/down*-based routings, and also reduce latency","Algorithm design and analysis,
System recovery,
Routing,
Multiprocessor interconnection networks,
Network topology,
Computer networks,
Concurrent computing,
Clustering algorithms,
Communication switching,
Throughput"
Projective Factorization of Multiple Rigid-Body Motions,"Given point correspondences in multiple perspective views of a scene containing multiple rigid-body motions, we present an algorithm for segmenting the correspondences according to the multiple motions. We exploit the fact that when the depths of the points are known, the point trajectories associated with a single motion live in a subspace of dimension at most four. Thus motion segmentation with known depths can be achieved by methods of subspace separation, such as GPCA or LSA. When the depths are unknown, we proceed iteratively. Given the segmentation, we compute the depths using standard techniques. Given the depths, we use GPCA or LSA to segment the scene into multiple motions. Experiments on the Hopkins 155 motion segmentation database show that our method compares favorably against existing affine motion segmentation methods in terms of segmentation error and execution time.","Layout,
Computer vision,
Cameras,
Motion segmentation,
Motion estimation,
Matrix decomposition,
Image segmentation,
Computational geometry,
Iterative algorithms,
Mechanical engineering"
A Multi-Core Debug Platform for NoC-Based Systems,"Network-on-chip (NoC) is generally regarded as the most promising solution for the future on-chip communication scheme in giga-scale integrated circuits. As traditional debug architecture for bus-based systems is not readily applicable to identify bugs in NoC-based systems, in this paper, we present a novel debug platform that supports concurrent debug access to the cores under debug (CUDs) and the NoC in a unified architecture. By introducing core-level debug probes in between the CUDs and their network interfaces and a system-level debug agent controlled by an off-chip multi-core debug controller, the proposed debug platform provides in-depth analysis features for NoC-based systems, such as NoC transaction analysis, multi-core cross-triggering and global synchronized timestamping. Therefore, the proposed solution is expected to facilitate the designers to identify bugs in NoC-based systems more effectively and efficiently. Experimental results show that the design-for-debug cost for the proposed technique in terms of area and traffic requirements is moderate","Network-on-a-chip,
System-on-a-chip,
Computer bugs,
Silicon,
Debugging,
Network interfaces,
Control systems,
Routing,
Time to market,
Computer science"
A Safe Regression Test Selection Technique for Web Services,"Web applications and Web services have to undergo rapid modifications, and these modifications must be supported by rapid verification. While regression testing (RT) is a major component of most major testing systems, the related techniques have only just begun to be applied to Web services. One of the more important techniques, Regression test selection (RTS) aims to reduce the cost of performing RT. This paper reports a control flow graph-based approach that makes it possible to apply a safe RTS technique to Web services in an end-to-end manner. Safe RTS techniques ensure that no modification revealing tests will be left unselected. A simplified purchase order system that involves in three Web services is used to illustrate the use of our approach.","Web services,
System testing,
Costs,
Computer science,
Application software,
Performance evaluation,
Flow graphs,
Web and internet services,
Software testing,
Software systems"
Random-Accessible Compressed Triangle Meshes,"With the exponential growth in size of geometric data, it is becoming increasingly important to make effective use of multilevel caches, limited disk storage, and bandwidth. As a result, recent work in the visualization community has focused either on designing sequential access compression schemes or on producing cache-coherent layouts of (uncompressed) meshes for random access. Unfortunately combining these two strategies is challenging as they fundamentally assume conflicting modes of data access. In this paper, we propose a novel order-preserving compression method that supports transparent random access to compressed triangle meshes. Our decompression method selectively fetches from disk, decodes, and caches in memory requested parts of a mesh. We also provide a general mesh access API for seamless mesh traversal and incidence queries. While the method imposes no particular mesh layout, it is especially suitable for cache-oblivious layouts, which minimize the number of decompression I/O requests and provide high cache utilization during access to decompressed, in-memory portions of the mesh. Moreover, the transparency of our scheme enables improved performance without the need for application code changes. We achieve compression rates on the order of 20:1 and significantly improved I/O performance due to reduced data transfer. To demonstrate the benefits of our method, we implement two common applications as benchmarks. By using cache-oblivious layouts for the input models, we observe 2-6 times overall speedup compared to using uncompressed meshes.","Bandwidth,
Data visualization,
Data structures,
Memory management,
Delay,
Space technology,
Cache storage,
Decoding,
Computer displays,
Computational modeling"
Stabilization of collective motion in three dimensions: A consensus approach,"This paper proposes a methodology to stabilize relative equilibria in a model of identical, steered particles moving in three-dimensional Euclidean space. Exploiting the Lie group structure of the resulting dynamical system, the stabilization problem is reduced to a consensus problem. We first derive the stabilizing control laws in the presence of all- to-all communication. Providing each agent with a consensus estimator, we then extend the results to a general setting that allows for unidirectional and time-varying communication topologies.","Communication system control,
Aerodynamics,
Control systems,
Unmanned aerial vehicles,
Mechanical variables control,
Vehicle dynamics,
Kinematics,
Motion control,
USA Councils,
Topology"
Visualizing Multivariate Volume Data from Turbulent Combustion Simulations,"To understand dynamic mechanisms, scientists need intuitive and convenient ways to validate known relationships and reveal hidden ones among multiple variables","Data visualization,
Combustion,
Rendering (computer graphics),
Biomedical imaging,
Magnetic resonance imaging,
Ultrasonic imaging,
X-ray imaging,
Computational modeling,
Fires,
Biomedical engineering"
The Uncanny Valley: Effect of Realism on the Impression of Artificial Human Faces,"Roboticists believe that people will have an unpleasant impression of a humanoid robot that has an almost, but not perfectly, realistic human appearance. This is called the uncanny valley, and is not limited to robots, but is also applicable to any type of human-like object, such as dolls, masks, facial caricatures, avatars in virtual reality, and characters in computer graphics movies. The present study investigated the uncanny valley by measuring observers' impressions of facial images whose degree of realism was manipulated by morphing between artificial and real human faces. Facial images yielded the most unpleasant impressions when they were highly realistic, supporting the hypothesis of the uncanny valley. However, the uncanny valley was confirmed only when morphed faces had abnormal features such as bizarre eyes. These results suggest that to have an almost perfectly realistic human appearance is a necessary but not a sufficient condition for the uncanny valley. The uncanny valley emerges only when there is also an abnormal feature.",
Adaptive computation offloading for energy conservation on battery-powered systems,"This paper considers the problem of extending the battery lifetime for a portable computer by off loading its computation to a server. Depending on the inputs, computation time for different instances of a program can vary significantly and they are often difficult to predict. Different from previous studies on computation off loading, our approach does not require estimating the computation time before the execution. We execute the program initially on the portable client with a timeout. If the computation is not completed after the timeout, it is off loaded to the server. We first set the timeout to be the minimum computation time that can benefit from off loading. This method is proved to be 2- competitive. We further consider collecting online statistics of the computation time and find the statistically optimal timeout. Finally, we provide guidelines to construct programs with computation off loading. Experiments show that our methods can save up to 17% more energy than existing approaches.","Energy conservation,
Portable computers,
Batteries,
Network servers,
Computer networks,
Computer science,
Guidelines,
Hardware,
Distributed computing,
Power engineering and energy"
Top-k Spatial Preference Queries,"A spatial preference query ranks objects based on the qualities of features in their spatial neighborhood. For example, consider a real estate agency office that holds a database with available flats for lease. A customer may want to rank the flats with respect to the appropriateness of their location, defined after aggregating the qualities of other features (e.g., restaurants, cafes, hospital, market, etc.) within a distance range from them. In this paper, we formally define spatial preference queries and propose appropriate indexing techniques and search algorithms for them. Our methods are experimentally evaluated for a wide range of problem settings.","Spatial databases,
Computer science,
Hospitals,
Lungs,
Geography,
Indexing,
Nearest neighbor searches,
Joining processes,
Database systems"
A PROBE-Based Heuristic for Graph Partitioning,"A new heuristic algorithm, PROBE_BA, which is based on the recently introduced metaheuristic paradigm population- reinforced optimization-based exploration (PROBE), is proposed for solving the Graph Partitioning Problem. The ""exploration"" part of PROBE_BA is implemented by using the differential-greedy algorithm of Battiti and Bertossi and a modification of the Kernighan-Lin algorithm at the heart of Bui and Moon's genetic algorithm BFS _GBA. Experiments are used to investigate properties of PROBE and show that PROBE_BA compares favorably with other solution methods based on genetic algorithms, randomized reactive tabu search, or more specialized multilevel partitioning techniques. In addition, PROBE_BA finds new best cut values for 10 of the 34 instances in Walshaw's graph partitioning archive.","Probes,
Partitioning algorithms,
Algorithm design and analysis,
Complexity theory,
Clustering algorithms,
Joining processes,
Genetic algorithms,
Heuristic methods"
Comprehensive Evaluation of an Educational Software Engineering Simulation Environment,"Software engineering educational approaches are often evaluated only anecdotally, or in informal pilot studies. We describe a more comprehensive approach to evaluating a software engineering educational technique (SimSE, a graphical, interactive, customizable, game-based software engineering simulation environment). Our method for evaluating SimSE went above and beyond anecdotal experience and approached evaluation from a number of different angles through a family of studies designed to assess SimSE's effectiveness and guide its development. In this paper, we demonstrate the insights and lessons that can be gained when using such a multi-angled evaluation approach. Our hope is that, from this paper, educators will: (1) learn ideas about how to more comprehensively evaluate their own approaches, and (2) be provided with evidence about the educational effectiveness of SimSE.","Software engineering,
Computational modeling,
Computer simulation,
Education,
Technological innovation,
Discrete event simulation,
Project management,
Software development management,
Engineering management,
Personnel"
Value-Directed Human Behavior Analysis from Video Using Partially Observable Markov Decision Processes,"This paper presents a method for learning decision theoretic models of human behaviors from video data. Our system learns relationships between the movements of a person, the context in which they are acting, and a utility function. This learning makes explicit that the meaning of a behavior to an observer is contained in its relationship to actions and outcomes. An agent wishing to capitalize on these relationships must learn to distinguish the behaviors according to how they help the agent to maximize utility. The model we use is a partially observable Markov decision process, or POMDP. The video observations are integrated into the POMDP using a dynamic Bayesian network that creates spatial and temporal abstractions amenable to decision making at the high level. The parameters of the model are learned from training data using an a posteriori constrained optimization technique based on the expectation-maximization algorithm. The system automatically discovers classes of behaviors and determines which are important for choosing actions that optimize over the utility of possible outcomes. This type of learning obviates the need for labeled data from expert knowledge about which behaviors are significant and removes bias about what behaviors may be useful to recognize in a particular situation. We show results in three interactions: a single player imitation game, a gestural robotic control problem, and a card game played by two people.",
An Efficient Pipelined Architecture for H.264/AVC Intra Frame Processing,"A number of recent efforts have been made to speed up H.264 intra frame coding. When these algorithms are implemented by dedicated hardware accelerators, these hardware resources are often wasted if intra predictions and reconstructions for 4times4 blocks are serialized. In order to avoid a hardware waste, this paper proposes a pipelined execution of the intra predictions and reconstructions of 4times4 blocks. The processing orders of 4times4 intra predictions are derived for both encoding and decoding, respectively, to reduce the dependencies between consecutively processed blocks and minimize pipeline stalls. The proposed pipelined execution of 4times4 intra predictions for encoding is integrated with the other intra frame encoding operations with an efficient scheduling that allows these other operations to be executed in parallel with intra prediction. When compared with the best previous work for intra frame coding (Suh et al., 2005), the execution time is decreased by 41 % even with reduced hardware resources","Automatic voltage control,
Hardware,
Pipeline processing,
Quantization,
Encoding,
Decoding,
Video compression,
ISO standards,
IEC standards,
Computer architecture"
A Concept of Symbiotic Computing and its Application to Telework,"In this paper, a concept of ""symbiotic computing"" is formalized to bridge an e-Gap between real space (RS) and digital space (DS). The symbiotic computing is a post ubiquitous computing model based on an agent-oriented computing model to bring in social heuristics and cognitive functions into DS to bridge the e-Gap. The symbiotic computing provides a cyberspace into which RS and DS are integrated through an e-Bridge, where we can receive the benefits easily and safely without IT skills. The last section of this paper describes an agent-based support system for home-based teleworkers designed based on the model.","Symbiosis,
Computer applications,
Teleworking,
Pervasive computing,
Bridges,
Ubiquitous computing,
Space technology,
Home computing,
Humans,
Cognition"
Autonomous Low Power Microsystem Powered by Vibration Energy Harvesting,"This paper reports, for the first time, the implementation of a microsystem powered entirely from ambient vibrations. Sufficient electrical energy is harvested to power a radio-frequency (RF) linked accelerometer based microsystem. The microsystem is energy aware and will adjust the measurement/transmit duty cycle according to the available energy; this is typically every 50 seconds during normal operation. The system is fully powered from 45 muWrms scavenged by a miniature electromagnetic (EM) vibration energy harvester of volume ; level of 0.6ms-2.","Voltage,
Microcontrollers,
Circuits,
Virtual manufacturing,
Radio frequency,
Accelerometers,
Batteries,
Vibration measurement,
Supercapacitors,
Coils"
Mexar2: AI Solves Mission Planner Problems,"Deep-space missions carry an ever larger set of different and complementary onboard payloads. Each payload generates data, and synthesizing it for optimized downlinking is one way to reduce the ratio of mission costs to science return. This is the main role of the Mars-Express scheduling architecture (Mexar2), an Al-based tool in daily use on the Mars-Express mission since February 2005. Mexar2 supports space mission planners continuously as they plan data downlinks from the spacecraft to Earth. The tool lets planners work at a higher abstraction level while it performs low-level, often-repetitive tasks. It also helps them produce a plan rapidly, explore alternative solutions, and choose the most robust plan for execution. Additionally, planners can analyze any problems over multiple days and identify payload overcommitments that cause resource bottlenecks and increase the risk of data losses. Mexar2 has significantly increased the data return over the whole Mars-Express mission duration. It's effectively become a work companion for mission planners at the European Space Agency's European Space Operations Center (ESOC) in Darmstadt, Germany.","Artificial intelligence,
Payloads,
Mars,
Geoscience,
Communication channels,
Councils,
Space missions,
Intelligent systems,
Safety,
Production"
"Hardware, Design and Implementation Issues on a Fpga-Based Smart Camera","Processing images to extract useful information in real-time is a complex task, dealing with large amounts of iconic data and requiring intensive computation. Smart cameras use embedded processing to save the host system from the low-level processing load and to reduce communication flows and overheads. Field programmable devices present special interest for smart cameras design: flexibility, reconfigurability and parallel processing skills are some specially important features. In this paper we present a FPGA-based smart camera research platform. The hardware architecture is described, and some design issues are discussed. Our goal is to use the possibility to reconfigure the FPGA device in order to adapt the system architecture to a given application. To that, a design methodology, based on pre-programmed processing elements, is proposed and sketched. Some implementation issues are discussed and a template tracking application is given as example, with its experimental results.","Hardware,
Smart cameras,
Face detection,
Data mining,
Field programmable gate arrays,
Computer architecture,
Object detection,
Face recognition,
Image processing,
Design methodology"
Global Software Development: Are Architectural Rules the Answer?,"Global software development (GSD) faces additional challenges as compared to single-site software development. Some of the better known challenges include temporal, geographical, and socio-cultural differences. To overcome these challenges, organizations need to revert to measures in order to deliver software in time in a distributed setting. Some of these measures may exist in the form of architectural rules: principles and statements on the software architecture that must be complied with throughout the organization. From the GSD literature we distilled four main GSD challenges and seven sub-challenges, or issues. For each issue, we list possible solutions and observe that solutions to GSD challenges may be obtained by adhering to architectural rules. We present a study on how two organizations involved in GSD solve the GSD challenges and issues. One of the organizations mainly uses rules regulating the architecture of the product. The other organization does not emphasize these architectural rules but rather focuses on the joint team effort in establishing and committing to measures that mainly pertain to the architecture process. We conclude that rules regulating a combination of both proves valuable in handling GSD challenges.","Programming,
Software architecture,
Computer architecture,
Software engineering,
Software measurement,
Guidelines,
Computer science,
Time measurement,
Engineering management,
Scheduling"
Identifying Source Cell Phone using Chromatic Aberration,"Chromatic aberration is the phenomenon where light of different wavelengths fail to converge at the same position on the focal plane. There are two kinds of chromatic aberration: longitudinal aberration causes different wavelengths to focus at different distances from the lens while lateral aberration is attributed to different wavelengths focusing at different positions on the sensor. In this paper, we estimate the parameters of lateral chromatic aberration by maximizing the mutual information between the corrected R and B channels with the G channel. The extracted parameters are then used as input features to a SVM classifier for identifying source cell phone of images. By considering only a certain part of the image when estimating the parameters, we reduce the runtime complexity of the algorithm dramatically while preserving the accuracy at a high level.",
Prioritizing Warning Categories by Analyzing Software History,"Automatic bug finding tools tend to have high false positive rates: most warnings do not indicate real bugs. Usually bug finding tools prioritize each warning category. For example, the priority of ""overflow "" is 1 and the priority of ""jumbled incremental"" is 3, but the tools 'prioritization is not very effective. In this paper, we prioritize warning categories by analyzing the software change history. The underlying intuition is that if warnings from a category are resolved quickly by developers, the warnings in the category are important. Experiments with three bug finding tools (FindBugs, JLint, and PMD) and two open source projects (Columba and jEdit) indicate that different warning categories have very different lifetimes. Based on that observation, we propose a preliminary algorithm for warning category prioritizing.","History,
Software tools,
Computer bugs,
Java,
Open source software,
Software debugging,
Control systems,
Performance analysis,
Computer science,
Artificial intelligence"
A Pattern-Theoretic Characterization of Biological Growth,"Mathematical and statistical modeling of biological growth is an important problem in medical diagnostics. Here, we seek tools to analyze changes in anatomical parts using images collected over time. We introduce a structured model, called Growth by Random Iterated Diffeomorphisms (GRID), that treats a cumulative growth deformation as a composition of several elementary deformations. Each elementary deformation applies to a small region by capturing deformation local to that region and is characterized by a seed and a radial deformation pattern around that seed. These GRID variables-seed locations and radial deformation patterns-are estimated from observed images in two steps: 1) estimate a cumulative deformation over an observation interval; 2) estimate GRID variables using maximum-likelihood criterion from this estimated cumulative deformation. We demonstrate this framework using an MRI image data of a rat's brain growth. For future statistical analysis, we propose a time-varying Poisson process for the seed placements and a random drawing from a predetermined catalog of deformations for the radial deformation patterns","Biological system modeling,
Neoplasms,
Deformable models,
Geometry,
Image analysis,
Animals,
Statistics,
Shape measurement,
Mathematical model,
Pixel"
Using a cascaded H-bridge STATCOM for rebalancing unbalanced voltages,"This paper considers specific issues related to using the cascade (also known as the H-bridge) multilevel STATCOM with unbalanced voltages and currents at the STATCOM-AC line connection terminals. Under this condition the average power into the individual phase legs of the converter, in general, is not zero, although the total three phase power is zero. The resultant phase cluster power unbalance can result in increasing or decreasing capacitor voltages within a phase leg. This paper will present a technique for zeroing the phase cluster average power under unbalanced line conditions when the control strategy is to rebalance unbalanced voltages.","Automatic voltage control,
Leg,
Capacitors,
Voltage control,
Topology,
Reactive power,
Computer science,
Energy storage,
Power electronics,
Adders"
Spatio-Temporal Markov Random Field for Video Denoising,"This paper presents a novel spatio-temporal Markov random field (MRF) for video denoising. Two main issues are addressed in this paper, namely, the estimation of noise model and the proper use of motion estimation in the denoising process. Unlike previous algorithms which estimate the level of noise, our method learns the full noise distribution nonparametrically which serves as the likelihood model in the MRF. Instead of using deterministic motion estimation to align pixels, we set up a temporal likelihood by combining a probabilistic motion field with the learned noise model. The prior of this MRF is modeled by piece-wise smoothness. The main advantage of the proposed spatio-temporal MRF is that it integrates spatial and temporal information adaptively into a statistical inference framework, where the posteriori is optimized using graph cuts with alpha expansion. We demonstrate the performance of the proposed approach on benchmark data sets and real videos to show the advantages of our algorithm compared with previous single frame and multi-frame algorithms.","Markov random fields,
Noise reduction,
Noise level,
Motion estimation,
Inference algorithms,
Filtering,
Motion compensation,
Image restoration,
Uncertainty,
Gaussian noise"
A General Discriminant Model for Color Face Recognition,"This paper presents a general discriminant model (GDM) for color face recognition. The GDM model involves two sets of variables: a set of color component combination coefficients for color image representation and a set of projection basis vectors for image discrimination. An iterative whitening-maximization (IWM) algorithm is designed to find the optimal solution of the model. The proposed algorithm is further extended to generate three color components (like the three color components of RGB color images) for further improving the face recognition performance. Experiments using the face recognition grand challenge (FRGC) database and the biometric experimentation environment (BEE) system show the effectiveness of the proposed model and algorithm. In particular, for the most challenging FRGC version 2 Experiment 4, which contains 12,776 training images, 16,028 controlled target images, and 8,014 uncontrolled query images, the proposed method achieves the face verification rate (ROC III) of 74.91% at the false accept rate of 0.1%.","Face recognition,
Color,
Image recognition,
Iterative algorithms,
Image databases,
Face detection,
Gray-scale,
Computer science,
Algorithm design and analysis,
Biometrics"
High Performance Dependable Multiprocessor II,"With the ever-increasing demand for higher bandwidth and processing capacity of today's space exploration, space science, and defense missions, the ability to efficiently apply commercial-off-the-shelf (COTS) processors for on-board computing has become a critical need. In response to this need, NASA's new millennium program (NMP) office commissioned the development of dependable multiprocessor (DM) technology for use in science and autonomy missions, but the technology is also applicable to a wide variety of DoD missions. The goal of the DM project is to provide spacecraft/payload processing capability 10x -100x what is available today, enabling heretofore unrealizable levels of science and autonomy. DM technology is being developed as part of the NMP ST8 (space technology 8) project. The objective of this NMP ST8 effort is to combine high-performance, fault tolerant, COTS-based cluster processing and fault tolerant middleware in an architecture and software framework capable of supporting a wide variety of mission applications. Dependable multiprocessor development is continuing as one of the four selected ST8 flight experiments planned to be flown in 2009.","Space technology,
Delta modulation,
Space exploration,
Fault tolerance,
Bandwidth,
Space missions,
High performance computing,
Space vehicles,
Payloads,
Middleware"
Performance Evaluation of Adaptive Routing Algorithms for achieving Fault Tolerance in NoC Fabrics,"Commercial designs are integrating from 10 to 100 embedded functional and storage blocks in a single system on chip (SoC) currently, and the number is likely to increase significantly in the near future. The communication requirements of these large Multi Processor SoCs (MP-SoCs) are convened by the emerging network-on-a-chip (NoC) paradigm. In the deep sub-micron (DSM) VLSI processes, it is difficult to guarantee correct fabrication with an acceptable yield without employing design techniques that take into account the intrinsic existence of manufacturing faults. To become a viable alternative IC design methodology the NoC paradigm must address the system-level reliability issues, which is going to be the dominant concern in the DSM and beyond silicon era. By incorporating adaptiveness in the data communication mechanism we are able to tolerate permanent manufacturing faults in the NoC interconnect architectures. The corresponding performance and cost figures must be carefully analyzed and weighted against the specific application requirements. In this paper we explore the performance tradeoffs associated with adaptive routing schemes in NoC fabrics.","Routing,
Fault tolerance,
Network-on-a-chip,
Fabrics,
System-on-a-chip,
Very large scale integration,
Fabrication,
Manufacturing processes,
Design methodology,
Telecommunication network reliability"
Novel Memory Reference Reduction Methods for FFT Implementations on DSP Processors,"Memory references in digital signal processors (DSP) are expensive due to their long latencies and high power consumption. Implementing fast Fourier transform (FFT) algorithms on DSP involves many memory references to access butterfly inputs and twiddle factors. Conventional FFT implementations require redundant memory references to load identical twiddle factors for butterflies from different stages in the FFT diagrams. In this paper, we present novel memory reference reduction methods to minimize memory references due to twiddle factors for implementing various different FFT algorithms on DSP. The proposed methods first group the butterflies with identical twiddle factors from different stages in the FFT diagrams and compute them before computing other butterflies with different twiddle factors, and then reduce the number of twiddle factor lookups by taking advantage of the properties of twiddle factors. Consequently, each twiddle factor is loaded only once and the number of memory references due to twiddle factors can be minimized. We have applied the proposed methods to implement radix-2 DIF FFT algorithm on TI TMS320C64x DSP. Experimental results show the proposed methods can achieve average of 76.4% reduction in the number of memory references, 53.5% saving of memory spaces due to twiddle factors, and average of 36.5% reduction in the number of clock cycles to compute radix-2 DIF FFT on DSP comparing to the conventional implementation. Similar performance gain is reported for implementing radix-2 DIT FFT algorithms using the new methods","Digital signal processing,
Signal processing algorithms,
Discrete Fourier transforms,
Computer science,
Fast Fourier transforms,
Signal processing,
Information technology,
Digital signal processors,
Delay,
Energy consumption"
Adaptive H.264/MPEG-4 SVC video over IEEE 802.16 broadband wireless networks,"In this paper we present a solution for delivering streaming video-on-demand to subscribers via an 802.16 broadband wireless access network. The solution leverages the forthcoming H.264/AVC Scalable Video Coding (SVC) scheme and a mechanism to perform rate adaptation based on monitoring changes to the amount of flow traffic in the network at any time. A simulation-based approach is used to determine how the system performs in a rural deployment. Results show that the scheme provides high utilization of the wireless access system, at over 96%. Further, it maintains smooth transmission rate for the video applications, and ensures that no interruptions in continuous video playback occur during the streaming session. Lastly, a comparison with single-layer H.264/AVC is performed, showing how the proposed solution performs better with respect to both system utilization and the fact that no clients suffer interruptions in continuous playback.","MPEG 4 Standard,
Static VAr compensators,
Wireless networks,
Streaming media,
Automatic voltage control,
Video compression,
Video coding,
Control systems,
Quality of service,
User centered design"
Micro Power Meter for Energy Monitoring of Wireless Sensor Networks at Scale,"We present SPOT, a scalable power observation tool that enables in situ measurement of nodal power and energy over a dynamic range exceeding four decades or a temporal resolution of microseconds. Using SPOT, every node in a sensor network can now be instrumented, providing unparalleled visibility into the dynamic power profile of applications and system software. Power metering at every node enables previously impossible empirical evaluation of low power designs at scale. The SPOT architecture and design meet challenges unique to wireless sensor networks and other low power systems, such as orders of magnitude difference in current draws between sleep and active states, short-duration power spikes during periods of brief activity, and the need for minimum perturbation of the system under observation.",
Robot-Assisted Sensor Network Deployment and Data Collection,"Wireless sensor networks have been widely used in many applications such as environment monitoring, surveillance systems and unmanned space explorations. However, poor deployment of sensor devices leads (1) bad network connectivity which makes data communication or data collection very hard; or (2) redundancy of coverage which wastes energy of sensors and causes redundant data in the network. Thus, in this paper, we propose using a mobile robot to assist the sensor deployment and data collection for unmanned explorations or monitoring. We assume that the robot can carry and deploy the sensor devices, and also have certain communication capacity to collect the data from the sensor devices. Given a set of interest points in an area, we study the following interesting problems: (1) how to decide minimum number of sensor devices to cover all the interest points; (2) how to schedule the robot to place these sensor devices in certain position so that the path of the robot is minimum; and (3) after the deployment of sensors, how to schedule the robot to visit and communicate with these sensor devices to collect data so that the path of the robot is minimum. We propose a complete set of heuristics for all these problems and verify the performances via simulation.","Robot sensing systems,
Monitoring,
Capacitive sensors,
Orbital robotics,
Wireless sensor networks,
Surveillance,
Space exploration,
Data communication,
Mobile robots,
Mobile communication"
Congestion Control for Small Buffer High Speed Networks,"There is growing interest in designing high speed routers with small buffers that store only tens of packets. Recent studies suggest that TCP NewReno, with the addition of a pacing mechanism, can interact with such routers without sacrificing link utilization. Unfortunately, as we show in this paper, as workload requirements grow and connection bandwidths increase, the interaction between the congestion control protocol and small buffer routers produce link utilizations that tend to zero. This is a simple consequence of the inverse square root dependence of TCP throughput on loss probability. In this paper we present a new congestion controller that avoids this problem by allowing a TCP connection to achieve arbitrarily large bandwidths without demanding the loss probability go to zero. We show that this controller produces stable behavior and, through simulation, we show its performance to be superior to TCP NewReno in a variety of environments. Lastly, because of its advantages in high bandwidth environments, we compare our controller's performance to some of the recently proposed high performance versions of TCP including HSTCP, STCP, and FAST. Simulations illustrate the superior performance of the proposed controller in a small buffer environment.","High-speed networks,
Optical buffering,
Buffer storage,
Bandwidth,
Throughput,
Computer science,
Communications Society,
Communication system control,
Protocols,
Costs"
A Semantic Web-Based Approach to Knowledge Management for Grid Applications,"Knowledge has become increasingly important to support intelligent process automation and collaborative problem solving in large-scale science over the Internet. This paper addresses distributed knowledge management, its approach and methodology, in the context of grid application. We start by analyzing the nature of grid computing and its requirements for knowledge support; then, we discuss knowledge characteristics and the challenges for knowledge management on the grid. A semantic Web-based approach is proposed to tackle the six challenges of the knowledge lifecycle - namely, those of acquiring, modeling, retrieving, reusing, publishing, and maintaining knowledge. To facilitate the application of the approach, a systematic methodology is conceived and designed to provide a general implementation guideline. We use a real-world Grid application, the GEODISE project, as a case study in which the core semantic Web technologies such as ontologies, semantic enrichment, and semantic reasoning are used for knowledge engineering and management. The case study has been fully implemented and deployed through which the evaluation and validation for the approach and methodology have been performed","Knowledge management,
Automation,
Collaborative work,
Large-scale systems,
Internet,
Grid computing,
Publishing,
Guidelines,
Semantic Web,
Ontologies"
Using Server Pages to Unify Clones in Web Applications: A Trade-Off Analysis,"Server page technique is commonly used for implementing Web application user interfaces. Server pages can represent many similar Web pages in a generic form. Yet our previous study revealed high rates of repetitions in Web applications, particularly in the user interfaces. Code duplication, commonly known as 'cloning', signals untapped opportunities to achieve simpler, smaller, more generic, and more maintainable Web applications. Using PHP server page technique, we conducted a case study to explore how far server page technique can be pushed to achieve clone-free Web applications. Our study suggests that clone unification using server pages affects system qualities (e.g., runtime performance) to an extent that may not be acceptable in many project situations. Our paper discusses the trade-offs we observed when applying server pages to unify clones in Web applications. We expect our findings to help in developing and validating complementary techniques that can unify clones without incurring such trade-offs.","Cloning,
Application software,
User interfaces,
Web pages,
Computer science,
Computer interfaces,
Error correction,
Productivity,
Application specific processors,
HTML"
Massive-Model Rendering Techniques: A Tutorial,The currently observed exponentially increasing size of 3D models prohibits rendering them using brute force methods. Researchers have proposed various output-sensitive rendering algorithms to overcome this challenge. This article provides an overview of this technology.,
A closer Look on Load Management,"The rising share of energy generation of renewable energies leads to a more volatile energy generation. Large conventional generation capacities have to be kept in the system in order to smooth out the generation patterns of distributed generators. The controllability of electricity generation is on decrease. To improve adaptability of power systems due to these new challenges, the concept of load management gains in importance. This paper provides a closer look on the potential of specific consumption processes for load management measures. It proposes a new solution to enhance the idea of the interruptible load for demand side management relying on the particular properties of inert thermal processes. A general taxonomy is given, whereas two different options for inert thermal processes are discussed and compared in detail.",
Performance Evaluation of an IEEE 802.15.4a Physical Layer with Energy Detection and Multi-User Interference,"We evaluate the performance of an IEEE 802.15.4a ultra-wide band (UWB) physical layer, with an energy-detection receiver, in the presence of multi-user interference (MUI). A complete packet based system is considered. We take into account packet detection and timing acquisition, the estimation of the power delay profile of the channel, and the recovery of the encoded payload. Energy detectors are known to have a low implementation complexity and to allow for avoiding the complex channel estimation needed by a Rake receiver. However, our results show that MUI severely degrades the performance of the energy detection receiver, even at low traffic rate. We demonstrate that using an IEEE 802.15.4a compliant energy detection receiver significantly diminishes one of the most appealing benefits of UWB, namely its robustness to MUI and thus the possibility to allow parallel transmissions. We further find that timing acquisition and data decoding both equally suffer from MUI.","Physical layer,
Interference,
Timing,
Delay estimation,
RAKE receivers,
Payloads,
Detectors,
Channel estimation,
Fading,
Multipath channels"
Distributed Aggregation Algorithms with Load-Balancing for Scalable Grid Resource Monitoring,"Scalable resource monitoring and discovery are essential to the planet-scale infrastructures such as grids and PlanetLab. This paper proposes a scalable grid monitoring architecture that builds distributed aggregation trees (DAT) on a structured P2P network like Chord. By leveraging Chord topology and routing mechanisms, the DAT trees are implicitly constructed from native Chord routing paths without membership maintenance. To balance the DAT trees, we propose a balanced routing algorithm on Chord that dynamically selects the parent of a node from its finger nodes by its distance to the root. This paper shows that this balanced routing algorithm enables the construction of almost completely balanced DATs, when nodes are evenly distributed in the Chord identifier space. We have evaluated the performance and scalability of a DAT prototype implementation with up to 8192 nodes. Our experimental results show that the balanced DAT scheme scales well to a large number of nodes and corresponding aggregation trees. Without maintaining explicit parent-child membership, it has very low overhead during node arrival and departure. We demonstrate that the DAT scheme performs well in grid resource monitoring.","Peer to peer computing,
Routing,
Computerized monitoring,
Scalability,
Prototypes,
Condition monitoring,
Distributed computing,
Central Processing Unit,
Aggregates,
Indexing"
Interactive robot task training through dialog and demonstration,"Effective human/robot interfaces which mimic how humans interact with one another could ultimately lead to robots being accepted in a wider domain of applications. We present a framework for interactive task training of a mobile robot where the robot learns how to do various tasks while observing a human. In addition to observation, the robot listens to the human's speech and interprets the speech as behaviors that are required to be executed. This is especially important where individual steps of a given task may have contingencies that have to be dealt with depending on the situation. Finally, the context of the location where the task takes place and the people present factor heavily into the robot's interpretation of how to execute the task. In this paper, we describe the task training framework, describe how environmental context and communicative dialog with the human help the robot learn the task, and illustrate the utility of this approach with several experimental case studies.","Robots,
Abstracts"
Measurement of Energy Costs of Security in Wireless Sensor Nodes,"Both correct transmission using hashing and protection of messages using encryption in sensor nodes require additional energy. This paper describes our measurement results for energy consumption in CrossBow and Ember sensor nodes for the process of exchanging data messages between nodes both in the clear and in a protected form. Full strength algorithms were loaded into and executed in nodes. It was found that the CPU operates for substantially longer times for both hashing and encryption operations compared to the time for handling messages without any security. The longer radio transmission times due to hashing were especially costly. Hence, security algorithms have great impacts on energy consumption in sensor nodes. For the full operational mode, with CPU processing and also radio transmission of messages, our results indicate that the lifetime of a transmitting node in a security regime is only about one-half of the lifetime without security.","Energy measurement,
Costs,
Wireless sensor networks,
Communication system security,
Energy consumption,
Data security,
Cryptography,
Sensor phenomena and characterization,
Protection,
Batteries"
EDZL Scheduling Analysis,"A schedulability test is derived for the global earliest deadline zero laxity (EDZL) scheduling algorithm on a platform with multiple identical processors. The test is sufficient, but not necessary, to guarantee that a system of independent sporadic tasks with arbitrary deadlines will be successfully scheduled, with no missed deadlines, by the multiprocessor EDZL algorithm. Global EDZL is known to be at least as effective as global earliest-deadline-first (EDF) in scheduling task sets to meet deadlines. It is shown, by testing on large numbers of pseudo-randomly generated task sets, that the combination of EDZL and the new schedulability test is able to guarantee that far more task sets meet deadlines than the combination of EDF and known EDF schedulability tests.","Processor scheduling,
Optimal scheduling,
Scheduling algorithm,
Switches,
System testing,
Delay,
Algorithm design and analysis,
Computer science,
Hybrid power systems,
Real time systems"
A 2D Vibration Array as an Assistive Device for Visually Impaired,"This paper deals with the design, simulation and implementation of a 2D vibration array used as a major component of an assistive wearable navigation device for visual impaired. The 2D vibration array consists of 16 (4x4) miniature vibrators connected to a portable computer, which is the main computing component of the entire wearable navigation system, called Tyflos. Tyflos consists of two miniature cameras (attached to a pair of dark glasses), a microphone, an ear speaker, the 2D vibration array, and a portable computer. The cameras capture images from the surrounding environment and after appropriate processing 3D representations are created. These 3D space representations are projected on the 2D array, which vibrates in various levels corresponding to the distances of the surrounding obstacles. The 2D array is attached to the user's chest in order to provide the appropriate sensation (via vibrations) of the distances from the surroundings.","Navigation,
Cameras,
Portable computers,
Prototypes,
Ear,
Microphone arrays,
Formal languages,
Machine vision,
Global Positioning System,
Speech synthesis"
Managing autonomy in robot teams: Observations from four experiments,"It is often desirable for a human to manage multiple robots. Autonomy is required to keep workload within tolerable ranges, and dynamically adapting the type of autonomy may be useful for responding to environment and workload changes. We identify two management styles for managing multiple robots and present results from four experiments that have relevance to dynamic autonomy within these two management styles. These experiments, which involved 80 subjects, suggest that individual and team autonomy benefit from attention management aids, adaptive autonomy, and proper information abstraction.","Robots,
Abstracts"
A New Ultra-wideband Microstrip-to-CPS Transition,"A novel ultra-wideband microstrip-to-CPS (coplanar stripline) transition has been developed. The transition is designed to provide the field and impedance matching between adjacent transmission lines. The proposed design is applicable to substrates with any dielectric constant. The transition provides the balun role as well as the impedance transformation. The fabricated transition in back-to-back configuration provides the insertion loss less than 1 dB per transition and the return loss better than 10 dB for frequencies from 5.39 GHz to over 40 GHz. In addition, a simulation study indicates that this transition can possess a 3 dB bandwidth of ~ 100 GHz.","Ultra wideband technology,
Microstrip,
Impedance matching,
Stripline,
Coplanar transmission lines,
Dielectric substrates,
Dielectric constant,
Insertion loss,
Frequency,
Bandwidth"
Robust Image Segmentation with Mixtures of Student's t-Distributions,"Gaussian mixture models have been widely used in image segmentation. However, such models are sensitive to outliers. In this paper, we consider a robust model for image segmentation based on mixtures of Student's t-distributions which have heavier tails than Gaussian and thus are not sensitive to outliers. The t-distribution is one of the few heavy tailed probability density functions (pdf) closely related to the Gaussian, that gives tractable maximum likelihood inference via the Expectation-Maximization (EM) algorithm. Numerical experiments that demonstrate the properties of the proposed model for image segmentation are presented.","Robustness,
Image segmentation,
Tail,
Clustering algorithms,
Pixel,
Probability density function,
Inference algorithms,
Maximum likelihood estimation,
Computer science,
Coherence"
Mobile Peer-to-Peer Data Dissemination with Resource Constraints,"Peer-to-peer data dissemination in a mobile ad-hoc environment is characterized by three resource constraints, including energy, communication bandwidth, and storage. Most of the existing studies deal with these constraints separately. In this paper we propose an algorithm called RANk-based dissemination (RANDI), which provides an integral treatment to the three constraints. The contribution is in determining how to prioritize the reports in terms of their relevance, when to transmit the reports, and how many to transmit. We experimentally compare RANDI with IDS and PeopleNet, two mobile peer-to-peer dissemination algorithms. The results show that RANDI significantly outperforms both algorithms.","Peer to peer computing,
Bandwidth,
Energy management,
Mobile computing,
Mobile communication,
Energy storage,
Vehicles,
Bluetooth,
Computer science,
Relays"
Conformance Checking of Access Control Policies Specified in XACML,"Access control is one of the most fundamental and widely used security mechanisms. Access control mechanisms control which principals such as users or processes have access to which resources in a system. To facilitate managing and maintaining access control, access control policies are increasingly written in specification languages such as XACML. The specification of access control policies itself is often a challenging problem. Furthermore, XACML is intentionally designed to be generic: it provides the freedom in describing access control policies, which are well-known or invented ones. But the flexibility and expressiveness provided by XACML come at the cost of complexity, verbosity, and lack of desirable-property enforcement. Often common properties for specific access control policies may not be satisfied when these policies are specified in XACML, causing the discrepancy between what the policy authors intend to specify and what the actually specified XACML policies reflect. In this position paper, we propose an approach for conducting conformance checking of access control policies specified in XACML based on existing verification and testing tools for XACML policies.","Access control,
Testing,
Software standards,
Standards organizations,
Control systems,
Specification languages,
Costs,
Markup languages,
NIST,
Computer science"
"A fast-match approach for robust, faster than real-time speaker diarization","During the past few years, speaker diarization has achieved satisfying accuracy in terms of speaker Diarization Error Rate (DER). The most successful approaches, based on agglomerative clustering, however, exhibit an inherent computational complexity which makes real-time processing, especially in combination with further processing steps, almost impossible. In this article we present a framework to speed up agglomerative clustering speaker diarization. The basic idea is to adopt a computationally cheap method to reduce the hypothesis space of the more expensive and accurate model selection via Bayesian Information Criterion (BIC). Two strategies based on the pitch-correlogram and the unscented-trans-form based approximation of KL-divergence are used independently as a fast-match approach to select the most likely clusters to merge. We performed the experiments using the existing ICSI speaker diarization system. The new system using KL-divergence fast-match strategy only performs 14% of total BIC comparisons needed in the baseline system, speeds up the system by 41% without affecting the speaker Diarization Error Rate (DER). The result is a robust and faster than real-time speaker diarization system.","Robustness,
Merging,
Error analysis,
Bayesian methods,
Real time systems,
Runtime,
Computer science,
Density estimation robust algorithm,
Iterative methods,
Automatic speech recognition"
Analysis of Computer Intrusions Using Sequences of Function Calls,"This paper demonstrates the value of analyzing sequences of function calls for forensic analysis. Although this approach has been used for intrusion detection (that is, determining that a system has been attacked), its value in isolating the cause and effects of the attack has not previously been shown. We also look for not only the presence of unexpected events but also the absence of expected events. We tested these techniques using reconstructed exploits in su, ssh, and lpr, as well as proof-of-concept code, and, in all cases, were able to detect the anomaly and the nature of the vulnerability.",
New Frontiers of Reverse Engineering,"Comprehending and modifying software is at the heart of many software engineering tasks, and this explains the growing interest that software reverse engineering has gained in the last 20 years. Broadly speaking, reverse engineering is the process of analyzing a subject system to create representations of the system at a higher level of abstraction. This paper briefly presents an overview of the field of reverse engineering, reviews main achievements and areas of application, and highlights key open research issues for the future.","Reverse engineering,
Software maintenance,
Software engineering,
Conferences,
Software testing,
Computer science,
Helium,
Organizing,
Computer Society,
Heart"
A Robust Class of Context-Sensitive Languages,"We define a new class of languages defined by multi-stack automata that forms a robust subclass of context-sensitive languages, with decidable emptiness and closure under boolean operations. This class, called multi-stack visibly pushdown languages (MVPLs), is defined using multi-stack pushdown automata with two restrictions: (a) the pushdown automaton is visible, i.e. the input letter determines the operation on the stacks, and (b) any computation of the machine can be split into k stages, where in each stage, there is at most one stack that is popped. MVPLs are an extension of visibly pushdown languages that captures noncontext free behaviors, and has applications in analyzing abstractions of multithreaded recursive programs, signifi- cantly enlarging the search space that can be explored for them. We show that MVPLs are closed under boolean operations, and problems such as emptiness and inclusion are decidable. We characterize MVPLs using monadic second-order logic over appropriate structures, and exhibit a Parikh theorem for them.","Robustness,
Automata,
Logic,
Computer science,
Application software,
XML,
SGML,
Tree data structures"
Agent-based Trust Model in Wireless Sensor Networks,"Wireless Sensor Networks (WSNs) have an excellent application to monitor environments such as military surveillance and forest fire. However, in many scenarios, WSNs are of interested to adversaries and these sensor nodes are deployed in open and unprotected environments, the security of WSNs is still an important issue. Traditionally cryptographic mechanisms such as authentication and encryption only address part of the problem of security in WSNs. In recent years, the use of reputation and trust systems has become an important mechanism for security in WSNs. In this paper, we develop a distributed agent- based trust model in WSNs. We also discuss some attacks in reputation and trust system, and offered some schemes to defense these attacks. The simulation results and analysis show that our model can detect the malicious nodes fast and accurately and efficiently prevent these attacks in WSNs.","Wireless sensor networks,
Peer to peer computing,
Cryptography,
Authentication,
Security,
Computer science,
Uncertainty,
Monitoring,
Software engineering,
Artificial intelligence"
Novel MAC Layer Handoff Schemes for IEEE 802.11 Wireless LANs,"Handoff in the IEEE 802.11 wireless LANs (WLANs) occurs whenever the mobile station (STA) changes its association from one access point (AP) to another owing to poor link quality. In a WLAN, small coverage areas of APs create frequent handoffs. Previous studies have shown that the typical handoff latency is high enough to hamper the service quality of multimedia streams like VoIP (voice over IP). In this paper, a novel scheme to reduce the handoff latency is proposed by using inter-AP communication to receive the probe response(s). Another adaptive scheme to enhance the quality of service (QoS) for multimedia streams during handoff is presented which builds upon the first scheme and adaptively distributes the total handoff latency. Through extensive simulations, we prove that our adaptive scheme decreases handoff latency significantly and achieves both fast and smooth hand-off that multimedia applications entail.","Wireless LAN,
Delay,
Streaming media,
Quality of service,
Probes,
Mobile computing,
Communications Society,
Laboratories,
Computer science,
Internet telephony"
SYNTHESIS: A Tool for Automatically Assembling Correct and Distributed Component-Based Systems,"SYNTHESIS is a tool for automatically assembling correct and distributed component-based systems. In our context, a system is correct when it is deadlock-free and performs only specified component interactions. In order to automatically synthesize the correct composition code, SYNTHESIS takes as input an high-level behavioural description for each component that must form the system to be built and a specification of the component interactions that must be enforced in the system. The automatically derived composition code is implemented as a set of distributed component wrappers that cooperatively interact with each other and with their wrapped components in order to prevent possible deadlocks and make the composed system exhibit only the specified interactions. The current version of SYNTHESIS supports two possible development platforms: Microsoft COM/DCOM, and EJB (Enterprise Java Beans).","Assembly systems,
System recovery,
Software engineering,
Java,
Computer science,
Business,
Software systems,
Displays,
Cooling,
Collaborative work"
A Graph Modeling of Semantic Similarity between Words,"The problem of measuring the semantic similarity between pairs of words has been considered a fundamental operation in data mining and information retrieval. Nevertheless, developing a computational method capable of generating satisfactory results close to what humans would perceive is still a difficult task somewhat owed to the subjective nature of similarity. In this paper, it is presented a novel algorithm for scoring the semantic similarity (SSA) between words. Given two input words w1and w2, SSA exploits their corresponding concepts, relationships, and descriptive glosses available in WordNet in order to build a rooted weighted graph Gsim. The output score is calculated by exploring the concepts present in Gsim and selecting the minimal distance between any two concepts c1 and c2 of w1 and w2 respectively. The definition of distance is a combination of: 1) the depth of the nearest common ancestor between c1 and c2 in Gsim, 2) the intersection of the descriptive glosses of c1 and c2, and 3) the shortest distance between c1 and c2 in Gsim. A correlation of 0.913 has been achieved between the results by SSA and the human ratings reported by Miller and Charles (1991) for a dataset of 28 pairs of nouns. Furthermore, using the full dataset of 65 pairs presented by Rubenstein and Goodenough (1965), the correlation between SSA results and the known human ratings is 0.903, which is higher than all other reported algorithms for the same dataset. The high correlations of SSA with human ratings suggest that SSA would be convenient in solving several data mining and information retrieval problems.","Humans,
Data mining,
Taxonomy,
Information retrieval,
Clustering algorithms,
Natural languages,
Computer science,
Ontologies,
Speech recognition,
Text recognition"
Locally Excluding a Minor,"We introduce the concept of locally excluded minors. Graph classes locally excluding a minor are a common generalisation of the concept of excluded minor classes and of graph classes with bounded local tree-width. We show that first-order model-checking is fixed-parameter tractable on any class of graphs locally excluding a minor. This strictly generalises analogous results by Flum and Grohe on excluded minor classes and Frick and Grohe on classes with bounded local tree-width. As an important consequence of the proof we obtain fixed-parameter algorithms for problems such as dominating or independent set on graph classes excluding a minor, where now the parameter is the size of the dominating set and the excluded minor. We also study graph classes with excluded minors, where the minor may grow slowly with the size of the graphs and show that again, first-order model-checking is fixed-parameter tractable on any such class of graphs.",
Incremental Learning of Boosted Face Detector,"In recent years, boosting has been successfully applied to many practical problems in pattern recognition and computer vision fields such as object detection and tracking. As boosting is an offline training process with beforehand collected data, once learned, it cannot make use of any newly arriving ones. However, an offline boosted detector is to be exploited online and inevitably there must be some special cases that are not covered by those beforehand collected training data. As a result, the inadaptable detector often performs badly in diverse and changeful environments which are ordinary for many real-life applications. To alleviate this problem, this paper proposes an incremental learning algorithm to effectively adjust a boosted strong classifier with domain-partitioning weak hypotheses to online samples, which adopts a novel approach to efficient estimation of training losses received from offline samples. By this means, the offline learned general-purpose detectors can be adapted to special online situations at a low extra cost, and still retains good generalization ability for common environments. The experiments show convincing results of our incremental learning approach on challenging face detection problems with partial occlusions and extreme illuminations.","Face detection,
Boosting,
Detectors,
Object detection,
Computer vision,
Lighting,
Pattern recognition,
Costs,
Computer science,
Laboratories"
Composing Services with JOLIE,"Service composition and service statefulness are key concepts in Web Service system programming. In this paper we present JOLIE, which is the full implementation of our formal calculus for service orchestration calledSOCK. JOLIE inherits all the formal semantics of SOCK and provides a C-like syntax which allows the programmer to design the service behaviour and the service deployment information separately. The service behaviour is exploited to design the interaction workflow and the computational functionalities of the service, whereas the service deployment information deals with service interface definition, statefulness and service session management. On the one hand, JOLIE offers a simple syntax for dealing with service composition and efficient multiple request processing; on the other hand, it is based on a formal semantics which offers a solid development base, along with the future possibility of creating automated tools for testing system properties such as deadlock freeness.","Web services,
Engines,
Calculus,
Programming profession,
Java,
Games,
Computer science,
Computer interfaces,
Solids,
Automatic testing"
A Service Oriented HLA RTI on the Grid,"Modeling and simulation permeate all areas of business, science and engineering. To promote the interoperability and reusability of simulation applications and link geographically dispersed simulation components, distributed simulation was introduced. While the high level architecture (HLA) is the IEEE standard for distributed simulation, a run time infrastructure (RTI) provides the actual implementation of the HLA. With increased size and complexity of simulation applications, large amounts of distributed computational and data resources are required. The Grid provides a flexible, secure and coordinated resource sharing environment which can facilitate distributed simulation execution. In this paper, we propose a service oriented HLA RTI (SOHR) framework which provides the functionalities of an RTI as Grid services and enables large scale distributed simulations to be conducted on a heterogeneous Grid environment. The various services in SOHR can be dynamically deployed, discovered and undeployed, leading to a scalable distributed simulation environment. While the communications between simulators are through Grid service invocations, the standard HLA interface is provided as a library to increase simulator reusability and interoperability. A subset of HLA specifications was implemented in a SOHR prototype based on GT4 and the experimental results have verified the feasibility of SOHR.","Computational modeling,
Resource management,
Distributed computing,
Computer simulation,
Web services,
Grid computing,
Application software,
Computer architecture,
Middleware,
Wide area networks"
Simultaneous Registration and Parcellation of Bilateral Hippocampal Surface Pairs for Local Asymmetry Quantification,"In clinical applications where structural asymmetries between homologous shapes have been correlated with pathology, the questions of definition and quantification of ""asymmetry"" arise naturally. When not only the degree but the position of deformity is thought relevant, asymmetry localization must also be addressed. Asymmetries between paired shapes have already been formulated in terms of (nonrigid) diffeomorphisms between the shapes. For the infinity of such maps possible for a given pair, we define optimality as the minimization of deviation from isometry under the constraint of piecewise deformation homogeneity. We propose a novel variational formulation for segmenting asymmetric regions from surface pairs based on the minimization of a functional of both the deformation map and the segmentation boundary, which defines the regions within which the homogeneity constraint is to be enforced. The functional minimization is achieved via a quasi-simultaneous evolution of the map and the segmenting curve, conducted on and between two-dimensional surface parametric domains. We present examples using both synthetic data and pairs of left and right hippocampal structures and demonstrate the relevance of the extracted features through a clinical epilepsy classification analysis",
"Endurance Enhancement of Flash-Memory Storage, Systems: An Efficient Static Wear Leveling Design","This work is motivated by the strong demand of reliability enhancement over flash memory. Our objective is to improve the endurance of flash memory with limited overhead and without many modifications to popular implementation designs, such as flash translation layer protocol (FTL) and NAND flash translation layer protocol (NFTL). A static wear leveling mechanism is proposed with limited memory-space requirements and an efficient implementation. The properties of the mechanism are then explored with various implementation considerations. Through a series of experiments based on a realistic trace, we show that the endurance of FTL and NFTL could be significantly improved with limited system overheads.","Flash memory,
Protocols,
Permission,
Computer science,
Reliability engineering,
Design engineering,
Computer network reliability,
Multimedia systems,
Computer network management,
Memory management"
Value Driven Security Threat Modeling Based on Attack Path Analysis,"This paper presents a quantitative threat modeling method, the threat modeling method based on attack path analysis (T-MAP), which quantifies security threats by calculating the total severity weights of relevant attack paths for commercial off the shelf (COTS) systems. Compared to existing approaches, T-MAP is sensitive to an organization's business value priorities and IT environment. It distills the technical details of thousands of relevant software vulnerabilities into management-friendly numbers at a high-level. T-MAP can help system designers evaluate the security performance of COTS systems and analyze the effectiveness of security practices. In the case study, we demonstrate the steps of using T-MAP to analyze the cost-effectiveness of how system patching and upgrades can improve security. In addition, we introduce a software tool that automates the T-MAP","Information security,
Investments,
Environmental economics,
Computer security,
Data security,
Information analysis,
Solid modeling,
Risk analysis,
Business,
Performance analysis"
Discrete Radio Power Level Consumption Model in Wireless Sensor Networks,"Research in the wireless sensor network field has been plagued by difficulties in realistic simulations. These difficulties are often the result of non-realistic assumptions which need to be removed from the equation. Recent work in the field has identified realistic radio consumption models, signal strength estimation and that reception cost may be more than the transmission cost. In our work we combine these techniques into a single model for estimating radio power costs. We also investigate the effects of discrete power levels on transmission cost and show that transmission costs do not always increase as the transmission distance increases.","Wireless sensor networks,
Costs,
Hardware,
Energy consumption,
Anisotropic magnetoresistance,
Equations,
Computer science,
Computational modeling,
Computer simulation,
Signal processing"
Collision detection: A survey,"A process of determining whether two or more bodies are making contact at one or more points is called collision detection or intersection detection. Collision detection is inseparable part of the computer graphics, surgical simulations, and robotics. There are varieties of methods for collision detection. We will review some of the most common ones. Algorithms for contact determination can be grouped into two general parts: broad-phase and narrow-phase. This paper provides a comprehensive classification of a collision detection literature into the two phases. Moreover, we have attempted to explain some of the existing algorithms which are not easy to interpret. Also, we have tried to keep sections self-explanatory without sacrificing depth of coverage.","Testing,
Object detection,
Performance evaluation,
Argon,
Phase detection,
Computational modeling,
Detection algorithms,
Facial animation,
Computer graphics,
Surgery"
Towards Mapping of Cities,"Map learning is a fundamental task in mobile robotics because maps are required for a series of high level applications. In this paper, we address the problem of building maps of large-scale areas like villages or small cities. We present our modified car-like robot which we use to acquire the data about the environment. We introduce our localization system which is based on an information filter and is able to merge the information obtained by different sensors. We furthermore describe out mapping technique that is able to compactly model three-dimensional scenes and allows us efficient and accurate incremental map learning. We additionally apply a global optimization techniques in order to accurately close loops in the environment. Our approach has been implemented and deeply tested on a real car equipped with a series of sensors. Experiments described in this paper illustrate the accuracy and efficiency of the presented techniques.","Cities and towns,
Mobile robots,
Robot sensing systems,
Intelligent sensors,
Large-scale systems,
Layout,
Testing,
Orbital robotics,
Simultaneous localization and mapping,
Laser modes"
Influence of selection and replacement strategies on linkage learning in BOA,"The Bayesian optimization algorithm (BOA) uses Bayesian networks to learn linkages between the decision variables of an optimization problem. This paper studies the influence of different selection and replacement methods on the accuracy of linkage learning in BOA. Results on concatenated m-k deceptive trap functions show that the model accuracy depends on a large extent on the choice of selection method and to a lesser extent on the replacement strategy used. Specifically, it is shown that linkage learning in BOA is more accurate with truncation selection than with tournament selection. The choice of replacement strategy is important when tournament selection is used, but it is not relevant when using truncation selection. On the other hand, if performance is our main concern, tournament selection and restricted tournament replacement should be preferred. These results aim to provide practitioners with useful information about the best way to tune BOA with respect to structural model accuracy and overall performance.","Couplings,
Bayesian methods,
Sampling methods,
Electronic design automation and methodology,
Genetic mutations,
Concatenated codes,
Buildings,
Parameter estimation,
Evolutionary computation,
Probability distribution"
Development of real-time motion artifact reduction algorithm for a wearable photoplethysmography,"This paper presents a motion artifact reduction algorithm for a real-time, wireless and wearable photoplethysmography (PPG) device for measuring heart beats. A wearable finger band PPG device consists of a 3-axis accelerometer, infrared LED, photo diode, a microprocessor and wireless module. Sources of the motion artifacts were investigated from the hand motions, through computing the correlations between the three directional finger motions and distorted PPG signals. A two-dimensional active noise cancellation algorithm was applied to compensate the distorted signals by motions, using the directional accelerometer data. NLMS (Normalized Least Mean Square) adaptive filter (4th order) was employed in the algorithm. As a result, the signals' distortion rates were reduced from 52.34% to 3.53%, at frequencies between 1 and 2.5 Hz, which representing daily motions such walking and jogging. The wearable health monitoring device equipped with the motion artifact reduction algorithm can be integrated as a terminal in a so-called ubiquitous healthcare system, which provides a continuous health monitoring without interrupting a daily life.",
Tracking recurrent concept drift in streaming data using ensemble classifiers,"Streaming data may consist of multiple drifting concepts each having its own underlying data distribution. We present an ensemble learning based approach to handle the data streams having multiple underlying modes. We build a global set of classifiers from sequential data chunks; ensembles are then selected from this global set of classifiers, and new classifiers created if needed, to represent the current concept in the stream. The system is capable of performing any-time classification and to detect concept drift in the stream. In streaming data historic concepts are likely to reappear so we don't delete any of the historic classifiers. Instead, we judiciously select only pertinent classifiers from the global set while forming the ensemble set for a classification task.","Statistics,
Machine learning,
Testing,
Application software,
Computer science,
Data mining,
Data flow computing,
Environmental economics,
Information filtering,
Decision trees"
On joint routing and server selection for MD video streaming in ad hoc networks,"For media streaming in ad hoc networks, service replication has been demonstrated to be a quite effective countermeasure to streaming interruptions caused by fragile paths and dynamic topology. In this paper, we study the problem of joint routing and server selection for double description (DD) video streaming in ad hoc networks. We formulate the task as a combinatorial optimization problem and present tight lower and upper bounds for the achievable distortion. The upper bound provides a feasible solution to the formulated problem. Our extensive numerical results show that the bounds are very close to each other for all the cases studied, indicating the near-global optimality of the derived upper bounding solution. Moreover, we observe significant gains in video quality achieved by the proposed approach over existing server selection schemes. This justifies the importance of jointly considering routing and server selection for optimal MD video streaming","Routing,
Network servers,
Streaming media,
Ad hoc networks,
Web server,
Network topology,
Upper bound,
Quality of service,
Spread spectrum communication,
Computer science"
Miss Rate Prediction Across Program Inputs and Cache Configurations,"Improving cache performance requires understanding cache behavior. However, measuring cache performance for one or two data input sets provides little insight into how cache behavior varies across all data input sets and all cache configurations. This paper uses locality analysis to generate a parameterized model of program cache behavior. Given a cache size and associativity, this model predicts the miss rate for arbitrary data input set sizes. This model also identifies critical data input sizes where cache behavior exhibits marked changes. Experiments show this technique is within 2 percent of the hit rate for set associative caches on a set of floating-point and integer programs using array and pointer-based data structures. Building on the new model, this paper presents an interactive visualization tool that uses a three-dimensional plot to show miss rate changes across program data sizes and cache sizes and its use in evaluating compiler transformations. Other uses of this visualization tool include assisting machine and benchmark-set design. The tool can be accessed on the Web at http://www.cs.rochester.edu/research/locality",
Miche: Modular Shape Formation by Self-Dissasembly,"We describe the design, implementation, and experimentation with a collection of robots that, starting from an amorphous arrangement, can be assembled into arbitrary shapes and then commanded to self-disassemble in an organized manner. Each of the 28 modules in the system is implemented as a 1.8-inch autonomous cube-shaped robot able to connect to and communicate with its immediate neighbors. Two cooperating microprocessors control each module's magnetic connection mechanisms and infrared communication interfaces. When assembled into a structure, the modules form a system that can be virtually sculpted using a computer interface. We report on the hardware design and experiments from hundreds of trials.","Shape,
Robotic assembly,
Amorphous materials,
Robots,
Microprocessors,
Communication system control,
Amorphous magnetic materials,
Optical fiber communication,
Assembly systems,
Computer interfaces"
Peak-Performance DFA-based String Matching on the Cell Processor,"The security of your data and of your network is in the hands of intrusion detection systems, virus scanners and spam filters, which are all critically based on string matching. But network links are getting faster and faster, and string matching is getting more and more difficult to perform in real time. Traditional processors are not keeping up with the performance demands, whereas specialized hardware will never be able to compete with commodity hardware in terms of cost effectiveness, reusability and ease of programming. Advanced multi-core architectures like the IBM Cell Broadband Engine promise unprecedented performance at a low cost, thanks to their popularity and production volume. Nevertheless, the suitability of the cell processor to string matching has not been investigated so far. In this paper we investigate the performance attainable by the cell processor when employed for string matching algorithms based on deterministic finite-state automata (DFA). Our findings show that the cell is an ideal candidate to tackle modern security needs: two processing elements alone, out of the eight available on one cell processor provide sufficient computational power to filter a network link with bit rates in excess of 10 Gbps.","Hardware,
Costs,
Data security,
Intrusion detection,
Matched filters,
Engines,
Production,
Automata,
Doped fiber amplifiers,
Computer networks"
RAPID: Reliable Probabilistic Dissemination in Wireless Ad-Hoc Networks,"In this paper, we propose a novel reliable probabilistic dissemination protocol, RAPID, for mobile wireless ad-hoc networks that tolerates message omissions, node crashes, and selfish behavior. The protocol employs a combination of probabilistic forwarding with deterministic corrective measures. The forwarding probability is set based on the observed number of nodes in each one-hop neighborhood, while the deterministic corrective measures include deterministic gossiping as well as timer based corrections of the probabilistic process. These aspects of the protocol are motivated by a theoretical analysis that is also presented in the paper, which explains why this unique protocol design is inherent to ad-hoc networks environments. Since the protocol only relies on local computations and probability, it is highly resilient to mobility and failures. The paper includes a detailed performance evaluation by simulation. We compare the performance and the overhead of RAPID with the performance of other probabilistic approaches. Our results show that RAPID achieves a significantly higher node coverage with a smaller overhead.","Ad hoc networks,
Broadcasting,
Computer network reliability,
Mobile computing,
Wireless application protocol,
Computational modeling,
Collaboration,
Telecommunication network reliability,
Robustness,
Computer science"
Opportunistic Relaying in Cellular Network for Capacity and Fairness Improvement,"In this paper, we study how the cooperative relaying can improve both capacity and fairness in cellular network. The capacity and fairness have a trade-off relationship, so increasing cell throughput deteriorates fairness and vice versa. First, we show that the achievable average throughput region can be enlarged by using the cooperative relaying. This enlarged region means that capacity and fairness can be improved at the same time with an adequate scheduling algorithm. Thus, secondly we propose a generalized scheduling algorithm for cooperative relaying. The proposed scheduling algorithm can improve both capacity and fairness at the expense of cooperation among users. From simulations, we show that the trade-off relationship can be surpassed and the unfairness problem in the heterogeneous channel condition can be solved by the opportunistic relaying.",
An Integrated Micro-Analytical System for Complex Vapor Mixtures,"A micro gas chromatograph (muGC) capable of quantitatively analyzing the components of complex vapor mixtures at trace concentrations is described. The muGC features a micro- preconcentrator/focuser (muPCF), dual-column pressure- and temperature-programmed separation module, and an integrated array of nanoparticle-coated chemiresistors. The latest design modifications and performance data are presented. Highlights include a 4-min separation of a 30-component mixture with a 3-m DRIE Si/glass microcolumn, a 14-sec separation of an 11-component mixture on a 25-cm microcolumn, a complete multi-vapor analysis from a hybrid microsystem that combines analytical, rf- wireless, and microcontroller modules, and a rapid analysis driven by a 4-stage peristaltic micropump.","Sensor arrays,
Performance analysis,
Glass,
Micropumps,
Biomedical monitoring,
Focusing,
Nanoparticles,
Prototypes,
Solid state circuits,
Intelligent sensors"
Towards Extreme(ly) Usable Software: Exploring Tensions Between Usability and Agile Software Development,"Design is an inherently multidisciplinary endeavor. This raises the question of how to develop systems in ways that can best leverage the perspectives, practices, and knowledge bases of these different areas. Agile software development and usability engineering both address important aspects of system design, but there are tensions between the methods that make them difficult to integrate. This work presents a development approach that draws from extreme programming (XP), a widely practiced agile software development process, and scenario-based design (SBD), an established usability engineering process. It describes three key questions that need to be addressed for agile software development methods and usability engineering practices to work together effectively, and it introduces interface architectures and design representations that can address these questions.","Usability,
Programming,
Design engineering,
Process design,
Computer architecture,
Software systems,
Computer science,
Best practices,
Surges,
Probes"
A Verifiable Language for Programming Real-Time Communication Schedules,"Distributed hard real-time systems require predictable communication at the network level and verifiable communication behavior at the application level. At the network level, communication between nodes must be guaranteed to happen within bounded time and one common approach is to restrict the network access by enforcing a time-division multiple access (TDMA) schedule. At the application level, the application's communication behavior should be verified to ensure that the application uses the predictable communication in the intended way. Network code is a domain-specific programming language to write a predictable verifiable distributed communication for distributed real-time applications. In this paper, we present the syntax and semantics of network code, how we can implement different scheduling policies, and how we can use tools such as model checking to formally verify the properties of network code programs. We also present an implementation of a runtime system for executing network code on top of RTLinux and measure the overhead incurred from the runtime system.","Schedules,
Time division multiple access,
Real time systems,
Runtime,
Timing,
Transceivers,
Computer languages"
Parallel Randomized State-Space Search,"Model checkers search the space of possible program behaviors to detect errors and to demonstrate their absence. Despite major advances in reduction and optimization techniques, state-space search can still become cost-prohibitive as program size and complexity increase. In this paper, we present a technique for dramatically improving the cost- effectiveness of state-space search techniques for error detection using parallelism. Our approach can be composed with all of the reduction and optimization techniques we are aware of to amplify their benefits. It was developed based on insights gained from performing a large empirical study of the cost-effectiveness of randomization techniques in state-space analysis. We explain those insights and our technique, and then show through a focused empirical study that our technique speeds up analysis by factors ranging from 2 to over 1000 as compared to traditional modes of state-space search, and does so with relatively small numbers of parallel processors.","Costs,
Java,
Concurrent computing,
Error correction,
Computer science,
Computer errors,
Parallel processing,
Performance analysis,
Logic,
Data structures"
Learning Log Explorer in E-Learning Diagnosis,"This study presents a learning behavior diagnosis system to study students' learning status from learning portfolios. The proposed linking layer enables the proposed system to work on various e-learning platforms without reprogramming. Additionally, the use of a supervisory agent enables teachers and students to obtain their learning status or information provided by the proposed system in both Web and e-mail. Furthermore, the computer engineering curriculum operating systems was adopted to evaluate the proposed system. Evaluations of confidence between learning status and learning achievement yield positive experimental results.","Joining processes,
Electronic learning,
Computers,
Real time systems"
Molecular Surface Abstraction,"In this paper we introduce a visualization technique that provides an abstracted view of the shape and spatio-physico-chemical properties of complex molecules. Unlike existing molecular viewing methods, our approach suppresses small details to facilitate rapid comprehension, yet marks the location of significant features so they remain visible. Our approach uses a combination of filters and mesh restructuring to generate a simplified representation that conveys the overall shape and spatio-physico-chemical properties (e.g. electrostatic charge). Surface markings are then used in the place of important removed details, as well as to supply additional information. These simplified representations are amenable to display using stylized rendering algorithms to further enhance comprehension. Our initial experience suggests that our approach is particularly useful in browsing collections of large molecules and in readily making comparisons between them.","Shape,
Visualization,
Displays,
Proteins,
Electrostatics,
Surface texture,
Chemical processes,
Filters,
Mesh generation,
Labeling"
Preference-Aware Query and Update Scheduling in Web-databases,"Typical Web-database systems receive read-only queries, that generate dynamic Web pages as a response, and write-only updates, that keep information up-to-date. Users expect short response times and low staleness. However, it may be extremely hard to apply all updates on time, i.e., keep zero staleness, and also get fast response times, especially in periods of bursty traffic. In this paper, we present the concept of quality contracts (QCs) which combines the two incomparable performance metrics: response time or quality of service (QoS), and staleness or quality of data (QoD). QCs allows individual users to express their preferences for the expected QoS and QoD of their queries by assigning ""profit"" values. To maximize the total profit from submitted QCs, we propose an adaptive algorithm, called QUTS. QUTS addresses the problem of prioritizing the scheduling of updates over queries using a two-level scheduling scheme that dynamically allocates CPU resources to updates and queries according to user preferences. We present the results of an extensive experimental study using real data (taken from a stock information Web site), where we show that QUTS performs better than baseline algorithms under the entire spectrum of QCs; QUTS also adapts fast to changing workloads.","Delay,
Dynamic scheduling,
Measurement,
Quality of service,
Databases,
Weather forecasting,
Processor scheduling,
Technology management,
Laboratories,
Computer science"
Test Case Prioritization for Black Box Testing,"Test case prioritization is an effective and practical technique that helps to increase the rate of regression fault detection when software evolves. Numerous techniques have been reported in the literature on prioritizing test cases for regression testing. However, existing prioritization techniques implicitly assume that source or binary code is available when regression testing is performed, and therefore cannot be implemented when there is no program source or binary code to be analyzed. In this paper, we presented a new technique for black box regression testing, and we performed an experiment to measure our technique. Our results show that the new technique is helpful to improve the effectiveness of fault detection when performing regression test in black box environment.","Fault detection,
Software testing,
Binary codes,
Performance evaluation,
History,
Computer science,
Runtime,
Performance analysis,
Life testing,
Sorting"
Efficient Arguments without Short PCPs,"Current constructions of efficient argument systems combine a short (polynomial size) PCP with a cryptographic hashing technique. We suggest an alternative approach for this problem that allows to simplify the underlying PCP machinery using a stronger cryptographic technique. More concretely, we present a direct method for compiling an exponentially long PCP which is succinctly described by a linear oracle function \pi : F^n \to F into an argument system in which the verifier sends to the prover O(n) encrypted field elements and receives O(1) encryptions in return. This compiler can be based on an arbitrary homomorphic encryption scheme. Applying our general compiler to the exponential size Hadamard code based PCP of Arora et al. (JACM 1998) yields a simple argument system for NP in which the communication from the prover to the verifier only includes a constant number of short encryptions. The main tool we use is a new cryptographic primitive which allows to efficiently commit to a linear function and later open the output of the function on an arbitrary vector. Our efficient implementation of this primitive is independently motivated by cryptographic applications.","Cryptography,
Machinery,
Polynomials,
Complexity theory,
Computational complexity,
Computer science,
Error correction,
Error correction codes,
Vectors,
Technological innovation"
Cost-Conscious Cleaning of Massive RFID Data Sets,"Efficient and accurate data cleaning is an essential task for the successful deployment of RFID systems. Although important advances have been made in tag detection rates, it is still common to see a large number of lost readings due to radio frequency (RF) interference and tag-reader configurations. Existing cleaning techniques have focused on the development of accurate methods that work well under a wide set of conditions, but have disregarded the very high cost of cleaning in a real application that may have thousands of readers and millions of tags. In this paper, we propose a cleaning framework that takes an RFID data set and a collection of cleaning methods, with associated costs, and induces a cleaning plan that optimizes the overall accuracy-adjusted cleaning costs by determining the conditions under which inexpensive methods are appropriate, and those under which more expensive methods are absolutely necessary.","Cleaning,
Radiofrequency identification,
Cost function,
Radio frequency,
Error correction,
Computer science,
Radiofrequency interference,
Optimization methods,
Appropriate technology,
Pharmaceutical technology"
A Simulation-Based Methodology for Evaluating the DPA-Resistance of Cryptographic Functional Units with Application to CMOS and MCML Technologies,"This paper explores the resistance of MOS current mode logic (MCML) against differential power analysis (DPA) attacks. Circuits implemented in MCML, in fact, have unique characteristics both in terms of power consumption and the dependency of the power profile from the input signal pattern. Therefore, MCML is suitable to protect cryptographic hardware from DPA and similar side-channel attacks. In order to demonstrate the effectiveness of different logic styles against power analysis attacks, the non-linear bijective function of the Kasumi algorithm (known as substitution box S7) was implemented with CMOS and MCML technology, and a set of attacks was performed using power traces derived from SPICE-level simulations. Although all keys were discovered for CMOS, only very few attacks to MCML were successful.","CMOS technology,
Cryptography,
CMOS logic circuits,
Circuit simulation,
Energy consumption,
Protection,
Hardware,
Performance analysis,
Algorithm design and analysis,
Analytical models"
Towards a Real-Time Minimally-Invasive Vascular Intervention Simulation System,"Recently, foundations rooted in physics have been laid down for the goal of simulating the propagation of a guide wire inside the vasculature. At the heart of the simulation lies the fundamental task of energy minimization. The energy comes from interaction with the vessel wall and the bending of the guide wire. For the simulation to be useful in actual training, obtaining the smallest possible optimization time is key. In this paper, we, therefore, study the influence of using different optimization techniques: a semianalytical approximation algorithm, the conjugate-gradients algorithm, and an evolutionary algorithm (EA), specifically the GLIDE algorithm. Simulation performance has been measured on phantom data. The results show that a substantial reduction in time can be obtained while the error is increased only slightly if conjugate gradients or GLIDE is used","Real time systems,
Wire,
Computational modeling,
Approximation algorithms,
Image segmentation,
Evolutionary computation,
Biomedical imaging,
Physics,
Heart,
Imaging phantoms"
Designing Tool Support for Translating Use Cases and UML 2.0 Sequence Diagrams into a Coloured Petri Net,"Using a case study on the specification of an elevator controller, this paper presents an approach that can translate given UML descriptions into a Coloured Petri Net (CPN) model. The UML descriptions must be specified in the form of Use Cases and UML 2.0 Sequence Diagrams. The CPN model constitutes one single, coherent and executable representation of all possible behaviours that are specified by the given UML artefacts. CPNs consitute a formal modelling language that enables construction and analysis of scalable, executable models of behaviour. A combined use of UML and CPN can be useful in several projects. CPN is well supported by CPN Tools and the work we present here is aimed at building a CPN Tools front-end engine that implements the proposed translation.","Unified modeling language,
Floors,
Elevators,
Buildings,
Concurrent computing,
Computer science,
Engines,
Feedback,
Optimized production technology,
Software standards"
Optimization of Robust Asynchronous Circuits by Local Input Completeness Relaxation,"As process, temperature and voltage variations become significant in deep submicron design, timing closure becomes a critical challenge using synchronous CAD flows. One attractive alternative is to use robust asynchronous circuits which gracefully accommodate timing discrepancies. However, these asynchronous circuits typically suffer from high area and latency overhead. In this paper, an optimization algorithm is presented which reduces the area and delay of these circuits by relaxing their overly-restrictive style. The algorithm was implemented and experiments performed on a subset of MCNC circuits. On average, 49.2% of the gates could be implemented in a relaxed manner, 34.9% area improvement was achieved, and 16.1% delay improvement was achieved using a simple heuristic for targeting the critical path in the circuit. This is the first proposed approach that systematically optimizes asynchronous circuits based on the notion of local relaxation while still preserving the circuit's overall timing-robustness.","Robustness,
Asynchronous circuits,
Delay,
Timing,
Design automation,
Cost function,
Temperature,
Logic circuits,
Voltage,
Electromagnetic interference"
An Effective P2P Search Scheme to Exploit File Sharing Heterogeneity,"Although the original intent of the peer-to-peer (P2P) concept is to treat each participant equally, heterogeneity widely exists in deployed P2P networks. Peers are different from each other in many aspects, such as bandwidth, CPU power, and storage capacity. Some approaches have been proposed to take advantage of the query forwarding heterogeneity such that the high bandwidth of powerful nodes can be fully utilized to maximize the system capacity. In this paper, we suggest using the query answering heterogeneity to directly improve the search efficiency of P2P networks. In our proposed differentiated search (DiffSearch) algorithm, the peers with high query answering capabilities will have higher priority to be queried. Because the query answering capabilities are extremely unbalanced among peers, a high query success rate can be achieved by querying only a small portion of a network. The search traffic is significantly reduced due to the shrunken search space. Our trace analysis and simulation show that the DiffSearch algorithm can save up to 60 percent of search traffic","Peer to peer computing,
Telecommunication traffic,
Costs,
Bandwidth,
Traffic control,
Indexing,
Algorithm design and analysis,
Analytical models,
Fault tolerant systems,
Scalability"
Robotic etiquette: Results from user studies involving a fetch and carry task,"This paper presents results, outcomes and conclusions from a series of Human Robot Interaction (HRI) trials which investigated how a robot should approach a human in a fetch and carry task. Two pilot trials were carried out, aiding the development of a main HRI trial with four different approach contexts under controlled experimental conditions. The findings from the pilot trials were confirmed and expanded upon. Most subjects disliked a frontal approach when seated. In general, seated humans do not like to be approached by a robot directly from the front even when seated behind a table. A frontal approach is more acceptable when a human is standing in an open area. Most subjects preferred to be approached from either the left or right side, with a small overall preference for a right approach by the robot. However, this is not a strong preference and it may be disregarded if it is more physically convenient to approach from a left front direction. Handedness and occupation were not related to these preferences. Subjects do not usually like the robot to move or approach from directly behind them, preferring the robot to be in view even if this means the robot taking a physically non-optimum path. The subjects for the main HRI trials had no previous experience of interacting with robots. Future research aims are outlined and include the necessity of carrying out longitudinal trials to see if these findings hold over a longer period of exposure to robots.","Robots,
Abstracts"
Design Methodology for Pipelined Heterogeneous Multiprocessor System,"Multiprocessor SoC systems have led to the increasing use of parallel hardware along with the associated software. These approaches have included coprocessor, homogeneous processor (e.g. SMP) and application specific architectures (i.e. DSP, ASIC). ASIPs have emerged as a viable alternative to conventional processing entities (PEs) due to its configurability and programmability. In this work, we introduce a heterogeneous multi-processor system using ASIPs as processing entities in a pipeline configuration. A streaming application is taken and manually broken into a series of algorithmic stages (each of which make up a stage in a pipeline). We formulate the problem of mapping each algorithmic stage in the system to an ASIP configuration, and propose a heuristic to efficiently search the design space for a pipeline-based multi ASIP system. We have implemented the proposed heterogeneous multiprocessor methodology using a commercial extensible processor (Xtensa LX from Tensilica Inc.). We have evaluated our system by creating two benchmarks (MP3 and JPEG encoders) which are mapped to our proposed design platform. Our multiprocessor design provided a performance improvement of at least 4.1 IX (JPEG) and 3.36X (MP3) compared to the single processor design. The minimum cost obtained through our heuristic was within 5.47% and 5.74% of the best possible values for JPEG and MP3 benchmarks respectively.","Design methodology,
Multiprocessing systems,
Application specific processors,
Digital audio players,
Pipelines,
Hardware,
Coprocessors,
Application software,
Computer architecture,
Digital signal processing"
Limitations of Equation-Based Congestion Control,"We study limitations of an equation-based congestion control protocol, called TCP-friendly rate control (TFRC). It examines how the three main factors that determine TFRC throughput, namely, the TCP-friendly equation, loss event rate estimation, and delay estimation, can influence the long-term throughput imbalance between TFRC and TCP. Especially, we show that different sending rates of competing flows cause these flows to experience different loss event rates. There are several fundamental reasons why TFRC and TCP flows have different average sending rates, from the first place. Earlier work shows that the convexity of the TCP-friendly equation used in TFRC causes the sending rate difference. We report two additional reasons in this paper: 1) the convexity of where is a loss event period and 2) different retransmission timeout period (RTO) estimations of TCP and TFRC. These factors can be the reasons for TCP and TFRC to experience initially different sending rates. But we find that the loss event rate difference due to the differing sending rates greatly amplifies the initial throughput difference; in some extreme cases, TFRC uses around 20 times more, or sometimes 10 times less, bandwidth than TCP. Despite these factors influencing the throughput difference, we also find that simple heuristics can greatly mitigate the problem.","Throughput,
Delay estimation,
Streaming media,
Computer science,
Protocols,
Difference equations,
Bandwidth,
Internet,
Engineering profession,
Upper bound"
An Optimal Flow Assignment Framework for Heterogeneous Network Access,"We consider a scenario where devices with multiple networking capabilities access networks with heterogeneous characteristics. In such a setting, we address the problem of efficient utilization of multiple access networks (wireless and/or wireline) by devices via optimal assignment of traffic flows with given utilities to different networks. We develop and analyze a device middleware functionality that monitors network characteristics and employs a Markov Decision Process (MDP) based control scheme that in conjunction with stochastic characterization of the available bit rate and delay of the networks generates an optimal policy for allocation of flows to different networks. The optimal policy maximizes, under available bit rate and delay constraints on the access networks, a discounted reward which is a function of the flow utilities. The flow assignment policy is periodically updated and is consulted by the flows to dynamically perform network selection during their lifetimes. We perform measurement tests to collect traces of available bit rate and delay characteristics on Ethernet and WLAN networks on a work day in a corporate work environment. We implement our flow assignment framework in ns-2 and simulate the system performance for a set of elastic video-like flows using the collected traces. We demonstrate that the MDP based flow assignment policy leads to significant enhancement in the QoS provisioning (lower packet delays and packet loss rates) for the flows, as compared to policies which do not perform dynamic flow assignment but statically allocate flows to different networks using heuristics like average available bit rate on the networks.","Bit rate,
Communication system traffic control,
Middleware,
Optimal control,
Process control,
Stochastic processes,
Character generation,
Performance evaluation,
Testing,
Ethernet networks"
Very Low-Complexity Hardware Interleaver for Turbo Decoding,"This brief presents a very low complexity hardware interleaver implementation for turbo code in wideband CDMA (W-CDMA) systems. Algorithmic transformations are extensively exploited to reduce the computation complexity and latency. Novel VLSI architectures are developed. The hardware implementation results show that an entire turbo interleave pattern generation unit consumes only 4 k gates, which is an order of magnitude smaller than conventional designs.","Hardware,
Decoding,
Turbo codes,
Multiaccess communication,
Very large scale integration,
Delay,
Computer architecture,
Circuits,
Computer science,
Digital signal processing"
Mining Personally Important Places from GPS Tracks,"The discovery of a person's personally important places involves obtaining the physical locations for a person's places that matter to his daily life and routines. This problem is driven by the requirements from emerging location-aware applications, which allow a user to pose queries awl obtain, information in reference, to places, e.g., ''home"", ''work"" or ''Northwest Health Club"". It is a challenge to map from physical locations to jxtrsonally meaningful places because GPS tracks are continuous data both spatially and temporally, while most existing data mining techniques expect discrete data. Previous work has explored algorithms to discover personal places from location data. However, they all have limitations. Our work proposes a two-step approach that discretized continuous GPS data into places and learns important places from the place features. Our approach was validated using real user data and shown to have good accuracy when applied in predicting not only important and frequent places, but also important and not so frequent places.","Global Positioning System,
Data mining,
Educational institutions,
Urban areas,
Clustering algorithms,
Computer science,
Motion pictures,
Rail transportation,
Partitioning algorithms,
Frequency"
A Generalized Bose-Chowla Family of Optical Orthogonal Codes and Distinct Difference Sets,"A new construction of optical orthogonal codes is provided in this correspondence which is a generalization of the well-known construction of distinct difference set (DDS) by Bose and Chowla. This construction is optimal with respect to the Johnson bound and has parameters n=qa-1, omega=q, and lambda=1","Multiaccess communication,
Optical fiber networks,
Upper bound,
Optical control,
Hamming weight,
Councils,
Information theory,
Computer science,
Binary codes,
Modular construction"
Biologically-inspired robotics vision monte-carlo localization in the outdoor environment,"We present a robot localization system using biologically-inspired vision. Our system models two extensively studied human visual capabilities: (1) extracting the ""gist"" of a scene to produce a coarse localization hypothesis, and (2) refining it by locating salient landmark regions in the scene. Gist is computed here as a holistic statistical signature of the image, yielding abstract scene classification and layout. Saliency is computed as a measure of interest at every image location, efficiently directing the time-consuming landmark identification process towards the most likely candidate locations in the image. The gist and salient landmark features are then further processed using a Monte-Carlo localization algorithm to allow the robot to generate its position. We test the system in three different outdoor environments - building complex (126times180 ft. area, 3794 testing images), vegetation-filled park (270times360 ft. area, 7196 testing images), and open-field park (450times585 ft. area, 8287 testing images) - each with its own challenges. The system is able to localize, on average, within 6.0, 10.73, and 32.24 ft., respectively, even with multiple kidnapped-robot instances.","Robot vision systems,
Humans,
Layout,
System testing,
Neuroscience,
Intelligent sensors,
Intelligent robots,
Robot localization,
Robustness,
Global Positioning System"
An Indoor Localization Aid for the Visually Impaired,"This paper presents an indoor human localization system for the visually impaired. A prototype portable device has been implemented, consisting of a pedometer and a standard white cane, on which a laser range finder and a 3-axis gyroscope have been mounted. A novel pose estimation algorithm has been developed for robustly estimating the heading and position of a person navigating in a known building. The basis of our estimation scheme is a two-layered extended Kalman filter (EKF) for attitude and position estimation. The first layer maintains an attitude estimate of the white cane, which is subsequently provided to the second layer where a position estimate of the user is generated. Experimental results are presented that demonstrate the reliability of the proposed method for accurate, real-time human localization.","Humans,
Velocity measurement,
Navigation,
Gyroscopes,
Sensor systems,
Sensor fusion,
Electronics packaging,
Wearable sensors,
Tactile sensors,
Robotics and automation"
An EM Technique for Multiple Transmitter Localization,"We propose an expectation-maximization (EM) technique for locating multiple transmitters based on power levels observed by a set of arbitrarily-placed receivers. Multiple transmitter localization is of interest for uncoordinated cognitive radio systems, which must identify and transmit over unused radio spectrum without cooperation from conventional transmitters. We employ the EM algorithm to reduce the dimensionality of the maximum-likelihood estimation problem. Because the EM algorithm finds only a locally optimal solution, we explore the use of clustering to generate ""smart"" initial estimates of the transmitter locations. Simulation results show that, as the number of sensors increases, the proposed EM technique achieves gains of up to an order of magnitude over constricted particle swarm optimization, a popular global optimization technique.","Cognitive radio,
Radio transmitters,
Radiofrequency identification,
Bandwidth,
Frequency,
Receivers,
Clustering algorithms,
Wireless sensor networks,
Interference,
Power measurement"
A Spatial Backoff Algorithm Using the Joint Control of Carrier Sense Threshold and Transmission Rate,"Traditional medium access control (MAC) protocols utilize temporal mechanisms such as access probability or backoff interval adaptation for contention resolution. Temporal contention resolution aims to separate transmissions from different nodes in time to achieve successful transmissions. We explore an alternative approach for wireless networks - named ""spatial backoff - that adapts the ""space"" occupied by the transmissions. By adapting the space occupied by transmissions, the set of ""locally"" competing nodes, and thus, the channel contention level, can be adjusted to reach a suitable level. There are different ways to realize spatial backoff. In this paper, we propose a dynamic spatial backoff algorithm using the joint control of carrier sense threshold and transmission rate. Our results suggest that spatial backoff is promising to improve the channel utilization.","Spatial resolution,
Peer to peer computing,
Media Access Protocol,
Access protocols,
Wireless networks,
Communication system control,
Communications technology,
Heuristic algorithms,
Aggregates,
Throughput"
A Framework for Empirical Evaluation of Model Comprehensibility,"If designers of modelling languages want their creations to be used in real software projects, the communication qualities of their languages need to be evaluated, and their proposals must evolve as a result of these evaluations. A key quality of communication artifacts is their comprehensibility. We present a flexible framework to evaluate the comprehensibility of model representations that is grounded on the underlying theory of the language to be evaluated, and on theoretical frameworks in cognitive science.","Software quality,
Software engineering,
Cognitive science,
Decoding,
Guidelines,
Software tools,
Computer industry,
Communication effectiveness,
Costs,
Production"
Resilient network codes in the presence of eavesdropping Byzantine adversaries,"Network coding can substantially improve network throughput and performance. However, these codes have a major drawback if the network contains hidden malicious nodes that can eavesdrop on transmissions and inject fake information. In this scenario, even a small amount of information injected by a single malicious hidden node could mix with and contaminate much of the information inside the network, causing a decoding error. We improve on previous work by providing a polynomial- time, rate-optimal distributed network code design that functions even in the presence of a Byzantine adversary with substantial eavesdropping capabilities. As long as the sum of the adversary's jamming rate Zo and his eavesdropping rate ZI is less than the network capacity C, (Zo + ZI < C), our codes attain the optimal rate of C - Zo. The network codes we design are information-theoretically secure and assume no knowledge of network topology. Prior to transmission, no honest node knows the location or strength of the adversary. In our code design, interior nodes are oblivious to the presence of adversaries and implement a classical low- complexity distributed network code design; only the source and destination need to be changed. Finally, our codes work for both wired and wireless networks.","Jamming,
Network coding,
Throughput,
Decoding,
Wireless networks,
Computer science,
Network topology,
Robustness,
Error correction codes,
Algorithm design and analysis"
A Critique of Mobility Models for Wireless Network Simulation,"Simulation is universally considered the most effective method of designing and evaluating new network protocols. When developing protocols for mobile networking, the chosen mobility model is one of the key determinants in the success of an accurate simulation. The main role of a mobility model is to mimic the movement behaviors of actual users. Several mobility models, with widely differing characteristics, are employed in contemporary simulation-supported research. Some are simple but far from representative of real user movement patterns, while others provide more complex and realistic modeling. Given the critical role of the mobility model in supporting realistic and accurate protocol simulations, its correct design and selection is essential. In this paper we critique a number of recent mobility models.","Wireless networks,
Computational modeling,
History,
Computer simulation,
Wireless application protocol,
Computer science,
Software engineering,
Design methodology,
Network topology,
Testing"
Visualizing Vitreous Using Quantum Dots as Imaging Agents,"Vitreous is transparent tissue located between the lens and the retina of the eye, thus, difficult to look at by even ophthalmological microscope. But vitreous is connected with some sight-threatening eye diseases, for example, retinal detachment, macular hole, epi-retinal membrane, and so forth. Quantum dots (QDs) have been applied to a wide range of biological studies by taking advantage of their fluorescence properties. We established a novel technique of aqueous colloidal QD (ACQD) as a vitreous lesion detector. When compared with some conventional dyes used for clinical situation, i.e. fluorescein, indocyanine green, and triamcinolone acetonide, ACQD exerted a higher performance to detect a Weiss Ring. Furthermore ACQD is also effective to perform vitrectomy, an eye surgery to cut and eliminate vitreous. Some functional structures in vitreous are detected clearly when ACQD was injected into an enucleated porcine eye. We demonstrated that ACQD enabled any ophthalmic surgeon to perform vitrectomy reliably, easily, and more safely. Taken together, the ACQD-oriented vitreous staining system will promote ophthalmological science, and it will raise the cure rate of eye diseases","Visualization,
Quantum dots,
Retina,
Diseases,
Lenses,
Microscopy,
Biomembranes,
Fluorescence,
Lesions,
Detectors"
1|A Kalman filter-based algorithm for IMU-camera calibration,"Vision-aided Inertial Navigation Systems (V-INS) can provide precise state estimates for the 3D motion of a vehicle when no external references (e.g., GPS) are available. This is achieved by combining inertial measurements from an IMU with visual observations from a camera under the assumption that the rigid transformation between the two sensors is known. Errors in the IMU-camera calibration process causes biases that reduce the accuracy of the estimation process and can even lead to divergence. In this paper, we present a Kalman filter-based algorithm for precisely determining the unknown transformation between a camera and an IMU. Contrary to previous approaches, we explicitly account for the time correlations of the IMU measurements and provide a figure of merit (covariance) for the estimated transformation. The proposed method does not require any special hardware (such as spin table or 3D laser scanner) except a calibration target. Simulation and experimental results are presented that validate the proposed method and quantify its accuracy.","Kalman filters,
Calibration,
Cameras,
Inertial navigation,
Motion estimation,
State estimation,
Vehicles,
Global Positioning System,
Time measurement,
Hardware"
Novel Techniques for Fast Torque Response of IPMSM Based on Space-Vector Control Method in Voltage Saturation Region,"In this paper, we propose two new voltage calculation methods for fast torque response of IPMSM in voltage saturation region, which are maximum torque response method and constant current response ratio method. Compared with the usual methods, which are constant phase angle method, constant back emf method and constant d-axis voltage method, the proposed methods give faster torque response confirmed by simulation and experimental results.",
On Searching Continuous k Nearest Neighbors in Wireless Data Broadcast Systems,"A continuous nearest neighbor (CNN) search, which retrieves the nearest neighbors corresponding to every point in a given query line segment, is important for location-based services such as vehicular navigation and tourist guides. It is infeasible to answer a CNN search by issuing a traditional nearest neighbor query at every point of the line segment due to the large number of queries generated and the overhead on bandwidth. Algorithms have been proposed recently to support CNN search in the traditional client- server systems but not in the environment of wireless data broadcast, where uplink communication channels from mobile devices to the server are not available. In this paper, we develop a generalized search algorithm for continuous k-nearest neighbors based on Hilbert Curve Index in wireless data broadcast systems. A performance evaluation is conducted to compare the proposed search algorithms with an algorithm based on R-tree Air Index. The result shows that the Hilbert Curve Index-based algorithm is more energy efficient than the R-tree-based algorithm.","Nearest neighbor searches,
Cellular neural networks,
Information retrieval,
Bandwidth,
Navigation,
Job shop scheduling,
Network servers,
Radio broadcasting,
Communication channels,
Energy efficiency"
Hot-Spot Avoidance With Multi-Pathing Over InfiniBand: An MPI Perspective,"Large scale InfiniBand clusters are becoming increasingly popular, as reflected by the TOP 500 supercomputer rankings. At the same time, fat tree has become a popular interconnection topology for these clusters, since it allows multiple paths to be available in between a pair of nodes. However, even with fat tree, hot-spots may occur in the network depending upon the route configuration between end nodes and communication pattern(s) in the application. To make matters worse, the deterministic routing nature of InfiniBand limits the application from effective use of multiple paths transparently and avoid the hot-spots in the network. Simulation based studies for switches and adapters to implement congestion control have been proposed in the literature. However, these studies have focussed on providing congestion control for the communication path, and not on utilizing multiple paths in the network for hot-spot avoidance. In this paper, we design an MPI functionality, which provides hot-spot avoidance for different communications, without a priori knowledge of the pattern. We leverage LMC (LID mask count) mechanism of InfiniBand to create multiple paths in the network and present the design issues (scheduling policies, selecting number of paths, scalability aspects) of our design. We implement our design and evaluate it with Pallas collective communication and MPI applications. On an InfiniBand cluster with 48 processes, MPI All-to-all personalized shows an improvement of 27%. Our evaluation with NAS parallel benchmarks on 64 processes shows significant improvement in execution time with this functionality.","Communication system control,
Large-scale systems,
Supercomputers,
Network topology,
Routing,
Switches,
Communication switching,
Scalability,
Sun,
Computer science"
Structure-Based Statistical Features and Multivariate Time Series Clustering,"We propose a new method for clustering multivariate time series. A univariate time series can be represented by a fixed-length vector whose components are statistical features of the time series, capturing the global structure. These descriptive vectors, one for each component of the multivariate time series, are concatenated, before being clustered using a standard fast clustering algorithm such as k-means or hierarchical clustering. Such statistical feature extraction also serves as a dimension-reduction procedure for multivariate time series. We demonstrate the effectiveness and simplicity of our proposed method by clustering human motion sequences: dynamic and high-dimensional multivariate time series. The proposed method based on univariate time series structure and statistical metrics provides a novel, yet simple and flexible way to cluster multivariate time series data efficiently with promising accuracy. The success of our method on the case study suggests that clustering may be a valuable addition to the tools available for human motion pattern recognition research.",
An FPGA Implementation of Decision Tree Classification,"Data mining techniques are a rapidly emerging class of applications that have widespread use in several fields. One important problem in data mining is classification, which is the task of assigning objects to one of several predefined categories. Among the several solutions developed, decision tree classification (DTC) is a popular method that yields high accuracy while handling large datasets. However, DTC is a computationally intensive algorithm, and as data sizes increase, its running time can stretch to several hours. In this paper, we propose a hardware implementation of decision tree classification. We identify the compute-intensive kernel (Gini score computation) in the algorithm, and develop a highly efficient architecture, which is further optimized by reordering the computations and by using a bitmapped data structure. Our implementation on a Xilinx Virtex-II Pro FPGA platform (with 16 Gini units) provides up to 5.58times performance improvement over an equivalent software implementation",
Generalized Tardiness Bounds for Global Multiprocessor Scheduling,"We consider the issue of deadline tardiness under global multiprocessor scheduling algorithms. We present a general tardiness-bound derivation that is applicable to a wide variety of such algorithms (including some whose tardiness behavior has not been analyzed before). Our derivation is very general: job priorities may change rather arbitrarily at runtime, arbitrary non-preemptive regions are allowed, and capacity restrictions may exist on certain processors. Our results show that, with the exception of static-priority algorithms, most global algorithms considered previously have bounded tardiness. In addition, our results provide a simple means for checking whether tardiness is bounded under newly-developed algorithms.","Processor scheduling,
Scheduling algorithm,
Partitioning algorithms,
Algorithm design and analysis,
Heuristic algorithms,
Multicore processing,
Real time systems,
Computer science,
Runtime,
Multiprocessing systems"
Adaptive Clock Gating Technique for Low Power IP Core in SoC Design,"Clock gating is a well-known technique to reduce chip dynamic power. This paper analyzes the disadvantages of some recent clock gating techniques and points out that they are difficult in system-on-chip (SoC) design. Based on the analysis of the intellectual property (IP) core model, an adaptive clock gating (ACG) technique which can be easily realized is introduced for the low power IP core design. ACG can automatically enable or disable the IP clock to reduce not only dynamic power but also leakage power with power gating technique. The experimental results on some IP cores in a real SoC show an average of 62.2% dynamic power reduction and 70.9% leakage power reduction without virtually performance impact.","Clocks,
Circuits,
Pipelines,
Power dissipation,
Process design,
Computer architecture,
System-on-a-chip,
Intellectual property,
Power system modeling,
Portable computers"
A Pattern System for Security Requirements Engineering,"We present a pattern system/or security requirements engineering, consisting of security problem frames and concretized security problem frames. These are special kinds of problem frames that serve to structure, characterize, analyze, and finally solve software development problems in the area of software and system security. We equip each frame with formal preconditions and postconditions. The analysis of these conditions results in a pattern system that explicitly shows the dependencies between the different frames. Moreover, we indicate related frames, which are commonly used together with the considered frame. Hence, our approach helps security engineers to avoid omissions and to cover all security requirements that are relevant for a given problem","Pattern analysis,
Data security,
Computer security,
Software systems,
Software engineering,
Programming,
Authentication,
Reliability engineering,
Computer science,
Instruments"
Reducing Connection Memory Requirements of MPI for InfiniBand Clusters: A Message Coalescing Approach,"Clusters in the area of high-performance computing have been growing in size at a considerable rate. In these clusters, the dominate programming model is the Message Passing Interface (MPI), so the MPI library has a key role in resource usage and performance. To obtain maximal performance, many clusters deploy a high-speed interconnect between compute nodes. One such interconnect, InfiniBand, has been gaining in popularity due to its various features including Remote Data Memory Access (RDMA), and high-performance. As a result, it is being deployed in a significant number of clusters and has been chosen as the standard interconnect for capacity clusters within the DOE Tri-Labs. As these clusters grow in size, care must be taken to ensure the resource usage does not increase too significantly with scale. In particular, the MPI library resource usage should not grow at a rate which will exhaust the node memory or starve user applications. In this paper we present our findings of current memory usage when all connections are created and design a message coalescing method to decrease memory usage significantly. Our models show that the default configuration of MVAPICH can grow to 1GB per process for 8K processes, while our enhancements reduce usage by an order of magnitude to around 120 MB per process while maintaining near-equal performance. We have validated our design on a 575-node cluster and shown no performance degradation for a variety of applications. We also increase the message rate attainable by over 150%.","Libraries,
Laboratories,
Computer networks,
Message passing,
US Department of Energy,
Degradation,
Scalability,
Computer science,
High performance computing,
Writing"
ParalleX: A Study of A New Parallel Computation Model,"This paper proposes the study of a new computation model that attempts to address the underlying sources of performance degradation (e.g. latency, overhead, and starvation) and the difficulties of programmer productivity (e.g. explicit locality management and scheduling, performance tuning, fragmented memory, and synchronous global barriers) to dramatically enhance the broad effectiveness of parallel processing for high end computing. In this paper, we present the progress of our research on a parallel programming and execution model - mainly, ParalleX. We describe the functional elements of ParalleX, one such model being explored as part of this project. We also report our progress on the development and study of a subset of ParalleX $the LITL-X at University of Delaware. We then present a novel architecture model - Gilgamesh II - as a ParalleX processing architecture. A design point study of Gilgamesh II and the architecture concept strategy are presented.","Concurrent computing,
Computational modeling,
High performance computing,
Degradation,
Delay,
Programming profession,
Productivity,
Processor scheduling,
Memory management,
Parallel processing"
Aggregating Bandwidth for Multihomed Mobile Collaborative Communities,"Multihomed, mobile wireless computing and communication devices can spontaneously form communities to logically combine and share the bandwidth of each other's wide-area communication links using inverse multiplexing. But, membership in such a community can be highly dynamic, as devices and their associated WWAN links randomly join and leave the community. We identify the issues and trade-offs faced in designing a decentralized inverse multiplexing system in this challenging setting and determine precisely how heterogeneous WWAN links should be characterized and when they should be added to, or deleted from, the shared pool. We then propose methods of choosing the appropriate channels on which to assign newly arriving application flows. Using video traffic as a motivating example, we demonstrate how significant performance gains can be realized by adapting allocation of the shared WWAN channels to specific application requirements. Our simulation and experimentation results show that collaborative bandwidth aggregation systems are, indeed, a practical and compelling means of achieving high-speed Internet access for groups of wireless computing devices beyond the reach of public or private access points","Bandwidth,
Collaboration,
Mobile communication,
Mobile computing,
Wireless communication,
Video sharing,
Traffic control,
Performance gain,
Computational modeling,
Internet"
Real-Time Divisible Load Scheduling for Cluster Computing,"Cluster computing has emerged as a new paradigm for solving large-scale problems. To enhance QoS and provide performance guarantees in cluster computing environments, various real-time scheduling algorithms and workload models have been investigated. Computational loads that can be arbitrarily divided into independent pieces represent many real-world applications. Divisible load theory (DLT) provides insight into distribution strategies for such computations. However, the problem of providing performance guarantees to divisible load applications has not yet been systematically studied. This paper investigates such algorithms for a cluster environment. Design parameters that affect the performance of these algorithms and scenarios when the choice of these parameters have significant effects are studied. A novel algorithmic approach integrating DLT and EDF (earliest deadline first) scheduling is proposed. For comparison, we also propose a heuristic algorithm. Intensive experimental results show that the application of DLT to real-time cluster-based scheduling leads to significantly better scheduling approaches","Processor scheduling,
Scheduling algorithm,
Clustering algorithms,
Resource management,
Collision mitigation,
Large Hadron Collider,
Heuristic algorithms,
Real time systems,
Computer science,
Large-scale systems"
Incorporating Auditory Feature Uncertainties in Robust Speaker Identification,"Conventional speaker recognition systems perform poorly under noisy conditions. Recent research suggests that binary time-frequency (T-F) masks be a promising front-end for robust speaker recognition. In this paper, we propose novel auditory features based on an auditory periphery model, and show that these features capture significant speaker characteristics. Additionally, we estimate uncertainties of the auditory features based on binary T-F masks, and calculate speaker likelihood scores using uncertainty decoding. Our approach achieves substantial performance improvement in a speaker identification task compared with a state-of-the-art robust front-end in a wide range of signal-to-noise conditions.","Uncertainty,
Feature extraction,
Noise robustness,
Acoustic noise,
Decoding,
Cepstral analysis,
Speaker recognition,
Filter bank,
Acoustical engineering,
Mel frequency cepstral coefficient"
Robust Priority Assignment for Fixed Priority Real-Time Systems,"This paper focuses on priority assignment for realtime systems using fixed priority scheduling. It introduces and defines the concept of a ""robust"" priority ordering: the most appropriate priority ordering to use in a system subject to variable amounts of additional interference from sources such as interrupts, operating system overheads, exception handling, cycle stealing, and task execution time overruns. The paper describes a robust priority assignment algorithm that can find the robust priority ordering for a wide range of fixed priority system models and additional interference functions. Proofs are given for a number of interesting theorems about robust priority assignment, and the circumstances under which a ""deadline minus jitter"" monotonic partial ordering forms part of the robust ordering. The paper shows that ""deadline minus jitter"" monotonic priority ordering is the robust priority ordering for a specific class of system, and that this property holds essentially independent of the additional interference function.","Robustness,
Real time systems,
Interference,
Job shop scheduling,
Jitter,
Scheduling algorithm,
Operating systems,
Computer science,
Processor scheduling,
Switches"
Single-Query Motion Planning with Utility-Guided Random Trees,"Randomly expanding trees are very effective in exploring high-dimensional spaces. Consequently, they are a powerful algorithmic approach to sampling-based single-query motion planning. As the dimensionality of the configuration space increases, however, the performance of tree-based planners that use uniform expansion degrades. To address this challenge, we present a utility-guided algorithm for the online adaptation of the random tree expansion strategy. This algorithm guides expansion towards regions of maximum utility based on local characteristics of state space. To guide exploration, the algorithm adjusts the parameters that control random tree expansion in response to state space information obtained during the planning process. We present experimental results to demonstrate that the resulting single-query planner is computationally more efficient and more robust than previous planners in challenging artificial and real-world environments.","State-space methods,
Space exploration,
Motion planning,
Utility theory,
Degradation,
Process planning,
Orbital robotics,
Robotics and automation,
Computer science,
Robustness"
Fast Worm Containment Using Feedback Control,"In a computer network, network security is accomplished using elements such as firewalls, hosts, servers, routers, intrusion detection systems, and honey pots. These network elements need to know the nature or anomaly of the worm a priori to detect the attack. Modern viruses such as Code Red, Sapphire, and Nimda spread quickly. Therefore, it is impractical if not impossible for human mediated responses to these fast-spreading viruses. Several epidemic studies show that automatic tracking of resource usage and control provides an effective method to contain the damage. In this paper, we propose a novel security architecture based on the control system theory. In particular, we describe a state-space feedback control model that detects and control the spread of these viruses or worms by measuring the velocity of the number of new connections an infected host makes. The mechanism's objective is to slow down a worm's spreading velocity by controlling (delaying) the number of new connections made by an infected host. A proportional and integral (PI) controller is used for a continuous control of the feedback loop. The approach proposed here has been verified in a laboratory setup, and we were able to contain the infection so that it affected less than 5 percent of the hosts. We have also implemented a protocol for exchanging control-specific information between the network elements. The results from the simulation and experimental setup combined with the sensitivity analysis demonstrate the applicability and accuracy of the approach.","Feedback control,
Computer worms,
Computer viruses,
Automatic control,
Velocity control,
Pi control,
Proportional control,
Computer networks,
Computer security,
Network servers"
Dynamic Load Balancing of Unbalanced Computations Using Message Passing,"This paper examines MPI's ability to support continuous, dynamic load balancing for unbalanced parallel applications. We use an unbalanced tree search benchmark (UTS) to compare two approaches, 1) work sharing using a centralized work queue, and 2) work stealing using explicit polling to handle steal requests. Experiments indicate that in addition to a parameter defining the granularity of load balancing, message-passing paradigms require additional parameters such as polling intervals to manage runtime overhead. Using these additional parameters, we observed an improvement of up to 2times in parallel performance. Overall we found that while work sharing may achieve better peak performance on certain workloads, work stealing achieves comparable if not better performance across a wider range of chunk sizes and workloads.","Load management,
Message passing,
Parallel programming,
Computer science,
Educational institutions,
Runtime,
Concurrent computing,
Communication system operations and management,
Shape,
Application software"
How to Study Wireless Mesh Networks: A hybrid Testbed Approach,"Simulation is the most famous way to study wireless an mobile networks since they offer a convenient combination of flexibility and controllability. However, their largest disadvantage is that the gained results are difficult to transfer into reality since not only the abstraction of the upper network layer are typically high, but also the environment of mobile and wireless networks is very complex. This is due to two reasons. First there are typically many simplifications in the models of the upper networking layers, and second the environment of mobile and wireless networks is in particular complicated and thus difficult to be considered in all details. In this paper we introduce UMIC-mesh, a hybrid testbed approach, that consists of real mesh nodes and a virtualization environment. On the one hand the virtualization allows the development and testing of software as if it was executed on real mesh routers, but in a more repeatable and controllable way. On the other hand the results and conclusions gained by a software evaluation in the testbed can be easily transferred into reality, since the testbed represents a high degree of realism.","Wireless mesh networks,
Software testing,
Wireless networks,
Costs,
System testing,
Wireless LAN,
Computer architecture,
Computer science,
Computational modeling,
Computer simulation"
Incorporating Domain Knowledge Into the Fuzzy Connectedness Framework: Application to Brain Lesion Volume Estimation in Multiple Sclerosis,"A method for incorporating prior knowledge into the fuzzy connectedness image segmentation framework is presented. This prior knowledge is in the form of probabilistic feature distribution and feature size maps, in a standard anatomical space, and ldquointensity hintsrdquo selected by the user that allow for a skewed distribution of the feature intensity characteristics. The fuzzy affinity between pixels is modified to encapsulate this domain knowledge. The method was tested by using it to segment brain lesions in patients with multiple sclerosis, and the results compared to an established method for lesion outlining based on edge detection and contour following. With the fuzzy connections (FC) method, the user is required to identify each lesion with a mouse click, to provide a set of seed pixels. The algorithm then grows the features from the seeds to define the lesions as a set of objects with fuzzy connectedness above a preset threshold. The FC method gave improved interobserver reproducibility of lesion volumes, and the set of pixels determined to be lesion was more consistent compared to the contouring method. The operator interaction time required to evaluate one subject was reduced from an average of 111 min with contouring to 16 min with the FC method.",
An Electrical Impedance Spectroscopy System for Breast Cancer Detection,"This paper describes Rensselaer's ACT 4 electrical impedance tomography system which has been developed for breast cancer detection. ACT 4 acquires electrical impedance data at a set of discrete frequencies in the range from 3.33 kHz to 1 MHz and can support up to 72 electrodes. The instrument applies either voltages or currents to all the electrodes simultaneously and measures the resulting currents and/or voltages. Radiolucent electrode arrays are applied to the compression plates of an x-ray mammography system for collecting impedance data in register with x-ray images. The analog front-end electronics are supported with a distributed digital system, including a computer, Digital Signal Processors (DSPs) and Field-Programmable Gate Arrays (FPGAs). A Microsoft Visual C/C++ -based user interface controls the system operation. The overall system architecture is presented as well as performance results.","Electrochemical impedance spectroscopy,
Breast cancer,
Cancer detection,
Electrodes,
Voltage,
Field programmable analog arrays,
X-ray imaging,
Field programmable gate arrays,
Tomography,
Frequency"
Constrained Texture Synthesis via Energy Minimization,"This paper describes CMS (constrained minimization synthesis), a fast, robust texture synthesis algorithm that creates output textures while satisfying constraints. We show that constrained texture synthesis can be posed in a principled way as an energy minimization problem that requires balancing two measures of quality: constraint satisfaction and texture seamlessness. We then present an efficient algorithm for finding good solutions to this problem using an adaptation of graphcut energy minimization. CMS is particularly well suited to detail synthesis, the process of adding high-resolution detail to low-resolution images. It also supports the full image analogies framework, while providing superior image quality and performance. CMS is easily extended to handle multiple constraints on a single output, thus enabling novel applications that combine both user-specified and image-based control","Minimization methods,
Collision mitigation,
Energy measurement,
Automatic control,
Robustness,
Image quality,
Image resolution,
Control system synthesis,
Pixel,
Constraint optimization"
GF(4) Based Synthesis of Quaternary Reversible/Quantum Logic Circuits,"Galois field sum of products (GFSOP) has been found to be very promising for reversible/quantum implementation of multiple-valued logic. In this paper, we show ten quaternary Galois field expansions, using which quaternary Galois field decision diagrams (QGFDD) can be constructed. Flattening of the QGFDD generates quaternary GFSOP (QGFSOP). These QGFSOP can be implemented as cascade of quaternary 1-qudit gates and multi-qudit Feynman and Toffoli gates. We also show the realization of quaternary Feynman and Toffoli gates using liquid ion-trap realizable 1-qudit gates and 2-qudit Muthukrishnan-Stroud gates. Besides the quaternary functions, this approach can also be used for synthesis of encoded binary functions by grouping 2-bits together into quaternary value. For this purpose, we show binary-to-quaternary encoder and quaternary-to- binary decoder circuits using quaternary 1-quidit gates and 2-qudit Muthukrishnan-Stroud gates.",
Research Progress on C-band Broadband Multibeam Klystron,"Several types of C-band broadband multibeam klystrons (MBKs) are under development at the Institute of Electronics, Chinese Academy of Sciences. These MBKs operate at various C-band frequencies and have peak powers of 30-200 kW, average powers of 2-10 kW, and bandwidths of 4%-7%. The design considerations for obtaining high power and wide instantaneous bandwidths and test results for these MBKs are presented in this paper. The main technical problems, including a power sag in the band, high voltage breakdown, and non-operating mode oscillation, are also discussed. Further research work for improving the performance of these MBKs is described","klystrons,
circuit oscillations,
electric breakdown"
Fiber Tract Clustering on Manifolds With Dual Rooted-Graphs,"We propose a manifold learning approach to fiber tract clustering using a novel similarity measure between fiber tracts constructed from dual-rooted graphs. In particular, to generate this similarity measure, the chamfer or Hausdorff distance is initially employed as a local distance metric to construct minimum spanning trees between pairwise fiber tracts. These minimum spanning trees are effective in capturing the intrinsic geometry of the fiber tracts. Hence, they are used to capture the neighborhood structures of the fiber tract data set. We next assume the high-dimensional input fiber tracts to lie on low-dimensional non-linear manifolds. We apply Locally Linear Embedding, a popular manifold learning technique, to define a low-dimensional embedding of the fiber tracts that preserves the neighborhood structures of the high-dimensional data structure as captured by the method of dual-rooted graphs. Clustering is then performed on this low-dimensional data structure using the k-means algorithm. We illustrate our resulting clustering technique on both synthetic data and on real fiber tract data obtained from diffusion tensor imaging.","Clustering algorithms,
Matrix decomposition,
Diffusion tensor imaging,
Computer science,
Tree graphs,
Data structures,
Radiology,
Hospitals,
Biomedical imaging,
Particle measurements"
Depth Information by Stage Classification,"Recently, methods for estimating 3D scene geometry or absolute scene depth information from 2D image content have been proposed. However, general applicability of these methods in depth estimation may not be realizable, as inconsistencies may be introduced due to a large variety of possible pictorial content. We identify scene categorization as the first step towards efficient and robust depth estimation from single images. To that end, we describe a limited number of typical 3D scene geometries, called stages, each having a unique depth pattern and thus providing a specific context for stage objects. This type of scene information narrows down the possibilities with respect to individual objects' locations, scales and identities. We show how these stage types can be efficiently learned and how they can lead to robust extraction of depth information. Our results indicate that stages without much variation and object clutter can be detected robustly, with up to 60% success rate.","Layout,
Shape,
Robustness,
Statistics,
Information geometry,
Solid modeling,
Image reconstruction,
Humans,
Intelligent systems,
Laboratories"
Compositional Boosting for Computing Hierarchical Image Structures,"In this paper, we present a compositional boosting algorithm for detecting and recognizing 17 common image structures in low-middle level vision tasks. These structures, called ""graphlets"", are the most frequently occurring primitives, junctions and composite junctions in natural images, and are arranged in a 3-layer And-Or graph representation. In this hierarchic model, larger graphlets are decomposed (in And-nodes) into smaller graphlets in multiple alternative ways (at Or-nodes), and parts are shared and re-used between graphlets. Then we present a compositional boosting algorithm for computing the 17 graphlets categories collectively in the Bayesian framework. The algorithm runs recursively for each node A in the And-Or graph and iterates between two steps -bottom-up proposal and top-down validation. The bottom-up step includes two types of boosting methods, (i) Detecting instances of A (often in low resolutions) using Adaboosting method through a sequence of tests (weak classifiers) image feature, (ii) Proposing instances of A (often in high resolution) by binding existing children nodes of A through a sequence of compatibility tests on their attributes (e.g angles, relative size etc). The Adaboosting and binding methods generate a number of candidates for node A which are verified by a top-down process in a way similar to Data-Driven Markov Chain Monte Carlo [18]. Both the Adaboosting and binding methods are trained off-line for each graphlet category, and the compositional nature of the model means the algorithm is recursive and can be learned from a small training set. We apply this algorithm to a wide range of indoor and outdoor images with satisfactory results.","Boosting,
Testing,
Image edge detection,
Image resolution,
Image segmentation,
Computer vision,
Image recognition,
Bayesian methods,
Proposals,
Inference algorithms"
New Features to Identify Computer Generated Images,"Discrimination of computer generated images from real images is becoming more and more important. In this paper, we propose the use of new features to distinguish computer generated images from real images. The proposed features are based on the differences in the acquisition process of images. More specifically, traces of demosaicking and chromatic aberration are used to differentiate computer generated images from digital camera images. It is observed that the former features perform very well on high quality images, whereas the latter features perform consistently across a wide range of compression values. The experimental results show that proposed features are capable of improving the accuracy of the state-of-the-art techniques.","Image generation,
Digital images,
Digital cameras,
Nonlinear filters,
Digital filters,
Feature extraction,
Wavelet transforms,
Interpolation,
Image coding,
Computer vision"
Real-Time Image Matching Based on Multiple View Kernel Projection,"This paper proposes a novel matching method for realtime finding the correspondences among different images containing the same object. The method utilizes an efficient Kernel Projection scheme to descript the image patch around a detected feature point. In order to achieve invariance and tolerance to geometric distortions, it combines a training stage based on generated synthetic views of the object. The two reliable and efficient methods cooperate together, resulting the core part of our novel multiple view kernel projection method (MVKP). Finally, considering the properties and distribution of the described feature vectors, we search for the best correspondence between two sets of features using a fast filtering vector approximation (FFVA) algorithm, which can be viewed as a fast lower-bound rejection scheme. Extensive experimental results on both synthetic and real data have demonstrated the effectiveness of the proposed approach.","Image matching,
Kernel,
Computer vision,
Robustness,
Real time systems,
Principal component analysis,
Filtering,
Layout,
Image databases,
Intelligent sensors"
LUT-based Image Rectification Module Implemented in FPGA,"This paper presents a real-time hardware architecture able to perform simultaneously, image rectification and image distortion removal. The entire process is based on Look-Up Tables (LUTs) relating pixels from rectified image and original image with sub-pixel precision. For increased flexibility, we created a parameterized VHDL version of the design, which allows us to generate different hardware configurations, based on adjustable parameters: image resolution, number of bits to store the sub-pixel precision. Other advantages of the proposed solution that worth to be mentioned are scalability (can be replicated for any number of rectified images) and portability on many FPGA-based hardware platforms. We analyze the performance of different configurations on a VirtexE600 FPGA. As rectifying an image based on bilinear interpolation has a blurring effect, with negative consequences on 3D reconstruction, we increased the sub-pixel precision and studied the impact on 3D lane detection accuracy, processing time and resource usage inside the chip.","Field programmable gate arrays,
Hardware,
Cameras,
Image reconstruction,
Table lookup,
Pixel,
Computer architecture,
Graphics,
Data mining,
Image resolution"
Temperature Effect on Heavy-Ion Induced Parasitic Current on SRAM by Device Simulation: Effect on SEU Sensitivity,"A temperature dependence analysis of single event transient currents induced by heavy-ions using TCAD simulation is performed in the 218 to 418 K range on a 0.18 mum SRAM cell manufactured by ATMEL. The single event upset (SEU) phenomena depends on both the heavy-ions-induced transient current and the technology. Temperature is shown to have a significant impact on the trends of heavy-ions-induced current. The SEU sensitivity is then expected to exhibit a large temperature dependence. However, a small influence on the SEU sensitivity is reported in this work on the studied technology.","Temperature sensors,
Random access memory,
Single event upset,
Semiconductor process modeling,
Temperature dependence,
Discrete event simulation,
Temperature distribution,
Poisson equations,
Semiconductor device modeling,
Analytical models"
Improved Model-Based Magnetic Resonance Spectroscopic Imaging,"Model-based techniques have the potential to reduce the artifacts and improve resolution in magnetic resonance spectroscopic imaging, without sacrificing the signal-to-noise ratio. However, the current approaches have a few drawbacks that limit their performance in practical applications. Specifically, the classical schemes use less flexible image models that lead to model misfit, thus resulting in artifacts. Moreover, the performance of the current approaches is negatively affected by the magnetic field inhomogeneity and spatial mismatch between the anatomical references and spectroscopic imaging data. In this paper, we propose efficient solutions to overcome these problems. We introduce a more flexible image model that represents the signal as a linear combination of compartmental and local basis functions. The former set represents the signal variations within the compartments, while the latter captures the local perturbations resulting from lesions or segmentation errors. Since the combined set is redundant, we obtain the reconstructions using sparsity penalized optimization. To compensate for the artifacts resulting from field inhomogeneity, we estimate the field map using alternate scans and use it in the reconstruction. We model the spatial mismatch as an affine transformation, whose parameters are estimated from the spectroscopy data.",
A Study of Quality Issues for Image Auto-Annotation With the Corel Dataset,"The Corel Image set is widely used for image annotation performance evaluation although it has been claimed that Corel images are relatively easy to annotate. The aim of this paper is to demonstrate some of the disadvantages of datasets like the Corel set for effective auto-annotation evaluation. We first compare the performance of several annotation algorithms using the Corel set and find that simple near neighbor propagation techniques perform fairly well. A support vector machine (SVM)-based annotation method achieves even better results, almost as good as the best found in the literature. We then build a new image collection using the Yahoo Image Search engine and query-by-single-word searches to create a more challenging annotated set automatically. Then, using three very different image annotation methods, we demonstrate some of the problems of annotation using the Corel set compared with the Yahoo-based training set. In both cases the training sets are used to create a set of annotations for the Corel test set","Testing,
Vocabulary,
Support vector machines,
Search engines,
Information retrieval,
Image retrieval,
Intelligent agent,
Computer science,
Electronic mail"
Probabilistically Driven Particle Swarms for Optimization of Multi Valued Discrete Problems : Design and Analysis,"A new particle swarm optimization (PSO) algorithm that is more effective for discrete, multi-valued optimization problems is presented. The new algorithm is probabilistically driven since it uses probabilistic transition rules to move from one discrete value to another in the search for an optimum solution. Properties of the binary discrete particle swarms are discussed. The new algorithm for discrete multi-values is designed with the similar properties. The algorithm is tested on a suite of benchmarks and comparisons are made between the binary PSO and the new discrete PSO implemented for ternary, quaternary systems. The results show that the new algorithm's performance is close and even slightly better than the original discrete, binary PSO designed by Kennedy and Eberhart. The algorithm can be used in any real world optimization problems, which have a discrete, bounded field","Particle swarm optimization,
Design optimization,
Algorithm design and analysis,
Benchmark testing,
Multidimensional systems,
System testing,
Performance analysis,
Power engineering and energy,
Computer science,
Power engineering computing"
Fast Genetic Programming and Artificial Developmental Systems on GPUs,"In this paper we demonstrate the use of the graphics processing unit (GPU) to accelerate evolutionary computation applications, in particular genetic programming approaches. We show that it is possible to get speed increases of several hundred times over a typical CPU implementation, catapulting GPU processing for these applications into the realm of HPC This increase in performance also extends to artificial developmental systems, where evolved programs are used to construct cellular systems. Feasibility of this approach to efficiently evaluate artificial developmental systems based on cellular automata is demonstrated.","Genetic programming,
Hardware,
Computer graphics,
Evolutionary computation,
Application software,
Central Processing Unit,
Computer architecture,
Computational modeling,
Computer science,
Acceleration"
Adequate and Precise Evaluation of Quality Models in Software Engineering Studies,"Many statistical techniques have been proposed and introduced to predict fault-proneness of program modules in software engineering. Choosing the ""best"" candidate among many available models involves performance assessment and detailed comparison. But these comparisons are not simple due to varying performance measures and the related verification and validation cost implications. Therefore, a methodology for precise definition and evaluation of the predictive models is still needed. We believe the procedure we outline here, if followed, has a potential to enhance the statistical validity of future experiments.","Software engineering,
Predictive models,
Software quality,
Software metrics,
Computer science,
Costs,
Logistics,
Classification tree analysis,
Neural networks,
Genetic algorithms"
Assurance Based Development of Critical Systems,"Assurance based development (ABD) is the synergistic construction of a critical computing system and an assurance case that sets out the dependability claims for the system and argues that the available evidence justifies those claims. Co-developing the system and its assurance case helps software developers to make technology choices that address the specific dependability goal of each component. This approach gives developers: (1) confidence that the technologies selected will support the system's dependability goal and (2) flexibility to deploy expensive technology, such as formal verification, only on components whose assurance needs demand it. ABD simplifies the detection - and thereby avoidance - of potential assurance difficulties as they arise, rather than after development is complete. In this paper, we present ABD together with a case study of its use.","Safety,
Computer science,
Formal verification,
Software systems,
Systems engineering and theory,
Aerospace engineering,
Documentation,
Europe,
Standards development"
Microprocessors in the Era of Terascale Integration,"Moore's law has deliver tera-scale level transistor integration capacity. Power, variability, reliability, aging, and testing are pose as barriers and challenges to harness this integration capacity. Advances in microarchitecture and programming systems discussed in this paper are potential solutions","Microprocessors,
Testing,
Moore's Law,
Threshold voltage,
Logic,
Transistors,
Single event upset,
Error correction codes,
Dielectrics,
Microarchitecture"
Feature selection for grasp recognition from optical markers,"Although the human hand is a complex biomechanical system, only a small set of features may be necessary for observation learning of functional grasp classes. We explore how to methodically select a minimal set of hand pose features from optical marker data for grasp recognition. Supervised feature selection is used to determine a reduced feature set of surface marker locations on the hand that is appropriate for grasp classification of individual hand poses. Classifiers trained on the reduced feature set of five markers retain at least 92% of the prediction accuracy of classifiers trained on a full feature set of thirty markers. The reduced model also generalizes better to new subjects. The dramatic reduction of the marker set size and the success of a linear classifier from local marker coordinates recommend optical marker techniques as a practical alternative to data glove methods for observation learning of grasping.","Data gloves,
Humans,
Fingers,
Biomedical optical imaging,
Grasping,
Robot kinematics,
Hidden Markov models,
Optical sensors,
Manipulators,
Robotics and automation"
A flow based approach for SSH traffic detection,"The basic objective of this work is to assess the utility of two supervised learning algorithms AdaBoost and RIPPER for classifying SSH traffic from log files without using features such as payload, IP addresses and source/destination ports. Pre-processing is applied to the traffic data to express as traffic flows. Results of 10-fold cross validation for each learning algorithm indicate that a detection rate of 99% and a false positive rate of 0.7% can be achieved using RIPPER. Moreover, promising preliminary results were obtained when RIPPER was employed to identify which service was running over SSH. Thus, it is possible to detect SSH traffic with high accuracy without using features such as payload, IP addresses and source/destination ports, where this represents a particularly useful characteristic when requiring generic, scalable solutions.","Payloads,
Telecommunication traffic,
Traffic control,
Application software,
Cryptography,
Engineering management,
Computer science,
Inspection,
Supervised learning,
Financial management"
Augmented Reality for Rehabilitation of Cognitive Disabled Children: A Preliminary Study,"We have designed a non-immersive recreational and educational augmented reality application (ARVe - Augmented Reality applied to Vegetal field) that allows young children to handle 2D and 3D plant entities in a simple and intuitive way. This application involves a task of pairing and provides visual, olfactory or auditory cues to help children in decision making. 93 children from a French elementary school (including 11 cognitive disabled ones) participated in a preliminary study. The objectives of this study were: (1) to investigate children performance and behaviour in using AR techniques, and (2) to examine specific attitudes of cognitive disabled children confronted to such techniques. We have observed that disabled children were very enthusiastic when using the application and showed a high motivation compared to most other pupils. Moreover, autistic and trisomic children were able to express some positive emotions when confronted to the application. These very encouraging results promote a widespread use of such tools for cognitive disabled children.","Augmented reality,
Virtual reality,
Education,
Autism,
Computer aided manufacturing,
Military computing,
Animals,
Olfactory,
Decision making,
Educational institutions"
DP-Miner: Design Pattern Discovery Using Matrix,"Design patterns document expert design experience in software system development. They have been applied in many existing software systems. However, pattern information is generally lost in the source code. Discovering design patterns from source code may help understand system designs and further change the systems. In this paper, we present a novel approach to discovering design patterns by defining the structural characteristics of each design pattern in terms of weight and matrix. Our discovery process includes several analysis phases. Our approach is based on the XMI standard so that it is compatible with other techniques following such standard. We also develop a toolkit to support our approach. An industrial size case study is conducted to evaluate our approach and tool","Pattern analysis,
Software systems,
Unified modeling language,
Software design,
XML,
Pattern matching,
Java,
Computer science,
System analysis and design,
Reverse engineering"
Automated Design of Misaligned-Carbon-Nanotube-Immune Circuits,"Carbon nanotube field-effect transistors (CNFETs) are promising candidates as extensions to silicon CMOS due to excellent CV/I device performance. An ideal CNFET inverter fabricated using a perfect CNFET technology can have 5.1 times faster FO4 delay and 2.6 times lower energy per cycle compared to a 32 nm silicon CMOS inverter. Two fundamental challenges prevent us from creating CNFET-based logic designs with the advantages quoted above: 1. misaligned carbon nanotubes (CNTs), and 2. Metallic CNTs. Misaligned CNTs can result in incorrect logic function implementations. This paper presents a technique for designing CNFET-based arbitrary logic functions that are guaranteed to be correct even in the presence of a large number of misaligned CNTs.","Circuits,
Inverters,
Semiconductivity,
Silicon,
Carbon nanotubes,
Lithography,
Permission,
CMOS technology,
Logic functions,
Etching"
A Road Detection Algorithm by Boosting Using Feature Combination,"Road detection is one of the most important branches of road following. In this paper we propose a classification-based road detection algorithm by boosting. To fully utilize potential region feature correlations and improve the accuracy of classification, this algorithm introduces the feature combination method into road detection. First, an over-completed feature set is constructed on several linear and non-linear combined functions. Second, a correlation feature set is selected from the over-completed feature set by feature selection algorithm. Then, the boosting, the support vector machine and the random forest classifiers are used to evaluate the correlation feature set and the raw feature set. The results of the experiment shows the performance of boosting classifier based on the correlation feature set provides the best outcome.","Detection algorithms,
Boosting,
Classification algorithms,
Machine learning algorithms,
Road vehicles,
Petrochemicals,
Intelligent vehicles,
Robustness,
Computer vision,
Image segmentation"
A hybrid systems approach to trajectory tracking control for juggling systems,"From a hybrid systems point of view, we provide a modeling framework and a trajectory tracking control design methodology for juggling systems. We present the main ideas and concepts in a one degree-of-freedom juggler, which consists of a ball bouncing on an actuated robot. We design a hybrid control strategy that, with only information of the ball's state at impacts, controls the ball to track a reference rhythmic pattern with arbitrary precision. We extend this hybrid control strategy to the case of juggling multiple balls with different rhythmic patterns. Simulation results for juggling of one and three balls with a single actuated robot are presented.","Trajectory,
Control systems,
Robots,
Mechanical systems,
Legged locomotion,
Control design,
Difference equations,
USA Councils,
Control system synthesis,
Differential equations"
Conversation Clock: Visualizing audio patterns in co-located groups,"Aural conversation is ephemeral by nature. The interaction history of conversation fades as the present moment demands the attention of participants. In this paper, we explore the nature of group interaction by augmenting aural conversation with a persistent visualization of audio input. This visualization, conversation clock, displays individual contribution via audio input and provides a corresponding social mirror over the course of interaction. This paper describes the implementation of conversation clock, provides observations on an initial pilot study, and outlines the future progression of this research","Clocks,
Visualization,
Mirrors,
History,
Collaborative work,
Videoconference,
Rhythm,
Interactive systems,
Video sharing,
Auditory displays"
Human Motion Capture Data Compression by Model-Based Indexing: A Power Aware Approach,"Human motion capture (MoCap) data can be used for animation of virtual human-like characters in distributed virtual reality applications and networked games. MoCap data compressed using the standard MPEG-4 encoding pipeline comprising of predictive encoding (and/or DCT decorrelation), quantization, and arithmetic/Huffman encoding, entails significant power consumption for the purpose of decompression. In this paper, we propose a novel algorithm for compression of MoCap data, which is based on smart indexing of the MoCap data by exploiting structural information derived from the skeletal virtual human model. The indexing algorithm can be fine-controlled using three predefined quality control parameters (QCPs). We demonstrate how an efficient combination of the three QCPs results in a lower network bandwidth requirement and reduced power consumption for data decompression at the client end when compared to standard MPEG-4 compression. Since the proposed algorithm exploits structural information derived from the skeletal virtual human model, it is observed to result in virtual human animation of visually acceptable quality upon decompression","Humans,
Data compression,
Indexing,
Animation,
MPEG 4 Standard,
Energy consumption,
Virtual reality,
Pipelines,
Predictive encoding,
Discrete cosine transforms"
Stereo-based 6D object localization for grasping with humanoid robot systems,"Robust vision-based grasping is still a hard problem for humanoid robot systems. When being restricted to using the camera system built-in into the robot's head for object localization, the scenarios get often very simplified in order to allow the robot to grasp autonomously. Within the computer vision community, many object recognition and localization systems exist, but in general, they are not tailored to the application on a humanoid robot. In particular, accurate 6D object localization in the camera coordinate system with respect to a 3D rigid model is crucial for a general framework for grasping. While many approaches try to avoid the use of stereo calibration, we will present a system that makes explicit use of the stereo camera system in order to achieve maximum depth accuracy. Our system can deal with textured objects as well as objects that can be segmented globally and are defined by their shape. Thus, it covers the cases of objects with complex texture and complex shape. Our work is directly linked to a grasping framework being implemented on the humanoid robot ARM AR and serves as its perception module for various grasping and manipulation experiments in a kitchen scenario.","Humanoid robots,
Robot vision systems,
Cameras,
Robot kinematics,
Stereo vision,
Shape,
Robustness,
Computer vision,
Object recognition,
Application software"
Skoll: A Process and Infrastructure for Distributed Continuous Quality Assurance,"Software engineers increasingly emphasize agility and flexibility in their designs and development approaches. They increasingly use distributed development teams, rely on component assembly and deployment rather than green field code writing, rapidly evolve the system through incremental development and frequent updating, and use flexible product designs supporting extensive end-user customization. While agility and flexibility have many benefits, they also create an enormous number of potential system configurations built from rapidly changing component implementations. Since today's quality assurance (QA) techniques do not scale to handle highly configurable systems, we are developing and validating novel software QA processes and tools that leverage the extensive computing resources of user and developer communities in a distributed, continuous manner to improve software quality significantly. This paper provides several contributions to the study of distributed, continuous QA (DCQA). First, it shows the structure and functionality of Skoll, which is an environment that defines a generic around-the-world, around-the-clock QA process and several sophisticated tools that support this process. Second, it describes several novel QA processes built using the Skoll environment. Third, it presents two studies using Skoll: one involving user testing of the Mozilla browser and another involving continuous build, integration, and testing of the ACE+TAO communication software package. The results of our studies suggest that the Skoll environment can manage and control distributed continuous QA processes more effectively than conventional QA processes. For example, our DCQA processes rapidly identified problems that had taken the ACE+TAO developers much longer to find and several of which they had not found. Moreover, the automatic analysis of QA results provided developers information that enabled them to quickly find the root causes of problems","Quality assurance,
Software quality,
Software testing,
Design engineering,
Assembly systems,
Writing,
Product design,
Software tools,
Distributed computing,
Software packages"
Layer-Adaptive Mode Decision and Motion Search for Scalable Video Coding with Combined Coarse Granular Scalability (CGS) and Temporal Scalability,"In this paper, we propose a layer-adaptive mode decision algorithm and a motion search scheme for the scalable video coding (SVC) with combined coarse granular scalability (CGS) and temporal scalability. To speed up the encoder while minimizing the loss in coding efficiency, our layer-adaptive mode decision recursively refers to the prediction modes and quantization parameter of the reference/base layer to minimize the number of modes tested at the enhancement layer. Moreover, our motion search scheme adaptively reuses the reference frame indices of the base layer and determines the initial search point using the motion vector at the base layer or the motion vector predictor at the enhancement layer. As compared with JSVM 8, the proposed algorithms provide up to 75% overall time saving and more than 85% time reduction for encoding enhancement layers with negligible loss in coding efficiency.","Scalability,
Video coding"
LASSOing HRI: Analyzing situation awareness in map-centric and video-centric interfaces,"Good situation awareness (SA) is especially necessary when robots and their operators are not collocated, such as in urban search and rescue (USAR). This paper compares how SA is attained in two systems: one that has an emphasis on video and another that has an emphasis on a three-dimensional map. We performed a within-subjects study with eight USAR domain experts. To analyze the utterances made by the participants, we developed a SA analysis technique, called LASSO, which includes five awareness categories: location, activities, surroundings, status, and overall mission. Using our analysis technique, we show that a map-centric interface is more effective in providing good location and status awareness while a video-centric interface is more effective in providing good surroundings and activities awareness.","Abstracts,
Robots,
Reliability"
Cell-Graph Mining for Breast Tissue Modeling and Classification,"We consider the problem of automated cancer diagnosis in the context of breast tissues. We present graph theoretical techniques that identify and compute quantitative metrics for tissue characterization and classification. We segment digital images of histopathological tissue samples using k-means algorithm. For each segmented image we generate different cell-graphs using positional coordinates of cells and surrounding matrix components. These cell-graphs have 500-2000 cells(nodes) with 1000-10000 links depending on the tissue and the type of cell-graph being used. We calculate a set of global metrics from cell-graphs and use them as the feature set for learning. We compare our technique, hierarchical cell graphs, with other techniques based on intensity values of images, Delaunay triangulation of the cells, the previous technique we proposed for brain tissue images and with the hybrid approach that we introduce in this paper. Among the compared techniques, hierarchical-graph approach gives 81.8% accuracy whereas we obtain 61.0%, 54.1% and 75.9% accuracy with intensity-based features, Delaunay triangulation and our previous technique, respectively.","Breast tissue,
Breast neoplasms,
Breast cancer,
USA Councils,
Image segmentation,
Lymph nodes,
Pixel,
Mass spectroscopy,
Computer science,
Biomedical engineering"
Conformal Embedding Analysis with Local Graph Modeling on the Unit Hypersphere,"We present the Conformal Embedding Analysis (CEA) for feature extraction and dimensionality reduction. Incorporating both conformal mapping and discriminating analysis, CEA projects the high-dimensional data onto the unit hypersphere and preserves intrinsic neighbor relations with local graph modeling. Through the embedding, resulting data pairs from the same class keep the original angle and distance information on the hypersphere, whereas neighboring points of different class are kept apart to boost discriminating power. The subspace learned by CEA is gray-level variation tolerable since the cosine-angle metric and the normalization processing enhance the robustness of the conformal feature extraction. We demonstrate the effectiveness of the proposed method with comprehensive comparisons on visual classification experiments.","Linear discriminant analysis,
Kernel,
Feature extraction,
Principal component analysis,
Euclidean distance,
Training data,
Face recognition,
Conformal mapping,
Robustness,
Computer vision"
Energy-Efficient Real-Time Task Scheduling in Multiprocessor DVS Systems,"Dynamic voltage scaling (DVS) circuits have been widely adopted in many computing systems to provide tradeoff between performance and power consumption. The effective use of energy could not only extend operation duration for hand-held devices but also cut down power bills of server systems. Moreover, while many chip makers are releasing multi-core chips and multiprocessor system-on-a-chips (SoCs), multiprocessor platforms for different applications become even more popular. Multiprocessor platforms could improve the system performance and accommodate the growing demand of computing power and the variety of application functionality. This paper summarizes our work on several important issues in energy-efficient scheduling for real-time tasks in multiprocessor DVS systems. Distinct from most previous work based on heuristics, we aim at the provision of approximated solutions with worst-case guarantees. The proposed algorithms are evaluated by a series of experiments to provide insights in system designs.","Energy efficiency,
Real time systems,
Voltage control,
Energy consumption,
Processor scheduling,
Job shop scheduling,
Dynamic voltage scaling,
Multiprocessing systems,
System performance,
Delay"
Ranking-Based Optimal Resource Allocation in Peer-to-Peer Networks,"This paper presents a theoretic framework of optimal resource allocation and admission control for peer-to-peer networks. Peer's behavioral rankings are incorporated into the resource allocation and admission control to provide differentiated services and even to block peers with bad rankings. These peers may be free-riders or suspicious attackers. A peer improves her ranking by contributing resources to the P2P system or deteriorates her ranking by consuming services. Therefore, the ranking-based resource allocation provides necessary incentives for peers to contribute their resources to the P2P systems. We define a utility function which captures the best wish for the source peer to serve competing peers, who request services from the source peer. Although the utility function is convex, Harsanyi-type social welfare functions are devised to obtain a unique optimal resource allocation that achieves max-min fairness. The parameters used in our model can be derived from the nature of the services or chosen by the source peer. No private information is required to reveal from individual peers. This prevents selfish peers to play the system strategically and cheat the resource allocation mechanism for their own benefits. The resource allocation and admission control are fully distributed and linearly scalable.","Resource management,
Peer to peer computing,
History,
Admission control,
Communications Society,
Computer science,
Information systems,
USA Councils,
IP networks,
Protocols"
Inverse Kinematics for a Point-Foot Quadruped Robot with Dynamic Redundancy Resolution,"In this work we examine the control of center of mass and swing leg trajectories in LittleDog, a point-foot quadruped robot. It is not clear how to formulate a function to compute forward kinematics of the center of mass of the robot as a function of actuated joint angles because point-foot walkers have no direct actuation between the feet and the ground. Nevertheless, we show that a whole-body Jacobian exists and is well defined when at least three of the feet are on the ground. Also, the typical approach of work-space centering for redundancy resolution causes destabilizing motions when executing fast motions. An alternative redundancy resolution optimization is proposed which projects single-leg inverse kinematic solutions into the nullspace. This hybrid approach seems to minimize 1) unnecessary rotation of the body, 2) twisting of the stance legs, and 3) whole-body involvement in achieving a step leg trajectory. In simulation, this control allows the robot to perform significantly more dynamic behaviors while maintaining stability.","Kinematics,
Jacobian matrices,
Legged locomotion,
Leg,
Stability,
Robotics and automation,
Intelligent robots,
Weight control,
Motion control,
Robot control"
E-Learning Ecosystem (ELES) - A Holistic Approach for the Development of more Effective Learning Environment for Small-and-Medium Sized Enterprises (SMEs),"As e-learning and technologies advanced significantly, practitioners and academics must find new ways to make the most of this rapid development. In the past, research development in this area was mainly focused solely on technological aspects and more recently, on e-learning and technologies for individualized learning. Much work has been done in this area to enhance e-learning systems. In this research, we proposed that an ecological and holistic approach is required for an improved learning environment. To do this, the concept of ecosystem will be explained, followed by a rationalization of this application to learning and e-learning. A definition to learning ecosystem (LES) is provided and this generalized definition is further applied to the e-learning ecosystem (ELES). Hereafter an identification and examination of the e-learning ecosystem will be presented in detail. Finally, an application of the e-learning ecosystem in small-and-medium sized organisations (SMEs) will be discussed. Prior to this, an overview on the usage of e-learning in SMEs will also be given. We conclude by highlighting the need to emphasize on the ecological and holistic approach for the development of more effective learning environments.","Electronic learning,
Ecosystems,
Information systems,
Biological system modeling,
Educational technology,
Computer science,
Australia,
Productivity,
Education,
Humans"
Blurred/Non-Blurred Image Alignment using Sparseness Prior,"Aligning a pair of blurred and non-blurred images is a prerequisite for many image and video restoration and graphics applications. The traditional alignment methods such as direct and feature-based approaches cannot be used due to the presence of motion blur in one image of the pair. In this paper, we present an effective and accurate alignment approach for a blurred/non-blurred image pair. We exploit a statistical characteristic of the real blur kernel - the marginal distribution of kernel value is sparse. Using this sparseness prior, we can search the best alignment which produces the sparsest blur kernel. The search is carried out in scale space with a coarse-to-fine strategy for efficiency. Finally, we demonstrate the effectiveness of our algorithm for image deblurring, video restoration, and image matting.","Kernel,
Image restoration,
Cameras,
Motion estimation,
Graphics,
Asia,
Image enhancement,
Satellites,
Biomedical imaging,
Layout"
Model predictive control allocation for overactuated systems - stability and performance,"Overactuated systems often arise in automotive, aerospace, and robotics applications, where for reasons of redundancy or performance constraints, it is beneficial to equip a system with more control inputs than outputs. This necessitates control allocation methods that distribute control effort amongst many actuators to achieve a desired effect. Until recently, most methods have treated the control allocation as static in the sense that different dynamic authorities of the actuators were not taken into account. Recent advances have used model predictive control allocation (MPCA) to consider the dynamic authorities of the actuators over a receding horizon. In this paper, we consider the dynamic control allocation problem for overactuated systems where each actuator has different dynamic control authority and hard saturation limits. A modular control design approach is proposed, where the controller consists of an outer loop controller that synthesizes a desired virtual control input signal and an inner loop controller that uses MPCA to achieve the desired virtual control signal. We derive sufficient stability conditions for the composite feedback system and show how these conditions may be realized by imposing an additional constraint on the MPCA design. An automotive example is provided to illustrate the effectiveness of the proposed algorithm.","Predictive models,
Predictive control,
Stability,
Actuators,
Vehicle dynamics,
Aerodynamics,
Control systems,
Automotive engineering,
Aerospace control,
Robots"
Path loss models for wireless communication channel along arm and torso: measurements and simulations,"In this paper, measurements are performed on a real human using two half-wavelength dipoles, considering different parts of the human body separately. Path loss models are developed for the on- body channels along the arm and torso. The measurement results are verified with FDTD (Finite-Difference Time-Domain) simulations, using an anatomically correct configuration of the arms.",
The Inevitable Stability of Software Change,"Real software systems change and become more complex over time. But which parts change and which parts remain stable? Common wisdom, for example, states that in a well-designed object-oriented system, the more popular a class is, the less likely it is to change from one version to the next, since changes to this class are likely to impact its clients. We have studied consecutive releases of several public domain, object-oriented software systems and analyzed a number of measures indicative of size, popularity, and complexity of classes and interfaces. As it turns out, the distributions of these measures are remarkably stable as an application evolves. The distribution of class size and complexity retains its shape over time. Relatively little code is modified over time. Classes that tend to be modified, however, are also the more popular ones, that is, those with greater Fan-In. In general, the more ""complex"" a class or interface becomes, the more likely it is to change from one version to the next.","Stability,
Software systems,
Size measurement,
Application software,
Time measurement,
Communications technology,
Australia,
Software measurement,
Shape,
Computer science"
Progressive Finite Newton Approach To Real-time Nonrigid Surface Detection,"Detecting nonrigid surfaces is an interesting research problem for computer vision and image analysis. One important challenge of nonrigid surface detection is how to register a nonrigid surface mesh having a large number of free deformation parameters. This is particularly significant for detecting nonrigid surfaces from noisy observations. Nonrigid surface detection is usually regarded as a robust parameter estimation problem, which is typically solved iteratively from a good initialization in order to avoid local minima. In this paper, we propose a novel progressive finite Newton optimization scheme for the non-rigid surface detection problem, which is reduced to only solving a set of linear equations. The key of our approach is to formulate the nonrigid surface detection as an unconstrained quadratic optimization problem which has a closed-form solution for a given set of observations. Moreover, we employ a progressive active-set selection scheme, which takes advantage of the rank information of detected correspondences. We have conducted extensive experiments for performance evaluation on various environments, whose promising results show that the proposed algorithm is more efficient and effective than the existing iterative methods.","Surface treatment,
Robustness,
Object detection,
Videos,
Computer vision,
Image analysis,
Equations,
Computer science,
Registers,
Parameter estimation"
A Statistical Analysis of Brain Morphology Using Wild Bootstrapping,"Methods for the analysis of brain morphology, including voxel-based morphology and surface-based morphometries, have been used to detect associations between brain structure and covariates of interest, such as diagnosis, severity of disease, age, IQ, and genotype. The statistical analysis of morphometric measures usually involves two statistical procedures: 1) invoking a statistical model at each voxel (or point) on the surface of the brain or brain subregion, followed by mapping test statistics (e.g., t test) or their associated p values at each of those voxels; 2) correction for the multiple statistical tests conducted across all voxels on the surface of the brain region under investigation. We propose the use of new statistical methods for each of these procedures. We first use a heteroscedastic linear model to test the associations between the morphological measures at each voxel on the surface of the specified subregion (e.g., cortical or subcortical surfaces) and the covariates of interest. Moreover, we develop a robust test procedure that is based on a resampling method, called wild bootstrapping. This procedure assesses the statistical significance of the associations between a measure of given brain structure and the covariates of interest. The value of this robust test procedure lies in its computationally simplicity and in its applicability to a wide range of imaging data, including data from both anatomical and functional magnetic resonance imaging (fMRI). Simulation studies demonstrate that this robust test procedure can accurately control the family-wise error rate. We demonstrate the application of this robust test procedure to the detection of statistically significant differences in the morphology of the hippocampus over time across gender groups in a large sample of healthy subjects.","Statistical analysis,
Testing,
Surface morphology,
Robustness,
Brain modeling,
Magnetic resonance imaging,
Diseases,
Computational modeling,
Robust control,
Error analysis"
Illustrative Deformation for Data Exploration,"Much of the visualization research has focused on improving the rendering quality and speed, and enhancing the perceptibility of features in the data. Recently, significant emphasis has been placed on focus+context (F+C) techniques (e.g., fisheye views and magnification lens) for data exploration in addition to viewing transformation and hierarchical navigation. However, most of the existing data exploration techniques rely on the manipulation of viewing attributes of the rendering system or optical attributes of the data objects, with users being passive viewers. In this paper, we propose a more active approach to data exploration, which attempts to mimic how we would explore data if we were able to hold it and interact with it in our hands. This involves allowing the users to physically or actively manipulate the geometry of a data object. While this approach has been traditionally used in applications, such as surgical simulation, where the original geometry of the data objects is well understood by the users, there are several challenges when this approach is generalized for applications, such as flow and information visualization, where there is no common perception as to the normal or natural geometry of a data object. We introduce a taxonomy and a set of transformations especially for illustrative deformation of general data exploration. We present combined geometric or optical illustration operators for focus+context visualization, and examine the best means for preventing the deformed context from being misperceived. We demonstrated the feasibility of this generalization with examples of flow, information and video visualization.","Data visualization,
Focusing,
Cognitive science,
Silver,
Surgery,
Information geometry,
Geometrical optics,
Taxonomy,
Lenses,
Navigation"
Exploring Load-Balance to Dispatch Mobile Sensors in Wireless Sensor Networks,"In this paper, a hybrid sensor network consisting of static and mobile sensors is considered, where static sensors are used to detect events, and mobile sensors can move to event locations to conduct more advanced analysis. By exploring the load balance concept, we propose a CentralSD algorithm to efficiently dispatch mobile sensors. Our algorithm is general in that the numbers of mobile sensors and events can be arbitrary. When mobile sensors are more than event locations, we transform the dispatch problem to a maximum-matching problem in a weighted bipartite graph. When there are fewer mobile sensors than event locations, we propose an efficient clustering scheme to group event locations so that the maximum-matching approach can still be applied. To reduce message cost, we also develop a distributed GridSD algorithm. Simulation results are presented to verify the effectiveness of the proposed algorithms.","Wireless sensor networks,
Sensor systems,
Mobile computing,
Sensor phenomena and characterization,
Costs,
Clustering algorithms,
Dispatching,
Event detection,
Cameras,
Computer science"
Model-based Motion Estimation of Elastic Surfaces for Minimally Invasive Cardiac Surgery,"In order to assist surgeons during surgery on moving organs, e.g. minimally invasive beating heart bypass surgery, a master-slave system which synchronizes surgical instruments with the organ's motion is desired. This synchronization requires reliable estimation of the organ's motion. In this paper, we present a new approach to motion estimation based on a state motion model for a partition of the heart's surface. Its motion behavior is described by a partial differential equation whose input function is assumed to be periodic. An estimator is used on one hand to predict future model states based on reconstruction of the input function and on the other hand to incorporate noisy spatially discrete measurements in order to improve state estimation. The model-based motion estimation is evaluated using a simple heart simulator. Measurements are obtained by reconstructing 3D position of markers on a pulsating membrane by means of a stereo camera system.",
Belief Propagation in a 3D Spatio-temporal MRF for Moving Object Detection,"Previous pixel-level change detection methods either contain a background updating step that is costly for moving cameras (background subtraction) or can not locate object position and shape accurately (frame differencing). In this paper we present a belief propagation approach for moving object detection using a 3D Markov random field (MRF) model. Each hidden state in the 3D MRF model represents a pixel's motion likelihood and is estimated using message passing in a 6-connected spatio-temporal neighborhood. This approach deals effectively with difficult moving object detection problems like objects camouflaged by similar appearance to the background, or objects with uniform color that frame difference methods can only partially detect. Three examples are presented where moving objects are detected and tracked successfully while handling appearance change, shape change, varied moving speed/direction, scale change and occlusion/clutter.","Belief propagation,
Object detection,
Cameras,
Shape,
State estimation,
Message passing,
Image motion analysis,
Inference algorithms,
Change detection algorithms,
Motion detection"
Motion Correction for Coronary Stent Reconstruction From Rotational X-ray Projection Sequences,"This paper presents a new method for 3-D tomographic reconstruction of stent in X-ray cardiac rotational angiography. The method relies on 2-D motion correction from two radiopaque markerballs located on each side of the stent. The two markerballs are on a guidewire and linked to the balloon, which is introduced into the artery. Once the balloon has been inflated, deflated, and the stent deployed, a rotational sequence around the patient is acquired. Under the assumption that the guidewire and the stent have the same 3-D motion during rotational acquisition, we developed an algorithm to correct cardiac stent motion on the 2-D X-ray projection images. The 3-D image of the deployed stent is then reconstructed with the Feldkamp algorithm using all the available projections. Although the correction is an approximation, we show that the intrinsic geometrical error of our method has no visual impact on the reconstruction when the 2-D markerball centers are exactly detected and the markerballs have the same 3-D motion as the stent. Qualitative and quantitative results on simulated sequences under different realistic conditions demonstrate the robustness of the method. Finally, results from animal data acquired on a rotational angiography device are presented.","Image reconstruction,
Arteries,
Angiography,
X-ray imaging,
Tomography,
Lesions,
Medical services,
Error correction,
Motion detection,
Robustness"
Using Automated Fix Generation to Secure SQL Statements,"Since 2002, over 10% of total cyber vulnerabilities were SQL injection vulnerabilities. Since most developers are not experienced software security practitioners, a solution for correctly fixing SQL injection vulnerabilities that does not require security expertise is desirable. In this paper, we propose an automated method for removing SQL injection vulnerabilities from Java code by converting plain text SQL statements into prepared statements. Prepared statements restrict the way that input can affect the execution of the statement. An automated solution allows developers to remove SQL injection vulnerabilities by replacing vulnerable code with generated secure code. In a formative case study, we tested our automated fix generation algorithm on five toy Java programs which contained seeded SQL injection vulnerabilities and a set of object traceability issues. The results of our case study show that our technique was able remove SQL injection vulnerabilities in five different statement configurations.","Java,
Information security,
Databases,
Input variables,
Computer science,
Automatic testing,
Automation,
Code standards,
Standards development,
Programming"
"Shining a Light on Human Pose: On Shadows, Shading and the Estimation of Pose and Shape","Strong lighting is common in natural scenes yet is often viewed as a nuisance for object pose estimation and tracking. In human shape and pose estimation, cast shadows can be confused with foreground structure while self shadowing and shading variation on the body cause the appearance of the person to change with pose. Rather than attempt to minimize the effects of lighting and shadows, we show that strong lighting in a scene actually makes pose and shape estimation more robust. Additionally, by recovering multiple body poses we are able to automatically estimate the lighting in the scene and the albedo of the body. Our approach makes use of a detailed 3D body model, the parameters of which are directly recovered from image data. We provide a thorough exploration of human pose estimation under strong lighting conditions and show: 1. the estimation of the light source from cast shadows; 2. the estimation of the light source and the albedo of the body from multiple body poses; 3. that a point light and cast shadows on the ground plane can be treated as an additional ""shadow camera"" that improves pose and shape recovery, particularly in monocular scenes. Additionally we introduce the notion of albedo constancy which employs lighting normalized image data for matching. Our experiments with multiple subjects show that rather than causing problems, strong lighting improves human pose and shape estimation.","Humans,
Shape,
Layout,
Cameras,
Light sources,
Lighting,
Robustness,
Computer science,
Shadow mapping,
Biological system modeling"
Adaptive Optimization of Rate Adaptation Algorithms in Multi-Rate WLANs,"Rate adaptation is one of the basic functionalities in today's 802.11 wireless LANs (WLANs). Although it is primarily designed to cope with the variability of wireless channels and achieve higher system spectral efficiency, its design needs careful consideration of cross-layer dependencies, in particular, link-layer collisions. Most practical rate adaptations focus on the time-varying characteristics of wireless channels, ignoring the impact of link-layer collisions. As a result, they may lose their effectiveness due to unnecessary rate downshift wrongly triggered by the collisions. Some recently proposed rate adaptations use RTS/CTS to suppress the collision effect by differentiating collisions from channel errors. The RTS/CTS handshake, however, incurs significant overhead and is rarely activated in infrastructure WLANs. In this paper, we introduce a new approach for optimizing the operation of rate adaptations by adjusting the rate-increasing and decreasing parameters based on link-layer measurement. To construct the algorithm, we study the impact of rate-increasing and decreasing thresholds on performance and show that dynamic adjustment of thresholds is an effective way to mitigate the collision effect in multi-user environments. Our method does not require additional probing overhead incurred by RTS/CTS exchanges and may be practically deployed without change in firmware. We demonstrate the effectiveness of our solution, comparing with existing approaches through extensive simulations.","Physical layer,
Microprogramming,
Computer science,
Wireless LAN,
Collision mitigation,
Counting circuits,
Fluctuations,
Modulation coding,
Channel coding,
Fading"
A Fast Evolutionary Algorithm for Traveling Salesman Problem,"In this paper we proposed a new algorithm based on Inver-over operator, for traveling salesman problems (TSP). Inver-over is based on simple inversion; however, knowledge taken from other individuals in the population influences its action. In the new algorithm we use some new strategies including selection operator, replace operator and some new control strategy, which have been proved to be very efficient to accelerate the converge speed. We also use this approach to solve dynamic TSP. A dynamic TSP is harder than a general TSP, which is a NP-hard problem, because the city number and the cost matrix of a dynamic TSP are time varying, the algorithm to solve the dynamic TSP problem, which is the hybrid of EN and Inver-Over algorithm. Through the experiment, the new algorithm shows great efficiency in solving the static TSP and dynamic TSP.",
VMNet: Realistic Emulation of Wireless Sensor Networks,"Many research activities on wireless sensor networks (WSNs) need detailed performance statistics about protocols, systems, and applications; however, current simulation tools and testbeds lack mechanisms to report these statistics realistically and conveniently. To address this need, we have developed a WSN emulator, VMNet. VMNet emulates networked sensor nodes at the level of CPU clock cycles and executes the binary code of real applications directly. It emulates the radio channel with loss and noise as well as emulates the peripherals in sufficient detail. Moreover, VMNet takes parameter values from the real world and logs detailed runtime information of emulated nodes. Consequently, the application performance, both in response time and in power consumption, is reported realistically in VMNet, as demonstrated by our comparison studies with real sensor networks","Emulation,
Wireless sensor networks,
Statistical analysis,
Wireless application protocol,
System testing,
Clocks,
Binary codes,
Runtime,
Delay,
Energy consumption"
A Layered Optimization Approach for Redundant Reader Elimination in Wireless RFID Networks,"The problem of redundant RFID reader elimination has instigated researchers to propose different optimization heuristics due to the rapid advance of technologies in large scale RFID systems. In this paper, we present a layered elimination optimization (LEO) which is an algorithm independent technique aims to detect maximum amount of redundant readers could be safely removed or turned off with preserving original RFID network coverage. A significant improvement of the LEO scheme is that number of ""write-to-tag"" operations could be largely reduced during the redundant reader identification phase. Moreover, LEO is a distributed scheme which does not need to collect global information for centralizing control, leading no communications and synchronizations among RFID readers. To evaluate the performance of the proposed techniques, we have implemented the LEO technique along with another redundant reader identification algorithm and other hybrid schemes. In experimental results, the LEO is shown to be effective and provides superior performance in terms of larger number of redundant reader could be detected and with lower algorithm overheads.","Radiofrequency identification,
Low earth orbit satellites,
Wireless sensor networks,
RFID tags,
Large-scale systems,
Computer science,
Communication system control,
Radio frequency,
Temperature sensors,
Computer networks"
A new GTS allocation scheme for IEEE 802.15.4 networks with improved bandwidth utilization,"The IEEE 802.15.4 standard enables device level wireless connectivity in personal area networks. Its medium access control (MAC) protocol supports the exclusive use of a wireless channel through guaranteed time slot (GTS). However, the bandwidth underutilization problem occurs in GTSs when the used bandwidth is less than the available. In this paper, a new GTS scheme is presented to allow more devices to share the bandwidth within the same period. The evaluation and analysis reveals that the bandwidth utilization is improved.","Bandwidth,
Media Access Protocol,
Sensor phenomena and characterization,
Monitoring,
Personal area networks,
Computer science,
Wireless application protocol,
Access protocols,
Medical services,
Wireless sensor networks"
AIS for misbehavior detection in wireless sensor networks: Performance and design principles,"A sensor network is a collection of wireless devices that are able to monitor physical or environmental conditions. These devices are expected to operate autonomously, be battery powered and have very limited computational capabilities. This makes the task of protecting a sensor network against misbehavior or possible malfunction a challenging problem. In this document we discuss performance of Artificial immune systems (AIS) when used as the mechanism for detecting misbehavior. We concentrate on performance of respective genes; genes are necessary to measure a network's performance from a sensor's viewpoint. We conclude that the choice of genes has a profound influence on the performance of the AIS. We identified a specific MAC layer based gene that showed to be especially useful for detection. We also discuss implementation details of AIS when used with sensor networks.","Wireless sensor networks,
Humans,
Immune system,
Computer networks,
Protocols,
Detectors,
Capacitive sensors,
Protection,
Microorganisms,
Condition monitoring"
Network Traffic Classification Using K-means Clustering,"Network traffic classification and application identification provide important benefits for IP network engineering, management and control and other key domains. Current popular methods, such as port-based and payload-based, have shown some disadvantages, and the machine learning based method is a potential one. The traffic is classified according to the payload-independent statistical characters. This paper introduces the different levels in network traffic-analysis and the relevant knowledge in machine learning domain, analysis the problems of port-based and payload-based methods in traffic classification. Considering the priority of the machine learning-based method, we experiment with unsupervised K-means to evaluate the efficiency and performance. We adopt feature selection to find an optimal feature set and log transformation to improve the accuracy. The experimental results on different datasets convey that the method can obtain up to 80% overall accuracy, and, after a log transformation, the accuracy is improved to 90% or more.","Telecommunication traffic,
Communication system traffic control,
IP networks,
Internet,
Protocols,
Computer networks,
Learning systems,
Machine learning,
Spine,
Computer science"
An Approach for Web Services Composition Based on QoS and Discrete Particle Swarm Optimization,"The discrete particle swarm optimization (DTSO) has many merits, for example simple, rapid convergence speed. This paper proposes an approach for web services composition based on quality of service (QoS) and DTSO. In the case of lots of web services, this approach has an obvious effect. The simulation results comparing with genetic algorithms (GAs) show that it can produce better results.","Web services,
Particle swarm optimization,
Quality of service,
Switches,
Genetic algorithms,
Software engineering,
Availability,
Artificial intelligence,
Distributed computing,
Computer science"
Design and Implementation of Cross-Domain Cooperative Firewall,"Security and privacy are two major concerns in supporting roaming users across administrative domains. In current practices, a roaming user often uses encrypted tunnels, e.g., Virtual Private Networks (VPNs), to protect the secrecy and privacy of her communications. However, due to its encrypted nature, the traffic flowing through these tunnels cannot be examined and regulated by the foreign network's firewall, which may lead the foreign network widely open to various attacks from the Internet. This threat can be alleviated if the users reveal their traffic to the foreign network or the foreign network reveals its firewall rules to the tunnel endpoints. However, neither approach is desirable in practice due to privacy concerns. In this paper, we propose a Cross-Domain Cooperative Firewall (CDCF) that allows two collaborative networks to enforce each other's firewall rules in an oblivious manner. In CDCF, when a roaming user establishes an encrypted tunnel between his home network and the foreign network, the tunnel endpoint (e.g., a VPN server) can regulate the traffic and enforce the foreign network's firewall rules, without knowing these rules. The key ingredients in CDCF are the distribution of firewall primitives across network domains, and the enabling technique of efficient oblivious membership verification. We have implemented CDCF and integrated it with the OpenVPN software, and evaluated its performance using extensive experiments. Our results show that CDCF can protect the foreign network from encrypted tunnel traffic with minimal overhead.","Cryptography,
Telecommunication traffic,
Privacy,
Virtual private networks,
Protection,
IP networks,
Collaborative work,
Home automation,
Network servers,
Software performance"
How Software Designs Decay: A Pilot Study of Pattern Evolution,"A common belief is that software designs decay as systems evolve. This research examines the extent to which software designs actually decay by studying the aging of design patterns in successful object oriented systems. Aging of design patterns is measured using various types of decay indices developed for this research. Decay indices track the internal structural changes of a design pattern realization and the code that surrounds the realization. Hypotheses for each kind of decay are tested. We found that the original design pattern functionality remains, and pattern decay is due to the ""grime "", non-pattern code, that grows around the pattern realization.","Software design,
Open source software,
Aging,
Software engineering,
Software systems,
Computer science,
Permission,
Software measurement,
Testing,
Cost function"
Mixed-Mode Simulation and Analysis of Digital Single Event Transients in Fast CMOSICs,"Single event transient (SET) pulses produced from heavy ion irradiation in digital integrated circuits (ICs) are modeled and analyzed using a mixed-mode approach, that is, three-dimensional (3-D) semiconductor device simulation coupled with a circuit solver. In this paper, we analyze the factors affecting the generation and propagation of digital SET pulses in fast CMOS ICs. Our mixed-mode simulations of various ion strike locations allowed to obtain agreement with measured data and explain the earlier published wide distribution of SET pulse widths created by heavy ion radiation in digital CMOS ICs at a given linear energy transfer (LET) value, which was observed experimentally, but not fully understood. We also indicate that the transient charge-collection current pulse (width and shape) on the struck device node is not directly related to the SET voltage pulse that will propagate through a logic circuit, and therefore current pulses alone should not be treated as a measure of DSET.","Analytical models,
Discrete event simulation,
Transient analysis,
Pulse measurements,
Pulse circuits,
Space vector pulse width modulation,
Circuit simulation,
Pulse shaping methods,
Shape measurement,
Digital integrated circuits"
Analysis of Energy Consumption in Clustered Wireless Sensor Networks,"Clustering is one of the most important approaches used in wireless sensor networks to save energy. Transmission range affects the clustering and in turn affects the energy consumption in the network. We establish an energy consumption model for clustered wireless sensor networks and solve the optimal transmission range problem. Using this model, the total energy consumption can be estimated beforehand based on the traffic pattern, energy model, and network deployment parameters. This model provides an insight into the energy consumption behavior in clustered wireless sensor networks and the relationship among major factors. The optimal transmission range for energy consumption is a function of the traffic load and the node density, but the effect of node density is very limited","Energy consumption,
Wireless sensor networks,
Telecommunication traffic,
Throughput,
Traffic control,
Clustering algorithms,
Network topology,
Routing,
Broadcasting,
Computer science"
Trusted Greedy Perimeter Stateless Routing,"Ad-hoc networks generally comprise of mobile wireless nodes having limited communication and computation resources. These nodes execute special routing protocols, which help to establish multi-hop communication despite a dynamic topology. The greedy perimeter stateless routing (GPSR) protocol is one such routing protocol that is frequently used to establish routes in an ad-hoc or sensor network. However, for its precise execution, it is imperative that all nodes depict sustained benevolent behaviour. However, such an altruistic setting would never work in a wireless environment, which is intrinsically physically insecure. Consequently, participating malicious nodes may launch an array of attacks against the routing protocol leading to route severing, elongation or loop creation. In this paper, we present an improved variant of the GPSR protocol that uses the inherent characteristics of the routing process to assess the trust in the network nodes. These trust levels are then used to influence the routing decisions so as to circumvent malevolent nodes in the network. Extensive simulations indicate that the packet delivery ratio of the trusted GPSR protocol surpasses that of the standard GPSR by as much as 30% when as many as 50% of the nodes are acting maliciously in the network.","Routing protocols,
Ad hoc networks,
Wireless sensor networks,
Mobile communication,
Computer science,
Software engineering,
Road transportation,
Australia,
Mobile computing,
Computer networks"
"log
n
P
and
log
3
P
: Accurate Analytical Models of Point-to-Point Communication in Distributed Systems","Many existing models of point-to-point communication in distributed systems ignore the impact of memory and middleware. Including such details may make these models impractical. Nonetheless, the growing gap between memory and CPU performance combined with the trend toward large-scale, clustered shared memory platforms implies an increased need to consider the impact of middleware on distributed communication. We present a general software-parameterized model of point-to-point communication for use in performance prediction and evaluation. We illustrate the utility of the model in three ways: 1) to derive a simplified, useful, more accurate model of point-to-point communication in clusters of SMPs, 2) to predict and analyze point-to-point and broadcast communication costs in clusters of SMPs, and 3) to express, compare, and contrast existing communication models. Though our methods are general, we present results on several Linux clusters to illustrate practical use on real systems",
Map-Enhanced UAV Image Sequence Registration and Synchronization of Multiple Image Sequences,"Registering consecutive images from an airborne sensor into a mosaic is an essential tool for image analysts. Strictly local methods tend to accumulate errors, resulting in distortion. We propose here to use a reference image (such as a high resolution map image) to overcome this limitation. In our approach, we register a frame in an image sequence to the map using both frame-to-frame registration and frame-to-map registration iteratively. In frame-to-frame registration, a frame is registered to its previous frame. With its previous frame been registered to the map in the previous iteration, we can derive an estimated transformation from the frame to the map. In frame-to-map registration, we warp the frame to the map by this transformation to compensate for scale and rotation difference and then perform an area based matching using mutual information to find correspondences between this warped frame and the map. These correspondences together with the correspondences in previous frames could be regarded as correspondences between the partial local mosaic and the map. By registering the partial local mosaic to the map, we derive a transformation from the frame to the map. With this two-step registration, the errors between each consecutive frames are not accumulated. We then extend our approach to synchronize multiple image sequences by tracking moving objects in each image sequence, and aligning the frames based on the object's coordinates in the reference image.","Image sequences,
Unmanned aerial vehicles,
Mutual information,
Streaming media,
Computer science,
Image sensors,
Image analysis,
Image sequence analysis,
Computer errors,
Image resolution"
Simultaneous Covariance Driven Correspondence (CDC) and Transformation Estimation in the Expectation Maximization Framework,"This paper proposes a new registration algorithm, Co-variance Driven Correspondences (CDC), that depends fundamentally on the estimation of uncertainty in point correspondences. This uncertainty is derived from the covariance matrices of the individual point locations and from the covariance matrix of the estimated transformation parameters. Based on this uncertainty, CDC uses a robust objective function and an EM-like algorithm to simultaneously estimate the transformation parameters, their covariance matrix, and the likely correspondences. Unlike the Robust Point Matching (RPM) algorithm, CDC requires neither an annealing schedule nor an explicit outlier process. Experiments on synthetic and real images using a polynomial transformation models in 2D and in 3D show that CDC has a broader domain of convergence than the well-known Iterative Closest Point (ICP) algorithm and is more robust to missing or extraneous structures in the data than RPM.","Uncertainty,
Covariance matrix,
Robustness,
Parameter estimation,
Iterative algorithms,
Scheduling algorithm,
Annealing,
Polynomials,
Convergence,
Iterative closest point algorithm"
Spectral Graph Theory and its Applications,"Spectral graph theory is the study of the eigenvalues and eigenvectors of matrices associated with graphs. In this tutorial, we will try to provide some intuition as to why these eigenvectors and eigenvalues have combinatorial significance, and will sitn'ey some of their applications.","Graph theory,
Eigenvalues and eigenfunctions,
Laplace equations,
Computer science,
Books,
Application software,
Combinatorial mathematics,
Physics education,
Image segmentation,
Testing"
Visual Analytics on Mobile Devices for Emergency Response,"Using mobile devices for visualization provides a ubiquitous environment for accessing information and effective decision making. These visualizations are critical in satisfying the knowledge needs of operators in areas as diverse as education, business, law enforcement, protective services, medical services, scientific discovery, and homeland security. In this paper, we present an efficient and interactive mobile visual analytic system for increased situational awareness and decision making in emergency response and training situations. Our system provides visual analytics with locational scene data within a simple interface tailored to mobile device capabilities. In particular, we focus on processing and displaying sensor network data for first responders. To verify our system, we have used simulated data of The Station nightclub fire evacuation.","Visual analytics,
Data visualization,
Decision making,
Information analysis,
Terrorism,
Mobile computing,
Computer graphics,
Sensor systems,
Data analysis,
Handheld computers"
A Threat Model Driven Approach for Security Testing,"In this paper, we propose a novel threat model-driven security testing approach for detecting undesirable threat behavior at runtime. Threats to security policies are modelled with UML (unified modeling language) sequence diagrams. From a design-level threat model we extract a set of threat traces, each of which is an event sequence that should not occur during the system execution. The same threat model is also used to decide what kind of information should be collected at runtime and to guide the code instrumentation. The instrumented code is recompiled and executed using test cases randomly generated. The execution traces are collected and analyzed to verify whether the aforementioned undesirable threat traces are matched. If an execution trace is an instance of a threat trace, security violations are reported and actions should be taken to mitigate the threat in the system. Thus the linkage between models, code implementations, and security testing are extended to form a systematic methodology that can test certain security policies.","Unified modeling language,
System testing,
Software testing,
National security,
Computer science,
Programming,
Computer security,
Runtime,
Instruments,
Neodymium"
Informative SNP Selection Methods Based on SNP Prediction,"The search for the association between complex diseases and single nucleotide polymorphisms (SNPs) or haplotypes has recently received great attention. For these studies, it is essential to use a small subset of informative SNPs, i.e., tag SNPs, accurately representing the rest of the SNPs. Tag SNP selection can achieve: 1) considerable budget savings by genotyping only a limited number of SNPs and computationally inferring all other SNPs or 2) necessary reduction of the huge SNP sets (obtained, e.g., from Affymetrix) for further fine haplotype analysis. In this paper, we show that the tag SNP selection strongly depends on how the chosen tags will be used-advantage of one tag set over another can only be considered with respect to a certain prediction method. We show how to separate tag selection from SNP prediction and propose greedy and local-minimization algorithms for tag SNP selection. We give two novel approaches to SNP prediction based on multiple linear regression (MLR) and support vector machines (SVMs). An extensive experimental study on various datasets including ten regions from hapMap project shows that the MLR prediction combined with stepwise tag selection uses fewer tags than the state-of-the-art method of Halperin The MLR-based method also uses on average 30% fewer tags than IdSelect for statistical covering all SNPs. The tag selection based on SVM SNP prediction uses fewer tags to achieve the same prediction accuracy as the methods of Halldorsson","Diseases,
Support vector machines,
Biological cells,
Computer science,
Prediction methods,
Linear regression,
Accuracy,
Genomics,
Bioinformatics"
Collision Pattern Modeling and Real-Time Collision Detection at Road Intersections,"The crash rate in road intersection demonstrates the need for a fast and accurate collision detection system. Ubiquitous computing research provides a significant opportunity to develop novel ways of improving road intersection safety. The existing intersection collision warning or avoidance systems are mostly built to suit a particular intersection. We suggest that an intersection collision detection system should be able to adapt to different types of intersections by acquiring the collision patterns of the intersection through data mining. Collision patterns that are specific to that intersection are stored in a knowledge base to select vehicles which are exposed to a high risk of collision. This algorithm increases the speed of collision detection calculation, as detection is not applied on all possible pairs in an intersection. The performance and accuracy of the algorithm are evaluated. This evaluation is done on a developed simulation bed and the results are presented.","Road accidents,
Australia,
Vehicle crash testing,
Intelligent transportation systems,
Computer crashes,
Road safety,
Data mining,
Traffic control,
Leg,
Information technology"
Cognitive Radio Networks: How Much Spectrum Sharing is Optimal?,"We explore the performance tradeoff between opportunistic and regulated access inherent in the design of multiuser cognitive radio networks. We consider a cognitive radio system with sensing limits at the secondary users and interference tolerance limits at the primary and secondary users. Our objective is to determine the optimal amount of spectrum sharing, i.e., the number of secondary users that maximizes the total deliverable throughput in the system. We begin with the case of perfect primary user detection and zero interference tolerance at each of the primary and secondary nodes. We find that the optimal fraction of licensed users lies between the two extremes of fully opportunistic and fully licensed operation and is equal to the traffic duty cycle. For the more involved case of imperfect sensing and non-zero interference tolerance constraints, we provide numerical simulation results to study the tradeoff between licensing and autonomy and the impact of primary user sensing and interference tolerance on the deliverable throughput.","Cognitive radio,
Throughput,
Traffic control,
Licenses,
Bandwidth,
Radiofrequency interference,
Face detection,
Access protocols,
Protection,
Computer science"
On the Performance of Iterative Demapping and Decoding Techniques over Quasi-Static Fading Channels,"In this paper, we investigate in detail the performance of iterative demapping and decoding techniques over quasi-static fading channels both with and without antenna diversity. In particular, we consider the effect on the system performance of various mapping schemes, different coding schemes, the inter- leaver size as well as space diversity. Results demonstrate that over quasi-static fading channels characterized by significant antenna diversity, mappings traditionally optimized for iterative receivers (e.g., Boronka mapping) outperform mappings more appropriate for non-iterative receivers (e.g., Gray mapping). In contrast, over quasi-static fading channels characterized by limited antenna diversity, Gray mapping always outperform Boronka mapping for all Eb/N0 and all iterations. Strikingly, this situation is in sharp contrast to that in the AWGN case. We note that we can further improve the performance of non-iterative systems (e.g., Gray mapping) having limited antenna diversity by increasing the memory size and removing the interleaving.",
Admission Control and Interference-Aware Scheduling in Multi-hop WiMAX Networks,"Multi-hop WiMAX networks based on IEEE 802.16 has the potential of easily providing high-speed wireless broadband access to areas with little or no existing wired infrastructure. WiMAX technology can be used as ""last mile"" broadband connections to deliver streaming audio or video to clients. Thus, quality of service (QoS) is very important for WiMAX networks. Providing QoS in multi-hop WiMAX networks such as WiMAX mesh or mobile multi-hop relay networks is challenging as multiple links can interfere with each other if they are scheduled at the same time. We propose efficient heuristic algorithms for scheduling flows in a centrally scheduled multi-hop WiMAX network. The proposed algorithms guarantee bandwidth and delay constraints of flows and allow multiple non-interfering links to be scheduled at the same time. We also define a ""schedule efficiency"" metric for comparing different flow scheduling algorithms. The simulation results show that the ""schedule flow subchannel"" algorithm leads to the best schedule efficiency.","Admission control,
Interference,
Spread spectrum communication,
WiMAX,
Scheduling algorithm,
Quality of service,
Streaming media,
Relays,
Heuristic algorithms,
Bandwidth"
A practically constant-time MPI Broadcast Algorithm for large-scale InfiniBand Clusters with Multicast,"An efficient implementation of the MPI_BCAST operation is crucial for many parallel scientific applications. The hardware multicast operation seems to be applicable to switch-based infiniband cluster systems. Several approaches have been implemented so far, however there has been no production-ready code available yet. This makes optimal algorithms to a subject of active research. Some problems still need to be solved in order to bridge the semantic gap between the unreliable multicast and MPI_BCAST. The biggest of those problems is to ensure the reliable data transmission in a scalable way. Acknowledgement-based methods that scale logarithmically with the number of participating MPI processes exist, but they do not meet the supernormal demand of high-performance computing. We propose a new algorithm that performs the MPI_BCAST operation in a practically constant time, independent of the communicator size. This method is well suited for large communicators and (especially) small messages due to its good scaling and its ability to prevent parallel process skew. We implemented our algorithm as a collective component for the Open MPI framework using native infiniband multicast and we show its scalability on a cluster with 116 compute nodes, where it saves up to 41% MPI_BCAST latency in comparison to the ""TUNED"" OpenMPl collective.",
Concurrent Error Detection Methods for Asynchronous Burst-Mode Machines,"Asynchronous controllers exhibit various characteristics that limit the effectiveness and applicability of the concurrent error detection (CED) methods developed for their synchronous counterparts. Asynchronous burst-mode machines (ABMMs), for example, do not have a global clock to synchronize the ABMM with the additional circuitry that is typically used by synchronous CED methods (for example, duplication). Therefore, performing effective CED in ABMMs requires a synchronization method that will appropriately enable the checker (for example, comparator) in order to avoid false alarms. Also, ABMMs contain redundant logic, which guarantees the hazard-free operation required for correct interaction between the circuit and its environment. Redundant logic, however,' allows some single event transients to manifest themselves only as hazards but not as logic discrepancies. Therefore, performing effective CED in ABMMs requires the ability to detect hazards with which synchronous CED methods are not concerned. In this work, we first devise hardware solutions for performing checking synchronization and hazard detection. We then demonstrate how these solutions enable the development of three complete CED methods for ABMMs. The first method (duplication-based CED) is an adaptation of the well-known duplication method within the context of ABMMs. The second method (transition-triggered CED) is a variation of duplication wherein the implementation cost is reduced by allowing hazards in the duplicate circuit. ln contrast to these two methods, which are nonintrusive, the third method (Berger code-based CED) is intrusive since it requires reencoding of the ABMM with check symbols based on the Berger code. Although this intrusiveness may slightly impact performance, Berger code-based CED incurs the lowest area overhead among the three methods, as indicated through experimental results","error detection codes,
asynchronous circuits"
Emotional Architecture for the Humanoid Robot Head ROMAN,"Humanoid robots as assistance or educational robots is an important research topic in the field of robotics. Especially the communication of those robots with a human operator is a complex task since more than 60% of human communication is conducted non-verbally by using facial expressions and gestures. Although several humanoid robots have been designed it is unclear how a control architecture can be developed to realize a robot with the ability to interact with humans in a natural way. This paper therefore presents a behavior-based emotional control architecture for the humanoid robot head ROMAN. The architecture is based on 3 main parts: emotions, drives and actions which interact with each other to realize the human-like behavior of the robot. The communication with the environment is realized with the help of different sensors and actuators which will also be introduced in this paper.","Humanoid robots,
Magnetic heads,
Educational robots,
Human robot interaction,
Robotics and automation,
Machine intelligence,
Eyes,
Computer science,
Communication system control,
Robot sensing systems"
A Bayesian network based trust model for improving collaboration in mobile ad hoc networks,"Functioning as fully decentralised distributed systems, without the need of predefined infrastructures, mobile ad hoc networks provide interesting solutions when setting up dynamic and flexible applications. However, these systems also bring up some problems. In such open environments, it is difficult to discover among the nodes, which are malicious and which are not, in order to be able to choose good partners for cooperation. One solution for this to be possible, is for the entities to be able to evaluate the trust they have in each other and, based on this trust, determine which entities they can cooperate with. In this paper, we present a trust model adapted to ad hoc networks and, more generally, to distributed systems. This model is based on Bayesian networks, a probabilistic tool which provides a flexible means of dealing with probabilistic problems involving causality. The model evaluates the trust in a server according, both, to direct experiences with the server and recommendations concerning its service. We show, through a simulation, that the proposed model can determine the best server out of a set of eligible servers offering a given service. Such a trust model, when applied to ad hoc networks, tends to increase the QoS of the various services used by a host. This, when applied to security related services thus increases the overall security of the hosts.","Bayesian methods,
Collaboration,
Mobile ad hoc networks,
Network servers,
Computer science,
Ad hoc networks,
Public key,
Application software,
Degradation,
Software systems"
Measuring Semantic Similarity in Wordnet,"Semantic similarity between words is a generic problem for many applications of computational linguistics and artificial intelligence. The difficulty of this task lies in how to find an effective way to simulate the process of human judgment of word similarity by combining and processing a number of information sources. This paper presents a novel model to measure semantic similarity between words in the WordNet, using edge-counting techniques. The fundamental idea of this model is based on the assumption that human judgment process for semantic similarity can be simulated by the ratio of common features to the total features between words. According to the experiment against a benchmark set by human similarity judgment, our measure achieves a better result. The correlation is 0.926 with average human judgment on a standard 28 word-pair dataset, which outperforms other previous reported methods.","Humans,
Length measurement,
Taxonomy,
Machine learning,
Cybernetics,
Joining processes,
Brain modeling,
Benchmark testing,
Solid modeling,
Computer science"
Are Digraphs Good for Free-Text Keystroke Dynamics?,"Research in keystroke dynamics has largely focused on the typing patterns found in fixed text (e.g. userid and passwords). In this regard, digraphs and trigraphs have proven to be discriminative features. However, there is increasing interest in free-text keystroke dynamics, in which the user to be authenticated is free to type whatever he/she wants, rather than a pre-determined text. The natural question that arises is whether digraphs and trigraphs are just as discriminative for free text as they are for fixed text. We attempt to answer this question in this paper. We show that digraphs and trigraphs, if computed without regard to what word was typed, are no longer discriminative. Instead, word-specific digraphs/trigraphs are required. We also show that the typing dynamics for some words depend on whether they are part of a larger word. Our study is the first to investigate these issues, and we hope our work will help guide researchers looking for good features for free-text keystroke dynamics.","Timing,
Biometrics,
Security,
Authentication,
Keyboards,
Fingerprint recognition,
Iris,
Pattern recognition,
Feature extraction,
Delay"
30 GHz CMOS Low Noise Amplifier,30 GHz low noise amplifier was designed and fabricated in a 90 nm digital CMOS process. The mm-wave amplifier has a peak gain of 20 dB at 28.5 GHz and a 3 dB bandwidth of 2.6 GHz with the input and output matching better than 12 dB and 17 dB over the entire band respectively. The NF is 2.9 dB at 28 GHz and less than 4.2 dB across the band and it can deliver 2 dBm of power to a matched load at its 1 dB compression point. The amplifier has a measured linearity of IIIP3=-7.5 dBm. It consumes 16.25 mW of power using a low supply voltage of 1 V and occupies an area (excluding the pads) of 1600 mum x 420 mum.,"Low-noise amplifiers,
CMOS process,
Frequency,
Inductors,
MIM capacitors,
Circuit noise,
CMOS technology,
Insulation,
Transistors,
Semiconductor device modeling"
Dynamical Systems for Discovering Protein Complexes and Functional Modules from Biological Networks,"Recent advances in high throughput experiments and annotations via published literature have provided a wealth of interaction maps of several biomolecular networks, including metabolic, protein-protein, and protein-DNA interaction networks. The architecture of these molecular networks reveals important principles of cellular organization and molecular functions. Analyzing such networks, i.e., discovering dense regions in the network, is an important way to identify protein complexes and functional modules. This task has been formulated as the problem of finding heavy subgraphs, the heaviest k-subgraph problem (k-HSP), which itself is NP-hard. However, any method based on the k-HSP requires the parameter k and an exact solution of k-HSP may still end up as a ""spurious"" heavy subgraph, thus reducing its practicability in analyzing large scale biological networks. We proposed a new formulation, called the rank-HSP, and two dynamical systems to approximate its results. In addition, a novel metric, called the standard deviation and mean ratio (SMR), is proposed for use in ""spurious"" heavy subgraphs to automate the discovery by setting a fixed threshold. Empirical results on both the simulated graphs and biological networks have demonstrated the efficiency and effectiveness of our proposal","Bioinformatics,
Large-scale systems,
Biology computing,
Genomics,
Fungi,
Protein engineering,
Computer science,
Throughput,
Cellular networks,
Biological system modeling"
New Directions in Contact Free Hand Recognition,"The ability to quickly compute hand geometry measurements from a freely posed hand offers advantages to biometric identification systems. While hand geometry systems are not new, typical measurements of lengths and widths of fingers and palms require rigid placement of the hand against pegs. Slight deviations in hand position, finger stretch or pressure can yield different measurements. This paper offers novel approaches to computing hand geometry measurements from frontal views of freely posed hands. These approaches offer advantages in hygiene, comfort and reliability. Our algorithms segment the hand from a known background under spot lights and locate feature points along the fingers and wrists. Given a database of 54 hand images, with three different images of the same hand of each subject, our approach uniquely identified a previously unseen hand with an overall accuracy of 92%.","Fingers,
Computational geometry,
Biometrics,
Length measurement,
Position measurement,
Pressure measurement,
Image segmentation,
Wrist,
Image databases,
Spatial databases"
Modeling the Marginal Distributions of Complex Wavelet Coefficient Magnitudes for the Classification of Zoom-Endoscopy Images,"In this paper, we propose a set of new image features for the classification of zoom-endoscopy images. The feature extraction step is based on fitting a two-parameter Weibull distribution to the wavelet coefficient magnitudes of sub-bands obtained from a complex wavelet transform variant. We show, that the shape and scale parameter possess more discriminative power than the classic mean and standard deviation based features for complex subband coefficient magnitudes. Furthermore, we discuss why the commonly used Rayleigh distribution model is suboptimal in our case.","Wavelet coefficients,
Cancer,
Filters,
Wavelet transforms,
Colonic polyps,
Delay,
Colon,
Discrete wavelet transforms,
Feature extraction,
Lesions"
A Software Birthmark Based on Dynamic Opcode n-gram,"A kind of dynamic opcode n-gram software birthmark is proposed in this paper based on Myles' software birthmark (in which static opcode n-gram set is regarded as the software birthmark). The dynamic opcode n-gram set is regarded as the software birthmark which is extracted from the dynamic executable instruction sequence of the program. And the new birthmark can not only keep the advantages of feature n-gram set based on static opcode, but also possesses high robustness to code compression, encryption, packing. The algorithm which is to evaluate the similarity of the birthmarks of two programs is improved employing the theory of Probability and Statistic. As a result, the time complexity of the improved algorithm decreases to 0(n) from O(n2), while the space complexity keeps unchanged. Finally, the validity of the scheme is proved by experiments.","Cryptography,
Computer crime,
Probability,
Statistics,
Invasive software,
Java,
Information science,
Robustness,
Genetic mutations,
Frequency"
Modeling and Verification of Web Services Composition based on CPN,"As the capability of an individual Web service is limited, it's necessary to create new functionalities with existing Web services. Web services composition is the ability to create a new value-added service by incorporating some existing Web services together. An important challenge for Web services composition is how to ensure the correctness and reliability of the WS-BPEL (Web services business process execution language) process and the composition. A graphical and formal modeling tool suitable for solving this problem is CPN (Colored Petri net), which provides formal semantics and a number of analysis techniques for modeling and verifying web services composition. This paper proposes a CPN-based model for web services composition, describes how to translate WS-BPEL to CPN model, and discusses the analysis and verification of services composition. The model enables efficient composition of Web services and validates the correctness of composition by formal verification.","Web services,
Petri nets,
Educational institutions,
Computer science,
Artificial intelligence,
Automation,
Semantic Web,
Parallel processing,
Formal verification,
Application software"
On-Line Handwritten Text Line Detection Using Dynamic Programming,In this paper we propose a novel approach to th tion of on-line handwritten text lines based on dynamic programming. We try to find the paths with the minimum cost between two consecutive text lines. Most steps of the proposed algorithm are based on off-line information. Hence the method can also be applied to off-line documents after a few minor changes. In our experiments we show that this dynamic programming based approach is better than a common on-line segmentation procedure.,"Dynamic programming,
Handwriting recognition,
Computer science,
Mathematics,
Text recognition,
Humans,
Cost function,
Filters,
Filtering,
Text analysis"
Cognitive Social Simulation Incorporating Cognitive Architectures,"Agent-based social simulation modeling, a social phenomena on the basis of models of autonomous agents has grown tremendously in recent decades. Researchers use this approach to study a wide range of social and economic issues, including social beliefs and norms, resource allocation, traffic patterns, social cooperation, stock market dynamics, group interaction and dynamics, and organizational decision making. Agent-based social simulation with multiagent systems in social computing can benefit from incorporating cognitive architectures. A cognitive architecture is a domain-generic computational cognitive model that captures essential structures and processes of the individual mind for the purpose of a broad (multiple-domain) analysis of cognition and behaviour.","Computational modeling,
Computer architecture,
Autonomous agents,
Resource management,
Traffic control,
Stock markets,
Decision making,
Multiagent systems,
Social network services,
Cognition"
Towards Automated Requirements Triage,"Budgetary restrictions and time-to-market deadlines often require stakeholders to prioritize requirements and decide which ones to include in a given product release. Lack of an effective prioritization and triage process can lead to problems such as missed deadlines, disorganized development efforts, and late discovery of architecturally significant requirements. Existing prioritization techniques do not provide sufficient automation for large projects with hundreds of stakeholders and thousands of potentially conflicting requests and requirements. This paper therefore proposes an approach for automating a significant part of the prioritization process. The proposed method utilizes a probabilistic traceability model combined with a standard hierarchical clustering algorithm to cluster incoming stakeholder requests into hierarchical feature sets. Additional cross-cutting clusters are then generated to represent factors such as architecturally significant requirements or impacted business goals. Prioritization decisions are initially made at the feature level and then more critical requirements are promoted according to their relationships with the identified cross-cutting concerns. The approach is illustrated and evaluated through a case study applied to the requirements of the ice breaker system.","Time to market,
Project management,
Medical treatment,
Computer science,
Information systems,
Automation,
Clustering algorithms,
Ice,
Personnel,
Security"
Chronos: Feedback Control of a Real Database System Performance,"It is challenging to process transactions in a timely fashion using fresh data, e.g., current stock prices, since database workloads may considerably vary due to dynamic data/resource contention. Further, transaction timeliness and data freshness requirements may compete for system resources. In this paper, we propose a novel feedback control model to support the desired data service delay by managing the size of the ready queue, which indicates the amount of the backlog in the database. We also propose a new self-adaptive update policy to adapt the freshness of cold data in a differentiated manner based on temporal data access and update patterns. Unlike most existing work on feedback control of real-time database (RTDB) performance, we actually implement and evaluate feedback control and database workload adaptation techniques in a real database testbed modeling stock trades. For performance evaluation, we undertake experiments in the testbed, which consists of thousands of client threads concurrently requesting database services for stock quotes, trades, and portfolio updates in a bursty manner. In these experiments, our database system supports the desired response time bound and data freshness, while processing a significantly larger number of transactions in time compared to the tested baselines.","Feedback control,
Database systems,
Transaction databases,
Delay,
Testing,
Yarn,
Computer science,
Portfolios,
Traffic control,
Finance"
A Quantitative Analysis of Power Consumption for Location-Aware Applications on Smart Phones,"The industry is producing new wireless mobile devices, such as smart phones, at an ever increasing pace. In terms of processors and memory, these devices are as powerful as the PCs were one decade ago. Therefore, they are perfectly suitable to become the first real-life platforms for ubiquitous computing. For instance, they can be programmed to run location-aware applications that provide people with real-time information relevant to their current places. Deploying such applications in our daily life, however, requires a good understanding of their power requirements in order to ensure that mobile devices can indeed support them. This paper presents a quantitative analysis of power consumption for location-aware applications in our SmartCampus project, which builds a large scale test-bed for mobile social computing. Based on this analysis, we conclude that carefully designed applications can run for up to six hours, while updating the user location frequently enough to support real-time location-aware communication.","Energy consumption,
Smart phones,
Application software,
Mobile computing,
Ubiquitous computing,
Testing,
Mobile communication,
Computer industry,
Bluetooth,
Batteries"
Towards Equivalence Checking Between TLM and RTL Models,"The always increasing complexity of digital system is overcome in design flows based on transaction level modeling (TLM) by designing and verifying the system at different abstraction levels. The design implementation starts from a TLM high-level description and, following a top- down approach, it is refined towards a corresponding RTL model. However, the bottom-up approach is also adopted in the design flow when already existing RTL IPs are abstracted to be reused into the TLM system. In this context, proving the equivalence between a model and its refined or abstracted version is still an open problem. In fact, traditional equivalence definitions and formal equivalence checking methodologies presented in the literature cannot be applied due to the very different internal characteristics of the models, including structure organization and timing. Targeting this topic, the paper presents a formal definition of equivalence based on events, and then, it shows how such a definition can be used for proving the equivalence in the RTL vs. TLM context, without requiring timing or structural similarities between the modules to be compared. Finally, the paper presents a practical use of the proposed theory, by proving the correctness of a methodology that automatically abstracts RTL IPs towards TLM implementations.","Digital systems,
Context modeling,
Timing,
Circuits,
Encoding,
Latches,
Computer science,
Abstracts,
Process design,
Hardware"
Energy-Aware Synthesis of Networks-on-Chip Implemented with Voltage Islands,"Voltage islands provide a very good opportunity for minimizing the energy consumption of core-based Networks-on-Chip (NoC) design by utilizing a unique supply voltage for the cores on each island. This paper addresses various complex design issues for NoC implementation with voltage islands. A novel design framework based on genetic algorithm is proposed to optimize both the computation and communication energy with the creation of voltage islands concurrently for the NoC using multiple supply voltages. The algorithm automatically performs tile mapping, routing path allocation, link speed assignment, voltage island partitioning and voltage assignment simultaneously. Experiments using both real-life and artificial benchmarks were performed and results show that, by using the proposed scheme, significant energy reduction is obtained.","Network synthesis,
Voltage,
Network-on-a-chip,
Energy consumption,
Algorithm design and analysis,
Genetic algorithms,
Design optimization,
Concurrent computing,
Partitioning algorithms,
Tiles"
A Novel Virtual Anchor Node-Based Localization Algorithm for Wireless Sensor Networks,"The accuracy of localization is a significant criterion to evaluate the practical utility of localization algorithm in wireless sensor networks. In mostly localization algorithms, one of the main methods to improve localization accuracy is to increase the number of anchor nodes. But the number of anchor nodes is always limited because of the hardware restrict, such as cost, energy consumption and so on. In this paper, we propose a novel algorithm with small extra logical overhead, which uses the shortest-hop path scheme to upgrade virtual anchor nodes, while the real number of physical anchors is the same as before. This algorithm firstly chooses out some special nodes from all the unknown ones to figure out more accurate positions of them, and then makes these ones as new virtual anchor nodes assist other unknowns in localizing together with the real anchors. The simulation results illustrate our algorithm has improved the accuracy of localization greatly.",
Evolutionary Gradient Search Revisited,"Evolutionary gradient search (EGS) is an approach to optimization that combines features of gradient strategies with ideas from evolutionary computation. Recently, several modifications to the algorithm have been proposed with the goal of improving its robustness in the presence of noise and its suitability for implementation on parallel computers. In this paper, the value of the proposed modifications is studied analytically. A scaling law is derived that describes the performance of the algorithm on the noisy sphere model and allows comparing it with competing strategies. The comparisons yield insights into the interplay of mutation, multire combination, and selection. Then, the covariance matrix adaptation mechanism originally formulated for evolution strategies is adapted for use with EGS in order to make the algorithm competitive on objective functions with large condition numbers of their Hessians. The resulting strategy is evaluated experimentally on a number of convex quadratic test functions.","Covariance matrix,
Computer science,
Evolutionary computation,
Noise robustness,
Concurrent computing,
Genetic mutations,
Testing,
Eigenvalues and eigenfunctions,
Councils,
Microelectronics"
On Requirements Visualization,"This paper summarizes the typical objectives and process of visualization and highlights the primary areas in which visualization systems and artifacts have been used to support requirements engineering activities to date. The paper suggests that the field has yet to realize some of the benefits that can arise from a well designed and task-oriented information visualization, falling behind other areas of software engineering in which visualization has been used to better effect. By way of an exemplar, the paper proposes the need for a way to visualize the multi-dimensional nature of requirements to help bring about a shared and rapid comprehension on the health of a project's requirements, and so support various diagnostic activities and decision making tasks during software development. It examines how new ways to 'see' the requirements could be developed, based on metaphor and mapping, provides some samples, and outlines a research agenda to explore a vision related to requirements sensing.","Data visualization,
Mathematical model,
Decision making,
Software engineering,
Conferences,
Computer science,
Programming,
Humans,
Impedance,
Documentation"
Scalable Peer-to-Peer Web Retrieval with Highly Discriminative Keys,"The suitability of peer-to-peer (P2P) approaches for full-text Web retrieval has recently been questioned because of the claimed unacceptable bandwidth consumption induced by retrieval from very large document collections. In this contribution we formalize a novel indexing/retrieval model that achieves high performance, cost-efficient retrieval by indexing with highly discriminative keys (HDKs) stored in a distributed global index maintained in a structured P2P network. HDKs correspond to carefully selected terms and term sets appearing in a small number of collection documents. We provide a theoretical analysis of the scalability of our retrieval model and report experimental results obtained with our HDK-based P2P retrieval engine. These results show that, despite increased indexing costs, the total traffic generated with the HDK approach is significantly smaller than the one obtained with distributed single-term indexing strategies. Furthermore, our experiments show that the retrieval performance obtained with a random set of real queries is comparable to the one of centralized, single-term solution using the best state-of-the-art BM25 relevance computation scheme. Finally, our scalability analysis demonstrates that the HDK approach can scale to large networks of peers indexing Web-size document collections, thus opening the way towards viable, truly-decentralized Web retrieval.","Peer to peer computing,
Indexing,
Scalability,
Engines,
Information retrieval,
Bandwidth,
Costs,
Traffic control,
Vocabulary,
Prototypes"
Sampling-Based Motion Planning With Sensing Uncertainty,"Sampling-based algorithms have dramatically improved the state of the art in robotic motion planning. However, they make restrictive assumptions that limit their applicability to manipulators operating in uncontrolled and partially unknown environments. This work describes how one of these assumptions - that the world is perfectly known - can be removed. We propose a utility-guided roadmap planner that incorporates uncertainty directly into the planning process. This enables the planner to identify configuration space paths that minimize uncertainty and, when necessary, efficiently pursue further exploration through utility-guided sensing of the workspace. Experimental results indicate that our utility-guided approach results in a robust planner even in the presence of significant error in its perception of the workspace. Furthermore, we show how the planner is able to reduce the amount of required sensing to compute a successful plan","Uncertainty,
Motion planning,
Process planning,
Feedback,
Control systems,
Robotics and automation,
Computer science,
Robot motion,
Manipulators,
Robustness"
The MIT Indoor Multi-Vehicle Flight Testbed,"This paper and video present the components and flight tests of an indoor, multi-vehicle testbed that was developed to study long duration UAV missions in a controlled environment. This testbed is designed to use real hardware to examine research questions related to single- and multi-vehicle health management, such as vehicle failures, refueling, and maintenance. The testbed has both aerial and ground vehicles that operate autonomously in a large, indoor flight test area and can be used to execute many different mission scenarios. The success of this testbed is largely related to our choice of vehicles, sensors, and the system's command and control architecture. The video presents flight test results from single- and multi-vehicle experiments over the past year.","Unmanned aerial vehicles,
Remotely operated vehicles,
Mobile robots,
Space technology,
Automatic testing,
Hardware,
Land vehicles,
System testing,
Performance evaluation,
Robotics and automation"
3D Variational Brain Tumor Segmentation using a High Dimensional Feature Set,"Tumor segmentation from MRI data is an important but time consuming task performed manually by medical experts. Automating this process is challenging due to the high diversity in appearance of tumor tissue, among different patients and, in many cases, similarity between tumor and normal tissue. One other challenge is how to make use of prior information about the appearance of normal brain. In this paper we propose a variational brain tumor segmentation algorithm that extends current approaches from texture segmentation by using a high dimensional feature set calculated from MRI data and registered atlases. Using manually segmented data we learn a statistical model for tumor and normal tissue. We show that using a conditional model to discriminate between normal and abnormal regions significantly improves the segmentation results compared to traditional generative models. Validation is performed by testing the method on several cancer patient MRI scans.","Neoplasms,
Image segmentation,
Magnetic resonance imaging,
Biomedical imaging,
Level set,
Computer science,
Brain,
Data mining,
Layout,
Shape"
A Hybrid Simulated Annealing with Kempe Chain Neighborhood for the University Timetabling Problem,"This paper addresses the problem of finding a feasible solution for the university course timetabling problem (UCTP), i.e. a solution that satisfies all the so-called hard constraints. The problem is reformulated through relaxing one of its hard constraints and then creating a soft constraint to address the relaxed constraint. The relaxed problem is solved in two steps. First, a graph-based heuristic is used to construct a feasible solution of the relaxed problem, and then, a simulated annealing (SA)-based approach is utilized to minimize the violation of the soft constraint. In order to strengthen the diversification ability of the method in the SA phase, a heuristic based on Kempe chain neighborhood is embedded into the standard approach. This strategy is tested on a well-known data set, and the results are very competitive compared to the current state of the art of the UCTP.","Simulated annealing,
Testing,
Genetic algorithms,
Computational modeling,
Computer simulation,
Computer science,
Australia,
Hydrogen,
Information science,
Costs"
Context-aware Semantic Service Discovery,"In the last few years telecommunications and Internet have spread all over the world, in a pervasive way, connecting millions of devices, people, sensors and services without a planned strategy. In such scenario the discovery of services represent still an open challenging research field. To address that problem this paper proposes a context-aware semantic service discovery architecture designed to perform distributed service discovery in heterogeneous networks. This novel architecture is technology independent and compatible with most of the existent service discovery protocols; it inherits and extends the results of the last research groups in the field of context-aware service discovery based on the use of semantic languages. The present work presents the first results of the service discovery design activity which has been carried out within DAIDALOS II (Designing Advanced network Interfaces for the Delivery and Administration of Location independent, Optimised personal Services), a project granted in the European 6th Framework Research Programme, within the IST (Information Society and Technology) thematic area.","Context-aware services,
Ontologies,
Information retrieval,
Object oriented modeling,
Context modeling,
Context awareness,
Pervasive computing,
Web and internet services,
Protocols,
Collaboration"
Automatic State-Based Test Generation Using Genetic Algorithms,"Although a lot of research has been done in the field of state-based testing, the automatic generation of test cases from a functional specification in the form of a state machine is not straightforward. This paper investigates the use of genetic algorithms in test data generation for the chosen paths in the state machine, so that the input parameters provided to the methods trigger the specified transitions.","Automatic testing,
Genetic algorithms,
Software testing,
Automata,
Unified modeling language,
Fault detection,
System testing,
Books,
Scientific computing,
Computer science"
Asynchronous Iterative Waterfilling for Gaussian Frequency-Selective Interference Channels: A Unified Framework,"In this paper we give an overview of recent results on the rate maximization game in the Gaussian frequency- selective interference channel. We focus on the competitive maximization of information rates, subject to global power and spectral mask constraints. To achieve the so-called Nash equilibrium points of the game Yu, Ginis and Cioffi proposed the sequential Iterative Waterfilling Algorithm (IWFA), where, at each iteration, the users choose, one after the other, their power allocation to maximize their own information rate, treating the interference generated by the others as additive colored Gaussian noise. To overcome the potential slow convergence of the sequential update, specially when the number of users is large, the simultaneous IWFA was proposed by the authors, where, at each iteration, all the users update their power allocations simultaneously, rather than sequentially. Recently, the authors showed that both the sequential and the simultaneous IWFAs are just special cases of a more general unified framework, given by the totally asynchronous IWFA. In this more general algorithm, the users update their power spectral density in a completely distributed and asynchronous way. Furthermore, the asynchronous setup includes another form of lack of synchronism where the transmission by the different users contains time and frequency synchronization offsets. A unified set of convergence conditions were provided for the whole class of algorithms obtained from the asynchronous IWFA. Interestingly, there is a key result used in the proof of convergence of the algorithms: an alternative interpretation of the waterfilling operator as a projector.","Interference channels,
Convergence,
Information rates,
Frequency synchronization,
Interference constraints,
Nash equilibrium,
Iterative algorithms,
Power generation,
Additive noise,
Gaussian noise"
HEXA: Compact Data Structures for Faster Packet Processing,"Data structures representing directed graphs with edges labeled by symbols from a finite alphabet are used to implement packet processing algorithms used in a variety of network applications. In this paper we present a novel approach to represent such data structures, which significantly reduces the amount of memory required. This approach called history-based encoding, execution and addressing (HEXA) challenges the conventional assumption that graph data structures must store pointers of lceillog2nrceil bits to identify successor nodes. We show how the data structures can be organized so that implicit information can be used to locate successors, significantly reducing the amount of information that must be stored explicitly. We demonstrate that the binary tries used for IP route lookup can be implemented using just two bytes per stored prefix (roughly half the space required by Eatherton's tree bitmap data structure) and that string matching can be implemented using 20-30% of the space required by conventional data representations. Compact representations are useful, because they allow the performance-critical part of packet processing algorithms to be implemented using fast, on-chip memory, eliminating the need to retrieve information from much slower off-chip memory. This can yield both substantially higher performance and lower power utilization. While enabling a compact representation, HEXA does not add significant complexity to the graph traversal and update, thus maintaining a high performance.","Data structures,
History,
Computer science,
Encoding,
Inspection,
Bandwidth,
Application software,
Tree data structures,
Information retrieval,
Maintenance engineering"
Industry-University IP Relations: Integrating Perspectives and Policy Solutions,"Despite a long and productive U.S. history, industry-university (I-U) relations have become increasingly testy around intellectual property (IP). The Bayh-Dole Act is cited the driver for sharply increased university patenting, less fundamental research focus, and disinterest in traditional missions, although there is little data to corroborate these conclusions. A National Science Foundation (NSF)-sponsored workshop points to I-U relationship issues in the context of the path a new technology must follow from lab to market. We propose some critical variables affecting I-U IP relationships; describe areas of agreement and contention between the parties, drawing also on secondary data and the broader literature of I-U relations; and offer IP policy observations of interest to universities, researchers and technology transfer managers, their industry counterparts, and government. We end with propositions for further research","technology transfer,
educational institutions,
government,
legislation,
manufacturing industries,
patents"
Cast Shadow Removal Combining Local and Global Features,"In this paper, we present a method using pixel-level information, local region-level information and global-level information to remove shadow. At the pixel-level, we employ GMM to model the behavior of cast shadow for every pixel in the HSV color space, as it can deal with complex illumination conditions. However, unlike the GMM for background which can obtain sample every frame, this model for shadow needs more frames to get the same number of sample, because shadow may not appear at the same pixel for each frame. Therefore, it will take a long time to converge. To overcome this drawback, we use the local region-level information to get more samples and global-level information to improve a preclassifier and then, by using it, we get samples which are more likely to be shadow. Also, at the local region-level, we use Markov random fields to represent dependencies between the label of single pixel and labels of its neighborhood. Moreover, to make global level information more robust, tracking information is used. Experimental results show that the proposed method is efficient and robust.","Lighting,
Object detection,
Layout,
Robustness,
Cameras,
Light sources,
Laboratories,
Pattern recognition,
Automation,
Markov random fields"
"ReliableWeb Services: Methodology, Experiment and Modeling","We identify parameters impacting Web services dependability, describe the methods of dependability enhancement by redundancy in space and redundancy in time, and perform a series of experiments to evaluate the availability of Web services. To increase the availability of Web services, we employ several replication schemes and compare them with a single service. The Web services are coordinated by a replication manager. It provides a round robin algorithm for scheduling the workload of the Web services and keeps updating the availability of each Web service. The replication algorithm and the detailed system configuration are described. Experiments are performed to evaluate the resulting service availability. Modeling on the Web services with Petri-net is constructed and verified through experiments with different applications. With the parameters obtained from the experiments, the proposed model can be engaged to demonstrate the characteristics of the Web service.",
Effective OpenMP Implementation and Translation For Multiprocessor System-On-Chip without Using OS,"It is attractive to use the OpenMP as a parallel programming model on a multiprocessor system-on-chip (MPSoC) because it is easy to write a parallel program in the OpenMP and there is no standard method for parallel programming on an MPSoC. In this paper, we propose an effective OpenMP implementation and translation for major OpenMP directives on an MPSoC with physically shared memories, hardware semaphores, and no operating system.","Multiprocessing systems,
Operating systems,
Parallel programming,
Hardware,
Computer science,
Electronic mail,
Yarn,
Libraries,
Linux,
Kernel"
Toward A Discriminative Codebook: Codeword Selection across Multi-resolution,"In patch-based object recognition, there are two important issues on the codebook generation: (I) resolution: a coarse codebook lacks sufficient discriminative power, and an over-fine one is sensitive to noise; (2) codeword selection: non-discriminative codewords not only increase the codebook size, but also can hurt the recognition performance. To achieve a discriminative codebook for better recognition, this paper argues that these two issues are strongly related and should be solved as a whole. In this paper, a multi-resolution codebook is first designed via hierarchical clustering. With a reasonable size, it includes all of the codewords which cross a large number of resolution levels. More importantly, it forms a diverse candidate codeword set that is critical to codeword selection. A Boosting feature selection approach is modified to select the discriminative codewords from this multi-resolution code-book. By doing so, the obtained codebook is composed of the most discriminative codewords culled from different levels of resolution. Experimental study demonstrates the better recognition performance attained by this codebook.","Object recognition,
Boosting,
Multiresolution analysis,
Image recognition,
Power engineering and energy,
Power generation,
Noise generators,
Histograms,
Robustness,
Shape"
Prediction Services for Distributed Computing,"Users of distributed systems such as the TeraGrid and Open Science Grid can execute their applications on many different systems. We wish to help such users, or the grid schedulers they use, select where to run applications by providing predictions of when tasks will complete if sent to different systems. We make predictions of file transfer times, batch scheduler queue wait times, and application execution times using historical information and instance-based learning techniques. Our prediction errors for data from the TACC lonestar system are 37 percent of mean file transfer time, 115 percent for mean queue wait time, and 72 percent of mean execution time. Our approach achieves significantly lower prediction error on other workloads. We have wrapped these prediction techniques with Web services, making predictions available to users of distributed systems as well as tools such as resource brokers and metaschedulers.","Distributed computing,
Web services,
Application software,
Relational databases,
Grid computing,
Processor scheduling,
Spatial databases,
Computer applications,
Costs,
Parallel processing"
Colored de Bruijn Graphs and the Genome Halving Problem,"Breakpoint graph analysis is a key algorithmic technique in studies of genome rearrangements. However, breakpoint graphs are defined only for genomes without duplicated genes, thus limiting their applications in rearrangement analysis. We discuss a connection between the breakpoint graphs and de Bruijn graphs that leads to a generalization of the notion of breakpoint graph for genomes with duplicated genes. We further use the generalized breakpoint graphs to study the genome halving problem (first introduced and solved by Nadia El-Mabrouk and David Sankoff). The El-Mabrouk-Sankoff algorithm is rather complex, and, in this paper, we present an alternative approach that is based on generalized breakpoint graphs. The generalized breakpoint graphs make the El-Mabrouk-Sankoff result more transparent and promise to be useful in future studies of genome rearrangements",
Desynchronizing a Chaotic Pattern Recognition Neural Network to Model Inaccurate Perception,"The usual goal of modeling natural and artificial perception involves determining how a system can extract the object that it perceives from an image that is noisy. The ""inverse"" of this problem is one of modeling how even a clear image can be perceived to be blurred in certain contexts. To our knowledge, there is no solution to this in the literature other than for an oversimplified model in which the true image is garbled with noise by the perceiver himself. In this paper, we propose a chaotic model of pattern recognition (PR) for the theory of ""blurring."" This paper, which is an extension to a companion paper demonstrates how one can model blurring from the view point of a chaotic PR system. Unlike the companion paper in which a chaotic PR system extracts the pattern from the input, in this case, we show that even without the inclusion of additional noise, perception of an object can be ""blurred"" if the dynamics of the chaotic system are modified. We thus propose a formal model and present an analysis using the Lyapunov exponents and the Routh-Hurwitz criterion. We also demonstrate experimentally the validity of our model by using a numeral data set. A byproduct of this model is the theoretical possibility of desynchronization of the periodic behavior of the brain (as a chaotic system), rendering us the possibility of predicting, controlling, and annulling epileptic behavior","Chaos,
Pattern recognition,
Neural networks,
Chaotic communication,
Biological neural networks,
Brain modeling,
Computer science,
Artificial neural networks,
Image recognition,
Context modeling"
Multirecipient Encryption Schemes: How to Save on Bandwidth and Computation Without Sacrificing Security,"This paper proposes several new schemes which allow a sender to send encrypted messages to multiple recipients more efficiently (in terms of bandwidth and computation) than by using a standard encryption scheme. Most of the proposed schemes explore a new natural technique called randomness reuse. In order to analyze security of our constructions, we introduce a new notion of multirecipient encryption schemes (MRESs) and provide definitions of security for them. We finally show a way to avoid ad hoc analyses by providing a general test that can be applied to a standard encryption scheme to determine whether the associated randomness reusing MRES is secure. The results and applications cover both asymmetric and symmetric encryption.","Cryptography,
Bandwidth,
Security,
Broadcasting,
Testing,
Engineering profession,
Computer science,
Laboratories,
Electronic mail"
Reputation Systems for Fighting Pollution in Peer-to-Peer File Sharing Systems,"Content pollution is a de facto problem in several currently popular Peer-to-Peer file sharing systems. Previously proposed strategies to fight content pollution include a peer reputation system named Scrubber as well as the alternative Credence object reputation system. This paper builds on previous work into three directions. First, it evaluates the cost-effectiveness of Scrubber and Credence in reducing pollution dissemination when polluters make use of collusion and Sybil attacks. Second, it modifies the Scrubber system to increase its effectiveness, in particular under collusion. Finally, it proposes a hybrid peer and object reputation system that combines the benefits of both strategies. We performed an extensive evaluation of all three systems, for various configurations and pollution mechanisms, as well as collusion and Sybil attacks. Our results show that the new hybrid system is much more effective and robust to malicious actions than any individual strategy, even in very uncooperative and unreliable peer communities.","Peer to peer computing,
Pollution,
Robustness,
Performance evaluation,
Computer science,
Application software,
Internet,
Motion pictures,
Feedback"
Using Software Distributions to Understand the Relationship among Free and Open Source Software Projects,"Success in the open source software world has been measured in terms of metrics such as number of downloads, number of commits, number of lines of code, number of participants, etc. These metrics tend to discriminate towards applications that are small and tend to evolve slowly. A problem is, however, how to identify applications in these latter categories that are important. Software distributions specify the dependencies needed to build and to run a given software application. We use this information to create a dependency graph of the applications contained in such a distribution. We explore the characteristics of this graph, and use it to define some metrics to quantify the dependencies (and dependents) of a given software application. We demonstrate that some applications that are invisible to the final user (such as libraries) are widely used by end-user applications. This graph can be used as a proxy to measure success of small, slowly evolving free and open source software.","Open source software,
Packaging,
Application software,
Software libraries,
Software packages,
Software measurement,
Environmental factors,
Software engineering,
Computer science,
Particle measurements"
An Efficient Mechanism for Establishing Connectivity in Wireless Sensor and Actor Networks,"Wireless sensor and actor networks (WSANs) employ powerful and mobile actor nodes that can perform application specific actions based on the received data from the sensors. As most of these actions are performed collaboratively among the actors, inter-actor connectivity is one of the desirable features of WSANs. In this paper, we propose a novel distributed algorithm for establishing a connected inter-actor network topology. Considering an initially partitioned actor network with intra-connected sub-networks, our algorithm pursues a coordinated actor movement in order to connect the sub-networks. The goal of this movement is to both minimize the total and maximum travel distances of the individual actors. Our algorithm considers the minimum connected dominating set of each sub-network when picking the appropriate actor to move so that the connectivity of each sub-network is not violated. We analytically study the performance of our algorithm. Extensive simulation experiments validate the analytical results and confirm the effectiveness of our approach.","Wireless sensor networks,
Robotics and automation,
Computer science,
Collaboration,
Partitioning algorithms,
Optimized production technology,
Robot sensing systems,
Robot kinematics,
Fires,
Mobile computing"
"OOPS for Motion Planning: An Online, Open-source, Programming System","The success of sampling-based motion planners has resulted in a plethora of methods for improving planning components, such as sampling and connection strategies, local planners and collision checking primitives. Although this rapid progress indicates the importance of the motion planning problem and the maturity of the field, it also makes the evaluation of new methods time consuming. We propose that a systems approach is needed for the development and the experimental validation of new motion planners and/or components in existing motion planners. In this paper, we present the online, open-source, programming system for motion planning (OOPSMP), a programming infrastructure that provides implementations of various existing algorithms in a modular, object-oriented fashion that is easily extendible. The system is open-source, since a community-based effort better facilitates the development of a common infrastructure and is less prone to errors. We hope that researchers will contribute their optimized implementations of their methods and thus improve the quality of the code available for use. A dynamic Web interface and a dynamic linking architecture at the programming level allows users to easily add new planning components, algorithms, benchmarks, and experiment with different parameters. The system allows the direct comparison of new contributions with existing approaches on the same hardware and programming infrastructure","Open source software,
Dynamic programming,
Motion planning,
Strategic planning,
Sampling methods,
Object oriented programming,
Robotics and automation,
Automatic programming,
Robot programming,
Optimization methods"
Emulating Optimal Replacement with a Shepherd Cache,"The inherent temporal locality in memory accesses is filtered out by the L1 cache. As a consequence, an L2 cache with LRU replacement incurs significantly higher misses than the optimal replacement policy (OPT). We propose to narrow this gap through a novel replacement strategy that mimics the replacement decisions of OPT. The L2 cache is logically divided into two components, a Shepherd Cache (SC) with a simple FIFO replacement and a Main Cache (MC) with an emulation of optimal replacement. The SC plays the dual role of caching lines and guiding the replacement decisions in MC. Our proposed organization can cover 40% of the gap between OPT and LRU for a 2MB cache resulting in 7% overall speedup. Comparison with the dynamic insertion policy, a victim buffer, a V-Way cache and an LRU based fully associative cache demonstrates that our scheme performs better than all these strategies.","Optimized production technology,
Proposals,
Microarchitecture,
Computer science,
Automation,
Supercomputers,
Computer science education,
Emulation,
History,
Frequency"
Eliciting Requirements for a Robotic Toy for Children with Autism - Results from User Panels,"The work presented in this paper was carried out within the IROMEC project that develops a robotic toy for children. Play has an important role in child development with many potential contributions to therapy, education and enjoyment. The project investigates how robotic toys can become social mediators, encouraging children with disabilities to discover a range of play styles, from solitary to social and cooperative play (with peers, carers/teachers, parents etc). This paper presents design issues for such robotic toys related specifically to children with autism as the end user target group. In order to understand the play needs of this user group, and to investigate how robotic toys could be used as a play tool to assist in the children's development, a panel of experts (therapists, teachers, parents) was formed and interviewed. Results of the expert panel interview s highlight key points characterizing the play of children with autism, and key points for consideration in the design of future robotic toys.","Autism,
Pediatrics,
Educational robots,
Human robot interaction,
Cognitive robotics,
Educational institutions,
Adaptive systems,
Computer science,
Medical treatment,
Computer science education"
Sparse Decomposition and Modeling of Anatomical Shape Variation,"Recent advances in statistics have spawned powerful methods for regression and data decomposition that promote sparsity, a property that facilitates interpretation of the results. Sparse models use a small subset of the available variables and may perform as well or better than their full counterparts if constructed carefully. In most medical applications, models are required to have both good statistical performance and a relevant clinical interpretation to be of value. Morphometry of the corpus callosum is one illustrative example. This paper presents a method for relating spatial features to clinical outcome data. A set of parsimonious variables is extracted using sparse principal component analysis, producing simple yet characteristic features. The relation of these variables with clinical data is then established using a regression model. The result may be visualized as patterns of anatomical variation related to clinical outcome. In the present application, landmark-based shape data of the corpus callosum is analyzed in relation to age, gender, and clinical tests of walking speed and verbal fluency. To put the data-driven sparse principal component method into perspective, we consider two alternative techniques, one where features are derived using a model-based wavelet approach, and one where the original variables are regressed directly on the outcome.","Shape,
Hospitals,
Nervous system,
Data mining,
Principal component analysis,
Anatomy,
Informatics,
Mathematical model,
Magnetic resonance,
Biomedical imaging"
A Modulo Scheduling Algorithm for a Coarse-Grain Reconfigurable Array Template,"Coarse grain reconfigurable arrays (CGRAs) have been drawing attention due to its programmability and performance. Compilation onto CGRAs is still an open problem. Several groups have proposed algorithms that software pipeline loops onto CGRAs. In this paper, we present an efficient modulo scheduling algorithm for a CGRA template. The novelties of the approach are the separation of resource reservation and scheduling, use of a compact three-dimensional architecture graph and a resource usage aware relocation algorithm. Preliminary experiments indicate that the proposed algorithm can find schedules with small initiation intervals within a reasonable amount of time.","Scheduling algorithm,
Computer architecture,
Job shop scheduling,
Processor scheduling,
Microprocessors,
USA Councils,
Software algorithms,
Registers,
Application software,
Computer science"
RSS-based Carrier Sensing and Interference Estimation in 802.11 Wireless Networks,"We analyze the carrier sensing and interference relations between the two wireless links and measure the impact of these relations on link capacity in two indoor 802.11a mesh network testbeds. We show that asymmetric carrier sensing and/or interference relations happen frequently in wireless networks; these asymmetric relations affect not only the level of performance degradation, but also the fairness of channel access. We then propose a new methodology that predicts the relation of carrier sensing and interference based on radio signal strength measurements. The measurement complexity increases only linearly with the number of wireless nodes. To our knowledge, the proposed methodology is the first trial that considers physical layer capture, and detects the source of interference that is out of the communication range. We validate the prediction methodology on an 11-node wireless mesh network testbed.","Interference,
Wireless networks,
Wireless sensor networks,
Testing,
Throughput,
Broadcasting,
Peer to peer computing,
Wireless mesh networks,
Network topology,
Laboratories"
A Domain-Specific On-Chip Network Design for Large Scale Cache Systems,"As circuit integration technology advances, the design of efficient interconnects has become critical. On-chip networks have been adopted to overcome scalability and the poor resource sharing problems of shared buses or dedicated wires. However, using a general on-chip network for a specific domain may cause underutilization of the network resources and huge network delays because the interconnects are not optimized for the domain. Addressing these two issues is challenging because in-depth knowledges of interconnects and the specific domain are required. Non-uniform cache architectures (NUCAs) use wormhole-routed 2D mesh networks to improve the performance of on-chip L2 caches. We observe that network resources in NUCAs are underutilized and occupy considerable chip area (52% of cache area). Also the network delay is significantly large (63% of cache access time). Motivated by our observations, we investigate how to optimize cache operations and and design the network in large scale cache systems. We propose a single-cycle router architecture that can efficiently support multicasting in on-chip caches. Next, we present fast-LRU replacement, where cache replacement overlaps with data request delivery. Finally we propose a deadlock-free XYX routing algorithm and a new halo network topology to minimize the number of links in the network. Simulation results show that our networked cache system improves the average IPC by 38% over the mesh network design with multicast promotion replacement while using only 23% of the interconnection area. Specifically, multicast fast-LRU replacement improves the average IPC by 20% compared with multicast promotion replacement. A halo topology design additionally improves the average IPC by 18% over a mesh topology","System-on-a-chip,
Network-on-a-chip,
Large-scale systems,
Integrated circuit interconnections,
Network topology,
Mesh networks,
Scalability,
Resource management,
Wires,
Delay effects"
On the Efficiency and Complexity of Distributed Spectrum Allocation,"We study the efficiency and complexity of a distributed spectrum allocation algorithm using explicit user coordination. Users self-organize into coordination groups and adjust spectrum assignments in each local group to approximate an optimal assignment. Compared to the conventional topology-based optimizations, local coordination can significantly reduce the computation and communication overhead required to adapt to topology variations. We focus on analyzing the end-user performance in terms of spectrum efficiency and communication overhead. We derive a theoretical lower bound on the amount of spectrum each user can get from coordination, and a theoretical upper bound on the algorithm convergence time. We also perform experiments to verify our analytical results.",
"Modeling of Si Etching Under Effects of Plasma Molding in Two-Frequency Capacitively Coupled Plasma in
SF
6
/
O
2
for MEMS Fabrication","We numerically investigated Si deep etching with several hundreds of micrometers such as that used in microelectromechanical system fabrication. This was carried out in SF6(83%)O2 at 300 mtorr in two-frequency capacitively coupled plasma using an extended vertically integrated computer-aided design for device processing (VicAddress). We estimated the local characteristics of plasma molding, including potential distribution and flux ion velocity distribution that are adjacent to an artificial microscale hole pattern. The sheath thickness is comparable to or even smaller than the size of the hole, and the sheath tends to wrap around the hole on a Si wafer. The distorted sheath field directly affects the incident flux and velocity distributions of ions. The angular distribution of SF5 + ions at the edge of the hole is strongly distorted from the normal incidence. That is, the ion flux becomes radially nonuniform in the vicinity of the hole pattern. The feature-profile evolution by radicals and ions under the presence of plasma molding indicates that the etching is enhanced particularly at the bottom corner due to the removal of the passivation (SiOxFy) layer by energetic ion, resulting in the suppression of anisotropy of the etch profile.","Plasma applications,
Etching,
Plasma properties,
Plasma devices,
Plasma materials processing,
Plasma sheaths,
Microelectromechanical systems,
Fabrication,
Design automation,
Passivation"
"A stochastic pattern generation and optimization framework for variation-tolerant, power-safe scan test","Process variation is an increasingly dominant phenomenon affecting both power and performance in sub-100 nm technologies. Cost considerations often do not permit over-designing the power supply infrastructure for test mode, considering the worst-case scenario. Test application must not over-exercise the power supply grids, lest the tests will damage the device or lead to false test failures. The problem of debugging a delay test failure can therefore be highly complex. We argue that false delay test failures can be avoided by generating ""safe"" patterns that are tolerant to on-chip variations. A statistical framework for power-safe pattern generation is proposed, which uses process variation information, power grid topology and regional constraints on switching activity. Experimental results are provided on benchmark circuits to demonstrate the effectiveness of the framework.","Stochastic processes,
Power generation,
Test pattern generators,
Power supplies,
Delay,
Costs,
Debugging,
Circuit testing,
Mesh generation,
Power grids"
Differential Evolution Based Particle Swarm Optimization,"A new, almost parameter-free optimization algorithm is developed in this paper as a hybrid of the barebones particle swarm optimizer (PSO) and differential evolution (DE). The DE is used to mutate, for each particle, the attractor associated with that particle, defined as a weighted average of its personal and neighborhood best positions. Results of this algorithm are compared to that of the barebones PSO, Von Neumann PSO, a DE PSO, and DE/rand/1/bin. These results show that the new algorithm provides excellent results with the added advantage that no parameter tuning is needed","Particle swarm optimization,
Computer science,
Stochastic processes,
Acceleration,
Topology,
Africa,
Search methods,
Birds,
Optimization methods,
Convergence"
Secure crash reporting in vehicular Ad hoc networks,"We present AutoCore, an automated crash reporting application that uses VANETs (Vehicular Ad hoc NETworks) to provide authenticated digital video and telemetry data. This data is recorded by vehicles either involved in or at the scene of a crash and can be used by investigators to reconstruct the events that lead up to the crash. To secure this application, we present a security infrastructure that extends the state of the art in VANET security. In particular, the contributions of this infrastructure include (a) the concept of Road-worthiness Certificates, (b) use of these certificates in a practical scheme for the distribution of cryptographic vehicle credentials issued by regional transportation authorities, (c) a decentralized scheme for conditionally anonymous, inter-vehicle communication, (d) efficient support for the roaming of vehicles between different transportation authority jurisdictions and (e) an evaluation of our security infrastructure using AutoCore.","Vehicle crash testing,
Ad hoc networks,
Road vehicles,
Data security,
Cryptography,
Protection,
Safety,
Telemetry,
Road transportation,
Communication system security"
Zonal Co-location Pattern Discovery with Dynamic Parameters,"Zonal co-location patterns represent subsets of feature- types that are frequently located in a subset of space (i.e., zone). Discovering zonal spatial co-location patterns is an important problem with many applications in areas such as ecology, public health, and homeland defense. However, discovering these patterns with dynamic parameters (i.e., repeated specification of zone and interest measure values according to user preferences) is computationally complex due to the repetitive mining process. Also, the set of candidate patterns is exponential in the number of feature types, and spatial datasets are huge. Previous studies have focused on discovering global spatial co-location patterns with a fixed interest measure threshold. In this paper, we propose an indexing structure for co-location patterns and propose algorithms (Zoloc-Miner) to discover zonal co- location patterns efficiently for dynamic parameters. Extensive experimental evaluation shows our proposed approaches are scalable, efficient, and outperform naive alternatives.","Data mining,
Environmental factors,
Public healthcare,
Indexing,
Symbiosis,
Birds,
Association rules,
Computer science,
USA Councils,
Application software"
Efficient Thermal via Planning Approach and Its Application in 3-D Floorplanning,"In this paper, we investigate thermal via (T-via) planning during three-dimensional (3-D) floorplanning. First, we consider the temperature constrained T-via planning (TVP) problem on a given 3-D floorplan. Second, we integrate dynamic TVP into 3-D floorplanning process. Our main contribution and results can be summarized as follows. We solve the temperature constrained TVP problem by solving a sequence of simplified interlayer and intralayer TVP subproblems. Each subproblem is formulated as convex programming problem and we derive nearly optimal solution for detailed T-via distribution. Based on the TVP solution, we implement the integrated TVP and 3-D floorplanning algorithm in a two-stage approach. Before floorplanning, blocks are assigned into different layers by solving a sequence of knapsack problems. During floorplanning, T-vias are allocated with white space redistribution to optimize T-via insertion. Experimental results show that our TVP approach can reduce T-vias by 12% compared with a recent published work (J. Cong and Y. Zhang, ""Thermal via planning for 3-D ICs,"" in Proc. Int. Conf. Comput.-Aided Des., Nov. 2005, pp.745-752). Compared with the postfloorplanning optimization approach, integrating TVP into floorplanning process can reduce T-vias by 16% with 21% runtime overhead","Temperature,
Thermal resistance,
Thermal conductivity,
Very large scale integration,
Design optimization,
Integrated circuit technology,
Integrated circuit interconnections,
Delay,
Computer science,
Design automation"
Multicollision Attacks on Some Generalized Sequential Hash Functions,"A multicollision for a function is a set of inputs whose outputs are all identical. A. Joux showed multicollision attacks on the classical iterated hash function. He also showed how these multicollision attacks can be used to get a collision attack on a concatenated hash function. In this paper, we study multicollision attacks in a more general class of hash functions which we term ""generalized sequential hash functions."" We show that multicollision attacks exist for this class of hash functions provided that every message block is used at most twice in the computation of the message digest","Concatenated codes,
Time measurement,
Terrorism,
Computer science,
Cryptography"
Optimal Replica Placement under TTL-Based Consistency,"Geographically replicating popular objects in the Internet speeds up content distribution at the cost of keeping the replicas consistent and up-to-date. The overall effectiveness of replication can be measured by the total communication cost consisting of client accesses and consistency management, both of which depend on the locations of the replicas. This paper investigates the problem of placing replicas under the widely used TTL-based consistency scheme. A polynomial-time algorithm is proposed to compute the optimal placement of a given number of replicas in a network. The new replica placement scheme is compared, using real Internet topologies and Web traces, against two existing approaches which do not consider consistency management or assume invalidation-based consistency scheme. The factors affecting their performance are identified and discussed","Internet,
Polynomials,
Network topology,
Cost function,
Computer networks,
Large-scale systems,
Delay,
Telecommunication traffic,
Performance gain,
Content management"
Fuzzy Model Based Recognition of Handwritten Hindi Numerals using Bacterial Foraging,This paper presents the recognition of handwritten Hindi numerals. The recognition is based on the modified exponential membership function fitted to the fuzzy sets derived from features consisting of normalized distances obtained using the Box approach. The exponential membership function is modified by two structural parameters that are estimated by optimizing the entropy subject to the attainment of membership function to unity. The optimization strategy used is the foraging model of E.coli bacteria. Two window sizes are used: one for ( ) and another for the rest of the numerals. Experimentation is carried out on a limited database of nearly 3500 samples. The overall recognition is found to be 96%.,
Online Web Cluster Capacity Estimation and Its Application to Energy Conservation,"Designers of data centers and Web servers aim to make on-demand allocation of resources to clients in order to lower the deployment cost of hosted services. Moreover, they must also minimize operating costs, such as energy consumption, by matching service-capacity demand with resource supply. However, since the term ""capacity"" is typically defined vaguely or inadequately, it is difficult to assess resource needs and, hence, servers, which are several times larger than needed at runtime, are usually deployed. The time-varying nature of the workload model further complicates the problem and necessitates an online capacity-estimation solution. To address this overprovisioning problem, we first define the capacity of a server cluster as the sustainable throughput subject to a request retransmission ratio constraint and then analyze different approaches to capacity estimation in a running system. Various capacity-estimation mechanisms, such as offline benchmarking and CPU-utilization evaluation, are discussed and compared with our queue-monitoring method. We employ several different data-collection methods (application instrumentation, user-space tools, simple network management protocol (SNMP), and kernel modules) to compare their effects on estimation accuracy. Of these, queue monitoring is found to provide a good and stable estimate of server capacity. To validate this finding, we propose a simple cluster- resizing mechanism and evaluate the energy-conservation performance. A good combination of data collection and online capacity estimation is found to make significantly more energy savings than traditional approaches (that is, static estimation and scheduled capacity). Our experimental results show that more than 40 percent of energy can be saved for regular daily usage patterns without any prior knowledge of the workload and that long start-up and shutdown delays affect energy savings considerably.",
Introducing Control Flow into Vectorized Code,"Single instruction multiple data (SIMD) functional units are ubiquitous in modern microprocessors. Effective use of these SIMD functional units is essential in achieving the highest possible performance. Automatic generation of SIMD instructions in the presence of control flow is challenging, however, not only because SIMD code is hard to generate in the presence of arbitrarily complex control flow, but also because the SIMD code executing the instructions in all control paths may slow compared to the scalar original, which may bypass a large portion of the code. One promising technique introduced recently involves inserting branches-on-superword-condition-codes (BOSCCs) to bypass vector instructions. In this paper, we describe two techniques that improve on the previous approach. First, BOSCCs are generated in a nested fashion so that even BOSCCs themselves can be bypassed by other BOSCCs. Second, we generate all vec_any_* instructions to bypass even some predicate-defining instructions. We implemented these techniques in a vectorizing compiler. On 14 kernels, the compiler achieves distinct speedups, including 1.99X over the previous technique that generates single- level BOSCCs and vec_any_ne only.",
On Analyzing Diffusion Tensor Images by Identifying Manifold Structure Using Isomaps,"This paper addresses the problem of statistical analysis of diffusion tensor magnetic resonance images (DT-MRI). DT-MRI cannot be analyzed by commonly used linear methods, due to the inherent nonlinearity of tensors, which are restricted to lie on a nonlinear submanifold of the space in which they are defined, namely R6. We estimate this submanifold using the isomap manifold learning technique and perform tensor calculations using geodesic distances along this manifold. Multivariate statistics used in group analyses also use geodesic distances between tensors, thereby warranting that proper estimates of means and covariances are obtained via calculations restricted to the proper subspace of R6. Experimental results on data with known ground truth show that the proposed statistical analysis method properly captures statistical relationships among tensor image data, and it identifies group differences. Comparisons with standard statistical analyses that rely on Euclidean, rather than geodesic distances, are also discussed",
Student Project Collaboration Using Wikis,"A wiki is a web tool that allows users to easily create and edit web pages collaboratively. The ease-of-editing feature and accessibility from anywhere by anyone make wikis ideal for project collaboration. Although the wiki was introduced more than ten years ago, its use is relatively new in academia. This research explores the potential uses of wikis in Software Engineering, especially for software project team collaboration and communication. The author introduced wikis in a project-based software engineering course, and students soon discovered a number of innovative ways in which wikis can augment collaborative software development activities. A student survey indicates that vast majority of students found the wiki to be a good tool for project collaboration, and most of them plan to use wikis for future projects even if not required to do so.","Collaboration,
Collaborative software,
Software engineering,
Collaborative tools,
Web pages,
Programming,
Collaborative work,
Software tools,
Computer science,
Education"
Cover Set Problem in Directional Sensor Networks,"A directional sensor network consists of a number of directional sensors, which can switch to several directions to extend their sensing ability to cover the interested targets in a given area. Because a directional sensor has a smaller angle of sensing range or even does not cover any target when it is deployed, how to cover the interested targets becomes a major problem in directional sensor networks. In this paper, we address the directional cover set problem (DCS) of finding a cover set in a directional sensor network, in which the directions cover all the targets. We propose both centralized and distributed algorithms for the DCS. We also introduce two applications that utilize these algorithms to extend the network work time while maximizing the coverage of the targets. Simulation results are presented to demonstrate the performance of these algorithms and the applications.","Distributed control,
Monitoring,
Infrared sensors,
Switches,
Distributed algorithms,
Computer networks,
Computer science,
Costs,
Scattering,
Scheduling algorithm"
Direct Power Control of Grid Connected Voltage Source Converters,"This paper proposes a new direct power control (DPC) strategy for grid connected voltage source converters (VSC). Similar to an electrical machine, source and converter flux are defined as the integration of the source and converter voltage respectively. Based on the defined flux, the proposed DPC scheme directly calculates the required converter voltage to eliminate the active and reactive power errors within each fixed time period. Thus, no extra power or current control loops are required, thereby simplifying the system design and improving transient performance. Constant converter switching frequency is achieved using space vector modulation which eases the design of the power converter and the AC harmonic filter. Simulation results for a 2 MW VSC system are provided to demonstrate the effectiveness and robustness of the proposed control strategy during variations of active and reactive power and AC voltage dip.","Power control,
Power conversion,
Switching converters,
Reactive power,
Current control,
Switching frequency,
Harmonic filters,
Robust control,
Voltage control,
Reactive power control"
Design and Implementation of a Low-power Baseband-system for RFID Tag,"This article describes a low power design approach for a UHF passive RFID tag baseband system. It proposes a new RFID tag baseband architecture which is compatible with the EPC C1G2 UHF RFID protocol. Advanced low power design approaches are adopted, including separating driving clocks, applying an improved Tausworthe sequence generator, moving window PIE decoding algorithm, idle scheme and parallel operating scheme. The tag supports three commands, which are read, write and query. It consists of a 136 bits one-time programmable memory, rectifier, charge pump, clock divider, analog frontend and baseband system. SimuLink co-verification approach is applied for system functional test. The chip was designed and fabricated successfully by using 0.18 mum 6 layers CMOS technology","RFID tags,
Baseband,
Clocks,
CMOS technology,
Passive RFID tags,
Radiofrequency identification,
Protocols,
Algorithm design and analysis,
Power generation,
Decoding"
Search-Oriented Deployment Strategies for Wireless Sensor Networks,"Wireless sensor networks have been widely considered as an effective tool in various application domains. One of the major implementation issues is on the network deployment problem. In this paper, we target issues in the deployment of wireless sensor nodes in three-dimensional indoor environments and with irregular radiation patterns for both message communicating and sensing. Our objective is to develop a more effective way to deploy sensor networks and to minimize the number of deployed nodes. We propose several search-oriented strategies to improve the performance of search algorithms, such as simulated annealing. The capability of the proposed strategies is demonstrated by real-case studies in the deployment of sensor networks in several office flats","Wireless sensor networks,
Antenna radiation patterns,
Simulated annealing,
Sensor phenomena and characterization,
Computer science,
Application software,
Indoor environments,
Heuristic algorithms,
Genetic algorithms,
Process planning"
Minimum-Energy Redundancy Resolution of Robot Manipulators Unified by Quadratic Programming and its Online Solution,"This paper presents the latest result regarding the unification of minimum-energy redundancy resolution of robot manipulators via a quadratic program. The presented quadratic programming (QP) formulation is general in the sense that it incorporates equality, inequality and bound constraints, simultaneously. This QP formulation covers the online avoidance of joint physical limits and environmental obstacles, as well as the optimization of various performance indices. Every term is endowed with clear physical meaning and utility. Motivated by the real-time solution to such robotic problems, four QP online solvers are briefly reviewed. That is, standard QP optimization routines, compact QP method, dual neural network as a QP solver, and state-of-the-art LVI - based primal-dual neural network as a QP solver. The QP- based unification of robots' redundancy resolution is substantiated by a large number of computer simulation results based on PUMA560, PA10, and planar robot arms.","Manipulators,
Quadratic programming,
Robotics and automation,
Acceleration,
Educational robots,
Robot sensing systems,
Sun,
Neural networks,
Equations,
Mechatronics"
Toward a Service-Oriented Development Through a Case Study,"The rapidly emerging technology of Web services paves a new cost-effective way of engineering software to quickly develop and deploy Web applications by dynamically integrating other independently developed Web-service components to conduct new business transactions. This paper reports our efforts on designing and developing a Web service of pass-through authentication (PTA) for 12 online electronic-payment Web applications. In accordance with how a PTA service is developed and integrated with a corresponding back-end e-payment system, our strategies can be categorized in three stages: end-to-end integration stage, Web-services-enabled stage, and Web-services-oriented stage. Derived from real-world industrial experience, this three-stage pathway can be applied to a broad range of Web-application development projects to guide smooth transformation from a specific application-oriented design and development model toward a reusable Web-services-oriented model. Furthermore, this paper contributes to an engineering process that leads to practical Web-services-oriented software development. New research issues revealed by this project are also reported.","Web services,
Application software,
Authentication,
Computer science,
Web and internet services,
Service oriented architecture,
Programming,
Logic,
Productivity"
Improving Text Classification by Using Encyclopedia Knowledge,"The exponential growth of text documents available on the Internet has created an urgent need for accurate, fast, and general purpose text classification algorithms. However, the ""bag of words"" representation used for these classification methods is often unsatisfactory as it ignores relationships between important terms that do not co-occur literally. In order to deal with this problem, we integrate background knowledge - in our application: Wikipedia - into the process of classifying text documents. The experimental evaluation on Reuters newsfeeds and several other corpus shows that our classification results with encyclopedia knowledge are much better than the baseline ""bag of words "" methods.","Text categorization,
Encyclopedias,
Wikipedia,
Asia,
Ontologies,
Data mining,
Computer science,
Internet,
Classification algorithms,
Frequency"
Low Power VLSI Design for a RFID Passive Tag baseband System Enhanced with an AES Cryptography Engine,"This paper describes a low power implementation of a secure EPC UHF Passive RFID Tag baseband system. To ensure the secure information transaction of the tag, traditionally the focus is on directly applying a low-complexity encryption engine. However, this approach could lead to the problem of known-plaintext attack (KPA). The attacker could make use of the known header to reveal the secret key. Our contributions are proposing a novel dataflow solution enforced by an AES cryptography engine embedded inside the passive RFID tag. Also, various low power design techniques are proposed to reduce the power consumption of the baseband of the passive tag. In particular, we propose a moving window PIE decoding algorithm and an improved Tausworthe sequence generator to reduce the power consumption. Other low power design techniques such as clock gating, optimal clock driving and parallel operations are extensively used in the design of the tag. The complete RFID tag which consists of an analog frontend, 136 bits one-time programmable (OTP) memory, charge pump, rectifier, clock divider, and the proposed baseband system, was designed using TSMC 0.18 mum process and verified. The area of the proposed baseband system is 0.446mm2 and from the power simulation, the overall power consumption of the baseband system with the AES encryption is about 4.695 uW.","Very large scale integration,
Passive RFID tags,
Baseband,
Cryptography,
Engines,
Energy consumption,
Clocks,
Decoding,
Power generation,
RFID tags"
CRF-driven Implicit Deformable Model,"We present a topology independent solution for segmenting objects with texture patterns of any scale, using an implicit deformable model driven by conditional random fields (CRFs). Our model integrates region and edge information as image driven terms, whereas the probabilistic shape and internal (smoothness) terms use representations similar to the level-set based methods. The evolution of the model is solved as a MAP estimation problem, where the target conditional probability is decomposed into the internal term and the image-driven term. For the later, we use discriminative CRFs in two scales, pixel- and patch-based, to obtain smooth probability fields based on the corresponding image features. The advantages and novelties of our approach are (i) the integration of CRFs with implicit deformable models in a tightly coupled scheme, (ii) the use of CRFs which avoids ambiguities in the probability fields, (iii) the handling of local feature variations by updating the model interior statistics and processing at different spatial scales, and (v) the independence from the topology. We demonstrate the performance of our method in a wide variety of images, from the zebra and cheetah examples to the left and right ventricles in cardiac images.","Deformable models,
Image segmentation,
Shape,
Active contours,
Topology,
Smoothing methods,
Pixel,
Statistics,
Biomedical imaging,
Computer vision"
uCast: Unified Connectionless Multicast for Energy Efficient Content Distribution in Sensor Networks,"In this paper, we present uCast, a novel multicast protocol for energy efficient content distribution in sensor networks. We design uCast to support a large number of multicast sessions, especially when the number of destinations in a session is small. In uCast, we do not keep any state information relevant to ongoing multicast deliveries at intermediate nodes. Rather, we directly encode the multicast information in the packet headers and parse these headers at intermediate nodes using a scoreboard algorithm proposed in this paper. We demonstrate that 1) uCast is powerful enough to support multiple addressing and unicast routing schemes and 2) uCast is robust, efficient, and scalable in the face of changes in network topology, such as those introduced by energy conservation protocols. We systematically evaluate the performance of uCast through simulations, compare it with other state-of-the-art protocols, and collect preliminary data from a running system based on the Berkeley motes platform","Energy efficiency,
Multicast protocols,
Unicast,
Routing protocols,
Robustness,
Network topology,
Energy conservation,
Distributed control,
Switches,
Multicast algorithms"
High-Throughput Automated System for Crystallizing Membrane Proteins in Lipidic Mesophases,"A high-throughput robotic system has been developed to enable the automatic handling of nanoliter volumes of highly viscous biomaterials for crystallizing membrane proteins using lipidic mesophases. The in meso method introduced a few years ago has produced crystal structures of a number of important membrane proteins. The bottleneck to achieve high throughput automation of this, so-called in meso method, is the handling of nanoliter volumes of the highly viscous cubic phase as part of the crystallization process. The cubic phase sticks to everything that it has contact with and has a tendency to disable dispensing robotic tools. In this paper, we discuss the factors that influence the successful and automatic delivery of nanoliter volumes of the cubic phase. A creative cubic-phase-based coordinate measuring mechanism is presented for controlling the dispensing distance of the cubic phase which is critically important for the successful performance of the system. A mathematical model describing the cubic-phase delivery is proposed and verified. We also present the optimization of liquid handling parameters for the successful and automatic delivery of different precipitant solutions. The performance characteristics of the robotic system in terms of accuracy and reproducibility of delivering nano volumes of highly viscous biomaterials and micro volumes of different precipitant solutions are reported. Note to Practitioners-Automatic handling of nanoliter volumes of highly viscous biomaterials is a practically challenging yet important task in research-and-development activities of biology and drug industry, including protein crystallization. The latter, however, is a key step for determining the structure of the proteins. Only when the structure of the proteins is revealed, can one understand the function of the proteins and/or develop drugs to cure various kinds of diseases. This paper uses a practical project, crystallizing membrane proteins using lipidic mesophases, as an example to develop approaches for automatically handling highly viscous biomaterials. This paper presents the factors that influence the successful and automatic delivery of nanoliter volumes of viscous biomaterials and introduces a new coordinate measuring mechanism using the viscous biomaterials for controlling the dispensing distance of the biomaterials, which is critical for the successful delivery of viscous materials. A mathematical model describing the delivery of viscous material is for guiding the design of practical delivering systems. This paper also presents a number of parameters for successful and automatic delivery of different precipitant solutions which are involved in the biomaterial screening activities in drug development and biological research. A simple and effective computer vision approach for measuring the volume of bio-solutions and biomaterials in the nanoliter scale is presented which is verified by the fluorescence intensity measurement of the delivered samples. The performance of the automatic system developed in terms of accuracy and reproducibility of delivering nanovolumes of highly viscous biomaterials is reported which proves that the proposed approach is not only practically useful but also feasible for many applications","Crystallization,
Biomembranes,
Nanobioscience,
Robotics and automation,
Protein engineering,
Robot kinematics,
Drugs,
Volume measurement,
Coordinate measuring machines,
Automatic control"
Joint Real-time Object Detection and Pose Estimation Using Probabilistic Boosting Network,"In this paper, we present a learning procedure called probabilistic boosting network (PBN) for joint real-time object detection and pose estimation. Grounded on the law of total probability, PBN integrates evidence from two building blocks, namely a multiclass boosting classifier for pose estimation and a boosted detection cascade for object detection. By inferring the pose parameter, we avoid the exhaustive scanning for the pose, which hampers real time requirement. In addition, we only need one integral image/volume with no need of image/volume rotation. We implement PBN using a graph-structured network that alternates the two tasks of foreground/background discrimination and pose estimation for rejecting negatives as quickly as possible. Compared with previous approaches, we gain accuracy in object localization and pose estimation while noticeably reducing the computation. We invoke PBN to detect the left ventricle from a 3D ultrasound volume, processing about 10 volumes per second, and the left atrium from 2D images in real time.","Object detection,
Boosting,
Detectors,
Ultrasonic imaging,
Real time systems,
Navigation,
Data systems,
Computer science,
Computer vision,
Application software"
CuNoC: A Scalable Dynamic NoC for Dynamically Reconfigurable FPGAs,"In this article, we present CuNoC, a new paradigm for intercommunication between modules dynamically placed on a chip for FPGA-based reconfigurable devices. The CuNoC is based on scalable communication unit called CU which allows the simultaneous communication between several processing elements placed on the chip. We present the basic concept of this communication approach, its main advantages and drawbacks with regards to the other main NoC approaches already proposed.","Network-on-a-chip,
Field programmable gate arrays,
Switches,
Tiles,
Routing,
Switching circuits,
Packet switching,
Communication switching,
Parallel processing,
Computer science"
Structure Prediction in Temporal Networks using Frequent Subgraphs,"There are several types of processes which can be modeled explicitly by recording the interactions between a set of actors over time. In such applications, a common objective is, given a series of observations, to predict exactly when certain interactions will occur in the future. We propose a representation for this type of temporal data and a generic, streaming, adaptive algorithm to predict the pattern of interactions at any arbitrary point in the future. We test our algorithm on predicting patterns in e-mail logs, correlations between stock closing prices, and social grouping in herds of Plains zebras. Our algorithm averages over 85% accuracy in predicting a set of interactions at any unseen timestep. To the best of our knowledge, this is the first algorithm that predicts interactions at the finest possible time grain","Computer science,
Predictive models,
Data mining,
Testing,
Prediction algorithms,
Accuracy,
Computational intelligence,
Application software,
Adaptive algorithm,
Current measurement"
Nonlinear and neural network-based control of a small four-rotor aerial robot,"Small four-rotor aerial robots, so called quadrotor UAVs, have an enormous potential for all kind of near-area surveillance and exploration in military and commercial applications. In addition, they offer the possibility to fly either in-or outdoor. However, stabilizing control and guidance of these vehicles is a difficult task because of the nonlinear dynamic behavior. This paper describes the development of a nonlinear vehicle control system based on a combination of state-dependent Riccati equations (SDRE) and neural networks. Some first simulation results underline the performance of this new control approach for the current realization.","Neural networks,
Robots,
Unmanned aerial vehicles,
Riccati equations,
Surveillance,
Navigation,
Nonlinear dynamical systems,
Vehicle dynamics,
Nonlinear control systems,
Control systems"
Space-Efficient Identity Based EncryptionWithout Pairings,"Identity Based Encryption (IBE) systems are often constructed using bilinear maps (a.k.a. pairings) on elliptic curves. One exception is an elegant system due to Cocks which builds an IBE based on the quadratic residuosity problem modulo an RSA composite N. The Cocks system, however, produces long ciphertexts. Since the introduction of the Cocks system in 2001 it has been an open problem to construct a space efficient IBE system without pairings. In this paper we present an IBE system in which ciphertext size is short: an encryption of an f.-bit message consists of a single element in Z/NZ plus lscr + 1 additional bits. Security, as in the Cocks system, relies on the quadratic residuosity problem. The system is based on the theory of ternary quadratic forms and as a result, encryption and decryption are slower than in the Cocks system.","Identity-based encryption,
Computer science,
Public key,
Elliptic curves,
Application software,
Public key cryptography,
Elliptic curve cryptography,
Computer security"
Bandwidth Sharing Schemes for Multimedia Traffic in the IEEE 802.11e Contention-Based WLANs,"Bandwidth allocation schemes have been well studied for mobile cellular networks. However, there is no study about this aspect reported for IEEE 802.11 contention-based distributed wireless LANs. In cellular networks, bandwidth is deterministic in terms of the number of channels by frequency division, time division, or code division. On the contrary, bandwidth allocation in contention- based distributed wireless LANs is extremely challenging due to its contention-based nature, packet-based network, and the most important aspect: only one channel is available, competed for by an unknown number of stations. As a consequence, guaranteeing bandwidth and allocating bandwidth are both challenging issues. In this paper, we address these difficult issues. We propose and study nine bandwidth allocation schemes, called sharing schemes, with guaranteed Quality of Service (QoS) for integrated voice/video/data traffic in IEEE 802.11e contention-based distributed wireless LANs. A guard period is proposed to prevent bandwidth allocation from overprovisioning and is for best-effort data traffic. Our study and analysis show that the guard period is a key concept for QoS guarantees in a contention-based channel. The proposed schemes are compared and evaluated via extensive simulations.","Bandwidth,
Quality of service,
Wireless LAN,
Channel allocation,
Land mobile radio cellular systems,
Telecommunication traffic,
Frequency conversion,
Costs,
Computer science,
Video sharing"
Feature Extraction for Multi-class BCI using Canonical Variates Analysis,"To propose a new feature extraction method with canonical solution for multi-class brain-computer interfaces (BCI). The proposed method should provide a reduced number of canonical discriminant spatial patterns (CDSP) and rank the channels sorted by power discriminability (DP) between classes. The feature extractor relays in canonical variates analysis (CVA) which provides the CDSP between the classes. The number of CDSP is equal to the number of classes minus one. We analyze EEG data recorded with 64 electrodes from 4 subjects recorded in 20 sessions. They were asked to execute twice in each session three different mental tasks (left hand imagination movement, rest, and words association) during 7 seconds. A ranking of electrodes sorted by power discriminability between classes and the CDSP were computed. After splitting data in training and test sets, we compared the classification accuracy achieved by linear discriminant analysis (LDA) in frequency and temporal domains. The average LDA classification accuracies over the four subjects using CVA on both domains are equivalent (57.89% in frequency domain and 59.43% in temporal domain). These results, in terms of classification accuracies, are also reflected in the similarity between the ranking of relevant channels in both domains. CVA is a simple feature extractor with canonical solution useful for multi-class BCI applications that can work on temporal or frequency domain.","Feature extraction,
Linear discriminant analysis,
Frequency domain analysis,
Electrodes,
Brain computer interfaces,
Data mining,
Relays,
Data analysis,
Electroencephalography,
Testing"
On the Sustained Use of a Test-Driven Development Practice at IBM,"Test-driven development (TDD) is an agile practice that is widely accepted and advocated by most agile methods and methodologists. In this paper, we report on a post hoc analysis of the results of an IBM team who has sustained use of TDD for five years and over ten releases of a Java-implemented product. The team worked from a design and wrote tests incrementally before or while they wrote code and, in the process, developed a significant asset of automated tests. The IBM team realized sustained quality improvement relative to a pre-TDD project and consistently had defect density below industry standards. As a result, our data indicate that the TDD practice can aid in the production of high quality products. This quality improvement would compensate for the moderate perceived productivity losses. Additionally, our data indicates that the use of TDD may decrease the degree to which code complexity increases as software ages, as measured by cyclomatic complexity metric.","Java,
Automatic testing,
Software testing,
Programming profession,
Production,
Writing,
Process design,
Automatic control,
Computer science,
Productivity"
A Hardware Approach to Real-Time Program Trace Compression for Embedded Processors,"Collecting the program execution traces at full speed is essential to the analysis and debugging of real-time software behavior of a complex system. However, the generation rate and the size of real-time program traces are so huge such that real-time program tracing is often infeasible without proper hardware support. This paper presents a hardware approach to compress program execution traces in real time in order to reduce the trace size. The approach consists of three modularized phases: 1) branch/target filtering; 2) branch/target address encoding; 3) Lempel-Ziv (LZ)-based data compression. A synthesizable RTL code for the proposed hardware is constructed to analyze the hardware cost and speed and typical multimedia benchmarks are used to measure the compression results. The results show that our hardware is capable of real-time compression and achieving compression ratio of 454:1, far better than 5:1 achieved by typical existing hardware approaches. Furthermore, our modularized approach makes it possible to trade off between the hardware cost (typically from 1 to 50K gates) and the achievable compression ratio (typically from 5:1 to 454:1)","Hardware,
Costs,
Software debugging,
Filtering,
Encoding,
Data compression,
Microprocessors,
Data acquisition,
Computer science,
Real time systems"
Low Cost Passive UHF RFID Packaging with Electromagnetic Band Gap (EBG) Substrate for Metal Objects,"Passive UHF (Ultra High Frequency) RFID (Radio Frequency Identification) is a promising technology for products tracking in logistics or routing packages in supply chain. However, the RFID tag is significantly affected by objects surround it especially metallic objects. In this paper, we disclosed a new method using electromagnetic band gap (EBG) material to insulate the UHF RFID tag from backside objects, so that the tag could work on water surface and even metal. The design of EBG material for UHF RFID which operates at 915 MHz was discussed in this paper. An ultrathin (<1.5 mm) tag on metal with a gain of 4 dBi was achieved in simulation and a prototype of 2.93 mm thick RFID on metal tag was measured in experiment. The simulation results showed that the RFID tag still could be read on EBG substrate with a good gain (~4 dBi). The measurement results indicated that the read rang of RFID tag with EBG substrate could reach up to 4 meters on metal template.","Costs,
Radiofrequency identification,
Passive RFID tags,
Packaging,
Periodic structures,
Metamaterials,
RFID tags,
Frequency,
Insulation life,
Logistics"
Experimental Study on Sentiment Classification of Chinese Review using Machine Learning Techniques,"Machine learning method in text classification has expanded from topic identification to more challenging tasks such as sentiment classification, and it is valuable to explore, compare methods applied in sentiment classification and investigate relevant influence factors. The chief aim of the present work is to compare four machine learning methods to sentiment classification of Chinese review. The corpus is made up of 16000 reviews from website. We investigate the factors which affect the performance: namely feature representation via Word-Based Unigram (WBU), Bigram (WBB) and Chinese Character-Based Bigram (CBB), Trigram (CBT); feature weighting schemes and feature dimensionality. Experimental evaluations show that performance depends on different settings. As a result, we draw a conclusion that Naive Bayes (NB) classifier obtains the best averaging performance when using WBB, CBT as features with bool weighting under different dimensionality to the task.","Machine learning,
Learning systems,
Data mining,
Computer science,
Niobium,
Text categorization,
Motion pictures,
Thumb,
Support vector machines,
Support vector machine classification"
Evolving neuromodulatory topologies for reinforcement learning-like problems,"Environments with varying reward contingencies constitute a challenge to many living creatures. In such conditions, animals capable of adaptation and learning derive an advantage. Recent studies suggest that neuromodulatory dynamics are a key factor in regulating learning and adaptivity when reward conditions are subject to variability. In biological neural networks, specific circuits generate modulatory signals, particularly in situations that involve learning cues such as a reward or novel stimuli. Modulatory signals are then broadcast and applied onto target synapses to activate or regulate synaptic plasticity. Artificial neural models that include modulatory dynamics could prove their potential in uncertain environments when online learning is required. However, a topology that synthesises and delivers modulatory signals to target synapses must be devised. So far, only handcrafted architectures of such kind have been attempted. Here we show that modulatory topologies can be designed autonomously by artificial evolution and achieve superior learning capabilities than traditional fixed-weight or Hebbian networks. In our experiments, we show that simulated bees autonomously evolved a modulatory network to maximise the reward in a reinforcement learning-like environment.","Network topology,
Animals,
Biological neural networks,
Circuits,
Signal generators,
Broadcasting,
Biological system modeling,
Network synthesis,
Signal synthesis,
Evolution (biology)"
Fast Genetic Algorithms Used for PID Parameter Optimization,"PID parameter optimization is an important problem in control field. This paper presents a kind of fast genetic algorithms, which have a lot of improvements about population, selection, crossover and mutation in comparison with simple genetic algorithms. These fast genetic algorithms are used in PID parameter optimization for common objective model to remedy flaws of simple genetic algorithms and accelerate the convergence. The algorithms are simulated with MATLAB programming. The simulation result shows that the PID controller with fast genetic algorithms has a fast convergence rate and a better dynamic performance.","Genetic algorithms,
Three-term control,
Biological cells,
Genetic mutations,
Automation,
Convergence,
Optimization methods,
Logistics,
Genetic engineering,
Educational institutions"
Using Games in Software Engineering Education to Teach Risk Management,"An innovative board game was developed at Carnegie Mellon University's Master in Software Engineering Program to teach risk management concepts. Piloted in two separate courses dealing with software project risk management, the goal of the game was to enhance practical learning and decision making through simulating a software development project. This activity was then compared to others used in the class, specifically lectures and case discussions, to assess its effectiveness in meeting learning objectives. The results clearly demonstrate the advantages of using a game method in teaching software engineering concepts. Findings will discuss how the game compares to other teaching activities in the classroom in terms of conveying relevant information, help in concept understanding and learning enjoyment.","Software engineering,
Risk management,
Game theory,
Education,
Educational programs,
Decision making,
Gas insulated transmission lines,
Project management,
Programming profession,
Teamwork"
Combining Collective Classification and Link Prediction,"The problems of object classification (labeling the nodes of a graph) and link prediction (predicting the links in a graph) have been largely studied independently. Commonly, object classification is performed assuming a complete set of known links and link prediction is done assuming a fully observed set of node attributes. In most real world domains, however, attributes and links are often missing or incorrect. Object classification is not provided with all the links relevant to correct classification and link prediction is not provided all the labels needed for accurate link prediction. In this paper, we propose an approach that addresses these two problems by interleaving object classification and link prediction in a collective algorithm. We investigate empirically the conditions under which an integrated approach to object classification and link prediction improves performance, and find that performance improves over a wide range of network types, and algorithm settings.","Data mining,
Labeling,
Iterative algorithms,
Conferences,
Computer science,
Educational institutions,
Interleaved codes,
Information analysis,
Performance analysis,
Roads"
Optimized transmission power control of interrogators for collision arbitration in UHF RFID systems,"The emergence of UHF RFID as one of the dominant technology trends has posed numerous unique challenges to researchers. This letter presents a novel, theoretically-grounded collision arbitration protocol, called TPC-CA, which optimally controls transmission power of RFID interrogators and thereby reducing redundant interrogator collisions","Power control,
Radiofrequency identification,
Transponders,
Optimal control,
Computer science,
Protocols,
Hafnium,
Power system modeling,
Impedance,
Receiving antennas"
Introducing Curvature into Globally Optimal Image Segmentation: Minimum Ratio Cycles on Product Graphs,"While the majority of competitive image segmentation methods are based on energy minimization, only few allow to efficiently determine globally optimal solutions. A graph-theoretic algorithm for finding globally optimal segmentations is given by the minimum ratio cycles, first applied to segmentation by Jermyn and Ishikawa (2001). In this paper we show that the class of image segmentation problems solvable by minimum ratio cycles is significantly larger than previously considered. In particular, they allow for the introduction of higher-order regularity of the region boundary. The key idea is to introduce an extended graph representation, where each node of the graph represents an image pixel as well as the orientation of the incoming line segment. With each graph edge representing a pair of adjacent line segments, edge weights can depend on the curvature. This way arbitrary positive functions of curvature can be introduced into globally optimal segmentation by minimum ratio cycles. In numerous experiments we demonstrate that compared to length-regularity the integration of curvature-regularity will drastically improve segmentation results. Moreover, we show an interesting relation to the snakes functional: minimum ratio cycles provide a way to find one of the few cases where the snakes functional has a meaningful global minimum.","Image segmentation,
Humans,
Computer science,
Minimization methods,
Pixel,
Image generation,
Testing,
Partial differential equations,
Prototypes,
Integral equations"
QoS-aware Service Composition Based on Tree-Coded Genetic Algorithm,"A novel tree-coding genetic algorithms (TGA) is presented for QoS-aware service composition. Since tree-coding schema can carry the information of static model of service workflow, this feature qualifies TGA to make the chromosomes to be encoded and decoded automatically, and keep the medial result for fitness computing. The Tree-coding can also support the services composition re-planning at runtime effectively. The experiment results show that TGA run faster than the one-dimensional coding GA when the optimal result is same, furthermore the algorithm with tree-coding is effective for re-planning.","Genetic algorithms,
Biological cells,
Quality of service,
Encoding,
Web services,
Genetic mutations,
Computer science,
Decoding,
Runtime,
Algorithm design and analysis"
Vascular Space Occupancy Weighted Imaging With Control of Residual Blood Signal and Higher Contrast-to-Noise Ratio,"It has been recently proposed that the local cerebral blood volume change during brain activation can be measured by a series of images whose contrast is dependent on vascular space occupancy (VASO). VASO takes advantage of the inversion recovery sequence to acquire images when the longitudinal magnetization (MZ) of blood is relaxing through zero. The degree of blood suppression, however, is not always well controlled as a consequence of spatial variations in inversion efficiency and blood T1. Furthermore, while blood is eliminated, the MZ of other tissues is also small, which makes the contrast-to-noise ratio inherently low in VASO. In this paper, diffusion gradients were applied to demonstrate residual intravascular signal in the original VASO. An alternative VASO-weighted imaging was then proposed using a longer inversion time at which the MZ difference between blood and gray matter was optimized. A global saturation immediately after image acquisition was employed to eliminate the MZ disparity between inflowing blood and the residual in-plane blood from previous acquisition. Feasibility was evaluated by numerical simulation and functional experiments. In human visual cortex, the fractional VASO signal and cerebral blood volume changes were found to be 0.6% and 44%, respectively (voxel size = 3.4 times 3.4 times 5.0 mm3). As compared to the original VASO, the presented method provided a largely comparable activation map and hemodynamic curve but was not confounded by the existence of blood. Results also demonstrated its advantages of 1.6-fold higher CNR and insensitivity to variant tissue/blood T1 as well as inversion efficiency.","Weight control,
Blood,
Radiology,
Extraterrestrial measurements,
Magnetization,
Humans,
Hemodynamics,
Volume measurement,
Numerical simulation,
Magnetic resonance imaging"
A Low Power Carbon Nanotube Chemical Sensor System,"This paper presents an energy efficient chemical sensor system that uses carbon nanotubes (CNT) as the sensor. The room-temperature operation of CNT sensors eliminates the need for micro hot-plate arrays, which enables the low energy operation of the system. The sensor interface chip is designed in a 0.18 mum CMOS process and consumes, at maximum, 32 muW at 1.83 kS/s conversion rate. The designed interface achieves 1.34% measurement accuracy over 10 kOmega -9 MOmega dynamic range. The functionality of the full system, including CNT sensors, has been successfully demonstrated.","Carbon nanotubes,
Chemical sensors,
Sensor arrays,
Immune system,
Circuits,
Sensor systems,
Electrical resistance measurement,
Dynamic range,
Monitoring,
Chemical and biological sensors"
Map-Enhanced UAV Image Sequence Registration,"Registering consecutive images from an airborne sensor into a mosaic is an essential tool for image analysts. Strictly local methods tend to accumulate errors, resulting in distortion. We propose here to use a reference image (such as a high resolution map image) to overcome this limitation. In our approach, we register a frame in an image sequence to the map using both frame-to-frame registration and frame-to-map registration iteratively. In frame-to-frame registration, a frame is registered to its previous frame. With its previous frame been registered to the map in the previous iteration, we can derive an estimated transformation from the frame to the map. In frame-to-map registration, we warp the frame to the map by this transformation to compensate for scale and rotation difference and then perform an area based matching using mutual information to find correspondences between this warped frame and the map. From these correspondences, we derive a transformation that further registers the warped frame to the map. With this two-step registration, the errors between each consecutive frames are not accumulated. We present results on real image sequences from a hot air balloon","Unmanned aerial vehicles,
Image sequences,
Mutual information,
Computer errors,
Computer science,
Image sensors,
Image analysis,
Image sequence analysis,
Image resolution,
Navigation"
Globalizing Software Development in the Local Classroom,"Given the requirement for software engineering graduates to operate in Global Software Development (GSD) environments, educators need to develop teaching methods to enhance and instill GSD knowledge in their students. In this paper, we discuss two projects that provided students with a first-hand learning experience of working within GSD teams. One project was with Siemens Corporate Research, whose focus was to shadow the development of a real-life GSD project. The second project, whose focus was virtual team software testing, was carried out in collaboration with Ball State University. In parallel with these projects we undertook qualitative research during which we analyzed students' own written reflections and face-to-face interviews that focused on their learning experiences in these contexts. We identified three specific forms of learning which had taken place: pedagogical, pragmatic and the acquisition of specific globally distributed knowledge. Our findings confirm that mimicking real work settings has educational benefits for problem-based learning environments.","Programming,
Software engineering,
Natural languages,
Virtual groups,
Global communication,
Computer science,
Information systems,
Education,
Project management,
Computer industry"
Node-Replacement Policies to Maintain Threshold-Coverage in Wireless Sensor Networks,"With the rapid deployment of wireless sensor networks, there are several new sensing applications with specific requirements. Specifically, target tracking applications are fundamentally concerned with the area of coverage across a sensing site in order to accurately track the target. We consider the problem of maintaining a minimum threshold-coverage in a wireless sensor network, while maximizing network lifetime and minimizing additional resources. We assume that the network has failed when the sensing coverage falls below the minimum threshold-coverage. We develop three node-replacement policies to maintain threshold-coverage in wireless sensor networks. These policies assess the candidature of each failed sensor node for replacement. Based on different performance criteria, every time a sensor node fails in the network, our replacement policies either replace with a new sensor or ignore the failure event. The node-replacement policies replace a failed node according to a node weight. The node weight is assigned based on one of the following parameters: cumulative reduction of sensing coverage, amount of energy increase per node, and local reduction of sensing coverage. We also implement a first-fail-first-replace policy and a no-replacement policy to compare the performance results. We evaluate the different node-replacement polices through extensive simulations. Our results show that given a fixed number of replacement sensor nodes, the node-replacement policies significantly increase the network lifetime and the quality of coverage, while keeping the sensing-coverage about a pre-set threshold.","Wireless sensor networks,
Sensor phenomena and characterization,
Target tracking,
Radar tracking,
Biosensors,
Sonar detection,
Application software,
IP networks,
Internet,
Quality of service"
Modeling Pairwise Key Establishment for Random Key Predistribution in Large-Scale Sensor Networks,"Sensor networks are composed of a large number of low power sensor devices. For secure communication among sensors, secret keys are required to be established between them. Considering the storage limitations and the lack of post-deployment configuration information of sensors, random key predistribution schemes have been proposed. Due to limited number of keys, sensors can only share keys with a subset of the neighboring sensors. Sensors then use these neighbors to establish pairwise keys with the remaining neighbors. In order to study the communication overhead incurred due to pairwise key establishment, we derive probability models to design and analyze pairwise key establishment schemes for large-scale sensor networks. Our model applies the binomial distribution and a modified binomial distribution and analyzes the key path length in a hop-by-hop fashion. We also validate our models through a systematic validation procedure. We then show the robustness of our results and illustrate how our models can be used for addressing sensor network design problems.","Large-scale systems,
Sensor phenomena and characterization,
Cities and towns,
Wireless sensor networks,
Sensor systems,
Secure storage,
Computer science,
Robustness,
Routing,
Communication system security"
2 GHz CMOS Voltage-Controlled Oscillator with Optimal Design of Phase Noise and Power Dissipation,An RF VCO design optimization strategy to achieve low phase noise and low bias current is presented for a cross-coupled LC-tuned CMOS oscillator topology. The impact of differential pair transistors' mode of operation and loading effects on the oscillator phase noise are investigated. The study shows that an optimal trade-off between thermal-noise-induced phase noise and DC power dissipation can be achieved when the oscillation amplitude is designed to set the differential pair transistors to operate at the boundary between saturation and triode regions. This design technique is employed to demonstrate a 2 GHz VCO achieving a low phase noise of -103 dBc/Hz at 100 kHz offset frequency while dissipating 2.67 mA bias current from a 1.8 V supply in a standard 0.18 mum CMOS process. The optimization strategy can be applied for other VCO design architectures to further enhance wireless communication system performance and battery lifetime.,
Point Placement by Phylogenetic Trees and its Application to Visual Analysis of Document Collections,"The task of building effective representations to visualize and explore collections with moderate to large number of documents is hard. It depends on the evaluation of some distance measure among texts and also on the representation of such relationships in bi- dimensional spaces. In this paper we introduce an alternative approach for building visual maps of documents based on their content similarity, through reconstruction of phylogenetic trees. The tree is capable of representing relationships that allows the user to quickly recover information detected by the similarity metric. For a variety of text collections of different natures we show that we can achieve improved exploration capability and more clear visualization of relationships amongst documents.","Phylogeny,
Text analysis,
Visualization,
Extraterrestrial measurements,
Multidimensional systems,
Computer graphics,
Text processing,
Internet,
Electronic mail,
Visual databases"
A New Operational Transformation Framework for Real-Time Group Editors,"Group editors allow a group of distributed human users to edit a shared multimedia document at the same time over a computer network. Consistency control in this environment must not only guarantee convergence of replicated data, but also attempt to preserve intentions of operations. Operational transformation (OT) is a well-established method for optimistic consistency control in this context and has drawn continuing research attention since 1989. However, counterexamples to previous works have often been identified despite the significant progress made on this topic over the past 15 years. This paper analyzes the root of correctness problems in OT and establishes a novel operational transformation framework for developing OT algorithms and proving their correctness",
Identification of nonlinear systems using particle swarm optimization technique,"System identification in noisy environment has been a matter of concern for researchers in many disciplines of science and engineering. In the past the least mean square algorithm (LMS), genetic algorithm (GA) etc. have been employed for developing a parallel model. During training by LMS algorithm the weights rattle around and does not converge to optimal solution. This gives rise to poor performance of the model. Although GA always ensures the convergence of the weights to the global optimum but it suffers from slower convergence rate. To alleviate the problem we propose a novel Particle Swarm Optimization (PSO) technique for identifying nonlinear systems. The PSO is also a population based derivative free optimization technique like GA, and hence ascertains the convergence of the model parameters to the global optimum, there by yielding the same performance as provided by GA but with a faster speed. Comprehensive computer simulations validate that the PSO based identification is a better candidate even under noisy condition both in terms of convergence speed as well as number of input samples used.","Nonlinear systems,
Particle swarm optimization,
Evolutionary computation"
A comparative analysis of techniques for predicting academic performance,"This paper compares the accuracy of decision tree and Bayesian network algorithms for predicting the academic performance of undergraduate and postgraduate students at two very different academic institutes: Can Tho University (CTU), a large national university in Viet Nam; and the Asian Institute of Technology (AIT), a small international postgraduate institute in Thailand that draws students from 86 different countries. Although the diversity of these two student populations is very different, the data-mining tools were able to achieve similar levels of accuracy for predicting student performance: 73/71% for {fail, fair, good, very good} and 94/93% for {fail, pass} at the CTU/AIT respectively. These predictions are most useful for identifying and assisting failing students at CTU (64% accurate), and for selecting very good students for scholarships at the AIT (82% accurate). In this analysis, the decision tree was consistently 3-12% more accurate than the Bayesian network. The results of these case studies give insight into techniques for accurately predicting student performance, compare the accuracy of data mining algorithms, and demonstrate the maturity of open source tools.","Performance analysis,
Data mining,
Decision trees,
Bayesian methods,
Prediction algorithms,
Accuracy,
Scholarships,
Predictive models,
Information analysis,
Algorithm design and analysis"
Construction of Irregular LDPC Codes by Quasi-Cyclic Extension,"In this correspondence, we propose an approach to construct irregular low-density parity-check (LDPC) codes based on quasi-cyclic extension. When decoded iteratively, the constructed irregular LDPC codes exhibit a relatively low error floor in the high signal-to-noise ratio (SNR) region and are subject to relatively few undetected errors. The LDPC codes constructed based on the proposed scheme remain efficiently encodable","Parity check codes,
Floors,
Iterative decoding,
Computer errors,
Computer science,
Signal to noise ratio,
Belief propagation,
Iterative algorithms,
Design optimization"
On Adjusting Power to Defend Wireless Networks from Jamming,"Wireless networks are susceptible to accidental or intentional radio interference. One way to cope with this threat is to have the radios compete with the jammer, whereby the network nodes adapt their transmission power to improve the chance for successful communication. In this paper, we examine issues associated with using power control both theoretically and experimentally. We begin by examining the two-party, single-jammer scenario, where we explore the underlying communication theory associated with jamming. We note that the effect of the jammer upon source-receiver communications is not isotropic. We then discuss the potential for improving communication reliability through experiments conducted using Mica2 motes, and in particular explore the feasibility of power- control for competing against jammers. Next, we turn to examining the more complicated scenario consisting of a multi- hop wireless network. We show the complex jamming effect by applying the non-isotropic model of jamming to a multi-hop wireless network, and it is necessary to have a feed-back based power control protocol to compete with jamming interference.","Wireless networks,
Jamming,
Spread spectrum communication,
Wireless sensor networks,
Electromagnetic interference,
Power control,
Wireless mesh networks,
Telecommunication network reliability,
Access protocols,
Sensor arrays"
Probabilistic Coverage Preserving Protocol with Energy Efficiency in Wireless Sensor Networks,"In this paper, we propose a k-coverage preserving protocol to achieve energy efficiency while ensuring the required coverage. In our protocol, we try to select a minimal active set of sensor nodes to reach energy conservation and maintain a complete area k-coverage. We model this problem as a minimum set cover problem and solve it by using a heuristic greedy algorithm. Based on the k-coverage preserving protocol, we then propose a protocol to deal with the probabilistic k-coverage requirement, in which each sensor could be assumed to be able to detect a nearby event with a certain probability. In the probabilistic k-coverage protocol, any point in the monitoring region can be sensed by at least k sensor nodes no lower than a confidence probability. Finally, we evaluate the performance of our protocols with simulations.",
Multi-state grid resource availability characterization,"The functional heterogeneity of non-dedicated computational grids will increase with the inclusion of resources from desktop grids, P2P systems, and even mobile grids. Machine failure characteristics, as well as individual and organizational policies for resource usage by the grid, will increasingly vary even more than they already do. Since grid applications also vary as to how well they tolerate the failure of the host on which they run, grid schedulers must begin to predict and consider how resources will transition between availability modes. Toward this goal, this paper introduces five availability states, and characterizes a Condor pool trace that uncovers when, how, and why its resources reside in, and transition between, these states. This characterization suggests resource categories that schedulers can use to make better mapping decisions. Simulations that characterize how a variety of jobs would run on the traced resources demonstrate this approach's potential for performance improvement. A simple predictor based on the previous day's behavior indicates that the states and categories arc somewhat predictable, thereby supporting the potential usefulness of multi-state grid resource availability characterization.","Availability,
Grid computing,
Scheduling,
Peer to peer computing,
Middleware,
Checkpointing,
Computer science,
Mobile computing,
Testing,
Centralized control"
Development of a Multifunctional Cosmetic Prosthetic Hand,"An innovative artificial hand is presented, which can help to restore both motor and sensory capabilities of upper extremity amputees. All requisite components of the revolutionary prosthesis fit into the small volume of the metacarpus. A new high-power actuating technology has been developed for maximizing the benefit in using the prosthetic hand by increasing the number of grasping patterns. An optional sensory feedback system has been designed for the prosthesis, which is based on mechanical vibration. First clinical trials with the prosthetic hand revealed a high acceptance, as the force necessary to hold an object securely was reduced significantly.","Prosthetic hand,
Fingers,
Grasping,
Switches,
Wrist,
Orthopedic surgery,
Force feedback,
Rehabilitation robotics,
Extremities,
Vibrations"
Analysis of Hierarchical EDF Pre-emptive Scheduling,"This paper focuses on scheduling different hard real-time applications on a uniprocessor when the earliest deadline first algorithm is used as the local scheduler, and the global scheduler of the system could be fixed priority (FP) or earliest deadline first (EDF). Each application task could be periodic or sporadic, bound or unbound, with arbitrary relative deadline which could be less than, equal to or greater than its period. A number of different server types are considered. This paper presents an exact and efficient schedulability test for the application tasks based on the capacity demand criterion when the global scheduler could be FP or EDF, in some cases, it is necessary and sufficient. Schedulability tests which are necessary and sufficient for several types of dynamic servers are presented when the global scheduler is EDF.",
An Analytic Potential-Based Model for Undoped Nanoscale Surrounding-Gate MOSFETs,"An analytic potential-based model for the undoped surrounding-gate MOSFETs is derived in the paper. The model is obtained from rigorously solving Poisson equation together with the drain-current formulation equivalent to Pao-Sah's double integral that is previously proposed for long-channel bulk MOSFETs. The model consists of an analytic drain-current equation that accounts for both drift and diffusion current components in terms of the potential at the oxide silicon interface and the silicon center of device body evaluated at the source and drain terminals. The model gives a fully self-consistent physical description for the channel potential, charge, and current that is valid for the subthreshold, linear, and saturation regions. The validity of the proposed model has been verified by extensive comparison with the exact numerical integrations and 2-D numerical simulation, which demonstrates model accuracy and prediction capability.","MOSFETs,
Mathematical model,
Analytical models,
Silicon,
Numerical models,
Integrated circuit modeling,
Electric potential"
Reactive deformation roadmaps: motion planning of multiple robots in dynamic environments,"We present a novel algorithm for motion planning of multiple robots amongst dynamic obstacles. Our approach is based on a new roadmap representation that uses deformable links and dynamically retracts to capture the connectivity of the free space. We use Newtonian physics and Hooke's Law to update the position of the milestones and deform the links in response to the motion of other robots and the obstacles. Based on this roadmap representation, we describe our planning algorithms that can compute collision-free paths for tens of robots in complex dynamic environments.",
Underground Structure Monitoring with Wireless Sensor Networks,"Environment monitoring in coal mines is an important application of wireless sensor networks (WSNs) that has commercial potential. We discuss the design of a structure-aware self-adaptive WSN system, SASA. By regulating the mesh sensor network deployment and formulating a collaborative mechanism based on a regular beacon strategy, SASA is able to rapidly detect structure variations caused by underground collapses. A prototype is deployed with 27 Mica2 motes. We present our implementation experiences as well as the experimental results. To better evaluate the scalability and reliability of SASA, we also conduct a large-scale trace-driven simulation based on real data collected from the experiments.",
Effects of Classification Methods on Color-Based Feature Detection With Food Processing Applications,"Color information is useful in vision-based feature detection, particularly for food processing applications where color variability often renders grayscale-based machine-vision algorithms that are difficult or impossible to work with. This paper presents a color machine vision algorithm that consists of two components. The first creates an artificial color contrast as a prefilter that aims at highlighting the target while suppressing its surroundings. The second, referred to here as the statistically based fast bounded box (SFBB), utilizes the principal component analysis technique to characterize target features in color space from a set of training data so that the color classification can be performed accurately and efficiently. We evaluate the algorithm in the context of food processing applications and examine the effects of the color characterization on computational efficiency by comparing the proposed solution against two commonly used color classification algorithms; a neural-network classifier and the support vector machine. Comparison among the three methods demonstrates that statistically based fast bounded box is relatively easy to train, efficient, and effective since with sufficient training data, it does not require any additional optimization steps; these advantages make SFBB an ideal candidate for high-speed automation involving live and/or natural objects. Note to Practitioners-Variability in natural objects is usually several orders of magnitude higher than that for manufactured goods and has remained a challenge. As a result, most solutions to inspection problems of natural products today still have humans in the loop. One of the factors influencing the success rate of color machine vision in detecting a target is its ability to characterize colors. When unrelated features are very close to the target in the color space, which may not pose a significant problem to an experienced operator, they appear as noise and often result in false detection. This paper illustrates the applicability of the algorithm with a number of representative automation problems in the context of food processing applications. As demonstrated experimentally, the artificial color contrast and statistically based fast bounded box methods can significantly improve the success rate of the detection by reducing the standard deviation of both the target and noise pixels, enlarging the separation between feature clusters in color space, and more tightly characterize the feature color from its background. The algorithm presented here has several advantages, including simplicity in training and fast classification, since only three simple checks of rectangular bounds are performed","Computer vision,
Colored noise,
Color,
Machine vision,
Training data,
Principal component analysis,
Computational efficiency,
Classification algorithms,
Support vector machines,
Support vector machine classification"
Handling shape and contact location uncertainty in grasping two-dimensional planar objects,"This paper addresses the problem of selecting contact locations for grasping objects in the presence of shape and contact location uncertainty. Focusing on two-dimensional planar objects and two finger grasps for simplicity, we present a principled approach for selecting contact points by analyzing the risk of force closure failure. The key contribution of this paper is the development of a method that incorporates shape uncertainty into grasp stability analysis. We propose a grasp quality metric that can be used to identify stable contact regions in the face of shape and contact location uncertainty. The proposed method successfully distinguishes grasps that are equivalent without uncertainty, and we illustrate the properties of this technique with simulation experiments in two classes of objects.",
Hardware Acceleration of Matrix Multiplication on a Xilinx FPGA,"The first MEMOCODE hardware/software co-design contest posed the following problem: optimize matrix-matrix multiplication in such a way that it is split between the FPGA and PowerPC on a Xilinx Virtex IIPro30. In this paper we discuss our solution, which we implemented on a Xilinx XUP development board with 256 MB of DRAM. The design was done by the five authors over a span of approximately 3 weeks, though of the 15 possible man-weeks, about 9 were actually spent working on this problem. All hardware design was done using Blue-spec SystemVerilog (BSV), with the exception of an imported Verilog multiplication unit, necessary only due to the limitations of the Xilinx FPGA toolflow optimizations.","Acceleration,
Field programmable gate arrays,
Switches,
Communication system control,
Control systems,
Random access memory,
Computer science,
Artificial intelligence,
Hardware design languages,
Design optimization"
Modeling for NASA Autonomous Nano-Technology Swarm Missions and Model-Driven Autonomic Computing,"NASA ANTS autonomous nano-technology swarm missions will be operating in the universe, and therefore rely much on high autonomy. This paper presents a novel technology for NASA's ANTS missions, named as model-driven autonomic computing. As the foundation for the technology, a new model is constructed for the ANTS system. Exceeding other existent models, the new hierarchical model overcomes the challenges of largeness, complexity, dynamicity and unexpectedness possessed by the ANTS system. Then, the paper exhibits the structure and functions of virtual neuron that is basic unit together with the model for the model-driven autonomic technology in ANTS missions. The paper also deploys self-configuration, self-healing, self-optimization and self-protection for ANTS. A case study, examples and simulations are illustrated.","NASA,
Space technology,
Neurons,
Space vehicles,
Computer science,
Particle swarm optimization,
Biology computing,
Autonomic nervous system,
Tree data structures,
Educational institutions"
Testing for Concise Representations,"We describe a general method for testing whether a function on n input variables has a concise representation. The approach combines ideas from the junta test of Fischer et al. 16 with ideas from learning theory, and yields property testers that make po!y(s/epsiv) queries (independent of n) for Boolean function classes such as s-term DNF formulas (answering a question posed by Parnas et al. [12]), sizes. decision trees, sizes Boolean formulas, and sizes Boolean circuits. The method can be applied to non-Boolean valued function classes as well. This is achieved via a generalization of the notion of van at ion/row Fischer et al. to non-Boolean functions. Using this generalization we extend the original junta test of Fischer et al. to work for non-Boolean functions, and give poly(s/e)-query testing algorithms for non-Boolean valued function classes such as sizes algebraic circuits and s-sparse polynomials over finite fields. We also prove an Omega(radic(s)) query lower bound for nonadaptively testing s-sparse polynomials over finite fields of constant size. This shows that in some instances, our general method yields a property tester with query complexity that is optimal (for nonadaptive algorithms) up to a polynomial factor.","Circuit testing,
Polynomials,
Decision trees,
Galois fields,
Boolean functions,
Computer science,
Input variables,
Neural networks,
Binary decision diagrams"
Brick& Mortar: an on-line multi-agent exploration algorithm,"When an emergency occurs within a building, it is critical to explore the area as fast as possible in order to find victims and identify hazards. We propose Brick&Mortar, an algorithm for the autonomous exploration of unknown terrains by a team of mobile nodes, referred to as agents. Because of the unreliability and short range of wireless communications in an indoor environment we suggest that agents communicate indirectly with each other by tagging the environment. Agents have no prior knowledge of the terrain map, but are able to coordinate in order to explore a variety of terrains with different topological features. In our experimental evaluation, we show that Brick&Mortar significantly outperforms the competing algorithms, namely ants and multiple depth first search, in terms of exploration time. The observed performance benefits suggest that our algorithm is suitable for safety-critical applications that require rapid area coverage for real-time event detection and response.",
Incorporating Training Errors for Large Margin HMMS Under Semi-Definite Programming Framework,"In this paper, we study how to incorporate training errors in large margin estimation (LME) under semi-definite programming (SDP) framework. Like soft-margin SVM, we propose to optimize a new objective function which linearly combines the minimum margin among positive tokens and an average error function of all negative tokens. The new method is named as soft-LME. It is shown the new soft-LME problem can still be converted into an SDP problem if we properly define the average error function of all negative tokens based on their discriminative functions. Some preliminary results on TIDIGITS show that the soft-LML/SDP method yields modest performance gain when training error rates are significant. Moreover, it is also shown that the soft-LML/SDP can achieve much faster convergence for all cases which we have investigated.","Hidden Markov models,
Optimization methods,
Support vector machines,
Error analysis,
Speech recognition,
Computer errors,
Performance gain,
Convergence,
Databases,
Computer science"
A Distributed Energy Balance Clustering Protocol for Heterogeneous Wireless Sensor Networks,"Sensor networks have a wide range of potential, practical and useful applications. However, there are issues that need to be addressed for efficient operation of sensor network systems in real applications. To extend the lifetime of a sensor network, clustering algorithm is a kind of key technique used to reduce energy consumption, which can increase network scalability and lifetime. In this paper, we proposed a distributed energy balance clustering (DEBC) protocol for heterogeneous network. Cluster heads are selected by a probability depending on the ratio between remaining energy of node and the average energy of network. The high initial and remaining energy nodes have more chances to be the cluster heads than the low energy nodes. Simulation result shows that our protocol provides longer lifetime than the existing clustering protocol and uses the best of energy in heterogeneous network.","Wireless application protocol,
Wireless sensor networks,
Clustering algorithms,
Energy consumption,
Batteries,
Application software,
Scalability,
Base stations,
Computer science,
Educational institutions"
"Requirements and design principles for multisimulation with multiresolution, multistage multimodels","The significance of simulation modeling at multiple levels, scales, and perspectives is well recognized. However, existing proposals for developing such models are often application specific. The position advocated in this paper is that generic design principles for specifying and realizing multiresolution, multistage models are still lacking. Requirements for simulation environments that facilitate multiresolution multistage model specification are introduced. A multimodel specification formalism based on graph of models is suggested along with design precepts to enable flexible dynamic model updating. The notion of multisimulation is introduced to enable exploratory simulation using various types of multimodels.","Computational modeling,
Runtime,
Proposals,
Taxonomy,
Uncertainty,
Symbiosis,
Computer science,
Software engineering,
Differential equations,
Costs"
An Interference-Aware Channel Assignment Scheme for Wireless Mesh Networks,"Multichannel communication in a wireless mesh network with routers having multiple radio interfaces significantly enhances the network capacity. Efficient channel assignment and routing is critical for realization of optimal throughput in such networks. In this paper, we investigate the problem of finding the largest number of links that can be activated simultaneously in a wireless mesh network subject to interference, radio and connectivity constraints. Our goal is to activate all such links and we present an interference aware channel assignment algorithm that realizes this goal. We show that the Link Interference Graph created by utilizing a frequently used interference model gives rise to a special class of graphs, known as overlapping double-disk (ODD) graphs. We prove that the Maximum Independent Set computation problem is NP-complete for this special class of graphs. We provide a Polynomial Time Approximation Scheme (PTAS) for computation of the Maximum Independent Set of an ODD graph. We use this PTAS to develop a channel assignment algorithm for a multiradio multichannel Wireless Mesh Network. We evaluate the performance of our channel assignment algorithm by comparing it with the optimal solution obtained by solving an integer linear program. Experimental results demonstrate that our channel assignment algorithm produces near optimal solution in almost all instances of the problem.","Wireless mesh networks,
Telecommunication traffic,
Throughput,
Interference constraints,
Routing,
Polynomials,
Traffic control,
Communications Society,
Computer science,
Computer networks"
Parallel XML Parsing Using Meta-DFAs,"By leveraging the growing prevalence of multicore CPUs, parallel XML parsing(PXP) can significantly improve the performance of XML, enhancing its suitability for scientific data which is often dominated by floating-point numbers. One approach is to divide the XML document into equal-sized chunks, and parse each chunk in parallel. XML parsing is inherently sequential, however, because the state of an XML parser when reading a given character depends potentially on all preceding characters. In previous work, we addressed this by using a fast preparsing scan to build an outline of the document which we called the skeleton. The skeleton is then used to guide the parallel full parse. The preparse is a sequential phase that limits scalability, however, and so in this paper, we show how the preparse itself can be parallelized using a mechanism we call a meta-DFA. For each state q of the original preparser the meta-DFA incorporates a complete copy of the preparser state machine as a sub-DFA which starts in state q. The meta-DFA thus runs multiple instances of the preparser simultaneously when parsing a chunk, with each possible preparser state at the beginning of a chunk represented by an instance. By pursuing all possibilities simultaneously, the meta-DFA allows each chunk to be preparsed independently in parallel. The parallel full parse following the preparse is performed using libxml2, and outputs DOM trees that are fully compatible with existing applications that use libxml2. Our implementation scales well on a 30 CPU Sun E6500 machine.","XML,
Skeleton,
Parallel processing,
Computer science,
Scalability,
Application software,
Hardware,
Grid computing,
Multicore processing,
Sun"
Domain-Specific Hybrid FPGA: Architecture and Floating Point Applications,"This paper presents a novel architecture for domain-specific FPGA devices. This architecture can be optimised for both speed and density by exploiting domain-specific information to produce efficient reconfigurable logic with multiple granularity. In the reconfigurable logic, general-purpose fine-grained units are used for implementing control logic and bit-oriented operations, while domain-specific coarse-grained units and heterogeneous blocks are used for implementing datapaths; the precise amount of each type of resources can be customised to suit specific application domains. Issues and challenges associated with the design flow and the architecture modelling are addressed. Examples of the proposed architecture for speeding up floating point applications are illustrated. Current results indicate that the proposed architecture can achieve 2.5 times improvement in speed and 18 times reduction in area on average, when compared with traditional FPGA devices on selected floating point benchmark circuits.",
Design of early warning flood detection systems for developing countries,"In developing countries, flooding due to natural disasters such as hurricanes and earthquakes results in massive loss of life and property. Warning communities of the incoming flood provides an effective solution to this by giving people sufficient time to evacuate and protect their property. However, the range of early warning system solutions introduces a tangle of conflicting requirements including cost and reliability, and creates several interesting problems from factors as diverse as technological, social, and political. The complexity of these systems and need for autonomy within the context of a developing country while remaining maintainable and accessible by non-technical personnel provides a challenge not often solved within developed countries, much less the developing. After describing this problem, the paper discusses a proposed solution for the problem, initial experiments in implementing the solution, and lessons learned through that work.","Floods,
Hurricanes,
Rivers,
Alarm systems,
Costs,
Intelligent sensors,
Sensor systems,
Storms,
Computer science,
Computational and artificial intelligence"
Modeling Appearances with Low-Rank SVM,"Several authors have noticed that the common representation of images as vectors is sub-optimal. The process of vectorization eliminates spatial relations between some of the nearby image measurements and produces a vector of a dimension which is the product of the measurements' dimensions. It seems that images may be better represented when taking into account their structure as a 2D (or multi-D) array. Our work bears similarities to recent work such as 2DPCA or Coupled Subspace Analysis in that we treat images as 2D arrays. The main difference, however, is that unlike previous work which separated representation from the discriminative learning stage, we achieve both by the same method. Our framework, ""low-rank separators "", studies the use of a separating hyperplane which are constrained to have the structure of low-rank matrices. We first prove that the low-rank constraint provides preferable generalization properties. We then define two ""low-rank SVM problems"" and propose algorithms to solve these. Finally, we provide supporting experimental evidence for the framework.","Support vector machines,
Pixel,
Principal component analysis,
Matrix decomposition,
Computer science,
Image analysis,
Particle separators,
Multidimensional systems,
Algorithm design and analysis,
Concatenated codes"
Analysis of Particle Methods for Simultaneous Robot Localization and Mapping and a New Algorithm: Marginal-SLAM,"This paper presents a new particle method, with stochastic parameter estimation, to solve the SLAM problem. The underlying algorithm is rooted on a solid probabilistic foundation and is guaranteed to converge asymptotically, unlike many existing popular approaches. Moreover, it is efficient in storage and computation. The new algorithm carries out filtering only in the marginal filtering space, thereby allowing for the recursive computation of low variance estimates of the map. The paper provides mathematical arguments and empirical evidence to substantiate the fact that the new method represents an improvement over the existing particle filtering approaches for SLAM, which work on the joint path state space.",
A Coherent Grid Traversal Approach to Visualizing Particle-Based Simulation Data,We present an approach to visualizing particle-based simulation data using interactive ray tracing and describe an algorithmic enhancement that exploits the properties of these data sets to provide highly interactive performance and reduced storage requirements. This algorithm for fast packet-based ray tracing of multilevel grids enables the interactive visualization of large time-varying data sets with millions of particles and incorporates advanced features like soft shadows. We compare the performance of our approach with two recent particle visualization systems: one based on an optimized single ray grid traversal algorithm and the other on programmable graphics hardware. This comparison demonstrates that the new algorithm offers an attractive alternative for interactive particle visualization.,"Data visualization,
Computational modeling,
Ray tracing,
Biological system modeling,
Computer simulation,
Graphics,
Hardware,
Geometry,
Isosurfaces,
Rendering (computer graphics)"
Real-Time Divisible Load Scheduling with Different Processor Available Times,"Providing QoS and performance guarantees to arbitrarily divisible loads has become a significant problem for many cluster-based research computing facilities. While progress is being made in scheduling arbitrarily divisible loads, some of proposed approaches may cause inserted idle times (IITs) that are detrimental to system performance. In this paper we propose a new approach that utilizes IITs and thus enhances the system performance. The novelty of our approach is that, to simplify the analysis, a homogenous system with IITs is transformed to an equivalent heterogeneous system, and that our algorithms can schedule real-time divisible loads with different processor available times. Intensive simulations show that the new approach outperforms the previous approach in all configurations. We also compare the performance of our algorithm to the current practice of manually splitting workloads by users. Simulation results validate the advantages of our approach.","Processor scheduling,
Scheduling algorithm,
Large Hadron Collider,
System performance,
Real time systems,
Collision mitigation,
Computer science,
Algorithm design and analysis,
Partitioning algorithms,
Bioinformatics"
Impact of Task Migration on Streaming Multimedia for Embedded Multiprocessors: A Quantitative Evaluation,"Dynamic task mapping solutions based on task migration has been recently explored to perform run-time reallocation of task to maximize performance and optimize energy consumption in MPSoCs. Even if task migration can provide high flexibility, its overhead must be carefully evaluated when applied to soft real-time applications. In fact, these applications impose deadlines that may be missed during the migration process. In this paper we first present a middleware infrastructure supporting dynamic task allocation for NUMA architectures. Then we perform an extensive characterization of its impact on multimedia soft realtime applications using a software FM Radio benchmark.",
Business Process-Based Regulation Compliance: The Case of the Sarbanes-Oxley Act,"Balance Sheets and Annual Financial Reports play a major role in determining the public worth of any company. In the wake of corporate scandals such as Enron and WorldCom, the US and other countries passed legislation governing reporting processes. The Sarbanes Oxley Act of 2002 (hereafter SOX) requires US national securities exchange and US national security associations not to list any securities of any issuer that is not in compliance with the act. In this paper, we present a business process-based solution to the SOX compliance problem and offer evidence that such a solution is feasible through an industrial case study. The proposed solution aims to support SOX reporting requirements based on core business processes and a continuous improvement of the company's adopted business processes. This means that the solution integrates SOX-related tasks into the ""daily work"" of a company, rather than achieve compliance on a project basis.","Companies,
National security,
Knowledge engineering,
Legislation,
Continuous improvement,
Stock markets,
Computer science,
Information technology,
Government,
Standards development"
Crazy Car Race Contest: Multicourse Design Curricula in Embedded System Design,"This paper reports on recent initiatives aimed at significantly enhancing the teaching of engineering design at the Westcoast University of Applied Sciences. A good design experience offers opportunities for learning to synthesize, solve, and utilize a given problem. Design problems should be open-ended, moderately difficult, and common to all groups. The outcome of creating a multicourse design project, with the intention of attending an international design contest (Crazy Car Race), is described. Students with different design experience have to work together to build a racing car which navigates a given route autonomously. The course structure, its placement in the regular curriculum, and the student and instructor's evaluation results are presented and discussed","mechanical engineering,
automobiles,
computer aided instruction,
design engineering,
educational courses,
electrical engineering education,
embedded systems"
Integrating Appearance and Motion Cues for Simultaneous Detection and Segmentation of Pedestrians,"We present a unified method for simultaneously acquiring both the location and the silhouette shape of people in outdoor scenes. The proposed algorithm integrates top-down and bottom-up processes in a balanced manner, employing both appearance and motion cues at different perceptual levels. Without requiring manually segmented training data, the algorithm employs a simple top-down procedure to capture the high-level cue of object familiarity. Motivated by regularities in the shape and motion characteristics of humans, interactions among low-level contour features are exploited to extract mid-level perceptual cues such as smooth continuation, common fate, and closure. A Markov random field formulation is presented that effectively combines the various cues from the top-down and bottom-up processes. The algorithm is extensively evaluated on static and moving pedestrian datasets for both detection and segmentation.","Motion detection,
Shape,
Data mining,
Layout,
Training data,
Markov random fields,
Image segmentation,
Object detection,
Feature extraction,
Computer science"
"The Future of Programming Environments: Integration, Synergy, and Assistance","Modern programming environments foster the integration of automated, extensible, and reusable tools. New tools can thus leverage the available functionality and collect data from program and process. The synergy of both will allow the automation of current empirical approaches. This leads to automated assistance in all development decisions for programmers and managers alike: ""For this task, you should collaborate with Joe, because it will likely require risky work on the mailbox class"".","Programming environments,
Programming profession,
Computer science,
History,
Manufacturing,
Production,
Collaborative software,
Quality management,
Project management,
Books"
Patch-based Image Correlation with Rapid Filtering,"This paper describes a patch-based approach for rapid image correlation or template matching. By representing a template image with an ensemble of patches, the method is robust with respect to variations such as local appearance variation, partial occlusion, and scale changes. Rectangle filters are applied to each image patch for fast filtering based on the integral image representation. A new method is developed for feature dimension reduction by detecting the ""salient"" image structures given a single image. Experiments on a variety images show the success of the method in dealing with different variations in the test images. In terms of computation time, the approach is faster than traditional methods by up to two orders of magnitude and is at least three times faster than a fast implementation of normalized cross correlation.",
Seamless Continuity of Service across WLAN and WMAN Networks: Challenges and Performance Evaluation,"Future network environments will be heterogeneous and mobile terminals will have the opportunity to dynamically select among many different access technologies. Therefore, it is important to provide service continuity in case of vertical handoff when terminals change the access technology. Two important wireless access technologies are WLAN (wireless local access networks) and WMAN (wireless metropolitan access networks). In this paper, we address several challenges related to a seamless integration of those technologies. We highlight important aspects for designing a WLAN/WMAN interworking architecture and we address important quality of service (QoS) issues for such interworked systems like degree of QoS support provided by the technologies, QoS mapping and signalling for vertical handoff. By formulating several interworking scenarios, where WLAN users with ongoing voice, video and data sessions handoff to WMAN, we study QoS and performance issues and analyze feasibility of seamless session continuity through simulations.","Wireless LAN,
Quality of service,
Wireless networks,
Authentication,
Performance analysis,
Switches,
Computer science,
Laboratories,
Signal design,
Signal mapping"
Robust live unicast video streaming with rateless codes,"We consider live unicast video streaming over a packet erasure channel. To protect the transmitted data, previous solutions use forward error correction (FEC), where the channel code rate is fixed in advance according to an estimation of the packet loss rate. However, these solutions are inefficient under dynamic and unpredictable channel conditions because of the mismatch between the estimated packet loss rate and the actual one. We introduce a new approach based on rateless codes and receiver feedback. For every source block, the sender keeps on transmitting the encoded symbols until it receives an acknowledgment from the receiver indicating that the block was decoded successfully. Within this framework, we provide an efficient algorithm to minimize bandwidth usage while ensuring successful decoding subject to an upper bound on the packet loss rate. Experimental results showed that compared to traditional fixed-rate FEC, our scheme provides significant bandwidth savings for the same playback quality.",
Pre-Coordination Mechanism for Fast Handover in WiMAX Networks,"The most challenging research issue of investigating broadband wireless access (BWA) technologies is how to support mobility in WiMAX networks smoothly and seamlessly. It is essential that providing continuous services of multimedia streaming data when a mobile subscriber station (MSS) is across a boundary of a serving area to another one. Although the IEEE 802.16e standard is proposed to tackle this difficult problem, the disruption time (DT) of handover is still too long to overcome the maximum delay time of real-time services. To deal with this problem, this paper proposes a pre-coordination mechanism (PCM) for supporting fast handover in WiMAX networks. This goal is achieved by measuring the distance between the BS and the MSS and predicting the time of handover occurs, and thus pre-allocating available resources for handover usages. Simulation results show that the enhanced DT of handover can achieve approximately to 11 milliseconds. The proposed mechanism is fully compatible with the IEEE 802.16e standard.","WiMAX,
Streaming media,
Delay effects,
Phase change materials,
Time measurement,
Base stations,
Computer science,
Sprites (computer),
Councils,
Contracts"
Urban Traffic Control Based on Learning Agents,"The optimization of traffic light control systems is at the heart of work in traffic management. Many of the solutions considered to design efficient traffic signal patterns rely on controllers that use pre-timed stages. Such systems are unable to identify dynamic changes in the local traffic flow and thus cannot adapt to new traffic conditions. An alternative, novel approach proposed by computer scientists in order to design adaptive traffic light controllers relies on the use of intelligent agents. The idea is to let autonomous entities, named agents, learn an optimal behavior by interacting directly in the system. By using machine learning algorithms based on the attribution of rewards according to the results of the actions selected by the agents, we can obtain a control policy that tries to optimize the urban traffic flow. In this paper, we explain how we designed an intelligent agent that learns a traffic light control policy. We also compare this policy with results from an optimal pre-timed controller.","Traffic control,
Lighting control,
Intelligent agent,
Control systems,
Heart,
Signal design,
Programmable control,
Adaptive control,
Machine learning algorithms,
Optimal control"
Waveform design for radar-embedded communications,"This paper considers the embedding of a covert communication signal amongst radar backscatter by means of a tag/transponder that lies within the illuminated area of the radar. Past approaches have operated on an inter-pulse basis whereby a communication symbol/identifier is relayed to an intended receiver by imparting a Doppler-like phase-shift to each of a successive series of incident radar pulses. In contrast, the approach proposed in this paper operates on an intra-pulse basis whereby the incident radar waveform at the tag/transponder is ""re-modulated"" into one of a set of different waveforms each representing a different communication symbol. The particular design issues for these re-modulated waveforms are discussed and three general design methods are proposed. The effectiveness of the different methods is assessed in terms of the probability of communication error as a function of the respective powers of the embedded communication signal, the masking radar backscatter, and noise. The relative ""covertness"" of the resulting waveforms is also discussed.","Backscatter,
Transponders,
Radar imaging,
Lighting,
Signal design,
Relays,
Data mining,
Signal processing,
Synthetic aperture radar,
Pulse modulation"
Privacy-Preserving Data Mining on Moving Object Trajectories,"The popularity of embedded positioning technologies in mobile devices and the development of mobile communication technology have paved the way for powerful location-based services (LBSs). To make LBSs useful and user- friendly, heavy use is made of context information, including patterns in user location data which are extracted by data mining methods. However, there is a potential conflict of interest: the data mining methods want as precise data as possible, while the users want to protect their privacy by not disclosing their exact movements. This paper aims to resolve this conflict by proposing a general framework that allows user location data to be anonymized, thus preserving privacy, while still allowing interesting patterns to be discovered. The framework allows users to specify individual desired levels of privacy that the data collection and mining system will then meet. Privacy-preserving methods are proposed for a core data mining task, namely finding dense spatio-temporal regions. An extensive set of experiments evaluate the methods, comparing them to their non- privacy-preserving equivalents. The experiments show that the framework still allows most patterns to be found, even when privacy is preserved.","Data mining,
Data privacy,
Databases,
Middleware,
Protection,
Mobile communication,
Communications technology,
Context,
Computer science,
Context-aware services"
Detecting Phantom Nodes in Wireless Sensor Networks,"In an adversarial environment, various kinds of security attacks become possible if malicious nodes could claim fake locations that are different from where they are physically located. In this paper, we propose a secure localization mechanism that detects the existence of these nodes, termed as phantom nodes, without relying on any trusted entities, an approach significantly different from the existing ones. The proposed mechanism enjoys a set of nice features. First, it does not have any central point of attack. All nodes play the role of verifier, by generating local map, i.e. a view constructed based on ranging information from its neighbors. Second, this distributed and localized construction results in quite strong results: even when the number of phantom nodes is greater than that of honest nodes, we can Alter out most phantom nodes. Our analysis and simulations under realistic noisy settings demonstrate our scheme is effective in the presence of a large number of phantom nodes.","Imaging phantoms,
Wireless sensor networks,
Peer to peer computing,
Filters,
Surveillance,
Routing,
Communications Society,
Helium,
Computer science,
Computer security"
Phighting the Phisher: Using Web Bugs and Honeytokens to Investigate the Source of Phishing Attacks,"This paper presents a summary of research findings for a new reacitve phishing investigative technique using Web bugs and honeytokens. Phishing has become a rampant problem in today 's society and has cost financial institutions millions of dollars per year. Today's reactive techniques against phishing usually involve methods that simply minimize the damage rather than attempting to actually track down a phisher. Our research objective is to track down a phisher to the IP address of the phisher's workstation rather than innocent machines used as intermediaries. By using Web bugs and honeytokens on the fake Web site forms the phisher presents, one can log accesses to the honeytokens by the phisher when the attacker views the results of the forms. Research results to date are presented in this paper","Computer bugs,
Costs,
Electronic mail,
Pixel,
Computer science,
Computer security,
Protection,
Law enforcement,
Computer crime,
Workstations"
A personalized robot companion? - The role of individual differences on spatial preferences in HRI scenarios,"This study investigated the relationship between individual differences and proxemic behaviour in an HRI setting involving a robot approaching a person. In total 33 participants took part in three different scenarios; verbal interaction, physical interaction and no interaction. Participant control over the robot, and approach direction was also varied. Measurements of the preferred robot approach distance was obtained, and analysed along with the participants' demographic and personality data. The results indicate differences in approach direction preferences based on gender. Also, results show that the participants' personality traits of extraversion and conscientiousness are associated with changes in approach distance preferences according to robot autonomy. The results are discussed in light of relevant literature from the social sciences.","Human robot interaction,
Navigation,
Robot sensing systems,
Adaptive systems,
Computer science,
Robot control,
Particle measurements,
Demography,
Employment,
Books"
Energy-Aware Scheduling for Real-Time Multiprocessor Systems with Uncertain Task Execution Time,"This paper presents an energy-aware method to schedule multiple real-time tasks in multiprocessor systems that support dynamic voltage scaling (DVS). The key difference from existing approaches is that we consider the probabilistic distributions of the tasks' execution time to partition the workload for better energy reduction. We analyze the problem of energy-aware scheduling for multiprocessor with probabilistic workload information and derive its mathematical formulation. As the problem is NP-hard, we present a polynomial-time heuristic method to transform the problem into a probability-based load balancing problem that is then solved with worst-fit decreasing bin-packing heuristic. Simulation results with synthetic, multimedia, and stereo- vision tasks show that our method saves significantly more energy than existing methods.",
Building a Self-Healing Operating System,"User applications and data in volatile memory are usually lost when an operating system crashes because of errors caused by either hardware or software faults. This is because most operating systems are designed to stop working when some internal errors are detected despite the possibility that user data and applications might still be intact and recoverable. Techniques like exception handling, code reloading, operating system component isolation, micro-rebooting, automatic system service restarts, watchdog timer based recovery and transactional components can be applied to attempt self-healing of an operating system from a wide variety of errors. Fault injection experiments show that these techniques can be used to continue running user applications after transparently recovering the operating system in a large percentage of cases. In cases where transparent recovery is not possible, individual process recovery can be attempted as a last resort.",
Approximate Nearest Subspace Search with Applications to Pattern Recognition,"Linear and affine subspaces are commonly used to describe appearance of objects under different lighting, viewpoint, articulation, and identity. A natural problem arising from their use is - given a query image portion represented as a point in some high dimensional space - find a subspace near to the query. This paper presents an efficient solution to the approximate nearest subspace problem for both linear and affine subspaces. Our method is based on a simple reduction to the problem of nearest point search, and can thus employ tree based search or locality sensitive hashing to find a near subspace. Further speedup may be achieved by using random projections to lower the dimensionality of the problem. We provide theoretical proofs of correctness and error bounds of our construction and demonstrate its capabilities on synthetic and real data. Our experiments demonstrate that an approximate nearest subspace can be located significantly faster than the exact nearest subspace, while at the same time it can find better matches compared to a similar search on points, in the presence of variations due to viewpoint, lighting etc.","Pattern recognition,
Nearest neighbor searches,
Space technology,
Error correction,
Computer vision,
Application software,
Image databases,
Search problems,
Art,
Machine vision"
Power-aware mapping for reconfigurable NoC architectures,"A core mapping method for reconfigurable network-on-chip (NoC) architectures is presented in this paper. In most of the existing methods, mapping is carried out based on the traffic characteristics of a single application. However, several different applications are implemented and integrated in the modern complex system-on-chips which should be considered by mapping methods. In the proposed method, the reconfiguration (which is achieved by embedding programmable switches between routers of a mesh-based NoC) allows us to dynamically change the network topology in order to adapt it with the running application and optimize the power and performance metrics. The presented network architecture can be configured as an application- specific topology, while it still holds the benefits of the regular NoC topologies such as modularity and predictable electrical properties. The experimental results show that this method can effectively adapt the NoC to the running application and improve the power consumption and performance of the system.","network-on-chip,
network topology"
Visualizing Large-Scale Uncertainty in Astrophysical Data,"Visualization of uncertainty or error in astrophysical data is seldom available in simulations of astronomical phenomena, and yet almost all rendered attributes possess some degree of uncertainty due to observational error. Uncertainties associated with spatial location typically vary significantly with scale and thus introduce further complexity in the interpretation of a given visualization. This paper introduces effective techniques for visualizing uncertainty in large-scale virtual astrophysical environments. Building upon our previous transparently scalable visualization architecture, we develop tools that enhance the perception and comprehension of uncertainty across wide scale ranges. Our methods include a unified color-coding scheme for representing log-scale distances and percentage errors, an ellipsoid model to represent positional uncertainty, an ellipsoid envelope model to expose trajectory uncertainty, and a magic-glass design supporting the selection of ranges of log-scale distance and uncertainty parameters, as well as an overview mode and a scalable WIM tool for exposing the magnitudes of spatial context and uncertainty.","Data visualization,
Large-scale systems,
Uncertainty,
Extraterrestrial measurements,
Earth,
Ellipsoids,
Context modeling,
Buildings,
Astronomy,
Data engineering"
A java distributed acquisition system for PET and SPECT imaging,"The Detector and Imaging Group at Jefferson Lab is developing various compact gamma cameras for clinical and preclinical systems. Both PET and SPECT systems are under development. To facilitate that development we have designed a highly flexible Java data acquisition tool that helps to minimize software induced dead-time while maintaining the highest possible data rates from our in house built ADCs. This tool interfaces with FPGA based multi-channel ADCs which our group has developed. Using this tool we are able to process data from a number of different detector types such as the SPECT Awake Animal Imaging system at Johns Hopkins University; as well as our PET systems at West Virginia University (WVU), the University of Florida (UF), and the National Technical University of Athens. Because of the inherent flexibility of our processing software, we are also able adjust detector readout parameters during operation to provide the best possible data presentation and calibration parameters. We are using the readout capability of this software with Kmax to provide a user friendly display of detector outputs such as raw images and individual channel spectra. To accommodate the high rate nature of PET detector systems we designed this software to be scalable across an Ethernet network as well as for multi-CPU computer systems, and it also has the capability to synchronize many ADCs connected to multiple computers. We have incorporated this distributed design into the six computer PEM (Positron Emission Mammography)/PET system at WVU, and the three computer cardiac PET detector at UF.","Java,
Positron emission tomography,
Optical imaging,
Computer networks,
Distributed computing,
Gamma ray detection,
Gamma ray detectors,
Cameras,
Data acquisition,
Software maintenance"
Optimistic Parallelization of Floating-Point Accumulation,"Floating-point arithmetic is notoriously nonassociative due to the limited precision representation which demands intermediate values be rounded to fit in the available precision. The resulting cyclic dependency in floating-point accumulation inhibits parallelization of the computation, including efficient use of pipelining. In practice, however, we observe that floating-point operations are ""mostly"" associative. This observation can be exploited to parallelize floating-point accumulation using a form of optimistic concurrency. In this scheme, we first compute an optimistic associative approximation to the sum and then relax the computation by iteratively propagating errors until the correct sum is obtained. We map this computation to a network of 16 statically-scheduled, pipelined, double-precision floating-point adders on the Virtex-4 LX160 (-12) device where each floating-point adder runs at 296 MHz and has a pipeline depth of 10. On this 16 PE design, we demonstrate an average speedup of 6times with randomly generated data and 3-7times with summations extracted from Conjugate Gradient benchmarks.","Floating-point arithmetic,
Pipeline processing,
Concurrent computing,
Hardware,
Delay,
Computer science,
Systems engineering and theory,
Error correction,
Computer networks,
Data mining"
Formal Software Analysis Emerging Trends in Software Model Checking,"The study of methodologies and techniques to produce correct software has been active for four decades. During this period, researchers have developed and investigated a wide variety of approaches, but techniques based on mathematical modeling of program behavior have been a particular focus since they offer the promise of both finding errors and assuring important program properties. The past fifteen years have seen a marked and accelerating shift towards algorithmic formal reasoning about program behavior - we refer to these as formal software analysis. In this paper, we define formal software analyses as having several important properties that distinguish them from other forms of software analysis. We describe three foundational formal software analyses, but focus on the adaptation of model checking to reason about software. We review emerging trends in software model checking and identify future directions that promise to significantly improve its cost-effectiveness.",
Software project economics: a roadmap,"The objective of this paper is to consider research progress in the field of software project economics with a view to identifying important challenges and promising research directions. I argue that this is an important sub-discipline since this will underpin any cost-benefit analysis used to justify the resourcing, or otherwise, of a software project. To accomplish this I conducted a bibliometric analysis of peer reviewed research articles to identify major areas of activity. My results indicate that the primary goal of more accurate cost prediction systems remains largely unachieved. However, there are a number of new and promising avenues of research including: how we can combine results from primary studies, integration of multiple predictions and applying greater emphasis upon the human aspects of prediction tasks. I conclude that the field is likely to remain very challenging due to the people-centric nature of software engineering, since it is in essence a design task. Nevertheless the need for good economic models will grow rather than diminish as software becomes increasingly ubiquitous.","Software engineering,
Costs,
Business,
Economic forecasting,
Productivity,
Bibliometrics,
Software systems,
Scheduling,
Computer science,
Software measurement"
Limits of Learning-Based Superresolution Algorithms,"Learning-based superresolution (SR) are popular SR techniques that use application dependent priors to infer the missing details in low resolution images (LRIs). However, their performance still deteriorates quickly when the magnification factor is moderately large. This leads us to an important problem: ""Do limits of learning-based SR algorithms exist?"" In this paper, we attempt to shed some light on this problem when the SR algorithms are designed for general natural images (GNIs). We first define an expected risk for the SR algorithms that is based on the root mean squared error between the superresolved images and the ground truth images. Then utilizing the statistics of GNIs, we derive a closed form estimate of the lower bound of the expected risk. The lower bound can be computed by sampling real images. By computing the curve of the lower bound w.r.t. the magnification factor, we can estimate the limits of learning-based SR algorithms, at which the lower bound of expected risk exceeds a relatively large threshold. We also investigate the sufficient number of samples to guarantee an accurate estimation of the lower bound.",
REAR: Reliable Energy Aware Routing Protocol for Wireless Sensor Networks,"In wireless sensor networks, micro sensor nodes dispersed in real environmental field have a constraint energy capacity, so energy-efficient mechanism for wireless communication on each sensor node is so crucial. Specially, the jobs sending and processing sensing data information from on sensor node to the others are more majority parts than merely sensing some events. Thus, energy-efficient routing protocol in wireless sensor networks is necessary for increasing the network lifetime and is also influenced by many challenging factors in terms of energy, processing, and storage capacities. In this paper, we designed and implemented a reliable energy aware routing (REAR) protocol for wireless sensor networks and evaluated the performance of REAR by comparing with existing routing protocols. REAR considers residual energy capacity of each sensor node in establishing routing paths and supports multi-path routing protocol for reliable data transmission. Furthermore, REAR allows each sensor node to confirm success of data transmission to other sensor nodes by supporting the DATA-ACK oriented packet transmission. Finally, the performance evaluation results show that REAR provides energy-efficiency and reliability related to wireless communication in wireless sensor networks.","Routing protocols,
Wireless sensor networks,
Capacitive sensors,
Energy efficiency,
Wireless communication,
Telecommunication network reliability,
Data communication,
Remote monitoring,
Sensor phenomena and characterization,
Chemical technology"
Speed Estimation From a Tri-axial Accelerometer Using Neural Networks,"We propose a speed estimation method with human body accelerations measured on the chest by a tri-axial accelerometer. To estimate the speed we segmented the acceleration signal into strides measuring stride time, and applied two neural networks into the patterns parameterized from each stride calculating stride length. The first neural network determines whether the subject walks or runs, and the second neural network with different node interactions according to the subject's status estimates stride length. Walking or running speed is calculated with the estimated stride length divided by the measured stride time. The neural networks were trained by patterns obtained from 15 subjects and then validated by 2 untrained subjects' patterns. The result shows good agreement between actual and estimated speeds presenting the linear correlation coefficient r = 0.9874. We also applied the method to the real field and track data.","Accelerometers,
Neural networks,
Legged locomotion,
Velocity measurement,
Length measurement,
Acceleration,
Magnetic field measurement,
Belts,
Magnetic sensors,
Navigation"
Access Point Selection for Improving Throughput Fairness in Wireless LANs,"We investigate the problem of access point selection in wireless LANs based on the IEEE 802.11 standard, when a station is within the vicinity of more than one access points. According to the proposed approach, the selection is based on the packet transmission delay for each access point that a station can associate to, which is related to the throughput that the station can achieve. Important features of the approach is that it considers the contention-based nature of IEEE 802.11's MAC layer, it can be applied to the 802.11e standard which can support different classes in terms of the minimum contention window, and it can be implemented solely at the wireless stations, which passively monitor the activity of each access point's channel, without requiring modifications to the access points. Experiments show that the proposed approach can achieve significantly higher throughput fairness compared to other approaches, without a significant decrease of the aggregate throughput.","Throughput,
Wireless LAN,
Aggregates,
Wireless networks,
Monitoring,
Computer science,
Delay estimation,
Bandwidth,
Quality of service,
Telecommunication services"
The Performance Impact of Kernel Prefetching on Buffer Cache Replacement Algorithms,"A fundamental challenge in improving file system performance is to design effective block replacement algorithms to minimize buffer cache misses. Despite the well-known interactions between prefetching and caching, almost all buffer cache replacement algorithms have been proposed and studied comparatively, without taking into account file system prefetching, which exists in all modern operating systems. This paper shows that such kernel prefetching can have a significant impact on the relative performance in terms of the number of actual disk l/Os of many well-known replacement algorithms; it can not only narrow the performance gap but also change the relative performance benefits of different algorithms. Moreover, since prefetching can increase the number of blocks clustered for each disk I/O and, hence, the time to complete the I/O, the reduction in the number of disk l/Os may not translate into proportional reduction in the total I/O time. These results demonstrate the importance of buffer caching research taking file system prefetching into consideration and comparing the actual disk l/Os and the execution time under different replacement algorithms.","Prefetching,
File systems,
Linux,
Algorithm design and analysis,
Operating systems,
Optimized production technology,
Clustering algorithms"
Using Stereo Matching for 2-D Face Recognition Across Pose,"We propose using stereo matching for 2-D face recognition across pose. We match one 2-D query image to one 2-D gallery image without performing 3-D reconstruction. Then the cost of this matching is used to evaluate the similarity of the two images. We show that this cost is robust to pose variations. To illustrate this idea we built a face recognition system on top of a dynamic programming stereo matching algorithm. The method works well even when the epipolar lines we use do not exactly fit the viewpoints. We have tested our approach on the PIE dataset. In all the experiments, our method demonstrates effective performance compared with other algorithms.","Face recognition,
Cameras,
Costs,
Jacobian matrices,
Three dimensional displays,
Robustness,
Image databases,
Computer science,
Educational institutions,
Dynamic programming"
Structural Selectivity Estimation for XML Documents,"Estimating the selectivity of queries is a crucial problem in database systems. Virtually all database systems rely on the use of selectivity estimates to choose amongst the many possible execution plans for a particular query. In terms of XML databases, the problem of selectivity estimation of queries presents new challenges: many evaluation operators are possible, such as simple navigation, structural joins, or twig joins, and many different indexes are possible. A new synopsis for XML documents is introduced which can be effectively used to estimate the selectivity of complex path queries. The synopsis is based on a lossy compression of the document tree that underlies the XML document, and can be computed in one pass from the document. It has several advantages over existing approaches: (1) it allows one to estimate the selectivity of queries containing all XPath axes, including the order-sensitive ones, (2) the estimator returns a range within which the actual selectivity is guaranteed to lie, with the size of this range implicitly providing a confidence measure of the estimate, and (3) the synopsis can be incrementally updated to reflect changes in the XML database.","XML,
Australia,
Database systems,
Database languages,
Computer science,
Data engineering,
Navigation,
Q measurement,
Size measurement,
Memory"
Polyphase Implementation of Non-recursive Comb Decimators for Sigma-Delta A/D Converters,"In a sigma-delta analog to digital (A/D) converter, the most computationally intensive block is the decimation filter and its hardware implementation may require millions of transistors. Since these converters are now targeted for a portable application, a hardware efficient design is an implicit requirement. In this effect, this paper presents a computationally efficient polyphase implementation of non-recursive cascaded integrator comb (CIC) decimators for sigma-delta converters (SDCs). The SDCs are operating at high oversampling frequencies and hence require large sampling rate conversions. The filtering and rate reduction are performed in several stages to reduce hardware complexity and power dissipation. The CIC filters are widely adopted as the first stage of decimation due to its multiplier free structure. In this research, the performance of polyphase structure is compared with the CICs using recursive and non-recursive algorithms in terms of power, speed and area. This polyphase implementation offers high speed operation and low power consumption. The polyphase implementation of 4th order CIC filter with a decimation factor of '64' and input word length of '4-bits' offers about 70% and 37% of power saving compared to the corresponding recursive and non-recursive implementations respectively. The same polyphase CIC filter can operate about 7 times faster than the recursive and about 3.7 times faster than the non-recursive CIC filters.","Delta-sigma modulation,
Hardware,
Analog-digital conversion,
Analog computers,
Digital filters,
Frequency conversion,
Sampling methods,
Filtering,
Power dissipation,
Energy consumption"
Mobile Social Software: Facilitating Serendipity or Encouraging Homogeneity?,Mobile social systems can offer heterogeneous views of cities rather than encouraging users to socialize with people they already know and privileging consumption- and entertainment-based urban experiences.,"Cities and towns,
Art,
Social implications of technology,
Design methodology,
Space technology,
Cultural differences,
Mobile handsets,
System testing,
Human computer interaction,
Social network services"
Environmental Bisimulations for Higher-Order Languages,"Developing a theory of bisimulation in higher-order languages can be hard. Particularly challenging can be: (1) the proof of congruence, as well as enhancements of the bisimulation proof method with ""up-to context"" techniques, and (2) obtaining definitions and results that scale to languages with different features. To meet these challenges, we present environmental bisimulations, a form of bisimulation for higher-order languages, and its basic theory. We consider four representative calculi: pure lambda-calculi (call-by-name and call-by-value), call-by-value lambda-calculus with higher-order store, and then higher-order pi-calculus. In each case: we present the basic properties of environmental bisimilarity, including congruence; we show that it coincides with contextual equivalence; we develop some up-to techniques, including up-to context, as examples of possible enhancements of the associated bisimulation method. Unlike previous approaches (such as applicative bisimulations, logical relations, Sumii-Pierce-Koutavas-Wand), our method does not require induction/indices on evaluation derivation/steps (which may complicate the proofs of congruence, transitivity, and the combination with up-to techniques), or sophisticated methods such as Howe's for proving congruence. It also scales from the pure lambda-calculi to the richer calculi with simple congruence proofs.","Proposals,
Robustness,
Concrete,
Logic,
Computer science,
Code standards,
Impedance matching"
Data Processing Tasks in Wireless GI Endoscopy: Image-Based Capsule Localization & Navigation and Video Compression,"The paper addresses data processing support that is required in capsule gastrointestinal endoscopy. First, capsule position estimation method using standard MPEG-7 image features (descriptors) is discussed. The proposed approach makes use of vector quantization, principal component analysis and neural networks. Next, new algorithms dedicated for virtual colonoscopy (VC) human body inspection are described. The VC images can be registered with endoscopic ones and help in capsule localization and navigation. Finally, an original, low- complexity, efficient image compression method, based on integer-to-integer 4x4 DCT transform, is presented and experimentally verified.","Data processing,
Endoscopes,
Navigation,
Video compression,
Virtual colonoscopy,
Gastrointestinal tract,
MPEG 7 Standard,
Vector quantization,
Principal component analysis,
Neural networks"
An Empirical Evaluation of the MuJava Mutation Operators,"Mutation testing is used to assess the fault-finding effectiveness of a test suite. Information provided by mutation testing can also be used to guide the creation of additional valuable tests and/or to reveal faults in the implementation code. However, concerns about the time efficiency of mutation testing may prohibit its widespread, practical use. We conducted an empirical study using the MuClipse automated mutation testing plug-in for Eclipse on the back end of a small web-based application. The first objective of our study was to categorize the behavior of the mutants generated by selected mutation operators during successive attempts to kill the mutants. The results of this categorization can be used to inform developers in their mutant operator selection to improve the efficiency and effectiveness of their mutation testing. The second outcome of our study identified patterns in the implementation code that remained untested after attempting to kill all mutants.","Genetic mutations,
Java,
Computer industry,
Computer science,
Automatic testing,
Fault detection,
Medical services,
Application software,
Character generation,
Computer aided instruction"
A Specialized Processor Suitable for AdaBoost-Based Detection with Haar-like Features,"Robust and rapid object detection is one of the great challenges in the field of computer vision. This paper proposes a hardware architecture suitable for object detection by Viola and Jones based on an AdaBoost learning algorithm with Haar-like features as weak classifiers. Our architecture realizes rapid and robust detection with two major features: hybrid parallel execution and an image scaling method. The first exploits the cascade structure of classifiers, in which classifiers located near the beginning of the cascade are used more frequently than subsequent classifiers. We assign more resources to the former classifiers to execute in parallel than subsequent classifiers. This dramatically improves the total processing speed without a great increase in circuit area. The second feature is a method of scaling input images instead of scaling classifiers. This increases the efficiency of hardware implementation while retaining a high detection rate. In addition we implement the proposed architecture on a Virtex-5 FPGA to show that it achieves real-time object detection at 30 frames per second on VGA video.","Computer vision,
Object detection,
Hardware,
Field programmable gate arrays,
Computer architecture,
Robustness,
Face detection,
Real time systems,
Image edge detection,
Face recognition"
Improving Semantic Concept Detection and Retrieval using Contextual Estimates,"In this paper we introduce a novel contextual fusion method to improve the detection scores of semantic concepts in images and videos. Our method consists of three phases. For each individual concept, the prior probability of the concept is incorporated with detection score of an individual SVM detector. Then probabilistic estimates of the target concept are computed using all of the individual SVM detectors. Finally, these estimates are linearly combined using weights learned from the training set. This procedure is applied to each target concept individually. We show significant improvements to our detection scores on the TRECVID 2005 development set and LSCOM-Lite annotation set. We achieved on average +3.9% improvements in 29 out of 39 concepts.","Support vector machines,
Detectors,
Support vector machine classification,
Videos,
Graphical models,
Computer science,
Image retrieval,
Shape,
Information retrieval,
Couplings"
Improving Image Search with PHETCH,"Keyword-based image search engines are hindered by the lack of proper labeling for images in their indices. In many cases the labels do not agree with the contents of the image itself, since images are generally indexed by their filename and the surrounding text in the Web page. Another popular approach to image search, content based image retrieval, suffers from a gap between the available low level data and the semantic needs of user searches. To overcome these problems we suggest human annotation of images with natural language descriptions. To this end we present Phetch, an engaging multiplayer game that allows people to attach accurate explanatory text captions to arbitrary images on the Web. People play the game because it is fun, and as a side effect we collect valuable information that can be applied towards improving image retrieval. Furthermore, the game can also be used for other novel applications.","Image retrieval,
Humans,
Natural languages,
Search engines,
Content based retrieval,
Labeling,
Information retrieval,
Computer vision,
Computer science,
Knowledge acquisition"
Strong Compound-Risk Factors: Efficient Discovery Through Emerging Patterns and Contrast Sets,"Odds ratio (OR), relative risk (RR) (risk ratio), and absolute risk reduction (ARR) (risk difference) are biostatistics measurements that are widely used for identifying significant risk factors in dichotomous groups of subjects. In the past, they have often been used to assess simple risk factors. In this paper, we introduce the concept of compound-risk factors to broaden the applicability of these statistical tests for assessing factor interplays. We observe that compound-risk factors with a high risk ratio or a big risk difference have an one-to-one correspondence to strong emerging patterns or strong contrast sets-two types of patterns that have been extensively studied in the data mining field. Such a relationship has been unknown to researchers in the past, and efficient algorithms for discovering strong compound-risk factors have been lacking. In this paper, we propose a theoretical framework and a new algorithm that unify the discovery of compound- risk factors that have a strong OR, risk ratio, or a risk difference. Our method guarantees that all patterns meeting a certain test threshold can be efficiently discovered. Our contribution thus represents the first of its kind in linking the risk ratios and ORs to pattern mining algorithms, making it possible to find compound- risk factors in large-scale data sets. In addition, we show that using compound-risk factors can improve classification accuracy in probabilistic learning algorithms on several disease data sets, because these compound-risk factors capture the interdependency between important data attributes.","Risk management,
Testing,
Data mining,
Biomedical measurements,
Isolation technology,
Data analysis,
Joining processes,
Large-scale systems,
Diseases,
Computer science"
0-60 GHz in four years: 60 GHz RF in digital CMOS,"In early 2001, most researchers were focusing on the spectrum from 1-5 GHz, with a few isolated research groups pushing circuits up to 24 GHz. The suggestion to exploit 60 GHz with CMOS seemed comic to some but the availability of nearly universal unlicensed spectrum was the obvious motivation. The Berkeley Wireless Research Center has succeeded in developing an extended BSIM3 model, a library of active and passive devices, and demonstrated the world fastest CMOS amplifier at 104 GHz.","CMOS integrated circuits,
Gain,
Noise,
Transmission line measurements,
Integrated circuit modeling,
Wireless communication,
Arrays"
Comparing Interpersonal Interactions with a Virtual Human to Those with a Real Human,"This paper provides key insights into the construction and evaluation of interpersonal simulators-systems that enable interpersonal interaction with virtual humans. Using an interpersonal simulator, two studies were conducted that compare interactions with a virtual human to interactions with a similar real human. The specific interpersonal scenario employed was that of a medical interview. Medical students interacted with either a virtual human simulating appendicitis or a real human pretending to have the same symptoms. In Study I (n = 24), medical students elicited the same information from the virtual and real human, indicating that the content of the virtual and real interactions were similar. However, participants appeared less engaged and insincere with the virtual human. These behavioral differences likely stemmed from the virtual human's limited expressive behavior. Study II (n = 58) explored participant behavior using new measures. Nonverbal behavior appeared to communicate lower interest and a poorer attitude toward the virtual human. Some subjective measures of participant behavior yielded contradictory results, highlighting the need for objective, physically-based measures in future studies.","Medical simulation,
Particle measurements,
Educational institutions,
Virtual reality,
Computational modeling,
Aerospace simulation,
Computer science,
Anthropometry,
Human computer interaction,
User interfaces"
Correction published 28 March 2008: Traveling waves on two- and three-dimensional periodic arrays of lossless scatterers,"The kd–βd (dispersion) equations are found for traveling waves on two- and three-dimensional infinite periodic arrays of small lossless acoustic monopoles, electric or magnetic dipoles, and magnetodielectric spheres. Using Floquet mode expansions and then expressions for the rapid summation of Schlömilch series, prohibitively slowly convergent summations are converted to forms that can be used for the efficient calculation of the kd–βd equations. Computer programs have been written to obtain the kd–βd diagrams for all the arrays treated, and representative numerical results are presented and discussed. Expressions, more accurate than the Clausius-Mossotti relations, are obtained for the effective or bulk permittivity and permeability of the arrays utilizing quantities readily available from the solutions of the kd–βd equations. Exact computable expressions for the fields of three-dimensional lossless or lossy magnetodielectric sphere arrays that are finite in the direction of the array axis, illuminated by a plane wave parallel to the array axis, are obtained from the analyses performed to obtain the kd–βd curves for the infinite arrays.",
Distributed optimal control of lighting based on stochastic hill climbing method with variable neighborhood,"In this research, a smart lighting system based on a new autonomous distributed control method was developed to control lighting using illuminance sensors. By using infrared ray communication, one illuminance sensor sends a luminance control directive to several lighting fixtures located nearby. The luminance is made to change randomly within a fixed range to optimize the illuminance using the stochastic hill climbing method. The result of operational experiments using illuminance sensors that were preset to the target illuminance showed that the brightness at specified locations approached the target illuminance that the illuminance sensors were set to. The electrical power consumed by the lighting system was also minimized. Moreover, in comparison with lighting driven autonomous distributed controls of smart lighting systems, better results were obtained, indicating that the method is an effective new distributed control method.","Optimal control,
Stochastic processes,
Intelligent sensors,
Distributed control,
Lighting control,
Communication system control,
Infrared sensors,
Control systems,
Sensor systems,
Optical fiber communication"
A DOE Set for Normalization-Based Extraction of Fill Impact on Capacitances,"Metal fills, which are used to reduce metal thickness variations due to chemical-mechanical polishing (CMP), increase the capacitances in a circuit. Although current extraction tools are accurate in handling grounded fills and regular interconnects, for floating fills, these tools are based on certain approximations, such as assuming that floating fills are grounded or each fill is merged with neighboring ones. To reduce such inaccuracies, the authors provide a design of experiments (DOE), which will be used in addition to what is available in the extraction tools for regular interconnects. Through the proposed DOE set, a design or mask house can generate normalized fill tables to remove the inaccuracies of the extraction tools in the presence of floating fills. The capacitance values are updated using these normalized fill tables. The proposed DOE enables extensive analyses of the fill impacts on coupling capacitances. The authors show through extensive 3D field solver simulations that the assumptions used in extractors result in significant inaccuracies. The authors present analyses of fill impacts for an example technology, and also provide analyses using the normalized fill tables to be used in the extraction flow for three different standard fill algorithms","US Department of Energy,
Capacitance,
Algorithm design and analysis,
Pattern analysis,
Integrated circuit interconnections,
Data mining,
Foundries,
Computer science,
Chemical engineering,
Equations"
Requirement Engineering in Service-Oriented System Engineering,"Service-oriented computing has received significant attentions recently, and many applications are being developed using this approach. Thus there is a need to analyze system requirements. This paper examines issues related to service-oriented requirement engineering (SORE). SORE focuses on modeling, specifying, and analyzing application requirements for software that will be developed in a service-oriented manner running in an SOA infrastructure. This paper also presents key features of SORE and some technical challenges.","Systems engineering and theory,
Service oriented architecture,
Application software,
Testing,
Collaborative work,
Runtime,
Communication system control,
Spine,
Computer science,
Mathematics"
Sparse Random Linear Codes are Locally Decodable and Testable,"We show that random sparse binary linear codes are locally testable and locally decodable (under any linear encoding) with constant queries (with probability tending to one). By sparse, we mean that the code should have only polynomially many codewords. Our results are the first to show that local decodability and testability can be found in random, unstructured, codes. Previously known locally decodable or testable codes were either classical algebraic codes, or new ones constructed very carefully. We obtain our results by extending the techniques of Kaufman and Litsyn [11] who used the MacWilliams Identities to show that ""almost-orthogonal"" binary codes are locally testable. Their definition of almost orthogonality expected codewords to disagree in n/2 plusmn O(radicn) coordinates in codes of block length n. The only families of codes known to have this property were the dual-BCH codes. We extend their techniques, and simplify them in the process, to include codes of distance at least n/2 - O(n1-gamma) for any gamma > 0, provided the number of codewords is O(nt) for some constant t. Thus our results derive the local testability of linear codes from the classical coding theory parameters, namely the rale and the distance of the codes. More significantly, we show that this technique can also be used to prove the ""self-correctability"" of sparse codes of sufficiently large distance. This allows us to show that random linear codes under linear encoding functions are locally decodable. This ought to be surprising in that the definition of a code doesn't specify the encoding function used! Our results effectively say that any linear function of the bits of the codeword can be locally decoded in this case.","Linear code,
Decoding,
Testing,
Encoding,
USA Councils,
Binary codes,
Error correction codes,
Computer science,
Probes"
An Evasive Maneuvering Algorithm for UAVs in See-and-Avoid Situations,"In this paper, we present an collision avoidance algorithm for unmanned aerial vehicles (UAVs) based on model predictive control. When a UAV encounters other aircraft that is estimated to approach closer than the minimum safety margin, the vehicle must execute an emergency evasive maneuver to avoid the impending collision at all cost. During this procedure, the unmanned vehicle must compute in real time a safe and plausible trajectory based on the collected information on the predicted future path of other vehicles. During the evasive maneuver, the trajectory generation and control problem is very stringent since the conflict-free trajectory must be plausible with respect to the given vehicle dynamics with limited control input. Therefore, in this research, we propose a model predictive control-based trajectory planner to satisfy the requirements listed so far due to its capability to explicitly address the control problem of constrained nonlinear dynamic systems. We consider a few scenarios involving nearby flying objects with various velocity and incident angle conditions. The proposed algorithm is validated in a head-on collision scenario using unmanned aerial vehicles.",
Autonomous Zebrafish Embryo Injection Using a Microrobotic System,"As an important embodiment of biomanipulation, injection of foreign materials (e.g., DNA, RNAi, sperms, proteins, and drug compounds) into individual cells has significant implications in genetics, transgenics, assisted reproduction, and drug discovery. This paper presents a microrobotic system for fully automated zebrafish embryo injection, which overcomes the problems inherent in manual operation, such as human fatigue and large variations in success rates due to poor reproducibility. Based on computer vision and motion control, the microrobotic system performs injection at a speed of 15 zebrafish embryos (chorion unremoved) per minute, with a survival rate of 98% (350 embryos), a success rate of 99% (350 embryos), and a phenotypic rate of 98.5% (210 embryos). The sample preparation technique and microrobotic control method are applicable to other biological injection applications such as the injection of mouse oocytes/embryos and Drosophila embryos to enable high-throughput biological and pharmaceutical research.","Embryo,
Drugs,
Biological materials,
DNA,
Proteins,
Genetics,
Manuals,
Humans,
Fatigue,
Reproducibility of results"
A robotic catapult based on the closed elastica and Its application to robotic tasks,"In this paper, we propose a novel robotic catapult for generating repeated impulsive motions which are considered as a key to achieve creature-like motions by a compact autonomous robot. The proposed robotic catapult is just a bended elastic strip whose two ends are fixed to two rotational joints, i.e., a mechanical closed-loop of an elastic material named the 'closed elastica'. By only driving one joint back and forth gradually, we can obtain repeated impulsive motions of the elastic strip. It is shown that this robotic catapult can be applied to some robotic tasks such as impulsive robotic swimming and fly casting manipulation.","Intelligent robots,
Strips,
Legged locomotion,
Mobile robots,
Acceleration,
Leg,
Joining materials,
Frequency,
USA Councils"
Subspace Properties of Randomized Network Coding,"Randomized network coding has network nodes randomly combine and exchange linear combinations of the source packets. A header appended to the packet, called coding vector, specifies the exact linear combination that each packet carries. The main contribution of this work is to investigate properties of the subspaces spanned by the collected coding vectors in each network node. We use these properties to exhibit the relationship between the network topology and the subspaces collected at the nodes. This allows us to passively infer the network topology for a general class of graphs.","Network coding,
Network topology,
Vectors,
Decoding,
Tree graphs,
Galois fields,
Protocols,
Synchronization,
Clocks,
Monitoring"
Survey of Traceability Approaches in Model-Driven Engineering,"Models have been used in various engineering fields to help managing complexity and represent information in different abstraction levels, according to specific notations and stakeholder's viewpoints. Model-Driven Engineering (MDE) gives the basic principles for the use of models as primary artefacts throughout the software development phases and presents characteristics that simplify the engineering of software in various domains, such as Enterprise Computing Systems. Hence, for its successful application, MDE processes must consider traceability practices. They help the understanding, capturing, tracking and verification of software artefacts and their relationships and dependencies with other artefacts during the software life-cycle. In this survey, we discuss the state-of-the-art in traceability approaches in MDE and assess them with respect to five general comparison criteria: representation, mapping, scalability, change impact analysis and tool support. As a complementary result, we have identified some open issues that can be better explored by traceability in MDE.","Model driven engineering,
Programming,
Engineering management,
Scalability,
Software engineering,
Distributed computing,
Computer science,
Application software,
Reverse engineering,
Software systems"
Active Text Drawing Styles for Outdoor Augmented Reality: A User-Based Study and Design Implications,"A challenge in presenting augmenting information in outdoor augmented reality (AR) settings lies in the broad range of uncontrollable environmental conditions that may be present, specifically large-scale fluctuations in natural lighting and wide variations in likely backgrounds or objects in the scene. In this paper, we present a active AR testbed that samples the user's field of view, and collects outdoor illuminance values at the participant's position. The main contribution presented herein is a user-based study (conducted using the testbed) that examined the effects on user performance of four outdoor background textures, four text colors, three text drawing styles, and two text drawing style algorithms for a text identification task using an optical, see-through AR system. We report significant effects for all these variables, and discuss design guidelines and ideas for future work","Augmented reality,
Graphical user interfaces,
Testing,
Design engineering,
Optical sensors,
Engineering drawings,
Systems engineering and theory,
Large-scale systems,
Fluctuations,
Virtual reality"
An Epidemic Theoretic Framework for Evaluating Broadcast Protocols in Wireless Sensor Networks,"While multi-hop broadcast protocols, such as Trickle, Deluge and MNP, have gained tremendous popularity as a means for fast and convenient propagation of data/code in large scale wireless sensor networks, they can, unfortunately, serve as potential platforms for virus propagation if the security is breached. To understand the vulnerability of such protocols and design defense mechanisms against piggy-backed virus attacks, it is critical to investigate the propagation process of these protocols in terms of their speed and reachability. In this paper, we propose a general framework based on the principles of epidemic theory, for vulnerability analysis of current broadcast protocols in wireless sensor networks. In particular, we develop a common mathematical model for the propagation that incorporates important parameters derived from the communication patterns of the protocol under test. Based on this model, we analyze the propagation rate and the extent of spread of a malware over typical broadcast protocols proposed in the literature. The overall result is an approximate but convenient tool to characterize a broadcast protocol in terms of its vulnerability to malware propagation. We have also performed extensive simulations which have validated our model.","Broadcasting,
Wireless application protocol,
Wireless sensor networks,
Authentication,
Digital signatures,
Data security,
Vehicle dynamics,
Computer science,
Data engineering,
Spread spectrum communication"
Christiansen Grammar Evolution: Grammatical Evolution With Semantics,"This paper describes Christiansen grammar evolution (CGE), a new evolutionary automatic programming algorithm that extends standard grammar evolution (GE) by replacing context-free grammars by Christiansen grammars. GE only takes into account syntactic restrictions to generate valid individuals. CGE adds semantics to ensure that both semantically and syntactically valid individuals are generated. It is empirically shown that our approach improves GE performance and even allows the solution of some problems are difficult to tackle by GE","Computer languages,
Computer science,
Automata,
Automatic programming,
Formal languages,
Genetic algorithms,
Program processors,
Turing machines,
Educational programs"
pFusion: A P2P Architecture for Internet-Scale Content-Based Search and Retrieval,"The emerging peer-to-peer (p2p) model has become a very powerful and attractive paradigm for developing Internet-scale systems for sharing resources, including files and documents. The distributed nature of these systems, where nodes are typically located across different networks and domains, inherently hinders the efficient retrieval of information. In this paper, we consider the effects of topologically aware overlay construction techniques on efficient p2p keyword search algorithms. We present the peer fusion (pFusion) architecture that aims to efficiently integrate heterogeneous information that is geographically scattered on peers of different networks. Our approach builds on work in unstructured p2p systems and uses only local knowledge. Our empirical results, using the pFusion middleware architecture and data sets from Akamai's Internet mapping infrastructure (AKAMAI), the active measurement project (NLANR), and the text retrieval conference (TREC) show that the architecture we propose is both efficient and practical","Internet,
Content based retrieval,
Peer to peer computing,
Information retrieval,
Keyword search,
Video sharing,
Power system modeling,
Computer architecture,
Video on demand,
Scattering"
Fast and Lazy Build of Acceleration Structures from Scene Hierarchies,"In this paper we show how to use structural information about a scene such as is contained in a scene graph to build SAH-based acceleration structures more efficiently. We provide a general method for doing so together with asymptotic analyses for both standard and lazy variants of our method. In particular, we show bounds of O(n) for full k-d tree builds over n primitives and O(v + log n) for lazy k-d tree builds over v visible primitives. We provide experimental results showing that these asymptotic properties translate into real-world speedups. In fact, without a method like ours, it is impossible to achieve better than O(n) for even the first split of a lazy build. We also show that under certain (realistic) assumptions on the scene structure, our method produces provably good acceleration structures. Finally, we provide experimental results demonstrating that our acceleration structures are of nearly indistinguishable quality to those produced with a full SAH build.","Acceleration,
Layout,
Tree graphs,
Ray tracing,
Computer graphics,
Buildings,
Geometry,
Mirrors,
Real time systems,
Performance gain"
Designing and Evaluating a Haptic System for Biomolecular Education,"In this paper we present an in situ evaluation of a haptic system, with a representative test population, we aim to determine what, if any, benefit haptics can have in a biomolecular education context. We have developed a haptic application for conveying concepts of molecular interactions, specifically in protein-ligand docking. Utilizing a semi-immersive environment with stereo graphics, users are able to manipulate the ligand and feel its interactions in the docking process. The evaluation used cognitive knowledge tests and interviews focused on learning gains. Compared with using time efficiency as the single quality measure this gives a better indication of a system's applicability in an educational environment. Surveys were used to gather opinions and suggestions for improvements. Students do gain from using the application in the learning process but the learning appears to be independent of the addition of haptic feedback. However the addition of force feedback did decrease time requirements and improved the students understanding of the docking process in terms of the forces involved, as is apparent from the students' descriptions of the experience. The students also indicated a number of features which could be improved in future development","Haptic interfaces,
Educational technology,
Force feedback,
Computer science education,
Visualization,
Chemical technology,
Physics education,
Biology,
Protein engineering,
Testing"
Delivering of Live Video Streaming for Vehicular Communication Using Peer-to-Peer Approach,"We design an application-layer overlay-network solution to deliver live video streaming for inter-vehicle communication (IVC) and vehicle-to-roadside communication (VRC). This solution includes a distributed clustering method called DVAC and an efficient peer-to-peer relay method called VAPER DVAC performs the distributed and message-driven procedures for cluster formation and maintenance. The design of cluster-head and cluster-tail in DVAC not only avoids duplication of media transmitting for members in the same cluster, but also helps to extend the link between clusters. Furthermore, VAPER utilizes the clustering infrastructure and the selected pairwise cluster-tails and cluster-heads to propagate live video streaming. On the one hand this method maintains the continuous play of live video streaming for VRC when lost of signal, and on the other hand it provides an efficient pairwise forwarding mechanism for IVC. The integration of DVAC and VAPER indeed provides a practical solution on delivering of live video streaming.","Streaming media,
Peer to peer computing,
Clustering algorithms,
Relays,
Road vehicles,
Road transportation,
Automotive engineering,
Computer science,
Clustering methods,
Availability"
Securing Vehicular Ad Hoc Networks,"Ad hoc networks are a new wireless networking paradigm for mobile hosts. In this paper, we designed an intelligent transport system. The ITS (intelligent transport system) includes two big function modules: Information processing application system and Road condition information transferring system. The main task of the road condition information transferring module is in charge of the information exchange of the car inside, car to car and car to road. The module works in ad hoc network, we call the network VANET (vehicular ad-hoc network) . Vehicular networks are likely to become the most relevant form of mobile ad hoc networks. For the sake of insuring the system can run normally, the information can be transferring correctly and fleetly, the security of VANET (vehicular ad-hoc network) of the road condition information transferring system is crucial. So integrate the characteristics of ad hoc network itself, in the ITS of this paper, we concern the security issues of VANETs from some aspects and provide the appropriate solving measures. To make sure the ITS can be used under the security pattern.","Ad hoc networks,
Roads,
Information security,
Communication system security,
Mobile communication,
Telecommunication traffic,
Intelligent systems,
Information processing,
Mobile ad hoc networks,
Information analysis"
Double-Sided Watermark Embedding and Detection,"Spread-spectrum schemes embed additive or multiplicative watermarks into the host contents for copyright protection. However, their performances are reduced by the interference from the host contents. Thus, in this work, we introduce a simple double-sided technique to utilize this interference for performance improvements. Different from the previous host interference rejection schemes, it does not reject the host interference. However, it can also achieve great performance enhancement over the traditional spread-spectrum schemes. Moreover, due to this nice property of not rejecting the host interference, it has a big advantage over the host interference rejection schemes in that the perceptual analysis can be easily implemented for our scheme to improve the perceptual quality of the watermarked contents. Our technique is effectively employed in both additive and multiplicative spread-spectrum schemes. Finally, its performance advantage is demonstrated through both theoretical and empirical comparisons.","Watermarking,
Quantization,
Decoding,
Spread spectrum communication,
Copyright protection,
Computer science,
Lattices,
Detectors,
Interference cancellation,
Probability distribution"
Explicit Incorporation of Prior Anatomical Information Into a Nonrigid Registration of Thoracic and Abdominal CT and 18-FDG Whole-Body Emission PET Images,"The aim of this paper is to develop a registration methodology in order to combine anatomical and functional information provided by thoracic/abdominal computed tomography (CT) and whole-body positron emission tomography (PET) images. The proposed procedure is based on the incorporation of prior anatomical information in an intensity-based nonrigid registration algorithm. This incorporation is achieved in an explicit way, initializing the intensity-based registration stage with the solution obtained by a nonrigid registration of corresponding anatomical structures. A segmentation algorithm based on a hierarchically ordered set of anatomy-specific rules is used to obtain anatomical structures in CT and emission PET scans. Nonrigid deformations are modeled in both registration stages by means of free-form deformations, the optimization of the control points being achieved by means of an original vector field-based approach instead of the classical gradient-based techniques, considerably reducing the computational time of the structure registration stage. We have applied the proposed methodology to 38 sets of images (33 provided by standalone machines and five by hybrid systems) and an assessment protocol has been developed to furnish a qualitative evaluation of the algorithm performance","Abdomen,
Computed tomography,
Whole-body PET,
Positron emission tomography,
Biomedical imaging,
Medical diagnostic imaging,
Anatomical structure,
Anatomy,
Image segmentation,
Deformable models"
Group-Based Private Authentication,"We propose a novel authentication scheme that ensures privacy of the provers. Our scheme is based on symmetric-key cryptography, and therefore, it is well-suited to resource constrained applications in large scale environments. A typical example for such an application is an RFID system, where the provers are low-cost RFID tags, and the number of the tags can potentially be very large. We analyze the proposed scheme and show that it is superior to the well-known key-tree based approach for private authentication both in terms of privacy and efficiency.","Authentication,
Privacy,
Radiofrequency identification,
Cryptography,
Large-scale systems,
RFID tags,
Identity-based encryption,
Wireless sensor networks,
Computer science,
Artificial intelligence"
A User Interaction Model for NFC Enabled Applications,"Near field communication (NFC) is a short-range wireless protocol that allows users to connect devices and access content and services by simply holding enabled devices near each other. This paper introduces a user interaction model for NFC enabled applications. Our model specifies that enabled devices take on the properties and context of the objects required in the interaction. This transformation leverages the existing knowledge users have about certain objects and thus can support a number of different applications tied together with simple, intuitive and repeatable interactions. In this paper, we present an overview of the model and the system we have implemented to enable evaluation. We also detail some research challenges we are pursuing","Motion pictures,
Radiofrequency identification,
Cellular phones,
Application software,
TV,
Computer science,
Wireless application protocol,
Access protocols,
Context modeling,
Educational institutions"
Task Specific Local Region Matching,"Many problems in computer vision require the knowledge of potential point correspondences between two images. The usual approach for automatically determining correspondences begins by comparing small neighborhoods of high saliency in both images. Since speed is of the essence, most current approaches for local region matching involve the computation of a feature vector that is invariant to various geometric and photometric transformations, followed by fast distance computations using standard vector norms. These algorithms include many parameters, and choosing an algorithm and setting its parameters for a given problem is more an art than a science. Furthermore, although invariance of the resulting feature space is in general desirable, there is necessarily a tradeoff between invariance and descriptiveness for any given task. In this paper we pose local region matching as a classification problem, and use powerful machine learning techniques to train a classifier that selects features from a much larger pool. Our algorithm can be trained on specific domains or tasks, and performs better than the state of the art in such cases. Since our method is an application of boosting, we refer to it as boosted region matching (BOOM).","Computer vision,
Photometry,
Robustness,
Computer science,
Knowledge engineering,
Art,
Machine learning,
Machine learning algorithms,
Boosting,
Layout"
Typical DoS/DDoS Threats under IPv6,"The DoS/DDoS attacks are always the leading threats to the Internet. With the development of Internet, IPv6 is inevitably taking the place of IPv4 as the main protocol of Internet. So the security issues of IPv6 become the focus of the present research. In this paper we mainly focus on the typical DoS/DDoS attacks under IPv6, which including the DoS attacks pertinent to IPv6 Neighbor Discovery protocol and DDoS attacks based on the four representative attack modes, they are respectively TCP-Flood, UDP-Flood, ICMP-Flood and Smurf. We do these attack experiments under IPv6 with and without IPSec configuration respectively. The experiments without IPSec validate the effectiveness of the typical DoS/DDoS attacks under IPv6, and those with IPSec show the effectiveness of IPSec against these attacks whose source addresses are spoofed.","Protocols,
Peer to peer computing,
Computer crime,
Internet,
Security,
Computer science,
Target tracking,
Probes,
Unicast"
Estimation over Wireless Sensor Networks,"Remote estimation problems are critical to many novel applications enabled by large-scale dense wireless sensor network. Individual sensors simultaneously sense, process and transmit measured information over a lossy wireless network to a central base station, which processes the data and produces an optimal estimate of the state. In this paper, we investigate the tradeoff between the estimation performance and the number of communicating nodes with respect to the major MAC protocols used in wireless sensor networks. We first construct a Markov model of the node behavior to study the correlation between packet reception probability and the number of communicating nodes. We then develop a multi-sensor measurement fusion model. This is used to feed a multi-sensor Kalman filtering algorithm to assess the impact of MAC protocols on estimation performance. We offer a target tracking example to illustrate our approach.","Wireless sensor networks,
Media Access Protocol,
Large-scale systems,
Loss measurement,
Propagation losses,
Base stations,
State estimation,
Wireless application protocol,
Feeds,
Kalman filters"
A closed loop stability analysis and parameter selection of the Particle Swarm Optimization dynamics for faster convergence,"This paper presents an alternative formulation of the PSO dynamics by a closed loop control system, and analyzes the stability behavior of the system by using Jury's test and root locus technique. Previous stability analysis of the PSO dynamics was restricted because of no explicit modeling of the non-linear element in the feedback path. In the present analysis, the nonlinear element model of the non-linear element is considered for closed loop stability analysis. Unlike the previous works on stability analysis, where the acceleration coefficients have been combined into a single term, this paper considered their separate existence for determining their suitable range to ensure stability of the dynamics. The range of parameters of the PSO dynamics, obtained by Jury's test and root locus technique were also confirmed by computer simulation of the PSO algorithm.","Stability analysis,
Particle swarm optimization,
Convergence,
Nonlinear dynamical systems,
Control systems,
Control system analysis,
System testing,
Feedback,
Acceleration,
Computer simulation"
RAxML-Cell: Parallel Phylogenetic Tree Inference on the Cell Broadband Engine,"Computational phylogeny is a challenging application even for the most powerful supercomputers. It is also an ideal candidate for benchmarking emerging multiprocessor architectures, because it exhibits fine- and coarse-grain parallelism at multiple levels. In this paper, we present the porting, optimization, and evaluation of RAxML on the cell broadband engine. RAxML is a provably efficient, hill climbing algorithm for computing phylogenetic trees, based on the maximum likelihood (ML) method. The cell broadband engine, a heterogeneous multi-core processor with SIMD accelerators which was initially marketed for set-top boxes, is currently being deployed on supercomputers and high-end server architectures. We present both conventional and unconventional, cell-specific optimizations for RAxML's search algorithm on a real cell multiprocessor. While exploring these optimizations, we present solutions to problems related to floating point code execution, complex control flow, communication, scheduling, and multilevel parallelization on the cell.","Phylogeny,
Engines,
Parallel processing,
Concurrent computing,
Microprocessors,
Computer architecture,
Military computing,
Computer science,
Supercomputers,
Multicore processing"
Visual Segmentation-Based Data Record Extraction from Web Documents,"Semi-structured data records contained in the Web pages provide useful information for shopping agents and metasearch engines. In this paper, we present a visual segmentation-based data record extraction (VSDR) method to extract data records from those Web pages. VSDR method first segments a Web page into semantic blocks using the spatial closeness and visual resemblance of data records, then neighboring and non-neighboring data records are extracted based on a compress and collapse technique. Experimental results slum that unlike the existing methods which only generate good results on their test domains, VSDR is a general data record extraction method that is able to produce quite stable and good results on a wide range of Web pages.","Data mining,
Web pages,
Humans,
Metasearch,
Databases,
Navigation,
Partitioning algorithms,
HTML,
Computer science,
Engines"
"EnLIGHTened Computing: An architecture for co-allocating network, compute, and other grid resources for high-end applications","Many emerging high performance applications require distributed infrastructure that is significantly more powerful and flexible than traditional grids. Such applications require the optimization, close integration, and control of all grid resources, including networks. The EnLIGHTened (ENL) computing project has designed an architectural framework that allows grid applications to dynamically request (in-advance or on-demand) any type of grid resource: computers, storage, instruments, and deterministic, high-bandwidth network paths, including lightpaths. Based on application requirements, the ENL middleware communicates with grid resource managers and, when availability is verified, co-allocates all the necessary resources. ENLpsilas domain network manager controls all network resource allocations to dynamically setup and delete dedicated circuits using generalized multiprotocol label switching (GMPLS) control plane signaling. In order to make optimal brokering decisions, the ENL middleware uses near-real-time performance information about grid resources. A prototype of this architectural framework on a national-scale testbed implementation has been used to demonstrate a small number of applications. Based on this, a set of changes for the middleware have been laid out and are being implemented.",
A New Design for 7:2 Compressors,"High order compressors play a specific role in realizing high speed multipliers. By increasing the demand for fast multiplication process, high order compressors have attracted many researchers to this field. In this paper a new implementation for 7:2 compressors, based on the conventional architecture, is proposed. According to the results, the design presented achieves a remarkable improvement in terms of speed (especially in low voltages) and power consumption over the best counterpart. This accomplishment is the direct result of shortening the critical delay path in the proposed circuit design. As the simulation results demonstrate, the structure presented here has improved the power consumption from minimum 0.07% (at supply voltage = 3.5 volt) through maximum 11% (at 1.2 volt), and the speed of the circuit from minimum 19% (at 3.5 volt) through maximum 23% (at 1.2 volt). HSPICE is the circuit simulator used, and the technology being used for simulations is 0.25 mum technology.","Compressors,
Delay,
Counting circuits,
Circuit simulation,
Energy consumption,
Adders,
Microprocessors,
Digital signal processing,
Design engineering,
Low voltage"
Deriving exact stochastic response times of periodic tasks in hybrid priority-driven soft real-time systems,"The aim of this paper is to allow for hybrid task sets in the context of stochastic real-time analysis. The paper goes beyond previous work by allowing for the presence of aperiodic tasks in the system. Instead of representing a task with a fixed activation period and a worst-case execution time (WCET), here a task is characterized by an arrival profile (AP) and an execution time profile (ETP), both given by random variables with known distributions. Any number of aperiodic tasks, with arbitrary arrival and execution time profiles, can be dealt with. To cope with the unbounded interference introduced by aperiodic tasks in the system, sporadic and aperiodic tasks are encapsulated within servers. The paper presents the calculus for obtaining the exact ETP of servers, which allows us to derive exact response time distributions of periodic tasks. Also, an example is used to show the potential and validity of the proposed approach.","Stochastic systems,
Delay,
Real time systems,
Stochastic processes,
Calculus,
Interference,
Random variables,
Telecommunication computing,
Computer science,
Embedded system"
Adaptive Foreground Object Extraction for Real-Time Video Surveillance with Lighting Variations,"In this paper we present an adaptive foreground object extraction algorithm for real-time video surveillance. The proposed algorithm improves the previous Gaussian mixture background models (GMMs) by applying a two-stage foreground/background classification procedure to remove the undesirable subtraction results due to shadow, automatic white balance, and sudden illumination change. The traditional background subtraction technique usually cannot work well for situations with lighting variations in the scene. In the proposed two-stage classification, an adaptive classifier is applied to the foreground pixels in a pixel-wise manner based on the normalized color and brightness gain information. Secondly, the remaining foreground candidate pixels are grouped into regions and the corresponding background regions are compared to check if they are foreground regions. Experimental results on some real surveillance video are shown to demonstrate the robustness of the proposed adaptive foreground extraction algorithm under a variety of different environments with lighting variations.","Video surveillance,
Subtraction techniques,
Robustness,
Gaussian distribution,
Probability density function,
Pixel,
Computer science,
Lighting,
Layout,
Brightness"
A High Secure Reversible Visible Watermarking Scheme,"A novel reversible visible watermarking algorithm is proposed. It can fully remove the watermark from the visible watermarked image such that the original image can be restored. Pixel values of original image beneath the watermark are mapped to a small range [alpha, alpha + 127] to generate a visible watermarked image. Since the mapping is many-to-one, taking inverse mapping can only approximate the original image. To restore the original image, the difference image of subtracting the approximated image from the original image and other side information are losslessly compressed to be embedded in the visible watermarked image by a reversible data embedding algorithm. We proposed a key-based scheme for the compromise between transparency and robustness. The key is a random variable with discrete normal distribution. In addition, only users with correct key can restore the original image. In the experimental results, we show the transparent degree of watermark can be controlled by the variance of the key. Users with wrong key can not restore the original image from the visible watermarked image.","Watermarking,
Image restoration,
Digital images,
Discrete cosine transforms,
Pixel,
Robustness,
Humans,
Discrete wavelet transforms,
Color,
Image coding"
A Ferry-based Intrusion Detection Scheme for Sparsely Connected Ad Hoc Networks,"Several intrusion detection approaches have been proposed for mobile ad hoc networks. Many of the approaches assume that there are sufficient neighbors to help monitor the transmissions and receptions of data packets by other nodes to detect abnormality. However, in a sparsely connected adhoc network, nodes usually have very small number of neighbors. In addition, new history based routing schemes e.g. Prophet have been proposed because traditional adhoc routing schemes do not work well in sparse ad hoc networks. In this paper, we propose a ferry-based intrusion detection and mitigation (FBIDM) scheme for sparsely connected ad hoc networks that use Prophet as their routing scheme. Via simulations, we study the effectiveness of the FBIDM scheme when malicious nodes launch selective data dropping attacks. Our results with different mobility models, ferry speed, traffic load scenarios indicate that the FBIDM scheme is promising in reducing the impact of such malicious attacks.","Intrusion detection,
Ad hoc networks,
Routing protocols,
Telecommunication traffic,
Wireless networks,
Computer science,
Mobile computing,
Computerized monitoring,
History,
Resilience"
Reduced-complexity detectors for Multi-h CPM in aeronautical telemetry,"A two-index, partial response continuous-phase modulation (CPM) known as ARTM CPM was adopted for use in aeronautical telemetry in the IRIG 106 standard in 2004. This waveform was selected because it achieves approximately three times the spectral efficiency of PCM/FM, the legacy modulation in IRIG-106. However, the optimum receiver requires 128 real-valued matched filters and keeps track of the waveform state with a trellis of 512 states and 2048 branches. Various complexity-reducing techniques are applied and the resulting loss in detection efficiency is quantified. It is shown that the full 512-state trellis is not required to achieve desirable detection efficiency: two different 32-state configurations were found to perform within 0.05 dB of optimal; two different 16-state configurations were found to perform within 0.80 dB of optimal; and an 8-state configuration was found to perform within 1.05 dB of optimal. The analysis and simulation results show that to achieve a given state complexity, proper combination of two or more complexity-reducing techniques generally outperforms the use of a single complexity-reducing technique","Detectors,
Telemetry,
Phase change materials,
Frequency modulation,
Pulse modulation,
Matched filters,
Quadrature phase shift keying,
Maximum likelihood detection,
Analytical models,
Computer science"
Soft-core Processor Customization using the Design of Experiments Paradigm,"Parameterized components are becoming more commonplace in system design. The process of customizing parameter values for a particular application, called tuning, can be a challenging task for a designer. Here we focus on the problem of tuning a parameterized soft-core microprocessor to achieve the best performance on a particular application, subject to size constraints. We map the tuning problem to a well-established statistical paradigm called design of experiments (DoE), which involves the design of a carefully selected set of experiments and a sophisticated analysis that has the objective to extract the maximum amount of information about the effects of the input parameters on the experiment. We apply the DoE method to analyze the relation between input parameters and the performance of a soft-core microprocessor for a particular application, using only a small number of synthesis/execution runs. The information gained by the analysis in turn drives a soft-core tuning heuristic. We show that using DoE to sort the parameters in order of impact results in application speedups of 6times-17times versus an un-tuned base soft-core. When compared to a previous single-factor tuning method, the DoE-based method achieves 3times-6times application speedups, while requiring about the same tuning runtime. We also show that tuning runtime can be reduced by 40-45% by using predictive tuning methods already built into a DoE tool",
Convergence Analysis of Bilateral Teleoperation with Constant Human Input,"In this paper the problem of bilateral teleoperation is studied for a class of human operator models that are not guaranteed to be passive. Specifically, the hard contact scenario is addressed where the human operator applies a constant force on the master robot and the slave robot interacts with the environment, which is modeled as a spring-damper system. In the delay free case, when the master/slave robots are coupled using the PD control strategy, the nonlinear master-slave teleoperation system is shown to be asymptotically stable. If the environment stiffness is known, then the steady state position of the master and slave robots is predicted. In the case of network delay and for a range of the proportional coupling gains, we demonstrate that the master/slave velocities asymptotically converge to the origin and the positions asymptotically converge to a non-zero equilibrium. Simulations results are also presented to verify the proposed results.","Convergence,
Master-slave,
Robot kinematics,
Teleoperators,
Delay,
Human robot interaction,
Couplings,
PD control,
Robot sensing systems,
Feedback"
TARP: A Trust-Aware Routing Protocol for Sensor-Actuator Networks,"Most routing protocols for sensor-actuator networks (SANETs) are built under the assumption that nodes normally cooperate in forwarding each other's messages. In practice, this assumption is not realistic; SANETs are environments where nodes may or may not cooperate. For several reasons, a node may fail to operate as planned at deployment time. As a result, when actually deployed, protocols and applications may not be as efficient as expected. In this paper, we present TARP (Trust-Aware Routing Protocol), a routing protocol for sensor-actuator networks that exploits past nodes' routing behavior and links' quality to determine efficient paths. We implemented TARP in a TinyOS-based SANET and conducted several experiments to evaluate its performance. The obtained results confirmed that TARP achieves substantial improvements in terms of energy consumption and scalability.","Routing protocols,
Temperature sensors,
Actuators,
Cooling,
Collaboration,
Quality of service,
Sensor systems and applications,
Safety,
Energy states,
Computer science"
A Robust Video Foreground Segmentation by Using Generalized Gaussian Mixture Modeling,"In this paper, we propose a robust video foreground modeling by using a finite mixture model of generalized Gaussian distributions (GDD). The model has a flexibility to model the video background in the presence of sudden illumination changes and shadows, allowing for an efficient foreground segmentation. In a first part of the present work, we propose a derivation of the online estimation of the parameters of the mixture of GDDS and we propose a Bayesian approach for the selection of the number of classes. In a second part, we show experiments of video foreground segmentation demonstrating the performance of the proposed model.","Robustness,
Image segmentation,
Shape,
Gaussian distribution,
Lighting,
Computer vision,
Computer science,
Application software,
Video surveillance,
Computerized monitoring"
Python Unleashed on Systems Biology,"Researchers at Cornell University have built an open source software system to model biomolecular reaction networks. SloppyCell is written in Python and uses third-party libraries extensively, but it also does some fun things with on-the-fly code generation and parallel programming.","Systems biology,
Biological system modeling,
Predictive models,
Chemicals,
Open source software,
Signal processing,
Uncertainty,
Differential equations,
Software libraries,
Markup languages"
QoS-Aware Streaming in Overlay Multicast Considering the Selfishness in Construction Action,"Most existing overlay multicast proposals have assumed that the nodes are cooperative and thus focus on the global topology optimization. However, a unique and important characteristic of overlay nodes is that, as application-layer agents, they can be selfish with their own interests. To achieve better quality-of-service (QoS) or to minimize forwarding overhead, an overlay node can behave selfishly in the information collection or in the overlay construction. While the former has recently been investigated, the impact of selfishness in the construction action remains unclear. In this paper, we present the first systematic study on the impact of selfishness in both tree and mesh overlay construction. Our investigation considers multiple QoS measures for streaming applications, including stream latency, resolution, and continuity. Our contribution is twofold: first, we analyze how for selfish overlay nodes to choose a construction-action policy to optimize their individual multi-metric QoS. Second, we demonstrate that the selfishness-aware policy for the construction action is consistent with the QoS optimization for the global multicast session, but not vice versa. The implication is significant: A globally optimal overlay construction itself can be vulnerable to individual selfishness; but, following our directions, we can design an overlay that is both globally optimal and selfish-resistant.",
"Electrostatic latching for inter-module adhesion, power transfer, and communication in modular robots","A simple and robust inter-module latch is possibly the most important component of a modular robotic system. This paper describes a latch based on electric fields and capacitive coupling. Our design provides not only significant adhesion forces, but can also be used for inter-module power transmission and communication. The key insight presented in this paper, and the factor that enables electrostatic adhesion to be effective at the macroscale, is the use of electric field attraction to generate frictional shear forces rather than electric field attraction alone. A second important insight is that a specific degree of flexibility in the electrodes is essential to maximize their mutual coupling and the resulting forces - electrodes which are too flexible or too rigid will perform less well. To evaluate the effectiveness of our latch we incorporate it into a cubic module 28 cm on a side. The result is a latch which requires almost zero static power and yet can hold 0.6 N/cm2 of latch area.","Electrostatics,
Adhesives,
Robustness,
Intelligent robots,
Electrodes,
Orbital robotics,
Helium,
USA Councils,
Couplings"
A Near-optimal Solution for the Heterogeneous Multi-processor Single-level Voltage Setup Problem,"A heterogeneous multi-processor (HeMP) system consists of several heterogeneous processors, each of which is specially designed to deliver the best energy-saving performance for a particular category of applications. A low-power real-time scheduling algorithm is required to schedule tasks on such a system to minimize its energy consumption and complete all tasks by their deadline. The problem of determining the optimal speed for each processor to minimize the total energy consumption is called the voltage setup problem. This paper provides a near-optimal solution for the HeMP single-level voltage setup problem. To our best knowledge, we are the first work that addresses this problem. Initially, each task is assigned to a processor in a local-optimal manner. We next propose a couple of solutions to reduce energy by migrating tasks between processors. Finally, we determine each processor's speed by its final workload and the deadline. We conducted a series of simulations to evaluate our algorithms. The results show that the local-optimal partition leads to a considerably better energy-saving schedule than a commonly-used homogeneous multi-processor scheduling algorithm. Furthermore, at all measurable configurations, our energy consumption is at most 3% more than the optimal value obtained by an exhaustive iteration of all possible task-to-processor assignments. In summary, our work is shown to provide a near-optimal solution at its polynomial-time complexity.","Voltage,
Energy consumption,
Processor scheduling,
Scheduling algorithm,
Partitioning algorithms,
Signal processing algorithms,
Real time systems,
Energy measurement,
Polynomials,
Computer science"
Image-Dependent Gamut Mapping as Optimization Problem,We explore the potential of image-dependent gamut mapping as a constrained optimization problem. The performance of our new approach is compared to standard reference gamut mapping algorithms in psycho-visual tests.,"Constraint optimization,
Psychology,
Testing,
Computer science,
Monitoring,
Printers,
Color,
Conference proceedings"
A Novel Surface Registration Algorithm With Biomedical Modeling Applications,"In this paper, we propose a novel surface matching algorithm for arbitrarily shaped but simply connected 3-D objects. The spherical harmonic (SPHARM) method is used to describe these 3-D objects, and a novel surface registration approach is presented. The proposed technique is applied to various applications of medical image analysis. The results are compared with those using the traditional method, in which the first-order ellipsoid is used for establishing surface correspondence and aligning objects. In these applications, our surface alignment method is demonstrated to be more accurate and flexible than the traditional approach. This is due in large part to the fact that a new surface parameterization is generated by a shortcut that employs a useful rotational property of spherical harmonic basis functions for a fast implementation. In order to achieve a suitable computational speed for practical applications, we propose a fast alignment algorithm that improves computational complexity of the new surface registration method from O(n3) to O(n2).","Shape,
Biomedical imaging,
Medical diagnostic imaging,
Mathematical model,
Ellipsoids,
Biomedical engineering,
Computer science,
Radiology,
Surface reconstruction,
Image analysis"
On Power-profiling and Pattern Generation for Power-safe Scan Tests,"With increasing use of low cost wire-bond packages for mobile devices, excessive dynamic IR-drop may cause tests to fail on the tester. Identifying and debugging such scan test failures is a very complex and effort-intensive process. A better solution is to generate correct-by-construction ""power-safe"" patterns. Moreover, with glitch power contributing to a significant component of dynamic power, pattern generation needs to be timing-aware to minimize glitching. In this paper, we propose a timing-based, power and layout-aware pattern generation technique that minimizes both global and localized switching activity. Techniques are also proposed for power-profiling and optimizing an initial pattern set to obtain a power-safe pattern set, with the addition of minimal patterns. The proposed technique also comprehends irregular power grid topologies for constraints on localized switching activity. Experiments on ISCAS benchmark circuits reveal the effectiveness of the proposed scheme","Power generation,
Test pattern generators,
Testing,
Delay,
Packaging,
Timing,
Hazards,
Application specific integrated circuits,
Instruments,
Computer science"
Verification of Business Process Quality Constraints Based on Visual Process Patterns,"Business processes usually have to consider certain constraints like domain specific and quality requirements. The automated formal verification of these constraints is desirable, but requires the user to provide an unambiguous formal specification. In particular since the notations for business process modeling are usually visual flow-oriented languages, the notational gap to the languages usually employed for the formal specification of constraints, e.g., temporal logic, is significant and hard to bridge. Thus, our approach relies on UML Activities as a single language for the specification of both business processes and the corresponding constraints. For the expression of such constraints, we have provided a process pattern definition language based on specialized Activities. In this paper, we describe how model checking can be employed for formal verification of business processes against such patterns. For this, we present an automated transformation of the business process and the corresponding patterns into a transition system and temporal logic, respectively.",
Petri Net Supervisors for Disjunctive Constraints,"The paper presents an approach for the design of supervisors for disjunctive constraints in which the supervisors are represented by labeled Petri nets. This approach extends our previous results in two ways. First, the supervisors are now guaranteed to be least restrictive. Second, the constraints may now also include the firing vector. The approach is illustrated on the readers/writers problem. While the results are obtained in the fully controllable and observable setting, issues arising when the system is partially controllable and partially observable are also discussed. The approach is developed under certain boundedness assumptions.",
Joint Optimization of Word Alignment and Epenthesis Generation for Chinese to Taiwanese Sign Synthesis,"This work proposes a novel approach to translate Chinese to Taiwanese sign language and to synthesize sign videos. An aligned bilingual corpus of Chinese and Taiwanese sign language (TSL) with linguistic and signing information is also presented for sign language translation. A two-pass alignment in syntax level and phrase level is developed to obtain the optimal alignment between Chinese sentences and Taiwanese sign sequences. For sign video synthesis, a scoring function is presented to develop motion transition-balanced sign videos with rich combinations of intersign transitions. Finally, the maximum a posteriori (MAP) algorithm is employed for sign video synthesis based on joint optimization of two-pass word alignment and intersign epenthesis generation. Several experiments are conducted in an educational environment to evaluate the performance on the comprehension of sign expression. The proposed approach outperforms the IBM Model2 in sign language translation. Moreover, deaf students perceived sign videos generated by the proposed method to be satisfactory","Handicapped aids,
Natural languages,
Videos,
Deafness,
Auditory system,
Avatars,
Animation,
Image recognition,
Virtual reality,
Books"
Genesis II - Standards Based Grid Computing,"In the past years, hype over Web services and their uses in emerging software applications has prompted the creation of many standards and proto-standards. The OGF has seen a number of standards making their way through design and edit pipelines. While this standards process progresses, it is important that implementations of these standards develop in parallel in order to validate the efforts of the standards authors while also providing feedback for further specification refinement. No specification exists in isolation but rather composes with others to form higher order products. These specifications will form the grid infrastructure of the future and an evaluation of this emerging work becomes increasingly relevant. Genesis II is a grid system implemented using these standards that serves both to provide the feedback described above as well as to function as a production level grid system for research at the University of Virginia.","Grid computing,
Web services,
Standards development,
Software standards,
Feedback,
Wrapping,
Computer science,
Application software,
Pipelines,
Production systems"
Learning-Based Segmentation Framework for Tissue Images Containing Gene Expression Data,"Associating specific gene activity with functional locations in the brain results in a greater understanding of the role of the gene. To perform such an association for the more than 20 000 genes in the mammalian genome, reliable automated methods that characterize the distribution of gene expression in relation to a standard anatomical model are required. In this paper, we propose a new automatic method that results in the segmentation of gene expression images into distinct anatomical regions in which the expression can be quantified and compared with other images. Our contribution is a novel hybrid atlas that utilizes a statistical shape model based on a subdivision mesh, texture differentiation at region boundaries, and features of anatomical landmarks to delineate boundaries of anatomical regions in gene expression images. This atlas, which provides a common coordinate system for internal brain data, is being used to create a searchable database of gene expression patterns in the adult mouse brain. Our framework annotates the images about four times faster and has achieved a median spatial overlap of up to 0.92 compared with expert segmentation in 64 images tested. This tool is intended to help scientists interpret large-scale gene expression patterns more efficiently","Image segmentation,
Gene expression,
Genomics,
Bioinformatics,
Shape,
Image databases,
Spatial databases,
Mice,
Testing,
Large-scale systems"
A fuzzy c means variant for clustering evolving data streams,"Clustering algorithms for streaming data sets are gaining importance due to the availability of large data streams from different sources. Recently a number of streaming algorithms have been proposed using crisp algorithms such as hard c means or its variants. The crisp cases may not be easily generalized to fuzzy cases as these two groups of algorithms try to optimize different objective functions. In this paper we propose a streaming variant of the fuzzy c means algorithm. At any stage during processing, a good streaming algorithm should be able to summarize data seen so far and also respond to evolving distributions. We study the tradeoff involved between summarization of data seen and response to an evolving distribution by varying the amount of history used by a streaming algorithm. Empirical evaluation of the performance of our algorithm using both artificial and real data sets under a noisy setting shows its effectiveness.","Clustering algorithms,
History,
Fuzzy sets,
Statistical distributions,
Telephony,
Monitoring"
Lifetime-Aware Leisure Degree Adaptive Routing Protocol for Mobile Ad Hoc Networks,"The intrinsic flexibility and independence of infrastructure enables mobile ad hoc networks (MANET) to be widely used in various environments such as disaster rescue, and battlefield. The conventional and power-aware routing protocols mainly hold strong preferences to the ""shortest paths"" and ""minimal energy consumption paths"" respectively, neglecting the real traffic on those paths, which may lead to so called ""hot spots"" on some links where heavy traffic converges. The load-aware protocols did not consider the remaining energy of each node, which is critical to a MANET. Therefore, we extend our proposed leisure degree adaptive routing (LDAR) algorithm [9] by presenting a new combined metric ""lifetime-aware leisure degree"", which reflects the transmission state, the remaining energy, and energy drain rate in a node as a whole. Based on this metric and a cross-layer design method involving the medium access control (MAC) layer and the network layer, we design a new routing protocol ""Lifetime-aware Leisure Degree Adaptive Routing""(l-LDAR), which uses a heuristic route selection mechanism in order to efficiently control the congestion and to balance the traffic load. Using this protocol, we can prolong the network lifetime and improve the performance of MANET. Simulation results show that 1-LDAR performs better than the Dynamic Source Routing (DSR) and the LDAR routing protocols under the circumstances of both static networks and mobile networks.","Routing protocols,
Mobile ad hoc networks,
Energy consumption,
Telecommunication traffic,
Batteries,
Computer science,
Media Access Protocol,
Cost function,
Relays,
Mobile communication"
Session Viewer: Visual Exploratory Analysis of Web Session Logs,"Large-scale session log analysis typically includes statistical methods and detailed log examinations. While both methods have merits, statistical methods can miss previously unknown sub- populations in the data and detailed analyses may have selection biases. We therefore built Session Viewer, a visualization tool to facilitate and bridge between statistical and detailed analyses. Taking a multiple-coordinated view approach, Session Viewer shows multiple session populations at the Aggregate, Multiple, and Detail data levels to support different analysis styles. To bridge between the statistical and the detailed analysis levels, Session Viewer provides fluid traversal between data levels and side-by-side comparison at all data levels. We describe an analysis of a large-scale web usage study to demonstrate the use of Session Viewer, where we quantified the importance of grouping sessions based on task type.","Data analysis,
Aggregates,
Large-scale systems,
Statistical analysis,
Data visualization,
Bridges,
Information analysis,
Frequency,
Buildings,
Human computer interaction"
A Self-Stabilizing Distributed Approximation Algorithm for the Minimum Connected Dominating Set,"Self-stabilization is a theoretical framework of non-masking fault-tolerant distributed algorithms. A self-stabilizing system tolerates any kind and any finite number of transient faults, such as message loss, memory corruption, and topology change. Because such transient faults occur so frequently in mobile ad hoc networks, distributed algorithms on them should tolerate such events. In this paper, we propose a self-stabilizing distributed approximation algorithm for the minimum connected dominating set, which can be used, for example, as a virtual backbone or routing in mobile ad hoc networks. The size of the solution by our algorithm is at most 8 |Dopt | + 1, where Dopt is a minimum connected dominating set. The time complexity is O(n2) steps.","Approximation algorithms,
Mobile ad hoc networks,
Distributed algorithms,
Network topology,
Spine,
Routing,
Computer networks,
Fault tolerance,
Distributed computing,
Information systems"
Detecting cognitive activity related hemodynamic signal for brain computer interface using functional near infrared spectroscopy,"The ideal non-invasive brain computer interface (BCI) transforms signals originating from human brain into commands that can control devices and applications. Hence, BCI provides a way for brain output that does not involve neuromuscular system. This represents an advantage for those individuals suffering from neuromuscular impairments such as amyotrophic lateral sclerosis (ALS) or various types of paralysis. In this study we propose to design a new noninvasive BCI that is based on optical means to measure brain activity by monitoring hemodynamic response. The proposed system uses functional near infrared (fNIR) spectroscopy to detect cognitive activity from prefrontal cortex elicited voluntarily by performing a mental task namely N-back test. Our findings indicate that fNIR signal correlates with cognitive tasks associated with working memory. These experimental outcomes compare favorably with previous functional magnetic resonance imaging (fMRI) and complement electroencephalogram (EEG) findings. Since fNIR can be implemented in the form of a wearable and minimally intrusive device, it also has the capacity to monitor brain activity under real life conditions in everyday environments leading the way to potential applications of fNIR in BCI development for communication and entertainment purposes.","Infrared detectors,
Hemodynamics,
Brain computer interfaces,
Infrared spectra,
Neuromuscular,
Humans,
Application software,
Optical design,
Monitoring,
Spectroscopy"
"Genetic Algorithm Approach for Adaptive Subcarrier, Bit, and Power Allocation","Orthogonal frequency division multiplexing (OFDM) is a promising technology for high data rate transmission in wideband wireless systems for achieving high downlink capacities in future cellular systems. In this paper, first each user is assigned one subcarrier (channel) with best channel-to-noise ratio for the channel and random power for each channel. The goal is to minimize the overall transmission power ensuring the optimum allocation of subcarriers for users. We used a simple genetic algorithm to improve the power requirements in a multiuser environment. The simulation results indicate that the proposed genetic algorithm performs better than the Wong's (1999) subcarrier, bit, and power allocation algorithm.","Genetic algorithms,
Resource management,
OFDM modulation,
Greedy algorithms,
Iterative algorithms,
Wideband,
AWGN,
Amplitude modulation,
Quadrature amplitude modulation,
Bit error rate"
"Optimum Prefix Adders in a Comprehensive Area, Timing and Power Design Space","Parallel prefix adder is the most flexible and widely-used binary adder for ASIC designs. Many high-level synthesis techniques have been developed to find optimal prefix structures for specific applications. However, the gap between these techniques and back-end designs is increasingly large. In this paper, we propose an integer linear programming method to build minimal-power prefix adders within given timing and area constraints. It counts both gate and wire capacitances in the timing and power models, considers static and dynamic power consumptions, and can handle gate sizing and buffer insertion to improve the performance further. The proposed method is also adaptive for non-uniform arrival time and required time on each bit position. Therefore our method produces the optimum prefix adder for realistic constraints.","Timing,
Wire,
Capacitance,
Computer science,
Application specific integrated circuits,
High level synthesis,
Integer linear programming,
Energy consumption,
Logic design,
Space exploration"
Evaluating Indirect Branch Handling Mechanisms in Software Dynamic Translation Systems,"Software dynamic translation (SDT) systems are used for program instrumentation, dynamic optimization, security, intrusion detection, and many other uses. As noted by many researchers, a major source of SDT overhead is the execution of code which is needed to translate an indirect branch's target address into the address of the translated destination block. This paper discusses the sources of indirect branch (IB) overhead in SDT systems and evaluates several techniques for overhead reduction. Measurements using SPEC CPU2000 show that the appropriate choice and configuration of IB translation mechanisms can significantly reduce the IB handling overhead. In addition, cross-architecture evaluation of IB handling mechanisms reveals that the most efficient implementation and configuration can be highly dependent on the implementation of the underlying architecture","Software systems,
Protection,
Computer science,
Application software,
Costs,
Instruments,
Computer architecture,
VLIW,
Lifting equipment,
Mars"
Towards a Gravity-Based Trust Model for Social Networking Systems,"Web-based social networks are emerging as the top applications on the Internet. With this immense popularity, many of the shortcomings of the current social network deployments are also coming to light. One of the glaring problems with existing web-based social networks is trust management. In this paper, we focus on trust modeling in social networks. Another allied issue that is not considered here is using trust in managing the activities within the social network. We introduce a gravity-based model for estimating trust. We present the complete model along with the trust computation algorithms. We present initial results from a simulation study that investigates the feasibility of the proposed scheme.","Social network services,
Peer to peer computing,
Computer network management,
Computational modeling,
Computer networks,
Computer science,
Application software,
IP networks,
Large-scale systems,
File servers"
Acoustic Micro-Doppler Gait Signatures of Humans and Animals,A micro-Doppler active acoustic sensing system is described. We report its use in acquiring gait signatures of humans and four-legged animals in indoor and outdoor environments. Signals from an accelerometer attached to the leg support the interpretation of the components in the measured micro-Doppler signature. The acoustic micro-Doppler system described in this paper is simpler and offers advantages over the widely used electromagnetic wave micro-Doppler radars. It can be implemented in custom integrated circuits and embedded in a multi-modal wireless sensor network for autonomous detection and classification.,"Humans,
Animals,
Accelerometers,
Leg,
Acoustic measurements,
Electromagnetic measurements,
Integrated circuit measurements,
Acoustic waves,
Electromagnetic scattering,
Radar"
A Framework for Evolving Multi-Shaped Detectors in Negative Selection,"This paper presents a framework to generate multi-shaped detectors with valued negative selection algorithms (NSA). In particular, detectors can take the form of hyper-rectangles, hyper-spheres and hyper-ellipses in the non-self space. These novel pattern detectors (in the complement space) are evolved using a genetic search (the structured genetic algorithm), which uses hierarchical genomic structures and a gene activation mechanism to encode multiple detector shapes. This genetic search (the structured GA) allows in maintaining diverse shapes while contributing to the proliferation of best suited detector shapes in expressed phenotype. The results showed that a significant coverage of the non-self space could be achieved with fewer detectors compared to other NSA approaches (using only single-shaped detectors). The uniform representation scheme and the evolutionary mechanism used in this work can serve as a baseline for further extension to use several shapes, providing an efficient coverage of non-self space.","Detectors,
Change detection algorithms,
Genomics,
Bioinformatics,
Computational intelligence,
Genetic algorithms,
Evolutionary computation,
Shape measurement,
Biological cells,
Computer science"
"Terrorism and Crime Related Weblog Social Network: Link, Content Analysis and Information Visualization","A Weblog is a Web site where entries are made in diary style, maintained by its sole author - a blogger, and displayed in a reverse chronological order. Due to the freedom and convenience of publishing in Weblogs, this form of media provides an ideal environment as a propaganda platform for terrorist groups to promote their ideologies and as an operation platform for organizing crimes. In this work, we present a framework to analyze and visualize Weblog social network embedded beneath relevant Weblogs gathered through topic-specific exploration. Link analysis uses the relationships between bloggers to construct the Weblog social network. Content analysis associates similar blog messages to unveil implicit relationships found in the semantics to further improve the Weblog social network analysis. Users can use different interactive information visualization techniques to explore various aspects of the underlying social network at different levels of abstraction. With the capability of analyzing and visualizing Weblog social networks in terrorist and crime related matters, intelligence agencies and law enforcement will be able to have an additional tools and means to ensure the national security.","Web sites,
computer crime,
data visualisation,
electronic publishing,
interactive systems,
legislation,
national security,
social sciences computing,
terrorism,
user interfaces"
On Computation and Communication with Small Bias,"We present two results for computational models that allow error probabilities close to 1/2. First, most computational complexity classes have an analogous class in communication complexity. The class PP in fact has two, a version with weakly restricted bias called PPcc, and a version with unrestricted bias called UPPcc. Ever since their introduction by Babai, Frankl, and Simon in 1986, it has been open whether these classes are the same. We show that PPcc subne UPPcc. Our proof combines a query complexity separation due to Beigel with a technique of Razborov that translates the acceptance probability of quantum protocols to polynomials. Second, we study how small the bias of minimal-degree polynomials that sign-represent Boolean functions needs to be. We show that the worst-case bias is at worst double- exponentially small in the sign-degree (which was very recently shown to be optimal by Podolski), while the average- case bias can be made single-exponentially small in the sign-degree (which we show to be close to optimal).","Complexity theory,
Polynomials,
Protocols,
Computational complexity,
Error probability,
Boolean functions,
Computer science,
Contracts,
Application software,
Quantum computing"
Realistic Simulation of Network Protocols in VANET Scenarios,"Simulation of network protocol behavior in Vehicular Ad Hoc Network (VANET) scenarios is the predominant basis for evaluating the applicability of particular protocols developed in the Mobile Ad Hoc Network (MANET) domain. As the selection of a mobility model influences the outcome of simulations to a great deal, the use of a representative model is necessary for producing meaningful evaluation results. In this work, we discuss and motivate the needs for coupling traffic micro simulation with standard network simulation. In particular, we developed such an integrated traffic/network simulation tool for evaluating network protocols in realistic VANET environments. In our work, we employed well-studied microsimulation models and wireless ad hoc network models from the domain of transportation and traffic science and the networking community, respectively. It could be shown that network simulations that make use of realistic traffic models produce vastly different results than those relying on commonly used simplistic models, while their adoption incurs only negligible performance penalties.","Protocols,
Traffic control,
Telecommunication traffic,
Mobile ad hoc networks,
Communication system traffic control,
Transportation,
Computational modeling,
Computer networks,
Ad hoc networks,
Integrated circuit modeling"
Improving Mobile Banking Security Using Steganography,"Upon development of m-commerce as one of the new branches of e-commerce, m-banking has emerged as one of the main divisions of m-commerce. As the m-banking was received very well, it has embarked upon supply of various services based on different systems and with the aid of various services such as the short messaging service (SMS). However, in spite of its advantages, m-banking is facing some challenges as well. One of these challenges is the issue of security of this system. This paper presents a method for increasing security of the information requested by users with the use of steganography method. In this method, instead of direct sending of the information, it is hidden in a picture by the password and is put on a site. Then the address of the picture is sent to the user. After receiving the address of the picture through SMS, the user downloads the picture by a special program. After entering the password, the user can witness the information extracted from the picture if the password is entered correctly. This project is written in J2ME language (Java 2 Micro Edition) and has been implemented on Nokia mobile phones, models N71 and 6680","Banking,
Steganography,
Information security,
Mobile handsets,
Message service,
Wireless application protocol,
Internet,
Mobile computing,
Computer security,
Computer science"
A cluster head selection method for wireless sensor networks based on fuzzy logic,"Sensor networks supported by recent technological advances in low power wireless communications along with silicon integration of various functionalities are emerging as a critically important computer class that enable novel and low cost applications. There are many fundamental problems that sensor networks research will have to address in order to ensure a reasonable degree of cost and system quality. Cluster formation and cluster head selection are important problems in sensor network applications and can drastically affect the network's communication energy dissipation. However, selecting of the cluster head is not easy in different environments which may have different characteristics. In this paper, in order to deal with this problem we propose a power reduction algorithm for sensor networks based on fuzzy logic. We evaluate the proposed method by simulations and show that proposed system makes a good selection of the cluster head.","Wireless sensor networks,
Fuzzy logic,
Application software,
Sensor phenomena and characterization,
Wireless communication,
Silicon,
Computer networks,
Cost function,
Sensor systems,
Energy dissipation"
Ubiquitous Robot: A New Paradigm for Integrated Services,"This paper presents the components and overall architecture of the ubiquitous robot (Ubibot) system developed to demonstrate ubiquitous robotics, a new paradigm for integrated services. The system has been developed on the basis of the definition of the ubiquitous robot as that of encompassing the software robot Sobot, embedded robot Embot and the mobile robot Mobot. This tripartite partition, which independently manifests intelligence, perception and action, enables the abstraction of intelligence through the standardization of sensory data and motor or action commands. The Ubibot system itself is introduced along with its component subsystems of Embots, the position Embot, vision Embot and sound Embot, the Mobots of Mybot and HSR, the Sobot, Rity, a virtual pet modeled as an artificial creature, and finally the middleware which seamlessly enables interconnection between other components. Three kinds of experiments are devised to demonstrate the fundamental features, of calm sensing, context awareness and seamless service transcending the spatial limitations in the abilities of earlier generation personal robots. The experiments demonstrate the proof of concept of this powerful new paradigm which shows great promise.","Intserv networks,
Mobile robots,
Robot sensing systems,
Intelligent robots,
Intelligent sensors,
Computer architecture,
Embedded software,
Artificial intelligence,
Standardization,
Positron emission tomography"
A Calibration Method for MEMS Inertial Sensors Based on Optical Tracking,"A MAG-muIMU which is based on MEMS accelerometers, gyroscopes and magnetometers has been developed for real-time estimation of human hand motions. Appropriate filtering, transformation and sensor fusion techniques are combined in the ubiquitous digital writing instrument (UDWI) to record the handwriting on any surface. However, because of the sensors' intrinsic biases and random noise such as circuit thermal noise, a calibration system that provides good reference measurement parameters must be used to compare the output of the MAG-muIMU sensors. We propose here a novel idea to calibrate three-dimensional linear accelerations, angular velocities and space attitude through optical tracking techniques. The optical tracking system (OTS) developed by our group consists of two parts: 1) 2D trajectory calibration that is used to obtain linear accelerations of the UDWI in a particular frame defined by us; 2) multiple camera calibration that is used for attitude calibration of the UDWI. An essential relationship to transform reference frames and angular velocities can be guaranteed after real-time attitude calibration. Hence, the entire nine-dimensional output of the MAG-muIMU can be rectified according to these more accurate data obtained from optical tracking.","Optical sensors,
Calibration,
Micromechanical devices,
Optical filters,
Optical noise,
Magnetic sensors,
Sensor systems,
Thermal sensors,
Circuit noise,
Acceleration"
Fractal-based Automatic Localization and Segmentation of Optic Disc in Retinal Images,"In this paper, we proposed a novel algorithm to detect optic disc location in retinal images. Optic disc is a bright disk area and all major blood vessels and nerves originate from it. With its high fractal dimension of blood vessel, optic disc can be easily differentiated from other bright regions such as hard exudates and artifacts. Compared with existing algorithms, ours has much lower computational cost and is more robust. With its location known, segmentation of optic disc can be done with simple local histogram analysis. The algorithm can be valuable for automated processing for early stage retinal disease.",
Laparoscopic Virtual Mirror New Interaction Paradigm for Monitor Based Augmented Reality,"A major roadblock for using augmented reality in many medical and industrial applications is the fact that the user cannot take full advantage of the 3D virtual data. This usually requires the user to move the virtual object, which disturbs the real/virtual alignment, or to move his head around the real objects, which is not always possible and/or practical. This problem becomes more dramatic when a single camera is used for monitor based augmentation, such as in augmented laparoscopic surgery. In this paper we introduce an interaction and 3D visualization paradigm, which presents a new solution to this old problem. The interaction paradigm uses an interactive virtual mirror positioned into the augmented scene, which allows easy and complete interactive visualization of 3D virtual data. This paper focuses on the exemplary application of such visualization techniques to laparoscopic interventions. A large number of such interventions aims at regions inside a specific organ, e.g. blood vessels to be clipped for tumor resection. We use high-resolution intra-operative imaging data generated by a mobile C-arm with cone-beam CT imaging capability. Both the C-arm and the laparoscope are optically tracked and registered in a common world coordinate frame. After patient positioning, port placement, and carbon dioxide insufflation, a C-arm volume is reconstructed during patient exhalation and superimposed in real time on the laparoscopic live video without any need for an additional patient registration procedure. To overcome the missing perception of 3D depth and shape when rendering virtual volume data directly on top of the organ's surface view, we introduce the concept of a laparoscopic virtual mirror: A virtual reflection plane within the live laparoscopic video, which is able to visualize a reflected side view of the organ and its interior. This enables the surgeon to observe the 3D structure of, for example, blood vessels by moving the virtual mirror within the augmented monocular view of the laparoscope.","Laparoscopes,
Mirrors,
Augmented reality,
Data visualization,
Biomedical imaging,
Blood vessels,
High-resolution imaging,
Optical imaging,
Biomedical monitoring,
Head"
Adding Angle of Arrival Modality to Basic RSS Location Management Techniques,"In this paper, we describe a radio-based localization approach that is based on the use of rotating directional antennas and a Bayesian network that combines both angle-of-arrival (AoA) and received signal strength (RSS). After describing our network, we extensively characterize the accuracy of our approach under a variety of measured signal distortion types. Next, using a combination of synthetic and trace-driven experiments, we show the impact of different signal distortions on localization performance. We found the use of directional antennas was effective at averaging out multi-path effects in indoor environments, which helped reduce the amount of training data required compared to previous approaches.","Bayesian methods,
Directional antennas,
Antenna measurements,
Distortion measurement,
Indoor environments,
Training data,
Computer network management,
Radio spectrum management,
Computer science,
Wireless networks"
Elucidating Factors that can Facilitate Veridical Spatial Perception in Immersive Virtual Environments,"Enabling veridical spatial perception in immersive virtual environments (IVEs) is an important yet elusive goal, as even the factors implicated in the often-reported phenomenon of apparent distance compression in HMD-based IVEs have yet to be satisfactorily elucidated. In recent experiments (Interrante et al., 2006), we have found that participants appear less prone to significantly underestimate egocentric distances in HMD-based IVEs, relative to in the real world, in the special case that they unambiguously know, through first-hand observation, that the presented virtual environment is a high fidelity 3D model of their concurrently occupied real environment. We had hypothesized that this increased veridicality might be due to participants having a stronger sensation of 'presence' in the IVE under these conditions of co-location, which state of mind leads them to act on their visual input in the IVE similarly as they would in the real world (the presence hypothesis). However, alternative hypotheses are also possible. Primary among these is the visual calibration hypothesis: participants could be relying on metric information gleaned from their exposure to the real environment to calibrate their judgments of sizes and distances in the matched virtual environment. It is important to disambiguate between the presence and visual calibration hypotheses because they suggest different directions for efforts to facilitate veridical distance perception in general (non-co-located) IVEs. In this paper, we present the results of an experiment that seeks novel insight into this question. Using a mixed within- and between-subjects design, we compare participants' relative ability to accurately estimate egocentric distances in three different virtual environment models: one that is an identical match to the occupied real environment; one in which each of the walls in our virtual room model has been surreptitiously moved ~10% inward towards the center of the room; and one in which each of the walls has been surreptitiously moved ~10% outwards from the center of the room. If the visual calibration hypothesis holds, then we should expect to see a degradation in the accuracy of peoples' distance judgments in the surreptitiously modified models, manifested as an underestimation of distances when the IVE is actually larger than the real room and as an overestimation of distances when the IVE is smaller. However, what we found is that distances were significantly underestimated in the virtual environment relative to in the real world in each of the surreptitiously modified room environments, while remaining reasonably accurate (consistent with our previous findings) in the case of the faithfully size-matched room environment. In a post-test survey, participants in each of the three room size conditions reported equivalent subjective levels of presence and did not indicate any overt awareness of the room size manipulation","Virtual environment,
Calibration,
Computer graphics,
Displays,
Computer science,
Computer architecture,
Degradation,
Chromium,
Virtual reality,
USA Councils"
Sensor Network-On-Chip,"In this paper, we present the sensor network-on-a-chip (SNOC) paradigm for designing robust and energy-efficient systems-on-a-chip (SOC). In this paradigm, computation in the presence of nanometer non-idealities such as process variations, leakage and noise is viewed as an estimation problem. Robust statistical signal processing theory is then employed to recover the performance of the system in the presence of errors especially timing errors. We apply this framework to design an energy-efficient and robust PN-code acquisition system for the wireless CDMA2000 standard. Simulations in IBM's 130 nm CMOS process technology demonstrate up to 30% power savings compared to the conventional architecture for a detection probability of PD = 0.5.","Network-on-a-chip,
Energy efficiency,
Noise robustness,
Hardware,
Computer networks,
Temperature sensors,
Sensor systems,
Signal processing,
CMOS process,
CMOS technology"
Matrix-Structural Learning (MSL) of Cascaded Classifier from Enormous Training Set,"Aiming at the problem when both positive and negative training set are enormous, this paper proposes a novel matrix-structural learning (MSL) method, as an extension to Viola and Jones' cascade learning method for object detection. Briefly speaking, unlike Viola and Jones' method that learn linearly by bootstrapping only negative samples, the proposed MSL method bootstraps both positive and negative samples in a matrix-like structure. Moreover, an accumulative way is further presented to improve the training efficiency of MSL by inheriting features learned previously during training procedure. The proposed method is evaluated on face detection problem. On a positive set containing 230000 face samples, only 12 hours are needed on a common PC with a 3.20 GHz Pentium IV processor to learn a classifier with false alarm rate less than 1/1000000. What's more, the accuracy of the learned detector exceeds the state-of-the-art results on the CMU+MIT frontal face test set.","Face detection,
Electronic switching systems,
Detectors,
Content addressable storage,
Object detection,
Testing,
Intelligent robots,
Costs,
Computers,
Laboratories"
Year,,
Optimal Hybrid RF-Wireless Optical Communication for Maximum Efficiency and Reliability,"Free space optical (FSO) communication systems provide tremendous potential for low-cost time-constrained high-bandwidth connectivity in a variety of network scenarios. However, these communication systems are highly unreliable due to the variations in the atmospheric channel thus making carrier grade reliabilities very difficult. Previous solutions suggest the use of a backup RF channel in conjunction with the FSO channel. The transmitted message is also duplicated on the RF channel allowing data recovery during the failure of the optical wireless channel. However, this leads to a wastage of the bandwidth of the RF channel and may sometimes also lead to channel ""flapping"". In this paper, we propose a novel coding mechanism that optimally achieves the capacity of the combined FSO and RF channels and still provides carrier grade (99.999%) reliabilities in the FSO link. The proposed mechanism uses nonuniform and rate-compatible LDPC codes to achieve the desired reliability and capacity limits. By avoiding data duplication, we preserve to a great extent the crucial security benefit of the FSO communication. Using simulations, we provide initial results on the performance of these codes to support our claims. In particular, we show that, more than two orders of magnitude improvements in the bit error rates and many-fold decrease in the outage probabilities are possible when using hybrid channel codes.","Optical fiber communication,
Radio frequency,
Availability,
Bandwidth,
Data security,
Atmospheric waves,
Data communication,
Redundancy,
Communication system security,
Wireless communication"
Soft Real-Time Scheduling on Performance Asymmetric Multicore Platforms,"This paper discusses an approach for supporting soft real-time periodic tasks in Linux on performance asymmetric multicore platforms (AMPs). Such architectures consist of a large number of processing units on one or several chips, where each processing unit is capable of executing the same instruction set at a different performance level. We discuss deficiencies of Linux in supporting periodic real-time tasks, particularly when cores are asymmetric, and how such deficiencies were overcome. We also investigate how to provide good performance for non-real-time tasks in the presence of a real-time workload. We show that this can be done by using deferrable servers to explicitly reserve a share of each core for non-real-time tasks. This allows non-real-time tasks to have priority over real-time tasks when doing so will not cause timing requirements to be violated, thus improving non-real-time response times. Experiments show that even small deferrable servers can have a dramatic impact on non-real-time task performance","Multicore processing,
Linux,
Real time systems,
Operating systems,
Multimedia systems,
Processor scheduling,
Computer science,
Computer architecture,
Timing,
Delay"
A Hybrid Disk-Aware Spin-Down Algorithm with I/O Subsystem Support,"To offset the significant power demands of hard disk drives in computer systems, drives are typically powered down during idle periods. This saves power, but accelerates duty cycle consumption, leading to earlier drive failure. Hybrid disks with a small amount of non-volatile flash memory (NVCache) are coming on the market. We present four I/O subsystem enhancements that exploit the characteristics of hybrid disks to improve system performance: 1) artificial idle periods, 2) a read-miss cache, 3) anticipatory spin-up, and 4) NVCache write-throttling. These enhancements reduce power consumption, duty cycling, NVCache block-erase impact, and the observed spinup latency of a hybrid disk, resulting in lower power consumption, greater reliability, and faster I/O.","Energy consumption,
Power demand,
Hard disks,
Drives,
Acceleration,
Nonvolatile memory,
Flash memory,
System performance,
Delay,
Power system reliability"
On the Timing Uncertainty in Delay-Line-based Time Measurement Applications Targeting FPGAs,This paper addresses important performance issues in delay-line-based timing applications targeting FPGA devices. The circuit under test is a TDC circuit implemented on a low-cost FPGA from XILINX. Various performance limitations such as uncertainty and non-uniformity in cell delays are described and corresponding optimization and improvement suggestions are made. Experimental results were obtained using ring oscillator-based test structures to inspect intra-die delay mismatches along the target FPGA's surface.,"Timing,
Delay effects,
Time measurement,
Field programmable gate arrays,
Circuit testing,
Delay lines,
Propagation delay,
Logic devices,
Clocks,
Application specific integrated circuits"
Additive Generators of Discrete Conjunctive Aggregation Operations,This paper deals with a class of conjunction-like binary operations defined on a finite totally ordered set. The concept of additive generator of a discrete conjunctive aggregation operation is introduced and we obtain a description of those operations having an additive generator by means of nonstrict Archimedean t-norms. The main goal is to give a procedure for deciding whether a conjunctive aggregation operation is additively generated or not. The problem of the existence of additive generators for discrete t-norms is also discussed.,"Additives,
Fuzzy logic,
Uncertainty,
Government,
Tin,
Mathematics,
Computer science,
Character generation,
Equations"
Dynamic Association for Load Balancing and Interference Avoidance in Multi-cell Networks,"In promising OFDMA systems, downlink signals originating from the same base station (BS) are orthogonal, while those from different BSs interfere with each other. As a consequence, inter-cell interference (ICI) becomes major performance degradation factor. Particularly, boundary users suffer from severe ICI in addition to the inherent near-far problem. To improve cell edge performances and support a more balanced data rate among all users, partial frequency reuse (PFR) and load-balancing schemes are investigated in this paper. We have formulated a utility maximization problem with network-wide proportional fairness (PF) as an objective in a multi-cell network with PFR. To solve this problem, we propose an offline optimal algorithm and also efficient online algorithms. Our online algorithms are based on simple inter/intra-handover and cell-site selection in which a metric is changed from the signal strength to the average throughput. Through extensive simulations, we demonstrate that our online algorithms can achieve network-wide PF very closely. Compared to the conventional system with a universal frequency reuse where a user is bound to the best signal strength base station, the proposed algorithms bring two types of performance gain: interference avoidance (IA) and load balancing (LB) gain. These gains improve the system performance, especially for users at the cell boundary.","Load management,
Interference,
Performance gain,
Signal to noise ratio,
Base stations,
Throughput,
Downlink,
Degradation,
Quality of service,
Bandwidth"
Situation classification for cognitive automobiles using case-based reasoning,"Driving a car in urban areas autonomously requires the ability of an in-depth analysis of the current situation. For understanding the current situation and deducing consequences for the execution of behaviors (maneuvers), higher-level reasoning about the situation has to take place. In this paper, an approach for situation interpretation for cognitive automobiles is presented. The approach relies on case-based reasoning to predict the evolvement of the current situation and to select the appropriate behavior. Case-based reasoning allows to utilize prior experiences in the task of situation assessment.","Automobiles,
Decision making,
Logic,
Knowledge based systems,
Urban areas,
Remotely operated vehicles,
Intelligent vehicles,
Data processing,
Mobile robots,
Navigation"
QoS Metrics for Evaluating Services from the Perspective of Service Providers,"In service-oriented architecture (SOA), actual quality of service (QoS) is not known until the service is executed since the internals of the service are hidden to service consumers. Hence, quality management of services before publishing is important in SOA environment. To ensure that a qualified service is published, defining metrics for evaluating the QoS is a prerequisite. Therefore, this paper proposes a set of QoS metrics for service providers especially considering the consumer's concern. We first identify unique features of SOA and define quality attributes and metrics. To show the applicability and usefulness of the QoS metrics, we perform a case study for hotel reservation service.","Service oriented architecture,
Quality of service,
ISO standards,
Runtime,
Publishing,
Web services,
IEC standards,
Contracts,
Sun,
Computer science"
An Average Link Interference-Aware Routing Protocol for Mobile Ad Hoc Networks,"The intrinsic flexibility and independence of infrastructure enable mobile ad hoc networks (MANET) to be widely used in various environments such as disaster rescue, and battlefield. Recently, researches have shown that interference can impose a significant impact on the performance of a MANET. However, there is no unified definition of interference in MANETs currently. Therefore, in this paper, we first introduce an explicit definition of interference. Based on this definition, we present a series of formulas for computing node-interference, link-interference and path-interference, and then propose an interference-aware routing protocol selecting paths with the minimum average link-interference between the source and destination. Paths found by our protocol tend to avoid the areas with high interferences, reducing the number of collisions and packet retransmissions, resulting in higher network throughput and longer lifetime. We implement our average link interference-aware routing protocol (ALIR) on dynamic source routing (DSR). The simulation results illustrate that our protocol is able to achieve better performance than DSR.","Interference,
Routing protocols,
Mobile ad hoc networks,
Communication system traffic control,
Mobile communication,
Computer science,
Throughput,
Computational modeling,
Ad hoc networks,
Broadcasting"
Teaching Advanced Features of Evolutionary Algorithms Using Japanese Puzzles,"In this paper, a method to teach advanced features of evolutionary algorithms (EAs), using a famous game known as Japanese puzzles is presented. The authors show that Japanese puzzles are constrained combinatorial optimization problems, that can be solved using EAs with different encodings, and are challenging problems for EAs. Other features, such as special operators and local search heuristics and its hybridization with genetic algorithms, can also be taught using these puzzles. The authors report an experience using this method in a course taught at the Universidad de Alcalaacute, Madrid, Spain","genetic algorithms,
education"
Two-phase method to solve fuzzy quadratic programming problems,"Quadratic programming problems are of up most importance in a variety of relevant practical fields, as e.g., portfolio selection. This work presents and develops an original and novel fuzzy sets based method that solves a class of quadratic programming problems with vagueness in the set of constraints. The method uses two phases to solve fuzzy quadratic programming problems, which eventually can be considered in the portfolio context. In the first phase we parametrize the fuzzy problem in several classical alpha-problems with different cutting levels. In the second phase each of these alpha-problems is solved by using conventional solving techniques. The final fuzzy solution to the former problem can be obtained by integrating all of these particular alpha-solutions. Some illustrative numerical examples illustrating the solution approach are solved and analyzed to show the efficiency of this proposed method.","Quadratic programming,
Fuzzy logic,
Fuzzy sets,
Linear programming,
Decision support systems,
Portfolios,
Functional programming,
Mathematical programming,
Vectors,
Symmetric matrices"
Space Efficient Streaming Algorithms for the Maximum Error Histogram,"We propose new algorithms for constructing maximum error (L∞) histograms in the data stream model. Our first algorithm (Min-Merge) achieves the following performance guarantee: using O(B) memory, it constructs a 2B-bucket histogram whose approximation error is at most the error of the optimal B-bucket histogram. Our second algorithm (Min-Increment) achieves a (1 + ε)-approximation of a B-bucket histogram using O(ε-1 B log U) space, where U is the size of the domain for data values. The memory requirements of these algorithms are a significant improvement over the previous best schemes for constructing near-optimal histograms in the data stream model, making them ideal for data summary applications where memory is at a premium, such as wireless sensor networks. Our Min-Increment algorithm also extends to the sliding window model without any asymptotic increase in space. Finally, using synthetic and real-world data, we show that our algorithms are indeed as space-efficient in practice as their theoretical analysis predicts - compared to previous best algorithms, they require two or more orders of magnitude less memory for the same approximation error.","Histograms,
Piecewise linear approximation,
Approximation algorithms,
Algorithm design and analysis,
Monitoring,
Piecewise linear techniques,
Computer errors,
Approximation error,
Wireless sensor networks,
Computer networks"
A Reverse Engineering Tool for Extracting Protocols of Networked Applications,"Networked applications play a significant role in today's interconnected world. It is important for software engineers to be able to understand and model the behavior of these applications during software maintenance. Some networked applications use legacy protocols in ways they were not intended to be used. Others use newly created protocols that are designed in an ad hoc way to simply meet requirements. Protocol usage needs to be understood so that applications can be effectively tested and maintained. In this paper we propose the first step in achieving this goal by presenting a dynamic analysis tool, called PEXT, that can reverse engineer a networked application's underlying protocol by analyzing a collection of packets captured from the application at runtime. We demonstrate the effectiveness of this tool by extracting a protocol from an FTP application, and comparing the extracted protocol to the documented FTP protocol defined in RFC 959.","Reverse engineering,
Application software,
Testing,
Access protocols,
Network servers,
Web server,
Software maintenance,
File servers,
Computer science,
Educational institutions"
Identification and Localization of Data Types within Large-Scale File Systems,"This research examines the application of statistical analysis techniques for the identification of data types embedded within a file to assist analysts with the location of data, potentially relevant to criminal activity. The results show that the statistical analysis can effectively aid identification of the types of data embedded in a file and the approximate location of these data types. This analysis identifies component data types, irrespective of the type of file being analyzed. When applied, this technique will allow analysts to more effectively and efficiently locate relevant data on a hard drive, especially on today's particularly large hard drives","Large-scale systems,
File systems,
Statistical analysis,
Forensics,
Information analysis,
Computer science,
Application software,
Data analysis,
Drives,
Costs"
Adaptive Noise Cancellation Using Accelerometers for the PPG Signal from Forehead,"For the upcoming ubiquitous computing environments in the u-health areas involve the measurement of physiological signals in the daily life. However, the measurement of those signals, such as the photoplethysmography (PPG), and the electrocardiogram (ECG), requires being still tight during the measurement in order to get the accurate result preventing noises caused by the casual movement. In this paper, we propose a method to obtain the accurate physiological signals in the situation where the little movement is allowed. By measuring PPG and motion signals at the forehead during in motion, we calibrate the distorted PPG signal with the motion signals. We show that the calibrated PPG signal is accurate enough by comparing with the result that is measured at the finger without any movement.","Noise cancellation,
Accelerometers,
Forehead,
Noise measurement,
Distortion measurement,
Ubiquitous computing,
Area measurement,
Electrocardiography,
Working environment noise,
Motion measurement"
Parallel Java: A Unified API for Shared Memory and Cluster Parallel Programming in 100% Java,"Parallel Java is a parallel programming API whose goals are (1) to support both shared memory (thread-based) parallel programming and cluster (message-based) parallel programming in a single unified API, allowing one to write parallel programs combining both paradigms; (2) to provide the same capabilities as OpenMP and MPI in an object oriented, 100% Java API; and (3) to be easily deployed and run in a heterogeneous computing environment of single-core CPUs, multi-core CPUs, and clusters thereof. This paper describes parallel Java's features and architecture; compares and contrasts parallel Java to other Java-based parallel middleware libraries; and reports performance measurements of parallel Java programs.","Java,
Parallel programming,
Libraries,
Message passing,
Concurrent computing,
Middleware,
Parallel processing,
Computer architecture,
Measurement,
Scientific computing"
Dynamic Area Coverage using Faulty Multi-Agent Swarms,"We consider the problem of distributed coverage of an unknown two-dimensional environment using a swarm of mobile mini-robots. In contrast to previous approaches for robotic area coverage, we assume that each robot (agent) in our system is limited in its communication range and memory capacity. Agents are also susceptible to sensor noise while communicating with other agents, and, can be subject to transient or permanent failures. The objective of the agents is to cover the entire environment while reducing the coverage time and the redundancy in the area covered by different agents. First, we describe our distributed coverage algorithm where each agent uses a local heuristic based on Manhattan distances and the information gained from other agents at each step to decide on its next action (movement). We then describe and analyze the fault model of our agents and show that the local heuristic used by the agents deteriorate linearly as the communication noise increases. Finally, we verify the performance of our system empirically within a simulated environment and show that the system is able to scale efficiently in the number of robots and size of the environment, and, determine the effect of communication faults and robot failures on the system performance.","Robot sensing systems,
Working environment noise,
Redundancy,
System performance,
Orbital robotics,
Multiagent systems,
Sensor systems,
Intelligent agent,
Computer science,
USA Councils"
Correlating Social Interactions to Release History during Software Evolution,"In this paper, we propose a method to reason about the nature of software changes by mining and correlating discussion archives. We employ an information retrieval approach to find correlation between source code change history and history of social interactions surrounding these changes. We apply our correlation method on two software systems, LSEdit and Apache Ant. The results of these exploratory case studies demonstrate the evidence of similarity between the content of free-form text emails among developers and the actual modifications in the code. We identify a set of correlation patterns between discussion and changed code vocabularies and discover that some releases referred to as minor should instead fall under the major category. These patterns can be used to give estimations about the type of a change and time needed to implement it.","History,
Vocabulary,
Software systems,
Open source software,
Computer science,
Application software,
Information retrieval,
Correlation,
Energy management,
Power system management"
Co-Evolving Influence Map Tree Based Strategy Game Players,"We investigate the use of genetic algorithms to evolve AI players for real-time strategy games. To overcome the knowledge acquisition bottleneck found in using traditional expert systems, scripts, or decision trees we evolve players through co-evolution. Our game players are implemented as resource allocation systems. Influence map trees are used to analyze the game-state and determine promising places to attack, defend, etc. These spatial objectives are chained to non-spatial objectives (train units, build buildings, gather resources) in a dependency graph. Players are encoded within the individuals of a genetic algorithm and co-evolved against each other, with results showing the production of strategies that are innovative, robust, and capable of defeating a suite of hand-coded opponents","Genetic algorithms,
Resource management,
Computational intelligence,
Knowledge acquisition,
Humans,
Buildings,
Computer science,
Genetic engineering,
Real time systems,
Expert systems"
An Efficient Method for Segmentation of MRI Spine Images,"The study of the segmentation of MRI Spine Image is of crucial importance for computer aided medical image identifying and clinical studies of neurological pathology. Computer characterization of a vertebra or disk is of limited clinical value if that structure cannot be accurately segmented and identified. However, Manual operations of tracking these structures are so tedious that automated method of spine tracking and segmentation are in high demand. In this paper, a vertebral disks segmentation method is proposed. The method can locate and label the disks through locating the spinal cord with Hough Transform. The efficiency of the proposed method is demonstrated by experiments using real MR images provided by College of Medicine, University of Cincinnati.","Image segmentation,
Magnetic resonance imaging,
Spine,
Biomedical imaging,
Educational institutions,
Spinal cord,
Medical diagnostic imaging,
Bone diseases,
Degenerative diseases,
Surgery"
Performance Evaluation of Scheduling Policies for Volunteer Computing,"BOINC, a middleware system for volunteer computing, allows hosts to be attached to multiple projects. Each host periodically requests jobs from project servers and executes the jobs. This process involves three interrelated policies: 1) of the runnable jobs on a host, which to execute? 2) when and from what project should a host request more work? 3) what jobs should a server send in response to a given request? 4) How to estimate the remaining runtime of a job? In this paper, we consider several alternatives for each of these policies. Using simulation, we study various combinations of policies, comparing them on the basis of several performance metrics and over a range of parameters such as job length variability, deadline slack, and number of attached projects.","Processor scheduling,
Computer networks,
Distributed computing,
Network servers,
Central Processing Unit,
Grid computing,
Middleware,
Application software,
Time sharing computer systems,
Runtime"
Test Case Prioritization Based on Varying Testing Requirement Priorities and Test Case Costs,"Test case prioritization is an effective and practical technique in regression testing. It schedules test cases in order of precedence that increases their ability to meet some performance goals, such as code coverage, rate of fault detection. In previous work, the test case prioritization techniques and metrics usually assumed that testing requirement priorities and test case costs are uniform. In this paper, basing on varying testing requirement priorities and test case costs, we present a new, general test case prioritization technique and an associated metric. The case study illustrates that the rate of ""units-oftesting-requirement-priority-satisfied- per-unit-test-case-cost"" can be increased, and then the testing quality and customer satisfaction can be improved.","Costs,
Software testing,
Fault detection,
Software quality,
Life testing,
Programming,
Feedback,
Computer science,
Processor scheduling,
Customer satisfaction"
Iterative approach to indicator-based multiobjective optimization,"An emerging trend in the design of evolutionary multiobjective optimization algorithms is to directly optimize a quality indicator of non-dominated solution sets such as the hypervolume measure. Some algorithms have been proposed to search for a set of a pre-specified number of non-dominated solutions that maximizes the given quality indicator. In this paper, we propose an iterative approach to indicator-based evolutionary multiobjective optimization. The main feature of our approach is that only a single solution is obtained by its single run. Thus multiple runs are needed to find a solution set. In each run, our approach searches for a solution with the maximum contribution to the hypervolume of the solution set obtained by its previous runs. We discuss several issues related to the implementation of such an iterative approach.",
Mind the gap: Expanding communication options in decentralized discrete-event control,"Frameworks that incorporate communication into decentralized supervisory control theory address the following problem: find locations in the evolution of the plant behavior where some supervisors send information so that a supervisor that was unable to make the correct control decision prior to receiving external information, is now capable of making the correct control decision. Proposed solutions to this problem identify an earliest and a latest placement where such communication results in the synthesis of a correct control solution. In addition to a first and last communication opportunity, there may be a selection of intermediate possibilities where communication would produce the correct control solution. We present a computable procedure to identify a broader range of suitable communication locations.","Communication system control,
Supervisory control,
Protocols,
Law,
Legal factors,
Automatic control,
Control systems,
Distributed control,
USA Councils,
Formal languages"
Bitstream Decompression for High Speed FPGA Configuration from Slow Memories,"In this paper, we present hardware decompression accelerators for bridging the gap between high speed FPGA configuration interfaces and slow configuration memories. We discuss different compression algorithms suitable for a decompression on FPGAs as well as on CPLDs with respect to the achievable compression ratio, throughput, and hardware overhead. This leads to various decompressor implementations with one capable to decompress at high data rates of up to 400 megabytes per second while only requiring slightly more than a hundred look-up tables. Furthermore, we present a sophisticated configuration bitstream benchmark.",
An Automated Framework for Validating Firewall Policy Enforcement,"The implementation of network security devices such as firewalls and IDSs are constantly being improved to accommodate higher security and performance standards. Using reliable and yet practical techniques for testing the functionality of firewall devices particularly after new filtering implementation or optimization becomes necessary to assure required security. Generating random traffic to test the functionality of firewall matching is inefficient and inaccurate as it requires an exponential number of test cases for a reasonable coverage. In addition, in most cases the policies used during testing are limited and manually generated representing fixed policy profiles. In this paper, we present a framework for automatic testing of the firewall policy enforcement or implementation using efficient random traffic and policy generation techniques. Our framework is a two-stage architecture that provides a satisfying coverage of the firewall operational states. A large variety of policies are randomly generated according to custom profiles and also based on the grammar of the access control list. Testing packets are then generated intelligently and proportional to the critical regions of the generated policies to validate the firewall enforcement for such policies. We describe our implementation of the framework based on Cisco IOS, which includes the policy generation, test cases generation, capturing and analyzing firewall out put, and creating detailed test reports. Our evaluation results show that the automated security testing is not only achievable but it also offers a dramatically higher degree of confidence than random or manual testing.","Filtering,
Automatic testing,
Computer security,
Information security,
Design optimization,
Computer science,
Information systems,
Telecommunication standards,
Random number generation,
Telecommunication traffic"
Optimal Energy Balanced Data Gathering in Wireless Sensor Networks,"Unbalanced energy consumption is an inherent problem in wireless sensor networks where some nodes may be overused and die out early, resulting in a short network lifetime. In this paper, we investigate the problem of balancing energy consumption for data gathering sensor networks. Our key idea is to exploit the tradeoff between hop-by-hop transmission and direct transmission to balance energy dissipation among sensor nodes. By assigning each node a transmission probability which controls the ratio between hop-by-hop transmission and direct transmission, we formulate the energy consumption balancing problem as an optimal transmission probability allocation problem. We discuss this problem for both chain networks and general networks. Moreover, we present the solution to compute the optimal number of sections in terms of maximizing the network lifetime. Numerical results demonstrate that our methods outperform the traditional hop-by-hop and direct transmission schemes and achieve significant lifetime extension especially for dense sensor networks.",
A Network-flow based Integral Optimal Algorithm for Lexicographic Maximum Lifetime Routing in Wireless Sensor Networks,"Longevity is one the key design goals in the wireless sensor networks. In many applications, longevity of all the sensor nodes in the network is equally important. Therefore, it is imperative to design network protocols that would keep the maximum number of nodes alive for longest possible duration. Towards this goal we study the lexicographic maximum lifetime (Lex-max-life) routing scheme. The objective of Lex-max-life routing is to maximize the time until the lirst set of sensor nodes deplete their battery energies (among all the nodes), then maximize the depletion time for the second set of sensor nodes if the depletion time for the first set of nodes is as long as possible, and then maximize the depletion time for the third set of nodes if the depletion times for the first and second set of nodes are as long as possible and so on. In our previous work by M. Patel et al. (2006), we have shown that if sensor nodes are equipped with non-adaptive transmitters then the Lex-max-life routing problem can be reduced to a min-cost flow or a convex-cost flow problem using a novel cost scaling technique. Hence, we obtain an integral optimal solution in polynomial time. However, the cost scaling technique is not suitable for large problem instances because the cost numbers grow very fast. In this paper, we propose a novel vectorial representation of link costs. The Lex-max-life routing problem is solved by a vectorial version of min-cost flow or convex-cost flow algorithm which (i) Improves running time complexity and (ii) Circumvents the number explosion phenomenon. Thus, developed algorithm is computationally very efficient, scalable and easy to implement.","Routing,
Wireless sensor networks,
Sensor phenomena and characterization,
Base stations,
Batteries,
Costs,
Energy consumption,
Design engineering,
Computer science,
Algorithm design and analysis"
Analysis of Privacy Disclosure in DNS Query,"When a DNS (domain name system) client needs to look up a name, it queries DNS servers to resolve the name on the Internet. The query information from the client was passed through one or more DNS servers. While useful, in the whole query transmission, we say it can leak potentially sensitive information: what a client wants to connect to, or what the client is always paying attention to. From the definition, the privacy problem is to prove that none of the private data can be inferred from the information which is made public. We first analyzed the complete DNS query process now in use; then, from each step of the DNS query process, we discussed the privacy disclosure problem in each step of the query: client side, query transmission process and DNS server side. Finally, we proposed a simple and flexible privacy-preserving query scheme ""range query"", which could maximally decrease privacy disclosure in the whole DNS query process. And we also discuss efficiency and implementation on the range query.","Query processing,
Internet,
Data privacy,
Web server,
Power system protection,
Computer science,
Domain Name System,
Information analysis,
Surveillance,
Instruments"
Notes for a Collaboration: On the Design of a Wiki-type Educational Video Lecture Annotation System,"We describe a collaborative annotation system for the production of rich media e-learning contents. Our system exploits a wiki-like interface that allows cooperative users to enrich didactical multimedia lectures with additional information, such as captions, annotations and comments. Before delivering them to users, these additional contents are automatically adapted and integrated with video lectures based on user profiles and requests. This way, the additional contents collaboratively provided by users not only represent an effective alternative that students may follow to better understand the lecture, but also allow for a fine-grained customization of the didactic material. This is an important aspect which represents a step forward towards the Web(2.0)-ification of e-learning technologies. An experimental assessment shows the viability of our approach.",
Wide-Area Egomotion Estimation from Known 3D Structure,"Robust egomotion recovery for extended camera excursions has long been a challenge for machine vision researchers. Existing algorithms handle spatially limited environments and tend to consume prohibitive computational resources with increasing excursion time and distance. We describe an egomotion estimation algorithm that takes as input a coarse 3D model of an environment, and an omnidirectional video sequence captured within the environment, and produces as output a reconstruction of the camera's 6-DOF egomotion expressed in the coordinates of the input model. The principal novelty of our method is a robust matching algorithm that associates 2D edges from the video with 3D line segments from the input model. Our system handles 3-DOF and 6-DOF camera excursions of hundreds of meters within real, cluttered environments. It uses a novel prior visibility analysis to speed initialization and dramatically accelerate image-to-model matching. We demonstrate the method's operation, and qualitatively and quantitatively evaluate its performance, on both synthetic and real image sequences.",
Hiding Location Information from Location-Based Services,"In many existing location-based services, a service provider becomes aware of the location of its customers and can, maybe inadvertently, leak this information to unauthorized entities. To avoid this information leak, the provider should be able to offer its services such that the provider does not learn any information about its customers' location. We present an architecture that provides this property and show that the architecture is powerful enough to support existing location- based services. Our architecture exploits trusted computing and private information retrieval. With the help of trusted computing, we ensure that a location-based service operates as expected by a customer and that information about the customer's location becomes inaccessible to a location-based service upon a compromise of the service. With the help of private information retrieval, we avoid that a service provider learns a customer's location by observing which of its location-specific information is being accessed.","Cellular phones,
Computer architecture,
Information retrieval,
Privacy,
Computer science,
Target tracking,
Software,
Computer bugs,
Service oriented architecture,
Cryptography"
Ad-hoc Localization in Urban District,"In this paper, we present a range-free ad-hoc localization algorithm called UPL (Urban Pedestrians Localization), for positioning mobile nodes in urban district. The design principle of UPL is two-fold. (1) We assume that location seeds are deployed sparsely due to deployment-cost constraints. Thus most mobile nodes cannot expect to meet these location seeds frequently. Therefore, each mobile node in UPL relies on location information received from its neighboring mobile nodes in order to estimate its area of presence. The area of presence of each mobile node becomes inexact as it moves, but it is helpful to reduce the areas of presence of the other mobile nodes. (2) To predict the area of presence of mobile nodes accurately under mobility, we employ information about obstacles such as walls, and present an algorithm to calculate the movable areas of mobile nodes considering obstacles. This also helps to reduce each node's area of presence. The experimental results have shown that by the above two ideas UPL could achieve 8 m positioning error in average with 10 m of radio range.","Peer to peer computing,
Cities and towns,
Mobile ad hoc networks,
Hardware,
Communications Society,
Information science,
Radio navigation,
Global Positioning System,
Region 1,
Collaboration"
A MIMO Mobile-to-Mobile Channel Model Derived from a Geometric Street Scattering Model,"In this paper, a frequency-nonselective multi-input multi-output (MIMO) channel model for mobile-to-mobile (M2M) communications is presented. This channel model is derived from a new geometric street scattering model. The street model consists of an infinite number of scatterers lying on the left and/or right hand side of the street. It is assumed that both the transmitter and the receiver are moving. The channel model takes into account the exact relationship between the angle of arrival (AOA) and the angle of departure (AOD). Based on this relationship, the statistical properties of the reference model are studied. Analytical solutions are provided for the three-dimensional (3D) space-time cross-correlation function (CCF), the 2D space CCF, and the temporal autocorrelation function (ACF). Starting from the reference model, where the number of scatterers is infinite, a deterministic simulation model is developed. The latter can easily be implemented on a computer, allowing the emulation of the statistical fading behaviour with a great deal of precision. Finally, simulation results show the excellent correspondence between the temporal and spatial correlation properties of the channel simulator and the reference model.","Solid modeling,
MIMO,
Scattering,
Computational modeling,
Frequency,
Mobile communication,
Transmitters,
Autocorrelation,
Emulation,
Fading"
The Impact of the Mobility Model on Delay Tolerant Networking Performance Analysis,"Delay tolerant networks (DTNs) are a class of networks that experience frequent and long-duration partitions due to sparse distribution of nodes. The topological impairments experienced within a DTN pose unique challenges for designing effective DTN routing protocols. For an important class of DTNs nodes depend on their mobility to carry message to the destination. It is therefore essential to understand the impact of commonly-used mobility models on performance analysis of DTN routing schemes. This paper shows how the underlying statistical properties of a frequently used mobility model can be used to enhance our understanding of DTN simulation results. Using these results we also analyze several DTN routing techniques, including direct transmission, Spray and Wait, and a novel core-assisted routing schemes","Disruption tolerant networking,
Performance analysis,
Mobile ad hoc networks,
Spraying,
Computer science,
Routing protocols,
Wildlife,
Buffer storage,
Bandwidth,
Power supplies"
A Rate-Distortion Optimization Algorithm for Rate Control in H.264,"This paper presents a novel rate-distortion (R-D) joint optimization rate control (RC) algorithm for H.264 encoding. For RC in H.264, one of the most important topics is to model the R-D characteristics accurately. To achieve this, an efficient linear model is proposed to model the distortion-quantization (D-Q) relation. With the proposed linear D-Q model, a closed-form solution is derived to calculate the optimal quantization parameter for encoding each macroblock (MB). The proposed RC algorithm can be applied to both P-frames and B-frames. It is shown by experimental results that the proposed algorithm can control the bit rates accurately with the R-D performance better than that of the RC algorithm JVT-G012 implemented in the H.264 reference software JM9.5.","Rate-distortion,
Encoding,
Quantization,
Bit rate,
Radio control,
Software algorithms,
Video coding,
Lagrangian functions,
Motion estimation,
Statistics"
Transmission Time-Based Mechanism to Detect Wormhole Attacks,"Important applications of Wireless Ad Hoc Networks make them very attractive to attackers, therefore more research is required to guarantee the security for Wireless Ad Hoc Networks. In this paper, we proposed a transmission time based mechanism (TTM) to detect wormhole attacks - one of the most popular & serious attacks in Wireless Ad Hoc Networks. TTM detects wormhole attacks during route setup procedure by computing transmission time between every two successive nodes along the established path. Wormhole is identified base on the fact that transmission time between two fake neighbors created by wormhole is considerably higher than that between two real neighbors which are within radio range of each other. TTM has good performance, little overhead and no special hardware is required.","Mobile ad hoc networks,
Time to market,
Computer networks,
Routing protocols,
Computer science,
Cryptography,
Proposals,
Hardware,
Intrusion detection,
Medical services"
Automatic resource specification generation for resource selection,"With an increasing number of available resources in large-scale distributed environments, a key challenge is resource selection. Fortunately, several middleware systems provide resource selection services. However, a user is still faced with a difficult question: ""What should I ask for?"" Since most users end up using naïve and suboptimal resource specifications, we propose an automated way to answer this question. We present an empirical model that given a workflow application (DAG-structured) generates an appropriate resource specification, including number of resources, the range of clock rates among the resources, and network connectivity. The model employs application structure information as well as an optional utility function that trades off cost and performance. With extensive simulation experiments for different types of applications, resource conditions, and scheduling heuristics, we show that our model leads consistently to close to optimal application performance and often reduces resource usage.",
Comparative Study of Supervised Machine Learning Techniques for Intrusion Detection,"Intrusion detection is an effective approach for dealing with various problems in the area of network security. This paper presents a comparative study of using supervised probabilistic and predictive machine learning techniques for intrusion detection. Two probabilistic techniques Naive Bayes and Gaussian and two predictive techniques decision tree and random forests are employed. Different training datasets constructed from the KDD99 dataset are employed for training. The ability of each technique for detecting four attack categories (DoS,Probe,R2L and U2R) have been compared. The statistical results to show the sensitivity of each technique to the population of attacks in a dataset have also been reported. We compare the performance of the techniques and also investigate the robustness of each technique by calculating their standard deviations with respect to the detection rate of each attack category.","Machine learning,
Intrusion detection,
Decision trees,
Pattern matching,
Computer security,
Machine learning algorithms,
Laboratories,
Computer science,
Data security,
Robustness"
A Localization Algorithm for Mobile Robots in RFID System,"This paper presents an efficient localization algorithm for an indoor mobile robot using the RFID system. The mobile robot carries a RFID reader at the bottom of the mobile robot, which reads the RFID tags on the floor to localize the mobile robot. Each RFID tag stores its own unique position which is used to calculate the position of the mobile robot. In this paper, a SLAM (simultaneously localization and mapping) algorithm is proposed to extend into RFID system for positioning and localization. Though the experiments, this algorithm has been proved to successfully achieve the mobile object position estimation and tracking in 2D range by using known RFID data.",
Digitally Evolving Models for Dynamically Adaptive Systems,"Developing a Dynamically Adaptive System (DAS) requires a developer to identify viable target systems that can be adopted by the DAS at runtime in response to specific environmental conditions, while satisfying critical properties. This paper describes a preliminary investigation into using digital evolution to automatically generate models of viable target systems. In digital evolution, a population of self-replicating computer programs exists in a user-defined computational environment and is subject to instruction-level mutations and natural selection. These ""digital organisms"" have no built-in ability to generate a model - each population begins with a single organism that only has the ability to self-replicate. In a case study, we demonstrate that digital evolution can be used to evolve known state diagrams and to further evolve these diagrams to satisfy system critical properties. This result shows that digital evolution can be used to aid in the discovery of the viable target systems of a DAS.","Adaptive systems,
Organisms,
Evolution (biology),
Computer aided instruction,
Genetic mutations,
Unified modeling language,
Space exploration,
Computer science,
Runtime environment,
Condition monitoring"
Integrating Temporal Logic as a State-Based Specification Language for Discrete-Event Control Design in Finite Automata,"This paper presents and analyzes a correct and complete translation algorithm that converts a class of propositional linear-time temporal-logic (PTL) formulae to deterministic finite (-trace) automata. The translation algorithm is proposed as a specification interface for finitary control design of discrete-event systems (DESs). While there has been a lot of computer science research that connects PTL formulae to omega-automata, there is relatively little prior work that translates state-based PTL formulae in the context of a finite-state DES model, to event-based finite automata-the formalism on which well-established control synthesis methods exist. The proposed translation allows control requirements to be more easily described and understood in temporal logic, widely recognized as a useful specification language for its intuitively appealing operators that provide the natural-language expressiveness and readability needed to express and explain these requirements. Adding such a translation interface could therefore effectively combine specifiability and readability in temporal logic with prescriptiveness and computability in finite automata. The former temporal-logic features support specification while the latter automata features support the prescription of DES dynamics and algorithmic computations. A practical implementation of the interface has been developed, providing an enabling technology for writing readable control specifications in PTL that it translates for discrete-event control synthesis in deterministic finite automata. Two application examples illustrate the use of the proposed temporal-logic interface. Practical implications of the complexity of the translation algorithm are discussed.","Logic design,
Specification languages,
Control design,
Automata,
Automatic control,
Algorithm design and analysis,
Discrete event systems,
Computer science,
Context modeling,
Control system synthesis"
Compile-time decided instruction cache locking using worst-case execution paths,"Caches are notorious for their unpredictability. It is difficult or even impossible to predict if a memory access results in a definite cache hit or miss. This unpredictability is highly undesired for real-time systems. The Worst-Case Execution Time (WCET) of a software running on an embedded processor is one of the most important metrics during real-time system design. The WCET depends to a large extent on the total amount of time spent for memory accesses. In the presence of caches, WCET analysis must always assume a memory access to be a cache miss if it can not be guaranteed that it is a hit. Hence, WCETs for cached systems are imprecise due to the overestimation caused by the caches. Modern caches can be controlled by software. The software can load parts of its code or of its data into the cache and lock the cache afterwards. Cache locking prevents the cache's contents from being flushed by deactivating the replacement. A locked cache is highly predictable and leads to very precise WCET estimates, because the uncertainty caused by the replacement strategy is eliminated completely. This paper presents techniques exploring the lockdown of instruction caches at compile-time to minimize WCETs. In contrast to the current state of the art in the area of cache locking, our techniques explicitly take the worst-case execution path into account during each step of the optimization procedure. This way, we can make sure that always those parts of the code are locked in the I-cache that lead to the highest WCET reduction. The results demonstrate that WCET reductions from 54% up to 73% can be achieved with an acceptable amount of CPU seconds required for the optimization and WCET analyses themselves.","Benchmark testing,
Optimization,
Algorithm design and analysis,
Context,
Software,
Transform coding,
Real time systems"
An Efficient Block-by-Block SVD-Based Image Watermarking Scheme,"This paper presents a block based digital image watermarking that is dependent on the mathematical technique of singular value decomposition (SVD). Traditional SVD watermarking already exists for watermark embedding on the image as a whole. In the proposed approach, the original image is divided into blocks, and then the watermark is embedded in the singular values (SVs) of each block separately. This segmentation process and watermarking on a block-by-block basis makes the watermark more robust to the attacks such as noise, compression, cropping and other attacks as the results reveal. Watermark detection is implemented by extracting the watermark from the SVs of the watermarked blocks. Extracting the watermark from one block at least is enough to ensure the existence of the watermark.",
Stereo-based Markerless Human Motion Capture for Humanoid Robot Systems,"In this paper, we present an image-based markerless human motion capture system, intended for humanoid robot systems. The restrictions set by this ambitious goal are numerous. The input of the system is a sequence of stereo image pairs only, captured by cameras positioned at approximately eye distance. No artificial markers can be used to simplify the estimation problem. Furthermore, the complexity of all algorithms incorporated must be suitable for real-time application, which is maybe the biggest problem when considering the high dimensionality of the search space. Finally, the system must not depend on a static camera setup and has to find the initial configuration automatically. We present a system, which tackles these problems by combining multiple cues within a particle filter framework, allowing the system to recover from wrong estimations in a natural way. We make extensive use of the benefit of having a calibrated stereo setup. To reduce search space implicitly, we use the 3D positions of the hands and the head, computed by a separate hand and head tracker using a linear motion model for each entity to be tracked. With stereo input image sequences at a resolution of 320 times 240 pixels, the processing rate of our system is 15 Hz on a 3 GHz CPU. Experimental results documenting the performance of our system are available in form of several videos.","Humans,
Humanoid robots,
Cameras,
Head,
Tracking,
Robot vision systems,
Particle filters,
Image sequences,
Image resolution,
Pixel"
Dynamic Lateral Polarization in CdZnTe Under High Flux X-Ray Irradiation,"The dynamic lateral polarization and charge steering effect was studied in 2D pixilated CdZnTe monolithic detector arrays designed for high flux X-ray imaging applications. While these detectors have shown the ability to work at 15 times 106 counts s-1 mm-2 and higher count-rates in pulse mode, we observed some detectors that exhibited a dynamic lateral polarization and charge steering effect causing non-uniform spatial response to the radiation field. The dynamic nature of the effect is shown by its flux dependence and reversibility upon changing the X-ray flux without a requirement to turn off the bias voltage. The effect causes the induced charge that would normally move from the cathode towards the anode to instead move laterally causing counts to shift away from a flux boundary. We show that the effect is not related to the physical boundary of the detector but rather related to the boundary of the irradiated area of the device. The dynamic polarization and charge steering effect can be attributed to the limited hole transport in the bulk material causing a buildup of a dynamic space-charge region under the irradiated area. The resulting lateral (perpendicular to the irradiation direction) electric field causes the lateral drift (steering) of the X-ray injected charge clouds. The static version of such lateral steering is often observed for charged structural defects in CdZnTe crystals. The studied 2D CdZnTe monolithic arrays were 16 times16 pixel devices having 0.4 mm times 0.4 mm area pixels on a 0.5 mm pitch and were fabricated using 8.7 mm times 8.7 mm times3.0 mm CdZnTe single crystals grown by the high-pressure electro-dynamic gradient freeze technique. The devices were probe tested in a system consisting of a custom 16 times16 pin probe head, 256 channel read-out electronics utilizing 8-channel fast bipolar ASIC chips, and a computer controlled 120 kVp X-ray source.","Polarization,
X-ray imaging,
Radiation detectors,
Crystals,
Probes,
Electronic equipment testing,
X-ray detection,
X-ray detectors,
Sensor arrays,
Voltage"
Constructing MANET Simulation Scenarios That Meet Standards,"Choosing an appropriate simulation scenario to study the performance of a MANET routing protocol is an important process. For example, routing will not be properly evaluated when a simulation scenario with a low average hop count or a large degree of network partitioning is used. To ensure that a simulation scenario provides an effective platform for testing a MANET routing protocol, we recommend that researchers use two metrics to characterize their simulation scenarios: the average shortest-path hop count and the average amount of network partitioning. In this paper, we provide researchers with several models that take the desired values for these two metrics as inputs, and output the simulation area and number of nodes required to create a simulation scenario that meets the researcher's target values for these two metrics to a close approximation. In this way, we provide several models that researchers can use to construct simulation scenarios that meet their standards in the evaluation of a MANET routing protocol.","Mobile ad hoc networks,
Routing protocols,
Computational modeling,
Testing,
Computer simulation,
Military computing,
Appropriate technology,
Delay,
Communication standards,
Uniform resource locators"
A Self-Tuning Configurable Cache,"The memory hierarchy of a system can consume up to 50% of microprocessor system power. Previous work has shown that tuning a configurable cache to a particular application can reduce memory subsystem energy by 62% on average. We introduce a self-tuning cache that performs transparent runtime cache tuning, thus relieving the application designer and/or compiler from predetermining an application's cache configuration. The self-tuning cache applies tuning at a determined tuning interval. A good interval balances tuning process energy overhead against the energy overhead of running in a sub-optimal cache configuration, which we show wastes much energy. We present a self-tuning cache that dynamically varies the tuning interval, resulting in average energy reduction of as much as 29%, falling within 13% of an oracle-based optimal method.",
Traps and pitfalls in secure clock synchronization,"Clock synchronization has become one of the enabling techniques to enable real-time on both application-and network level. One of the most promising and currently intensively discussed approaches is IEEE1588, a master slave based synchronization protocol, which is intended to be a protocol not only limited for one application use, but for many domains such as telecom, test and measurement or factory automation. For some of these application domains security is a crucial feature, not only to prevent malicious attacks, but also to avoid accidental disturbances such as wrongly configured devices in the net. For the sake of these security requirements in version 2 of the IEEE1588 standard an informative annex describes an extension of the widely accepted protocol. Nevertheless not only the extension of a protocol with security fields defines a secure system, also a policy has to declare what to do in certain cases. This paper describes this security extension and gives and extensive analysis on the applicable threads as well as an attack of the master and approaches to include version 2 switches in a secure IEEE1588 clock synchronized network.","Clocks,
Synchronization,
Protocols,
Security,
Master-slave,
Telecommunications,
Automatic testing,
Current measurement,
Manufacturing automation,
Yarn"
A Selective Anchor Node Localization Algorithm for Wireless Sensor Networks,"Wireless sensor network (WSN) are more and more widely used in many different scenarios. The localization information is an important criterion for the capability of WSN. Nowadays, there are many localization algorithms. DV-hop is a classical Range-free localization algorithm, by which unknown nodes can obtain anchors' information within designated hops, and estimate the distances from themselves to anchors. Unknown nodes then use the information to localize themselves. But the estimative distances may incur large error, and will jeopardize the localization precision. In order to solve the problem, we propose a selective anchor node localization algorithm (SANLA) for wireless sensor networks in this paper, and it can make unknown nodes choose three anchors which are the most accurate to execute trilateration. The experiment results illustrate that our algorithm is valid and effective.","Wireless sensor networks,
Global Positioning System,
Communication system traffic control,
Algorithm design and analysis,
Wireless communication,
Military computing,
Information technology,
Sun,
Computer science,
Costs"
Geometric Theory and Control of Linear Parameter Varying Systems,"Linear Parameter Varying (LPV) systems appear in a form of LTI state space representations where the elements of the A(rho), B(rho), C(rho) matrices depend on an unknown but at any time instant measurable vector parameter rho isin V. This paper describes a geometric view of LPV systems. Geometric concepts and tools of invariant subspaces and algorithms for LPV systems afflne in the parameters will be presented and proposed. Application of these results will be shown and referenced in solving various analysis (controllability/observability) problems, controller design and fault detection problems associated to LPV systems.","Control systems,
Nonlinear systems,
Vectors,
Control system analysis,
Optimal control,
State-space methods,
Time measurement,
Observability,
Functional analysis,
Performance analysis"
Interactive Decision Support for Multiobjective COTS Selection,"In the past decade component-based software engineering (CBSE) has gained considerable attention from both industry and the scientific community. Obviously, selecting the ""best"" combination of commercial off-the-shelf (COTS) components plays a critical role in CBSE. This task becomes demanding, as multiple objectives and constraints have to be taken into account. So far, the selection process has been tackled by such traditional techniques as the weighted scoring method or the analytical hierarchy process. This paper introduces a decision support approach that more properly addresses that challenge. The approach involves first determining (feasible) Pareto-efficient alternatives, then allowing decision makers to interactively explore the solution space until they find the most appealing solution. It not only works without extensive a priori preference information (such as criteria weights), it can also be easily integrated into existing COTS selection frameworks",
Knowledge Connectivity vs. Synchrony Requirements for Fault-Tolerant Agreement in Unknown Networks,"In self-organizing systems, such as mobile ad-hoc and peer-to-peer networks, consensus is a fundamental building block to solve agreement problems. It contributes to coordinate actions of nodes distributed in an ad-hoc manner in order to take consistent decisions. It is well known that in classical environments, in which entities behave asynchronously and where identities are known, consensus cannot be solved in the presence of even one process crash. It appears that self-organizing systems are even less favorable because the set and identity of participants are not known. We define necessary and sufficient conditions under which fault-tolerant consensus become solvable in these environments. Those conditions are related to the synchrony requirements of the environment, as well as the connectivity of the knowledge graph constructed by the nodes in order to communicate with their peers.","Fault tolerance,
Detectors,
Peer to peer computing,
Computer crashes,
Wireless sensor networks,
Ad hoc networks,
Protocols,
Network topology,
Computer science,
Sufficient conditions"
Importance of IP Alias Resolution in Sampling Internet Topologies,"Internet measurement studies utilize traceroute-based path traces to build representative Internet maps. These maps are then used to analyze various topological characteristics of the Internet. IP alias resolution is an important step in building a map from a set of collected path traces. In this paper, we study the impact of incomplete IP alias resolution on Internet measurement studies. Using a set of synthetic topologies and a genuine topology map, we experimentally show that the accuracy/completeness of alias resolution has an important effect on the observed topological characteristics. The results obtained in this work point out the importance of IP alias resolution and call for further research in alias resolution.","Sampling methods,
Internet,
Network topology,
Computer science,
Probes,
Protocols,
Debugging,
Telecommunication traffic,
Best practices,
IP networks"
Automatic Test Case Generation from UML Sequence Diagram,"This paper presents a novel approach of generating test cases from UML design diagrams. Our approach consists of transforming a UML sequence diagram into a graph called the sequence diagram graph (SDG) and augmenting the SDG nodes with different information necessary to compose test vectors. These information are mined from use case templates, class diagrams and data dictionary. The SDG is then traversed to generate test cases. The test cases thus generated are suitable for system testing and to detect interaction and scenario faults.","Automatic testing,
Unified modeling language,
System testing,
Object oriented modeling,
Computer science,
Design engineering,
Dictionaries,
Information technology,
Fault detection,
Application software"
ALARM: Anonymous Location-Aided Routing in Suspicious MANETs,"In many traditional mobile network scenarios, nodes establish communication on the basis of persistent public identities. However, in some hostile and suspicious MANET settings, node identities must not be exposed and node movements must be untraceable. Instead, nodes need to communicate on the basis of nothing more than their current locations. In this paper, we address some interesting issues arising in such MANETs by designing an anonymous routing framework (ALARM). It uses nodes' current locations to construct a secure MANET map. Based on the current map, each node can decide which other nodes it wants to communicate with. ALARM takes advantage of some advanced cryptographic primitives to achieve node authentication, data integrity, anonymity and untraceability (tracking-resistance). It also offers resistance to certain insider attacks.",
A UPF-UKF Framework For SLAM,"In this paper we propose a SLAM framework which is based on an algorithm that combines an unscented particle filter (UPF) and unscented Kalman filters (UKFs). A UPF is used to estimate robot's poses and the UKFs are used to represent landmark positions. UPF can estimate robot poses more consistently and accurately than generic particle filters (PFs), especially when models are highly non-linear or noises are not Gaussian. UKF can update landmarks more accurately compared to popular EKF's when highly non-linear observation models are used. In addition, our algorithm avoids the calculation of the Jacobian for both motion model and the observation model, which could be extremely difficult for high order systems. The calculation cost of a UPF is on the same order of magnitude as a particle filter (PF), which uses Kalman filters to generate proposal distributions, and the calculation cost of a UKF is equivalent to an EKF. As a result, our SLAM framework is more accurate than other popular SLAM frameworks while its efficiency is maintained. Simulation results are shown to validate the performance goals.",
Dynamic Replanning of Web Service Workflows,"The composition of Web Services to workflows is one of the major challenges in the area of service-oriented computing. To meet the business and user requirements, it is crucial to manage the quality of service (QoS) of Web Service workflows. In our approach, we calculate the execution plan of workflows on the QoS attributes ex ante based on predictions. However, due to the volatile nature of the Internet and the web servers, the runtime behavior of Web Services is likely to differ from the predictions. Therefore, we propose replanning as a mechanism to adapt the execution plan to the actual behavior of already executed services by a dynamic service selection at runtime, ensuring that the QoS and cost requirements will still be met. In this paper, we discuss replanning strategies, show how replanning leads to cost-savings in most cases, and evaluate the additional overhead caused by the adaptation of the execution plan at runtime.",
A Feature Modeling Support for Non-Functional Constraints in Service Oriented Architecture,"It is important in service oriented architecture (SOA) to separate functional and non-functional requirements for services because different applications use services in different non-functional contexts. In order to maximize the reusability of services, a set of constraints (e.g., dependency and mutual exclusion constraints) among non-functional requirements tend to be complicated to maintain. Currently, those non-functional constraints are informally specified in natural languages, and developers need to ensure that their applications satisfy the constraints in manual and ad-hoc manners. This paper proposes a model-driven development framework, through the notion of feature modeling, to explicitly and graphically specify non-functional constraints in SOA. The proposed framework allows developers to validate non-functional constraints in their applications in an automatic and consistent way. This paper also describes how the proposed framework is implemented and effectively used for service-oriented application development.",
"CAD-based Security, Cryptography, and Digital Rights Management","Manufacturing variability is inherent to many silicon and nano-scale technologies and can be manifested in many different ways and modalities (e.g. power and delay). We propose a flow that starts with gate-level integrated circuit (IC) characterization which results in unique identification (ID). The ID's are an integrated part of the design functionality and software and provide a basis for conceptually new CAD-based security protocols. As an examples, we present a new IC metering schemes that ensure very low overhead and digital right management in horizontally integrated IC market. Therefore, after many years of CAD importing and benefiting from many other areas such as numerical analysis, theoretical CS, VLSI design, computer architectures, and compilers, CAD has its historical chance to impact many fields of computer science and engineering through manufacturing variability-based security and right management.",
Design of a Dynamic Composition Handler for ESB-based Services,"The capability to compose services dynamically is a unique feature of Service-Oriented Computing (SOC), where services are published, discovered, and composed at runtime to deliver the expected service functionality to service clients. With dynamic composition, published services are searched and the most appropriate services are selected and reused at runtime. Also, newly published services can be reused without changing the client programs, so it yields an opportunity for dynamic adaptation. Despite the benefits, current programming languages, SOC platforms, business process modeling languages, and tools have a limitation on supporting the full dynamic composition. Rather, they require either manual composition or static binding of available services. In this paper, we present a design of a Dynamic Composition Handler on Enterprise Service Bus (ESB). We analyze different types of service compositions to clarity what dynamic composition really holds in SOC. Then, we present a design of Dynamic Composition Handler which consists of four elements; Invocation Listener, Service Router, Service Discoverer, and Interface Adapter. Using the framework, services can be discovered, selected, composed, and adapted at runtime. Since practical dynamic composition method depends on certain standard middleware platforms, we adopt ESB for our framework. We believe that our framework enables practical dynamic composition and realizes the whole benefit of it.","Middleware,
Computer languages,
Dynamic programming,
Web services,
Design engineering,
Computer science,
Context-aware services,
Runtime environment,
Feedback,
Design methodology"
Quantum Realization of Some Ternary Circuits Using Muthukrishnan-Stroud Gates,We present realization of ternary Toffoli gate and modified Fredkin gate for quantum computing on top of ion trap realizable Muthukrishnan-Stroud primitive gates. Our design methodology is based on first realizing the quantum circuits using generalized ternary gates and Feynman gates and then replacing them with their equivalent realization using Muthukrishnan-Stroud gates. Our realization of ternary Toffoli gate is more efficient than the previously published result and modified Fredkin gate is realized for the first time in literature using Muthukrishnan-Stroud gates.,"Circuits,
Quantum computing,
DH-HEMTs,
Computer science,
Multivalued logic,
Design methodology,
Quantum mechanics,
Computational modeling,
Computer simulation,
Physics"
Evaluation of a Decentralized Architecture for Large Scale Collaborative Intrusion Detection,"An important problem in network intrusion detection is how to detect large scale coordinated attacks such as scans, worms and denial-of-service attacks. These coordinated attacks can be difficult to detect at an early stage, since the evidence of the attack may be widely distributed across different subnetworks in the Internet. A critical issue for research is how to detect these large scale attacks by correlating information from multiple intrusion detection systems in an efficient manner. Several collaborative detection systems have been proposed in the literature. However, these proposals have lacked large scale testing in real networks, and the practicalities of how to optimize the trade-off between detection accuracy and reaction time of these systems has not been demonstrated. To address these challenges, we propose LarSID, a scalable decentralized large scale intrusion detection framework. LarSID provides a service for defending against attacks by sharing potential evidence of intrusions between participant intrusion detection systems via a distributed hash table (DHT) architecture. In particular, we investigate how to optimize the trade-off between detection accuracy and reaction time of LarSID based on an analysis of a large, real-world intrusion detection dataset (DShield Dataset), which has been collected from over 1600 firewall administrators across the world. LarSID has been deployed and tested on the PlanetLab testbed, and is built on top of OpenDHT - a public DHT service. Our experimental results show significant reductions in detection latency compared to a centralized detection architecture. Currently, LarSID has been deployed on 128 PlanetLab nodes as a large scale intrusion detection service.",
Integrating building automation systems and wireless sensor networks,"Building automation systems (BAS) are used to control and improve indoor building climate at reduced costs. By integrating BAS with wireless sensor networks, the need for cabling can be removed, and both installation and operational costs significantly reduced. Furthermore, temporary BAS installations are made possible. By implementing and evaluating the open BAS standard BACnet on resource-constrained sensor nodes we show that integrating existing standard BAS protocols with wireless sensor networks is feasible.","Automation,
Wireless sensor networks,
Costs,
Temperature sensors,
Sensor systems,
Automatic control,
Control systems,
Batteries,
Centralized control,
Wireless application protocol"
High-Throughput Cooperative Communications with Complex Field Network Coding,"Relay-based cooperative communications can achieve spatial diversity gains, enhance coverage and potentially increase capacity. If considered for large networks, traditional relaying schemes suffer from spectral inefficiency that can be improved through network coding at a physical layer. These considerations motivate the complex field network coding (CFNC) scheme proposed in this paper. As opposed to network coding over Galois field, where wireless links cannot improve throughput limitations as the number of sources increases, CFNC achieves throughput as high as 1/2 symbol per source per time slot. With improved throughput, CFNC-based relaying achieves full diversity gain regardless of the channel signal-to-noise-ratio (SNR) and the underlying constellation, and is general enough to incorporate transmissions from sources to a common destination as well as information exchange among the sources at the same time.","Network coding,
Relays,
Throughput,
Diversity methods,
Galois fields,
Physical layer,
AWGN,
Error probability,
Bandwidth,
Collaborative work"
Hilbert Transform Based FBP Algorithm for Fan-Beam CT Full and Partial Scans,This paper presents a new type of filtered backprojection (FBP) algorithm for fan-beam full- and partial-scans. The filtering is shift-invariant with respect to the angular variable. The backprojection does not include position-dependent weights through the Hilbert transform and the one-dimensional transformation between the fan- and parallel-beam coordinates. The strong symmetry of the filtered projections directly leads to an exact reconstruction for partial data. The use of the Hilbert transform avoids the approximation introduced by the nonuniform cutoff frequency required in the ramp filter-based FBP algorithm. Variance analysis indicates that the algorithm might lead to a better uniformity of resolution and noise in the reconstructed image. Numerical simulations are provided to evaluate the algorithm with noise-free and noisy projections. Our simulation results indicate that the algorithm does have better stability over the ramp-filter-based FBP and circular harmonic reconstruction algorithms. This may help improve the image quality for in place computed tomography scanners with single-row detectors,
The Bohm Plasma-Sheath Model and the Bohm Criterion Revisited,"The plasma-sheath model and the Bohm criterion introduced by Bohm are among the earliest attempts to separately model the plasma and the sheath and to find a way to join the plasma and the sheath solutions. Although there is hardly a paper on plasma-sheath modeling that does not quote the Bohm criterion, Bohm's paper and his results are widely misunderstood. The reason for this is that, in his paper, Bohm himself misinterpreted his result by concluding that the sheath edge coincides with the reference point of his plasma-sheath model. As a result, the criterion for the reference point obtained by Bohm to ensure monotonicity of his sheath solution (i.e., the Bohm criterion) was erroneously applied to the sheath edge and was used in literature as a criterion for sheath formation. In this paper, we show that the Bohm criterion when applied to the sheath edge contradicts Bohm's own definition of the sheath and cannot be obtained from Bohm's plasma-sheath model.",
Peer2Me - rapid application framework for mobile peer-to-peer applications,"This paper presents the Peer2Me framework that enables developers to create mobile peer-to-peer applications. The framework provides an API that is easy to adopt, yet capable of creating advanced peer-to-peer applications. The framework was built to provide applications providing pure peer-to-peer network where all nodes have the same responsibility and services. Further, the framework provides services to transparently manage the detection of new and lost peers. The message component of the framework makes it possible to exchange any kind of data between peers including Java objects. The Peer2Me has been implemented in Java 2 Micro Edition (J2ME) and runs on standard mobile phones. The framework supports management and communication of mobile ad hoc networks (MANETs) like Bluetooth. The paper describes the architecture, the API and some of the applications developed using the Peer2Me framework. Further, we share and discuss experiences from developing mobile peer-to-peer applications.","Peer to peer computing,
Mobile communication,
Mobile handsets,
Bluetooth,
Ad hoc networks,
Mobile computing,
Protocols"
Ontology Evaluation and Ranking using OntoQA,"Ontologies form the cornerstone of the Semantic Web and are intended to help researchers to analyze and share knowledge, and as more ontologies are being introduced, it is difficult for users to find good ontologies related to their work. Therefore, tools for evaluating and ranking the ontologies are needed. In this paper, we present OntoQA, a tool that evaluates ontologies related to a certain set of terms and then ranks them according a set of metrics that captures different aspects of ontologies. Since there are no global criteria defining how a good ontology should be, OntoQA allows users to tune the ranking towards certain features of ontologies to suit the need of their applications. We also show the effectiveness of OntoQA in ranking ontologies by comparing its results to the ranking of other comparable approaches as well as expert users.","Ontologies,
Semantic Web,
Distributed computing,
Large-scale systems,
Distributed information systems,
Computer science,
Information analysis,
Bioinformatics,
Biology computing,
Humans"
Adaptive Search Range Motion Estimation Algorithm for H.264/AVC,"In this paper, a novel variable search range algorithm is proposed for the H.264/AVC. First we evaluate the influence on the bitrate and PSNR when the search range is set from a small range to a increasing wide range. The simulation shows interesting results that sometimes when the search range is set to a smaller search range it can achieve better result at both the bitrate and the PSNR comparing with the reference software with the fixed search range of 16 times 16. Furthermore, we find that how much the search range could be reduced to achieve an optimum search range have strong relation with the average of the frame motion vectors. Based on these simulation results a adaptive search range algorithm which estimate the most efficient search range from the previous frames with a adaptively search range adjustment method is proposed. Simulation results show that proposed algorithm could achieve the motion estimation by average 45% computation complexity compare with the reference software with equivalent PSNR and the bitrate results.","Motion estimation,
Automatic voltage control,
Bit rate,
PSNR,
Cities and towns,
Motion compensation,
Computational modeling,
Optimization methods,
Systems engineering and theory,
Focusing"
Bus Access Optimisation for FlexRay-based Distributed Embedded Systems,"FlexRay will very likely become the de-facto standard for in-vehicle communications. Its main advantage is the combination of high speed static and dynamic transmission of messages. In the previous work the authors have shown that not only the static but also the dynamic segment can be used for hard-real time communication in a deterministic manner. This paper proposed techniques for optimising the FlexRay bus access mechanism of a distributed system, so that the hard real-time deadlines are met for all the tasks and messages in the system. The authors have evaluated the proposed techniques using extensive experiments",
Adaptive Web Services Testing,"Web services (WS) and service-oriented architecture (SOA) present a set of unique testing challenges. As services are distributed, it is necessary to test them using a distributed architecture. Furthermore, as these services may keep on changing, testing needs to be adaptive. This paper proposes an adaptive testing framework which can continuously learn and improve the built-in test strategies. The framework allows different test cases to be selected based on the recent test results. The framework also has a windowing mechanism to evaluate and select test cases.","Web services,
Service oriented architecture,
Collaboration,
Quality of service,
Software testing,
Computer science,
Application software,
Cybernetics,
Programming,
Computer architecture"
Hidden Markov Models for Activity Recognition in Ambient Intelligence Environments,"Context-aware computing offers several advantages for human computer interaction by augmenting ambient intelligence environments with computational artifacts that can be responsive to the needs of users. One of the main challenges in context-aware computing is context recognition. While some contextual variables, such as location, can be easily recognized, others, such as activity are more complex to estimate. This paper describes an approach to estimate activities in a working environment. The approach is based on information gathered from a workplace study, in which 196 hours of detailed observation of hospital workers were recorded. This data is used to train a Hidden Markov Model to estimate user activity. The results indicate that the user activity can be correctly estimated 92.6% of the time. We compare our results with the use of neuronal networks and human observers familiar with those work practice. We discuss how these results can be used for context-aware applications.","Hidden Markov models,
Ambient intelligence,
Hospitals,
Context-aware services,
Pervasive computing,
Context,
Humans,
Computational intelligence,
Biological neural networks,
Application software"
City-Climbers at Work,"The video presents recent progress of the wall-climbing robot project at the City College of New York. The robots are named as City-Climbers which adopt a novel adhesive mechanism based on aerodynamic attraction to achieve good balance between strong adhesion force and high mobility. The video demonstrates that the City-Climber robots can operate on virtually any kind of smooth or rough surfaces and have the capabilities to move on the ground, climb walls, and transit between them. The modular design achieves both fast motion of each module on planar surfaces and smooth transition between the surfaces by a set of two modules. The video also displays the Fluent simulation results of the aerodynamic attraction with the aim to optimize the design. DSP-based control system is introduced which enables the robot to operate both manually and autonomously",
Unscented Filtering in a Unit Quaternion Space for Spacecraft Attitude Estimation,"A novel approach to the straightforward implementation of unscented filtering in a unit quaternion space is proposed for spacecraft attitude estimation. A method of weighted mean computation for quaternions is derived in a rotational space, leading to a quaternion with a unit norm. Quaternion multiplication, which allows a quaternion in the filter to lie in the unit quaternion space, is then used for predicted covariance computations and quaternion updates. In this study, quaternion process noise, which increases the uncertainty in attitude orientation, is either modeled as the vector part of the quaternion or as a rotation vector. Simulation results indicate that the proposed approach successfully estimates spacecraft attitude.","Filtering,
Quaternions,
Space vehicles,
Space technology,
Aerospace engineering,
Filters,
Systems engineering and theory,
Computer science,
Uncertainty,
Modeling"
Prediction and Modeling for the Time-Evolving Ultra-Wideband Channel,"We conduct a feasibility study of ultra-wideband (UWB) channel prediction to answer the following two questions: Is the UWB channel predictable? Is UWB channel prediction useful? We setup the problem in the following way: A receiver travels along a linear trajectory at a constant velocity. The transmitter and environment are stationary. Using past channel measurements, the receiver predicts future measurements of the channel, assuming its direction of movement and velocity remain constant. Our approach is to decompose the time evolution of the channel, which is jointly correlated in time and delay, in terms of the time evolution of individual paths, which are independent across delay. A measurement campaign was conducted in the Berkeley Wireless Research Center, where measurements were taken with line-of-sight (LOS) and non-line-of-sight (NLOS) conditions. We develop a channel prediction algorithm, and evaluate results in terms of the matched filter output energy (MFOE). Iterating through the six strongest paths, our prediction algorithm achieves more than 70% (40%) of the possible MFOE over a prediction distance of 34 cm for the LOS (NLOS) conditions. These results are good since the coherence distance, being the distance for which the channel is approximately constant, is less than 1 cm.","Predictive models,
Ultra wideband technology,
Transmitters,
Velocity measurement,
Prediction algorithms,
Delay effects,
Signal processing algorithms,
Matched filters,
Scattering,
Computer science"
Manycasting Over Optical Burst-Switched Networks,"In this paper, we discuss for the first time the issue of supporting manycasting service over optical burst- switched (OBS) networks. One of the primary challenges in providing manycasting service over OBS networks is to reduce data loss due to burst contentions. We propose two new schemes, static over-provisioning (SOP) and dynamic membership (DM), to alleviate this data loss problem. The proposed schemes take into consideration the specific properties of manycasting, and the schemes may complement existing contention resolution schemes. The effectiveness of the proposed schemes is verified through simulation.","Optical fiber networks,
Optical losses,
Wavelength division multiplexing,
Routing,
Delta modulation,
Cost function,
Communications Society,
Computer science,
Computer networks,
Optical computing"
Automated Choreographer Synthesis for Web Services Composition Using I/O Automata,"We study the problem of synthesis of a choreographer in Web service composition for a given set of services and a goal. Services and goal are represented using I/O automata which can succinctly and precisely describe the interfaces of the services. Our technique considers existence and synthesis of two types of the choreographers: a simple choreographer capable of only relaying outputs from one service to input of another and a transducing choreographer which is capable of storing and reusing inputs/outputs from the services. The central theme of our technique relies on generating I/O automata representation of all possible choreographed behavior of existing services (captured in form of universal service automaton, a concept introduced in this paper) and verifying that the goal can be simulated by the universal set of choreographed behaviors.",
Finite Generalized Gaussian Mixture Modeling and Applications to Image and Video Foreground Segmentation,"In this paper, we propose a finite mixture model of generalized Gaussian distributions (GDD) for robust segmentation and data modeling in the presence of noise and outliers. The model has more flexibility to adapt the shape of data and less sensibility for over-fitting the number of classes than the Gaussian mixture. In a first part of the present work, we propose a derivation of the maximum-likelihood estimation of the parameters of the new mixture model and we propose an information-theory based approach for the selection of the number of classes. In a second part, we propose some applications relating to image, motion and foreground segmentation to measure the performance of the new model in image data modeling with comparison to the Gaussian mixture.","Image segmentation,
Noise robustness,
Shape,
Gaussian distribution,
Noise shaping,
Application software,
Computer science,
Gaussian noise,
Maximum likelihood estimation,
Computer vision"
Channel-Adaptive Optimal OFDMA Scheduling,"Joint subcarrier, power and rate allocation in orthogonal frequency division multiple access (OFDMA) scheduling is investigated for both downlink and uplink wireless transmissions. Using a time-sharing argument, a convex formulation is obtained avoiding the NP-hardness of the usual 0-1 integer program solution. It is rigourously established that the optimal allocation can be obtained almost surely through a greedy water-filling approach with linear complexity in the number of users and subcarriers. Stochastic approximation is further employed to develop on-line algorithms which are capable of dynamically learning the underlying channel distribution and asymptotically converges to the off-line optimal solution from arbitrary initial values.","Optimal scheduling,
Downlink,
Time sharing computer systems,
Fading,
Stochastic processes,
Processor scheduling,
AWGN,
Collaborative work,
Government,
OFDM"
An Energy-Efficient Scheduling Algorithm Using Dynamic Voltage Scaling for Parallel Applications on Clusters,"In the past decade cluster computing platforms have been widely applied to support a variety of scientific and commercial applications, many of which are parallel in nature. However, scheduling parallel applications on large scale clusters is technically challenging due to significant communication latencies and high energy consumption. As such, shortening schedule length and conserving energy consumption are two major concerns in designing economical and environmentally friendly clusters. In this paper, we propose an energy-efficient scheduling algorithm (TDVAS) using the dynamic voltage scaling technique to provide significant energy savings for clusters. The TDVAS algorithm aims at judiciously leveraging processor idle times to lower processor voltages (i.e., the dynamic voltage scaling technique or DVS), thereby reducing energy consumption experienced by parallel applications running on clusters. Reducing processor voltages, however, can inevitably lead to increased execution times of parallel task. The salient feature of the TDVAS algorithm is to tackle this problem by exploiting tasks precedence constraints. Thus, TDVAS applies the DVS technique to parallel tasks followed by idle processor times to conserve energy consumption without increasing schedule lengths of parallel applications. Experimental results clearly show that the TDVAS algorithm is conducive to reducing energy dissipation in large-scale clusters without adversely affecting system performance.","Energy efficiency,
Scheduling algorithm,
Dynamic voltage scaling,
Energy consumption,
Processor scheduling,
Clustering algorithms,
Large-scale systems,
Voltage control,
Concurrent computing,
Delay"
Occupancy grid mapping: An empirical evaluation,"In this paper a quantitative analysis of robotic mapping utilising the fields dominant paradigm, the occupancy grid, is presented. The aim of this work is to determine which approach to the robotic mapping problem imbues a mobile robot with the greatest ability to create an accurate representation of its operating environment. We accomplish this by analysing the performance of several established mapping techniques using identical test data. Through evaluating the maps generated by these paradigms using an extensible benchmarking suite that our group has developed we outline which paradigm yields the greatest representational ability.","Mobile robots,
Robot sensing systems,
Sensor phenomena and characterization,
Sonar detection,
Uncertainty,
Grid computing,
Information systems,
Information analysis,
Performance analysis,
Testing"
Development of Vision-Based Navigation for a Robotic Wheelchair,"Our environment is replete with visual cues intended to guide human navigation. For example, there are building directories at entrances and room numbers next to doors. By developing a robot wheelchair system that can interpret these cues, we will create a more robust and more usable system. This paper describes the design and development of our robot wheelchair system, called Wheeley, and its vision-based navigation system. The robot wheelchair system uses stereo vision to build maps of the environment through which it travels; this map can then be annotated with information gleaned from signs. We also describe the planned integration of an assistive robot arm to help with pushing elevator buttons and opening door handles.",
Data access history cache and associated data prefetching mechanisms,"Data prefetching is an effective way to bridge the increasing performance gap between processor and memory. As computing power is increasing much faster than memory performance, we suggest that it is time to have a dedicated cache to store data access histories and to serve prefetching to mask data access latency effectively. We thus propose a new cache structure, named Data Access History Cache (DAHC), and study its associated prefetching mechanisms. The DAHC behaves as a cache for recent reference information instead of as a traditional cache for instructions or data. Theoretically, it is capable of supporting many well known history-based prefetching algorithms, especially adaptive and aggressive approaches. We have carried out simulation experiments to validate DAHC design and DAHC-based data prefetching methodologies and to demonstrate performance gains. The DAHC provides a practical approach to reaping data prefetching benefits and its associated prefetching mechanisms are proven more effective than traditional approaches.","History,
Prefetching,
Drain avalanche hot carrier injection,
Delay,
Government,
Bridges,
Performance gain,
Cache memory,
Sun,
Computer science"
A Service-Oriented Analysis and Design Approach to Developing Adaptable Services,"The publish-discover-compose paradigm of service-oriented computing (SOC) presents a new challenge on the service applicability and adaptability. Services in SOC are not just for dedicated service clients, rather for a group of diverse and unknown potential service clients. Hence, service providers try to develop and publish services which can be applicable to various potential clients. Service clients try to locate right services without knowing much about service providers in advance. A challenging problem with this little coupling between providers and clients is that available services should be highly adaptable to various service clients and various service contexts. To develop such adaptable services, the service variability among potential clients and contexts must carefully be analyzed and designed. However, current works on service oriented analysis and design (SOAD) largely focus on defining processes for developing business processes and services without considering variability in details. In this paper, we analyze the key artifacts of common SOAD methods and identify types of variation points over the artifacts. Then, we present a systematic SOAD process for developing highly adaptable business services. We also present a case study to show the feasibility of the process. By developing adaptable services by using our proposed framework, the applicability and reusability of such services can be increased.","Context-aware services,
Service oriented architecture,
Object oriented modeling,
Computer science,
Design methodology,
Guidelines,
Computer architecture,
Process design"
Performance Analysis of Offloading Systems in Mobile Wireless Environments,"Offloading is an approach to leverage the severity of resource constrained nature of mobile devices (such as PDAs, mobile phones) by migrating part of the computation of applications to some nearby resource-rich surrogates (e.g., desktop PCs, mobility support stations). It is an essential mechanism for the execution of pervasive services. However, the mobile nature of mobile devices and the unstable connectivity of wireless links all render a less predictability of the performance of a pervasive service running under the control of offloading systems. This paper proposes an analytical model to express the performance of offloading systems in mobile wireless environments. We investigate the surrogate unreachability when mobile devices move following random waypoint (RWP) mobility scheme. We model the failure recovery time and total execution time of pervasive applications that run under the control of offloading systems. Detailed evaluation and analysis results are reported and the results of this paper can be used as design guidance for pervasive service offloading systems.",
Optimal Delay-Power Tradeoff in Wireless Transmission with Fixed Modulation,"Cross-layer scheduling is an emerging and powerful solution that can significantly improve the quality of service (QoS) and power efficiency in wireless networks by jointly using channel and buffer states to adjust transmission power and rate. In conventional scheduling methods, adaptive modulation and coding (AMQ) is adopted to realize rate adaptation. However, this may greatly increase the transceiver and protocol complexity. To overcome this, scheduling methods based on fixed modulation are studied in this paper. In this case, the scheduler only needs to determine whether the transmitter is active at given channel and buffer state. When the transmitter is active, it will transmit in a fixed data rate, by using power adaptation to achieve a required SNR at the receiver. Our aim is to minimize the average packet delay given constraint on average power. To do that, we shall derive the analytical results of delay, packet-loss rate, and power consumption of cross-layer scheduling. It will then be shown that there exists a fundamental tradeoff between delay and power, which characterize the performance limit of cross-layer scheduling. In addition, a delay-optimal scheduling method will also be presented.","Delay,
Transmitters,
Processor scheduling,
Quality of service,
Wireless networks,
Transceivers,
Power engineering and energy,
Protocols,
Energy consumption,
Cross layer design"
Modeling Count Data from Multiple Sensors: A Building Occupancy Model,"Knowledge of the number of people in a building at a given time is crucial for applications such as emergency response. Sensors can be used to gather noisy measurements which when combined, can be used to make inferences about the location, movement and density of people. In this paper we describe a probabilistic model for predicting the occupancy of a building using networks of people-counting sensors. This model provides robust predictions given typical sensor noise as well as missing and corrupted data from malfunctioning sensors. We experimentally validate the model by comparing it to a baseline method using real data from a network of optical counting sensors in a campus building.","Optical sensors,
Humans,
Hidden Markov models,
Optical noise,
Monitoring,
Counting circuits,
Noise robustness,
Predictive models,
Computer science,
Urban planning"
Intelligent cognitive radio: Research on learning and evaluation of CR based on Neural Network,"This paper introduces the research of cognitive engine and application of artificial intelligence techniques in cognitive radio. The limitation of CR engine based on GA is analyzed, propose for improvement is proposed. The decision maker of CR engine should consider both the changeable factors and the unchangeable factors such as cost, bandwidth, signal rate and ARQ. Based on Neural Network, the method of evaluating and learning best decision is proposed. Several key architectural issues for cognitive radio engine based on Neural Network are discussed, including knowledge base information model and learning model Neural Network design.","Intelligent networks,
Cognitive radio,
Chromium,
Neural networks,
Engines,
Artificial intelligence,
Artificial neural networks,
Learning,
Costs,
Bandwidth"
High Speed Differential Drive Mobile Robot Path Following Control With Bounded Wheel Speed Commands,"The great majority of path following control laws for either kinematical or dynamical mobile robot models are designed assuming ideal actuators, i.e. assuming that any commanded velocity or torque (in the kinematical and dynamical cases respectively) will be instantly implemented regardless of its value. Real actuators are far from being ideal. In particular, only bounded velocities and torques can be realized for any given command. With reference to the kinematical model of a differential drive mobile robot, a known path following control law is modified to account for actuator velocity saturation. The proposed solution is experimentally shown to be particularly useful for high speed applications where accounting for actuator velocity saturation may have a large influence on performance.","Mobile robots,
Robot control,
Wheels,
Actuators,
Vehicle dynamics,
Velocity control,
Torque control,
Vehicles,
Automatic control,
Control design"
An Efficient FFT For OFDM Based Cognitive Radio On A Reconfigurable Architecture,"Cognitive radio is a promising technology to utilize non-used parts of the spectrum that actually are assigned to licensed services. An adaptive OFDM based cognitive radio system has the capacity to nullify individual carriers to avoid interference to the licensed user. Therefore, there could be a considerably large number of zero-valued inputs/outputs for the IFFT/FFT in the OFDM transceiver. Due to the wasted operations on zero values, the standard FFT is no longer efficient. Based on this observation, we propose to use a computationally efficient IFFT/FFT as an option for OFDM based cognitive radio. Mapping this algorithm onto a reconfigurable architecture is discussed.","OFDM,
Cognitive radio,
Reconfigurable architectures,
Interference,
Baseband,
Communications Society,
Mathematics,
Computer science,
Transceivers,
Resource management"
Reducing UK-Means to K-Means,"This paper proposes an optimisation to the UK-means algorithm, which generalises the k-means algorithm to han- dle objects whose locations are uncertain. The location of each object is described by a probability density function (pdf). The UK-means algorithm needs to compute expected distances (EDs) between each object and the cluster repre- sentatives. The evaluation of ED from first principles is very costly operation, because the pdf 's are different and arbi- trary. But UK-means needs to evaluate a lot of EDs. This is a major performance burden of the algorithm. In this pa- per, we derive a formula for evaluating EDs efficiently. This tremendously reduces the execution time of UK-means, as demonstrated by our preliminary experiments. We also il- lustrate that this optimised formula effectively reduces the UK-means problem to the traditional clustering algorithm addressed by the k-means algorithm.","Clustering algorithms,
Uncertainty,
Probability density function,
Global Positioning System,
Computational efficiency,
Data mining,
Conferences,
Computer science,
Costs,
Boosting"
Automatic Trace-Based Performance Analysis of Metacomputing Applications,"The processing power and memory capacity of independent and heterogeneous parallel machines can be combined to form a single parallel system that is more powerful than any of its constituents. However, achieving satisfactory application performance on such a metacomputer is hard because the high latency of inter-machine communication as well as differences in hardware of constituent machines may introduce various types of wait states. In our earlier work, we have demonstrated that automatic pattern search in event traces can identify the sources of wait states in parallel applications running on a single computer. In this article, we describe how this approach can be extended to metacomputing environments with special emphasis on performance problems related to inter-machine communication. In addition, we demonstrate the benefits of our solution using a real-world multi-physics application.","Performance analysis,
Metacomputing,
Application software,
Concurrent computing,
Delay,
Parallel machines,
Grid computing,
Hardware,
Weather forecasting,
Computer science"
Model Checking Safety-Critical Systems Using Safecharts,"With rapid developments in science and technology, we now see the ubiquitous use of different types of safety-critical systems in our daily lives such as in avionics, consumer electronics, and medical systems. In such systems, unintentional design faults might result in injury or even death to human beings. To make sure that safety-critical systems are really safe, there is a need to verify them formally. However, the verification of such systems is getting more and more difficult because designs are becoming very complex. To cope with high design complexity, currently, model-driven architecture design is becoming a well-accepted trend. However, existing methods of testing and standards conformance are restricted to implementation code, so they do not fit very well with model-based approaches. To bridge this gap, we propose a model-based formal verification technique for safety-critical systems. In this work, the model-checking paradigm is applied to the Safecharts model, which was used for modeling but not yet used for verification. Our contributions listed are as follows: first, the safety constraints in Safecharts are mapped to semantic equivalents in timed automata for verification. Second, the theory for safety constraint verification is proven and implemented in a compositional model checker (that is, the state-graph manipulator (SGM)). Third, prioritized and urgent transitions are implemented in SGM to model the risk semantics in Safecharts. Finally, it is shown that the priority-based approach to mutual exclusion of resource usage in the original Safecharts is unsafe and corresponding solutions are proposed. Application examples show the feasibility and benefits of the proposed model-driven verification of safety-critical systems","automata theory,
computational linguistics,
formal verification,
graph theory,
safety-critical software"
Computing Iceberg Cubes by Top-Down and Bottom-Up Integration: The StarCubing Approach,"Data cube computation is one of the most essential but expensive operations in data warehousing. Previous studies have developed two major approaches, top-down versus bottom-up. The former, represented by the multiway array cube (called the multiway) algorithm, aggregates simultaneously on multiple dimensions; however, it cannot take advantage of a priori pruning when computing iceberg cubes (cubes that contain only aggregate cells whose measure values satisfy a threshold, called the iceberg condition). The latter, represented by BUC, computes the iceberg cube bottom-up and facilitates a priori pruning. BUC explores fast sorting and partitioning techniques; however, it does not fully explore multidimensional simultaneous aggregation. In this paper, we present a new method, star-cubing, that integrates the strengths of the previous two algorithms and performs aggregations on multiple dimensions simultaneously. It utilizes a star-tree structure, extends the simultaneous aggregation methods, and enables the pruning of the group-bys that do not satisfy the iceberg condition. Our performance study shows that star-cubing is highly efficient and outperforms the previous methods","Aggregates,
Warehousing,
Multidimensional systems,
Partitioning algorithms,
Sorting,
Data mining,
Data analysis,
Regression analysis,
Database systems,
Costs"
Divisible Load Scheduling: An Approach Using Coalitional Games,"Scheduling divisible loads in distributed systems is the subject of divisible load theory (DLT). In this paper we show that coalitional game theory is a natural fit for modeling DLT as the participants in the scheduling algorithm must cooperate in order to execute a job. We devise a coalitional scheduling game in which the job owners and the independent organizations that own processors form coalitions in order to maximize their profits. We examine the payoffs to the participants and show that the core of the proposed coalitional scheduling game is non-empty. Then we examine the ""fair sharing"" of the payoffs among the participants using the Shapley value. Finally we study by simulation the properties of the proposed coalitional scheduling game considering different distributed systems configurations.","Processor scheduling,
Game theory,
Costs,
Scheduling algorithm,
Power system modeling,
Computer science,
Parallel processing,
Stability,
Distributed computing,
Power system interconnection"
Skeleton-based Hierarchical Shape Segmentation,"We present an effective framework for segmenting 3D shapes into meaningful components using the curve skeleton. Our algorithm identifies a number of critical points on the curve skeleton, either fully automatically as the junctions of the curve skeleton, or based on user input. We use these points to construct a partitioning of the object surface using geodesies. Because it is based on the curve skeleton, our segmentation intrinsically reflects the shape symmetry and topology. By using geodesies we obtain segments that have smooth, minimally twisting borders. Finally, we present a hierarchical segmentation of shapes which reflects the hierarchical structure of the curve skeleton. We describe a voxel-based implementation of our method which is robust and noise resistant, computationally efficient, able to handle shapes of complex topology, and which delivers level- of-detail segmentations. We demonstrate the framework on various real-world 3D shapes.","Shape,
Skeleton,
Geophysics computing,
Topology,
Image segmentation,
Mathematics,
Computer science,
Partitioning algorithms,
Noise robustness,
Noise shaping"
Capturing Spontaneous Conversation and Social Dynamics: A Privacy-Sensitive Data Collection Effort,"The UW dynamic social network study is an effort to automatically observe and model the creation and evolution of a social network formed through spontaneous face-to-face conversations. We have collected more than 4,400 hours of data that capture the real world interactions between 24 subjects over a period of 9 months. The data was recorded in completely unconstrained and natural conditions, but was collected in a manner that protects the privacy of both study participants and non-participants. Despite the privacy constraints, the data allows for many different types of inference that are in turn useful for studying the prosodic and paralinguistic features of truly spontaneous speech across many subjects and over an extended period of time. This paper describes the new challenges and opportunities presented in such a study, our data collection effort, the problems we encountered, and the resulting corpus.",
A New Asymmetric SRAM Cell to Reduce Soft Errors and Leakage Power in FPGA,"Soft errors in semiconductor memories occur due to charged particle strikes at the cell nodes. In this paper, we present a new asymmetric memory cell to increase the soft error tolerance of SRAM. At the same time, this cell can be used at the reduced supply voltage to decrease the leakage power without significantly increasing the soft error rate of SRAM. A major use of this cell is in the configuration memory of FPGA. The cell is designed using a 70nm process technology and verified using Spice simulations. Soft error tolerance results are presented and compared with standard SRAM cell and an existing increased soft error tolerance cell. Simulation results show that our cell has lowest soft error rate at the various supply voltages",
A New Dynamic Multi-objective Optimization Evolutionary Algorithm,"Dynamic multi-objective optimization problems are very common in real-world applications. The researches on applying evolutionary algorithm into such problems are attracting more and more researchers. In this paper, a new dynamic multi-objective optimization evolutionary algorithm which utilizes hyper-mutation operator to deal with dynamics and geometrical Pareto selection to deal with multi-objective is introduced. The experimental results show that the performance is satisfactory.","Evolutionary computation,
Genetic algorithms,
Global Positioning System,
Sampling methods,
Educational institutions,
Computer science,
Application software,
Pareto optimization,
Minimization methods"
Practical Implementation of Stochastic Parameterized Model Order Reduction via Hermite Polynomial Chaos,"This paper describes the stochastic model order reduction algorithm via stochastic Hermite polynomials from the practical implementation perspective. Comparing with existing work on stochastic interconnect analysis and parameterized model order reduction, we generalized the input variation representation using polynomial chaos (PC) to allow for accurate modeling of non-Gaussian input variations. We also explore the implicit system representation using sub-matrices and improved the efficiency for solving the linear equations utilizing block matrix structure of the augmented system. Experiments show that our algorithm matches with Monte Carlo methods very well while keeping the algorithm effective. And the PC representation of non-Gaussian variables gains more accuracy than Taylor representation used in previous work (Wang et al., 2004).","Stochastic processes,
Polynomials,
Chaos,
Integrated circuit interconnections,
Equations,
Stochastic systems,
Analytical models,
Circuit simulation,
Taylor series,
System performance"
Natural deictic communication with humanoid robots,"A simple view of deictic communication only includes the indication process and recognition process: a person points at an object and says something about it such as ""look at this,"" and then the other person recognizes the pointing gesture and pays attention to the indicated object. However, this simple view lacks three important processes: attention synchronization, context focus, and believability establishment. We refer to these three processes as ""facilitation processes"" and implement them in a humanoid robot with a motion capturing system. An experiment with 30 subjects revealed that the facilitation processes make deictic communication natural.","Humanoid robots,
Context,
Intelligent robots,
Cities and towns,
Human robot interaction,
USA Councils,
Computer interfaces,
Knowledge engineering,
Educational institutions"
A Proposal of Metrics for Botnet Detection Based on Its Cooperative Behavior,"In this paper, we propose three metrics for detecting botnets through analyzing their behavior. Our social infrastructure (i.e., the Internet) is currently experiencing the danger of bots' malicious activities as the scale of botnets increases. Although it is imperative to detect botnet to help protect computers from attacks, effective metrics for botnet detection have not been adequately researched. In this work we measure enormous amounts of traffic passing through the Asian Internet Interconnection Initiatives (AIII) infrastructure. To validate the effectiveness of our proposed metrics, we analyze measured traffic in three experiments. The experimental results reveal that our metrics are applicable for detecting botnets, but further research is needed to refine their performance","Proposals,
Internet,
Computer crime,
Telecommunication traffic,
Command and control systems,
Laboratories,
Protection,
Scattering,
Wide area networks,
Asia"
Arterial Pulse System: Modern Methods For Traditional Indian Medicine,"Ayurveda is one of the most comprehensive healing systems in the world and has classified the body system according to the theory of Tridosha to overcome ailments. Diagnosis similar to the traditional pulse-based method requires a system of clean input signals, and extensive experiments for obtaining classification features. In this paper we briefly describe our system of generating pulse waveforms and use various feature detecting methods to show that an arterial pulse contains typical physiological properties. The beat-to-beat variability is captured using a complex B-spline mother wavelet based peak detection algorithm. We also capture - to our knowledge for the first time - the self- similarity in the physiological signal, and quantifiable chaotic behavior using recurrence plot structures.","Electrocardiography,
Heart,
Wrist,
Pulse measurements,
Medical diagnostic imaging,
Chaos,
Fractals,
Transducers,
Pulse generation,
Computer vision"
A scalable distributed algorithm for shape transformation in multi-robot systems,"Distributed reconfiguration is an important problem in multi-robot systems such as mobile sensor nets and metamorphic robot systems. In this work, we present a scalable distributed reconfiguration algorithm, hierarchical median decomposition, to achieve arbitrary target configurations. Our algorithm is built on top of a novel distributed median consensus estimator. The algorithms presented are fully distributed and do not require global communication. We show results from simulations in an open source multi-robot simulator.","Distributed algorithms,
Shape,
Multirobot systems,
Robot sensing systems,
Intelligent robots,
Mobile robots,
Sensor systems,
Communication system control,
Distributed computing,
USA Councils"
Thermal Imaging of the Superficial Temporal Artery: An Arterial Pulse Recovery Model,"We present a novel model for measurement of the arterial pulse from the superficial temporal artery (STA) using passive thermal infrared (IR) sensors. The proposed approach has a physical and physiological basis and as such is of fundamental nature. Thermal IR camera is used to capture the heat pattern from superficial arteries, and a blood vessel model is used to describe the pulsatile nature of the blood flow. A multiresolution wavelet-based signal analysis approach is used to extract the arterial pulse waveform, which lends itself to various physiological measurements. We validate the results using a traditional contact vital-sign monitor as a ground truth. Eight people of different age, race and gender have been tested in our study consistent with IRB approval. The resultant arterial pulse waveforms exactly matched the ground-truth readings. The essence of our approach is the automatic detection of region of arterial pulse measurement (ROM), from which the arterial pulse waveform is extracted. To the best of our knowledge, the correspondence between non-contact thermal IR imaging based measurements of the arterial pulse in the time domain and traditional contact approaches has never been reported in the literature.","Arteries,
Pulse measurements,
Optical imaging,
Biomedical monitoring,
Infrared sensors,
Thermal sensors,
Cameras,
Blood vessels,
Biomedical imaging,
Blood flow"
Dynamic Malleability in Iterative MPI Applications,"Malleability enables a parallel application's execution system to split or merge processes modifying granularity. While process migration is widely used to adapt applications to dynamic execution environments, it is limited by the granularity of the application's processes. Malleability empowers process migration by allowing the application's processes to expand or shrink following the availability of resources. We have implemented malleability as an extension to the PCM (process checkpointing and migration) library, a user-level library for iterative MPI applications. PCM is integrated with the Internet operating system (IOS), a framework for middleware-driven dynamic application reconfiguration. Our approach requires minimal code modifications and enables transparent middleware- triggered reconfiguration. Experimental results using a two-dimensional data parallel program that has a regular communication structure demonstrate the usefulness of malleability.","Phase change materials,
Libraries,
Application software,
Resource management,
Computer science,
Availability,
Checkpointing,
Internet,
Operating systems,
Middleware"
Finger Movement Classification for an Electrocorticographic BCI,"We study the problem of distinguishing between individual finger movements of one hand using electrocorticographic (ECOG) signals. In previous work, we have shown that ECOG signals have high predictive accuracy and spatial resolution for classifying hand versus tongue movements. In this paper, we significantly extend this paradigm by studying the first 5-class classification problem for ECOG, and show that an average 5-class accuracy of 23% across 6 subjects is possible using as little as 10min of training data. In addition to opening up possibilities for higher-bandwidth brain-computer interfaces, the use of finger movements for control may yield a more intuitive mapping from ECOG signals to control of a prosthetic. Although this study uses real movements, our results provide the foundation for understanding ECOG signal changes during finger movement.","Fingers,
Electroencephalography,
Biomedical electrodes,
Spatial resolution,
Tongue,
Brain computer interfaces,
Training data,
Prosthetics,
Communication system control,
Epilepsy"
Task space control with prioritization for balance and locomotion,"This paper addresses locomotion with active balancing, via task space control with prioritization. The center of gravity (COG) and foot of the swing leg are treated as task space control points. Floating base inverse kinematics with constraints is employed, thereby allowing for a mobile platform suitable for locomotion. Different techniques of task prioritization are discussed and we clarify differences and similarities of previous suggested work. Varying levels of prioritization for control are examined with emphasis on singularity robustness and the negative effects of constraint switching. A novel controller for task space control of balance and locomotion is developed which attempts to address singularity robustness, while minimizing discontinuities created by constraint switching. Controllers are evaluated using a quadruped robot simulator engaging in a locomotion task.","Robot kinematics,
Foot,
Legged locomotion,
Orbital robotics,
Leg,
Jacobian matrices,
Mobile robots,
Space technology,
Robust control,
Open loop systems"
A Hardwired Context-Based Adaptive Binary Arithmetic Encoder for H. 264 Advanced Video Coding,"We propose a full hardwired context-based adaptive binary arithmetic encoder for H.264/AVC. Our architecture includes a 14-way context pair generator, a 3-stage pipelined circuit for getting neighboring data and a 4-stage pipelined multiple-mode arithmetic encoder. The context pair generator is composed of binarization and context modeling and it can operate with arithmetic encoder concurrently. The arithmetic encoder can process one bin per cycle. Our whole CABAC encoder is able to process 0.67 bins per cycle on the average. Its performance is adequate for 1080 p (HDTV) resolution at 30 fps when running at 60 MHz.",
Basic Abstractions for an Autonomic Network Architecture,"The ANA (Autonomic Network Architecture) project aims at providing a framework to flexibly host, interconnect, and federate multiple heterogeneous networks in an autonomic way, i.e. without requiring active human intervention. The guiding design principle is to strive for a maximum degree of flexibility at all levels of the architecture in order to inherently support heterogeneity and evolution. This paper describes the core abstractions and concepts of ANA (as defined during the first year of the project) and introduces their basic operation and interaction. While not autonomic themselves, the core architectural principles of ANA will enable autonomicity by not imposing a ""one-size-fits-all"" approach where protocols and paradigms are fixed by the architecture. We indeed argue that only the capacity of the network to be polyfunctional and fully adaptable will justify the label ' autonomic'. A prototype ofANA is currently being developed and will be released in 2008: the goal is to demonstrate the feasibility of autonomic networking within the 4 years of the project.","Humans,
IP networks,
Computer architecture,
Protocols,
Computer science,
National electric code,
Europe,
Prototypes,
Clouds"
Sleep-based Topology Control for Wakeup Scheduling in Wireless Sensor Networks,"Wireless sensor network applications require both energy-efficiency and low latency for reporting urgent but rare events. Various wakeup scheduling schemes have been proposed to save energy by employing duty cycles and to reduce end-to-end delay by synchronizing nodes' wakeup times and shortening setup latencies. Most sensor applications incur a predominant ""convergecast"" traffic pattern in which several close-by sensor nodes simultaneously send reports to the base station, which generates ""spatially-correlated contention."" This contention, specific to sensor networks, can significantly prolong delay, degrade throughput, and impair energy-efficiency. However, most wakeup scheduling and MAC protocols proposed for sensor networks did not address this contention and can be adversely affected. We propose a distributed topology control technique to schedule nodes' wakeup time slots, and design a MAC protocol to benefit from this topology control for improving energy-efficiency and delay, and efficiently handling spatially-correlated contention. Through analysis and simulations, we show that our topology control and MAC can either reduce the end- to-end delay by roughly half or extend the network lifetime by three times, when compared against the nearest competing approach, and achieves twice the throughput under spatially-correlated contention.","Network topology,
Wireless sensor networks,
Delay,
Energy efficiency,
Throughput,
Media Access Protocol,
Telecommunication traffic,
Base stations,
Degradation,
Distributed control"
Linear genetic programming of parsimonious metaheuristics,"We use a form of grammar-based linear Genetic Programming (GP) as a hyperheuristic, i.e., a search heuristic on the space of heuristics. This technique is guided by domain- specific languages that one designs taking inspiration from elementary components of specialised heuristics and metaheuristics for a domain. We demonstrate this approach for traveling- salesperson problems for which we test different languages, including one containing a looping construct. Experimentation with benchmark instances from the TSPLIB shows that the GP hyperheuristic routinely and rapidly produces parsimonious metaheuristics that find tours whose lengths are highly competitive with the best real-valued lengths from literature.","Genetic programming,
Space exploration,
Processor scheduling,
Optimization methods,
Benchmark testing,
Personnel,
Parallel processing,
Computational modeling,
Simulated annealing,
Evolutionary computation"
Marmite: Towards End-User Programming for the Web,"Many information tasks on the web require users to make use of multiple websites, for both content as well as information processing or visualization features. Programmers have created ""mashups,"" which customize or combine the functionality of multiple websites by extracting information from web pages or accessing web services APIs. Non-programmers lack the tools or skills create such customizations because of the programming obstacles involved. Marmite is a tool that empowers non-programmers to create functionality similar to those found in mashups.","Mashups,
Web services,
Programming profession,
Web pages,
Books,
XML,
State feedback,
Libraries,
Filters,
Computer science"
Dimensionality Reduction and Attack Recognition using Neural Network Approaches,"Most current Intrusion Detection Systems (IDS) examine all data features to detect intrusion. Also existing intrusion detection approaches have some limitations, namely impossibility to process a large number of audit data for realtime operation, low detection and recognition accuracy. To overcome these limitations, we apply modular neural network models to detect and recognize attacks in computer networks. They are based on the combination of principal component analysis (PCA) neural networks and multilayer perceptrons (MLP). PCA networks are employed for important data extraction and to reduce high dimensional data vectors. We present two PCA neural networks for feature extraction: linear PCA (LPCA) and nonlinear PCA (NPCA). MLP is employed to detect and recognize attacks using feature-extracted data instead of original data. The proposed approaches are tested with the help of KDD-99 dataset. The experimental results demonstrate that the designed models are promising in terms of accuracy and computational time for real world intrusion detection.","Neural networks,
Intrusion detection,
Principal component analysis,
Computer vision,
Computer networks,
Multi-layer neural network,
Multilayer perceptrons,
Data mining,
Vectors,
Feature extraction"
A Message Scheduling Scheme for All-to-All Personalized Communication on Ethernet Switched Clusters,"We develop a message scheduling scheme for efficiently realizing all-to-all personalized communication (AAPC) on Ethernet switched clusters with one or more switches. To avoid network contention and achieve high performance, the message scheduling scheme partitions AAPC into phases such that 1) there is no network contention within each phase and 2) the number of phases is minimum. Thus, realizing AAPC with the contention-free phases computed by the message scheduling algorithm can potentially achieve the minimum communication completion time. In practice, phased AAPC schemes must introduce synchronizations to separate messages in different phases. We investigate various synchronization mechanisms and various methods for incorporating synchronizations into the AAPC phases. Experimental results show that the message scheduling-based AAPC implementations with proper synchronization consistently achieve high performance on clusters with many different network topologies when the message size is large",
A Direct Torque Control Scheme for Induction Motor Drives Using the Current Model Flux Estimation,"The paper deals with a direct torque control (DTC) scheme for Induction Motor (IM) drives where flux and torque of the motor are estimated by the IM current model instead of by the voltage model. The scheme is intended to enable operation of the DTC IM drives at zero speed. As a return, its implementation requires the knowledge of speed, rotor time constant and inductive parameters of the motor. In the paper the performance of the proposed scheme is studied for a drive equipped with an incremental encoder and commanded with a constant flux. Using the position information delivered by the encoder, a suitable representation of the current model is formulated that makes the drive operation feasible at true zero speed. Operating with a constant flux command, the variations in the rotor time constant dominate over the other parameters; their effect on the drive behavior is analyzed in steady state founding that the motor flux and torque are deviated from the references of a quantity, which is a function of the torque reference. Experimental results are given to substantiate both the capabilities of the scheme of developing the full torque at a standstill and the theoretical findings on the drive behavior under rotor time constant mismatch.","Torque control,
Induction motor drives,
Stators,
Induction motors,
Rotors,
Steady-state,
Voltage control,
Inverters,
Frequency estimation,
Computer science"
One Scan Connected Component Labeling Technique,"This paper, presents a new component labeling algorithm which is based on scanning and labeling the objects in a single scan. The algorithm has the ability to test the four and eight connected branches of the object. This algorithm, which is fast and requires low memory allocation, can also process an image that contains large numbers of objects. The algorithm is used to scan the image from left to right and from top to bottom to find the unlabeled objects. A comparison analysis is performed with other component labeling algorithms. Our algorithm has shown an outstanding performance with respect to the processing time. A practical application with computer based mammography is also included.",
Bounds on the Network Coding Capacity for Wireless Random Networks,"Recently, it has been shown that the max flow capacity can be achieved in a multicast network using network coding. In this paper, we propose and analyze a more realistic model for wireless random networks. We prove that the capacity of network coding for this model is concentrated around the expected value of its minimum cut. Furthermore, we establish upper and lower bounds for wireless nodes using Chernoff bounds. Our experiments show that our theoretical predictions are well matched by simulation results.","Network coding,
Solid modeling,
Throughput,
Unicast,
Decoding,
Routing,
Computer science,
Predictive models,
Graph theory,
Multicast algorithms"
Handwritten Carbon Form Preprocessing Based on Markov Random Field,"This paper proposes a statistical approach to degraded handwritten form image preprocessing including binarization and form line removal. The degraded image is modeled by a Markov random field (MRF) where the prior is learnt from a training set of high quality binarized images, and the probabilistic density is learnt on-the-fly from the gray-level histogram of input image. We also modified the MRF model to implement form line removal. Test results of our approach show excellent performance on the data set of handwritten carbon form images.","Markov random fields,
Degradation,
Testing,
Image restoration,
Lighting,
Histograms,
Energy resolution,
Image resolution,
Handwriting recognition,
Clustering algorithms"
Fair Coalitions for Power-Aware Routing in Wireless Networks,"Several power-aware routing schemes have been developed for wireless networks under the assumption that nodes are willing to sacrifice their power reserves in the interest of the network as a whole. But, in several applications of practical utility, nodes are organized in groups, and as a result, a node is willing to sacrifice in the interest of other nodes in its group but not necessarily for nodes outside its group. Such groups arise naturally as sets of nodes associated with a single owner or task. We consider the premise that groups will share resources with other groups only if each group experiences a reduction in power consumption. Then, the groups may form a coalition in which they route each other's packets. We demonstrate that sharing between groups has different properties from sharing between individuals and investigate fair, mutually beneficial sharing between groups. In particular, we propose a Pareto-efficient condition for group sharing based on max-min fairness called fair coalition routing. We propose distributed algorithms for computing the fair coalition routing. Using these algorithms, we demonstrate that fair coalition routing allows different groups to mutually beneficially share their resources",
Stochastic Double Array Analysis and Convergence of Consensus Algorithms with Noisy Measurements,"This paper considers consensus-seeking of networked agents in an uncertain environment where each agent has noisy measurements of its neighbors' states. We propose stochastic approximation type algorithms with a decreasing step size. We first establish consensus results in a two-agent model via a stochastic double array analysis. Next, we generalize the analysis to a class of well studied symmetric models and obtain consensus results.","Stochastic processes,
Algorithm design and analysis,
Convergence,
Stochastic resonance,
Quantization,
Working environment noise,
Communication system control,
Cities and towns,
Noise measurement,
Computer networks"
Clocking scheme for nanomagnet QCA,"Quantum-dot Cellular Automata (QCA) was previously demonstrated using aluminum tunnel junction single-electron transistor technology at mK temperatures, and molecular QCA is under development for operation at room temperature (RT). All of the basic building blocks needed for QCA have been experimentally demonstrated. Our work on nanomagnet-based QCA (NMQCA) holds the most promise for achieving viable RT operation in the near term. One requirement of the QCA architecture is a low-power clock structure. In this paper, we demonstrate the design and simulation of an on-chip low-power clock circuit that can facilitate the realization of the nanomagnet-based fully functional logic circuit on a single chip.","Clocks,
Wire,
RLC circuits,
Magnetic separation,
Magnetic resonance imaging,
Magnetic flux,
Magnetic fields"
Distributed Quad-Tree for Spatial Querying in Wireless Sensor Networks,"In contrast to the traditional wireless sensor network (WSN) applications that perform only data collection and aggregation, new generation of information processing applications, such as pursuit-evasion games, tracking, evacuation, and disaster relief applications, require in-network information storage and querying. Due to the resource limitations of WSNs, it is challenging to implement in-network information storage and querying in a resilient, energy-efficient, and distributed manner. To address these challenges, we exploit location information and geometry of the network and present an in-network querying infrastructure, namely distributed quad-tree (DQT) structure. DQT satisfies efficient in-network information storage as well as distance-sensitive querying: the cost of answering a query for an event is at most a constant factor (in our case 2radic2 ) of the distance ""d"" to the nearest event in the network. DQT construction is local and does not require any communication. Moreover, due to its minimalist infrastructure and stateless nature, DQT shows graceful resilience to the face of failures.","Wireless sensor networks,
Information processing,
Application software,
Energy efficiency,
Resilience,
Delay,
Cost function,
Road accidents,
Communications Society,
Computer science"
On a Routing Problem Within Probabilistic Graphs and its Application to Intermittently Connected Networks,"Given a probabilistic graph G representing an intermittently connected network and routing algorithm A, we wish to determine a delivery subgraph G[A] of G with at most k edges, such that the probability Conn2 (G[A]) that there is a path from source s to destination t (in a graph H chosen randomly from the probability space defined by G[A]) is maximized. To the best of our knowledge, this problem and its complexity has not been addressed in the literature. Also, there is the corresponding distributed version of the problem where the delivery subgraph G[A] is to be constructed distributively, yielding a routing protocol. Our proposed solution to this routing problem is multi-fold: First, we prove the hardness of our optimization problem of finding a delivery subgraph that maximizes the delivery probability and discuss the hardness of computing the objective function Conn2(G[A]); Second, we present an algorithm to approximate Conn2(G[A]) and compare it with an optimal algorithm; Third, we focus on intermittently connected networks, and model the users' mobility within them; and Fourth, we propose an edge-constrained routing protocol (EC-SOLAR-KSP) based on the insights obtained from the first step and the contact probabilities computed in the third step. We then highlight the protocol's novelty and effectiveness by comparing it with a probabilistic routing protocol, and an epidemic routing protocol proposed in literature.",
A Statistical Approach to Inverting the Born Ratio,"We examine the problem of fluorescence molecular tomography using the normalized Born approximation, termed herein the Born ratio, from a statistical perspective. Experimentally verified noise models for received signals at the excitation and emission wavelengths are combined to generate a stochastic model for the Born ratio. This model is then utilized within a maximum likelihood framework to obtain an inverse solution based on a fixed point iteration. Results are presented for three experimental scenarios: phantom data with a homogeneous background, phantoms implanted within a small animal, and in vivo data using an exogenous probe.","Imaging phantoms,
Fluorescence,
Tomography,
Scattering,
Approximation methods,
Noise generators,
Signal to noise ratio,
Signal generators,
Stochastic resonance,
Animals"
"Analytical and Experimental IP Encapsulation Efficiency Comparison of GSE, MPE, and ULE over DVB-S2","Transmitting variable-length network layer (IP) packets over satellite links with fixed frame lengths (or lengths depending on the ACM transmission mode) requires (IP) encapsulation. For DVB-S links with fixed-size 188-byte TS packets, MPE and ULE encapsulations are available. DVB-S2 provides a compatibility mode to pack TS packets into the longer base band frames (BBFrames). Thus MPE and ULE are also available for S2, as well as a native generic stream encapsulation (GSE), avoiding the double overhead of TS and BBFrame encapsulation. The present paper gives a short overview on the available encapsulation protocols for DVB-S2, and then provides a mathematical efficiency calculation model for these encapsulations, in order to allow for performing theoretical efficiency simulations. Comparison graphs of the efficiency values are presented, using both the efficiency models and measurements from real satellite traffic.","Encapsulation,
Digital video broadcasting,
Protocols,
Satellite broadcasting,
Traffic control,
Modeling,
Telecommunication computing,
Systems engineering and theory,
Artificial satellites,
TV broadcasting"
Image Encryption Method Based on Chaotic Map,"The combination of the chaotic theory and the cryptography is an important study field of the image encryption. Classical Arnold cat map can change the position of image pixel points through iteration, but these pixel points will return to the original position after iterating many times. It is obvious not enough to carry on the encryption by using it only. If the encrypted result with cat map is carried to Lu chaotic map to encrypt again, it can attain the purpose of confusion and diffusion through changing the pixel value of each point. The system security lies in the initial sensitivity of chaotic map. By using the Matlab to simulate, comparing with the encryption of single map, the method has the high improvement in anti-attacking.","Cryptography,
Chaos,
Chaotic communication,
Pixel,
Internet,
Communications technology,
Security,
Organizational aspects,
Electronic mail,
Safety"
Parsimonious Model Selection for Tissue Segmentation and Classification Applications: A Study Using Simulated and Experimental DTI Data,"One aim of this work is to investigate the feasibility of using a hierarchy of models to describe diffusion tensor magnetic resonance (MR) data in fixed tissue. Parsimonious model selection criteria are used to choose among different models of diffusion within tissue. Using this information, we assess whether we can perform simultaneous tissue segmentation and classification. Both numerical phantoms and diffusion weighted imaging (DWI) data obtained from excised pig spinal cord are used to test and validate this model selection framework. Three hierarchical approaches are used for parsimonious model selection: the Schwarz criterion (SC), the F-test t-test (F-t), proposed by Hext, and the F-test F-test (F-F), adapted from Snedecor. The F-t approach is more robust than the others for selecting between isotropic and general anisotropic (full tensor) models. However, due to its high sensitivity to the variance estimate and bias in sorting eigenvalues, the F-F and SC are preferred for segmenting models with transverse isotropy (cylindrical symmetry). Additionally, the SC method is easier to implement than the F-t and F-F methods and has better performance. As such, this approach can be efficiently used for evaluating large MRI data sets. In addition, the proposed voxel-by-voxel segmentation framework is not susceptible to artifacts caused by the inhomogeneity of the variance in neighboring voxels with different degrees of anisotropy, which might contaminate segmentation results obtained with the techniques based on voxel averaging.","Diffusion tensor imaging,
Tensile stress,
Magnetic resonance imaging,
Anisotropic magnetoresistance,
Magnetic resonance,
Image segmentation,
Imaging phantoms,
Spinal cord,
Testing,
Robustness"
Blind Estimation of Multiple Carrier Frequency Offsets,"Multiple carrier-frequency offsets (CFO) arise in a distributed antenna system, where data are transmitted simultaneously from multiple antennas. In such systems the received signal contains multiple CFOs due to mismatch between the local oscillators of transmitters and receiver. This results in a time-varying rotation of the data constellation, which needs to be compensated for at the receiver before symbol recovery. This paper proposes a new approach for blind CFO estimation and symbol recovery. The received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (MIMO) problem. By applying blind MIMO system estimation techniques, the system response is estimated and used to subsequently transform the multiple CFOs estimation problem into many independent single CFO estimation problems. Furthermore, an initial estimate of the CFO is obtained from the phase of the MIMO system response. The Cramer-Rao lower bound is also derived, and the large sample performance of the proposed estimator is compared to the bound.","Frequency estimation,
MIMO,
Phase locked loops,
Transmitting antennas,
Constellation diagram,
Receiving antennas,
Time varying systems,
Data engineering,
Mobile antennas,
Local oscillators"
Visual Vehicle Egomotion Estimation using the Fourier-Mellin Transform,"This paper is concerned with the problem of estimating the motion of a single camera from a sequence of images, with an application scenario of vehicle egomotion estimation. Egomotion estimation has been an active area of research for many years and various solutions to the problem have been proposed. Many methods rely on optical flow or local image features to establish the spatial relationship between two images. A new method of egomotion estimation is presented which makes use of the Fourier-Mellin Transform for registering images in a video sequence, from which the rotation and translation of the camera motion can be estimated. The Fourier-Mellin Transform provides an accurate and efficient way of computing the camera motion parameters. It is a global method that takes the contributions from all pixels into account. The performance of the proposed approach is compared to two variants of optical flow methods and results are presented for a real-world video sequence taken from a moving vehicle.","Fourier transforms,
Cameras,
Motion estimation,
Remotely operated vehicles,
Application software,
Image motion analysis,
Robot vision systems,
Image registration,
Information technology,
Optical sensors"
MobileTest: A Tool Supporting Automatic Black Box Test for Software on Smart Mobile Devices,"With the development of mobile computing and pervasive computing, smart mobile devices such as PDAs or smart-phones are gradually becoming an indispensable part of our daily life. However, as the software running on these devices becomes more and more powerful and complex, the testing of these mobile applications poses great challenges for mobile application vendors and phone manufacturers. In this paper, we introduce MobileTest, a tool supporting automatic black box test for software on smart mobile devices. The objectives and the design of the testing tool are thoroughly discussed. The paper also adopts a sensitive-event based approach to simplify the design of test cases and enhance the test cases' efficiency and reusability. Finally, we conducted an experiment in a real testing project. Measurement data of the testing process shows that MobileTest can effectively reduce the complexity of automatic test on smart mobile devices.","Automatic testing,
Software testing,
Software tools,
Application software,
Pervasive computing,
Mobile computing,
Computer science,
Personal digital assistants,
Manufacturing,
Context"
SZTAKI Desktop Grid: a Modular and Scalable Way of Building Large Computing Grids,"So far BOINC based desktop grid systems have been applied at the global computing level. This paper describes an extended version of BOINC called SZTAKI desktop grid (SZDG) that aims at using desktop grids (DGs) at local (enterprise/institution) level. The novelty of SZDG is that it enables the hierarchical organisation of local DGs, i.e., clients of a DG can be DGs at a lower level that can take work units from their higher level DG server. More than that, even clusters can be connected at the client level and hence work units can contain complete MPI programs to be run on the client clusters. In order to easily create master/worker type DG applications a new API, called as the DC-API has been developed. SZDG and DC-API has been successfully applied both at the global and local level, both in academic institutions and in companies to solve problems requiring large computing power.","Grid computing,
Distributed computing,
Computer science,
Middleware,
Contracts,
Internet,
Personal communication networks,
Application software,
Automation,
Computer vision"
Avoiding Traffic Jam Using Ant Colony Optimization - A Novel Approach,"Ant colony optimization (ACO) is a meta-heuristic based on colony of artificial ants which work cooperatively, building solutions by moving on the problem graph and by communicating through artificial pheromone trails mimicking real ants. One of the active research directions is the application of ACO algorithms to solve dynamic shortest path problems. Solving traffic jams is one such problem where the cost i.e. time to travel increases during rush hours resulting in tremendous strain on daily commuters and chaos. This paper describes a new approach-DSATJ (Dynamic System for Avoiding Traffic Jam) which aims at choosing an alternative optimum path to avoid traffic jam and then resuming that same path again when the traffic is regulated. The approach is inspired by variants of ACO algorithms. Traffic jam is detected through pheromone values on edges which are updated according to goodness of solution on the optimal tours only. Randomness is introduced in the probability function to ensure maximum exploration by ants. Experiments were carried out with the partial road map of North-West region of Delhi, India, to observe the performance of our approach.","Ant colony optimization,
Vehicle dynamics,
Cities and towns,
Computer science,
Capacitive sensors,
Roads,
Routing,
Computational intelligence,
Application software,
Educational institutions"
Model-Driven Simulation of Grid Scheduling Strategies,"Simulation studies of grid scheduling strategies require representative workloads to produce dependable results. Real production grid workloads have shown diverse correlation structures and scaling behavior, which are different than the characteristics of the available supercomputer workloads and cannot be captured by Poisson or simple distribution-based models. We present models that are able to reproduce various correlation structures, including pseudo-periodicity and long range dependence. By conducting model-driven simulation, we quantitatively evaluate the performance impacts of workload correlations in grid scheduling. The results indicate that autocorrelations in workloads result in worse system performance, both at the local and the grid level. It is shown that realistic workload modeling is not only possible, but also necessary to enable dependable grid scheduling studies.","Processor scheduling,
Autocorrelation,
Grid computing,
Computational modeling,
Computer simulation,
Supercomputers,
Degradation,
Computer science,
Laboratories,
Production"
Minimum Coverage Breach and Maximum Network Lifetime in Wireless Sensor Networks,"Network lifetime is a critical issue in Wireless Sensor Networks. It is possible to extend network lifetime by organizing the sensors into a number of sensor covers. However, with the limited bandwidth, coverage breach (i.e, targets that are not covered) can occur if the number of available time-slots/channels is less than the number of sensors in a sensor cover. In this paper, we study a joint optimization problem in which the objective is to minimize the coverage breach as well as to maximize the network lifetime. We show a ""trade-off"" scheme by presenting two strongly related models, which aim to tradeoffs between the two conflicting objectives. The main approach of our models is organizing sensors into non-disjoint sets, which is different from the current most popular approach and can gain longer network lifetime as well as less coverage breach. We proposed two algorithms for the first model based on linear programming and greedy techniques, respectively. Then we transform these algorithms to solve the second model by revealing the strong connection between the models. Through numerical simulation, we showed the good performance of our algorithms and the pictures of the tradeoff scheme in variant scenarios, which coincide with theoretical analysis very well. It is also showed that our algorithms could obtain less breach rate than the one proposed in [2].","Wireless sensor networks,
Bandwidth,
Batteries,
Computer science,
Organizing,
Linear programming,
Numerical simulation,
Performance analysis,
Algorithm design and analysis,
Energy efficiency"
What Hackers Learn that the Rest of Us Don't: Notes on Hacker Curriculum,"To learn security skills, students and developers must be able to switch from their traditional conditioning to the attacker's way of thinking. Exposure to the hacker culture through hacker conferences such as Defcon and others, Phrack and similar publications, and to comprehensive collections such as Packet Storm helps provide the necessary culture slunk or ""a-ha"" moment and should be integral to every in-depth security curriculum. Recipes for preventing particular kinds of exploits are only a small part of the value these materials provide. The primary and much underappreciated value of these sources lies in facilitating a deeper understanding of the underlying systems by exposing their designers' implicit assumptions and concentrating the students' and developers' attention on the bigger picture of the system and its environment, especially on issues typically glossed over.","Computer hacking,
Hardware,
Computer security,
Standards development,
Education,
Operating systems,
Reverse engineering,
Software testing,
System testing,
Computer industry"
A Novel Error Concealment Method for Stereoscopic Video Coding,"A novel error concealment method is proposed for two-view based stereoscopic video coding to address the challenging problem of adaptively combining inter-view correlation and temporal correlation. First, the disparity vectors of the lost macroblocks' neighboring macroblocks are used to recover the lost or erroneously received motion or disparity vectors. Then we propose a novel error concealment method based on overlapped block motion and disparity compensation, whose weights are determined by the side match criterion and viewpoints. Simulation results show that the subjective and objective performances of the proposed technique are both superior to those of conventional temporal error concealment methods for stereoscopic video coding.","Video coding,
Video compression,
Computer errors,
Motion compensation,
Decoding,
Computer science,
Video sequences,
Layout,
Robustness,
Image reconstruction"
Privacy in Location-Based Services: State-of-the-Art and Research Directions,"The explosive growth of location-detection devices (e.g., GPS-like devices and handheld devices) along with wireless communications and mobile databases results in realizing location-based applications that deliver specific information to their users based on their current locations. Examples of such applications include location-based store finder, location-based traffic reports, and location-based advertisements. Although location-based services promise safety and convenience, they threaten the privacy and security of users as such services explicitly require users to share private location information with the service. If a user wants to keep her location information private, she has to turn off her location-aware device and temporarily unsubscribe from the service. Recent studies show that such privacy concerns - ranging from worries over employers snooping on their workers' whereabouts to fears of tracking by potential stalkers - are a serious obstacle to wider adoption of location-based services. This article aims to provide practitioners, researchers, and graduate students with the state of the art and major research issues in the important and practical research area of location privacy. In general, the tutorial is divided into the following five parts: (1) legislative issues and privacy concerns, (2) location privacy in mobile environments, (3) privacy attack models, (4) privacy-aware location query processing; (5) concluding remarks.","Query processing,
Data privacy,
Law,
Application software,
Computer science,
Data engineering,
Explosives,
Handheld computers,
Wireless communication,
Mobile computing"
Integrated Droplet Routing in the Synthesis of Microfluidic Biochips,"Microfluidic biochips are revolutionizing many areas of biochemistry and biomedical sciences. Several synthesis tools have recently been proposed for the automated design of biochips from the specifications of laboratory protocols. However, only a few of these tools address the problem of droplet routing in microfluidic arrays. These methods typically rely on post-synthesis droplet routing to implement biochemical protocols. Such an approach is not only time-consuming, but it also imposes an undue burden on the chip user. Moreover, post-synthesis droplet routing does not guarantee that feasible droplet pathways can be found for area-constrained biochip layouts; non-routable fabricated biochips must be discarded. We present a droplet-routing-aware automated synthesis tool for microfluidic biochips. Droplet routability, defined as the ease with which droplet pathways can be determined, is estimated and integrated in the synthesis flow. The proposed approach allows architectural-level design choices and droplet-routing-aware physical design decisions to be made simultaneously. We use a large-scale protein assay as a case study to evaluate the proposed synthesis method.",
How Good are Local Features for Classes of Geometric Objects,"Recent work in object categorization often uses local image descriptors such as SIFT to learn and detect object categories. Such descriptors explicitly code local appearance and have shown impressive results on objects with sufficient local appearance statistics. However, many important object classes such as tools, cups and other man-made artifacts seem to require features that capture the respective shape and geometric layout of those object classes. Therefore this paper compares, on a novel data collection of 10 geometric object classes, various shape-based features with appearance-based descriptors such as SIFT. The analysis includes a direct comparison of feature statistics as well as results within standard recognition frameworks, which are partly intuitive, but sometimes surprising.","Shape,
Statistics,
Object recognition,
Image segmentation,
Performance evaluation,
Computer science,
Object detection,
Statistical analysis,
Focusing,
Motorcycles"
Human Centered Ubiquitous Display in Intelligent Space,"Projection based information display system is proposed in this paper. Normally, human should approach information sources which are located around our living environment, e.g. bulletin boards, artificial signs, local maps, etc. However, the proposed system is able to afford human with relevant information by projecting it on where the human is facing in so that the human does not need to move for seeking information. The proposing system is based on intelligent space and a projector mounted mobile robot which is called ubiquitous display.","Humans,
Displays,
Intelligent robots,
Intelligent sensors,
Intelligent actuators,
Intelligent agent,
Computer networks,
Space technology,
Intelligent networks,
Orbital robotics"
A Rationale for the use of Optical Mice Chips for Economic and Accurate Vehicle Tracking,"Accurate vehicle localization is a frequent requirement of tracking applications. Unfortunately cost is often the limiting factor in the selection of tolerable error limits. This paper provides a rationale for using optical navigation chips out of optical computer mice for economic and accurate vehicle tracking. Supporting evidence is provided in the form of a comparative analysis of several tracking technologies. The technologies analyzed include varieties of GPS, accelerometers, laser rangefinders, and optical mouse sensors. The comparative analysis presented in this paper uses a model based on cost, accuracy, speed, and range.","Mice,
Vehicles,
Optical sensors,
Costs,
Optical computing,
Application software,
Computer errors,
Navigation,
Global Positioning System,
Accelerometers"
ReSHAPE: A Framework for Dynamic Resizing and Scheduling of Homogeneous Applications in a Parallel Environment,"A traditional application scheduler running on a parallel cluster only supports static scheduling where the number of processors allocated to an application remains fixed throughout the lifetime of the job. Due to unpredictability in job arrival times and varying resource requirements, static scheduling can result in idle system resources thereby decreasing the overall system throughput. In this paper we present a prototype framework called ReSHAPE, which supports dynamic resizing of parallel MPI applications executed on distributed memory platforms. The framework includes a scheduler that supports resizing of applications, an API to enable applications to interact with the scheduler, and a library that makes resizing viable. Applications executed using the ReSHAPE scheduler framework can expand to take advantage of additional free processors or can shrink to accommodate a high priority application, without getting suspended. Experimental results show that the ReSHAPE framework can improve individual job turn-around time and overall system throughput.","Dynamic scheduling,
Processor scheduling,
Resource management,
Throughput,
Application software,
Quality of service,
Runtime library,
Computer science,
Prototypes,
Supercomputers"
Robust Region-of-Interest Determination Based on User Attention Model Through Visual Rhythm Analysis,"This paper investigates a user attention model based on the visual rhythm analysis for automatically determining the region-of-interest (ROI) in a video. The visual rhythm, an abstraction of a video, is a thumbnail version of a fully video by a 2D image that captures the temporal information of a video sequence. Four sampling lines, including diagonal, anti-diagonal, vertical and horizontal lines, are employed to obtain four visual rhythm maps in order to analyze the location of the ROI from video data. Via the variation on visual rhythms, object and camera motions can be efficiently distinguished. The proposed scheme can extract the ROI accurately with very low computational complexity. The promising results from the experiments demonstrate that the moving object is effectively and efficiently extracted.",
Towards Optimal Multi-level Tiling for Stencil Computations,"Stencil computations form the performance-critical core of many applications. Tiling and parallelization are two important optimizations to speed up stencil computations. Many tiling and parallelization strategies are applicable to a given stencil computation. The best strategy depends not only on the combination of the two techniques, but also on many parameters: tile and loop sizes in each dimension; computation-communication balance of the code; processor architecture; message startup costs; etc. The best choices can only be determined through design-space exploration, which is extremely tedious and error prone to do via exhaustive experimentation. We characterize the space of multi-level tilings and parallelizations for 2D/3D Gauss-Siedel stencil computation. A systematic exploration of a part of this space enabled us to derive a design which is up to a factor of two faster than the standard implementation.","Concurrent computing,
Tiles,
Law,
Legal factors,
Space exploration,
High performance computing,
Parallel processing,
Shape,
Analytical models,
Gaussian processes"
PMScan : A power-managed scan for simultaneous reduction of dynamic and leakage power during scan test,"In sub-70 nm technologies, leakage power becomes a significant component of the total power. Designers address this concern by extensive use of adaptive voltage scaling techniques to reduce dynamic as well as leakage power. Low-power scan test schemes that have evolved in the past primarily address dynamic power reduction, and are less effective in reducing the total power. We propose a power-managed scan (PMScan) scheme which exploits the presence of adaptive voltage scaling logic to reduce test power. We also discuss some practical implementation challenges that arise when the proposed scheme is employed on industrial designs. Experimental results on benchmark circuits and industrial designs show a significant reduction in dynamic and leakage power. The proposed method can also be used as a vehicle to trade-off test application time with test power by suitably adjusting the scan shift frequency and scan-mode power supplies.",
Impact of Auger Recombination on Charge Collection of a 6H-SiC Diode by Heavy Ions,Charge collection efficiency (CCE) generated in a 6H-SiC p+n diode by impact of heavy ions was evaluated by the transient ion beam induced current (TIBIC) technique. Numerical analysis by using technology computer aided design (TCAD) concludes that the Auger recombination process reduces CCE. Comparing experimentally measured CCEs with calculated ones revealed that the ambipolar Auger coefficient of about 3 times 10-29 cm6/s.,"Diodes,
Silicon carbide,
Ion beams,
Large Hadron Collider,
Thermal conductivity,
Radiation detectors,
Alpha particles,
Particle measurements,
Spontaneous emission,
Current measurement"
Multidomain Diagnosis of End-to-End Service Failures in Hierarchically Routed Networks,"Probabilistic inference was shown effective in the nondeterministic diagnosis of end-to-end service failures when applied in a centralized management system where the manager possesses a global knowledge of the system structure and state. Since many networks are organized into multiple administrative domains that may be unable to share configuration and state information, these centralized techniques are not applicable to them. This paper proposes a fault localization technique suitable for multidomain networks with hierarchical routing. The proposed technique divides the computational effort and system knowledge among multiple, hierarchically organized managers. Each manager performs fault localization in the domain it manages and requires only the knowledge of its own domain. We show through simulation that the proposed approach not only improves the feasibility of fault localization in multidomain networks, but also increases the effectiveness of probabilistic diagnosis and makes it realizable in networks of considerable size","Knowledge management,
Condition monitoring,
Fault diagnosis,
Protocols,
Availability,
Computer network management,
Routing,
Computational modeling,
Bridges,
Propagation delay"
A Multiscale Parametric Background Model for Stationary Foreground Object Detection,"Detection of stationary foreground objects within a dynamic scene is one of the goals of a video surveillance system. A parametric background maintenance and updating scheme, based on a multiple Gaussian mixture model that operates on multiple time scales, is proposed. Each color cluster in the proposed model is assigned a weight which measures the time duration and temporal recurrence frequency of the cluster. Sudden illumination changes are handled by using an adaptive histogram template whereas gradual illumination changes are automatically resolved with the adaptive background model. Stationary foreground objects are detected by maintaining their temporal history in the dynamic scene at multiple time scales. Experimental results show that the proposed scheme performs well in three distinct real-world settings.",
Online Appearance Model Learning for Video-Based Face Recognition,"In this paper, we propose a novel online learning method which can learn appearance models incrementally from a given video stream. The data of each frame in the video can be discarded as soon as it has been processed. We only need to maintain a few linear eigenspace models and a transition matrix to approximately construct face appearance manifolds. It is convenient to use these learnt models for video-based face recognition. There are mainly two contributions in this paper. First, we propose an algorithm which can learn appearance models online without using a pre-trained model. Second, we propose a method for eigenspace splitting to prevent that most samples cluster into the same eigenspace. This is useful for clustering and classification. Experimental results show that the proposed method can both learn appearance models online and achieve high recognition rate.",
Customization of Register File Banking Architecture for Low Power,"Register file banking is an effective alternative to monolithic register files in embedded processor based systems. In this work, we propose techniques for performing application specific customization of register file banking structure. First, we propose two techniques based on: (i) profiling; and (ii) static application analysis to arrive at a customized energy-efficient bank configuration for a given application on a dual bank register file. We also propose a technique to extend the exploration to a multi-bank register file architecture and an associated register allocation algorithm for further power reduction. This reduces register file power consumption by allocating variables in frequently accessed basic blocks to separate appropriately sized register file bank of active registers. Experimental results indicate that our customized dual bank configuration inferred by both techniques gives energy savings of 40% over a monolithic register file, and the multi-bank register file customization gives a further 15-20% energy savings","Banking,
Registers,
Reduced instruction set computing,
Energy efficiency,
Energy consumption,
Frequency estimation,
Computer architecture,
Computer science,
Power engineering and energy,
Application software"
I/O Performance Optimization Techniques for Hybrid Hard Disk-Based Mobile Consumer Devices,"A hybrid hard disk employs the advantages of both a hard disk and a NAND flash memory, thus making it a cost-effective fast secondary storage device. In this paper, we improve its I/O performance by combining an intelligent data pinning policy for the flash memory with a caching technique which is aware of access patterns for the flash memory and DRAM. Our proposed techniques reduce the system boot time and application launching time while reducing energy consumption, which is vital in the mobile devices. We built SimHybrid, a flexible trace-driven hybrid hard disk evaluation environment, and used it to demonstrate how a hybrid hard disk can achieve significantly better I/O performance than a traditional hard disk while using much less energy.",
Learning GMRF Structures for Spatial Priors,"The goal of this paper is to find sparse and representative spatial priors that can be applied to part-based object localization. Assuming a GMRF prior over part configurations, we construct the graph structure of the prior by regressing the position of each part on all other parts, and selecting the neighboring edges using a Lasso-based method. This approach produces a prior structure which is not only sparse, but also faithful to the spatial dependencies that are observed in training data. We evaluate the representation power of the learned prior structure in two ways: first is drawing samples from the prior, and comparing them with the samples produced by the GMRF priors of other structures; second is comparing the results when applying different priors to a facial components localization task. We show that the learned graph captures meaningful geometrical variations with significantly sparser structure and leads to better parts localization results.","Training data,
Computational efficiency,
Computer science,
Bayesian methods,
Markov random fields,
Computer vision,
Gaussian distribution,
Computational complexity,
Tree graphs,
Algorithm design and analysis"
Reasoning about Feature Models in Higher-Order Logic,"A mechanically formalized feature modeling meta-model is presented. This theory is a generic higher-order formalization of a mathematical model synthesizing several feature modeling approaches found in the literature. This meta-model supports not only a better understanding of the various approaches to feature modeling, but also supports reasoning about and within feature model approaches, feature models, and on feature trees and their configurations.",
"Software Deployment, Past, Present and Future","This paper examines the dimensions influencing the past and present and speculates on the future of software deployment. Software deployment is a post- production activity that is performed for or by the customer of a piece of software. Today's software often consists of a large number of components each offering and requiring services of other components. Such components are often deployed into distributed, heterogeneous environments adding to the complexity of software deployment. This paper sets out a standard terminology for the various deployment activities and the entities over which they operate. Six case studies of current deployment technologies are made to illustrate various approaches to the deployment problems. The paper then examines specific deployment issues in more detail before examining some of the future directions in which the field of deployment might take.","Software performance,
Assembly,
Computer science,
Operating systems,
Terminology,
Packaging,
Computer languages,
Middleware,
Software systems,
Hardware"
An Approximation Algorithm for Data Storage Placement in Sensor Networks,Data storage has become an important issue in sensor networks as a large amount of collected data needs to be archived for future information retrieval. This paper proposes to introduce storage nodes that can store data collected from the sensors in their proximities. The storage nodes alleviate the heavy load of transmitting all the data to a central place for archiving and reduce the communication cost induced by the network query. This paper considers the storage node placement problem to minimize the total power consumption for data funneling to the storage nodes and data query. We formulate it as an integer linear programming problem and present an approximation algorithm based on a rounding technique. Our simulation shows that our approximation algorithm performs well in practice.,"Approximation algorithms,
Memory,
Wireless sensor networks,
Costs,
Energy consumption,
Sensor systems and applications,
Batteries,
Sensor systems,
Application software,
Computer science"
Web Information Extraction by HTML Tree Edit Distance Matching,"The main issue for effective Web information extraction is how to recognize similar patterns in a Web page. Traditionally, it has been shown that pattern matching by using the HTML DOM tree is more efficient than the simple string matching approach. Nonetheless, previous tree-based pattern matching methods have problems by assuming that all HTML tags have the same values, assigning the same weight to each node in HTML trees. This paper proposes an enhanced tree matching algorithm that improves the tree edit distance method by considering the characteristics of HTML features. We assign different values to different HTML tree nodes according to their weights for displaying the corresponding data objects in the browser. Pattern matching of HTML patterns is done by obtaining the maximum mapping values of two HTML trees that are constructed with weighted node values from HTML data objects. Experiments are done over several Web commerce sites to evaluate the effectiveness of the proposed HTML tree matching algorithm.","Data mining,
HTML,
Pattern matching,
Pattern recognition,
Web pages,
Vegetation mapping,
Dynamic programming,
Information technology,
Computer science,
Business"
3D Face Recognition in the Presence of Expression: A Guidance-based Constraint Deformation Approach,"Three-dimensional human face recognition in the presence of expression is a big challenge, since the shape distortion caused by facial expression greatly weakens the rigid matching. This paper proposes a guidance-based constraint deformation(GCD) model to cope with the shape distortion by expression. The basic idea is that, the face model with non-neutral expression is deformed toward its neutral one under certain constraint so that the distortion is reduced while inter-class discriminative information is preserved. The GCD model exploits the neutral 3D face shape to guide the deformation, meanwhile applies a rigid constraint on it. Both steps are smoothly unified in the Poisson equation framework. The GCD approach only needs one neutral model for each person in the gallery. The experimental results, carried out on the large 3D face databases-FRGC v2.0, demonstrate that our method significantly outperforms ICP method for both identification and authentication mode. It shows the GCD model is promising for coping with the shape distortion in 3D face recognition.","Face recognition,
Deformable models,
Probes,
Humans,
Surface treatment,
Shape measurement,
Educational institutions,
Poisson equations,
Databases,
Authentication"
Control of DFIG-Based Wind Generation Systems under Unbalanced Network Supply,"This paper develops a dynamic model and control scheme for DFIG systems to improve the performance and stability under unbalanced grid conditions. A dynamic DFIG model containing the positive and negative sequence components is presented using stator voltage orientation. The proposed model accurately illustrates the active power, reactive power and torque oscillations, and provides a basis for DFIG control system design during unbalanced network supply. Various control targets such as eliminating the oscillations of the torque, active/reactive power are discussed and the required rotor negative sequence current for fulfilling different control targets are described. Performance of a DFIG-based wind turbine under unbalanced condition using the proposed control method is evaluated by simulation studies using Matlab/Simulink. The proposed control scheme significantly attenuates the DFIG torque or active power oscillations during network unbalance whereas significant torque/power oscillations exist with the conventional control schemes.","Control systems,
Torque control,
Mathematical model,
Power system modeling,
Reactive power control,
Stability,
Stators,
Voltage,
Control system synthesis,
Wind turbines"
Numerical Reconstruction of the HOSVD Based Canonical Form of Polytopic Dynamic Models,The main objective of the paper is to introduce how the concept of tensor HOSVD (higher order singular value decomposition) can be carried over to the TP (tensor product) dynamic models. We term this decomposition as HOSVD based canonical form of TP model or polytopic model form. The key idea and the basic concept of this decomposition was proposed recently with the TP model transformation based control design methodology. The novelty of this paper is to present the mathematical background of this concept. The paper shows convergency theorems how the TP model transformation is capable of reconstructing this HOSVD based canonical form numerically.,"Tensile stress,
Control design,
Linear matrix inequalities,
Nonlinear dynamical systems,
Vectors,
Informatics,
Automation,
Lightweight structures,
Singular value decomposition,
Matrix decomposition"
Tracking Multiple Targets Using Binary Proximity Sensors,"Recent work has shown that, despite the minimal information provided by a binary proximity sensor, a network of such sensors can provide remarkably good target tracking performance. In this paper, we examine the performance of such a sensor network for tracking multiple targets. We begin with geometric arguments that address the problem of counting the number of distinct targets, given a snapshot of the sensor readings. We provide necessary and sufficient criteria for an accurate target count in a one-dimensional setting, and provide a greedy algorithm that determines the minimum number of targets that is consistent with the sensor readings. While these combinatorial arguments bring out the difficulty of target counting based on sensor readings at a given time, they leave open the possibility of accurate counting and tracking by exploiting the evolution of the sensor readings across time. To this end, we develop a particle filtering algorithm based on a cost function that penalizes changes in velocity. An extensive set of simulations, as well as experiments with passive infrared sensors, are reported. We conclude that, despite the combinatorial complexity of target counting, probabilistic approaches based on fairly generic models for the trajectories yield respectable tracking performance.","Target tracking,
Magnetic sensors,
Acoustic sensors,
Infrared image sensors,
Collaborative work,
Radar tracking,
Ultra wideband radar,
Radar imaging,
Millimeter wave radar,
Magnetometers"
Semantic Hierarchies for Recognizing Objects and Parts,"This paper describes the construction and use of a novel representation for the recognition of objects and their parts, the semantic hierarchy. Its advantages include improved classification performance, accurate detection and localization of object parts and sub-parts, and explicitly identifying the different appearances of each object part. The semantic hierarchy algorithm starts by constructing a minimal feature hierarchy and proceeds by adding semantically equivalent representatives to each node, using the entire hierarchy as a context for determining the identity and locations of added features. Part detection is obtained by a bottom-up top-down cycle. Unlike previous approaches, the semantic hierarchy learns to represent the set of possible appearances of object parts at all levels, and their statistical dependencies. The algorithm is fully automatic and is shown experimentally to substantially improve the recognition of objects and their parts.","Mouth,
Object detection,
Computer science,
Mathematics,
Lighting,
Shape,
Probability distribution,
Nose,
Horses,
Graphical models"
Exploiting peak anisotropy for tracking through complex structures,"This work shows that multi-fibre reconstruction techniques, such as Persistent Angular Structure (PAS) MRI or QBall Imaging, provide much more information than just discrete fibre orientations, which is all that previous tractography algorithms exploit from them. We show that the shapes of the peaks of the functions output by multiple-fibre reconstruction algorithms reflect the underlying distribution of fibres. Furthermore, we show how to exploit this extra information to improve Probabilistic Index of Connectivity (PICo) tractography. The method uses the Bingham distribution to model the uncertainty in fibre-orientation estimates obtained from peaks in the PAS or QBall Orientation Distribution Function (ODF). The Bingham model captures anisotropy in the uncertainty, allowing the method to track through fanning and bending structures, which previous methods do not recover reliably. We devise a new calibration procedure to construct a mapping from peak shape to Bingham parameters. We test the accuracy of the calibration using a bootstrap experiment. Finally, we show that exploiting the peak shape in this way can provide improved PICo tractography results.","Anisotropic magnetoresistance,
Image reconstruction,
Magnetic resonance imaging,
Shape,
Diffusion tensor imaging,
Biomedical imaging,
Uncertainty,
Reconstruction algorithms,
Calibration,
Streaming media"
Triangulation Based Multi Target Tracking with Mobile Sensor Networks,"We study the problem of designing motion-planning and sensor assignment strategies for tracking multiple targets with a mobile sensor network. We focus on triangulation based tracking where two sensors merge their measurements in order to estimate the position of a target. We present an iterative and distributed algorithm for the tracking problem. An iteration starts with an initialization phase where targets are assigned to sensor pairs. Afterwards, assigned sensors relocate to improve their estimates. We refer to the problem of computing new locations for sensors (for given target assignments) as one-step tracking. After observing that one-step tracking is computationally hard, we show how it can be formulated as an energy-minimization problem. This allows us to adapt well-studied distributed algorithms for energy minimization. We present simulations to compare the performance of two such algorithms and conclude the paper with a description of the full tracking strategy. The utility of the presented strategy is demonstrated with simulations and experiments on a sensor network platform","Target tracking,
Distributed algorithms,
Mobile computing,
Minimization methods,
Mobile communication,
Computer networks,
Image sensors,
Energy consumption,
Robotics and automation,
Position measurement"
Cooperative diversity in the presence of a misbehaving relay : Performance analysis,"Cooperative wireless communications offers a new dimension of diversity by emulating transmit antenna diversity to provide reliable communications. In cooperative diversity, single-antenna radios behave as relays between a source and destination, and the performance improvements are due to cooperation of the source and the relay. However, a misbehaving relay can degrade the envisaged performance improvements severely. In practice, there are no mechanisms to ensure adherence of the relay to cooperation strategy. Due to this dependency on relaypsilas behavior cooperative diversity presents a new security challenge at the physical layer. In this paper, we investigate performance of cooperative diversity in the presence of a semi-malicious relay which does not conform to rules of cooperation at all time. The relay behavior is characterized by a probabilistic cooperation model which exploits the uncertainty in the wireless channel. Based on this model, we obtain the performance degradation in cooperative diversity both by analysis and simulation.","Relays,
Performance analysis,
Diversity reception,
Wireless communication,
Physical layer,
Communication system security,
Decoding,
Degradation,
Quality of service,
Transmitting antennas"
A Family of Directional Relation Models for Extended Objects,"In this paper, we introduce a family of expressive models for qualitative spatial reasoning with directions. The proposed family is based on the cognitive plausible cone-based model. We formally define the directional relations that can be expressed in each model of the family. Then, we use our formal framework to study two interesting problems: computing the inverse of a directional relation and composing two directional relations. For the composition operator, in particular, we concentrate on two commonly used definitions, namely, consistency-based and existential composition. Our formal framework allows us to prove that our solutions are correct. The presented solutions are handled in a uniform manner and apply to all of the models of the family.","Geographic Information Systems,
Artificial intelligence,
Multimedia databases,
Computer Society,
Spatial databases,
Intelligent systems,
Deductive databases,
Intelligent structures,
Multimedia systems"
Automatic Web Services Composition Using Combining HTN and CSP,"Semantic Web services and their composition portend a future where the Web can behave more intelligently for solving general real-life problems. To solve such real-life problems requires a set of appropriate services to be composed via planning, scheduled, and then executed: those are logical services composition, physical composition, and execution. Web service composition is the most difficult aspect and is our focus. In this paper, we propose a framework combining logical composition (using HTN) and physical composition (using CSP) for automatic services composition. The framework covers the entire procedures to deal with a user's request, domain analysis of the request, task flow decision and CSP creation by the planner, and solving the CSP by a distributed CSP solver for intelligent Web service composition. Implementation of the framework and an evaluation of it in terms of various problem solving types are then explained. We also discuss the necessity and synergy effect of the combined HTN and CSP for a framework that automates Web service composition and execution.","Web services,
Semantic Web,
Ontologies,
Problem-solving,
Artificial intelligence,
Power system planning,
Meeting planning,
Information technology,
Computer science,
Processor scheduling"
Parallel Scripting with Python,The combination of the Python language and the bulk synchronous parallel computing model make developing and testing parallel programs a much more pleasurable experience.,"Concurrent computing,
Parallel processing,
Parallel programming,
Computer languages,
Context modeling,
Computational modeling,
Multithreading,
Tellurium,
Seminars,
Scientific computing"
Face Recognition for Smart Interactions,"In this paper an overview of face recognition research activities at the interACT Research Center is given. The face recognition efforts at the interACT Research Center consist of development of a fast and robust face recognition algorithm and fully automatic face recognition systems that can be deployed for real-life smart interaction applications. The face recognition algorithm is based on appearances of local facial regions that are represented with discrete cosine transform coefficients. Three fully automatic face recognition systems have been developed that are based on this algorithm. The first one is the ""door monitoring system"" that observes the entrance of a room and identifies the subjects while they are entering the room. The second one is the ""portable face recognition system"" that aims at environment-free face recognition and recognizes the user of a machine. The third system, ""3D face recognition system"", performs fully automatic face recognition on 3D range data.","Face recognition,
Application software,
Magnetic heads,
Lighting,
Portable computers,
Computer science,
Electronic mail,
Robustness,
Discrete cosine transforms,
Monitoring"
Leveraging RSSI for Robotic Repair of Disconnected Wireless Sensor Networks,"Many recent deployments of environmental sensor networks have focused on obtaining measurements across large and inhospitable areas. With increasing scale it becomes impractical to deploy or maintain such systems by hand. This paper evaluates large scale network disconnectivity and highlights the underlying issues related to the environment and node characteristics. Furthermore, it examines how a low cost and adaptive method of robotic repair can be applied to large area networks using received signal strength measurements for simple navigation and placement.","Robot sensing systems,
Wireless sensor networks,
Robotics and automation,
USA Councils,
Area measurement,
Large-scale systems,
Fresnel reflection,
Computer science,
Cities and towns,
Sensor phenomena and characterization"
Fritz - A Humanoid Communication Robot,"In this paper, we present the humanoid communication robot Fritz. Our robot communicates with people in an intuitive, multimodal way. Fritz uses speech, facial expressions, eye-gaze, and gestures to interact with people. Depending on the audio-visual input, our robot shifts its attention between different persons in order to involve them into the conversation. He performs human-like arm gestures during the conversation and also uses pointing gestures generated with eyes, head, and arms to direct the attention of its communication partners towards objects of interest. To express its emotional state, the robot generates facial expressions and adapts the speech synthesis. We discuss experiences made during two public demonstrations of our robot.","Humanoid robots,
Robot sensing systems,
Human robot interaction,
Eyes,
Magnetic heads,
Arm,
Speech synthesis,
User interfaces,
Facial animation,
Computer science"
Hierarchical Spatial Hashing for Real-time Collision Detection,"We present a new, efficient and easy to use collision detection scheme for real-time collision detection between highly deformable tetrahedral models. Tetrahedral models are a common representation of volumetric meshes which are often used in physically based simulations, e.g. in Virtual surgery. In a deformable models environment collision detection usually is a performance bottleneck since the data structures used for efficient intersection tests need to be rebuilt or modified frequently. Our approach minimizes the time needed for building a collision detection data structure. We employ an infinite hierarchical spatial grid in which for each single tetrahedron in the scene a well fitting grid cell size is computed. A hash function is used to project occupied grid cells into a finite ID hash table. Only primitives mapped to the same hash index indicate a possible collision and need to be checked for intersections. This results in a high performance collision detection algorithm which does not depend on user defined parameters and thus flexibly adapts to any scene setup.",
Lightweight kernel-level primitives for high-performance MPI intra-node communication over multi-core systems,"Modern processors have multiple cores on a chip to overcome power consumption and heat dissipation issues. As more and more compute cores become available on a single node, it is expected that node-local communication will play an increasingly greater role in overall performance of parallel applications such as MPI applications. It is therefore crucial to optimize intra-node communication paths utilized by MPI libraries. In this paper, we propose a novel design of a kernel extension, called LiMIC2, for high-performance MPI intra-node communication over multi-core systems. LiMIC2 can minimize the communication overheads by implementing lightweight primitives and provide portability across different interconnects and flexibility for performance optimization. Our performance evaluation indicates that LiMIC2 can attain 80% lower latency and more than three times improvement in bandwidth. Also the experimental results show that LiMIC2 can deliver bidirectional bandwidth greater than 11GB/s.","Libraries,
Bandwidth,
Kernel,
Protocols,
Magnetic cores,
Receivers,
Driver circuits"
Learning Conversations in World of Warcraft,"We examine learning culture in a popular online game, world of warcraft. We analyze the way players learn this complex game through chat conversation with peers. We describe three kinds of learning: fact finding, devising tactics/strategy, and acquiring game ethos. We investigate learning in the zone of proximal development as specified in cultural-historical activity theory. We examine the emotional tenor of learning conversations, noting their drama, humor, and intimacy","Games,
Ethics,
North America,
Asia,
Europe,
Internet,
Educational institutions,
Discussion forums"
Using Revenue Management to Determine Pricing of Reservations,"Grid economy provides a mechanism or incentive for resource owners to be part of the Grid, and encourages users to utilize resources optimally and effectively. Advance reservation technique allows users to request resources in the future. However, few research has been done on determining pricing of such reservations. In this paper, we present a novel approach of using revenue management (RM) to determine pricing of reservations in Grids in order to increase pro ts. Hence, the aim of RM is to periodically update the prices in response to market demands, by charging different fares to different customers for a same resource. We evaluate the effectiveness of RM and show that by segmenting customers, charging them with different pricing schemes and protecting resources for them who are willing to pay more, will result in an increase of total revenue for that resource. Moreover, using RM techniques ensure that resources are allocated to applications that are highly valued by the users.","Pricing,
Resource management,
Processor scheduling,
Grid computing,
Computer networks,
Peer to peer computing,
Computer applications,
Quality of service,
Conference management,
Engineering management"
Optimal Linear Combination of Facial Regions for Improving Identification Performance,"This paper presents a novel 3D multiregion face recognition algorithm that consists of new geometric summation invariant features and an optimal linear feature fusion method. A summation invariant, which captures local characteristics of a facial surface, is extracted from multiple subregions of a 3D range image as the discriminative features. Similarity scores between two range images are calculated from the selected subregions. A novel fusion method that is based on a linear discriminant analysis is developed to maximize the verification rate by a weighted combination of these similarity scores. Experiments on the Face Recognition Grand Challenge V2.0 dataset show that this new algorithm improves the recognition performance significantly in the presence of facial expressions.","Face recognition,
Nose,
Feature extraction,
Linear discriminant analysis,
Testing,
Large-scale systems,
Bones,
Aging,
Councils,
Computer science"
Keypoint Descriptors for Matching Across Multiple Image Modalities and Non-linear Intensity Variations,"In this paper, we investigate the effect of substantial inter-image intensity changes and changes in modality on the performance of keypoint detection, description, and matching algorithms in the context of image registration. In doing so, we modify widely-used keypoint descriptors such as SIFT and shape contexts, attempting to capture the insight that some structural information is indeed preserved between images despite dramatic appearance changes. These extensions include (a) pairing opposite-direction gradients in the formation of orientation histograms and (b) focusing on edge structures only. We also compare the stability of MSER, Laplacian-of-Gaussian, and Harris corner keypoint location detection and the impact of detection errors on matching results. Our experiments on multimodal image pairs and on image pairs with significant intensity differences show that indexing based on our modified descriptors produces more correct matches on difficult pairs than current techniques at the cost of a small decrease in performance on easier pairs. This extends the applicability of image registration algorithms such as the Dual-Bootstrap which rely on correctly matching only a small number of keypoints.","Image registration,
US Department of Defense,
Biomedical imaging,
Change detection algorithms,
Lighting,
Computer science,
Shape,
Histograms,
Image edge detection,
Stability"
Surface Dependent Representations for Illumination Insensitive Image Comparison,"We consider the problem of matching images to tell whether they come from the same scene viewed under different lighting conditions. We show that the surface characteristics determine the type of image comparison method that should be used. Previous work has shown the effectiveness of comparing the image gradient direction for surfaces with material properties that change rapidly in one direction. We show analytically that two other widely used methods, normalized correlation of small windows and comparison of multiscale oriented filters, essentially compute the same thing. Then, we show that for surfaces whose properties change more slowly, comparison of the output of whitening filters is most effective. This suggests that a combination of these strategies should be employed to compare general objects. We discuss indications that Gabor jets use such a mixed strategy effectively, and we propose a new mixed strategy. We validate our results on synthetic and real images",
Providing Quality of Service Support in Object-Based File System,"Bourbon is a quality of service framework designed to work with the Ceph object-based storage system. Ceph is a highly scalable distributed file system that can scale up to tens of thousands of object-based storage devices (OSDs). The Bourbon framework enables Ceph to become QoS-aware by providing the capability to isolate performance between different classes of workloads. The Bourbon framework is enabled by Q-EBOFS, a QoS-aware enhancement of the EBOFS object-based file system. Q-EBOFS allows individual OSDs to become QoS-aware, and by leveraging on the random element of the CRUSH data distribution algorithm employed by Ceph, it is possible for a collection of independent QoS-aware OSDs to provide class-based performance isolation at the global level. This preserves the highly scalable nature of Ceph by avoiding the introduction of any centralized components or the need to collect and propagate global state information. This paper presents the Bourbon framework by first describing Q-EBOFS, and then examines how a collection of OSDs running Q-EBOFS can work together to provide global-level QoS.",
Towards Sharp Inapproximability For Any 2-CSP,"We continue the recent line of work on the connection between semidefinite programming-based approximation algorithms and the Unique Games Conjecture. Given any-boolean 2-CSP (or more generally, any nonnegative objective function on two boolean variables), we show how to reduce the search for a good inapproximability result to a certain numeric minimization problem. The key objects in our analysis are the vector triples arising when doing clause-by-clause analysis of algorithms based on semidefinite programming. Given a weighted set of such triples of a certain restricted type, which are ""hard"" to round in a certain sense, we obtain a Unique Games-based inapproximability matching this ""hardness"" of rounding the set of vector triples. Conversely, any instance together with an SDP solution can be viewed as a set of vector triples, and we show that we can always find an assignment to the instance which is at least as good as the ""hardness"" of rounding the corresponding set of vector triples. We conjecture that the restricted type required for the hardness result is in fact no restriction, which would imply that these upper and lower bounds match exactly. This conjecture is supported by all existing results for specific 2-CSPs. As an application, we show that Max 2-AND is hard to approximate within 0.87435. This improves upon the best previous hardness of alphaGW + epsi ap 0.87856, and comes very close to matching the approximation ratio of the best algorithm known, 0.87401. It also establishes that balanced instances of Max 2-AND, i.e., instances in which each variable occurs positively and negatively equally often, are not the hardest to approximate, as these can be approximated within a factor alphaGW.","Approximation algorithms,
Algorithm design and analysis,
Neodymium,
Computer science,
Minimization"
On-the-fly Object Modeling while Tracking,"To implement a persistent tracker, we build a set of view-dependent object appearance models adoptively and automatically while tracking an object under different viewing angles. This collection of acquired models is indexed with respect to the view sphere. The acquired models aid recovery from tracking failure due to occlusion and changing view angle. In this paper, view-dependent object appearance is represented by intensity patches around detected Harris corners. The intensity patches from a model are matched to the current frame by solving a bipartite linear assignment problem with outlier exclusion and missed inlier recovery. Based on these reliable matches, the change in object rotation, translation and scale is estimated between consecutive frames using Procrustes analysis. The experimental results show good performance using a collection of view-specific patch-based models for detection and tracking of vehicles in low-resolution airborne video.","Target tracking,
Object detection,
Machine learning,
Solid modeling,
Character recognition,
Computer science,
Vehicle detection,
Computer vision,
Spatial resolution,
Image resolution"
Disk Aware Discord Discovery: Finding Unusual Time Series in Terabyte Sized Datasets,"The problem of finding unusual time series has recently attracted much attention, and several promising methods are now in the literature. However, virtually all proposed methods assume that the data reside in main memory. For many real-world problems this is not be the case. For example, in astronomy, multi-terabyte time series datasets are the norm. Most current algorithms faced with data which cannot fit in main memory resort to multiple scans of the disk/tape and are thus intractable. In this work we show how one particular definition of unusual time series, the time series discord, can be discovered with a disk aware algorithm. The proposed algorithm is exact and requires only two linear scans of the disk with a tiny buffer of main memory. Furthermore, it is very simple to implement. We use the algorithm to provide further evidence of the effectiveness of the discord definition in areas as diverse as astronomy, Web query mining, video surveillance, etc., and show the efficiency of our method on datasets which are many orders of magnitude larger than anything else attempted in the literature.","Astronomy,
Computer science,
USA Councils,
Video surveillance,
Portfolios,
Data mining,
Data engineering,
Search engines,
Intrusion detection,
Investments"
An Ant Based Hyper-heuristic for the Travelling Tournament Problem,"The travelling tournament problem is a challenging sports timetabling problem which is widely believed to be NP-hard. The objective is to establish a feasible double round robin tournament schedule, with minimum travel distances. This paper investigates the application of an ant based hyper-heuristic algorithm for this problem. Ant algorithms, a well known meta-heuristic, have been successfully applied to various problems. Whilst hyper-heuristics are an emerging technology, which operate at a higher level of abstraction than meta-heuristics. This paper presents a framework which employs ant algorithms as a hyper-heuristic. We show that this approach produces good quality solutions for the traveling tournament problem when compared with results from the literature","Round robin,
TV,
Computer science,
Computational intelligence,
Processor scheduling,
Educational institutions,
Symmetric matrices,
Repeaters"
An Ad Hoc Review of Digital Forensic Models,"Digital forensics has been the subject of academic study for a relatively brief period of time. One of the foundational ways in which researchers try to understand the scientific basis of a discipline is to construct models which reflect their observations. This paper reviews a collection of fifteen published papers which represent data points in the development of digital forensic models. It is neither an exhaustive review, nor exhaustive list of all available papers","Digital forensics,
Education,
Internet,
Fading,
Information systems,
Computer security,
Data security,
Information security,
National security,
Knowledge engineering"
Supervised Learning by Training on Aggregate Outputs,"Supervised learning is a classic data mining problem where one wishes to be be able to predict an output value associated with a particular input vector. We present a new twist on this classic problem where, instead of having the training set contain an individual output value for each input vector, the output values in the training set are only given in aggregate over a number of input vectors. This new problem arose from a particular need in learning on mass spectrometry data, but could easily apply to situations when data has been aggregated in order to maintain privacy. We provide a formal description of this new problem for both classification and regression. We then examine how k-nearest neighbor, neural networks, and support vector machines can be adapted for this problem.",
Automatic Music Genre Classification using Modulation Spectral Contrast Feature,"In this paper, we proposed a novel feature, called octave-based modulation spectral contrast (OMSC), for music genre classification. OMSC is extracted from long-term modulation spectrum analysis to represent the time-varying behavior of music signals. Experimental results have shown that OMSC outperforms MFCC and OSC. If OMSC is integrated with MFCC and OSC, the classification accuracy is 84.03% for seven music genre classification.","Mel frequency cepstral coefficient,
Multiple signal classification,
Feature extraction,
Support vector machines,
Support vector machine classification,
Data mining,
Speech,
Cepstral analysis,
Histograms,
Linear discriminant analysis"
Extracting Terrain Features from Range Images for Autonomous Random Stepfield Traversal,"One of the challenges of rescue robotics is to create robots that can autonomously traverse rough, unstructured terrain. Although mechanical engineering can produce very capable robots, mechanical engineering alone will not drive them. In this paper, we present a terrain feature extractor that can be taught to find significant features in range images of terrain around a robot from a human expert. This novel approach has the advantage that it potentially allows the human expert's knowledge to be captured rapidly. A terrain model is generated from the many points in the range sensor data. Techniques from the field of knowledge acquisition are then used to find patterns in the terrain model. A knowledge acquisition system can then be taught to drive a robot in unstructured terrain based on these features. We evaluate the performance of the initial stages of the feature extractor on a real robot, traversing NIST specification red stepfields.","Feature extraction,
Robot sensing systems,
NIST,
Humans,
Mechanical engineering,
Computer science,
Australia,
Image sensors,
Mechanical sensors,
Sensor phenomena and characterization"
Bipedal Walking and Running with Compliant Legs,"Passive dynamics plays an important role in legged locomotion of the biological systems. The use of passive dynamics provides a number of advantages in legged locomotion such as energy efficiency, self-stabilization against disturbances, and generating gait patterns and behavioral diversity. Inspired from the theoretical and experimental studies in biomechanics, this paper presents a novel bipedal locomotion model for walking and running behavior which uses compliant legs. This model consists of three-segment legs, two servomotors, and four passive joints that are constrained by eight tension springs. The self-organization of two gait patterns (walking and running) is demonstrated in simulation and in a real-world robot. The analysis of joint kinematics and ground reaction force explains how a minimalistic control architecture can exploit the particular leg design for generating different gait patterns. Moreover, it is shown how the proposed model can be extended for controlling locomotion velocity and gait patterns with the simplest control architecture.","Legged locomotion,
Leg,
Velocity control,
Biological systems,
Energy efficiency,
Biomechanics,
Biological system modeling,
Servomotors,
Springs,
Pattern analysis"
Technology-Assisted Learning and Learning Style: A Longitudinal Field Experiment,"From a student's perspective, technology-assisted learning provides convenient access to interactive contents in a hyperlinked multimedia environment that allows increased control over the pace and timing of the presented material. Previous research examining different aspects of technology-assisted learning has found equivocal results concerning its effectiveness and outcomes. We extend prior studies by conducting a longitudinal field experiment to compare technology-assisted with face-to-face learning for students' learning of English. Our comparative investigation focuses on learning effectiveness, perceived course learnability, learning-community support, and learning satisfaction. In addition, we analyze the effects of different learning styles in moderating the effectiveness of and satisfaction with technology-assisted learning. Overall, our results show significantly greater learning effectiveness with technology-assisted learning than with conventional face-to-face learning. Learning style has noticeable influences on the effectiveness and outcomes of technology-assisted learning. We also observe an apparently important interaction effect with the medium for delivery, which may partially explain the equivocal results of previous research.","Information technology,
Technology management,
Electronic learning,
Timing,
Conducting materials,
Computer aided instruction,
Educational technology,
Computer science education,
Multimedia systems,
Councils"
A Component Based Deformable Model for Generalized Face Alignment,"This paper presents a component based deformable model for generalized face alignment, in which a novel bi-stage statistical framework is proposed to account for both local and global shape characteristics. Instead of using statistical analysis on the entire shape as in previous alignment work, we build separate Gaussian models for shape components to preserve more detailed local shape deformations. In each model of components the Markov Network is integrated to provide simple geometry constraints for our search strategy. In order to make a better description of the nonlinear interrelationships over the shape components, the Gaussian process latent variable model is adopted to obtain enough control of full range shape variations. Furthermore, we propose an illumination-robust feature to lead the local fitting of every shape point when light conditions change dramatically. Based on this approach, our system can generate optimal shape for images with exaggerated expressions and under variable illumination, as evidenced by extensive experimentation.","Deformable models,
Principal component analysis,
Markov random fields,
Facial animation,
Face detection,
Image reconstruction,
Shape control,
Active shape model,
Biological system modeling,
Gaussian processes"
Development of a Wearable Vibrotactile Feedback Suit for Accelerated Human Motor Learning,"When a human learns a new motor skill from a teacher, they learn using multiple channels: They receive high level information aurally about the skill, visual information about how another performs the skill, and at times, tactile information, from a teacher's physical guidance of the student. This research proposes a novel approach, the application of this tactile feedback through a robotic wearable system, while a student tries to learn from a teacher. Initial tests on a 5-DOF robotic suit show a decrease in motion errors of over 20%, and an accelerated learning rate of 7%, both conservative given the system setup and statistically very significant (p les 0.01). This research is intended in use of sports training, motor rehabilitation after neurological damage, dance, postural retraining for health, and many other contexts.","Acceleration,
Humans,
Optical feedback,
Educational robots,
Education,
Machine vision,
Software performance,
Real time systems,
Robotics and automation,
System testing"
Year,,
Visual Data Mining in Software Archives to Detect How Developers Work Together,"Analyzing the check-in information of open source software projects which use a version control system such as CVS or SUBVERSION can yield interesting and important insights into the programming behavior of developers. As in every major project tasks are assigned to many developers, the development must be coordinated between these programmers. This paper describes three visualization techniques that help to examine how programmers work together, e.g. if they work as a team or if they develop their part of the software separate from each other. Furthermore, phases of stagnation in the lifetime of a project can be uncovered and thus, possible problems are revealed. To demonstrate the usefulness of these visualization techniques we performed case studies on two open source projects. In these studies interesting patterns of developers' behavior, e.g. the specialization on a certain module can be observed. Moreover, modules that have been changed by many developers can be identified as well as such ones that have been altered by only one programmer.","Data mining,
Open source software,
Programming profession,
Data visualization,
Information analysis,
Control systems,
Information retrieval,
Frequency,
Computer science,
Mathematical programming"
A Novel Method to Estimate IP Traffic Matrix,"This letter proposes a novel method to estimate IP traffic matrix (TM). By using the generalized matrix inverse, we are able to overcome the challenge of ill-posed nature of the problem. We describe the inference of TM into an optimization problem and then solve this problem by calculating the {1}- INVERSE of the routing matrix. Numerical results are provided to demonstrate the accuracy of this method.",
FPGA implementation of 3D discrete wavelet transform for real-time medical imaging,"3D discrete wavelet transform (DWT) is a compute-intensive task that is usually implemented on specific architectures in many real-time medical imaging systems. In this paper, a novel area-efficient high-throughput 3D DWT architecture is proposed based on distributed arithmetic. A tap-merging technique is used to reduce the size of DA lookup tables. The proposed architectures were designed in VHDL and mapped to a Xilinx Virtex-E FPGA. The synthesis results show the proposed architecture has a low area cost and can run up to 85 MHz, which can perform a five-level 3D wavelet analysis for seven 128 times 128 times 128 volume images per second.","Discrete wavelet transforms,
Field programmable gate arrays,
Biomedical imaging,
Computer architecture,
Real time systems,
Arithmetic,
Table lookup,
Costs,
Wavelet analysis,
Image analysis"
Improved prediction of heart motion using an adaptive filter for robot assisted beating heart surgery,"Robot assisted heart surgery allows surgeons to operate on a heart while it is still beating as if it had been stopped. The robot actively cancels heart motion by closely following a point of interest (POI) on the heart surface -- a process called active relative motion canceling (ARMC). Due to the high bandwidth of the POI motion, it is necessary to supply the controller with an estimate of the immediate future of the POI over a prediction horizon. In this paper, a prediction algorithm, using an adaptive filter to generate future position estimates, is implemented and studied. The effects of predictor parameters on tracking performance are studied. Finally, the predictor is evaluated using a 3 degrees of freedom test-bed and prerecorded heart motion data.","Heart,
Adaptive filters,
Surgery,
Motion estimation,
Motion control,
Tracking,
Intelligent robots,
USA Councils,
Bandwidth,
Blood vessels"
Performance Benchmark of DSP and FPGA Implementations of Low-Level Vision Algorithms,"Selecting an embedded hardware platform for image processing has a big influence on the achievable performance. This paper reports our work on a performance benchmark of different implementations of some low-level vision algorithms. The algorithms are implemented on both Digital Signal Processor (DSP) and Field Programmable Gate Array (FPGA) high-speed embedded platforms. The target platforms are a TITMS320C6414 DSP and an Altera Stratix FPGA. The implementations are evaluated, compared and discussed. The DSP implementations outperform the FPGA implementations, but at the cost of spending all its resources to these tasks. FPGAs, however, are well suited to algorithms, which benefit from parallel execution.","Digital signal processing,
Field programmable gate arrays,
Signal processing algorithms,
Image processing,
Computer vision,
Hardware,
Application software,
Embedded system,
Costs,
Digital signal processors"
Background Removal of Multiview Images by Learning Shape Priors,"Image-based rendering has been successfully used to display 3-D objects for many applications. A well-known example is the object movie, which is an image-based 3-D object composed of a collection of 2-D images taken from many different viewpoints of a 3-D object. In order to integrate image-based 3-D objects into a chosen scene (e.g., a panorama), one has to meet a hard challenge-to efficiently and effectively remove the background from the foreground object. This problem is referred to as multiview images (MVIs) segmentation. Another task requires MVI segmentation is image-based 3-D reconstruction using multiview images. In this paper, we propose a new method for segmenting MVI, which integrates some useful algorithms, including the well-known graph-cut image segmentation and volumetric graph-cut. The main idea is to incorporate the shape prior into the image segmentation process. The shape prior introduced into every image of the MVI is extracted from the 3-D model reconstructed by using the volumetric graph cuts algorithm. Here, the constraint obtained from the discrete medial axis is adopted to improve the reconstruction algorithm. The proposed MVI segmentation process requires only a small amount of user intervention, which is to select a subset of acceptable segmentations of the MVI after the initial segmentation process. According to our experiments, the proposed method can provide not only good MVI segmentation, but also provide acceptable 3-D reconstructed models for certain less-demanding applications.","Shape,
Image segmentation,
Motion pictures,
Rendering (computer graphics),
Information science,
Layout,
Three dimensional displays,
Image reconstruction,
Virtual reality,
Computer science"
Managing Top-down Changes in Service-Oriented Enterprises,"A Service Oriented Enterprise (SOE) provides an efficient and flexible platform where multiple Web services can cooperate together to provide a value-added service. Change management is one of the fundamental issues in enabling SOEs. In this paper, we propose a framework that facilitates in automatically managing top-down changes in SOEs. We start with formalizing a SOE's schema since it is a central concept for specifying and managing top-down changes in SOEs. We then propose a change model as a guide to react to changes. Algorithms are proposed to implement changes by refining a SOE's behavior.","Web services,
Collaboration,
Member services,
Humans,
Concrete,
Space exploration,
Computer science,
Explosions,
Innovation management,
Interference"
The Research of Improved Apriori Algorithm for Mining Association Rules,"The efficiency of mining association rules is an important field of Knowledge Discovery in Databases. The Apriori algorithm is a classical algorithm in mining association rules. This paper presents an improved Apriori algorithm to increase the efficiency of generating association rules. This algorithm adopts a new method to reduce the redundant generation of sub-itemsets during pruning the candidate itemsets, which can form directly the set of frequent itemsets and eliminate candidates having a subset that is not frequent in the meantime. This algorithm can raise the probability of obtaining information in scanning database and reduce the potential scale of itemsets.","Data mining,
Association rules,
Itemsets,
Frequency,
Transaction databases,
Educational institutions,
Testing,
Algorithm design and analysis"
Analog placement with common centroid constraints,"In order to reduce parasitic mismatch in analog circuits, some groups of devices are required to share a common centroid while being placed. Devices are split into smaller ones and placed with a common center point. We will address this problem of handling common centroid constraint in placement. A new representation called center-based corner block list (C-CBL) is proposed which is a natural extension of corner block list (CBL) [1] to represent a common centroid placement of a set of device pairs. C-CBL is complete and non-redundant in representing any common centroid mosaic packings with pairs of blocks to be matched. To address the same problem with an additional constraint that devices are required to be placed uniformly to average out the parasitic errors, a grid-based approach is proposed. Experimental results show that both approaches are fast and promising, and have high scalability that even large data sets can be handled effectively.","Analog circuits,
Scalability,
Computer science,
Circuit optimization,
Degradation,
Threshold voltage,
Capacitors,
Resistors,
Data structures,
Simulated annealing"
Distributed Consensus Control for Second-Order Agents with Fixed Topology and Time-Delay,"In this paper, distributed consensus control is investigated for networks of agents with double integrator dynamics. Two kinds of networks are analyzed, i.e., directed networks with fixed topology and undirected networks with fixed topology and time-delay. For each of the networks, a sufficient and necessary condition is given to guarantee the consensus. It is proved that the largest tolerable time-delay is only related to the largest eigenvalue of the graph Laplacian. Finally, two numerical examples are provided to illustrate the obtained results.","Distributed control,
Vehicle dynamics,
Telecommunication network topology,
Network topology,
Telecommunication control,
Laplace equations,
Graph theory,
Electronic mail,
Tree graphs,
Laboratories"
RobuSTore: a distributed storage architecture with robust and high performance,"Emerging large-scale scientific applications require to access large data objects in high and robust performance. We propose RobuSTore, a storage architecture that combines erasure codes and speculative access mechanisms for parallel write and read in distributed environments. The mechanisms can effectively aggregate the bandwidth from a large number of distributed disks and statistically tolerate pear-disk performance variation. Our simulation results affirm the high and robust performance of RobuSTore in both write and read operations compared to traditional parallel storage systems. For example, for a 1GB data access using 64 disks, RobuSTore achieves average bandwidth of 186MBps for write and 400MBps for read, nearly 6x and 15x that achieved by a RAID-0 system. The standard deviation of access latency is only 0.5 second, about 9% of the write latency and 20% of the read latency, and a 5-fold improvement from RAID-0. The improvements are achieved at moderate cost: about 40% increase in I/O operations and 2x-3x increase in storage capacity utilization.","Robustness,
Delay,
Computer architecture,
Computer science,
Data engineering,
Large-scale systems,
Aggregates,
Bandwidth,
Application software,
Costs"
UML Profiles for Design Decisions and Non-Functional Requirements,"A software architecture is composed of a collection of design decisions. Each design decision helps or hinders certain Non-Functional Requirements (NFR). Current software architecture views focus on expressing components and connectors in the system. Design decisions and their relationships with non-functional requirements are often captured in separate design documentation, not explicitly expressed in any views. This disassociation makes architecture comprehension and architecture evolution harder. In this paper, we propose a UML profile for modeling design decisions and an associated UML profile for modeling non-functional requirements in a generic way. The two UML profiles treat design decisions and nonfunctional requirements as first-class elements. Modeled design decisions always refer to existing architectural elements and thus maintain traceability between the two. We provide a mechanism for checking consistency over this traceability. An exemplar is given as a way to demonstrate the feasibility of our approach.","Unified modeling language,
Computer architecture,
Documentation,
Software architecture,
Connectors,
Software engineering,
Australia,
Computer science,
Design engineering,
Laboratories"
A ZigBee-Based Power Monitoring System with Direct Load Control Capabilities,"ZigBee is a wireless networking standard that has characteristics of low power consumption, having up to 65535 device nodes, and low data rates. It is suitable to be applied in many home and industrial applications that could operate in harsh radio environments and in isolated locations. In this paper, a novel ZigBee-based powering monitoring system (PMS) is proposed. In particular, the proposed PMS utilizes ZigBee for wireless communication, DSP (Digital Signal Processing) for realtime power parameters computation, and Web Services for constructing the communication infrastructure among distributed systems across networks. A paradigm system for monitoring campus power consumptions is constructed and experimented to validate the effectiveness of the proposed PMS. The testing results show that the functions of the proposed PMS comply with the designed objectives. The paradigm PMS also demonstrates good performances in direct load control and warning message transmission. This work possesses novelty for first time integrating ZigBee, DSP, and Web Services technologies to develop PMS, and can be a useful reference for future developments of PMS.","Monitoring,
Load flow control,
ZigBee,
Digital signal processing,
Energy consumption,
Web services,
Electrical equipment industry,
Wireless communication,
Computer networks,
Distributed computing"
An Analysis of 10-Gigabit Ethernet Protocol Stacks in Multicore Environments,"This paper analyzes the interactions between the protocol stack (TCP/IP or iWARP over 10-Gigabit Ethernet) and its multicore environment. Specifically, for host-based protocols such as TCP/IP, we notice that a significant amount of processing is statically assigned to a single core, resulting in an imbalance of load on the different cores of the system and adversely impacting the performance of many applications. For host-offloaded protocols such as iWARP, on the other hand, the portions of the communication stack that are performed on the host, such as buffering of messages and memory copies, are closely tied with the associated process, and hence do not create such load imbalances. Thus, in this paper, we demonstrate that by intelligently mapping different processes of an application to specific cores, the imbalance created by the TCP/IP protocol stack can be largely countered and application performance significantly improved. At the same time, since the load is a better balanced in host-offloaded protocols such as iWARP, such mapping does not adversely affect their performance, thus keeping the mapping generic enough to be used with multiple protocol stacks.","Ethernet networks,
Protocols,
Multicore processing,
TCPIP,
Computer science,
Network interfaces,
Computer buffers,
Libraries"
Performance Improvement of Two-Dimensional Packet Classification by Filter Rephrasing,"Packet classification categorizes incoming packets into multiple forwarding classes in a router based on predefined filters. It is important in fulfilling the requirements of differentiated services. To achieve fast packet classification, a new approach, namely ldquofilter rephrasing,rdquo is proposed to encode the original filters by exploiting the hierarchical property of the filters. Filter rephrasing could dramatically reduce the search and storage complexity incurred in packet classification. We incorporate a well-known scheme-rectangle search-with filter rephrasing to improve the lookup speed by at least a factor of 2 and decreases 70% of the storage expenses. As compared with other existing schemes, the proposed scheme exhibits a better balance between speed, storage, and computation complexity. Consequently, the scalable effect of filter rephrasing is suitable for backbone routers with a great number of filters.",
Reliability Assessment of Optical p-Cycles,"Two recovery techniques suited for the next generation Internet are studied: traditional protection rings (BLSRs) and a novel, preconfigured protection cycles (p-cycles) technique. Theoretical formulas describing the reliability function as well as mean time to failure are derived. On the basis of our analysis, we show that p-cycles should not be used in wide-area networks since their reliability performance is outside the desired bounds.",
Process Discovery from Model and Text Artefacts,"Modeling is an important and time consuming part of the business process management life-cycle. An analyst reviews existing documentation and queries relevant domain experts to construct both mental and concrete models of the domain. To aid this exercise, we propose the Rapid Business Process Discovery (R-BPD) framework and prototype tool that can query heterogeneous information resources (e.g. corporate documentation, web-content, code e.t.c.) and rapidly constructproto-models to be incrementally adjusted to correctness by an analyst. This constitutes a departure from building and constructing models toward just editing them. We believe this rapid mixed-initiative modeling will increase analyst productivity by significant orders of magnitude over traditional approaches. Furthermore, the possibility of using the approach in distributed and real-time settings seems appealing and may help in significantly improving the quality of the models being developed w.r.t. being consistent, complete, and concise.",
Coverage-based Clustering of Wireless Sensor and Actor Networks,"In addition to the miniaturized sensor nodes, Wireless Sensor and Actor Networks (WSANs) employ significantly more capable actor nodes that can perform application specific actions to deal with events detected and reported by the sensors. Since these actions can be taken at any spot within the monitored region, the actors should strive to provide maximal coverage of the area. In addition, minimizing the time to decide which actor should take what action is of utmost important for responsiveness. In this paper, we propose a distributed actor positioning and clustering algorithm which employs actors as cluster-heads and places them in such a way that the coverage of actors is maximized and the data gathering and acting process times are minimized. Such placement of actors is done by determining the -hop Independent Dominating Set (IDS) of the underlying sensor network. The performance of the approach is validated through simulations.","Wireless sensor networks,
Monitoring,
Delay,
Sea measurements,
Fires,
Intrusion detection,
Radiation detectors,
Pollution measurement,
Robot sensing systems,
Computer science"
Target Coverage With QoS Requirements in Wireless Sensor Networks,"It has been a challenging problem to support target coverage with QoS requirements in wireless sensor networks, because we need to consider both energy constraint and so-called target Q-coverage requirement, i.e. different targets may require different sensing quality in terms of the number of transducers, data sampling rate, etc. First we prove that this problem is NP- complete, then we formulate the optimization using Linear Programming techniques, however, directly solving this optimization seems desperately complex due to the combinatorial complexity. Thus a column generation based approach is developed to solve this problem by decomposing the original problem into a master problem as well as a corresponding sub problem and solves them iteratively. Numerical results are given to verify our approach and gain some insights into the performance aspects in designing such surveillance systems.","Wireless sensor networks,
Monitoring,
Iterative algorithms,
Computer science,
Sampling methods,
Linear programming,
Surveillance,
Sensor systems and applications,
Sensor phenomena and characterization,
Intelligent networks"
Non-isolated Half Bridge Buck Based Converter for VRM Application,"High efficiency and fast transient response are very critical to voltage regulator module (VRM) and they are contradicted to each other. Continuous output voltage drop with increased current and power densities force small duty cycle operation, resulting in lower efficiency and worse transient response. This paper presents a new, transformer based, nonisolated, buck derived, half bridge converter for VRM application. The transformer in the presented topology helps in decreasing the top switch current as well as the bottom switch voltage stresses, it also extends the duty cycle to a favorable range enhancing the efficiency and transient response by optimizing MOSFET selections. The proposed converter is designed, analyzed and the experimental results are presented.","Switches,
Transient response,
Stress,
Regulators,
Inductors,
Bridge circuits,
Topology,
Low voltage,
Inductance,
MOSFET circuits"
Accurate and Scalable Surface Representation and Reconstruction from Images,"We introduce a new surface representation method, called patchwork, to extend three-dimensional surface reconstruction capabilities from multiple images. A patchwork is the combination of several patches that are built one by one. This design potentially allows for the reconstruction of an object with arbitrarily large dimensions while preserving a fine level of detail. We formally demonstrate that this strategy leads to a spatial complexity independent of the dimensions of the reconstructed object and to a time complexity that is linear with respect to the object area. The former property ensures that we never run out of storage and the latter means that reconstructing an object can be done in a reasonable amount of time. In addition, we show that the patchwork representation handles equivalently open and closed surfaces, whereas most of the existing approaches are limited to a specific scenario, an open or closed surface, but not both. The patchwork concept is orthogonal to the method chosen for surface optimization. Most of the existing optimization techniques can be cast into this framework. To illustrate the possibilities offered by this approach, we propose two applications that demonstrate how our method dramatically extends a recent accurate graph technique based on minimal cuts. We first revisit the popular carving techniques. This results in a well-posed reconstruction problem that still enjoys the tractability of voxel space. We also show how we can advantageously combine several image-driven criteria to achieve a finely detailed geometry by surface propagation. These two examples demonstrate the versatility and flexibility of patchwork reconstruction. They underscore other properties inherited from patchwork representation: Although some min-cut methods have difficulty in handling complex shapes (e.g., with complex topologies), they can naturally manipulate any geometry through the patchwork representation while preserving their intrinsic qualities. The above properties of patchwork representation and reconstruction are demonstrated with real image sequences","Surface reconstruction,
Image reconstruction,
Layout,
Geometry,
Cameras,
Level set,
Optimization methods,
Shape,
Topology,
Image sequences"
Reconstruction for Gated Dynamic Cardiac PET Imaging Using a Tensor Product Spline Basis,A maximum likelihood reconstruction algorithm for gated dynamic cardiac PET studies was developed and evaluated. A two dimensional tensor product spline basis spanning the time and gate domain is proposed. The activity variations introduced by the biochemical kinetics and cardiac motion are modeled as conic combinations of the B-spline basis functions. The basis explicitly takes the cyclic nature of the heart motion into account. We make use of the expectation-maximization (EM) algorithm to derive a closed form iteration scheme for the reconstruction. The proposed algorithm is validated through computer simulations of the dynamic NCAT beating heart phantom and the kinetics of 13N-ammonia uptake. We used the Monte Carlo simulator GATE to simulate a typical PET scanner. We qualitatively found that a reconstruction using cubic splines resulted in smoother images while better delineating the myocardial wall. Fourth order spline modeling reduced the mean squared error (MSE) of binned reconstruction by 56% whereas conventional Gaussian filtering reduced the MSE by 38%. Spline modeling in the gate domain reduced the MSE by 72% compared to a reduction of 47% obtained with filtering. Quantitative evaluation of the reconstructed motion information suggested that the number of basis functions in the gate domain could be reduced from 16 in a framed approach to 5 for reconstructions using a higher order spline interpolation,"Image reconstruction,
Positron emission tomography,
Tensile stress,
Spline,
Kinetic theory,
Heart,
Computational modeling,
Filtering,
Reconstruction algorithms,
Computer simulation"
Agent-Based Modeling of Ambidextrous Organizations: Virtualizing Competitive Strategy,"Turbulence, uncertainty, dynamic processes, and networks increasingly characterize competitive markets and business strategies. Consequently, there's a need to model such markets and strategies as dynamic, evolutionary processes that is, as complex adaptive systems. Agent-based modeling, a rich platform for studying complex evolving systems, is used to model a market where ambidextrous and nonambidextrous organizations compete for buyers. Viewing competitive market and business processes as interactions among agents who mutually influence each other reduces economics to its most microscopic level. Social networks such as the Internet have attracted much research attention because of the rise in stock fraud on the Internet.","Technological innovation,
Microscopy,
Environmental economics,
Marketing and sales,
Uncertainty,
Adaptive systems,
Data analysis,
Analytical models,
Game theory,
Time series analysis"
Sentence Similarity based on Dynamic Time Warping,"This paper investigates the problem of the similarity measure between very short text of sentence length, which can be used in a variety of applications that involve text knowledge representation and discovery. A novel method to calculate sentence similarity is proposed, which takes into account the semantic information, word order and the contribution of different parts of speech in a sentence. The experiment result on the set of selected sentence pairs shows that our method greatly outperforms other previously reported methods.","Time measurement,
Humans,
Knowledge representation,
Speech analysis,
Computer science,
Knowledge engineering,
Length measurement,
Application software,
Area measurement,
Text mining"
Probabilistic Collision Prediction for Vision-Based Automated Road Safety Analysis,"This work aims at addressing the many problems that have hindered the development of vision-based systems for automated road safety analysis. The approach relies on traffic conflicts used as surrogates for collision data. Traffic conflicts are identified by computing the collision probability for any two road users in an interaction. A complete system is implemented to process traffic video data, detect and track road users, and analyze their interactions. Motion patterns are needed to predict road users' movements and determine their probability of being involved in a collision. An original incremental algorithm for the learning of prototype trajectories as motion patterns is presented. The system is tested on real world traffic data, including a few traffic conflict instances. Traffic patterns are successfully learnt on two datasets, and used for collision probability computation and traffic conflict detection.","Road accidents,
Road safety,
Telecommunication traffic,
Intelligent sensors,
Intelligent transportation systems,
Computer vision,
Road transportation,
USA Councils,
Prototypes,
System testing"
An Improved Threshold Selection Algorithm Based on Particle Swarm Optimization for Image Segmentation,"This paper proposes an effective threshold selection method of image segmentation based on particle swarm optimization (PSO), which is embedded into two-dimensional Otsu algorithm. Traditional image segmentation methods are time-consuming computation and become an obstacle in real time application systems. In this paper, the threshold selection approach based on PSO is proposed to deal with threshold selection of image segmentation. The threshold is obtained through PSO. PSO is realized successfully in the process of solving the threshold selection problem. The experiments of segmenting images are illustrated to show that the proposed method can get ideal segmentation result with less computation cost.",
Supporting the Investigation and Planning of Pragmatic Reuse Tasks,"Software reuse has long been promoted as a means to increase developer productivity; however, reusing source code is difficult in practice and tends to be performed in an ad hoc manner. This is problematic because poor decisions can be made either to attempt an unwise, overly complex reuse task, or to avoid a reuse task that would have saved time and effort. This paper describes a lightweight tool that supports the investigation and planning of pragmatic reuse tasks. The tool helps developers to identify the dependencies from the source code they wish to reuse, and to decide how to deal with those dependencies. Questions about pragmatic reuse are evaluated through a survey of industrial developers. The tool is evaluated through the planning and execution of reuse tasks by industrial developers.","Software performance,
Navigation,
Data visualization,
Global Positioning System,
Displays,
Laboratories,
Computer science,
Productivity,
Computer industry,
Performance evaluation"
Automatic Recognition of Partial Shoeprints Based on Phase-Only Correlation,"In this paper, a method for automatically recognizing partial shoeprint images for use in forensic science is presented. The technique uses the phase-only correlation (POC) for shoeprints matching. The main advantage of this method is its capability to match low quality shoeprint images accurately and efficiently. In order to achieve superior performance, the use of a spectral weighting function is also proposed. Experiments were conducted on a database of images of 100 different shoes available on the market. For experimental evaluation, test images including different perturbations such as noise addition, blurring and textured background addition were generated. Results have shown that the proposed method is very practical and provides high performance when processing low quality partial-prints. The use of a weighting function provides an improvement in the recognition rate in particularly difficult cases.","Footwear,
Forensics,
Image recognition,
Fourier transforms,
Image databases,
Testing,
Background noise,
Layout,
Shape,
Consumer electronics"
Normalized Cuts Revisited: A Reformulation for Segmentation with Linear Grouping Constraints,"Indisputably Normalized Cuts is one of the most popular segmentation algorithms in computer vision. It has been applied to a wide range of segmentation tasks with great success. A number of extensions to this approach have also been proposed, ones that can deal with multiple classes or that can incorporate a priori information in the form of grouping constraints. However, what is common for all these suggested methods is that they are noticeably limited and can only address segmentation problems on a very specific form. In this paper, we present a reformulation of Normalized Cut segmentation that in a unified way can handle all types of linear equality constraints for an arbitrary number of classes. This is done by restating the problem and showing how linear constraints can be enforced exactly through duality. This allows us to add group priors, for example, that certain pixels should belong to a given class. In addition, it provides a principled way to perform multi-class segmentation for tasks like interactive segmentation. The method has been tested on real data with convincing results.",
Taverna Workflows: Syntax and Semantics,"This paper presents the formal syntax and the operational semantics of Taverna, a workflow management system with a large user base among the e-Science community. Such formal foundation, which has so far been lacking, opens the way to the translation between Taverna workflows and other process models. In particular, the ability to automatically compile a simple domain-specific process description into Taverna facilitates its adoption by e-scientists who are not expert workflow developers. We demonstrate this potential through a practical use case.",
Clock-Tree Aware Placement Based on Dynamic Clock-Tree Building,"Minimization of clock network is traditionally achieved by clock routing, which may be helpless for a poor placement result. In this paper, a novel dynamic clock-tree building technique integrated into placement for zero-skew design is proposed. This method combines a pre-designed clock-tree with the force-directed placement procedure to navigate the register placement for minimizing the clock network. Meanwhile, a new model of multi-level bounding box and technique of multi-level attractive force are proposed to give a better local distribution of registers. Experiments on several standard-cell benchmarks indicate an average 26.1% clock network reduction with the logic cell placement preserved well.","Clocks,
Routing,
Buildings,
Navigation,
Logic,
Very large scale integration,
Power dissipation,
Computer science,
Frequency,
Geometry"
Study of millimeter-wave radar for helicopter assisted landing system,"This paper discusses the development of an algorithm used to simulate the effectiveness of millimeter-wave radar in imaging a rough terrain, for the purpose of helicopter assisted landing. Using an externally generated terrain and the physical optics approximation, the algorithm computes the backscatter response of the terrain when illuminated by a real aperture antenna. Results are presented from simulating terrains with different macroscopic features, such as a hump, ditch or a slope. It shown that operating at millimeter-wave, more specifically at W-Band frequencies, is ideal for such an application where a compact sensor is required to achieve high resolution imaging.","Millimeter wave radar,
Helicopters,
Computational modeling,
Optical imaging,
Physical optics,
Approximation algorithms,
Optical computing,
Physics computing,
Backscatter,
Apertures"
High-Dimensional Feature Matching: Employing the Concept of Meaningful Nearest Neighbors,"Matching of high-dimensional features using nearest neighbors search is an important part of image matching methods which are based on local invariant features. In this work we highlight effects pertinent to high-dimensional spaces that are significant for matching, yet have not been explicitly accounted for in previous work. In our approach, we require every nearest neighbor to be meaningful, that is, sufficiently close to a query feature such that it is an outlier to a background feature distribution. We estimate the background feature distribution from the extended neighborhood of a query feature given by its k nearest neighbors. Based on the concept of meaningful nearest neighbors, we develop a novel high-dimensional feature matching method and evaluate its performance by conducting image matching on two challenging image data sets. A superior performance in terms of accuracy is shown in comparison to several state-of-the-art approaches. Additionally, to make search for k nearest neighbors more efficient, we develop a novel approximate nearest neighbors search method based on sparse coding with an overcomplete basis set that provides a ten-fold speed-up over an exhaustive search even for high dimensional spaces and retains excellent approximation to an exact nearest neighbors search.","Nearest neighbor searches,
Image matching,
Computer vision,
Voting,
Vocabulary,
Image retrieval,
Vector quantization,
Information science,
Image coding,
Geometry"
Hardened by Design Techniques for Implementing Multiple-Bit Upset Tolerant Static Memories,"We present a novel MBU-tolerant design, which utilizes layout-based interleaving and multiple-node disruption tolerant memory latches. This approach protects against grazing incidence particle strikes, which produce disruptions with the widest possible spatial separation. Advantages with respect to size, complexity, and MBU tolerance are realized when this approach is compared to existing solutions.","Latches,
Interleaved codes,
Protection,
Redundancy,
Single event upset,
Integrated circuit layout,
Particle scattering,
Logic,
Error correction codes,
Robustness"
An effective genetic algorithm to improve wireless sensor network lifetime for large-scale surveillance applications,"Wireless sensor network lifetime for large-scale surveillance systems is defined as the time span that all targets can be covered. One approach to extend the lifetime is to divide the deployed sensors into disjoint subsets of sensors, or sensor covers, such that each sensor cover can cover all targets and work by turns. The more sensor covers can be found, the longer sensor network lifetime can be prolonged. Finding the maximum number of sensor covers can be solved via transformation to the Disjoint Set Covers (DSC) problem, which has been proved to be NP-complete. For this optimization problem, existing heuristic algorithms either get unsatisfactory solutions in some cases or take exponential time complexity. This paper proposes a genetic algorithm to solve the DSC problem. The simulation results show that the proposed algorithm can get near-optimal solutions with polynomial computation time and can improve the performance of the most constrained-minimum constraining heuristic algorithm by 16% in solution quality.",
Accompanying persons with a mobile robot using motion prediction and probabilistic roadmaps,"To ensure the safety of people, it is important that mobile robots operating in populated environments are able to take the motions of humans in their vicinity into account. An especially demanding task in this respect is accompanying a person walking through an unknown and busy environment, because it requires the robot to stay close to his client and simultaneously prevent bumping into any passers-by. This paper presents a local navigation planning approach for collision avoidance, which aims at achieving this goal. The presented technique uses probabilistic roadmaps to plan collision-free paths to a given target location relative to the robot. A laser-based people tracking component is used to estimate the motions of humans in the robot's surrounding, and a potential field method is applied for predicting the humans' future trajectories based on this information. In addition to preventing collisions, the predictions enable us to choose appropriate target locations relative to the person being attended. We tested our method on real robots and in simulations. The experiments carried out in an office environment confirm that the integrated motion prediction actually improves the performance of the collision avoidance and the robot's ability to stay close to the client it accompanies.","Mobile robots,
Humans,
Legged locomotion,
Collision avoidance,
Safety,
Navigation,
Motion planning,
Target tracking,
Motion estimation,
Trajectory"
Exploiting Symbolic Techniques in Automated Synthesis of Distributed Programs with Large State Space,"Automated formal analysis methods such as program verification and synthesis algorithms often suffer from time complexity of their decision procedures and also high space complexity known as the state explosion problem. Symbolic techniques, in which elements of a problem are represented by Boolean formulae, are desirable in the sense that they often remedy the state explosion problem and time complexity of decision procedures. Although symbolic techniques have successfully been used in program verification, their benefits have not yet been exploited in the context of program synthesis and transformation extensively. In this paper, we present a symbolic method for automatic synthesis of fault-tolerant distributed programs. Our experimental results on synthesis of classical fault-tolerant distributed problems such as Byzantine agreement and token ring show a significant performance improvement by several orders of magnitude in both time and space complexity. To the best of our knowledge, this is the first illustration where programs with large state space (beyond 2100) is handled during synthesis.","State-space methods,
Fault tolerance,
Safety,
Explosions,
Automatic control,
Context modeling,
Automata,
Computer science,
Electronic mail,
Algorithm design and analysis"
Semantic-Enhanced Personalized Recommender System,"Personalized recommender systems have emerged as a powerful method for improving both the content of customers and the profit of providers in e-business environment. Nowadays, many kinds of recommender methods have been proposed to provide personalized services. However, all these techniques have not made full use of the semantic information of objects, which leading them to an unsatisfying performance. Collaborative filter (CF) system, as the most popular personalized recommender systems, has such well-known limitations as sparsity, scalability and cold-start problem. A semantic-enhanced collaborative recommender system is proposed in this paper. The semantic information of objects is extracted to support the recommendation process. This study compares the performance of the proposed technique with the traditional CF approaches. Experimental results demonstrate the effectiveness of the proposed method.","Recommender systems,
Ontologies,
Collaboration,
Scalability,
Information filtering,
Information filters,
Machine learning,
Cybernetics,
Software libraries,
Clustering algorithms"
Multiobjective Genetic Algorithm for Extracting Subgroup Discovery Fuzzy Rules,"This paper presents a multiobjective genetic algorithm for obtaining fuzzy rules for subgroup discovery. This kind of fuzzy rules lets us represent knowledge about patterns of interest in an explanatory and understandable form which can be used by the expert. The multiobjective algorithm proposed in this paper defines three objectives. One of them is used as a restriction on the rules in order to obtain a Pareto front composed of a set of quite different rules with a high degree of coverage over the examples. The other two objectives take into account the support and the confidence of the rules. The use of the mentioned objective as restriction allows us the extraction of a set of rules which describe more complete information on most of the examples. Experimental evaluation of the algorithm, applying it to a market problem shows the validity of the proposal obtaining novel and valuable knowledge for the experts",
QoS-Aware Service Composition in Dino,"A major advantage offered by Web services technologies is the ability to dynamically discover and invoke services. This ability is particularly important for operations of many applications executing in open dynamic environments. The QoS properties of the required and provided services play a significant role in dynamic discovery and invocation of services in open dynamic environments. In this paper, we discuss our approach to QoS specification and service provider selection, in the context of our work on the Dino project. The service provider selection algorithm used in Dino takes into account the relative benefit offered by a provider with respect to the requester-specified QoS criteria, and the trustworthiness of the provider. We explain our approach using an example from the automotive domain.","Quality of service,
Vehicle dynamics,
Runtime,
Monitoring,
Web services,
Context-aware services,
Specification languages,
Software systems,
Computer science,
Educational institutions"
Exact Bayesian network learning in estimation of distribution algorithms,"This paper introduces exact learning of Bayesian networks in estimation of distribution algorithms. The estimation of Bayesian network algorithm (EBNA) is used to analyze the impact of learning the optimal (exact) structure in the search. By applying recently introduced methods that allow learning optimal Bayesian networks, we investigate two important issues in EDAs. First, we analyze the question of whether learning more accurate (exact) models of the dependencies implies a better performance of EDAs. Second, we are able to study the way in which the problem structure is translated into the probabilistic model when exact learning is accomplished.","Bayesian methods,
Electronic design automation and methodology,
Learning systems,
Algorithm design and analysis,
Data mining,
Artificial intelligence,
Random variables,
Performance analysis,
Machine learning algorithms,
Evolutionary computation"
FPGA-accelerated seed generation in Mercury BLASTP,"BLASTP is the most popular tool for comparative analysis of protein sequences. In recent years, an exponential increase in the size of protein sequence databases has required either exponentially more runtime or a cluster of machines to keep pace. To address this problem, we have designed and built a high-performance FPGA-accelerated version of BLASTP, Mercury BLASTP. In this paper, we focus on seed generation, the first stage of the BLASTP algorithm. Our seed generator is capable of processing database residues at up to 219 Mresidues/second for 2048- residue queries. The full Mercury BLASTP pipeline, including our seed generator, achieves a speedup of 37times over the popular NCBI BLASTP software on a 2.8 GHz Intel P4 CPU, with sensitivity more than 99% that of the software. Our architecture can be generalized to accelerate the seed generation stage in other important biocomputing applications.","Sequences,
Databases,
Proteins,
Acceleration,
Pipelines,
Biology computing,
Computer architecture,
Bioinformatics,
Jacobian matrices,
Computer science"
Streaming Simplification of Tetrahedral Meshes,"Unstructured tetrahedral meshes are commonly used in scientific computing to represent scalar, vector, and tensor fields in three dimensions. Visualization of these meshes can be difficult to perform interactively due to their size and complexity. By reducing the size of the data, we can accomplish real-time visualization necessary for scientific analysis. We propose a two-step approach for streaming simplification of large tetrahedral meshes. Our algorithm arranges the data on disk in a streaming, I/O-efficient format that allows coherent access to the tetrahedral cells. A quadric-based simplification is sequentially performed on small portions of the mesh in-core. Our output is a coherent streaming mesh which facilitates future processing. Our technique is fast, produces high quality approximations, and operates out-of-core to process meshes too large for main memory","Data visualization,
Geometry,
Data structures,
Scientific computing,
Tensile stress,
Solid modeling,
Current measurement,
Fluid dynamics,
Partial differential equations,
Hardware"
Interactive Genetic Algorithms for User Interface Design,"We attack the problem of user fatigue in using an interactive genetic algorithm to evolve user interfaces in the XUL interface definition language. The interactive genetic algorithm combines computable user interface design metrics with subjective user input to guide evolution. Individuals in our population represent interface specifications and we compute an individual's fitness from a weighted combination of user input and user interface design guidelines. Results from our preliminary study involving three users indicate that users are able to effectively bias evolution towards user interface designs that reflect both user preferences and computed guideline metrics. Furthermore, we can reduce fatigue, defined by the number of choices needing to be made by the human designer, by doing two things. First, asking the user to pick just two (the best and worst) user interfaces from among a subset of nine shown. Second, asking the user to make the choice once every t generations, instead of every single generation. Our goal is to provide interface designers with an interactive tool that can be used to explore innovation and creativity in the design space of user interfaces.","Genetic algorithms,
User interfaces,
Algorithm design and analysis,
Guidelines,
Fatigue,
Computer interfaces,
Space exploration,
Humans,
Technological innovation,
Physics computing"
High-Throughput Ligand Screening via Preclustering and Evolved Neural Networks,"The pathway for novel lead drug discovery has many major deficiencies, the most significant of which is the immense size of small molecule diversity space. Methods that increase the search efficiency and/or reduce the size of the search space increase the rate at which useful lead compounds are identified. Artificial neural networks optimized via evolutionary computation provide a cost and time-effective solution to this problem. Here, we present results that suggest that preclustering of small molecules prior to neural network optimization is useful for generating models of quantitative structure-activity relationships for a set of HIV inhibitors. Using these methods, it is possible to prescreen compounds to separate active from inactive compounds or even active and mildly active compounds from inactive compounds with high predictive accuracy while simultaneously reducing the feature space. It is also possible to identify ""human interpretable"" features from the best models that can be used for proposal and synthesis of new compounds in order to optimize potency and specificity.","Neural networks,
Artificial neural networks,
Drugs,
Lead compounds,
Evolutionary computation,
Cost function,
Computational efficiency,
Human immunodeficiency virus,
Inhibitors,
Accuracy"
Good Practices for Educational Software Engineering Projects,"Recent publications indicate the importance of software engineering in the computer science curriculum. In this paper, we present the final part of software engineering education at University of Groningen in the Netherlands and Vaxjo University in Sweden, where student teams perform an industrial software development project. It furthermore presents the main educational problems encountered in such real-life projects and explains how this international course addresses these problems. The main contribution of this paper is a set of seven good practices for project based software engineering education.","Software engineering,
Computer industry,
Programming,
Computer science,
Software quality,
Computer science education,
Software maintenance,
Project management,
Uncertainty,
Sequential analysis"
SWORD: A SAT like prover using word level information,"Solvers for Boolean Satisfiabilily (SAT) are state-of-the-art to solve verification problems. But when arithmetic operations are considered, the verification performance degrades with increasing data-path width. Therefore, several approaches that handle a higher level of abstraction have been studied in the past. But the resulting solvers are still not robust enough to handle problems that mix word level structures with bit level descriptions. In this paper, we present the satisfiability solver SWORD — a SAT like solver that facilitates word level information. SWORD represents the problem in terms of modules that define operations over bit vectors. Thus, word level information and structural knowledge become available in the search process. The experimental results show that on our benchmarks SWORD is more robust than Boolean SAT, K⋆BMDs or SMT.","Arithmetic,
Surface-mount technology,
Degradation,
Data structures,
Boolean functions,
Robustness,
Computer science,
Digital circuits,
Formal verification,
Integer linear programming"
A Fuzzy Description Logic with Product T-norm,"Fuzzy description logics (fuzzy DLs) have been proposed as a language to describe structured knowledge with vague concepts. It is well known that the choice of the fuzzy operators may determine some logical properties. However, up to date the study of fuzzy DLs has been restricted to the Lukasiewicz logic and the ""Zadeh semantics"". In this work, we propose a novel semantics combining the common product t-norm with the standard negation. We show some interesting properties of the logic and propose a reasoning algorithm based on a mixture of tableaux rules and the reduction to mixed integer quadratically constrained programming.","Fuzzy logic,
Fuzzy set theory,
Concrete,
Fuzzy sets,
Logic programming,
Fuzzy reasoning,
Quadratic programming,
Integer linear programming,
Inference algorithms,
Knowledge representation"
State space analysis of disturbance observer and a robust stability condition,"In this paper, we analyze the classical linear disturbance observer (DOB) approach in the state space. Our tool for the analysis is the singular perturbation theory. With the tool, an almost necessary and sufficient condition is proposed for robust stability of the closed-loop system with the DOB when the Q-filter has sufficiently large bandwidth. The proposed analysis enlightens the power and the limitations of the classical DOB approach, which have not been well discussed in the literature.","State-space methods,
Robust stability,
Bandwidth,
Robust control,
Uncertainty,
Optical control,
Control systems,
Steady-state,
Feedback,
Frequency domain analysis"
The Art of Deception: Adaptive Precision Reduction for Area Efficient Physics Acceleration,"Physics-based animation has enormous potential to improve the realism of interactive entertainment through dynamic, immersive content creation. Despite the massively parallel nature of physics simulation, fully exploiting this parallelism to reach interactive frame rates will require significant area to place the large number of cores. Fortunately, interactive entertainment requires believability rather than accuracy. Recent work shows that real-time physics has a remarkable tolerance for reduced precision of the significant in floating-point (FP) operations. In this paper, we describe an architecture with a hierarchical floating-point unit (FPU) that leverages dynamic precision reduction to enable efficient FPU sharing among multiple cores. This sharing reduces the area required by these cores, thereby allowing more cores to be packed into a given area and exploiting more parallelism.","Art,
Acceleration,
Parallel processing,
Animation,
Computational modeling,
Physics computing,
Microarchitecture,
Accelerated aging,
Computer science,
Silicon"
Intrusion-Resilient Secret Sharing,"We introduce a new primitive called intrusion-resilient secret sharing (IRSS), whose security proof exploits the fact that there exist functions which can be efficiently computed interactively using low communication complexity in k, but not in k-1 rounds. IRSS is a means of sharing a secret message amongst a set of players which comes with a very strong security guarantee. The shares in an IRSS are made artificially large so that it is hard to retrieve them completely, and the reconstruction procedure is interactive requiring the players to exchange k short messages. The adversaries considered can attack the scheme in rounds, where in each round the adversary chooses some player to corrupt and some function, and retrieves the output of that function applied to the share of the corrupted player. This model captures for example computers connected to a network which can occasionally he infected by malicious software like viruses, which can compute any function on the infected machine, but cannot sent out a huge amount of data. Using methods from the bounded-retrieval model, we construct an IRSS scheme which is secure against any computationally unbounded adversary as long as the total amount of information retrieved by the adversary is somewhat less than the length of the shares, and the adversary makes at most k-1 corruption rounds (as described above, where k rounds are necessary for reconstruction). We extend our basic scheme in several ways in order to allow the shares sent by the dealer to be short (the players then blow them up locally) and to handle even stronger adversaries who can learn some of the shares completely. As mentioned, there is an obvious connection between IRSS schemes and the fact that there exist functions with an exponential gap in their communication complexity for k and k-1 rounds. Our scheme implies such a separation which is in several aspects stronger than the previously known ones.","Cryptography,
Complexity theory,
Cryptographic protocols,
Computer science,
Computer networks,
Computer security,
Computer viruses,
Information retrieval,
Art,
Information security"
Probabilistic Estimation of Whole Body Contacts for Multi-Contact Robot Control,"Today most robots interact with the surroundings only with their end-effectors. However there are many benefits to utilizing contact along the entire length of robot body and links especially for human-like robots. Existing control strategies for link contact require knowledge of the contact point. In an uncertain environment, locating link contact point is difficult for most robots as they do not possess skin capable of sensing. We propose a probabilistic approach to link contact estimation based on geometric considerations and compliant motions. Since for many robots, link geometry is also uncertain, we broaden our approach to simultaneously estimate link shape and environment contact. Our experimental results demonstrate that efficiency of control is significantly improved by link contact estimation","Robot control,
Robot sensing systems,
Humans,
Manipulators,
Skin,
Humanoid robots,
Force control,
Orbital robotics,
Parameter estimation,
Robotics and automation"
Thin Client Visualization,"We have developed a Web 2.0 thin client visualization framework called GeoBoosttrade. Our framework focuses on geospatial visualization and using scalable vector graphics (SVG), AJAX, RSS and GeoRSS we have built a complete thin client component set. Our component set provides a rich user experience that is completely browser based. It includes maps, standard business charts, graphs, and time-oriented components. The components are live, interactive, linked, and support real time collaboration.","Visualization,
Collaboration,
Graphics,
Displays,
Java,
Application software,
Visual analytics,
Shape,
Streaming media,
Portals"
ASSL - Autonomic System Specification Language,"This article is an overview of the Autonomic System Specification Language (ASSL). ASSL is a framework for formally specifying and generating autonomic systems. The latter are specified as formal executable models with an interaction protocol and autonomic elements. We explain in detail the architecture of the ASSL framework and demonstrate how to specify autonomic systems. In this paper, we do not talk about syntax and semantic aspects of ASSL, since these are going to be tackled by our ongoing research and described in other papers.","Specification languages,
Runtime,
Formal specifications,
Software engineering,
Computer vision,
Programming,
Organisms,
Computer science,
Environmental management,
Software development management"
Roll-over Detection and Sleep Quality Measurement using a Wearable Sensor,"Mental health management in the workplace has attracted increasing attention in recent years. As sleep is an essential physiological function that is strongly related to mental condition, easy and daily sleep management is considered essential for maintaining good mental health. In this paper, we detected roll-over movements during sleep using a wearable armband-shaped sensor, SenseWear Pro2 Armband. We then classified sleep depth into two stages, light sleep and deep sleep, based on roll-over frequency. We also propose a new index designated the Sleep Quality Score (SQS). Finally, we performed long-term measurement and compared SQS in healthy people and patients with major depression disorder.","Sleep,
Wearable sensors,
Mental disorders,
Acceleration,
Employment,
Frequency,
Educational technology,
Humans,
Accelerometers,
Skin"
Representing Diffusion MRI in 5-D Simplifies Regularization and Segmentation of White Matter Tracts,"We present a new five-dimensional (5-D) space representation of diffusion magnetic resonance imaging (dMRI) of high angular resolution. This 5-D space is basically a non-Euclidean space of position and orientation in which crossing fiber tracts can be clearly disentangled, that cannot be separated in three-dimensional position space. This new representation provides many possibilities for processing and analysis since classical methods for scalar images can be extended to higher dimensions even if the spaces are not Euclidean. In this paper, we show examples of how regularization and segmentation of dMRI is simplified with this new representation. The regularization is used with the purpose of denoising and but also to facilitate the segmentation task by using several scales, each scale representing a different level of resolution. We implement in five dimensions the Chan-Vese method combined with active contours without edges for the segmentation and the total variation functional for the regularization. The purpose of this paper is to explore the possibility of segmenting white matter structures directly as entirely separated bundles in this 5-D space. We will present results from a synthetic model and results on real data of a human brain acquired with diffusion spectrum magnetic resonance imaging (MRI), one of the dMRI of high angular resolution available. These results will lead us to the conclusion that this new high-dimensional representation indeed simplifies the problem of segmentation and regularization.","Magnetic resonance imaging,
Image segmentation,
Anisotropic magnetoresistance,
Image resolution,
Brain modeling,
Tensile stress,
Biomedical signal processing,
Biomedical imaging,
Hospitals,
Optical fiber theory"
Packet Reordering in Network Processors,"Network processors today consist of multiple parallel processors (micro engines) with support for multiple threads to exploit packet level parallelism inherent in network workloads. With such concurrency, packet ordering at the output of the network processor cannot be guaranteed. This paper studies the effect of concurrency in network processors on packet ordering. We use a validated Petri net model of a commercial network processor, Intel IXP 2400, to determine the extent of packet reordering for IPv4 forwarding application. Our study indicates that in addition to the parallel processing in the network processor, the allocation scheme for the transmit buffer also adversely impacts packet ordering. In particular, our results reveal that these packet reordering results in a packet retransmission rate of up to 61%. We explore different transmit buffer allocation schemes namely, contiguous, strided, local, and global which reduces the packet retransmission to 24%. We propose an alternative scheme, packet sort, which guarantees complete packet ordering while achieving a throughput of 2.5 Gbps. Further, packet sort outperforms the in-built packet ordering schemes in the IXP processor by up to 35%.","Yarn,
Throughput,
Parallel processing,
Concurrent computing,
Random access memory,
Supercomputers,
Computer science education,
Computer science,
Design automation,
Educational technology"
VANET: On Mobility Scenarios and Urban Infrastructure. A Case Study,"In [1] we show how vehicles can opportunistically exploit infrastructure through open access points (APs) to efficiently communicate with other vehicles. We also highlight the importance of the use of a correct mobility model, since the advantages that may derive from the use of an infrastructure may not be appreciated because of a lack of accuracy. We continue our study based on realistic vehicular mobility traces of downtown Portland, Oregon, obtained from extremely detailed large scale traffic simulations performed at the Los Alamos National Laboratories (LANL). This mobility model is used to evaluate both flat and opportunistic infrastructure routing. We here build upon [1] and extend that work to: (a) assess the impact of a range of mobility models on network performance and; (b) discuss the performance trend we may expect during the day, as urban mobility patterns change. We here compare results obtained with CORSIM [2] traces and Random Waypoint (RWP) [3] to the results obtained with realistic mobility traces.","Telecommunication traffic,
Traffic control,
Vehicles,
Testing,
Computer science,
Protocols,
Large-scale systems,
Laboratories,
Routing,
Ad hoc networks"
WS-Policy4MASC - A WS-Policy Extension Used in the MASC Middleware,"WS-Policy4MASC is a new XML language that we developed for specification of monitoring and control (particularly, adaptation) policies in the Manageable and Adaptable Services Compositions (MASC) middleware. It extends the Web Services Policy Framework (WS-Policy) by defining new types of policy assertions. Goal policy assertions specify requirements and guarantees to be met in desired normal operation. Action policy assertions specify actions to be taken if certain conditions are met or not met. Utility policy assertions specify monetary values assigned to particular situations. Meta-policy assertions are used to specify which action policy assertions are alternatives and which business value-driven conflict resolution strategy should be used. WS- Policy4MASC also enables detailed specification of additional information necessary for run-time policy-driven management. We evaluated feasibility of the WS- Policy4MASC solutions by implementing a policy repository and other modules in MASC. We examined their usefulness on a set of realistic scenarios.","Middleware,
Web services,
Monitoring,
XML,
Quality of service,
Computer science,
Control systems,
Australia Council,
Software engineering,
Software development management"
Variant Enhanced Dynamic Frame Slotted ALOHA Algorithm for Fast Object Identification in RFID System,"This paper analyses the practical faults existing in enhanced dynamic frame slotted ALOHA (EDFSA) algorithms, and discusses the strategy to improve the efficiency of anti-collision algorithm in RFID system. EDFSA algorithm divides the tags into a number of groups and allows only one group of tags to respond when the number of tags is much larger than optimal efficiency tags. However the efficiency of the system can still be improved. We propose a new anti-collision algorithm called by the name of variant enhanced dynamic frame slotted ALOHA algorithm (VEDFSA) to improve the efficiency of the system that can solve the problem above by dynamic divide tags into groups during the anti-collision solving procedure.","Radiofrequency identification,
Optical character recognition software,
Protocols,
Binary trees,
Fault diagnosis,
Computer science,
Algorithm design and analysis,
Heuristic algorithms,
Layout,
NP-hard problem"
On scheduling of peer-to-peer video services,"Peer-to-peer (P2P) video systems provide a cost-effective way for a large number of hosts to collaborate for video sharing. Two features characterize such a system: 1) a video is usually available on many participating hosts, and 2) different hosts typically have different sets of videos, though some may partially overlap. From a client's perspective, it can be served by any host having the video it requests. From a server's perspective, it be used to serve any client requesting the videos it has. Thus, an important question is, which servers should be used to serve which clients in the system? In this paper, we refer to this problem as service scheduling and show that different matches between clients and servers can result in significantly different system performance. Finding a right server for each client is challenging not only because a client can choose only the servers that are within its limited search scope, but also because clients arrive at different times, which are not known a priori. In this paper, we address these challenges with a novel technique called Shaking. While the proposed technique makes it possible for a client to be served by a server that is beyond the client's own search scope, it is able to dynamically adjust the match between the servers and their pending requests as new requests arrive. Our performance study shows that our new technique can dynamically balance the system workload and significantly improve the overall system performance","Peer to peer computing,
Video sharing,
IEEE members,
System performance,
Network servers,
Collaborative work,
File servers,
Computer science,
Processor scheduling,
Delay"
A uniform geometrical theory of diffraction for predicting fields of sources near or on thin planar positive/negative material discontinuities,"Relatively simple and accurate closed form Uniform Geometrical Theory of Diffraction (UTD) solutions are obtained for describing the radiated and surface wave fields, respectively, which are excited by sources near or on thin, planar, canonical two-dimensional (2-D) double positive/double negative (DPS/DNG) material discontinuities. Unlike most previous works, which analyze the plane wave scattering by such DPS structures via the Wiener-Hopf (W-H) or Maliuzhinets methods, the present development can also treat problems of the radiation by and coupling between antennas near or on finite material coatings on large metallic platforms. The latter is made possible mainly through the introduction of important higher-order UTD slope diffraction terms which are developed here in addition to first-order UTD. The present solutions are simpler to use because, in part, they do not contain the complicated split functions of the W-H solutions nor the complex Maliuzhinets functions. Unlike the latter methods based on approximate boundary conditions, the present solutions, which are developed via a heuristic spectral synthesis approach, recover the proper local plane wave Fresnel reflection and transmission coefficients and surface wave constants of the DPS/DNG material. They also include the presence of backward surface waves in DNG media. Besides being asymptotic solutions of the wave equation, the present UTD diffracted fields satisfy reciprocity, the radiation condition, boundary conditions on the conductor, and the Karp-Karal lemma which dictates that the first-order UTD space waves vanish on a material interface.","Diffraction,
Surface waves,
Surface impedance,
Surface treatment,
Impedance,
Scattering,
Antennas"
Key Refreshing in Identity-Based Cryptography and its Applications in MANETs,This paper introduces a lightweight and secure framework enabling the refreshing of private keys in identity-based public key infrastructures. The framework is applied to enable secure inter-operation between entities with different trusted authorities in dynamic coalition environments. The approach is particularly well-suited to coalition forming in computation and bandwidth-limited MANETs.,
SEDAN: Secure and Efficient protocol for Data Aggregation in wireless sensor Networks,"Energy is a scarce resource in Wireless Sensor Networks. Some studies show that more than 70% of energy is consumed in data transmission. Since most of the time, the sensed information is redundant due to geographically collocated sensors, most of this energy can be saved through data aggregation. Furthermore, data aggregation improves bandwidth usage. Unfortunately, while aggregation eliminates redundancy, it makes data integrity verification more complicated since the received data is unique. In this paper, we present a new protocol that provides secure aggregation for wireless sensor networks. Our protocol is based on a two hops verification mechanism of data integrity. Our solution is essentially different from existing solutions in that it does not require referring to the base station for verifying and detecting faulty aggregated readings, thus providing a totally distributed scheme to guarantee data integrity. We carried out simulations using TinyOS environment. Simulation results show that the proposed protocol yields significant savings in energy consumption while preserving data integrity.","Wireless application protocol,
Wireless sensor networks,
Base stations,
Redundancy,
Monitoring,
Computer networks,
Large scale integration,
Broadcast technology,
Broadcasting,
Data security"
Robustness of Local Binary Patterns in Brain MR Image Analysis,"The aging population in developed countries has shifted considerable research attention to diseases related to age. Because age is one of the highest risk factors for neurodegenerative diseases, the need for automated brain image analysis has significantly increased. Magnetic resonance imaging (MRI) is a commonly used modality to image brain. MRI provides high tissue contrast; hence, the existing brain image analysis methods have often preferred the intensity information to others, such as texture. Recently, an easy-to- compute texture descriptor, local binary pattern (LBP), has shown promise in various applications outside the medical field. In this paper, after extensive experiments, we show that rotation-invariant LBP is invariant to some common MRI artifacts that makes it possible to use it in various high-level brain MR image analysis applications.","Robustness,
Image analysis,
Image texture analysis,
Magnetic resonance imaging,
Brain,
Image motion analysis,
Diseases,
Aging,
Biomedical imaging,
Medical diagnostic imaging"
A Classwise PCA-based Recognition of Neural Data for Brain-Computer Interfaces,"We present a simple, computationally efficient recognition algorithm that can systematically extract useful information from any large-dimensional neural datasets. The technique is based on classwise Principal Component Analysis, which employs the distribution characteristics of each class to discard non-informative subspace. We propose a two-step procedure, comprising of removal of sparse non-informative subspace of the large-dimensional data, followed by a linear combination of the data in the remaining subspace to extract meaningful features for efficient classification. Our method produces significant improvement over the standard discriminant analysis based methods. The classification results are given for iEEG and EEG signals recorded from the human brain.","Brain computer interfaces,
Electroencephalography,
Application software,
Decoding,
Data mining,
Humans,
Electrodes,
Image databases,
Biomedical engineering,
Statistics"
Hybrid of Evolution and Reinforcement Learning for Othello Players,"Although the reinforcement learning and evolutionary algorithm show good results in board evaluation optimization, the hybrid of both approaches is rarely addressed in the literature. In this paper, the evolutionary algorithm is boosted using resources from the reinforcement learning. 1) The initialization of initial population using solution optimized by temporal difference learning 2) Exploitation of domain knowledge extracted from reinforcement learning. Experiments on Othello game strategies show that the proposed methods can effectively search the solution space and improve the performance","Evolutionary computation,
Optimization methods,
Space exploration,
Books,
Computational intelligence,
Computer science,
Learning systems,
Fluctuations"
Kinematics and statics of robotic catapults based on the closed elastica,"Two types of robotic catapults based on the closed elastica are proposed as robotic elements for generating impulsive motions. To obtain impulsive motions, these catapults utilize the snap-through buckling of an elastic material induced by applying driving torques to the ends of the elastic strip. In this paper, we show the effectiveness of the various types of the robotic catapults based on the kinematics and statics of a serial- chain approximation model. The numerical simulations show that these catapults can increase the elastic energy necessary for impulsive motions even if the driving torques decrease.","Kinematics,
Intelligent robots,
Strips,
Shape,
Numerical simulation,
Acceleration,
USA Councils,
Dolphins,
Gravity"
A Sensor Web Middleware with Stateful Services for Heterogeneous Sensor Networks,"As sensor networks become more pervasive there emerges a need for interfacing applications to perform common operations and transformations on sensor data. Web Services provide an interoperable and platform independent solution to these needs. A key challenge of using Web Services in this context is how to support ongoing sensor queries that persist over an extended period of time. In this paper we introduce Web Service Resource Framework (WSRF) mechanisms into the core services implementation of the NICTA Open Sensor Web Architecture (NOSA). NOSA is a suite of middleware services for sensor network applications which are built upon the OpenGIS Consortium's Sensor Web Enablement standard. WSRF expands the functionality of our services to handle simultaneous observational queries to heterogeneous Sensor Networks. It facilitates the adoption of a multi-user, multi-threaded service environment. Using components from the Globus Middleware platform, NOSA takes a major step forward to achieving the vision of a Sensor Grid.","Middleware,
Service oriented architecture,
Sensor phenomena and characterization,
Web services,
XML,
Encoding,
Computer architecture,
Laboratories,
Computer science,
Software engineering"
Timing variation-aware high-level synthesis,"The timing closure problem is one of the most important problems in the design automation. However, the rapid increase of the impact of the process variation on circuit timing makes the problem much more complicated and unpredictable to tackle in synthesis. This work addresses a new problem of high-level synthesis (HLS) that effectively takes into account the timing variation. Specifically, the work addresses the following four problems: (1) how can the statistical static timing analysis (SSTA) used in logic synthesis be modified and applied to the delay and yield computation in HLS? (2) how does the resource binding affect yield? (3) how does the scheduling affect yield? (4) how can scheduling and resource binding tasks be combined together to efficiently solve the problem with the objective of minimizing latency under yield constraint?","Timing,
High level synthesis,
Job shop scheduling,
Processor scheduling,
Computer science,
Logic,
Clocks,
Delay effects,
Yield estimation,
Design automation"
Specifying Semantic Web Service Compositions using UML and OCL,"The semantic web promises to bring automation to the areas of web service discovery, composition and invocation. In order to realize these benefits, rich semantic descriptions of web services must be created by the software developer. A steep learning curve and lack of tool support for developing such descriptions thus far have created significant adoption barriers for semantic web service technologies. In this paper, we present a model-driven architecture based approach for specifying semantic web service compositions through the use of a UML profile that extends class and activity diagrams. This profile is used in transformations that facilitate automatic construction of OWLS specifications from UML diagrams. Conditions required by the composition, such as those on control constructs, are specified using OCL and transformed into SWRL during the construction process.","Semantic Web,
Unified modeling language,
Web services,
Ontologies,
OWL,
Intelligent agent,
Service oriented architecture,
Software systems,
Computer science,
Automation"
Autonomous blimp control using model-free reinforcement learning in a continuous state and action space,"In this paper, we present an approach that applies the reinforcement learning principle to the problem of learning height control policies for aerial blimps. In contrast to previous approaches, our method does not require sophisticated hand- tuned models, but rather learns the policy online, which makes the system easily adaptable to changing conditions. The blimp we apply our approach to is a small-scale vehicle equipped with an ultrasound sensor that measures its elevation relative to the ground. The major problem in the context of learning control policies lies in the high-dimensional state-action space that needs to be explored in order to identify the values of all state-action pairs. In this paper, we propose a solution to learning continuous control policies based on the Gaussian process model. In practical experiments carried out on a real robot we demonstrate that the system is able to learn a policy online within a few minutes only.","Learning,
Ultrasonic imaging,
Gaussian processes,
State estimation,
Intelligent robots,
USA Councils,
Land vehicles,
Road vehicles,
Ultrasonic variables measurement"
Impact of an Improved Combination of Signals From Array Coils in Diffusion Tensor Imaging,"An improved method for the combination of signals from array coils is presented as a way to reduce the influence of the noise floor on the estimation of diffusion tensor imaging (DTI) parameters. By an optimized combination of signals from the array channels and complex averaging of measurements, this method leads to a significant reduction of the noise bias. This combination algorithm allows computation of accurate tensors by using the simple two point method and is shown to provide results similar to the ones obtained using the standard signal combination and a nonlinear regression method with noise parameter estimation. In many applications, the use of this combination method would result in a scan time reduction in comparison to the current standard. The effects of the improved combination on diffusion decay curves, fractional anisotropy maps, and apparent diffusion coefficient (ADC) profiles are demonstrated.","Coils,
Diffusion tensor imaging,
Phased arrays,
Anisotropic magnetoresistance,
Magnetic resonance imaging,
Signal to noise ratio,
Noise reduction,
Tensile stress,
1f noise,
Visualization"
Face Recognition Based on Curvefaces,"A new method called curvefaces was firstly presented for face recognition, which is based on curvelet transform. Curvelet is the latest multiscale geometric analysis tool. Contrast to wavelet transform, curvelet transform directly takes edges as the basic representation elements and is anisotropic with strong direction. It is a multiresolution, band pass and directional function analysis method which is useful to represent the image edges and the curved singularities in images more efficiently. It yields a more sparse representation of the image than wavelet and ridgelet transform. In face recognition, the curvelet coefficients can better represent the main features of the faces. The support vector machine (SVM) can then be used to classify the images. SVM is based on the statistical learning theory and is especially valid for small sample set and can get high recognition rate. Multi-class SVM is employed in this paper. The simulation shows that the proposed method is better than wavelet based method.","Face recognition,
Wavelet transforms,
Support vector machines,
Support vector machine classification,
Anisotropic magnetoresistance,
Frequency,
Computer science,
Image resolution,
Image analysis,
Automation"
An Initial Performance Evaluation of Rapid PHY Selection (RPS) for Energy Efficient Ethernet,"The IEEE 802.3 energy efficient Ethernet (EEE) study group is considering rapid PHY Selection (RPS) as a mechanism to quickly switch the data rate of an Ethernet link to match link data rate with link utilization. When switching the data rate, RPS causes a momentary disruption of the link. This disruption may cause packet loss due to buffer overflow in upstream switches. We emulate RPS using PAUSE flow control and experimentally study the possible effects of RPS on TCP and UDP file transfer. We show that RPS has little or no perceivable effect on performance, but has some subtle effects on TCP throughput if PAUSE flow control is enabled in the file server.","Physical layer,
Energy efficiency,
Ethernet networks,
Switches,
Packet switching,
Local area networks,
US Department of Transportation,
Computer networks,
Computer science,
Data engineering"
Capacity regions for linear and abelian network codes,"While linear network codes are proved suboptimal in general multicast scenarios, the loss of throughput due to the use of linear network codes is still unknown. This paper attempts to investigate the loss in throughput by identifying the capacity regions for linear network codes. We prove that the capacity region can be identified by taking intersection of a set of hyperplanes induced by the network and the convex cone closure of the set of all linear representable entropy functions. We also extend the study of network coding capacity region to abelian network codes which contain linear network codes as a subclass. For the case of two multicast sessions, we obtain an inner bound making use of the convex closure of entropy functions which are abelian group representable.","Network coding,
Entropy,
Throughput,
Base stations,
Broadcasting,
Downlink,
Bandwidth,
Computer science,
Joining processes,
Routing"
DpRouter: A Fast and Accurate Dynamic-Pattern-Based Global Routing Algorithm,"This paper presents a fast and accurate global routing algorithm, DpRouter, based on two efficient techniques: (1) dynamic pattern routing (Dpr), and (2) segment movement. These two techniques enable DpRouter to explore large solution space to achieve better routability with low time complexity. Compared with the state-of-the-arts, experimental results show that we consistently obtain better routing quality in terms of both congestion and wire length, while simultaneously achieving a more than 30x runtime speedup. We envision that this algorithm can be further leveraged in other routing applications, such as FPGA routing.","Routing,
Heuristic algorithms,
Wire,
Runtime,
Space exploration,
Integrated circuit synthesis,
Educational programs,
Computer science,
Field programmable gate arrays,
Table lookup"
Does SOA Improve the Supply Chain? An Empirical Analysis of the Impact of SOA Adoption on Electronic Supply Chain Performance,"Service oriented architecture (""SOA"") has been viewed as a strategic approach to IT that provides increased flexibility. However, there is scant research evidence of SOA adoption leading to tangible performance benefits across a cross section of firms. We fill this research gap by empirically analyzing the impact of SOA adoption on the performance of electronic supply chains for a cross section of large US firms. We find that adoption of SOA does lead to better performance of the electronic supply chain. We also find that SOA moderates firm's ability to leverage electronically integrated customers to achieve better electronic supply chain performance. Further, we show that the impact of SOA adoption is fully mediated through its moderation effect on the firm's ability to leverage electronically integrated customers to achieve higher electronic supply chain performance. Lastly, the paper discusses how IT managers can make informed SOA adoption decisions","Semiconductor optical amplifiers,
Supply chains,
Performance analysis,
Service oriented architecture,
Application software,
Software architecture,
Computer architecture,
Communication standards,
Simple object access protocol,
XML"
Penalized Probabilistic Clustering,"While clustering is usually an unsupervised operation, there are circumstances in which we believe (with varying degrees of certainty) that items A and B should be assigned to the same cluster, while items A and C should not. We would like such pairwise relations to influence cluster assignments of out-of-sample data in a manner consistent with the prior knowledge expressed in the training set. Our starting point is probabilistic clustering based on gaussian mixture models (GMM) of the data distribution. We express clustering preferences in a prior distribution over assignments of data points to clusters. This prior penalizes cluster assignments according to the degree with which they violate the preferences. The model parameters are fit with the expectation-maximization (EM) algorithm. Our model provides a flexible framework that encompasses several other semisupervised clustering models as its special cases. Experiments on artificial and real-world problems show that our model can consistently improve clustering results when pairwise relations are incorporated. The experiments also demonstrate the superiority of our model to other semisupervised clustering methods on handling noisy pairwise relations.",
Reliable network-on-chip based on generalized de Bruijn graph,"In this paper, we propose the generalized de Bruijn graph as a reliable and efficient network topology for a Network-on-Chip (NoC) design. We also propose a reliable routing algorithm to detour a problematic (i.e., faulty or congested) link. Our experimental results show that the latency and energy consumption of generalized de Bruijn graph are much less with compared to Mesh and Torus, the two common NoC architectures in the literature. The low energy consumption of de Bruijn graph-based NoC makes it suitable for portable devices which have to operate on limited batteries. Also, the gate level implementation of the proposed reliable routing shows a small area, power, and timing overheads due to the proposed reliable routing algorithm.","Network-on-a-chip,
Routing,
Network topology,
Computer network reliability,
Delay,
Fault tolerance,
Energy consumption,
Power system reliability,
Shift registers,
Computer science"
Cotransformation Provides Area and Accuracy Improvement in an HDL Library for LNS Subtraction,"The reduction of the cumbersome operations of multiplication, division, and powering to addition, subtraction and multiplication is what makes the Logarithmic Number System (LNS) attractive. Addition and subtraction, though, are the bottleneck of every LNS circuit, for which there are implementation techniques that tradeoff area, latency and accuracy. This paper reviews the methods of interpolation, multipartite tables and cotransformation for LNS addition and subtraction, but special focus is given on a novel version of cotransformation, for which a new special case is identified. Synthesis results compare an already published Hardware Description Language (HDL) library for LNS arithmetic that uses only multipartite tables or 2nd-order interpolation against a variation of the same library combined with cotransformation. Exhaustive simulation and a graphics example illustrate that the proposed library has smaller area requirements and is more accurate than the earlier library, at the cost of an increase in the latency of the hardware.",
BioRoute: a network-flow based routing algorithm for digital microfluidic biochips,"Due to the recent advances in microfluids, digital microfluidic biochips are expected to revolutionize laboratory procedures. One critical problem for biochip synthesis is the droplet routing problem. Unlike traditional VLSI routing problems, in addition to routing path selection, the biochip routing problem needs to address the issue of scheduling droplets under the practical constraints imposed by the fluidic property and the timing restriction of the synthesis result. In this paper, we present the first network-flow based routing algorithm that can concurrently route a set of non-interfering nets for the droplet routing problem on biochips. We adopt a two-stage technique of global routing followed by detailed routing. In global routing, we first identify a set of non-interfering nets and then adopt the network-flow approach to generate optimal global-routing paths for the nets. In detailed routing, we present the first polynomial-time algorithm for simultaneous routing and scheduling using the global-routing paths with a negotiation-based routing scheme. The experimental results show the robustness and efficiency of our algorithm.","Routing,
Microfluidics,
Optical detectors,
Laboratories,
Electrodes,
Reservoirs,
Very large scale integration,
Timing,
Nanobioscience,
Optical arrays"
Rapidly Prototyped Orthotweezers for Automated Microassembly,"We describe the design, fabrication, and testing of an ultra-low cost orthotweezers system for microassembly. By utilizing rapid prototyping technology, compliant mechanisms, and commodity-grade actuators and sensors, we significantly reduce the complexity and cost of the previous Orthotweezers system without sacrificing functionality. With a force resolution of 0.7mN and a worst case mean positioning repeatability of 23 mum, the system is capable of dexterously manipulating rectangular parts with dimensions 200 mum times 200 mum times 100 mum. Such blocks can then be temporarily attached to thin, delicate, or oddly shaped parts to enable handling and ultimately assembly of micromechanical structures. Strategies for using compliance to compensate for uncertainty introduced by less expensive fabrication methods, actuators, and sensors are also discussed.","Prototypes,
Microassembly,
Fabrication,
Actuators,
System testing,
Sensor systems,
Cost function,
Force sensors,
Assembly,
Micromechanical devices"
SEU-Mitigation Placement and Routing Algorithms and Their Impact in SRAM-Based FPGAs,"In this paper, the authors propose a new SEU-mitigative placement and routing of circuits in the FPGAs which is based on the popular VPR tool. The VPR tool is modified so that during placement and routing, decisions are taken with awareness of SEU-mitigation. Moreover, no redundancies during the placement and routing are used but the algorithms are based on the SEU avoidance. Using the modified tool, i.e., S-VPR, the role of placement and routing algorithms on the fault-tolerance of circuits implemented on FPGAs is achieved. The secondary propose of this paper is to find which of placement or routing is more suited for decreasing SEU sensibility of circuits and to find whether these SEU sensibility reductions are cumulative or not when they applied in sequence. We have investigated the effect of S-VPR on several MCNC benchmarks and the results of the placement and routing have been compared to the traditional one. The evaluations of results show that placement and routing can decrease the SEU rate of circuits implemented on FPGAs about 18% and 12%, respectively. However, it increases critical path delay and power consumptions of the circuits up to 5% and 8%, respectively. This means that without any redundancies, just by means of fault-avoidance method, mitigation of SEU effects would decrease up to 22% significantly and this method is notable compared to previous TMR and DWC mechanisms","Routing,
Field programmable gate arrays,
Single event upset,
Circuit faults,
Redundancy,
Digital systems,
Space technology,
Packaging,
DH-HEMTs,
Computer science"
A Combinatorial Procurement Auction for QoS-Aware Web Services Composition,"Business processes and application functionality are becoming available as internal web services inside enterprise boundaries as well as becoming available as commercial web services from enterprise solution vendors and web services marketplaces. Typically there are multiple web service providers offering services capable of fulfilling a particular functionality, although with different Quality of Service (QoS). Dynamic creation of business processes requires composing an appropriate set of web services that best suit the current need. This paper presents a novel combinatorial auction approach to QoS aware dynamic web services composition. Such an approach would enable not only stand-alone web services but also composite web services to be a part of a business process. The combinatorial auction leads to an integer programming formulation for the web services composition problem. An important feature of the model is the incorporation of service level agreements. We describe a software tool QWESC for QoS-aware web services composition based on the proposed approach.","Procurement,
Web services,
Quality of service,
Business,
Linear programming,
Automation,
Meteorological radar,
Capacity planning,
Genetic algorithms,
Service oriented architecture"
"A Note on Problem Difficulty Measures in Black-Box Optimization: Classification, Realizations and Predictability","Various methods have been defined to measure the hardness of a fitness function for evolutionary algorithms and other black-box heuristics. Examples include fitness landscape analysis, epistasis, fitness-distance correlations etc., all of which are relatively easy to describe. However, they do not always correctly specify the hardness of the function. Some measures are easy to implement, others are more intuitive and hard to formalize. This paper rigorously defines difficulty measures in black-box optimization and proposes a classification. Different types of realizations of such measures are studied, namely exact and approximate ones. For both types of realizations, it is proven that predictive versions that run in polynomial time in general do not exist unless certain complexity-theoretical assumptions are wrong.",
Barebones particle swarm methods for unsupervised image classification,"A clustering method that is based on barebones particle swarm (BB) is developed in this paper. BB is a variant of particle swarm optimization (PSO) where parameter tuning is not required. The proposed algorithm finds the centroids of a user specified number of clusters, where each cluster groups together similar patterns. The application of the proposed clustering algorithm to the problem of unsupervised classification and segmentation of images is investigated. To illustrate its wide applicability, the proposed algorithms are then applied to synthetic, MRI and satellite images. Experimental results show that the BB-based clustering algorithm performs very well compared to other state-of-the-art clustering algorithms in all measured criteria.","Particle swarm optimization,
Image classification,
Clustering algorithms,
Partitioning algorithms,
Iterative algorithms,
Image segmentation,
Pixel,
Gaussian distribution,
Clustering methods,
Magnetic resonance imaging"
Fusion of Multiple Camera Views for Kernel-Based 3D Tracking,"We present a computer vision system to robustly track an object in 3D by combining evidence from multiple calibrated cameras. Its novelty lies in the proposed unified approach to 3D kernel based tracking, that amounts to fusing the appearance features from all available camera sensors, as opposed to tracking the object appearance in the individual 2D views and fusing the results. The elegance of the method resides in its inherent ability to handle problems encountered by various 2D trackers, including scale selection, occlusion, view-dependence, and correspondence across different views. We apply the method on the CHIL project database for tracking the presenter's head during lectures inside smart rooms equipped with four calibrated cameras. As compared to traditional 2D based mean shift tracking approaches, the proposed algorithm results in 35% relative reduction in overall 3D tracking error and a 70% reduction in the number of tracker re-initializations.","Kernel,
Computer vision,
Robustness,
Smart cameras,
Humans,
Computer science,
Sensor phenomena and characterization,
Databases,
Histograms,
Head"
How the Cyber Defense Exercise Shaped an Information-Assurance Curriculum,"In this article, we provide a brief history of the Cyber Defense Exercise (CDX), describe Air Force Institute of Technology (AFIT's) participation in it, and explain how the experience shaped the information-assurance curriculum and course format at AFIT. The CDX is an annual competition designed to give students the opportunity to learn and demonstrate best practices in defensive information assurance. The CDX's fundamental objective is to design and implement a network that provides specified IT services and defend it against an onslaught of cyberattacks and natural events. CDX participants included blue forces (students), red forces (attacker) and a white cell.","Educational institutions,
Information security,
Computer networks,
Computer hacking,
Military computing,
Computer crime,
Computer security,
National security,
Best practices,
Computer architecture"
Finding Maximum Margin Segments in Speech,"Maximum margin clustering (MMC) is a relatively new and promising kernel method. In this paper, we apply MMC to the task of unsupervised speech segmentation. We present three automatic speech segmentation methods based on MMC, which are tested on TIMIT and evaluated on the level of phoneme boundary detection. The results show that MMC is highly competitive with existing unsupervised methods for the automatic detection of phoneme boundaries. Furthermore, initial analyses show that MMC is a promising method for the automatic detection of sub-phonetic information in the speech signal.","Kernel,
Speech analysis,
Support vector machines,
Speech processing,
Computer science,
Automatic speech recognition,
Automatic testing,
Information analysis,
Signal analysis,
Clustering methods"
Middleware based Inpatient Healthcare Information System,"The paper presents a multi-tier, integrated, distributed, inpatient healthcare information system based on service oriented architecture (SOA) .NET environment in National Taiwan University Hospital (NTUH). The architecture and outcomes of the newly developed inpatient information system (IIS) platform are discussed in details. We also present mechanisms of integration as well as interoperability among the components and multi-database in IIS via health level seven (HL7) Middleware layer. The preliminary performance of the current operating IIS is evaluated and analyzed to verify the efficiency and effectiveness of the architecture we designed.","Middleware,
Medical services,
Information systems,
Service oriented architecture,
Portals,
Hospitals,
Picture archiving and communication systems,
Switches,
Engines,
Computer science"
Assessing Network Service Profitability: Modeling From Market Science Perspective,"Network service providers regularly conduct network planning and upgrade processes to keep their businesses profitable. The effectiveness of a network upgrade/planning decision is intrinsically tied to the ability of a provider to retain and grow its customer population. This paper examines the crucial linkage between network performance, customer satisfaction and profitability of network service, and presents an analytical modeling approach from market science perspective. We derive a generalized forecasting model that projects service profitability from the underlying network service infrastructure and the subscriber population. Through simulation studies and analysis, we show how such approach captures key factors and trends influencing service profitability and how it can significantly improve current network planning and upgrade processes.","Profitability,
Analytical models,
Process planning,
Customer satisfaction,
Performance analysis,
Investments,
Joining processes,
Mathematical model,
Couplings,
Economic forecasting"
Being Sensitive to Uncertainty,"Predictive modeling's effectiveness is hindered by inherent uncertainties in the input parameters. Sensitivity and uncertainty analysis quantify these uncertainties and identify the relationships between input and output variations, leading to the construction of a more accurate model. This survey introduces the application, implementation, and underlying principles of sensitivity and uncertainty quantification","Sensitivity analysis,
Diseases,
Predictive models,
Sampling methods,
Computational modeling,
Input variables,
Probability density function,
Algorithm design and analysis,
Measurement uncertainty,
Statistical analysis"
Multi-Resource Manycast over Optical Burst Switched Networks,"We define and investigate the problem of multi-resource manycast over optical burst switched (OBS) networks for supporting distributed computing applications. In multi-resource manycast, each destination has multiple computing resources, and each source generates requests that require multiple resources. For a given request, the problem is to select a set of destinations that have the required available resources and to And routes to these destinations. This problem differs from the traditional manycast problem in that different destinations have different resource availability. The objective is to minimize the resource blocking rate, which results from burst contention in the OBS network and from resource unavailability at the destinations. We investigate various approaches to implement multi-resources manycast over OBS networks, and verify the effectiveness of the proposed schemes through simulation.","Optical fiber networks,
Distributed computing,
Costs,
Availability,
Grid computing,
Wavelength division multiplexing,
Computer networks,
Computer science,
Optical computing,
Application software"
Portable and Efficient Continuous Data Protection for Network File Servers,"Continuous data protection, which logs every update to a file system, is an enabling technology to protect file systems against malicious attacks and/or user mistakes, because it allows each file update to be undoable. Existing implementations of continuous data protection work either at disk access interface or within the file system. Despite the implementation complexity, their performance overhead is significant when compared with file systems that do not support continuous data protection. Moreover, such kernel-level file update logging implementation is complex and cannot be easily ported to other operating systems. This paper describes the design and implementation of four user-level continuous data protection implementations for NFS servers, all of which work on top of the NFS protocol and thus can be easily ported to any operating systems that support NFS. Measurements obtained from running standard benchmarks and real-world NFS traces on these user-level continuous data protection systems demonstrate a surprising result: Performance of NFS servers protected by pure user-level continuous data protection schemes is comparable to that of unprotected vanilla NFS servers.","Protection,
File servers,
File systems,
Operating systems,
Hardware,
Humans,
Delay,
Computer science,
Protocols,
Loss measurement"
A Banded Smith-Waterman FPGA Accelerator for Mercury BLASTP,"Large-scale protein sequence comparison is an important but compute-intensive task in molecular biology. The popular BLASTP software for this task has become a bottleneck for proteomic database search. One third of this software's time is spent executing the Smith-Waterman dynamic programming algorithm. This work describes a novel FPGA design for banded Smith-Waterman, an algorithmic variant tuned to the needs of BLASTP. This design has been implemented in Mercury BLASTP, our FPGA-accelerated version of the BLASTP algorithm. We show that Mercury BLASTP runs 6-16 times faster than software BLASTP on a modern CPU while delivering 99% identical results.","Field programmable gate arrays,
Proteomics,
Databases,
Acceleration,
Biology computing,
Algorithm design and analysis,
Hardware,
Protein sequence,
Dynamic programming,
Computational efficiency"
Variable-Length Unit Selection in TTS Using Structural Syntactic Cost,"This paper presents a variable-length unit selection scheme based on syntactic cost to select text-to-speech (TTS) synthesis units. The syntactic structure of a sentence is derived from a probabilistic context-free grammar (PCFG), and represented as a syntactic vector. The syntactic difference between target and candidate units (words or phrases) is estimated by the cosine measure with the inside probability of PCFG acting as a weight. Latent semantic analysis (LSA) is applied to reduce the dimensionality of the syntactic vectors. The dynamic programming algorithm is adopted to obtain a concatenated unit sequence with minimum cost. A syntactic property-rich speech database is designed and collected as the unit inventory. Several experiments with statistical testing are conducted to assess the quality of the synthetic speech as perceived by human subjects. The proposed method outperforms the synthesizer without considering syntactic property. The structural syntax estimates the substitution cost better than the acoustic features alone",
Stochastic Analysis and Improvement of the Reliability of DHT-Based Multicast,"This paper investigates the reliability of application-level multicast based on a distributed hash table (DHT) in a highly dynamic network. Using a node residual lifetime model, we derive the stationary end-to-end delivery ratio of data streaming between a pair of nodes in the worst case, and show through numerical examples that in a practical DHT network, this ratio can be very low (e.g., less than 50%). Leveraging the property of heavy-tailed lifetime distribution, we then consider three optimizing techniques, namely senior member overlay (SMO), longer-lived neighbor selection (LNS), and reliable route selection (RRS), and present quantitative analysis of data delivery reliability under these schemes. In particular, we discuss the tradeoff between delivery ratio and the load imbalance among nodes. Simulation experiments are also used to evaluate the multicast performance under practical settings. Our model and analytic results provide useful tools for reliability analysis for other overlay-based applications (e.g., those involving persistent data transfers).","Stochastic processes,
Peer to peer computing,
Multicast protocols,
Computer network reliability,
Senior members,
Application software,
Communications Society,
Speech analysis,
Telecommunication network reliability,
Computer science"
Symmetries of non-rigid shapes,"Symmetry and self-similarity is the cornerstone of Nature, exhibiting itself through the shapes of natural creations and ubiquitous laws of physics. Since many natural objects are symmetric, the absence of symmetry can often be an indication of some anomaly or abnormal behavior. Therefore, detection of asymmetries is important in numerous practical applications, including crystallography, medical imaging, and face recognition, to mention a few. Conversely, the assumption of underlying shape symmetry can facilitate solutions to many problems in shape reconstruction and analysis. Traditionally, symmetries are described as extrinsic geometric properties of the shape. While being adequate for rigid shapes, such a description is inappropriate for non-rigid ones. Extrinsic symmetry can be broken as a result of shape deformations, while its intrinsic symmetry is preserved. In this paper, we pose the problem of finding intrinsic symmetries of non-rigid shapes and propose an efficient method for their computation.","Shape,
Physics,
Biomedical imaging,
Face detection,
Crystallography,
Face recognition,
Image reconstruction,
Humans,
Neoplasms,
Computer science"
Representing and recognizing complex events in surveillance applications,"In this paper, we investigate the problem of representing and maintaining rule knowledge for a video surveillance application. We focus on complex events representation which cannot be straightforwardly represented by canonical means. In particular, we highlight the ongoing efforts for a unifying framework for computable rule and taxonomical knowledge representation.","OWL,
Ontologies,
Application software,
Engines,
Semantic Web,
Mathematics,
Computer science,
Video surveillance,
Knowledge representation,
Intelligent systems"
Competency-Based Learning Object Sequencing Using Particle Swarms,"In e-learning initiatives, sequencing problem concerns arranging a particular set of learning units in a suitable succession for a particular learner. Sequencing is usually performed by instructors, who create general and ordered series rather than learner personalized sequences. This paper proposes an innovative intelligent technique for learning object automated sequencing using particle swarms. E-learning standards are promoted in order to ensure interoperability. Competencies are used to define relations between learning objects within a sequence, so that the sequencing problem turns into a permutation problem and AI techniques can be used to solve it. Particle Swarm Optimization (PSO) is one of such techniques and it has proven with good performance solving a wide variety of problems. An implementation of the PSO, for learning object sequencing, is presented and its performance in a real scenario is discussed.","Particle swarm optimization,
Electronic learning,
Measurement standards,
Artificial intelligence,
Courseware,
Adaptive systems,
Computer science,
Testing,
Assembly,
Code standards"
Dynamic Multi-User Load Balancing in Distributed Systems,"In this paper, we review two existing static load balancing schemes based on M/M/1 queues. We then use these schemes to propose two dynamic load balancing schemes for multi-user (multi-class) jobs in heterogeneous distributed systems. These two dynamic load balancing schemes differ in their objective. One tries to minimize the expected response time of the entire system while the other tries to minimize the expected response time of the individual users. The performance of the dynamic schemes is compared with that of the static schemes using simulations with various loads and parameters. The results show that, at low communication overheads, the dynamic schemes show superior performance over the static schemes. But as the overheads increase, the dynamic schemes (as expected) yield similar performance to that of the static schemes.","Load management,
Delay,
Communication networks,
Distributed computing,
Computer science,
USA Councils,
System performance,
Nash equilibrium,
Computer networks,
Distributed processing"
The Use of Time-Frequency Distributions for Epileptic Seizure Detection in EEG Recordings,"Epileptic seizures are manifestations of epilepsy, which is a serious brain dynamic disorder. The analysis of the electroencephalographic (EEG) recordings provides valuable insight and improved understanding of the mechanisms causing epileptic disorders. An epileptic seizure is usually identified by polyspike activity; rhythmic waves for a wide variety of frequencies and amplitudes as well as spike-and-wave complexes. The detection of all these waveforms in the EEG is a crucial component in the diagnosis of epilepsy. Time-frequency analysis is particularly effective for representing various aspects of nonstationary signals such as trends, discontinuities, and repeated patterns where other signal processing approaches fail or are not as effective. In this paper a novel method of analysis of EEG signals using time-frequency analysis, and classification using artificial neural network, is introduced. EEG segments are analyzed using a time-frequency distribution and then, several features are extracted for each segment representing the energy distribution over the time-frequency plane. The features are used for the training of a neural network. Short-time Fourier transform and several time-frequency distributions are compared. The proposed approach is tested using a publicly available database and satisfactory results are obtained (89-100% accuracy).","Time frequency analysis,
Epilepsy,
Electroencephalography,
Signal processing,
Artificial neural networks,
Signal analysis,
Feature extraction,
Biological neural networks,
Fourier transforms,
Testing"
On the Secure Software Development Process: CLASP and SDL Compared,"Development processes for software construction are common knowledge and mainstream practice in most development organizations. Unfortunately, these processes offer little support in order to meet security requirements. Over the years, research efforts have been invested in specific methodologies and techniques for secure software engineering, yet complete, dedicated processes have been proposed only recently. In this paper, two high-profile processes for the development of secure software, namely OWASP's CLASP and Microsoft's SDL, are evaluated and compared in detail. The paper identifies the commonalities, discusses the specificity of each approach, and proposes suggestions for improvement.","Programming,
Security,
Software engineering,
Documentation,
Computer science,
Guidelines,
Best practices,
Risk management,
Counting circuits,
Books"
Multiobjective genetic estimation to induction motor parameters,"In order to simplify the offline identification of induction motor parameters, a method based on optimization using a multiobjective genetic algorithm is proposed. The non- dominated sorting genetic algorithm (NSGA-II) is used to minimize the error between the actual data and an estimated model. The robustness of the method is shown by identifying parameters of the induction motor in three different cases. The simulation results show that the method successfully estimates the motor parameters.","Induction motors,
Genetic algorithms,
Parameter estimation,
Robustness,
Computer science education,
Optimization methods,
Sorting,
Hydrogen,
Degradation"
Detecting Information Flows: Improving Chaff Tolerance by Joint Detection,"The problem of detecting encrypted information flows using timing information is considered. An information flow consists of both information-carrying packets and irrelevant packets called chaff. A relay node can perturb the timing of information-carrying packets as well as adding or removing chaff packets. The goal is to detect whether there is an information flow through certain nodes of interest by analyzing the transmission times of these nodes. Under the assumption that the relay of information-carrying packets is subject to a bounded delay constraint, fundamental limits on detection are characterized as the minimum amount of chaff needed for an information flow to mimic independent traffic. A detector based on the optimal chaff-inserting algorithms is proposed. The detector guarantees detection in the presence of an amount of chaff proportional to the total traffic size; furthermore, the proportion increases to 100% exponentially fast as the number of hops on the flow path increases.","Detectors,
Timing,
Relays,
Delay,
Cryptography,
Telecommunication traffic,
Mobile ad hoc networks,
Noise measurement,
Government,
Helium"
DNA sequence compression - Based on the normalized maximum likelihood model,"Genomic data provide challenging problems that have been studied in a number of fields such as statistics, signal processing, information theory, and computer science. This article shows that the methodologies and tools that have been recently developed in these fields for modeling signals and processes appear to be most promising for genomic research","DNA,
Sequences,
Genomics,
Bioinformatics,
Proteins,
Organisms,
Signal processing,
Encoding,
Biological information theory,
Biomedical signal processing"
Disciplining Orchestration and Conversation in Service-Oriented Computing,"We give a formal account of a calculus for modeling service-based systems, suitable to describe both service composition (orchestration) and the protocol that services run when invoked (conversation). The calculus includes primitives for defining and invoking services, for isolating conversations between clients and servers, and for orchestrating services. The calculus is equipped with a reduction and a labeled transition semantics related by an equivalence result. To hint how the structuring mechanisms of the language can be exploited for static analysis we present a simple type system guaranteeing the compatibility between client and server protocols, an application of bisimilarity to prove equivalence among services, and we discuss deadlock-avoidance.","Calculus,
Web services,
Informatics,
Protocols,
Application software,
Middleware,
Stress,
Software engineering,
Computer science,
Mathematics"
MIMO identification with optimal experiment design for rigid robot manipulators,"This paper proposes a practical parameters identification procedure for robot manipulators. It is based on the well-known maximum likelihood method adopting particular techniques to facilitate the estimation: condition number reduction methods for the input data matrix with optimal trajectory planning, and two different methods for variances estimation. Moreover, the identification problem is solved with reference to the multi input multi output coupled system. A closed loop identification is needed because the system is open loop unstable, and, moreover, because to correctly execute the identification procedure, the robot has to track an optimal reference input. Some solutions are also presented to overtake common identification problems, such as bias of the estimated parameters, outliers detection and elimination, and noise sensitivity of the estimation. The presented procedure was successfully tested on a COMAU SMART3-S2 industrial manipulator demonstrating its efficiency.","MIMO,
Manipulators,
Parameter estimation,
Maximum likelihood detection,
Maximum likelihood estimation,
Trajectory,
Service robots,
Robot sensing systems,
Tracking loops,
Testing"
Preparing Software Engineering Graduates for an Industry Career,"The lack of preparedness of software engineering (SE) graduates for a professional career is a common complaint raised by industry practitioners. The career progression of many new graduates is severely impacted due to the lack of well rounded skills. For example, some of the technically stronger graduates lack communication and managerial skills and vise versa. Industry based capstone projects, incorporated as a part of an undergraduate degree, are a well accepted means of preparing students for their professional careers. Software Engineering undergraduates at the University of Melbourne engage in such industry based projects both in the penultimate and final years of their degree. Though aimed at providing students a real-life SE experience and preparing them for industry, we observed these projects to fail in some cases in giving the necessary breadth of skills. We believe this failure to be due to the lack of an objective framework to guide student learning outcomes during projects. To address this problem we developed an objective skill-based framework, focusing on managerial, engineering and personal skills. In this paper we present this framework and share our experiences of using it.",
A Fast KNN Algorithm for Text Categorization,"The KNN algorithm applied to text categorization is a simple, valid and non-parameter method. The traditional KNN has a fatal defect that the time of similarity computing is huge. The practicality will be lost when the KNN algorithm is applied to text categorization with the high dimension and huge samples. In this paper, a method called TFKNN(Tree-Fast-K-Nearest-Neighbor) is presented, which can search the exact k nearest neighbors quickly. In the method, a SSR tree for searching K nearest neighbors is created, in which all child nodes of each non-leaf node are ranked according to the distances between their central points and the central point of their parent. Then the searching scope is reduced based on the tree. Subsequently , the time of similarity computing is decreased largely.","Text categorization,
Nearest neighbor searches,
Machine learning algorithms,
Machine learning,
Cybernetics,
Mathematics,
Computer science,
Systems engineering and theory,
Web sites,
Support vector machines"
Fast and Reliable Stream Processing over Wide Area Networks,"We present a replication-based approach that enables both fast and reliable stream processing over wide area networks. Our approach replicates stream processing operators in a manner where operator replicas compete with each other to make the earliest impact. Therefore, any processing downstream from such replicas can proceed by relying on the fastest replica without being held back by slow or failed ones. Furthermore, our approach allows replicas to produce output in different orders so as to avoid the cost of forcing an identical execution across replicas, without sacrificing correctness. We first consider semantic issues for correct replicated stream processing and, based on a formal foundation, extend common stream-processing primitives. Next, we discuss strategies for deploying replicas. Finally, we present preliminary remits obtained from experiments on Planet-Lab that substantiate the potential benefits of our approach.","Wide area networks,
Network servers,
Delay,
Application software,
Monitoring,
Computer network reliability,
Computer science,
Costs,
Distributed processing,
Surges"
A Comparative Study on 2D Curvature Estimators,"Curvature is a frequently used property in two-dimensional (2D) shape analysis, directly or for derived features such as corners or convex and concave arcs. This paper presents curvature estimators which follow approaches in differential geometry. Digital-straight segment approximation (as known from digital geometry) is used in those estimators. Results of multigrid experiments are evaluated leading to a comparative performance analysis of several curvature estimators",
"DejaVu: Transparent User-Level Checkpointing, Migration, and Recovery for Distributed Systems","In this paper, we present a new fault tolerance system called DejaVu for transparent and automatic checkpointing, migration, and recovery of parallel and distributed applications. DejaVu provides a transparent parallel checkpointing and recovery mechanism that recovers from any combination of systems failures without any modification to parallel applications or the OS. It uses a new runtime mechanism for transparent incremental checkpointing that captures the least amount of state needed to maintain global consistency and provides a novel communication architecture that enables transparent migration of existing MPI codes, without source-code modifications. Performance results from the production-ready implementation show less than 5% overhead in real-world parallel applications with large memory footprints.","Checkpointing,
Application software,
Concurrent computing,
Runtime,
Stability,
Computer networks,
Distributed computing,
Laboratories,
Computer science,
Fault tolerant systems"
Sensor Network with Delay Tolerance (SeNDT),"As the technology underlying sensor networks becomes more advanced and reliable, ubiquitous computing will come closer to being a reality. This paper details the design and deployment of several sensor nodes that use the delay tolerant network approach in order to meet the relevant application requirements. Two applications are presented, lake water quality monitoring in rural lakes and noise level logging in urban areas and along motorways. The installations are discussed and initial results presented.","Disruption tolerant networking,
Educational institutions,
Costs,
Hardware,
Lakes,
Monitoring,
Telecommunication network reliability,
Protocols,
Mechanical sensors,
Urban areas"
Design Analysis of a High-Resolution Panoramic Camera Using Conventional Imagers and a Mirror Pyramid,"Wide field of view (FOV) and high-resolution image acquisition is highly desirable in many vision-based applications. Several systems have reported the use of reflections off mirror pyramids to capture high-resolution, single-viewpoint, and wide-FOV images. Using a dual mirror pyramid (DMP) panoramic camera as an example, in this paper, we examine how the pyramid geometry, and the selection and placement of imager clusters can be optimized to maximize the overall panoramic FOV, sensor utilization efficiency, and image uniformity. The analysis can be generalized and applied to other pyramid-based designs","Image analysis,
Cameras,
Mirrors,
Optical imaging,
Optical sensors,
Image sensors,
Layout,
Geometrical optics,
High-resolution imaging,
Optical reflection"
Multi-Layer Integrated Anomaly Intrusion Detection System for Mobile Adhoc Networks,"Most intrusion detection systems for mobile ad hoc networks are focusing on either routing protocols or MAC layer traffic. This paper focuses on the design of a new anomaly detection system for each node of the network, which contains detection subsystem for MAC layer, routing layer and application layer. Audit data taken from MAC level/network level/application level from the traces in Glomosim and are preprocessed separately for each layer's detection subsystem. Feature data sets for each layer are selected from normal transactions. The detection subsystem contains normal profiles obtained from the feature vectors of training data sets. In our work, we used Bayesian classification algorithm, Markov chain construction algorithm and association rule mining algorithm for anomaly detection in MAC layer, routing layer and application layer respectively for effective intrusion detection. Test data obtained from the network traffic is feed in to the detection subsystems. If there is any deviation from normal behavior, it is considered as abnormal or anomaly based on predefined thresholds. Intrusion results from detection subsystems of all the three layers are integrated at local integration module and the final result is sent to the global integration module. Intrusion results are received also from the neighbor nodes and are sent to the global integration module for making a final decision","Intrusion detection,
Telecommunication traffic,
Mobile ad hoc networks,
Routing protocols,
Media Access Protocol,
Training data,
Bayesian methods,
Classification algorithms,
Association rules,
Data mining"
Autonomous rigid body attitude synchronization,"This paper studies some extensions to the decentralized attitude synchronization of identical rigid bodies. Considering fully actuated Euler equations, the communication links between the rigid bodies are limited and the available information is restricted to relative orientations and angular velocities. In particular, no leader nor external reference dictates the swarm's behavior. The control laws are derived using two classical approaches of nonlinear control - tracking and energy shaping. This leads to a comparison of two corresponding methods which are currently considered for distributed synchronization - consensus and stabilization of mechanical systems with symmetries.","Communication system control,
Mechanical systems,
Attitude control,
Nonlinear equations,
Shape control,
USA Councils,
Angular velocity,
Distributed decision making,
Heuristic algorithms,
Distributed computing"
A Geometric Optimization Approach to Detecting and Intercepting Dynamic Targets,"A methodology is developed to deploy a mobile sensor network for the purpose of detecting and capturing mobile targets in the plane. The sensing-pursuit problem considered in this paper is analogous to the Marco Polo game, in which the pursuer must capture multiple mobile targets that are sensed intermittently, and with very limited information. In this paper, the mobile sensor network consists of a set of robotic sensors that must track and capture mobile targets based on the information obtained through cooperative detections. Since the sensors are installed on robotic platforms and have limited range, the geometry of the platforms and of the sensors field-of- view play a key role in obstacle avoidance and target detection. Thus, a new cell decomposition approach is presented to formulate the probability of detection and the cost of operating the robots based on the geometric properties of the network. Numerical simulations verify the validity and flexibility of our methodology.","Robot kinematics,
Vehicle dynamics,
Robot sensing systems,
Target tracking,
Intelligent sensors,
Object detection,
Sensor phenomena and characterization,
Motion planning,
Mobile robots,
Sensor systems"
A Novel Current Control System for PMSM Considering Effects from Inverter in Overmodulation Range,"In this paper, we propose a novel current control system for PMSM when an inverter operates in the overmodulation range. Because of the effect from harmonic components generated from an inverter in this range, an unstable problem in current control system will occur. For solving this problem, the method of harmonic currents compensation is proposed. However, the difficulty in this method is that how much we can estimate harmonic current accurately. There are two new topics in harmonic components estimation those we use in our proposed current control system. First, the harmonic voltages estimation method based on the conventional Sine-PWM modulation method of an inverter. Second, the harmonic currents estimation method that not consider only a model of PMSM, but also an effect from current controllers. The effectiveness of our proposed system is confirmed by computer simulation and experimental results.","Current control,
Inverters,
Voltage control,
Control systems,
Electric current control,
Frequency,
Nonlinear control systems,
Open loop systems,
Computer simulation,
Permanent magnet motors"
Delaunay Triangulation with Transactions and Barriers,"Transactional memory has been widely hailed as a simpler alternative to locks in multithreaded programs, but few nontrivial transactional programs are currently available. We describe an open-source implementation of Delaunay triangulation that uses transactions as one component of a larger parallelization strategy. The code is written in C+ +, for use with the RSTM software transactional memory library (also open source). It employs one of the fastest known sequential algorithms to triangulate geometrically partitioned regions in parallel; it then employs alternating, barrier-separated phases of transactional and partitioned work to stitch those regions together. Experiments on multiprocessor and multicore machines confirm excellent single-thread performance and good speedup with increasing thread count. Since execution time is dominated by geometrically partitioned computation, performance is largely insensitive to the overhead of transactions, but highly sensitive to any costs imposed on shamble data that are currently ""privatized"".","Open source software,
Partitioning algorithms,
Multicore processing,
Java,
Computer science,
Software libraries,
Yarn,
High performance computing,
Time sharing computer systems,
Costs"
Energy Minimization with Soft Real-time and DVS for Uniprocessor and Multiprocessor Embedded Systems,"Energy-saving is extremely important in real-time embedded systems. Dynamic voltage scaling (DVS) is one of the prime techniques used to achieve energy-saving. Due to the uncertainties in execution times of some tasks of systems, this paper models each varied execution time as a random variable. By using probabilistic approach, we propose two optimal algorithms, one for uniprocessor and one for multiprocessor to explore soft real-time embedded systems and avoid over-designing them. Our goal is to minimize the expected total energy consumption while satisfying the timing constraint with a guaranteed confidence probability. The solutions can be applied to both hard and soft real-time systems. The experimental results show that our approach achieves significant energy-saving than previous work","Real time systems,
Voltage control,
Embedded system,
Timing,
Energy consumption,
Dynamic voltage scaling,
Hardware,
Streaming media,
Application software,
Flow graphs"
Using machine learning to predict learner emotional state from brainwaves,Intelligent Tutoring Systems (ITS) learner model has progressively evolved. Initially composed of a cognitive module it was extended with a psychological module and an emotional module. The learner model still remains non-exhaustive. Methods of data collection on the cognitive and emotional state of the learner often lack precision and objectivity. In this paper we introduce an emomental agent. It interacts with an ITS to communicate the emotional state of the learner based upon his mental state. The mental state is obtained from the learner's brainwaves. The agent learns to predict the learner's emotions by using machine learning techniques.,"Machine learning,
Psychology,
Brain computer interfaces,
Frequency,
Laboratories,
Computer science,
Intelligent systems,
Learning systems,
Educational activities,
Brain modeling"
Can Immersive Virtual Humans Teach Social Conversational Protocols?,"We investigated the effects of using immersive virtual humans to teach users social conversational verbal and non-verbal protocols in south Indian culture. The study was conducted using a between-subjects experimental design, and compared instruction and interactive feedback from immersive virtual humans against instruction based on a written study guide with illustrations of the social protocols. Participants were then tested on how well they learned the social conversational protocols by exercising the social conventions in front of videos of real people. The results of our study suggest that participants who trained with the virtual humans performed significantly better than the participants who studied from literature.",
Experiments in robotic boat localization,"We are motivated by the prospect of automating microbial observing systems. To this end we have designed and built a robotic boat as part of a sensor network for monitoring aquatic environments. In this paper, we describe a dynamic model of the boat, an algorithm for estimating its location by integrating various sensor inputs, a controller for waypoint following and extensive field experiments (over 10 km aggregate) to validate each of these. We test the localization accuracy in different sensing regimes as a prelude to accommodating sensing failures.","Boats,
Sea surface,
Ocean temperature,
Spatial resolution,
Robot sensing systems,
Aggregates,
Sampling methods,
Intelligent robots,
USA Councils,
Robotics and automation"
Route Planning based on Floyd Algorithm for Intelligence Transportation System,"A novel path planning approach based on Floyd algorithm for intelligence transportation system (ITS) is presented in this paper. For the route net can be converted into a topology, the optimal path in the route net of transportation system can be treated as the shortest path in a topology as well. So Floyd algorithm, which solves the shortest path in topology perfectly, is developed to solve the optimal route. And the globally optimal solution for ITS is obtained.","Intelligent transportation systems,
Roads,
Topology,
Vehicles,
Navigation,
Path planning,
Cities and towns,
Costs,
Technology planning,
Sun"
Ontology-based context-aware middleware for smart spaces,"Context-awareness enhances human-centric, intelligent behavior in a smart environment; however, context-awareness is not widely used due to the lack of effective infrastructure to support context-aware applications. This paper presents an agent-based middleware for providing context-aware services for smart spaces to afford effective support for context acquisition, representation, interpretation, and utilization to applications. The middleware uses a formal context model, which combines first order probabilistic logic (FOPL) and web ontology language (OWL) ontologies, to provide a common understanding of contextual information to facilitate context modeling and reasoning about imperfect and ambiguous contextual information and to enable context knowledge sharing and reuse. A context inference mechanism based on an extended Bayesian network approach is used to enable automated reactive and deductive reasoning. The middleware is used in a case study in a smart classroom, and performance evaluation result shows that the context reasoning algorithm is good for non-time-critical applications and that the complexity is highly sensitive to the size of the context dataset.","Context,
Ontologies,
Context modeling,
Middleware,
Sensors,
Probabilistic logic,
Unified modeling language"
Construction of Simplified Boundary Surfaces from Serial-sectioned Metal Micrographs,We present a method for extracting boundary surfaces from segmented cross-section image data. We use a constrained Potts model to interpolate an arbitrary number of region boundaries between segmented images. This produces a segmented volume from which we extract a triangulated boundary surface using well-known marching tetrahedra methods. This surface contains staircase-like artifacts and an abundance of unnecessary triangles. We describe an approach that addresses these problems with a voxel-accurate simplification algorithm that reduces surface complexity by an order of magnitude. Our boundary interpolation and simplification methods are novel contributions to the study of surface extraction from segmented cross-sections. We have applied our method to construct polycrystal grain boundary surfaces from micrographs of a sample of the metal tantalum.,"Surface morphology,
Grain boundaries,
Image segmentation,
Data mining,
Data visualization,
Crystalline materials,
Computer graphics,
Application software,
Materials science and technology,
Data analysis"
The Cost of Punctuality,"In an influential paper titled ""The benefits of relaxing punctuality"" [2], Alur, Feder, and Henzinger introduced Metric Interval Temporal Logic (MITL) as a fragment of the real-time logic metric temporal logic (MTL) in which exact or punctual timing constraints are banned. Their main result showed that model checking and satisfiability for MITL are both EXPSPACE-Complete. Until recently, it was widely believed that admitting even the simplest punctual specifications in any linear-time temporal logic would automatically lead to undecidability. Although this was recently disproved, until now no punctual fragment of MTL was known to have even primitive recursive complexity (with certain decidable fragments having provably non-primitive recursive complexity). In this paper we identify a ""co-flat' subset of MTL that is capable of expressing a large class of punctual specifications and for which model checking (although not satisfiability) has no complexity cost over MITL. Our logic is moreover qualitatively different from MITL in that it can express properties that are not timed-regular. Correspondingly, our decision procedures do not involve translating formulas into finite-state automata, but rather into certain kinds of reversal-bounded Turing machines. Using this translation we show that the model checking problem for our logic is EXPSPACE-Complete.","Costs,
Automata,
Timing,
Turing machines,
Automatic logic units,
Real time systems,
Time factors,
Computer science"
Accuracy of Spread Spectrum Techniques for Ultrasonic Indoor Location,This paper presents an assessment of the accuracy of impulsive and spread spectrum based algorithms for indoor ultrasonic location. Ultrasonic location systems have been proposed for pervasive computing applications. Previous systems have focused on the use of impulsive and direct sequence spread spectrum (DSSS) signalling. The use of Frequency Hopped Spread Spectrum (FHSS) signalling has not been previously studied for ultrasonic location. It is shown herein that FHSS outperforms DSSS and impulsive signalling under conditions of noise and reverberation. The accuracy of location for FHSS is shown to be twice that of DSSS under typical conditions. FHSS also provides opportunities for simplified transducer construction.,"Spread spectrum communication,
Radio frequency,
Ultrasonic transducers,
Pervasive computing,
Reverberation,
Frequency synchronization,
Random sequences,
Working environment noise,
Delay estimation,
Computer science"
Large-scale maximum likelihood-based phylogenetic analysis on the IBM BlueGene/L,"Phylogenetic inference is a grand challenge in Bioinformatics due to immense computational requirements. The increasing popularity of multi-gene alignments in biological studies, which typically provide a stable topological signal due to a more favorable ratio of the number of base pairs to the number of sequences, coupled with rapid accumulation of sequence data in general, poses new challenges for high performance computing. In this paper, we demonstrate how state-of-the-art Maximum Likelihood (ML) programs can be efficiently scaled to the IBM BlueGene/L (BG/L) architecture, by porting RAxML, which is currently among the fastest and most accurate programs for phylogenetic inference under the ML criterion. We simultaneously exploit coarse-grained and fine-grained parallelism that is inherent in every ML-based biological analysis. Performance is assessed using datasets consisting of 212 sequences and 566,470 base pairs, and 2,182 sequences and 51,089 base pairs, respectively. To the best of our knowledge, these are the largest datasets analyzed under ML to date. The capability to analyze such datasets will help to address novel biological questions via phylogenetic analyses. Our experimental results indicate that the fine-grained parallelization scales well up to 1, 024 processors. Moreover, a larger number of processors can be efficiently exploited by a combination of coarse-grained and fine-grained parallelism. Finally, we demonstrate that our parallelization scales equally well on an AMD Opteron cluster with a less favorable network latency to processor speed ratio. We recorded super-linear speedups in several cases due to increased cache efficiency.","Large-scale systems,
Phylogeny,
Prefetching,
Drain avalanche hot carrier injection,
History,
Delay,
Government,
Bridges,
Performance gain,
Cache memory"
A Distributed Trust Establishment Scheme for Mobile Ad Hoc Networks,"Wireless ad hoc networks have generated much interest, both in research literature and the telecommunication industry. The attractiveness of these networks lies in the fact that unlike other wireless networks, ad hoc networks are self-organized: the hosts constituting the networks can communicate with each other without reliance on centralized or specialized entities such as base stations. As these networks find more application, the need for adequate security mechanism is increasingly becoming important. Trust establishment and management are essential for any security framework of these networks. In this paper, we present a trust establishment scheme for ad hoc networks based on distributed trust model. A trust initiator is introduced only in the system-bootstrapping phase to initiate the protocol. A fully self-organized trust establishment approach is then adopted to handle the dynamic topology of the network and the membership changes of the nodes, while ensuring trust establishment among the nodes with shorter trust chains and very high probability. The simulation results show that our scheme is highly robust and scalable in the dynamic environment of ad hoc networks","Mobile ad hoc networks,
Peer to peer computing,
Ad hoc networks,
Computer science,
Robustness,
Computer network management,
Technology management,
Telecommunication network management,
Engineering management,
Communication industry"
"StarBED2: Large-scale, Realistic and Real-time Testbed for Ubiquitous Networks","Nowadays many new technologies are being developed and introduced for Internet, home networks, and sensor networks. The new technologies must be evaluated in detail before deployment. However the above mentioned networks have a large number of nodes and a complicated topology. Therefore it is difficult to analyze such networks using typical network simulators. Accordingly testbeds for these networks must be able to perform accurately emulation of large-scale networks with a complex topology. In order to implement a testbed that satisfies these requirements, we developed a large-scale, realistic and real-time network testbed, StarBED, using hundreds of PCs, and switched networks. We are now implementing StarBED2, which expands StarBED so as to be suitable for emulating ubiquitous networks by introducing several new concepts. In this paper we describe first the present StarBED, its design concept, overall architecture, implemented functionalities, and some of the experiments we performed. Then we introduce StarBED2, its design policy, architecture, and additional components.","Large-scale systems,
Testing,
IP networks,
Network topology,
Home automation,
Emulation,
Computer networks,
Communications technology,
Information science,
Computer bugs"
Using Topology Aggregation for Efficient Shared Segment Protection Solutions in Multi-Domain Networks,"The dynamic routing problem for Overlapping Segment Shared Protection (OSSP) in multi-domain networks has not received a lot of interest so far as it is more complex than in single-domain networks. Difficulties lie in the lack of complete and global knowledge about network topologies and bandwidth allocation whereas this knowledge is easily available in single-domain networks. We propose a two-step routing approach for the OSSP based on a topology aggregation scheme and link cost estimation: an inter-domain step and an intra-domain step. We propose two different heuristics, GROS and DYPOS for the inter-domain step, and a ""Blocking-go-back"" strategy in order to reduce the blocking rate in the intra-domain step. We compare the performance of the two heuristics against an optimal single-domain approach. We show that both heuristics lead to resource efficient solutions that are not far from the optimal ones. Moreover, both heuristics require relatively small computational efforts and are scalable for multi-domain networks.","Network topology,
Protection,
Bandwidth,
Routing,
Channel allocation,
Costs,
Computer networks,
Computer science,
Operations research,
Information systems"
Word Topical Mixture Models for Dynamic Language Model Adaptation,"This paper considers dynamic language model adaptation for Mandarin broadcast news recognition. A word topical mixture model (TMM) is proposed to explore the co-occurrence relationship between words, as well as the long-span latent topical information, for language model adaptation. The search history is modeled as a composite word TMM model for predicting the decoded word. The underlying characteristics and different kinds of model structures were extensively investigated, while the performance of word TMM was analyzed and verified by comparison with the conventional probabilistic latent semantic analysis-based language model (PLSALM) and trigger-based language model (TBLM) adaptation approaches. The large vocabulary continuous speech recognition (LVCSR) experiments were conducted on the Mandarin broadcast news collected in Taiwan. Very promising results in perplexity as well as character error rate reductions were initially obtained.","Adaptation model,
Natural languages,
Broadcasting,
Predictive models,
Performance analysis,
History,
Decoding,
Vocabulary,
Speech recognition,
Error analysis"
Bounded-Collision Memory-Mapping Schemes for Data Structures with Applications to Parallel Memories,"Techniques are developed for mapping structured data to an ensemble of parallel memory modules in a way that limits the number of conflicts, i.e., simultaneous accesses by distinct processors to the same memory module. The techniques determine, for any given conflict tolerance c, the smallest ensemble that allows one to store any n-node data structure ""of type X"" in such a way that no more than c nodes of a structure are stored on the same module. This goal is achieved by determining the smallest c-perfect universal graphs for data structures ""of type X."" Such a graph is the smallest graph that contains a homomorphic image of each n-node structure ""of type X"" with each node of the image holding < c nodes of the structure. In the current paper, ""type X"" refers to rooted binary trees and three array-like structures: chaotic arrays, ragged arrays, and rectangular arrays. For each of these families of data structures, the number of memory modules needed to achieve conflict tolerance c is determined to within constant factors.","Data structures,
Simultaneous localization and mapping,
Binary trees,
Chaos,
Application software,
Parallel architectures,
Labeling,
Multiprocessing systems,
Algorithm design and analysis,
Delay"
An Effective Guidance Strategy for Abstraction-Guided Simulation,"Despite major advances in formal verification, simulation continues to be the dominant workhorse for functional verification. Abstraction-guided simulation has long been a promising framework for leveraging the power of formal techniques to help simulation reach difficult target states (assertion violations or coverage targets): model checking a smaller, abstracted version of the design avoids complexity blow-up, yet computes approximate distances from any state of the actual design to the target; these approximate distances are used during random simulation to guide the simulator. Unfortunately, the performance of previous work has been unreliable-sometimes great, sometimes poor. The problem is the guidance strategy. Because the abstract distances are approximate, a greedy strategy will get stuck in local optima. Previous works expanded the search horizon to try to avoid dead-ends. We explore such heuristics and find that they tend to perform poorly, adding too much search overhead for limited ability to escape dead-ends. Based on these experiments, we propose a new guidance strategy, which pursues a more global search and is better able to avoid getting stuck. Experiments show that our new guidance strategy is highly effective in most cases that are hard for random simulation and beyond the capacity of formal verification.","Computational modeling,
Concrete,
Formal verification,
Performance analysis,
State-space methods,
Computer simulation,
Analytical models,
Permission,
Computer science,
Logic design"
New anatomical-prior-based image reconstruction method for PET/SPECT,"We propose a new anatomical-prior-based reconstruction method for PET/SPECT. The method first creates a template image simulating a PET/SPECT image from an anatomical image, and incorporates a distance between the template image and the reconstructed image into the cost function for image reconstruction. The most significant feature of the proposed method is that the distance between the template image and the reconstructed image is evaluated by using L1/L0 norm distance functions. We demonstrate that the use of L1/L0 norm distances possesses an incredibly strong power in improving the reconstructed image.","Image reconstruction,
Positron emission tomography,
Cost function,
Computed tomography,
Magnetic resonance imaging,
Smoothing methods,
Reconstruction algorithms,
Iterative methods,
Nuclear and plasma sciences,
Brain modeling"
Living with a Tabletop: Analysis and Observations of Long Term Office Use of a Multi-Touch Table,"Multi-touch tabletops have been the focus of significant recent study but, to date, few devices have moved from prototype to installed use. In this paper, we present observation and analysis of a subject who has used a direct-touch tabletop as his primary computing environment for the past 13 months, driving all manner of applications in a standard MS Windows environment. We present the results of three research instruments: a structured interview with the user, an analysis of touch and click locations when operating in desktop and tabletop modes over several days, and linguistic analysis of email composition over several months. From the product of these instruments we then report on several open avenues for research, including physical parameters, hardware limitations, touch vs. click in the WIMP, and text entry techniques.","Instruments,
Keyboards,
Mice,
Performance analysis,
Portable computers,
Conferences,
Computer science,
Prototypes,
Hardware,
Displays"
On Coercion-Resistant Electronic Elections with Linear Work,"Remote electronic voting over the Internet is a promising concept to afford convenience to voters and to increase election turnouts. However, before employing electronic voting systems in regular elections, problems such as coercion and vote selling have to be solved. Juels, Catalano and Jakobsson introduced a strong security requirement that deals with theses concerns. Coercion resistance improves on the former security notion of receipt freeness by taking additional real-life threats into account. In this paper, we present a coercion-resistant election scheme with a linear work factor. The scheme is based on the previous proposal of Juels et al., which exhibited a quadratic work factor, and employs Smith's idea to achieve a speedup to linear work. It, however, overcomes the drawbacks of these preceding solutions. We also present an evaluation of the scheme and identify the drawbacks and the real world aspects related to the scheme","Nominations and elections,
Electronic voting,
Security,
Cryptography,
Internet,
Proposals,
Counting circuits,
Computer science,
Electronic voting systems,
Casting"
ANDES: an Anomaly Detection System for Wireless Sensor Networks,"In this paper, we propose ANDES, a framework for detecting and finding the root causes of anomalies in operational wireless sensor networks (WSNs). The key novelty of ANDES is that it correlates information from two sources: one in the data plane as a result of regular data collection in WSNs, the other in the management plane implemented via a separate routing protocol, making it resilient to routing anomaly in the data plane. Evaluation using a 32-node sensor testbed shows that ANDES is effective in detecting fail-stop failures and most routing anomalies with negligible computing and storage overhead.","Wireless sensor networks,
Testing,
Computer science,
Hardware,
Fault detection,
Routing protocols,
Application software,
Embedded software,
Computer bugs,
Telecommunication traffic"
Orthogonal Neighborhood Preserving Embedding for Face Recognition,"In this paper, we propose a new algorithm called Orthogonal Neighborhood Preserving Embedding (ONPE) for face recognition. ONPE can preserve local geometry information and is based on the local linearity assumption that each data point and its k nearest neighbors lie on a linear manifold locally embedded in the image space. ONPE is based on Neighborhood Preserving Embedding (NPE), but overcomes the metric distortion problem of NPE, while metric distortion usually leads to performance degradation. Besides, we propose a classification method (ONPC) based on the ONPE, which use local label propagation method in the reduced space for face recognition. ONPC is based on the natural assumption that the local neighborhood information is also preserved in reduced space, and the label of a data point can be obtained in the reduced space by the labels of its neighbors. Experimental results on two face databases demonstrate the effectiveness of our proposed method.","Face recognition,
Principal component analysis,
Nearest neighbor searches,
Linear discriminant analysis,
Information geometry,
Eigenvalues and eigenfunctions,
Nonlinear distortion,
Computer science,
Automation,
Extraterrestrial measurements"
SpiralView: Towards Security Policies Assessment through Visual Correlation of Network Resources with Evolution of Alarms,"This article presents SpiralView, a visualization tool for helping system administrators to assess network policies. The tool is meant to be a complementary support to the routine activity of network monitoring, enabling a retrospective view on the alarms generated during and extended period of time. The tool permits to reason about how alarms distribute over time and how they correlate with network resources (e.g., users, IPs, applications, etc.), supporting the analysts in understanding how the network evolves and thus in devising new security policies for the future. The spiral visualization plots alarms in time, and, coupled with interactive bar charts and a users/applications graph view, is used to present network data and perform queries. The user is able to segment the data in meaningful subsets, zoom on specific related information, and inspect for relationships between alarms, users, and applications. In designing the visualizations and their interaction, and through tests with security experts, several ameliorations over the standard techniques have been provided.","Data visualization,
Intrusion detection,
Monitoring,
Data security,
Information security,
Spirals,
Computer security,
Telecommunication traffic,
Engines,
Computer interfaces"
Coevolving Strategies for General Game Playing,"The General Game Playing Competition (Genesereth et al., 2005) poses a unique challenge for artificial intelligence. To be successful, a player must learn to play well in a limited number of example games encoded in first-order logic and then generalize its game play to previously unseen games with entirely different rules. Because good opponents are usually not available, learning algorithms must come up with plausible opponent strategies in order to benchmark performance. One approach to simultaneously learning all player strategies is coevolution. This paper presents a coevolutionary approach using neuroevolution of augmenting topologies to evolve populations of game state evaluators. This approach is tested on a sample of games from the General Game Playing Competition and shown to be effective: It allows the algorithm designer to minimize the amount of domain knowledge built into the system, which leads to more general game play and allows modeling opponent strategies efficiently. Furthermore, the general game playing domain proves to be a powerful tool for developing and testing coevolutionary methods","Artificial intelligence,
Artificial neural networks,
Logic,
Algorithm design and analysis,
Computational intelligence,
Learning,
Benchmark testing,
Topology,
System testing,
Power system modeling"
Volumetric Quantification of Atherosclerotic Plaque in CT Considering Partial Volume Effect,"Coronary artery calcification (CAC) is quantified based on a computed tomography (CT) scan image. A calcified region is identified. Modified expectation maximization (MEM) of a statistical model for the calcified and background material is used to estimate the partial calcium content of the voxels. The algorithm limits the region over which MEM is performed. By using MEM, the statistical properties of the model are iteratively updated based on the calculated resultant calcium distribution from the previous iteration. The estimated statistical properties are used to generate a map of the partial calcium content in the calcified region. The volume of calcium in the calcified region is determined based on the map. The experimental results on a cardiac phantom, scanned 90 times using 15 different protocols, demonstrate that the proposed method is less sensitive to partial volume effect and noise, with average error of 9.5% (standard deviation (SD) of 5-7 mm3) compared with 67% (SD of 3-20 mm3) for conventional techniques. The high reproducibility of the proposed method for 35 patients, scanned twice using the same protocol at a minimum interval of 10 min, shows that the method provides 2-3 times lower interscan variation than conventional techniques","Computed tomography,
Calcium,
Arteries,
Coronary arteriosclerosis,
Reproducibility of results,
Protocols,
Atherosclerosis,
Biomedical imaging,
Programmable control,
Iterative algorithms"
Binary differential evolution strategies,"Differential evolution has shown to be a very powerful, yet simple, population-based optimization approach. The nature of its reproduction operator limits its application to continuous-valued search spaces. However, a simple discretization procedure can be used to convert floating-point solution vectors into discrete-valued vectors. This paper considers three approaches in which differential evolution can be used to solve problems with binary-valued parameters. The first approach is based on a homomorphous mapping, while the second approach interprets the floating-point solution vector as a vector of probabilities, used to decide on the appropriate binary value. The third approach normalizes solution vectors and then discretize these normalized vectors to form a bitstring. Empirical results are provided to illustrate the efficiency of both methods in comparison with particle swarm optimizers.","Optimization methods,
Genetic mutations,
Particle swarm optimization,
Probability density function,
Arithmetic,
Biological cells,
Difference equations,
Differential equations,
Stochastic processes,
Discrete transforms"
Traffic Trace Artifacts due to Monitoring Via Port Mirroring,"Port-mirroring techniques are supported by many of today's medium and high-end Ethernet switches. The ubiquity and low-cost of port mirroring has made it a popular method for collecting packet traces. Despite its wide-spread use little work has been reported on the impacts of this monitoring method upon the measured network traffic. In particular, we focus upon each of delay and jitter (tinting difference), packet-reordering, and packet-loss statistics. We compare the port-mirroring method with inserting a passive TAP (test access point), such as a fibre splitter, into a monitored link. Despite a passive TAP being transparent to monitored traffic, port-mirroring popularity arises from its limited set-up disruption, and (potentially) easier management This paper documents experimental comparison of traffic using the passive TAP and port-mirroring functionality, and shows that port-mirroring will introduce significant changes to the inter-packet timing, packet-reordering, and packet-loss - even at very low levels of utilisation.","Switches,
Telecommunication traffic,
Timing,
Statistics,
Testing,
Computerized monitoring,
Computer science,
Ethernet networks,
Delay,
Jitter"
Experience Management Wikis for Reflective Practice in Software Capstone Projects,"Software engineering curriculum guidelines state that students should practice methods, techniques, and tools. A capstone project is one possibility to address this aim. A capstone project helps the students to increase their problem solving competencies, improve their social skills (e.g., communication skills), and gather practical experience. A crux of such projects is that students perform ldquoreflectiverdquo practice in order to learn from their experiences. The authors believe that experience gathering and reuse are effective techniques to stimulate reflective activities. An adapted free- and open-source Wiki-based system called software organization platform (SOP) is used to support students in managing their observations and experiences. The system can be used for experience exchange within the team and for experience reuse in forthcoming projects. The results of a case study show that standard Wiki functions improve communication and information sharing by means of explicit observation and experience documentation. A total of 183 documented observations and experiences at the end of the project provide a measure for the amount of reflection students have had during the capstone project. Still, the advantages of using Wikis will decrease when no technical adaptations of the Wiki to the learning objectives and to the software engineering tasks are made. Limitations of the case study, future evaluation steps, and planned developments of SOP will be provided in this paper.","Open source software,
Software engineering,
Knowledge based systems"
Design Considerations for Collaborative Visual Analytics,"Information visualization leverages the human visual system to support the process of sensemaking, in which information is collected, organized, and analyzed to generate knowledge and inform action. Though most research to date assumes a single-user focus on perceptual and cognitive processes, in practice, sensemaking is often a social process involving parallelization of effort, discussion, and consensus building. This suggests that to fully support sensemaking, interactive visualization should also support social interaction. However, the most appropriate collaboration mechanisms for supporting this interaction are not immediately clear. In this article, we present design considerations for asynchronous collaboration in visual analysis environments, highlighting issues of work parallelization, communication, and social organization. These considerations provide a guide for the design and evaluation of collaborative visualization systems.","Collaboration,
Visual analytics,
Collaborative work,
Data visualization,
Information analysis,
Decision making,
Humans,
Visual system,
Collaborative software,
Displays"
Solving Large Scale Binary Quadratic Problems: Spectral Methods vs. Semidefinite Programming,"In this paper we introduce two new methods for solving binary quadratic problems. While spectral relaxation methods have been the workhorse subroutine for a wide variety of computer vision problems - segmentation, clustering, image restoration to name a few - it has recently been challenged by semidefinite programming (SDP) relaxations. In fact, it can be shown that SDP relaxations produce better lower bounds than spectral relaxations on binary problems with a quadratic objective function. On the other hand, the computational complexity for SDP increases rapidly as the number of decision variables grows making them inapplicable to large scale problems. Our methods combine the merits of both spectral and SDP relaxations -better (lower) bounds than traditional spectral methods and considerably faster execution times than SDP. The first method is based on spectral subgradients and can be applied to large scale SDPs with binary decision variables and the second one is based on the trust region problem. Both algorithms have been applied to several large scale vision problems with good performance.","Large-scale systems,
Computer vision,
Relaxation methods,
Image segmentation,
Image restoration,
Mathematical programming,
Quadratic programming,
Algorithms,
Computational complexity,
Optimization methods"
An Adaptive Strategy for Resource Allocation Modeled as Minority Game,"In a computer system, different agents need different resources to complete their tasks. The objective of resource allocation is to allocate the resources to the agents so that they can complete their tasks and at the same time make good utilization of resources. In this paper, we model the resource allocation problem as a minority game. In this way, the system has the advantages of a distributed system but with reduced overhead. Also, this approach is autonomous and adaptive. We design an adaptive strategy for agents to play in the game model. We perform simulations of a single-choice model, a multi-choice model in which agents have (and do not have) preferences over the resources. Simulations show that agents with the proposed adaptive strategy are able to make more right decisions and better resource utilization than previous work.","Resource management,
Computer science,
Protocols,
Game theory,
Utility theory,
Computational modeling,
Decision making,
Humans,
Psychology"
A High-Performance Hardwired CABAC Decoder,"We present a high-performance hardwired context-based adaptive binary arithmetic decoder (CABAD) for H.264/AVC. Based on an analysis of decoding time for different types of syntax elements, we propose three parallel processing techniques. Our decoder takes 309 clock cycles to decode a typical I-type macroblock. It needs to run at only 45 MHz for 1080HD application. Therefore, our architecture is suitable for low power mobile applications.",
Deploying a Heterogeneous Wireless Sensor Network,"Placing few heterogeneous nodes in wireless sensor network is an effective way to increase network lifetime and reliability. In this paper, we address the deployment problem of heterogeneous wireless sensor networks. We present an algorithm to decide how many and where heterogeneous nodes should be deployed in the wireless sensor network The core algorithm, based on the locations of all sensor nodes, can optimize placement of heterogeneous nodes in an arbitrary sensor network. Finally, the evaluation results of this algorithm are shown in this paper.","Wireless sensor networks,
Computer science,
Costs,
Filtering,
Batteries,
Microprocessors,
Ethernet networks,
Telecommunication network reliability,
Road transportation,
Monitoring"
Implementation of Distributed Loop Scheduling Schemes on the TeraGrid,"Grid computing can be used for high performance computations. However, a serious difficulty in concurrent programming of such heterogeneous systems is how to deal with scheduling and load balancing of such systems which may consist of heterogeneous computers on different sites. Distributed scheduling schemes suitable for parallel loops with independent iterations on heterogeneous computer clusters have been proposed and analyzed in the past. In this article, we implement the previous schemes in MPICH-G2 and MPIg on the TeraGrid. We present performance results for three loop scheduling schemes on single and multi-site TeraGrid clusters.","Grid computing,
Distributed computing,
Application software,
Protocols,
Processor scheduling,
Bandwidth,
Libraries,
Delay,
Computer science,
Laboratories"
Designing your Next Empirical Study on Program Comprehension,"The field of program comprehension is characterized by both the continuing development of new tools and techniques and the adaptation of existing techniques to address program comprehension needs for new software development and maintenance scenarios. The adoption of these techniques and tools in industry requires proper experimentation to assess the advantages and disadvantages of each technique or tool and to let the practitioners choose the most suitable approach for a specific problem. The objective of this working session is to encourage researchers and practitioners working in the area of program comprehension to join forces to design and carry out studies related to program comprehension, including observational studies, controlled experiments, case studies, surveys, and contests, and to develop standards for describing and carrying out such studies in a way that facilitates replication of data and aggregation of the results of related studies.","Software maintenance,
Computer science,
Software tools,
Programming,
Force control,
Standards development,
Software engineering,
Documentation,
Collaboration,
Best practices"
Layered Approach to Intrinsic Evolvable Hardware using Direct Bitstream Manipulation of Virtex II Pro Devices,"An integrated platform for fast genetic operators is presented to support intrinsic evolution on Xilinx Virtex II pro field programmable gate arrays (FPGAs). Dynamic bitstream compilation is achieved by directly manipulating the bitstream using a layered design. Experimental results on a case study have shown that a full design as well as a full repair is achievable using this platform with an average time of 0.4 microseconds to perform the genetic mutation, 0.7 microseconds to perform the genetic crossover, and 5.6 milliseconds for one input pattern intrinsic evaluation. This represents a performance advantage of three orders of magnitude over JBITS and more than seven orders of magnitude over the Xilinx design tool driven flow for realizing intrinsic genetic operators on a Virtex II Pro device.","Hardware,
Field programmable gate arrays,
Application software,
Runtime,
Table lookup,
Performance evaluation,
Genetic mutations,
Fabrics,
Circuit faults,
Computer science"
Efficient Indexing For Articulation Invariant Shape Matching And Retrieval,"Most shape matching methods are either fast but too simplistic to give the desired performance or promising as far as performance is concerned but computationally demanding. In this paper, we present a very simple and efficient approach that not only performs almost as good as many state-of-the-art techniques but also scales up to large databases. In the proposed approach, each shape is indexed based on a variety of simple and easily computable features which are invariant to articulations and rigid transformations. The features characterize pairwise geometric relationships between interest points on the shape, thereby providing robustness to the approach. Shapes are retrieved using an efficient scheme which does not involve costly operations like shape-wise alignment or establishing correspondences. Even for a moderate size database of 1000 shapes, the retrieval process is several times faster than most techniques with similar performance. Extensive experimental results are presented to illustrate the advantages of our approach as compared to the best in the field.","Indexing,
Shape,
Robustness,
Spatial databases,
Testing,
Automation,
Computer science,
Educational institutions,
Information retrieval,
Web pages"
Orchestration of Network-Wide Active Measurements for Supporting Distributed Computing Applications,"Recent computing applications such as videoconferencing and grid computing run their tasks on distributed computing resources connected through networks. For such applications, knowledge of the network status such as delay, jitter, and available bandwidth can help them select proper network resources to meet the Quality-of-Service (QoS) requirements. Also, the applications can dynamically change the resource selection if the current selection is found to experience poor performance. For such purposes, Internet Service Providers (ISPs) have started to instrument their networks with Network Measurement Infrastructures (NMIs) that run active measurement tasks periodically and/or on demand. However, one problem that most network engineers have overlooked is the measurement conflict problem, which happens when multiple active measurement tasks inject probing packets into the same network segment at the same time, resulting in misleading reports of network performance due to their combined effects. This paper proposes enhanced Earliest Deadline First (EDF) algorithms that allow ""Concurrent Executions"" to orchestrate offline/online measurement jobs in a conflict-free manner. The simulation study shows that our measurement scheduling mechanism can improve the schedulable utilization of offline measurement tasks up to 300 percent and the response time of on-demand jobs up to 50 percent. Further, we implement and deploy our scheduling mechanism in a real working NMI for monitoring the Internet2 Abilene network. As a case study, we show the utility of our algorithms in the widely used Network Weather Service (NWS).","Time measurement,
Scheduling,
Bandwidth,
Topology,
Grid computing,
Distributed computing,
Real-time systems"
Evolutionary algorithms and the Vertex Cover problem,"Experimental results have suggested that evolutionary algorithms may produce higher quality solutions for instances of vertex cover than a very well known approximation algorithm for this NP-complete problem. A theoretical analysis of the expected runtime of the (1+1)-EA on a well studied instance class confirms such a conjecture for the considered class. Furthermore, a class for which the (1+1)-EA takes exponential optimization time is examined. Nevertheless, given polynomial time, the evolutionary algorithm still produces a better solution than the approximation algorithm. Recently, the existence of an instance class has been proved for which the (1+1)-EA produces poor approximate solutions, given polynomial time. Here it is pointed out that, by using multiple runs, the (1+1)-EA finds the optimal cover of each instance of the considered graph class in polynomial time.","Evolutionary computation,
Polynomials,
Runtime,
Approximation algorithms,
Helium,
Algorithm design and analysis,
NP-complete problem,
Design optimization,
Computational intelligence,
Application software"
99% (Biological) Inspiration...,"Greater understanding of biology in modern times has enabled significant breakthroughs in improving healthcare, quality of life, and eliminating many diseases and congenital illnesses. Simultaneously there is a move towards emulating nature and copying many of the wonders uncovered in biology, resulting in ""biologically inspired"" systems. Significant results have been reported in a wide range of areas, with systems inspired by nature enabling exploration, communication, and advances that were never dreamed possible just a few years ago. We warn, that as in many other fields of endeavor, we should be inspired by nature and biology, not engage in mimicry. We describe some results of biological inspiration that augur promise in terms of improving the safety and security of systems, and in developing self-managing systems, that we hope will ultimately lead to self-governing systems","Birds,
Systems biology,
Biology computing,
Feathers,
Muscles,
Aircraft manufacture,
Aircraft propulsion,
NASA,
Computer science,
Medical services"
A Mobile Beacon Based Approach for Sensor Network Localization,"Mobile beacon localization approaches have been proposed in the literature as an interesting alternative for centralized/decentralized approaches because of their low cost, their accuracy and their low energy consumption. However, their main drawback in a random deployment remains the lack of a well defined mobile beacon trajectory that ensures localization for all the nodes with acceptable error estimations. In this paper we give a solution to the question ""What is the optimum mobile beacon trajectory and when should the beacon packets be sent?"", and then we propose a simple localization algorithm. This algorithm is based on one mobile beacon. We study three different known curves and we justify the use of the Hilbert space filling curve to be the mobile beacon trajectory. We further compare the localization error of our proposed algorithm and other existing mobile beacon based algorithms. Our results are verified by simulation on Omnet++.","Mobile computing,
Hilbert space,
Filling,
Sensor phenomena and characterization,
Temperature sensors,
Monitoring,
Costs,
Energy consumption,
Computer networks,
Computer science"
Equalized interconnects for on-chip networks: modeling and optimization framework,"This paper presents a modeling framework for fast design space exploration and optimization of equalized on-chip interconnects. The exploration is enabled by cross-layer modeling that connects the transistor and wire parameters to link performance, equalization coefficients, and architecture-friendly metrics (delay, energy-per-bit, and throughput density). Appropriate models are derived to speed-up the search by more than two orders of magnitude and make a million point design space searchable in less than two hours on a standard machine. With this approach we are able to find the best link design for target throughput, power and area constraints, thus enabling the architectural optimization of energy-efficient on-chip networks. For the same latency and throughput density, equalized interconnects optimized using the new methodology have up to 10times better energy-efficiency than optimized repeater interconnects.","Network-on-a-chip,
Throughput,
Design optimization,
Delay,
Energy efficiency,
Optimization methods,
Space exploration,
Wire,
Constraint optimization,
Repeaters"
Time Series Prediction Based on Linear Regression and SVR,"The application of SVR in the time series prediction is increasingly popular. Because some time series prediction based on SVR wasn 't very nice in the efficiency of the forecast, this article presents a new regression based on linear regression and SVR. The new regression separates time series into linear part and nonlinear part, then predicts the two parts respectively, and finally integrates the two parts to forecast. Experiments show that the new regression advances the precision of the forecasting compared to the common SVR.","Linear regression,
Economic forecasting,
Weather forecasting,
Time series analysis,
Neural networks,
Support vector machines,
Computer science,
Application software,
Fluctuations,
Additives"
Modeling Detection Metrics in Randomized Scheduling Algorithm in Wireless Sensor Networks,"In wireless sensor networks, in order to minimize energy consumption and extend network lifetime, some sensors are put in the sleep mode while the other sensor nodes are in the active mode for the sensing and communication tasks. In a randomized scheduling algorithm, a set of sensors work alternatively. In this paper, we provide an analytical model for the randomized scheduling algorithm, and derive detection delay and detection probability. Simulations are conducted to validate analytical results.","Scheduling algorithm,
Wireless sensor networks,
Intrusion detection,
Event detection,
Analytical models,
Computer science,
Delay,
USA Councils,
Computational modeling,
Communications Society"
Preliminary results in accelerating profile HMM search on FPGAs,"Comparison between biosequences and probabilistic models is an increasingly important part of modern DNA and protein sequence analysis. The large and growing number of such models in today's databases demands computational approaches to searching these databases faster, while maintaining high sensitivity to biologically meaningful similarities. This work describes an FPGA-based accelerator for comparing proteins to hidden Markov models of the type used to represent protein motifs in the popular HM-MER motif finder. Our engine combines a systolic array design with enhancements to pipeline the complex Viterbi calculation that forms the core of the comparison, and to support coarse-grained parallelism and streaming of multiple sequences within one FPGA. Performance estimates based on a functioning VHDL realisation of our design show a 190 times speedup over the same computation in optimised software on a modern general-purpose CPU.","Acceleration,
Hidden Markov models,
Field programmable gate arrays,
Biological system modeling,
Databases,
DNA,
Protein sequence,
Biology computing,
Engines,
Systolic arrays"
Probabilistic Fusion Tracking Using Mixture Kernel-Based Bayesian Filtering,"Even though sensor fusion techniques based on particle filters have been applied to object tracking, their implementations have been limited to combining measurements from multiple sensors by the simple product of individual likelihoods. Therefore, the number of observations is increased as many times as the number of sensors, and the combined observation may become unreliable through blind integration of sensor observations - especially if some sensors are too noisy and non-discriminative. We describe a methodology to model interactions between multiple sensors and to estimate the current state by using a mixture of Bayesian filters - one filter for each sensor, where each filter makes a different level of contribution to estimate the combined posterior in a reliable manner. In this framework, an adaptive particle arrangement system is constructed in which each particle is allocated to only one of the sensors for observation and a different number of samples is assigned to each sensor using prior distribution and partial observations. We apply this technique to visual tracking in logical and physical sensor fusion frameworks, and demonstrate its effectiveness through tracking results.","Bayesian methods,
Sensor fusion,
Sensor phenomena and characterization,
State estimation,
Sensor systems,
Filtering,
Particle filters,
Particle tracking,
Particle measurements,
Adaptive systems"
KeyRev: An Efficient Key Revocation Scheme for Wireless Sensor Networks,"Key management is a core mechanism to ensure the security of applications and network services in wireless sensor networks. It includes two aspects: key distribution and key revocation. Key distribution has been extensively studied in the context of sensor networks. However, key revocation has received relatively little attention. Existing key revocation schemes can be divided into two categories: centralized key revocation scheme and distributed key revocation scheme. In this paper, we first summarize the current key revocation schemes for sensor networks. Then, we propose an efficient centralized key revocation scheme, KeyRev, for wireless sensor networks. Unlike most proposed key revocation schemes focusing on removing the compromised keys, we propose to use key updating techniques to obsolesce the keys owned by the compromised sensor nodes and thus remove the nodes from the network. Our analyses show that the KeyRev scheme is secure inspite of not removing the pre-distributed key materials at compromised sensor nodes. Simulation results also indicate that the KeyRev scheme is scalable and performs very well in wireless sensor networks.","Wireless sensor networks,
Protocols,
Peer to peer computing,
Data security,
USA Councils,
Computer network management,
Computational modeling,
Communications Society,
Computer science,
Engineering management"
Modelling the Evolution of Cooperative Behavior in Ad Hoc Networks using a Game Based Model,In this paper we address the problem of cooperation and selfish behavior in ad hoc networks. We present a new game theory based model to study cooperation between nodes. This model has some similarities with the iterated prisoner's dilemma under the random pairing game. In such game randomly chosen players receive payoffs that depend on the way they behave. The network gaming model includes a simple reputation collection and trust evaluation mechanisms. In our proposition a decision whether to forward or discard a packet is determined by a strategy based on the trust level in the source node of the packet and some general information about behavior of the network. A genetic algorithm (GA) is applied to evolve strategies for the participating nodes. These strategies are targeted to maximize the throughput of the network by enforcing cooperation. Experimental results show that proposed strategy based approach successfully enforces cooperation maximizing the network throughput,
Bayesian Kernel Methods for Analysis of Functional Neuroimages,"We propose an approach to analyzing functional neuroimages in which (1) regions of neuronal activation are described by a superposition of spatial kernel functions, the parameters of which are estimated from the data and (2) the presence of activation is detected by means of a generalized likelihood ratio test (GLRT). Kernel methods have become a staple of modern machine learning. Herein, we show that these techniques show promise for neuroimage analysis. In an on-off design, we model the spatial activation pattern as a sum of an unknown number of kernel functions of unknown location, amplitude, and/or size. We employ two Bayesian methods of estimating the kernel functions. The first is a maximum a posteriori (MAP) estimation method based on a reversible-jump Markov-chain Monte-Carlo (RJMCMC) algorithm that searches for both the appropriate model complexity and parameter values. The second is a relevance vector machine (RVM), a kernel machine that is known to be effective in controlling model complexity (and thus discouraging overfitting). In each method, after estimating the activation pattern, we test for local activation using a GLRT. We evaluate the results using receiver operating characteristic (ROC) curves for simulated neuroimaging data and example results for real fMRI data. We find that, while RVM and RJMCMC both produce good results, RVM requires far less computation time, and thus appears to be the more promising of the two approaches.","Bayesian methods,
Kernel,
Neuroimaging,
Biomedical imaging,
Testing,
Biomedical engineering,
Statistical analysis,
Machine learning,
Computational modeling,
Computer science"
Modeling and Characterizing Power Variability in Multicore Architectures,"Parameter variation due to manufacturing error is an unavoidable consequence of technology scaling in future generations. The impact of random variation in physical factors such as gate length and interconnect spacing have a profound impact on not only performance of chips, but also their power behavior. While circuit-level techniques such as adaptive body-biasing can help to mitigate mal-fabricated chips, they cannot completely alleviate severe within die variations forecasted for near future designs. Despite the large impact that power variability have on future designs, there is a lack of published work that examines architectural implications of this phenomenon. In this work, we develop architecture level models that model power variability due to manufacturing error and examine its influence on multicore designs. We introduce VariPower, a tool for modeling power variability based on an microarchitectural description and floorplan of a chip. In particular, our models are based on layout level SPICE simulations and project power variability for different microarchitectural blocks using statistical analysis. Using VariPower: (1) we characterize power variability for multicore processors, (2) explore application sensitivity to power variability, and (3) examine clustering techniques that can appropriately classify groups of processors and chips that have similar variability characteristics","Multicore processing,
Fabrication,
Integrated circuit interconnections,
Microarchitecture,
Delay,
Virtual manufacturing,
Character generation,
Power generation,
Energy consumption,
Computer architecture"
Accelerating and Adapting Precomputation Threads for Effcient Prefetching,"Speculative precomputation enables effective cache prefetching for even irregular memory access behavior, by using an alternate thread on a multithreaded or multi-core architecture. This paper describes a system that constructs and runs precomputation based prefetching threads via event-driven dynamic optimization. Precomputation threads are dynamically constructed by a runtime compiler from the program's frequently executed hot traces, and are adapted to the memory behavior automatically. Both construction and execution of the prefetching threads happen in another thread, imposing little overhead on the main thread. This paper also presents several techniques to accelerate the precomputation threads, including colocation of p-threads with hot traces, dynamic stride prediction, and automatic adaptation of runahead and jumpstart distance. The adaptive prefetching achieves 42% speedup, a 17% improvement over existing p-thread prefetching schemes","Acceleration,
Yarn,
Prefetching,
Aerodynamics,
Hardware,
Delay,
Computer architecture,
Runtime,
Monitoring,
Computer science"
Low-Overhead LogGP Parameter Assessment for Modern Interconnection Networks,"Network performance measurement and prediction is very important to predict the running time of high performance computing applications. The LogP model family has been proven to be a viable tool to assess the communication performance of parallel architectures. However, non-intrusive LogP parameter assessment is still a very difficult task. We compare well known measurement methods for Log(G)P parameters and discuss their accuracy and network contention. Based on this, a new theoretically exact measurement method that does not saturate the network is derived and explained in detail. Our method only uses benchmarked values instead of computed parameters to compute other parameters to avoid propagation of first-order errors. A methodology to detect protocol changes in the underlying communication subsystem is also proposed. The applicability of our method and the protocol change detection is shown for the low-level API as well as MPI implementations of different modern high performance interconnection networks. The whole method is implemented in the tool Netgauge and it is available as open source to the public.","Multiprocessor interconnection networks,
Protocols,
Predictive models,
Switches,
Libraries,
Hardware,
Parallel processing,
Processor scheduling,
Communication switching,
Computer science"
Distributed Load Balancing Algorithm for Adaptive Channel Allocation for Cognitive Radios,"The problem of channel allocation has been extensively studied in the context of cellular networks. There is a substantial amount of work in the field of dynamic frequency assignment for WLANs and mesh networks as well. The growing interest in the cognitive radio technology has made efficient spectrum allocation a problem that is attracting a lot of attention. In this paper we present two quite simple to implement, distributed algorithms for selecting channels, in cognitive radio environment, so that the load is distributed (smoothed) over them.","Load management,
Channel allocation,
Cognitive radio,
Frequency,
Land mobile radio cellular systems,
Radio spectrum management,
Wireless networks,
Telecommunication traffic,
Multiaccess communication,
Computer science"
Cooperative Source Coding with Encoder Breakdown,"This paper provides an inner bound to the rate- distortion region of a source coding setup in which two encoders are allowed some collaboration to describe a pair of discrete memoryless sources. We further require some robustness in case one of the encoders breaks down. This is modeled by having a second decoder, observing the messages from only one of the encoders. We prove the tightness of this inner bound for two special cases. In the first, one of the sources is required to be recovered losslessly if there is no encoder breakdown. In the second, the robustness requirement is dropped and only one of the sources is to be represented. For the second case, we explicitly compute the rate-distortion region for the quadratic Gaussian and binary Hamming problems.","Source coding,
Electric breakdown,
Decoding,
Rate-distortion,
Collaboration,
Robustness,
Random variables,
Distortion measurement,
Error correction,
Relays"
MASC - .NET-Based Middleware for Adaptive Composite Web Services,"MASC (manageable and adaptive service compositions) is a policy-based middleware for monitoring of Web service compositions and their dynamic adaptation to various runtime changes. MASC policies are described in our new WS-Policy extension called WS-Policy4MASC. Compared with recent related works, MASC has several distinctive characteristics, such as coordination of adaptation on the SOAP messaging layer and the business process orchestration layer, use of both technical and business metrics for adaptation decisions, and extending the power and flexibility of the new Microsoft .NET 3.0 platform. In this paper, we focus on MASC support for adaptation to address business exceptions and manage runtime faults. For example, a sub-process (or an activity) can be added, deleted, replaced, skipped, or retried. We have been implementing a MASC proof-of-concept prototype and evaluating it on adaptation scenarios from a stock trading case study. Our performance studies of the prototype indicate that overheads introduced by MASC are acceptable.","Middleware,
Web services,
Runtime,
Australia,
Prototypes,
Quality of service,
Computer science,
Engineering management,
Computerized monitoring,
Simple object access protocol"
Trajectory Series Analysis based Event Rule Induction for Visual Surveillance,"In this paper, a generic rule induction framework based on trajectory series analysis is proposed to learn the event rules. First the trajectories acquired by a tracking system are mapped into a set of primitive events that represent some basic motion patterns of moving object. Then a minimum description length (MDL) principle based grammar induction algorithm is adopted to infer the meaningful rules from the primitive event series. Compared with previous grammar rule based work on event recognition where the rules are all defined manually, our work aims to learn the event rules automatically. Experiments in a traffic crossroad have demonstrated the effectiveness of our methods. Shown in the experimental results, most of the grammar rules obtained by our algorithm are consistent with the actual traffic events in the crossroad. Furthermore the traffic lights rule in the crossroad can also be leaned correctly with the help of eliminating the irrelevant trajectories.","Surveillance,
Hidden Markov models,
Trajectory,
Event detection,
Layout,
Stochastic processes,
Pattern analysis,
Prototypes,
Feeds,
Uncertainty"
A New SOA Data-Provenance Framework,"Due to the dynamic nature, such as runtime composition and evaluation, it is critical for an SOA system to consider its data provenance, which concerns security, reliability, and integrity of data as it is routed in the system. In a traditional software system, one can focus on the software itself to determine the security, reliability, and integrity; however, in an SOA system, one also needs to consider the origins and routes of data and their impact, i.e., data provenance. This paper first analyzes the unique natures and characteristics of data provenance in an SOA system. Then it proposes a new framework to classify data provenance and collect data in SOA systems. Finally, this paper uses an example to illustrate these concepts and techniques","Semiconductor optical amplifiers,
Service oriented architecture,
Data security,
Geographic Information Systems,
Standards publication,
Computer science,
Reliability engineering,
Runtime,
Software systems,
Access control"
Efficient lookup on unstructured topologies,"We present LMS, a protocol for efficient lookup on unstructured networks. Our protocol uses a virtual namespace without imposing specific topologies. It is more efficient than existing lookup protocols for unstructured networks, and thus is an attractive alternative for applications in which the topology cannot be structured as a Distributed Hash Table (DHT). We present analytic bounds for the worst-case performance of LMS. Through detailed simulations (with up to 100,000 nodes), we show that the actual performance on realistic topologies is significantly better. We also show in both simulations and a complete implementation (which includes over five hundred nodes) that our protocol is inherently robust against multiple node failures and can adapt its replication strategy to optimize searches according to a specific heuristic. Moreover, the simulation demonstrates the resilience of LMS to high node turnover rates, and that it can easily adapt to orders of magnitude changes in network size. The overhead incurred by LMS is small, and its performance approaches that of DHTs on networks of similar size","Peer to peer computing,
Network topology,
Protocols,
Least squares approximation,
Performance analysis,
Robustness,
Resilience,
Distributed algorithms,
Distributed computing,
Computer science"
A High-Throughput Low-Power AES Cipher for Network Applications,"We propose a full-featured high-throughput low-power AES cipher which is suitable for widespread network applications. Different modes of operation are implemented, i.e., the ECB, CBC, CTR and CCM modes. Our cipher utilizes a cost-efficient two-stage pipeline for the CCM mode by a single datapath. With the design-for-test circuitry, the maximum throughput is 4.27 Gbps using a 0.13mum CMOS technology with a 333MHz clock rate. The hardware cost is 86.2K gates with the power of 40.9mW.",
WormShield: Fast Worm Signature Generation with Distributed Fingerprint Aggregation,"Fast and accurate generation of worm signatures is essential to contain zero-day worms at the Internet scale. Recent work has shown that signature generation can be automated by analyzing the repetition of worm substrings (that is, fingerprints) and their address dispersion. However, at the early stage of a worm outbreak, individual edge networks are often short of enough worm exploits for generating accurate signatures. This paper presents both theoretical and experimental results on a collaborative worm signature generation system (WormShield) that employs distributed fingerprint filtering and aggregation over multiple edge networks. By analyzing real-life Internet traces, we discovered that fingerprints in background traffic exhibit a Zipf-like distribution. Due to this property, a distributed fingerprint filtering reduces the amount of aggregation traffic significantly. WormShield monitors utilize a new distributed aggregation tree (DAT) to compute global fingerprint statistics in a scalable and load-balanced fashion. We simulated a spectrum of scanning worms including CodeRed and Slammer by using realistic Internet configurations of about 100,000 edge networks. On average, 256 collaborative monitors generate the signature of CodeRedl-v2 135 times faster than using the same number of isolated monitors. In addition to speed gains, we observed less than 100 false signatures out of 18.7-Gbyte Internet traces, yielding a very low false-positive rate. Each monitor only generates about 0.6 kilobit per second of aggregation traffic, which is 0.003 percent of the 18 megabits per second link traffic sniffed. These results demonstrate that the WormShield system offers distinct advantages in speed gains, signature accuracy, and scalability for large-scale worm containment.",
Estimating Road Traffic Congestion using Cell Dwell Time with Simple Threshold and Fuzzy Logic Techniques,"In this paper, we studied the relationship between cell dwell time (CDT) and the degree of road traffic congestion. CDT is the duration that a mobile phone remains associated with a base station. As the mobile phone in a vehicle travels along a road and encounters different degrees of congestion, the value of CDT varies accordingly. Intutively, the higher value of CDT indicates the worse degree of congestion. This study investigated ways to estimate the degree of road traffic congestion based on measurements of CDT taken from major traffic routes in the metropolitan area of Bangkok, Thailand. We classifed measurements of CDT into three levels of traffic congestion based on duration using two techniques -simple threshold and fuzzy logic, and verified the results with human observations of traffic conditions. The results suggested that the duration of CDT can be used to estimate the degree of congestion with accuracy level upto 85%, which is comparable to a similar technique used to estimate the degree of congestion based on velocity. This suggested a potential use of CDT as traffic probe data.","Fuzzy logic,
Vehicles,
Mobile handsets,
Intelligent sensors,
Costs,
Intelligent transportation systems,
Road transportation,
Urban areas,
Probes,
Cities and towns"
Option Valuation Applied to Implementing Demand Response via Critical Peak Pricing,"The purpose of this paper is to examine the economic and technical perspectives of critical peak pricing plan as an active demand response(DR) program. To implement a good DR program, there are three perspectives to be considered: regulatory, economic, and technical perspectives. This paper will assume that the regulatory perspective of DR is determined as critical peak pricing(CPP) plan and examine the other two. The economic perspective of CPP plan is the incentive of the plan conductor, or the profit of an energy service provider(ESP). The technical perspective is a method to maximize the incentive of CPP plan, or an ESP's profit. An ESP should decide when to call critical peaks within certain constraints to maximize her profit. This is done by predicting the market prices and following a similar method as evaluating a swing option. The numerical example will show the optimal critical peak decisions.",
An efficient algorithm for time separation of events in concurrent systems,"The time separation of events (TSE) problem is that of finding the maximum and minimum separation between the times of occurrence of two events in a concurrent system. It has applications in the performance analysis, optimization and verification of concurrent digital systems. This paper introduces an efficient polynomial-time algorithm to give exact bounds on TSE's for choice-free concurrent systems, whose operational semantics obey the max-causality rule. A choice-free concurrent system is modeled as a strongly-connected marked graph, where delays on operations are modeled as bounded intervals with unspecified distributions. While previous approaches handle acyclic systems only, or else require graph unfolding until a steady-state behavior is reached, the proposed approach directly identifies and evaluates the asymptotic steady-state behavior of a cyclic system via a graph-theoretical approach. As a result, the method has significantly lower computational complexity than previously-proposed solutions. A prototype CAD tool has been developed to demonstrate the feasibility and efficacy of our method. A set of experiments have been performed on the tool as well as two existing tools, with noticeable improvement on runtime and accuracy for several examples.","Delay,
Steady-state,
Timing,
Performance analysis,
Circuits,
Polynomials,
Computational complexity,
Prototypes,
Measurement,
Convergence"
Load Balancing in the Bulk-Synchronous-Parallel Setting using Process Migrations,"The Paderborn University BSP (PUB) library is a powerful C library that supports the development of bulk synchronous parallel programs for various parallel machines. To utilize idle times on workstations for parallel computations, we implement virtual processors using processes. These processes can be migrated to other hosts, when the load of the machines changes. In this paper we describe the implementation for a Linux workstation cluster. We focus on process migration and show first benchmarking results.","Load management,
Workstations,
Libraries,
Concurrent computing,
Linux,
Kernel,
Processor scheduling,
Checkpointing,
Yarn,
Switches"
A Fast and Stable Algorithm for Obstacle-Avoiding Rectilinear Steiner Minimal Tree Construction,"In routing, finding a rectilinear Steiner minimal tree (RSMT) is a fundamental problem. Today's design often contains rectilinear obstacles, like macro cells, IP blocks, and pre-routed nets. Therefore obstacle-avoiding RSMT (OARSMT) construction becomes a very practical problem. In this paper we present a fast and stable algorithm for this problem. We use a partitioning based method and an ant colony optimization based method to construct obstacle-avoiding Steiner minimal tree (OASMT). Besides, two heuristics are proposed to do the rectilinearization and refinement to further improve wirelength. The experimental results show our algorithm achieves the best wirelength results in most of the test cases and the runtime is very small even for the larger cases each of which has both the number of terminals and the number of obstacles more than 100.","Steiner trees,
Routing,
Partitioning algorithms,
Runtime,
Ant colony optimization,
Tree graphs,
Ultra large scale integration,
Computer science,
Circuit testing,
Very large scale integration"
An Adaptive Resource Flowing Scheme amongst VMs in a VM-Based Utility Computing,"In order to optimize the using of server resources which host different services such as Web services, this paper describes an adaptive and dynamic resource flowing scheme amongst VMs in a VM-based utility computing environment called ADVM (adaptive and dynamic virtual machine). In our scheme, VMs adjust their resources (CPU and memory) adaptively to share the physical resources efficiently. ADVM provides an on-the-fly, transparent and lazy resource flowing amongst VMs over a single server. By using dynamic priority and the strategy of adjusting resources, no VM interferes with others even via contention for some resources. To demonstrate the elegance of our strategy, we present a prototype implementation of ADVM. The experimental results indicate that ADVM effectively enforce the system performance based on Xen.","Voice mail,
Resource management,
Virtual manufacturing,
Computers,
Virtual machining,
Prototypes,
System performance,
Network servers,
Costs,
Research and development"
A Comprehensive Approach to Service Adaptation,"In service-oriented computing (SOC), services are designed not just for a dedicated client, but for a family of potential clients who typically discover and compose services dynamically. For services to be generic and hence serviceable to different users in a given domain, the service variability among different clients must be analyzed and modeled into service components. Furthermore, effective methods for dynamically adapting services for different invocations and contexts must be provided. Nonetheless, the research on service variability management and service adaptation is still in early stage. In this paper, we survey representative software adaptation methods, and propose four types of service variability. Then we present practical adaptation methods for resolving the four types of service variability. The proposed adaption methods presented in this paper can be implemented in a typical Web service environment with WSDL, UDDI and BPEL.","Web services,
Computer science,
Context-aware services,
Computer applications,
Taxonomy,
Logic"
A CMOS linear-in-dB high-linearity variable-gain amplifier for UWB receivers,"This paper presents a CMOS linear-in-dB variable gain amplifier (VGA) that provides a variable gain range over 90 dB with 3 dB bandwidth greater than 400 MHz at 54 dB gain. The maximum output 1 dB compression point is 9 dBm. Maximum gain error is +/-2 dB. It consumes total 22 mW with 1.8 V supply, including control circuit. This VGA is fabricated in TSMC 0.18 um CMOS process and demonstrate the performance of the proposed dB-linear VGA.","Linearity,
Frequency response,
Circuits,
Voltage,
Degradation,
CMOS technology,
Gain control,
Differential amplifiers,
Capacitance,
Dynamic range"
Modeling of Multiple Valued Gene Regulatory Networks,"In silico modeling of Gene Regulatory Networks has gained a lot of attention recently as it gives a very powerful tool to experimental biologists to gather the knowledge gained from different biological experiments and understand the dynamics of the overall system. One of the key dynamics that is often interesting is the steady states of the networks which biologically corresponds to the cellular states. In our previous paper, we gave an efficient method called GenYsis to compute these steady states in Boolean representation of Gene Regulatory Network. It has been observed that protein may be expressed at more then two level of expression. This may result in different cellular outcomes. To address this issue, we present here a multiple-level modeling methodology that allows us to be more accurate. In this paper we extend our software GenYsis to model gene regulatory networks where each node in the network may take multiple values.","Immune system,
Biological system modeling,
Steady-state,
Gene expression,
Data structures,
Boolean functions,
Power system modeling,
Cellular networks,
Biology computing,
Computer networks"
Robust Coding Over Noisy Overcomplete Channels,"We address the problem of robust coding in which the signal information should be preserved in spite of intrinsic noise in the representation. We present a theoretical analysis for 1- and 2-D cases and characterize the optimal linear encoder and decoder in the mean-squared error sense. Our analysis allows for an arbitrary number of coding units, thus including both under- and over-complete representations, and provides insights into optimal coding strategies. In particular, we show how the form of the code adapts to the number of coding units and to different data and noise conditions in order to achieve robustness. We also present numerical solutions of robust coding for high-dimensional image data, demonstrating that these codes are substantially more robust than other linear image coding methods such as PCA, ICA, and wavelets","Noise robustness,
Image coding,
Independent component analysis,
Principal component analysis,
Noise reduction,
Decoding,
Entropy,
Cognition,
Computer science,
Redundancy"
Harmonic Currents Estimation and Compensation Method for Current Control System of IPMSM in Overmodulation Range,"For high performance and wide speed range control of an IPMSM, using the overmodulation range of an inverter is one of the effective methods. However, to use this overmodulation range with the closed loop currents control system, there are many problems occur and the main problem is caused by the harmonic currents generated from harmonic voltages in the inverter output voltages. Therefore, in the ususal control methods of an IPMSM in the overmodulation range, these problems are avoided by not using the closed loop currents control or by using the closed loop currents control with the filters in the feedback part. With these methods, the problems from harmonic currents can be solved, but the other new problems also occur too. The method of harmonic currents compensation is also proposed and this method seems to be more effective than the others. However, in this method, the proposed harmonic currents estimation method is quite difficult in practical use, because the real value of the inverter output voltages are used, then the carrier frequency components will cause an aliasing problem in the estimation part and this problem will become more serious when the carrier frequency is high. To solve the above problem, in this paper, we propose the harmonic currents estimation and compensation method based on the inverter harmonic voltages model. In our proposed method, we use only the inverter commanded voltages in the harmonic voltages estimation, then this method is more easy for the practical use. Moreover, by using the inverter harmonic voltages model, the data of both amplitude and phase angle of harmonic voltages are available, then we can estimate the harmonic currents by using only simple phasor method compared with the complex dynamic model method in the former. The experimental results are shown to confirm our proposed estimation and compensation methods.","Current control,
Voltage,
Inverters,
Power harmonic filters,
Frequency estimation,
Virtual manufacturing,
Electric current control,
Modulation,
Cities and towns,
Control systems"
Computation of Static Execute After Relation with Applications to Software Maintenance,"In this paper, we introduce static execute after (SEA) relationship among program components and present an efficient analysis algorithm. Our case studies show that SEA may approximate static slicing with perfect recall and high precision, while being much less expensive and more usable. When differentiating between explicit and hidden dependencies, our case studies also show that SEA may correlate with direct and indirect class coupling. We speculate that SEA may find applications in computation of hidden dependencies and through it in many maintenance tasks, including change propagation and regression testing.",
Integrated Debugging of Large Modular Robot Ensembles,"Creatively misquoting Thomas Hobbes, the process of software debugging is nasty, brutish, and all too long. This holds all the more true in robotics, which frequently involves concurrency, extensive nondeterminisism, event-driven components, complex state machines, and difficult platform limitations. Inspired by the challenges we have encountered while attempting to debug software on simulated ensembles of tens of thousands of modular robots, we have developed a new debugging tool particularly suited to the characteristics of highly parallel, event- and state-driven robotics software. Our state capture and introspection system also provides data that may be used in higher-level debugging tools as well. We report on the design of this promising debugging system, and on our experiences with it so far.",
Super mode propagation in low index medium,We investigate a novel waveguide geometry consisting of a high dielectric medium adjacent to a metal plane with a thin low dielectric spacer. The mechanism of operation is explained and simulation results are presented.,
Holistic Software Engineering Education Based on a Humanitarian Open Source Project,"For the past year, Trinity College has utilized Sahana, a free and open source disaster management system, as a foundation to teach software engineering. The goals of the use of the Sahana project are threefold: to provide students with a real-world software engineering experience; to introduce students to the open-source development model; and to attract a wider variety of students into computing due to the real-world and humanitarian nature of the Sahana project. This paper discusses an approach for using open source software as a foundation to teach software engineering in a Liberal Arts environment by involving students in an ongoing, real-world project from the very beginning, allowing students with a wide range of backgrounds to participate. Results of a learning survey of a small group of students who have participated in the project are presented. The paper also provides guidance to others contemplating incorporating open source projects into their software engineering courses or curriculum.","Software engineering,
Open source software,
Educational institutions,
Java,
Programming,
Art,
Computer industry,
Software tools,
Application software,
Systems engineering education"
Modeling and Analysis of Three-Phase Hybrid Transformer Using Matrix Converter,"This paper deals with modeling and analysis of a new solution for a three-phase AC transformer with electromagnetic and electric (hybrid) coupling. The electromagnetic coupling is realized by means of the conventional transformer. The electrical coupling is realized by means of a matrix converter (MC), which is supplied from an auxiliary secondary winding of the transformer. In this paper there are descriptions of the proposed solution, with a presentation of their modeling and analysis of their properties. The steady-state properties analysis is based on the D-Q transformation method and four terminal descriptions.","Matrix converters,
Phase transformers,
Electromagnetic coupling,
Circuits,
Pulse width modulation converters,
Regulators,
Filters,
Power quality,
Voltage fluctuations,
Pulse width modulation"
Economics-Driven Data Management: An Application to the Design of Tabular Data Sets,"Organizational data repositories are recognized as critical resources for supporting a large variety of decision tasks and for enhancing business capabilities. As investments in data resources increase, there is also a growing concern about the economic aspects of data resources. While the technical aspects of data management are well examined, the contribution of data management to economic performance is not. Current design and implementation methodologies for data management are driven primarily by technical and functional requirements, without considering the relevant economic factors sufficiently. To address this gap, this study proposes a framework for optimizing data management design and maintenance decisions. The framework assumes that certain design characteristics of data repositories and data manufacturing processes significantly affect the utility of the data resources and the costs associated with implementing them. Modeling these effects helps identify design alternatives that maximize net-benefit, defined as the difference between utility and cost. The framework for the economic assessment of design alternatives is demonstrated for the optimal design of a large data set","Environmental economics,
Costs,
Resource management,
Investments,
Design methodology,
Design optimization,
Technology management,
Marketing and sales,
Manufacturing processes,
Quality management"
An Energy-Efficient Framework for Large-Scale Parallel Storage Systems,"Huge energy consumption has become a critical bottleneck for further applying large-scale cluster systems to build new data centers. Among various components of a data center, storage subsystems are one of the biggest consumers of energy. In this paper, we propose a novel buffer-disk based framework for large-scale and energy-efficient parallel storage systems. To validate the efficiency of the proposed framework, a buffer-disk scheduling algorithm is designed and implemented. Our algorithm can provide more opportunities for underlying disk power management schemes to save energy by keeping a large number of idle data disks in sleeping mode as long as possible. The trace-driven simulation results based on a revised disksim simulator show that this new framework can significantly improves the energy efficiency of large-scale parallel storage systems.","Energy efficiency,
Large-scale systems,
Energy storage,
Buffer storage,
Energy consumption,
Scheduling algorithm,
High performance computing,
Algorithm design and analysis,
Humans,
Computer science"
INformation fusion engine for real-time decision-making (INFERD): A perceptual system for cyber attack tracking,"Information fusion engine for real-time decision- making (INFERD) is a perceptual information fusion engine designed and developed for the purpose of cyber attack tracking and network situational awareness. While the original application was cyber orientated, the engine itself is designed to generalize and has been ported to other application environments such as maritime domain awareness and medical syndromic surveillance. Comparisons and contrasts are drawn to the traditional Kalman ground target tracking science, motivating high level architectural modules and presenting the cyber environment complexities and assumptions. Performance results are presented showing success in both detection accuracy and temporal expedience, an important design goal.",
Feature Selection via Coalitional Game Theory,"We present and study the contribution-selection algorithm (CSA), a novel algorithm for feature selection. The algorithm is based on the multiperturbation shapley analysis (MSA), a framework that relies on game theory to estimate usefulness. The algorithm iteratively estimates the usefulness of features and selects them accordingly, using either forward selection or backward elimination. It can optimize various performance measures over unseen data such as accuracy, balanced error rate, and area under receiver-operator-characteristic curve. Empirical comparison with several other existing feature selection methods shows that the backward elimination variant of CSA leads to the most accurate classification results on an array of data sets.",
Lazy Reconfiguration Forest (LRF) - An Approach for Motion Planning with Multiple Tasks in Dynamic Environments,"We present a novel algorithm for robot motion planning in dynamic environments. Our approach extends rapidly-exploring random trees (RRTs) in several ways. We assume the need to simultaneously plan and maintain paths for multiple tasks with respect to the current state of a moving robot in a dynamic environment. Our algorithm dynamically maintains a forest of trees by splitting, growing and merging them on the fly to adapt to moving obstacles and robot motion. In order to minimize tree maintenance, we only validate the task paths, rather than the entire forest. The root of the inhabited tree moves with the robot. Dynamic re-planning is integrated with tree and forest maintenance. Coupling the robot motion with the planner enables us to support multiple tasks, for example providing an ""escape"" path while moving to a goal. The robot is free to move along whichever task path it chooses. We highlight the work by showing fast results in simulated environments with moving obstacles.","Motion planning,
Robot kinematics,
Robot motion,
Robot sensing systems,
Path planning,
US Department of Energy,
Robotics and automation,
USA Councils,
Intelligent robots,
Space exploration"
Reduction of Quality (RoQ) Attacks on Dynamic Load Balancers: Vulnerability Assessment and Design Tradeoffs,"One key adaptation mechanism often deployed in networking and computing systems is dynamic load balancing. The goal from employing dynamic load balancers is to ensure that the offered load would be judiciously distributed across resources to optimize the overall performance. To that end, this paper discovers and studies new instances of Reduction of Quality (RoQ) attacks that target the dynamic operation of load balancers. Our exposition is focused on a number of load balancing policies that are either employed in current commercial products or have been proposed in literature for future deployment. Through queuing theory analysis, numerical solutions, simulations and Internet experiments, we are able to assess the impact of RoQ attacks through the potency metric. We identify the key factors, such as feedback delay and averaging parameters, that expose the trade-offs between resilience and susceptibility to RoQ attacks. These factors could be used to harden load balancers against RoQ attacks. To the best of our knowledge, this work is the first to study adversarial exploits on the dynamic operation of load balancers.","Load management,
Delay,
Computer science,
USA Councils,
Resource management,
Feedback,
Communications Society,
Educational institutions,
Computer networks,
Queueing analysis"
A Triplet-based Computer Architecture Supporting Parallel Object Computing,"A real scalable triplet-based computer architecture TriBA is proposed in this paper. TriBA is an object-oriented chip multi-processor that supports truly parallel execution of objects from hardware. Cores on the same chip are connected via triplet-based hierarchical interconnection network (THIN), which has simple topology and computing locality characteristic. A distributed deterministic routing algorithm (DDRA) is elaborated, already proposed for THIN. Runtime objects are mapped to processor cores according to their coupling degree which can also transfer to idle cores if needed. TriBA achieves the unification of software architecture and computer, and also relieves the burden of parallel programming.","Computer architecture,
Concurrent computing,
Hardware,
Multiprocessor interconnection networks,
Network topology,
Computer networks,
Routing,
Runtime,
Software architecture,
Parallel programming"
Holding-time-aware and availability-guaranteed connection provisioning in optical WDM mesh networks,"In optical wavelength-division multiplexing (WDM) networks, the progress in new technologies is paving the road towards dynamic optical transport networks in which leasable circuits can be set up and released on a short-term basis while providing huge capacities to bandwidth-hungry applications. Since interruption of a high-speed optical connection means a huge loss of data information, such connections need to be protected against failures. On the other hand, the explosive growth of different traffic types such as data, voice and video requires differentiated services besides availability-guaranteed bandwidth. Therefore, future network carriers need to meet strict SLA (service level agreement) guidelines, thus guaranteeing a level of service, as well as achieving efficient resource utilization. Unprotected, shared-path, and dedicated-path protection techniques can be used to meet the differentiated availability requirements. Recently, among the other service level specifications (SLSs), many new applications are identified by a predictable or known-in-advance holding-time. So, for dynamic provisioning of availability-guaranteed connections in an optical mesh network, we investigate a new algorithm which exploits the knowledge of connection holding times to accomplish minimum backup capacity allocation as compared to the previous holding-time-unaware approach.","Availability,
Optical fiber networks,
Heuristic algorithms,
High speed optical techniques,
Materials requirements planning,
Routing,
Wavelength division multiplexing"
From Desktop Applications Towards Ajax Web Applications,"Ajax is a set of different technologies that work together to create new and powerful Web applications. Ajax is demonstrating its usefulness in real world applications. The most important Internet companies as: Google, Yahoo, Amazon, Microsoft, are developing rich Web applications based on Ajax. Many developers do not know how to use these technologies to build Ajax Applications. In this paper, we present an overview about Ajax. Here, we discuss the term Ajax and the technologies used. Also, we show how Ajax is working inside, and how the technologies work together to achieve a rich behavior.","Application software,
Internet,
Java,
XML,
Cascading style sheets,
Computer science,
Information systems,
Software engineering,
Asynchronous communication,
Explosions"
Discovering Relational Patterns across Multiple Databases,"Relational patterns across multiple databases can reveal special pattern relationships hidden inside data collections. Existing research in data mining has made significant efforts in discovering different types of patterns from single or multiple databases, but how to find patterns that have a higher support in database A than in database B with a given support threshold a is still an open problem. We propose in this paper DRAMA, a systematic framework for discovering relational patterns across multiple databases. More specifically, given a series of data collections, we try to discover patterns from different databases with patterns' relationships satisfying the user specified constraints. Our method seeks to build a hybrid frequent pattern tree (HFP-tree) from multiple databases, and mine patterns from the HFP-tree by integrating users' constraints into the pattern mining process.","Relational databases,
Computer science,
Data mining,
Transaction databases,
Data analysis,
Organizing,
Pattern analysis,
Marketing and sales,
Association rules,
Itemsets"
Regular Perturbation Analysis for Trajectory Linearization Control,"Trajectory linearization control (TLC) is a nonlinear control design method, which combines an open-loop nonlinear dynamic inversion and a linear time-varying feedback stabilization. TLC achieves exponential stability along the nominal trajectory, therefore it provides robust stability and performance. In this paper, stability analysis of TLC with regular perturbation is presented. By integrating the Lyapunov second method with the linear time-varying (LTV) system spectra theory, which is a first method of Lyapunov. The analysis assesses stability robustness of TLC and identifies its relationship with the closed-loop PD-eigenvalues. Thus the analysis provides a guideline to design and real-time tuning of the time-varying closed-loop PD-eigenvalues.","Robust stability,
Stability analysis,
Open loop systems,
Control systems,
Linear feedback control systems,
Nonlinear dynamical systems,
Time varying systems,
Control design,
Guidelines,
Vehicle dynamics"
Reliability of Enhancement-mode AlGaN/GaN HEMTs Fabricated by Fluorine Plasma Treatment,"The reliability of enhancement-mode AlGaN/GaN HEMTs fabricated by the fluorine plasma treatment technique was investigated by applying OFF-state and ON- state long-term high-electric-field stress. A moderate negative shift (-0.25 V) was observed in the threshold voltage after 288 hours of stress. This shift, however, can be eliminated with an enhancement/depletion dual-gate configuration which effectively prevents high electric field from influencing the fluorine plasma treated area.","Aluminum gallium nitride,
Gallium nitride,
HEMTs,
MODFETs,
Thermal stresses,
Plasma stability,
Plasma devices,
Plasma temperature,
Threshold voltage,
Plasma sources"
Biswapped Networks and Their Topological Properties,"In this paper, we propose a new class of interconnection networks, called ""biswapped networks"" (BSNs). Each BSN is built of 2n copies of some n-node basis network using a simple rule for connectivity that ensures its regularity, modularity, fault tolerance, and algorithmic efficiency. In particular, if the basis network is a Cayley digraph then so is the resulting BSN. Our proposed networks provide a systematic construction strategy for large, scalable, modular, and robust parallel architectures, while maintaining many desirable attributes of the underlying basis network that comprises its clusters. We show how key parameters of a BSN are related to the corresponding parameters of its basis network and obtain a number of results on internode distances, Hamiltonian cycles, and node-disjoint paths. We also discuss the relationship between BSNs and swapped or OTIS networks.","Multiprocessor interconnection networks,
Distributed computing,
Computer science,
Fault tolerance,
Modular construction,
Robustness,
Parallel architectures,
Parallel processing,
Software engineering,
Artificial intelligence"
A histogram-matching approach to the evolution of bin-packing strategies,"We present a novel algorithm for the one- dimension offline bin packing problem with discrete item sizes based on the notion of matching the item-size histogram with the bin-gap histogram. The approach is controlled by a constructive heuristic function which decides how to prioritise items in order to minimise the difference between histograms. We evolve such a function using a form of linear register-based genetic programming system. We test our evolved heuristics and compare them with hand-designed ones, including the well- known best fit decreasing heuristic. The evolved heuristics are human-competitive, generally being able to outperform high- performance human-designed heuristics.","Heuristic algorithms,
Humans,
Genetic programming,
Algorithm design and analysis,
Histograms,
Computer science,
Genetic algorithms,
Testing,
Computer industry,
Application software"
Stochastic Computational Fluid Mechanics,Stochastic simulations in computational fluid dynamics let researchers model uncertainties beyond numerical discretization errors. The authors present examples of stochastic simulations of compressible and incompressible flows and provide analytical solutions for verifying these newly emerging methods for stochastic modeling,"Stochastic processes,
Uncertainty,
Computational fluid dynamics,
Polynomials,
Differential equations,
Stochastic systems,
Chaos,
Computational modeling,
Stability,
Large-scale systems"
Stemming Versus Light Stemming as Feature Selection Techniques for Arabic Text Categorization,"This paper compares and contrasts two feature selection techniques when applied to Arabic corpus; in particular; stemming, and light stemming were employed. With stemming, words are reduced to their stems. With light stemming, words are reduced to their light stems. Stemming is aggressive in the sense that it reduces words to their 3-letters roots. This affects the semantics as several words with different meanings might have the same root. Light stemming, by comparison, removes frequently used prefixes and suffixes in Arabic words. Light stemming doesn't produce the root and therefore doesn't affect the semantics of words; it maps several words, which have the same meaning to a common syntactical form. The effectiveness of above two feature selection techniques was assessed in a text categorization exercise for Arabic corpus. This corpus consists of 15000 documents that fall into three categories. The K-nearest neighbors (KNN) classifier was used in this work. Several experiments were carried out using two different representations of the same corpus; the first version uses stem- vectors; and the second uses light stem-vectors as representatives of documents. These two representations were assessed in terms of size, time and accuracy. The light stem representation was superior in terms of classifier accuracy when compared with stemming.","Text categorization,
Classification tree analysis,
Computer science,
Information resources,
Internet,
Resource management,
Information management,
Sorting,
Information filtering,
Information filters"
A Decomposition-based Constraint Optimization Approach for Statically Scheduling Task Graphs with Communication Delays to Multiprocessors,"The paper presents a decomposition strategy to speed up constraint optimization for a representative multiprocessor scheduling problem. In the manner of Benders decomposition, our technique solves relaxed versions of the problem and iteratively learns constraints to prune the solution space. Typical formulations suffer prohibitive run times even on medium-sized problems with less than 30 tasks. Our decomposition strategy enhances constraint optimization to robustly handle instances with over 100 tasks. Moreover, the extensibility of constraint formulations permits realistic application and resource constraints, which is a limitation of common heuristic methods for scheduling. The inherent extensibility, coupled with improved run times from a decomposition strategy, posit constraint optimization as a powerful tool for resource constrained scheduling and multiprocessor design space exploration","Constraint optimization,
Delay,
Processor scheduling,
Robustness,
Space exploration,
Microarchitecture,
Integer linear programming,
Signal processing algorithms,
Taxonomy,
Scheduling algorithm"
Throughput Comparison of CSMA and CDMA slotted ALOHA in Inter-Vehicle Communication,"This paper compare the throughput performance of a carrier-sense multiple access (CSMA) scheme that uses an orthogonal frequency division multiple access (OFDM) physical layer and a CDMA slotted ALOHA scheme based on multicarrier CDMA (MC-CDMA) in inter-vehicle communication (IVC). In particular, we compare the throughputs in a situation where a hidden terminal is no longer negligible. We encounter such a situation in an intersection where two or more cars are crossing by. In a hidden terminal situation, the CSMA scheme may degrade throughput. On the other hand, CDMA slotted ALOHA has a resistance to the hidden terminal situation as it can support simultaneous packets. However, the transmission data rate is much slow because of the spreading. Thus the total throughput may not be as high as the CSMA based OFDM even in a hidden terminal situation. As a result, we show that the CSMA based OFDM system shows better total throughput than CDMA slotted ALOHA in all region. But, only the signal from the nearest car can be received. On the contrary, CDMA slotted ALOHA can be received many packets simultaneously. This is much preferable feature for safety driving.","Multiaccess communication,
Throughput,
OFDM,
Multicarrier code division multiple access,
Physical layer,
Degradation,
Protocols,
Radio broadcasting,
Frequency conversion,
Safety"
A Location-Map Free Reversible Data Hiding Method using Block-Based Single Parameter,"This paper proposes a reversible data hiding method that embeds an L-level data sequence to images in the spatial domain. Though reversible data hiding once distorts the image to hide data into it, the distorted image is completely separated to the original image and the hidden data. The proposed method uses only one parameter to embed and extract data, and it extracts data without any location map. In addition, it can control embedding capacity according to payload for suppression of embedding distortion. Simulation results show the effectiveness of the proposed method.","Data encapsulation,
Data mining,
Image restoration,
Pixel,
Signal restoration,
Payloads,
Data engineering,
Systems engineering and theory,
Embedded computing,
Medical control systems"
On Model-Based Analysis of Ear Biometrics,"Ears are a new biometric with major advantage in that they appear to maintain their structure with increasing age. Most current approaches are holistic and describe the ear by its general properties. We propose a new model-based approach, capitalizing on explicit structure and with the advantages of being robust in noise and occlusion. Our model is a constellation of generalized ear parts, which is learned off-line using an unsupervised learning algorithm over an enrolled training set of 63 ear images. The Scale Invariant Feature Transform (SIFT), is used to detect the features within the ear images. In recognition, given a profile image of the human head, the ear is enrolled and recognised from the parts selected via the model. We achieve an encouraging recognition rate, on an image database selected from the XM2VTS database. A head-to-head comparison with PCA is also presented to show the advantage derived by the use of the model in successful occlusion handling.",
Fast and Accurate Automatic Registration for MR-Guided Procedures Using Active Microcoils,"A fast, robust, accurate, and automatic registration technique based on magnetic resonance (MR) active microcoils (active markers) for registration of tracked medical devices to preprocedural MR-images is presented. This allows for a straightforward integration of position measurement systems into clinical procedures. The presented method is useful for guidance purposes in clinical applications with high demands on accuracy and ease-of-use (e.g., neurosurgical or orthopedic applications). The determination of the positions of the active markers is integrated into the preparation phase of the actual MR imaging scan. The technique features a generic interface using DICOM standards for communication with navigation workstations linked to an MR system. The position of the active markers is fixed with respect to a reference system of an optical positioning measurement system (OPMS) and thus the coregistration of the MR system and the OPMS is established. In a phantom study, a mean overall targeting accuracy of 0.9plusmn0.1 mm was achieved and compared favorably to results obtained from manual registration tests (1.8plusmn0.3 mm) carried out in parallel. For a test person trained for both registration methods, workflow improvements of 3-6 min per registration step were found. The need for manual interaction is entirely eliminated thus avoiding user-bias, which is advantageous for the usage in clinical routine. The method improves the ease-of-use of tracking equipment during stereotactic guidance. The method is finally demonstrated in a volunteer study using a model of a Mayfield skull clamp with integrated active and optical reference markers","Position measurement,
Biomedical optical imaging,
Testing,
Robustness,
Magnetic resonance,
Biomedical imaging,
Neurosurgery,
Orthopedic surgery,
Optical imaging,
DICOM"
A Replicate Empirical Comparison between Pair Development and Software Development with Inspection,"In 2005, we studied the development effort and effect of quality comparisons between software development with Fagan's inspection and pair development. Three experiments were conducted in Thailand: two classroom experiments and one industry experiment. We found that in the classroom experiments, the pair development group had less average development effort than the inspection group with the same or higher level of quality. The industry experiment's result showed pair development to have a bit more effort but about 40% fewer major defects. However, since this set of experiments was conducted in Thailand, the results may be different if we conducted the experiment in other countries due to the impact of cultural differences. To investigate this we conducted another experiment with computer science graduate students at USC in Fall 2006. Unfortunately, the majority of the graduate students who participated in the experiment were from India, a country in which the culture is not much different from Thailand. As a result, we cannot compare the impact of cultural differences in this paper. However, the results showed that the experiment can be replicated in other countries where the cultures are similar.","Programming,
Inspection,
Software quality,
Cultural differences,
Testing,
Software engineering,
Costs,
Software measurement,
Computer science,
Computer industry"
LeCramFS: an efficient compressed file system for flash-based portable consumer devices,"Cost-effectiveness is one of the most critical factors in the development of portable consumer devices. The use of compression techniques is a simple but effective solution for achieving such cost-effectiveness. This paper gives a comprehensive analysis of file system level compression techniques for portable consumer devices. Since conventional compressed file systems are optimized for disk like devices and relatively abundant computing resources, they are not suitable for portable devices with small amount of memory and weak processing power. This paper presents the design and implementation of LeCramFS, which is a new read-only compressed file system designed for small portable devices and flash memory. Experiments by implementation show that the proposed file system outperforms conventional ones in terms of memory-efficiency and I/O performance.","File systems,
Flash memory,
Consumer electronics,
Computer science,
System performance,
Portable computers,
Profitability,
Semiconductor device manufacture,
Costs,
Large-scale systems"
Stabilizing Peer-to-Peer Spatial Filters,"In this paper, we propose and prove correct a distributed stabilizing implementation of an overlay, called DR-tree, optimized for efficient selective dissemination of information. DR-tree copes with nodes dynamicity (frequent joins and leaves) and memory and counter program corruptions, that is, the processes can connect/disconnect at any time, and their memories and programs can be corrupted. The maintenance of the structure is local and requires no additional memory to guarantee its stabilization. The structure is balanced and is of height 0(logm(N)), which makes it suitable for performing efficient data storage or search. We extend our overlay in order to support complex content-based filtering in publish/subscribe systems. Publish/subscribe systems provide useful platforms for delivering data (events) from publishers to subscribers in a decoupled fashion in distributed networks. Developing efficient publish/subscribe schemes in dynamic distributed systems is still an open problem for complex subscriptions (spanning multi-dimensional intervals). Embedding a publish/subscribe system in a DR-trees is a new and viable solution. The DR-tree overlay also guarantees subscription and publication times logarithmic in the size of the network while keeping its space requirement low (comparable to its DHT-based counterparts). Nonetheless, the DR- tree overlay helps in eliminating the false negatives and drastically reduces the false positives in the embedded publish/subscribe system.","Peer to peer computing,
Spatial filters,
Subscriptions,
Routing,
Costs,
Filtering,
Computer science,
Counting circuits,
Memory,
Maintenance"
Performance of Network-Coding in Multi-Rate Wireless Environments for Multicast Applications,"This paper investigates the interaction between net-work coding and link-layer transmission rate diversity in multi-hop wireless networks. By appropriately mixing data packets at intermediate nodes, network coding allows a single multicast flow to achieve higher throughput to a set of receivers. Broadcast applications can also exploit link-layer rate diversity, whereby individual nodes can transmit at faster rates at the expense of corresponding smaller coverage area. We first demonstrate how combining rate-diversity with network coding can provide a larger capacity for data dissemination of a single multicast flow, and how consideration of rate diversity is critical for maximizing system throughput. We also study the impact of both network coding and rate diversity on the dissemination latency for a class of quasi real-time applications, where the freshness of disseminated data is important. Our results provide evidence that network coding may lead to a latency-vs-throughput tradeoff in wireless environments, and that it is thus necessary to adapt the degree of network coding to ensure conformance to both throughput and latency objectives.","Network coding,
Broadcasting,
Diversity reception,
Throughput,
Government,
Delay,
Spread spectrum communication,
Performance gain,
Routing,
Unicast"
Development of Image Processing System Based on DSP and FPGA,"Real-time image processing system is widely used in many field, it is required to have high speed. In order to satisfy the demand, an image processing system structure based on DSP and FPFA is presented, that is DSP is used as advanced image processing unit and FPGA as logic unit for image sampling and display. The hardware configuration and working principle is introduced firstly, and then some key problems which include of image data stored mode, color space conversion and image transmission based on EDMA are described. Finally the program flowchart for developing image processing software is given. The developed system can acquire image, display image and make some image processing operations which include of geometry transform, orthographic transform, operations based on pixels, image compression and color space conversion. The developed system can meet the real-time requirement and has been used in our teaching.","Image processing,
Digital signal processing,
Field programmable gate arrays,
Real time systems,
Displays,
Image converters,
Logic,
Image sampling,
Hardware,
Image communication"
Towards a conceptual framework for visual analytics of time and time-oriented data,"Time is an important data dimension with distinct characteristics that is common across many application domains. This demands specialized methods in order to support proper analysis and visualization to explore trends, patterns, and relationships in different kinds of time-oriented data. The human perceptual system is highly sophisticated and specifically suited to spot visual patterns. For this reason, visualization is successfully applied in aiding these tasks. But facing the huge volumes of data to be analyzed today, applying purely visual techniques is often not sufficient. Visual analytics systems aim to bridge this gap by combining both, interactive visualization and computational analysis. In this paper, we introduce a concept for designing visual analytics frameworks and tailored visual analytics systems for time and time-oriented data. We present a number of relevant design choices and illustrate our concept by example.",
Automatic Camera Network Localization using Object Image Tracks,"Camera networks are being used in more applications as different types of sensor networks are used to instrument large spaces. Here we show a method for localizing the cameras in a camera network to recover the orientation and position up to scale of each camera, even when cameras are wide-baseline or have different photometric properties. Using moving objects in the scene, we use an intra-camera step and an inter-camera step in order to localize. The intra-camera step compares frames from a single camera to build the tracks of the objects in the image plane of the camera. The inter-camera step uses these object image tracks from each camera as features for correspondence between cameras. We demonstrate this idea on both simulated and real data.","Layout,
Light sources,
Calibration,
Computer science,
Smart cameras,
Head,
Humans,
Application software,
Image sensors,
Instruments"
GeoAds: A Middleware Architecture for Music Service with Location-Aware Advertisement,"This paper presents a middleware architecture to support location-based services (LBSs) based on heterogeneous localization systems. A GEOlNFO gateway is implemented to hide the discrepancy of different localization systems from application developers. Hence, application developers can deploy their LBSs easily without considering the underlying localization techniques. To demonstrate the detailed process and advantages of the proposed architecture, we present a location-based service, called GeoAds, which is a streaming music service with geographical advertisement. This novel integration is expected to attract more mobile users' attention.","Middleware,
Global Positioning System,
Shape,
Computer architecture,
Application software,
Indoor environments,
Streaming media,
Computer science,
Communication industry,
Computer industry"
A simple tactile sensor system for robot manipulator and object edge shape recognition,"This paper proposes a simple tactile sensing and control method for robot to realize an active object surface recognition. The tactile sensor implemented on the robot hand consists of three force sensitive resistors with the peripheral circuits. The robot arm equipped with the sensor can perform active object recognition. Our research goal is to realize an effective robot controller for the 3D object handling. An additional final aim is to produce ""artifacts"" that is a kind of artificial creature, which enables to cooperate works between human and machine (robot) in a natural and intuitive manner. Throughout these works, we aim to realize a harmonized human-machine environment. The two examples of applications of the proposed tactile sensor are also introduced. First is a hand pose control to keep the moving direction normal to the object, which is often required in pushing an object for positioning. This application can be used for human-machine interaction tasks. Next is a hand pose and motion control for 3D-object edge tracing, which is required for industrial use such as welding machines and welding inspection. In these applications, we need no information about object shape or orientation in advance. We also show three experimental results using the proposed sensor unit. The system and results of experiments are presented.",
Linear and Quadratic Subsets for Template-Based Tracking,"We propose a method that dramatically improves the performance of template-based matching in terms of size of convergence region and computation time. This is done by selecting a subset of the template that verifies the assumption (made during optimization) of linearity or quadraticity with respect to the motion parameters. We call these subsets linear or quadratic subsets. While subset selection approaches have already been proposed, they generally do not attempt to provide linear or quadratic subsets and rely on heuristics such as textured-ness. Because a naive search for the optimal subset would result in a combinatorial explosion for large templates, we propose a simple algorithm that does not aim for the optimal subset but provides a very good linear or quadratic subset at low cost, even for large templates. Simulation results and experiments with real sequences show the superiority of the proposed method compared to existing subset selection approaches.","Convergence,
Optimization methods,
Computer vision,
Linearity,
Testing,
Computer science,
Laboratories,
Explosions,
Cost function,
Computational efficiency"
Estimating Road Traffic Congestion from Cell Dwell Time using Neural Network,"In this study, we investigated an alternative method to estimate the degree of road traffic congestion based on a new measurement metric called cell dwell time (CDT) using simple feedforward backpropagation neural network. CDT is the duration that a cellular phone is registered to a base station before handing off to another base station. As a vehicle with cellular phone traverses along the road, cell handoffs occur and the values of CDT vary. Our assumption is that the values of CDT relate to the degree of traffic congestion and that high CDTs indicate congested traffic. In this study, we measured series of CDTs while driving along arterial roads in Bangkok metropolitan area. Human judgment of traffic condition was recorded into one of the three levels indicating congestion degree -free flow, moderate, or highly congested. Neural network was then trained and tested using the collected data against human perception. The results showed promising performance of congestion estimation with accuracy of 79.43%, precision ranging from 73.53% to 85.19%, and mean square error of 0.44.","Telecommunication traffic,
Neural networks,
Cellular phones,
Base stations,
Humans,
Time measurement,
Backpropagation,
Feedforward neural networks,
Road vehicles,
Area measurement"
Stochastic optimization of bipedal walking using gyro feedback and phase resetting,"We present a method to optimize the walking pattern of a humanoid robot for forward speed using suitable metaheuristics. Our starting point is a hand-tuned open-loop gait that we enhance with two feedback control mechanisms. First, we employ a P-controller that regulates the foot angle in order to reduce angular velocity of the robot's body. Second, we introduce a phase resetting mechanism that starts the next step at the moment of foot contact. Using a physics-based simulation, we demonstrate that such feedback control is essential for achieving fast and robust locomotion.","Stochastic processes,
Legged locomotion,
Robot sensing systems,
Optimization methods,
Humanoid robots,
Foot,
Sampling methods,
Trajectory,
Feedback control,
Learning"
An Articulatory Feature-Based Tandem Approach and Factored Observation Modeling,"The so-called tandem approach, where the posteriors of a multilayer perceptron (MLP) classifier are used as features in an automatic speech recognition (ASR) system has proven to be a very effective method. Most tandem approaches up to date have relied on MLPs trained for phone classification, and appended the posterior features to some standard feature hidden Markov model (HMM). In this paper, we develop an alternative tandem approach based on MLPs trained for articulatory feature (AF) classification. We also develop a factored observation model for characterizing the posterior and standard features at the HMM outputs, allowing for separate hidden mixture and state-tying structures for each factor. In experiments on a subset of Switchboard, we show that the AF-based tandem approach is as effective as the phone-based approach, and that the factored observation model significantly outperforms the simple feature concatenation approach while using fewer parameters.","Hidden Markov models,
Automatic speech recognition,
Multilayer perceptrons,
Speech recognition,
Feature extraction,
Concatenated codes,
Computer science,
Standards development,
Acoustic signal processing,
Speech processing"
Efficient Pilot Pattern for OFDM-based Cognitive Radio Channel Estimation - Part 1,"Nowadays research in the area of Cognitive Radio has grown tremendously. In the application of spectrum pooling in OFDM-based Cognitive Radio system there will be subcarriers that are deactivated due to the access of Licensed Users. The unloaded subcarriers that are caused by Licensed Users access create problems in performing channel estimation. The unloaded subcarriers restrict putting the pilot symbols in time and frequency lattice. This issue should be considered in the designing pilot symbols grid. In the part I of this paper, new pilot patterns for OFDM-based Cognitive Radio channel estimation are proposed. These new pilot patterns are the modification of rectangular pilot patterns by shifting the pilot subcarricrs into the adjacent subcarriers which are free from the interference of Licensed Users. Using the proposed pilot patterns, the channel estimation based on two cascaded one dimensional (2 times 1-D) Wiener filter is investigated by considering the appearance of narrowband and wideband Licensed Users. Results have shown that the proposed pilot pattern outperforms the regular pilot pattern by around 6.5 dB gain at Mean Square Error value of 2 times 10-3 in the case of narrowband interference.","Cognitive radio,
Channel estimation,
OFDM modulation,
Wiener filter,
Digital video broadcasting,
Robustness,
Interference,
Narrowband,
Radar,
Mathematics"
First Responders' Crystal Ball: How to Scry the Emergency from a Remote Vehicle,"Successes and failures during rescue operations after hurricane Katrina and the Twin Towers attack demonstrated the importance of supporting first responders with adequate means to perform their operations in an effective and safe way. From a networking point of view, one of the main challenges is that of providing first responders with multimedia information about the emergency as soon as possible, even from a remote location. To this aim, we designed an inter-vehicular communication system able to quickly discover and transmit real time multimedia information from around a crisis area to approaching first responders' vehicles. As vehicular communications are highly variable in nature, we endowed our system with a transmission range estimator that is put to good use to reduce the number of hops that a video triggering message sent by a vehicle will experience to reach its destination. Experimental results demonstrate the efficacy of our scheme in reducing the message delivery time and the traffic generated.",
A passive approach to rogue access point detection,"Unauthorized or rogue access points (APs) produce security vulnerabilities in enterprise/campus networks by circumventing inherent security mechanisms. We propose to use the round trip time (RTT) of network traffic to distinguish between wired and wireless nodes. This information coupled with a standard wireless AP authorization policy allows the differentiation (at a central location) between wired nodes, authorized APs, and rogue APs. We show that the lower capacity and the higher variability in a wireless network can be used to effectively distinguish between wired and wireless nodes. Further, this detection is not dependant upon the wireless technology (802.11a, 802.11b, or 802.11g), is scalable, does not contain the inefficiencies of current solutions, remains valid as the capacity of wired and wireless links increase, and is independent of the signal range of the rogue APs.","Communication system security,
Wireless networks,
Information security,
Probes,
Computer security,
National security,
Telecommunication traffic,
Authorization"
A Voronoi Approach for Scalable and Robust DV-Hop Localization System for Sensor Networks,"Localization systems have been identified as a key issue to the development and operation of the Wireless Sensor Networks (WSN). A DV-Hop localization system works by transforming the distance to all beacon nodes from hops to units of length measurement (e.g., meters, feet) using the average size of a hop as a correction factor. Despite its advantages, a DV-Hop algorithm has some disadvantages, such as its large communication cost that limits its scalability, and its mapping from hops to distance units that introduces errors that are propagated to the computation of a node location. This last issue has been solved by some recent works, but the scalability problem still is an open problem that limits this technique to small or medium sized networks. In this work, we propose a novel approach that uses Voronoi diagrams in order to scale a DV-Hop localization algorithm while mantaining or even reducing its localization error. Two types of localization can result from the proposed algorithm: the physical location of the node (e.g., latitude, longitude), or a region limited by the node's Voronoi cell. The algorithm evaluation is performed by comparison with similar algorithms. We show how the proposed algorithm can scale in different aspects such as communication and processing costs when increasing the number of nodes and beacons.",
How to synthesize nets from languages - a survey,"In this paper we present a survey on methods for the synthesis of Petri nets from behavioral descriptions given as languages. We consider place/transition Petri nets, elementary Petri nets and Petri nets with inhibitor arcs. For each net class we consider classical languages, step languages and partial languages as behavioral description. All methods are based on the notion of regions of languages. We identify two different types of regions and two different principles of computing from the set of regions of a language a finite Petri net generating this language. For finite or regular languages almost each combination of Petri net class, language type, region type and computation principle can be considered to compute such a net. Altogether, we present a framework for region-based synthesis of Petri nets from languages which integrates almost all known approaches and fills several remaining gaps in literature.","Petri nets,
Control system synthesis,
Computational modeling,
Computer science,
Information technology,
Inhibitors,
Electrical equipment industry,
Manufacturing industries,
Hardware,
Control systems"
Visualizing Software Architecture Evolution Using Change-Sets,"When trying to understand the evolution of a software system it can be useful to visualize the evolution of the system's architecture. Existing tools for viewing architectural evolution assume that what a user is interested in can be described in an unbroken sequence of time, for example the changes over the last six months. We present an alternative approach that provides a lightweight method for examining the net effect of any set of changes on a system's architecture. We also present Motive, a prototype tool that implements this approach, and demonstrate how it can be used to answer questions about software evolution by describing case studies we conducted on two Java systems.","Software architecture,
Evolution (biology),
Computer architecture,
Software systems,
Data visualization,
Packaging,
Software prototyping,
Software tools,
Computer science,
Java"
Lightpaths routing for single link failure survivability in IP-over-WDM networks,"High speed all optical network is a viable option to satisfy the exponential growth of internet usage in the recent years. Optical networks offer very high bit rates and, by employing technologies like internet protocol over wavelength division multiplexing (IP-over-WDM), these high bit rates can be effectively utilized. However, failure of a network component, carrying such high speed data traffic can result in enormous loss of data in a few seconds and persistence of a failure can severely degrade the performance of the entire network. Designing IP-over-WDM networks, which can withstand failures, has been subject of considerable interest in the research community recently. Most of the research is focused on the failure of optical links in the network. This paper addresses the problem of designing IP-over-WDM networks that do not suffer service degradation in case of a single link failure. The paper proposes an approach based on the framework provided by a re- cent paper by M. Kurant and P. Thiran. The proposed approach can be used to design large survivable IP-over-WDM networks.","Topology,
Network topology,
IP networks,
Wavelength division multiplexing,
Routing,
Optical fiber networks,
Approximation algorithms"
Mining temporal mobile sequential patterns in location-based service environments,"In recent years, a number of studies have been done on location-based service (LBS) due to the wide applications. One important research issue is the tracking and prediction of users' mobile behavior. In this paper, we propose a novel data mining algorithm named TMSP-Mine for efficiently discovering the temporal mobile sequential patterns (TMSPs) of users in LBS environments. To our best knowledge, this is the first work on mining the mobile sequential patterns associated with moving paths and time intervals in LBS environments. Furthermore, we propose novel location prediction strategies that utilize the discovered TMSPs to effectively predict the next movement of mobile users. Finally, we conducted a series of experiments to evaluate the performance of the proposed method under different system conditions by varying various parameters.","Data mining,
Biological cells,
Genetic algorithms,
Mobile handsets,
Business,
Mobile computing,
Computer science,
Application software,
Switching systems,
Accuracy"
Imaging the Distribution of Magnetic Nanoparticles With Ultrasound,"Magnetic nanoparticles can be caused to oscillate under the influence of an incident ultrasonic wave. If the particles are momentarily aligned with a magnetizing pulse creating a macroscopic magnetization, this oscillation will result in a time-varying magnetic moment which should be detectable as an induced voltage in a nearby pickup coil. In this way, focused ultrasound can be used to map, or image, the spatial distribution of the magnetic particles after these particles have been introduced into the body. The magnetic particles could be antibody-labeled to target tumor cells or used as a cardiovascular contrast agent, among other applications. The magnitude of the induced signal is estimated for one micron particles with a Fe/tissue volume fraction of 10-6, which is about the limit of detectability for MRI superparamagnetic contrast agents consisting of single domain iron-oxide particles. One advantage of this method compared to conventional MRI is potentially greater sensitivity due to the absence of a large background signal","Ultrasonic imaging,
Nanoparticles,
Magnetic particles,
Magnetic resonance imaging,
Magnetization,
Magnetic moments,
Voltage,
Coils,
Focusing,
Tumors"
Speech Emotion Recognition using an Enhanced Co-Training Algorithm,"In previous systems of speech emotion recognition, supervised learning are frequently employed to train classifiers on lots of labeled examples. However, the labeling of abundant data requires much time and many human efforts. This paper presents an enhanced co-training algorithm to utilize a large amount of unlabeled speech utterances for building a semi-supervised learning system. It uses two conditionally independent attribute views(i.e. temporal features and statistic features) of unlabeled examples to augment a much smaller set of labeled examples. Our experimental results demonstrate that compared with the method based on the supervised training, the proposed system makes 9.0% absolute improvement on female model and 7.4% on male model in terms of average accuracy. Moreover, the enhanced co-training algorithm achieves comparable performance to the co-training prototype, while it can reduce the classification noise which is produced by error labeling in the process of semi-supervised learning.","Speech enhancement,
Emotion recognition,
Statistics,
Semisupervised learning,
Humans,
Prototypes,
Feature extraction,
Labeling,
Data mining,
Supervised learning"
Technology Validation: NMP ST8 Dependable Multiprocessor Project II,"With the ever-increasing demand for higher bandwidth and processing capacity of today's space exploration, space science, and defense missions, the ability to efficiently apply Commercial-Off-The-Shelf (COTS) processors for on-board computing has become a critical need. In response to this need, NASA's New Millennium Program (NMP) commissioned the development of Dependable Multiprocessor (DM) technology for use in science and autonomy missions, but the technology is also applicable to a wide variety of DoD missions. The goal of the DM project is to provide spacecraft/payload processing capability lOx -lOOx what is available today, enabling heretofore unrealizable levels of science and autonomy. DM technology is being developed as part of the NMP ST8 (Space Technology 8) project. The objective of this NMP ST8 effort is to combine high-performance, fault tolerant, COTS-based cluster processing and fault tolerant middleware in an architecture and software framework capable of supporting a wide variety of mission applications. Dependable Multiprocessor development is continuing as one of the four selected ST8 flight experiments planned to be flown in 2009.","Space technology,
Delta modulation,
Space exploration,
Fault tolerance,
Bandwidth,
Space missions,
Space vehicles,
Payloads,
Middleware,
Computer architecture"
The Capacity of MIMO-based Wireless Mesh Networks,"Data transmission in wireless mesh networks (WMNs) has a multi-hop nature where data is originated/destinated to one source/sink node. Therefore, the transmission capacity at the physical layer depends on multiple factors such as the network topology, the access policy, and the transmission configuration. When multiple input multiple output (MIMO) technology is used at the physical layer of each node, the nodes that are geographically in the same transmission range, can transmit simultaneously. The different transmissions can be separated spatially using one of the decoding MIMO methods. In this paper, we investigate the per-node capacity in a WMN using MIMO links. We introduce the problem of the transmission configuration in WMNs, defined as the number of transmit antennas used at each node, and its effect on the network capacity. The goal of this study is to provide ideas for network architects about how to dimension wireless mesh networks in order to have an optimal capacity with minimal equipment requirements.","Wireless mesh networks,
MIMO,
Transmitting antennas,
Ad hoc networks,
Interference,
Physical layer,
Wireless networks,
Computer science,
Spread spectrum communication,
Receiving antennas"
Context-Aware Middleware Support for the Nomadic Mobile Services on Multi-homed Handheld Mobile Devices,"Nowadays, a variety of handheld mobile devices are capable of connecting to the Internet using multiple network interfaces. This is referred to as multi-homing. In addition to this, enriched computation resources allow them to host nomadic mobile services and provide these services to the clients located anywhere in the Internet. Potential advantages of multi-homing for nomadic mobile services typically includes: an increased service availability and improved service performance. However, applications running on the handheld mobile devices either do not, or cannot, exploit multi-homing. In this paper we address the problem of providing infrastructural support to the nomadic mobile services that can fully exploit multi-homing. To this end we propose to incorporate multi-homing functionality and support in a middleware layer to reduce the complexity of the design and maintenance of these services. The proposed solution uses a context-aware computing approach to realize this functionality. We report the initial experimental results in the remote tele-monitoring of a patient equipped with a Body Area Network.","Context-aware services,
Middleware,
IP networks,
Body area networks,
Mobile communication,
Context,
Network interfaces,
Web and internet services,
Availability,
Service oriented architecture"
Common approach to functional safety and system security in building automation and control systems,"Building automation and control systems (BACS) are an important part of modern automated buildings. More and more they are also responsible for functions affecting people 's safety, security and health. Thus the respective technology is supposed to work reliably, securely, safely and efficiently. The two important features of such a BACS are functional safety and system security (short safety and security) of both the network nodes and the communication protocols. Up to now little effort has been made to specify a life cycle for a safe and secure BACS that defines requirements for the different stages of the product life of a BACS. Special focus is related to the commonalities between the development of safety and security systems to benefit from these commonalities in development.","Automation,
Automatic control,
Control systems,
Communication system security,
Data security,
Protocols,
Buildings,
Computer security,
Health and safety,
Computer network reliability"
Composite Models of Objects and Scenes for Category Recognition,"This paper presents a method of learning and recognizing generic object categories using part-based spatial models. The models are multiscale, with a scene component that specifies relationships between the object and surrounding scene context, and an object component that specifies relationships between parts of the object. The underlying graphical model forms a tree structure, with a star topology for both the contextual and object components. A partially supervised paradigm is used for learning the models, where each training image is labeled with bounding boxes indicating the overall location of object instances, but parts or regions of the objects and scene are not specified. The parts, regions and spatial relationships are learned automatically. We demonstrate the method on the detection task on the PASCAL 2006 Visual Object Classes Challenge dataset, where objects must be correctly localized. Our results demonstrate better overall performance than those of previously reported techniques, in terms of the average precision measure used in the PASCAL detection evaluation. Our results also show that incorporating scene context into the models improves performance in comparison with not using such contextual information.","Layout,
Context modeling,
Object detection,
Image classification,
Large-scale systems,
Motorcycles,
Bicycles,
Computer science,
Graphical models,
Tree data structures"
Fourier tags: Smoothly degradable fiducial markers for use in human-robot interaction,"In this paper we introduce the Fourier tag, a synthetic fiducial marker used to visually encode information and provide controllable positioning. The Fourier tag is a synthetic target akin to a bar-code that specifies multi-bit information which can be efficiently and robustly detected in an image. Moreover, the Fourier tag has the beneficial property that the bit string it encodes has variable length as a function of the distance between the camera and the target. This follows from the fact that the effective resolution decreases as an effect of perspective. This paper introduces the Fourier tag, describes its design, and illustrates its properties experimentally.","Degradation,
Robot vision systems,
Robot kinematics,
Computer science,
Cameras,
Computer vision,
Encoding,
Noise robustness,
Underwater tracking,
Application software"
"An Underwater Sensor Network with Dual Communications, Sensing, and Mobility","This paper describes an underwater sensor network with dual communication and support for sensing and mobility. The nodes in the system are connected acoustically for broadcast communication using an acoustic modem we developed. The nodes are connected optically for higher speed point to point data transfers using an optical modem we developed. We describe the hardware details of the underwater sensor node and the communication and networking protocols. Finally, we present and discuss the results from experiments with this system.","Underwater communication,
Acoustic sensors,
Optical sensors,
Temperature sensors,
Sensor phenomena and characterization,
Hardware,
Optical network units,
Underwater acoustics,
Modems,
Intelligent sensors"
Lower bounds on the rate-distortion function of LDGM codes,We analyze the performance of low-density generator matrix (LDGM) codes for lossy source coding. We first develop a generic technique for deriving lower bounds on the effective rate-distortion functions of binary linear codes. This result provides a source coding analog of a classical result due to Gallager for channel coding over the binary symmetric channel. We illustrate this method for the ensemble of check-regular low- density generator matrix (LDGM) codes by deriving an explicit lower bound on its rate-distortion performance as a function of the check degree.,
On the Convergence of Immune Algorithms,"Immune algorithms have been used widely and successfully in many computational intelligence areas including optimization. Given the large number of variants of each operator of this class of algorithms, this paper presents a study of the convergence properties of immune algorithms in general, conducted by examining conditions which are sufficient to prove their convergence to the global optimum of an optimization problem. Furthermore problem independent upper bounds for the number of generations required to guarantee that the solution is found with a defined probability are derived in a similar manner as performed previously, in literature, for genetic algorithms. Again the independence of the function to be optimised leads to an upper bound which is not of practical interest, confirming the general idea that when deriving time bounds for evolutionary algorithms the problem class to be optimised needs to be considered",
Self-Adaptive Neighbor Discovery in Ad Hoc Networks with Directional Antennas,"In this paper we present a novel neighbor discovery protocol for networks with directional antennas. The novelty of this protocol lies in two aspects. Firstly, in order to cope with mobility issues, we adjust the frequency of neighbor discovery attempts according to the dynamics of the network Secondly, to improve power efficiency and reduce overhead, the protocol has the ability to limit neighbor discovery attempts in directions where no new neighbors are likely to be found. The superior performance of our protocol is shown through simulations. Furthermore, we provide an analytical model to analyze directional neighbor discovery protocols in general. This model reveals the impacts of using directional antennas in neighbor discovery process. The accuracy of this analytical model is validated through simulations.",
On the Performance of Multi-robot Target Tracking,"In this paper, we study the accuracy of cooperative localization and target tracking (CLATT) in a team of mobile robots, and derive analytical upper bounds for the position uncertainty. The obtained bounds provide a description of the asymptotic positioning performance of the robots and the targets as a function of the sensor characteristics and the structure of the graph of relative position measurements. By employing an extended Kalman filter (EKF) formulation for data fusion, two key asymptotic results are derived. The first provides the guaranteed worst-case positioning accuracy, whereas the second determines an upper bound on the expected covariance of the estimates. We investigate the effects of jointly estimating the targets' and the robots' position, and demonstrate that it results in better accuracy for the robots' position estimates. The theoretical results are confirmed both in simulation and experimentally.","Target tracking,
Robot sensing systems,
Position measurement,
Upper bound,
Mobile robots,
Performance analysis,
Sensor phenomena and characterization,
Velocity measurement,
State estimation,
Robotics and automation"
A Novel Fast Anti-Collision Algorithm for RFID Systems,Tag anti-collision algorithm for RFID system is a significant issue for fast tag identification. This paper presents a novel fast tag anti-collision algorithm called DJ (detection and jump) algorithm which splits the tag reading procedure into two steps including collision detection and jump reading. The proposed new algorithm highly improves the tag reading efficiency comparing to the popular Q-algorithm in EPC (electronic product code) international standard. Our simulation evaluations show that the proposed DJ algorithm outperforms Q-algorithm in both identification delay and power consumption.,"Radiofrequency identification,
Computer science,
Delay,
Energy consumption,
Algorithm design and analysis,
Protocols,
Product codes,
Code standards,
RFID tags,
Radio frequency"
Segment-Based Map Matching,"We address the problem of determining the path of a vehicle on a given vector map of roads, based on tracking data such as that obtained from onboard GPS receivers. We describe a method that is based on a piecewise matching of track segments to map features. A notable feature of our method is that it is applicable to a large class of existing methods. We discuss metrics for evaluating the output of map-matching methods and briefly describe our implementation of a map-matching system based on our methods.","Global Positioning System,
Road vehicles,
Trajectory,
Topology,
Road transportation,
Measurement errors,
Gaussian distribution,
Geometry,
Intelligent vehicles,
Computer vision"
An Energy Efficient MAC in Wireless Sensor Networks to Provide Delay Guarantee,"This paper presents RTMAC, a realtime MAC protocol for wireless sensor network that can provide delay guarantee. RTMAC is based on TDMA protocol, but it is carefully designed to overcome the high latency of traditional TDMA protocols. It also conserves energy when a node may not be transmitting or receiving packets. We discuss the details of time slot assignment procedure of RTMAC and then present delay analysis of the protocol. We compare the performance of RTMAC with the well known energy efficient MAC protocol S-MAC using simulation. The simulation results show that RTMAC is better than S-MAC in terms of providing delay guarantee to packets.","Energy efficiency,
Wireless sensor networks,
Media Access Protocol,
Wireless application protocol,
Time division multiple access,
Access protocols,
Information technology,
Delay effects,
Patient monitoring,
Earthquakes"
Building Robust Wireless LAN for Industrial Control with the DSSS-CDMA Cell Phone Network Paradigm,"Wireless LAN for industrial control (IC-WLAN) provides many benefits, such as mobility, low deployment cost, and ease of reconfiguration. However, the top concern is robustness of wireless communications. Wireless control loops must be maintained under persistent adverse channel conditions, such as noise, large-scale path loss, fading, and many electromagnetic interference sources in industrial environments. The conventional IEEE 802.11 WLANs, originally designed for high bandwidth instead of high robustness, are therefore inappropriate for IC-WLAN. A solution lies in the direct sequence spread spectrum (DSSS) technology: by deploying the largest possible processing gain (slowest bit rate) that fully exploits the low data rate feature of industrial control, much higher robustness can be achieved. We hereby propose using DSSS-CDMA to build IC-WLAN. We carry out fine-grained physical layer simulations and Monte Carlo comparisons. The results show that DSSS-CDMA IC-WLAN provides much higher robustness than IEEE 802.11/802.15.4 WLAN, so that reliable wireless industrial control loops become feasible. We also show that deploying larger processing gain is preferable, to deploying more intensive convolutional coding. The DSSS-CDMA IC-WLAN scheme also opens up a new problem space for interdisciplinary study, involving real-time scheduling, resource management, communication, networking, and control","Robust control,
Wireless LAN,
Industrial control,
Spread spectrum communication,
Cellular phones,
Noise robustness,
Wireless communication,
Communication system control,
Electromagnetic interference,
Costs"
Security Conscious Web Service Composition with Semantic Web Support,"A Web service is a software system designed to support interoperable application-to-application interactions over the Internet. Recently, there has been a growing interest in Web service composition, and some languages (e.g.. WSBPEL and BPML) for modeling the composition have been proposed. In this paper, we focus on security constraints of Web service composition with semantic Web support, which have not been deeply investigated so far. Based on our prior work, we present a method for modeling security constraints and a brokered architecture, which exploits the REI reasoner. to build composite Web services according to the specified security constraints.","Web services,
Semantic Web,
Service oriented architecture,
Data security,
Computer security,
Software systems,
Software design,
Web and internet services,
Privacy,
Simple object access protocol"
An Asymptotically Efficient Simulation-Based Algorithm for Finite Horizon Stochastic Dynamic Programming,"We present a simulation-based algorithm called ""Simulated Annealing Multiplicative Weights"" (SAMW) for solving large finite-horizon stochastic dynamic programming problems. At each iteration of the algorithm, a probability distribution over candidate policies is updated by a simple multiplicative weight rule, and with proper annealing of a control parameter, the generated sequence of distributions converges to a distribution concentrated only on the best policies. The algorithm is ""asymptotically efficient,"" in the sense that for the goal of estimating the value of an optimal policy, a provably convergent finite-time upper bound for the sample mean is obtained","Stochastic processes,
Dynamic programming,
Simulated annealing,
Probability distribution,
Upper bound,
Uncertainty,
Computer science,
Mathematics,
Statistics,
Random number generation"
"A robust, input voltage adaptive and low energy consumption level converter for sub-threshold logic","A new level converter (LC) is proposed for logic voltage shifting between sub-threshold voltage to normal high voltage. By employing 2 PMOS diodes, the LC shows good operation robustness with sub-threshold logic input. The switching delay of the proposed LC can adapt with the input logic voltage which is more suitable for power aware systems. With a simpler circuit structure, the energy consumption of the LC is smaller than that of the existing sub-threshold LC. Simulation results demonstrate the performance improvement and energy reduction of the proposed LC. Test chip was fabricated using 0.18 mum CMOS process. Measurement results show that our proposed LC can operate correctly with an input at as low as 127 mV and an output voltage at 1.8V.","Robustness,
Low voltage,
Energy consumption,
Circuit testing,
Diodes,
Delay,
CMOS logic circuits,
Circuit simulation,
CMOS process,
Semiconductor device measurement"
Tractability and learnability arising from algebras with few subpowers,"A k-edge operation phi on a finite set A is a k + 1-ary operation that satisfies the identities phi (x,x,y,...,y) ap phi(x,y,x,y,...,y) ap y, phi(y,y,y,x,y,...,y) ap phi(y,y,y,y,x,y,...,y) ap ... ap ... phi(y,y,y,...,y,x) ap y. We prove that any constraint language .. that, for some k > 1, has a k-edge operation as a polymorphism is globally tractable. We also show that the set of relations definable over Gamma using quantified generalized formulas is polynomially exactly learnable using improper equivalence queries. Special instances of k-edge operations are Mal'cev and near-unanimity operations and so this class of constraint languages includes many well known examples.","Algebra,
Polynomials,
Mathematics,
Computer science,
Informatics,
Statistics,
Logic,
Equations,
Vectors,
Gaussian processes"
Kinodynamic motion planning on roadmaps in dynamic environments,"In this paper we present a new method for kinodynamic motion planning in environments that contain both static and moving obstacles. We present an efficient two-stage approach: in the preprocessing phase, it constructs a roadmap that is collision-free with respect to the static obstacles and encodes the kinematic constraints on the robot. In the query phase, it plans a time-optimal path on the roadmap that obeys the dynamic constraints (bounded acceleration, curvature derivative) on the robot and avoids collisions with any of the moving obstacles. We do not put any constraints on the motions of the moving obstacles, but we assume that they are completely known when a query is performed. We implemented our method, and experiments confirm its good performance.","Orbital robotics,
Motion planning,
Kinematics,
State-space methods,
Intelligent robots,
USA Councils,
Acceleration,
Road accidents,
Path planning"
Locating License Plate Based on Edge Features of Intensity and Saturation Subimages,"This paper proposes a novel license plate localization algorithm for automatic license plate recognition (LPR) systems. The proposed approach uses color edge information to refine the edge points extracted in a gray-level image. In the proposed LPR system, input images are processed as follows: (1) de- interlacing by the proposed vertical median filter firstly; (2) locating license plates by detecting edge in the intensity and the saturation subimages; (3) identifying the numbers by matching the seven invariants of characters. In our experiments, 368 images are tested during two hours test. The license plate location rate of success is 97.0% with 11 images failed to locate.","Licenses,
Image edge detection,
Filters,
Character recognition,
Feature extraction,
Data mining,
Testing,
Interpolation,
Computer science,
Jacobian matrices"
Document Representation and Dimension Reduction for Text Clustering,"Increasingly large text damsels and the high dimensionality associated with natural language create a great challenge in text mining, In this research, a systematic study is conducted. in which three different document representation methods for text are used, together with three Dimension Reduction Techniques (DRT), in the context of the text clustering problem. Several standard benchmark datasets are used. The three Document representation methods considered are based on the vector space model, and they include word, multi-word term, and character N-gram representations. The dimension reduction methods are. independent component analysis (ICA). latent semantic indexing (LSI), and a feature selection technique based on Document Frequency (DF). Results are compared in terms of clustering performance, using the k-means clustering algorithm. Experiments show that ICA and LSI are clearly belter than DF on all darascls. For word and N-gram representation. ICA generally gives better results compared with LSI. Experiments also show that the word representation gives better clustering results compared to term and N-gram representation. Finally, for the N-gram representation, it is demonstrated that a profile length (before dimensionality reduction) of 2000 is sufficient to capture the information and in most cases, a -4-gram representation gives better performance than 3-gram representation.","Independent component analysis,
Large scale integration,
Indexing,
Clustering algorithms,
Data mining,
Computer science,
Natural languages,
Text mining,
Frequency,
Communications technology"
Applications of multi-objective evolutionary algorithms in economics and finance: A survey,"This paper provides a state-of-the-art survey of applications of multi-objective evolutionary algorithms in economics and finance reported in the specialized literature. A taxonomy of applications within this area is proposed, and a brief review of the most representative research reported to date is then provided. In the final part of the paper, some potential paths for future research within this area are identified.","Evolutionary computation,
Finance,
Application software,
Taxonomy,
Portfolios,
Computer science,
Algorithm design and analysis,
Investments,
Environmental factors,
Sociology"
Volume Splitting and Its Applications,"Splitting a volumetric object is a useful operation in volume graphics and its applications, but is not widely supported by existing systems for volume-based modeling and rendering. In this paper, we present an investigation into two main algorithmic approaches, namely, explicit and implicit splitting, for modeling and rendering splitting actions. We consider a generalized notion based on scalar fields, which encompasses discrete specifications (e.g., volume data sets) as well as procedural specifications (e.g., hypertextures) of volumetric objects. We examine the correctness, effectiveness, efficiency, and deficiencies of each approach in specifying and controlling a spatial and temporal specification of splitting. We propose methods for implementing these approaches and for overcoming their deficiencies. We present a modeling tool for creating specifications of splitting functions, and describe the use of volume scene graphs for facilitating direct rendering of volume splitting. We demonstrate the use of these approaches with examples of volume visualization, medical illustration, volume animation, and special effects","Graphics,
Rendering (computer graphics),
Animation,
Data visualization,
Layout,
Biomedical imaging,
Explosions,
Focusing,
Surgery,
Silver"
A NOR Emulation Strategy over NAND Flash Memory,"This work is motivated by a strong market demand in the replacement of NOR flash memory with NAND flash memory to cut down the cost in many embedded-system designs, such as mobile phones. Different from LRU-related caching or buffering studies, we are interested in prediction-based prefetching based on given execution traces of application executions. An implementation strategy is proposed in the storage of the prefetching information with limited SRAM and run-time overheads. An efficient prediction procedure is presented based on information extracted from application executions to reduce the performance gap between NAND flash memory and NOR flash memory in reads. With the behavior of a target application extracted from a set of collected traces, we show that data access to NOR flash memory can be responded effectively over the proposed implementation.","Emulation,
Flash memory,
Prefetching,
Random access memory,
Costs,
Mobile handsets,
Embedded system,
Computer science,
Microprogramming,
Runtime"
Gpnocsim - A General Purpose Simulator for Network-On-Chip,"Network-on-chip (NoC) has gained considerable attention over the last few years as a paradigm for implementing communication among the system components embedded in a single chip. As such, there has been an increased need for defining and developing simulation software for carrying out simulation of NoC architectures. In this paper, we present gpNoCsim, a Java based general-purpose network simulator for NoC, which is built upon the object-oriented modular design of the NoC architecture components. Here we demonstrate the use of our proposed simulator by simulating several existing well-known architectures. We have also provided the guidelines on how to simulate future architectures. Finally, we have validated the outputs of gpNoCsim with the studies of existing NoC architectures.","Network-on-a-chip,
Object oriented modeling,
Switches,
Computer architecture,
Computational modeling,
Computer science,
Java,
Network topology,
Communications technology,
Computer simulation"
"The Design, Deployment, and Analysis of SignetLab: A Sensor Network Testbed and Interactive Management Tool","The emergence of small, inexpensive, network-capable sensing devices led to a great deal of research on the design and implementation of sensor networks. A critical step in taking protocols from theory to actual deployment is comprehensive testing on physical sensor networks. Sensor network testbeds provide one way to facilitate such testing without requiring the deployment of a specialized sensor network for each protocol. However, for such testbeds to be useful, they must not overwhelm researchers with maintenance tasks and high learning curves. Previous work in testbed design has primarily focused on creating interfaces to maximize their usage by convenient scheduling of jobs and output access. In this work, we present two contributions to sensor network testbed design. The first is a unique management tool that allows users to program, interact with, and receive data from nodes in the network, filling a gap in current testbed management solutions. The second is the design, deployment, and analysis of the SignetLab testbed. The analysis of the testbed and its results provide quantitative measurements of the impact of physical deployment on signal propagation characteristics. Additionally, we present two case studies where researchers have used the testbed and discuss the user experiences and lessons learned.","Testing,
Protocols,
Wireless sensor networks,
Software tools,
Hardware,
Space technology,
Backplanes,
Information analysis,
Computer network management,
Disaster management"
A novel modular climbing caterpillar using low-frequency vibrating passive suckers,"This paper presents a novel modular climbing caterpillar named ZC-I. After a related survey on the topic, a systematical summarizing on basic functions provided by this system is given. ZC-I features fast-building mechanical structure and low-frequency vibrating passive attachment principle. Active joints actuated by RC servos endow the connecting modules with the ability of changing shapes in two dimensions. After that the discussion focuses on the various locomotion capabilities. Linear movement, turning movement, lateral movement, rotating and rolling movement are achieved based on an inspired control model to produce rhythmic motion. In the end a conclusion and future work are given.","Climbing robots,
Prototypes,
Kinematics,
Servomechanisms,
Joining processes,
Shape,
Adhesives,
Electromagnetic forces,
Electromagnets,
Mobile robots"
Underwater environment reconstruction using stereo and inertial data,"The underwater environment presents many challenges for robotic sensing including highly variable lighting, the presence of dynamic objects, and the six degree of freedom (6DOF) 3D environment. Yet in spite of these challenges the aquatic environment presents many real and practical applications for robotic sensors. A common requirement of many of these tasks is the need to construct accurate 3D representations of structures in the environment. In order to address this requirement we have developed a stereo vision-inertial sensing device that we have successfully deployed to reconstruct complex 3D structures in both the aquatic and terrestrial domains. The sensor temporally combines 3D information, obtained using stereo vision algorithms with a 3DOF inertial sensor. The resulting point cloud model is then converted to a volumetric representation and a textured polygonal mesh is extracted for later processing. Recently obtained underwater reconstructions of wrecks and coral obtained with the sensor are presented.",
Airway Segmentation and Measurement in CT Images,"In this paper we describe a methodology for constructing the airways from cone beam CT data and representing changes before and after a medical procedure. A seed region is automatically detected for the first CT slice using a heuristic algorithm incorporating morphological filtering. Our approach then extracts relevant contours on 3D slices by using gradient vector flow (GVF) snakes, modified by an edge detection and snake-shifting step. Following this, a 3D model is constructed. We then estimate the volume of the airway based on segmented 3D shape.",
Inverse Dynamics Control with Floating Base and Constraints,"In this paper, we address the issues of compliant control of a robot under contact constraints with a goal of using joint space based pattern generators as movement primitives, as often considered in the studies of legged locomotion and biological motor control. For this purpose, we explore inverse dynamics control of constrained dynamical systems. When the system is overconstrained, it is not straightforward to formulate an inverse dynamics control law since the problem becomes an ill-posed one, where infinitely many combinations of joint torques are possible to achieve the desired joint accelerations. The goal of this paper is to develop a general and computationally efficient inverse dynamics algorithm for a robot with a free floating base and constraints. We suggest an approximate way of computing inverse dynamics algorithm by treating constraint forces computed with a Lagrange multiplier method as simply external forces based on Featherstone's floating base formulation of inverse dynamics. We present how all the necessary quantities to compute our controller can be efficiently extracted from Featherstone's spatial notation of robot dynamics. We evaluate the effectiveness of the suggested approach on a simulated biped robot model","Legged locomotion,
Control systems,
Heuristic algorithms,
Robot control,
Biological control systems,
Orbital robotics,
Motor drives,
Torque control,
Acceleration,
Lagrangian functions"
On Experiences in a Complex and Competitive Gaming Domain: Reinforcement Learning Meets RoboCup,"RoboCup soccer simulation features the challenges of a fully distributed multi-agent domain with continuous state and action spaces, partial observability, as well as noisy perception and action execution. While the application of machine learning techniques in this domain represents a promising idea in itself, the competitive character of RoboCup also evokes the desire to head for the development of learning algorithms that are more than just a proof of concept. In this paper, we report on our experiences and achievements in applying reinforcement learning (RL) methods in the scope of our Brainstormers competition team within the Simulation League of RoboCup during the past years","Machine learning,
Brain modeling,
Robot kinematics,
Computational intelligence,
Computational modeling,
Intelligent robots,
Mathematics,
Computer science,
Cognitive science,
Computer simulation"
A generalized synchronization theorem for discrete-time chaos system with application in data encryption scheme,"A constructive theorem of generalized synchronization (GS) for discrete-time chaos system (DTCS) is introduced. Based on the theorem, one can design a GS driven DTCS via a driving chaotic DTCS. As an application, a GS DTCS is constructed based on the Lorenz three-dimensional chaotic map and the Theorem. Using the DTCS designs a encryption scheme with ""one-time pad"" function. This scheme is able successfully to encrypt and decrypt original information without any loss. The scheme is sensitive to the perturbations of the parameters and initial conditions of the DTCS. Any perturbations which are larger than 10-15 will make corresponding decryptions become impossible. The key space of the scheme is as large as 10204. The analysis of the key space, sensitivity of key parameters show that this scheme has sound security. Numerical simulations show that our scheme is effective to be used in secure communication.","Chaos,
Cryptography,
Chaotic communication,
Numerical simulation,
Application software,
Computer science,
Information security,
Nonlinear dynamical systems,
Nonlinear systems,
Systems engineering and theory"
A Discrete Differential Evolution Algorithm for the Total Earliness and Tardiness Penalties with a Common Due Date on a Single-Machine,"In this paper, a discrete differential evolution (DDE) algorithm is presented to solve the single machine total earliness and tardiness penalties with a common due date. A new binary swap mutation operator called Bswap is presented. In addition, the DDE algorithm is hybridized with a local search algorithm to further improve the performance of the DDE algorithm. The performance of the proposed DDE algorithm is tested on 280 benchmark instances ranging from 10 to 1000 jobs from the OR Library. The computational experiments showed that the proposed DDE algorithm has generated better results than those in the literature in terms of both solution quality and computational time","Optimal scheduling,
Single machine scheduling,
Scheduling algorithm,
Genetic algorithms,
Computational intelligence,
Processor scheduling,
Genetic mutations,
Benchmark testing,
Libraries,
Heuristic algorithms"
Remote Control Laboratory with Moodle Booking System,"In order to enhance learning of control design, minimise the gap between theory and practice and especially to support learning by doing we built remote control laboratory. This paper describes a Moodle based booking system for our remote laboratory. The web-based remote laboratory is presented through a RC oscillator experiment for learning of control design. In RC oscillator experiment, interactivity between the controller design parameters in Bode plot and Root Locus with real experiment has been implemented in order to support full visualisation of the controller design. An experiment for control of a mechatronic device, it is a DC motor, was implemented. Special safety measures built into the experiment are represented in the paper.","Control systems,
Radio control,
Control design,
Oscillators,
DC motors,
Remote laboratories,
Education,
Visualization,
Internet,
Licenses"
Rare Itemset Mining,"A pattern is a collection of events/features that occur together in a transaction database. Previous studies in the field are often dedicated to the problem of frequent pattern mining where only patterns that appear frequently in the input data are mined. As a result, patterns involving events/features that appear in few data sets are not captured. In some domains, such as the detection of computer attacks, fraudulent transactions in financial institutions, those patterns, also known as rare patterns, are more interesting than frequent patterns. We propose a framework to represent different categories of interesting patterns and then instantiate it to the specific case of rare patterns. Later on, we present a generic framework to mine patterns based on the Apriori approach. In this paper we are interested by the patterns composed of a set of items, also called itemsets. Thus, we instantiate the generalized Apriori framework to mine rare itemsets. The resulting approach is Apriori-like and the mine idea behind it is that if the itemset lattice representing the itemset space in classical Apriori approaches is traversed on a bottom-up manner, equivalent properties to the Apriori exploration of frequent itemsets are provided to mine rare itemsets. This include an anti-monotone property and a level- wise exploration of the itemset space. As demonstrated by our experiments, our approach is effective in identifying all rare itemsets and is more efficient than the existing approach.",
Multi-level tiling: M for the price of one,"Tiling is a widely used loop transformation for exposing/exploiting parallelism and data locality. High-performance implementations use multiple levels of tiling to exploit the hierarchy of parallelism and cache/register locality. Efficient generation of multi-level tiled code is essential for effective use of multi-level tiling. Parameterized tiled code, where tile sizes are not fixed but left as symbolic parameters can enable several dynamic and run-time optimizations. Previous solutions to multi-level tiled loop generation are limited to the case where tile sizes are fixed at compile time. We present an algorithm that can generate multi-level parameterized tiled loops at the same cost as generating single-level tiled loops. The efficiency of our method is demonstrated on several benchmarks. We also present a method-useful in register tiling-for separating partial and full tiles at any arbitrary level of tiling. The code generator we have implemented is available as an open source tool.","Parallel processing,
Tiles,
Registers,
Runtime,
Permission,
Milling machines,
Computer science,
Costs,
Computer languages,
Program processors"
Towards a Brain-Computer Interface for Dexterous Control of a Multi-Fingered Prosthetic Hand,"Advances in brain-computer interfaces (BCI) have enabled direct neural control of robotic and prosthetic devices. However, it remains unknown whether cortical signals can be decoded in real-time to replicate dexterous movements of individual fingers and the wrist. In this study, single unit activity from 115 task-related neurons in the primary motor cortex (Ml) of a trained rhesus monkey were recorded, as it performed individuated movements of the fingers and wrist of the right hand. Virtual multi-unit ensembles, or voxels, were created by randomly selecting contiguous subpopulations of these neurons. Non-linear hierarchical filters using artificial neural networks (ANNs) were designed to asynchronously decode the activity from multiple virtual ensembles, in real-time. The decoded output was then used to actuate individual fingers of a robotic hand. An average real-time decoding accuracy of greater than 95 % was achieved with all neurons from randomly placed voxels containing 48 neurons, and up to 80% with as few as 25 neurons. These results suggest that dexterous control of individual digits and wrist of a prosthetic hand can be achieved by real-time decoding of neuronal ensembles from the Ml hand area in primates.",
Iris localization via intensity gradient and recognition through bit planes,"Iris recognition is very hot topic in both research and practical applications. In this paper, a robust algorithm is proposed for iris localization and a very simple method is employed for feature extraction. Iris localization is the key step in iris recognition systems because all subsequent steps depend highly on its accuracy. The proposed algorithm utilizes important property of gradient of intensity level in the grey scale images (after converting the images into greyscale if not). Then iris is normalized into a dimensionless rectangular strip of size 128*512 pixels and different features are extracted based upon bit plane slicing of the strip to get binary iris code. ROC curves are also drawn for different features. Matching decision is based on accumulative sum of bitwise XOR of different iris codes. Experiments show that proposed localization algorithm is very effective. Results have been tabulated by evaluating the developed algorithm with 1000 eye images and recognition accuracy has reached up to 99.6%.",
Identifying energy-efficient concurrency levels using machine learning,"Multicore microprocessors have been largely motivated by the diminishing returns in performance and the increased power consumption of single-threaded ILP microprocessors. With the industry already shifting from multicore to many-core microprocessors, software developers must extract more thread-level parallelism from applications. Unfortunately, low power-efficiency and diminishing returns in performance remain major obstacles with many cores. Poor interaction between software and hardware, and bottlenecks in shared hardware structures often prevent scaling to many cores, even in applications where a high degree of parallelism is potentially available. In some cases, throwing additional cores at a problem may actually harm performance and increase power consumption. Better use of otherwise limitedly beneficial cores by software components such as hypervisors and operating systems can improve system-wide performance and reliability, even in cases where power consumption is not a main concern. In response to these observations, we evaluate an approach to throttle concurrency in parallel programs dynamically. We throttle concurrency to levels with higher predicted efficiency from both performance and energy standpoints, and we do so via machine learning, specifically artificial neural networks (ANNs). One advantage of using ANNs over similar techniques previously explored is that the training phase is greatly simplified, thereby reducing the burden on the end user. Using machine learning in the context of concurrency throttling is novel. We show that ANNs are effective for identifying energy-efficient concurrency levels in multithreaded scientific applications, and we do so using physical experimentation on a state-of-the-art quad-core Xeon platform.","Magnetic cores,
Program processors,
Concurrent computing,
Scalability,
Artificial neural networks,
Energy efficiency,
Benchmark testing"
daVinci Code: A Multi-Model Simulation and Analysis Tool for Multi-Body Systems,"This paper discusses the design and current capabilities of a new software tool, dVC, capable of simulating planar systems of bodies experiencing unilateral contacts with friction. Since different problems require different levels of accuracy, dVC provides user-selectable body types (rigid or locally-compliant), motion models (first-order, quasi-static, dynamic), and several state-of-the-art time-stepping methods. One can also choose to include friction between each body and the plane of motion. To support optimal and robust part design, dVC also allows on-the-fly changes to parameters of the geometric and physical models. The results obtained for three representative planar problems are presented: the design of a passive part-orienting device, the planning of a mesoscale assembly operation, and the design of a grasp strategy.","Analytical models,
Friction,
Computational modeling,
Computer simulation,
Software tools,
Solid modeling,
Legged locomotion,
Robotic assembly,
Libraries,
Robotics and automation"
Skeleton Based Parametric Solid Models: Ball B-Spline Curves,"This paper proposes a parametric solid representation of freeform tubular objects - Ball B-Spline Curves (BBSCs), which are skeleton based parametric solid model. Their fundamental properties, algorithms and modeling methods are investigated. BBSC directly defines objects in B-Spline function form (not a procedure method, like sweeping), that uses control ball instead of control point in B-Spline curve. BBSC describes not only every point inside 3D solid objects, but also provides its center curve in B-Spline form directly. So the representation is more flexible for modeling, manipulation and deformation.","Skeleton,
Solid modeling,
Spline,
Shape,
Surface topography,
Surface reconstruction,
Irrigation,
Educational institutions,
Information science,
Deformable models"
Delaunay Meshing of Isosurfaces,"We present an isosurface meshing algorithm, DelIso, based on the Delaunay refinement paradigm. This paradigm has been successfully applied to mesh a variety of domains with guarantees for topology, geometry, mesh gradedness, and triangle shape. A restricted Delaunay tri- angulation, dual of the intersection between the surface and the three dimensional Voronoi diagram, is often the main ingredient in Delaunay refinement. Computing and storing three dimensional Voronoi/Delaunay diagrams become bottlenecks for Delaunay refinement techniques since isosurface computations generally have large input datasets and output meshes. A highlight of our algorithm is that we find a simple way to recover the restricted Delaunay triangulation of the surface without computing the full 3D structure. We employ techniques for efficient ray tracing of isosurfaces to generate surface sample points, and demonstrate the effectiveness of our implementation using a variety of volume datasets.",
An efficient framework for high-level power exploration,"Although SystemC is considered the most promising language for system-on-chip functional modeling, it doesn't come with power modeling capabilities. This work presents PowerSC, a novel power estimation framework which instruments SystemC for power characterization, modeling and estimation. Since it is entirely based on SystemC, PowerSC allows consistent power modeling from the highest to the lowest abstraction level. Besides, the framework's API provides facilities to integrate alternative modeling techniques, either at the same or at different abstraction levels. As a result, the required power evaluation infrastructure is reduced to a minimum: the standard SystemC library, the PowerSC library itself and a C++ compiler. Experimental results show both the effectiveness and the efficiency of our framework. On the one hand, two well-known macromodeling techniques were easily integrated into the framework, leading to acceptable average errors at the RT level. On the other hand, library characterization was more than 13times faster as compared to a typical industrial flow.",
"Diagnosis, Modeling and Tolerance of Scan Chain Hold-Time Violations","Errors in timing closure process during the physical design stage may result in systematic silicon failures, such as scan chain hold time violations, which prohibit the test of manufactured chips. In this paper, we propose a set of techniques that enable the accurate pinpointing of hold time violating scan cells, their modeling and tolerance, paving the way for the generation of valid test data that can be used to test chips with such systematic failures. The process yield is thus restored, as chips that are functional in mission mode can still be identified and shipped out, despite the existence of scan chain hold time failures. The techniques that we propose are non-intrusive, as they utilize only basic scan capabilities, and thus impose no design changes. Scan cells with hold time violations can be identified with maximal possible resolution, enabling the incorporation of the associated impact during the ATPG process and thus the generation of valid test data for the chips with such systematic failures","Timing,
System testing,
Manufacturing processes,
Circuit testing,
Virtual manufacturing,
Automatic test pattern generation,
Silicon,
Degradation,
Computer science,
Computer errors"
Rate Control for Hierarchical B-picture Coding with Scaling-factors,"The coding performance can be further improved when the hierarchical B-picture coding is introduced into H.264/AVC. However, the existing rate control schemes can not work efficiently in such new coding framework. This paper proposes a novel rate control algorithm when hierarchical B-picture coding is used in H.264/AVC. Firstly, a set of scaling-factors applied in designing cascaded quantizer for the B frames at different temporal levels is introduced. Based on the designed scaling-factors, an efficient bit-allocation strategy for hierarchical B-picture coding is presented. The experiments show that the proposed rate control algorithm can further improve PSNR up to 0.7dB compared to the existing hierarchical B-picture coding in H.264/AVC, while the mismatch of target bit rate and real bit rate does not exceed 2%.",
Multiscale Sparse Image Representationwith Learned Dictionaries,"This paper introduces a new framework for learning multiscale sparse representations of natural images with overcomplete dictionaries. Our work extends the K-SVD algorithm [1], which learns sparse single-scale dictionaries for natural images. Recent work has shown that the K-SVD can lead to state-of-the-art image restoration results [2, 3]. We show that these are further improved with a multi-scale approach, based on a Quadtree decomposition. Our framework provides an alternative to multiscale pre-defined dictionaries such as wavelets, curvelets, and contourlets, with dictionaries optimized for the data and application instead of pre-modelled ones.",
R-MAC: Reservation Medium Access Control Protocol for Wireless Sensor Networks,"Energy consumption is a critical issue in wireless sensor networks as the battery of a sensor node, in most cases, cannot be recharged or replaced after deployment. In order to detect an event, a sensor node spends most of the time in monitoring its environment, during which a significant amount of energy can be saved by placing the radio in the low power sleep mode when no reception and/or transmission of data is involved. In this paper, we discuss the design of a new MAC protocol for wireless sensor networks, which mainly avoids overhearing, collisions, and frequent commutation between sleep and active modes. These issues are generally considered to be the most important reasons behind energy waste in heavy loaded conditions of wireless sensor networks. The proposed protocol, called Reservation-MAC (R-MAC), uses two separate periods during the communication process. In the first period, nodes compete for time slots reservation for their future transmissions, and in the second period, each node transmits its data or receive data from a corresponding sender. Once a node is aware of its transmission and/or reception time slot, it stays active only for these time slots and goes back to the sleep mode during the remaining time of the transmission period. In our experiments, the performance of the R-MAC protocol is studied in saturated conditions and compared with the well known S-MAC and T- MAC protocols. Depending on the traffic load, the proposed MAC protocol significantly improves the energy consumption compared to S-MAC and T-MAC.","Media Access Protocol,
Wireless application protocol,
Access protocols,
Wireless sensor networks,
Sensor phenomena and characterization,
Energy consumption,
Communication system control,
Network topology,
Monitoring,
Biosensors"
Deception in Honeynets: A Game-Theoretic Analysis,"Recently, honeynets became one of the main tools for understanding the characteristics of malicious attacks and the behavior of the attackers. However the attackers may identify the honeypots and avoid attacking them. Thus the honeynet administrators must be able to deceive the attackers and induce them to attack the honeypots. In this paper we propose a game theoretic framework for modeling deception in honeynets. The framework is based on extensive games of imperfect information. We study the equilibrium solutions of these games and show how they are used to determine the strategies of the attacker and the honeynet system.","Monitoring,
Game theory,
Probes,
Large-scale systems,
Computer worms,
Computational modeling,
Computer simulation,
Conferences,
Information analysis,
Scalability"
A hybrid approach for complete motion planning,"We present an efficient algorithm for complete motion planning that combines approximate cell decomposition (ACD) with probabilistic roadmaps (PRM). Our approach uses ACD to subdivide the configuration space into cells and computes localized roadmaps by generating samples within these cells. We augment the connectivity graph for adjacent cells in ACD with pseudo-free edges that are computed based on localized roadmaps. These roadmaps are used to capture the connectivity of free space and guide the adaptive subdivision algorithm. At the same time, we use cell decomposition to check for path non-existence and generate samples in narrow passages. Overall, our hybrid algorithm combines the efficiency of PRM methods with the completeness of ACD-based algorithms. We have implemented our algorithm on 3-DOF and 4-DOF robots. We demonstrate its performance on planning scenarios with narrow passages or no collision-free paths. In practice, we observe up to 10 times improvement in performance over prior complete motion planning algorithms.","Motion planning,
Orbital robotics,
USA Councils,
Computer science,
Path planning,
Intelligent robots,
Layout,
Shape,
Labeling"
Stabilization of bilinear systems via linear state feedback control,"In this paper we consider the problem of stabilizing a bilinear system via linear state feedback control. A procedure is proposed which, given a polytope V surrounding the origin of the state space, finds, if existing, a controller in the form u = Kx, such that the zero equilibrium point of the closed loop system is asymptotically stable and V is enclosed into the domain of attraction of the equilibrium. The controller design requires the solution of a convex optimization problem involving linear matrix inequalities. An example illustrates the applicability of the proposed technique.","Nonlinear systems,
State feedback,
Linear feedback control systems,
Control systems,
Power system modeling,
Sliding mode control,
State-space methods,
Closed loop systems,
Linear matrix inequalities,
Biological system modeling"
A Hybrid Control Strategy for Robust Contact Detection and Force Regulation,"We present an innovative hybrid control strategy for contact detection and force regulation of robotic manipulators. This hybrid architecture controls the robotic manipulator during the following stages of interaction with the work environment: the free motion, the transition phase, and the constrained motion. The proposed control strategy is to switch between a position and a force controller with hysteresis relying only on contact force measurements. We implement this strategy in a hybrid controller and provide a design procedure which depends on the viscoelastic parameters of the work environment. Our controller guarantees contact detection and force regulation without bounce-off effects between the robotic manipulator and the work environment from compact sets of initial conditions. Additionally, the resulting closed-loop system is robust to measurement noise. We include simulations that show how the proposed hybrid control strategy guarantees good performance in the cases of stiff and compliant work environments, and in the presence of measurement noise.","Force control,
Robust control,
Contacts,
Robots,
Manipulators,
Switches,
Force measurement,
Noise measurement,
Working environment noise,
Motion control"
Data-Purpose Algebra: Modeling Data Usage Policies,"Data is often encumbered by restrictions on the ways in which it may be used. These restrictions on usage may be determined by statute, by contract, by custom, or by common decency, and they are used to control collection of data, diffusion of data, and the inferences that can be made over the data. In this paper, we present a data-purpose algebra that can be used to model these kinds of restrictions in various different domains. We demonstrate the utility of our approach by modeling part of the Privacy Act (5 USC xi552a)1, which states that data collected about US citizens can be used only for the purposes for which it was collected. We show (i) how this part of the Privacy act can be represented as a set of restrictions on data usage, (ii) how the authorized purposes of data flowing through different government agencies can be calculated, and (iii) how these purposes can be used to determine whether the Privacy Act is being enforced appropriately.","Algebra,
Data privacy,
Contracts,
Government,
Computer science,
Artificial intelligence,
Laboratories,
Monitoring,
Credit cards,
Databases"
ORIGAMI: Mining Representative Orthogonal Graph Patterns,"In this paper, we introduce the concept of alpha-orthogonal patterns to mine a representative set of graph patterns. Intuitively, two graph patterns are alpha-orthogonal if their similarity is bounded above by alpha. Each alpha-orthogonal pattern is also a representative for those patterns that are at least beta similar to it. Given user defined alpha, beta isin [0,1], the goal is to mine an alpha-orthogonal, beta-representative set that minimizes the set of unrepresented patterns. We present ORIGAMI, an effective algorithm for mining the set of representative orthogonal patterns. ORIGAMI first uses a randomized algorithm to randomly traverse the pattern space, seeking previously unexplored regions, to return a set of maximal patterns. ORIGAMI then extracts an alpha-orthogonal, beta-representative set from the mined maximal patterns. We show the effectiveness of our algorithm on a number of real and synthetic datasets. In particular, we show that our method is able to extract high quality patterns even in cases where existing enumerative graph mining methods fail to do so.","Proteins,
Data mining,
Social network services,
Databases,
Chaos,
Pattern analysis,
Computer science,
IP networks,
Web sites,
Blogs"
A Wireless Medical Monitoring Over a Heterogeneous Sensor Network,"This paper presents a heterogeneous sensor network system that has the capability to monitor physiological parameters from multiple patient bodies by means of different communication standards. The system uses the recently opened medical band called MICS (medical implant communication service) between the sensor nodes and a remote central control unit (CCU) that behaves as a base station. The CCU communicates with another network standard (the Internet or a mobile network) for a long distance data transfer. The proposed system offers mobility to patients and flexibility to medical staff to obtain patient's physiological data on demand basis via Internet. A prototype sensor network including hardware, firmware and software designs has been implemented and tested by incorporating temperature and pulse rate sensors on nodes. The developed system has been optimized for power consumption by having the nodes sleep when there is no communication via a bidirectional communication.","Wireless sensor networks,
Biomedical monitoring,
Temperature sensors,
Patient monitoring,
Sensor systems,
Communication standards,
Condition monitoring,
Remote monitoring,
Microwave integrated circuits,
Implants"
Multi-state decoding of point-and-click control signals from motor cortical activity in a human with tetraplegia,"Basic neural prosthetic control of a computer cursor has been recently demonstrated by Hochberg et al. (2006) using the BrainGate system (Cyberkinetics Neurotechnology Systems, Inc.). While these results demonstrate the feasibility of intracortically-driven prostheses for humans with paralysis, a practical cursor-based computer interface requires more precise cursor control and the ability to ""click"" on areas of interest. Here we present the first practical point and click device that decodes both continuous states (e.g. cursor kinematics) and discrete states (e.g. click states) from a single neural population in human motor cortex. We describe a probabilistic multi-state decoder and the necessary training paradigms that enable point and click cursor control by a human with tetraplegia using an implanted microelectrode array. We present results from multiple recording sessions and quantify the point and click performance","Decoding,
Humans,
Neurons,
Neuroscience,
Control systems,
Research and development,
Neural prosthesis,
Velocity control,
Collaborative work,
Hidden Markov models"
Automatic White Balance with Color Temperature Estimation,"This paper discusses automatic white balance in digital camera. Auto white balance is the most important image processing step in the image pipeline of digital camera, since it controls the color appearance of the image.","Chromium,
Temperature distribution,
Digital cameras,
Pipelines,
Color,
Temperature sensors,
Voting,
Light sources,
Lighting,
Equations"
"Road Sign Detection and Recognition using Colour Segmentation, Shape Analysis and Template Matching","The paper presents a system for detection and recognition of road signs with red boundaries and black symbols inside. The detection is invariant to varying lighting conditions and shadows. The algorithm is tested on RGB images taken from camera. These images are converted to HSV colour space. Colour segmentation for red regions is applied on the whole image. All red regions are labeled forming objects. Each of the red objects is tested for its shape. Final image contains only those red objects which are triangular or circular in shape. Finding the black regions within the accepted red objects area, results in extraction of pictogram. This pictogram is then matched with templates in database, hence recognizing the meaning of road sign. The paper presents a revised edition of fuzzy shape detector and a recognition module that uses template matching to recognize rotated and affine transformed road signs.","Color,
Shape,
Roads,
Testing,
Image segmentation,
Image databases,
Detectors,
Machine learning,
Cybernetics,
Java"
GDClust: A Graph-Based Document Clustering Technique,"This paper introduces a new technique of document clustering based on frequent senses. The proposed system, GDClust (graph-based document clustering) works with frequent senses rather than frequent keywords used in traditional text mining techniques. GDClust presents text documents as hierarchical document-graphs and utilizes an apriori paradigm to find the frequent subgraphs, which reflect frequent senses. Discovered frequent subgraphs are then utilized to generate sense-based document clusters. We propose a novel multilevel Gaussian minimum support approach for candidate subgraph generation. GDClust utilizes English language ontology to construct document-graphs and exploits graph-based data mining technique for sense discovery and clustering. It is an automated system and requires minimal human interaction for the clustering purpose.","Data mining,
Humans,
Ontologies,
Books,
Association rules,
Clustering algorithms,
Chemical analysis,
Chemical technology,
Conferences,
Computer science"
High-available grid services through the use of virtualized clustering,"Grid applications comprise several components and web-services that make them highly prone to the occurrence of transient software failures and aging problems. This type of failures often incur in undesired performance levels and unexpected partial crashes. In this paper we present a technique that offers high-availability for Grid services based on concepts like virtualization, clustering and software rejuvenation. To show the effectiveness of our approach, we have conducted some experiments with OGSA-DAI middleware. One of the implementations of OGSA-DAI makes use of use of Apache Axis V1.2.1, a SOAP implementation that suffers from severe memory leaks. Without changing any bit of the middleware layer we have been able to anticipate most of the problems caused by those leaks and to increase the overall availability of the OGSA-DAI Application Server. Although these results are tightly related with this middleware it should be noted that our technique is neutral and can be applied to any other Grid service that is supposed to be high-available.",
Model Predictive Control for Constrained Discrete Time Systems: An Optimal Perturbation Analysis Approach,"In the paper we consider a model predictive control (MPC) strategy based on approximating the MPC optimal control solution by a nominal solution (either pre-computed in advance on-line or computed off-line) and a perturbation solution, which is easily computable and corrects the nominal solution. Unlike similar approaches in prior literature (see e.g., (A.E. Bryson and Y. Ho, 1975), (F.L. Lewis, 1986)), our approach is capable of enforcing piecewise state and control constraints. We describe how the basic approach can be integrated within the framework of forecasting MPC (FMPC) and illustrate its effectiveness by means of an example.","Predictive models,
Predictive control,
Discrete time systems,
Optimal control,
Chemical industry,
Sun,
Electrical equipment industry,
Industrial control,
Linear approximation,
Cities and towns"
Dynamic Reconfigurable Testing of Service-Oriented Architecture,"SOA (service-oriented architecture) presents unique requirements and challenges for testing. Dynamic reconfiguration in SOA software means that testing need to be adaptive to the changes of the service-oriented applications at runtime. This paper presents a ConfigTest approach to enable the online change of test organization, test scheduling, test deployment, test case binding, and service binding. ConfigTest is based on our previous research on the MAST (multi-agents-based service testing) framework. It extends MAST with a new test broker architecture, configuration management and event-based subscription/notification mechanism. The test broker decouples test case definition from its implementation and usage. It also decouples the testing system from the services under test. With the configuration management, ConfigTest allows the test agents to bind dynamically to each other and build up their collaborations at runtime. The event mechanism enables that a change in one test artifact can be notified to all the others which subscribe their interests to the change event. This paper presents and analyzes the collaboration diagrams of various testing reconfiguration scenarios and illustrates the ConfigTest approach with an example of service-based book ordering system.",
MultiRelational k-Anonymity,"k-anonymity protects privacy by ensuring that data cannot be linked to a single individual. In a k-anonymous dataset, any identifying information occurs in at least k tuples. Much research has been done to modify a single table dataset to satisfy anonymity constraints. This paper extends the definitions of k-anonymity to multiple relations and shows that previously proposed methodologies either fail to protect privacy, or overly reduce the utility of the data, in a multiple relation setting. A new clustering algorithm is proposed to achieve multirelational anonymity.","Protection,
Data privacy,
Clustering algorithms,
Databases,
Couplings,
Books"
Voice over Internet Protocol on Mobile Devices,"Voice over internet protocol (VoIP) is a way to carry out a telephone conversation over a data network. VoIP products promise converged telecommunications and data services that are cheaper, more versatile and provide good voice quality as compared to traditional offerings. Although VoIP is widely used, VoIP on mobile devices is still in its infancy. Currently, there are a number of VoIP solutions for mobile phones. However, VoIP solutions developed using Java 2 platform micro edition(J2ME) are not available. Java based solutions are widely compatible with many devices. In this paper, strong focus has been granted to cross-device compatibility through the use of the widely supported J2ME framework. The implementation details of VoIP client using J2ME are illustrated.","Internet telephony,
Codecs,
Web server,
Java,
Transport protocols,
Analog-digital conversion,
Australia,
Mobile handsets,
Ground penetrating radar,
Network servers"
DOI measurement with monolithic scintillation crystals: A primary performance evaluation,We report a first assessment of image quality enhancement achieved by the implementation of depth of interaction detection with monolithic crystals. The method of interaction depth measurement is based on analogue computation of the standard deviation with an enhanced charge divider readout. This technique of depth of interaction detection was developed in order to provide fast and determination of this parameter at a reasonable increase of detector cost. The detector consists of an large-sized monolithic scintillator coupled to a position sensitive photomultiplier tube. A special design feature is the flat-topped pyramidal shape of the crystal. This reduces image compression near the edges of the scintillator. We studied the image enhancement qualitatively with a FDG filled hot spot phantom and quantitatively by displacing a single point source along a radial axis. An important uniformity improvement was observed for the reconstructed image of the hot spot phantom when depth of interaction correction was applied. A moderate improvement of the spatial resolution was observed when reconstructing the images of the point source with depth of interaction correction.,
Emergency alleviation of thermal overloads using model predictive control,"An approach inspired of Model Predictive Control is proposed to determine a sequence of control actions aimed at alleviating thermal overloads in emergency conditions. The algorithm brings the line currents below their limits in the time interval left by protections, while accounting for constraints on control changes at each step. Its closed-loop nature allows to compensate for measurement noise and model uncertainties.","Predictive models,
Predictive control,
Protection,
Control systems,
Open loop systems,
Noise measurement,
Transformers,
Power system modeling,
Conducting materials,
Constraint optimization"
A Tradeoff between Energy and Bandwidth Efficiency in Wireless Networks,"In this paper, we study the bandwidth efficiency and energy efficiency of wireless ad hoc networks in which the energy supply of nodes and bandwidth are the primary resource constraints. For bandwidth efficiency, we consider the total number of channel uses between source and destination node. Also, energy consumption of the receiver in order to process each coded bit is considered as well as the wasted energy of a nonideal amplifier in the transmitter. With the location of relaying nodes, we investigate the impact on bandwidth efficiency and energy efficiency for both common rate and common power Schemes. All nodes in a network operate in half-duplex mode, hence more channel uses are required in a multihop transmission. This leads to a loss in bandwidth efficiency but a potential gain in energy efficiency because each node can save its transmitting power. We investigate this tradeoff between energy efficiency and bandwidth efficiency of the network similar in nature to that found by Shannon for a single link. We also develop performance characterizations in terms of transport efficiency, which combines bandwidth efficiency and energy efficiency into a single metric. It is shown that equidistant relaying cases perform better than non-equidistant relaying cases for either common rate or common power schemes. For non-equidistant relaying cases, it is also shown that common power schemes achieve a higher bandwidth efficiency than do common rate schemes, whereas energy efficiency depends on the received signal-to-noise ratio at the destination node.","Bandwidth,
Wireless networks,
Energy efficiency,
Relays,
Mobile ad hoc networks,
Energy consumption,
Transmitters,
Spread spectrum communication,
Propagation losses,
Potential energy"
Essential Dimensions of Latent Semantic Indexing (LSI),"Latent semantic indexing (LSI) is commonly used to match queries to documents in information retrieval applications. LSI has been shown to improve retrieval performance for some, but not all, collections, when compared to traditional vector space retrieval. In this paper, we first develop a model for understanding which values in the reduced dimensional space contain the term relationship (latent semantic) information. We then test this model by developing a modified version of LSI that captures this information, essential dimensions of LSI (EDLSI). EDLSI significantly improves retrieval performance on corpora that previously did not benefit from LSI, and offers improved runtime performance when compared with traditional LSI. Traditional LSI requires the use of a dimensionality reduction parameter which must be tuned for each collection. Applying our model, we have also shown that a small, fixed dimensionality reduction parameter (k=10) can be used to capture the term relationship information in a corpus","Indexing,
Large scale integration,
Information retrieval,
Educational institutions,
Testing,
Training data,
Mathematics,
Computer science,
Application software,
Runtime"
Ground truth evaluation of large urban 6D SLAM,"In the past many solutions for simultaneous localization and mapping (SLAM) have been presented. Recently these solutions have been extended to map large environments with six degrees of freedom (DoF) poses. To demonstrate the capabilities of these SLAM algorithms it is common practice to present the generated maps and successful loop closing. Unfortunately there is often no objective performance metric that allows to compare different approaches. This fact is attributed to the lack of ground truth data. For this reason we present a novel method that is able to generate this ground truth data based on reference maps. Further on, the resulting reference path is used to measure the absolute performance of different 6D SLAM algorithms building a large urban outdoor map.","Simultaneous localization and mapping,
Measurement,
Mobile robots,
Robot kinematics,
Robot sensing systems,
Computer science,
Intelligent robots,
USA Councils,
Optimization methods"
Design of a spring backbone micro endoscope,"This work introduces a modified endoscope structure that employs a spring as its backbone. This design allows backdrivability of the mechanism due to the flexibility of the spring structure. Furthermore, the full 3 DOF motion including compression and two rotational motions can be realized. The geometric analysis, kinematic modeling, and actuator sizing for this device are conducted. Both simulation and experiment have been performed to show the effectiveness of this device.",
Representing and Characterizing Handwritten Mathematical Symbols through Succinct Functional Approximation,"We model on-line ink traces for a set of 219 symbols to ""best fit"" low-degree polynomial series. Using a collection of mathematical writing samples, we find that in many cases this provides a succinct way to model the stylus movements of actual test users. Furthermore, even without further similarity-processing, the polynomial coefficients from the writing samples form clusters which often contain the same character as written by different users. We find this style of characterization to be an attractive tool due to the suitability of the representation to computation and mathematical analysis.",
Elimination of Harmonics in a Five-Level Diode-Clamped Multilevel Inverter Using Fundamental Modulation,"In this study, elimination of harmonics in a five- level diode-clamped multilevel inverter (DCMLI) has been implemented by using fundamental modulation switching. The proposed method eliminates harmonics by generating negative harmonics with switching angles calculated for selective harmonic elimination method. In order to confirm the proposed method, first Matlab/Simulink and PSIM simulation results are given. Then the proposed method is also validated by experiments with Opal-RT controller and a 10 kW three- phase, five-level DCMLI prototype.",
Cognitive Technology for improving Ultra-Wideband (UWB) Coexistence,"Cognitive radio technology enables the opportunistic operation of secondary devices in frequency bands allocated to primary users. In this paper we explore how this technology can enable ultra-wideband (UWB) systems to coexist with primary users. The distinguishing aspect of cognitive radio technology is the ability to detect and avoid primary users. We discuss two options for detecting the presence of primary devices - energy detection and preamble detection. The presence of multiple UWB devices can aid detection by enabling cooperative sensing of the primary. We analyze various techniques for cooperative sensing which differ in the amount of information they need to exchange between radios and the regime in which they are potentially advantageous. Furthermore, the wideband aspect of UWB offers many challenges to detection but also facilitates the detection process in several ways. These distinguishing aspects of wideband detection are also highlighted in this paper.",
Lane Reservation for Highways (Position Paper),"The only way to keep up with the ever-increasing number of cars on roads is through constant change and improvement in the transportation infrastructure. Construction of new roads is constrained by space and financial resources. Therefore, there is a need to devise ways to make optimal use of the existing infrastructure. In this position paper, we describe a lane reservation system for highways. The idea is to allow drivers to reserve a slot on a high-priority lane by paying a premium price. The high-priority lane would provide congestion free travel between any two points on the highway. We describe the design of our system, the challenges that need to be solved and the evaluation methodology we are planning to adopt.","Road transportation,
Intelligent transportation systems,
Computer science,
Intelligent systems,
Mobile communication,
Computer network management,
Communication system traffic control,
Rails,
Automobiles,
USA Councils"
A Light-weight Framework for Hosting Web Services on Mobile Devices,"In the ubiquitous era, for Web services to become a universal communication paradigm, mobile devices enabled with Web services should be considered as an equal participant of the service-oriented architecture. Here, mobile devices play the role of clients, providers, or even brokers. To establish a distributed application framework on a P2P network environment, this paper presents a light-weight framework for hosting Web services on mobile devices. The proposed framework contains several built-in functionalities such as the processing of SOAP messages, the execution and migration of services, the management of context and service directory, and the publishing and discovery of services. To evaluate the performance of the proposed mobile Web service framework, a real-world scenario has been tested on physical devices connected by Bluetooth.","Web services,
Context-aware services,
Costs,
Mobile computing,
Simple object access protocol,
Middleware,
Availability,
Computer science,
Mobile communication,
Service oriented architecture"
Short-term Traffic Flow Prediction Based on Incremental Support Vector Regression,"In this paper, a new short-term traffic flow prediction model and method based on incremental support vector regression (ISVR) is proposed, according to the data collected sequentially by the probe vehicle or loop detectors, which can update the prediction function in real time via incremental learning way. As a result, it is fitter for the real engineering application. The ISVR model was tested by using the 1-880 database, and the result shows that this model is superior to the back-propagation neural network (BPNN) model.","Traffic control,
Predictive models,
Mathematical model,
Support vector machines,
Telecommunication traffic,
Vehicle detection,
Neural networks,
Educational institutions,
Computer science,
Least squares methods"
POSANT: A Position Based Ant Colony Routing Algorithm for Mobile Ad-hoc Networks,"Availability of cheap positioning instruments like GPS receivers makes it possible for routing algorithms to use the position of nodes in an ad hoc mobile network. Regular position based routing algorithms fail to find a route from a source to a destination in some cases when the network contains nodes with irregular transmission ranges or they find a route that is much longer than the shortest path. On the other hand, routing algorithms based on ant colony optimization find routing paths that are close to the shortest paths even if the nodes in the network have different transmission ranges. The drawback of these algorithms is the large number of messages that needs to be sent or the long delay before the routes are established. In this paper we propose POSANT, a reactive routing algorithm for mobile ad hoc networks which combines the idea of ant colony optimization with information about the position of nodes. Our simulations show that POSANT has a shorter route establishment time while using a smaller number of control messages than other ant colony routing algorithms.",
Invariant Road Sign Recognition with Fuzzy ARTMAP and Zernike Moments,"In this paper, a novel approach to recognize road signs is developed. Images of road signs are collected by a digital camera mounted in a vehicle. They are segmented using colour information and all objects which represent signs are extracted, normalized to 36times36 pixels, and used to train a fuzzy ARTMAP neural network by calculating Zernike moments for these objects as features. Sign borders and pictograms are investigated in this study. Zernike moments of sign borders and speed-limit signs of 210 and 150 images are calculated as features. A fuzzy ARTMAP is trained directly with features, or by using PCA for dimension reduction, or by using LDA algorithm as dimension reduction and data separation algorithm. Two fuzzy ARTMAP Neural Networks are trained. The first NN determines the class of the sign from the shape of its border and the second one determines the sign itself from its pictogram. Training and testing of both NNs is done offline by using still images. In the online mode, the system loads the fuzzy ARTMAP neural networks, and performs recognition process. An accuracy of about 99% is achieved in sign border recognition and 96% for Speed-Limit recognition.",
Hierarchical Verification of Galois Field Circuits,"This paper proposes a hierarchical method for the formal hardware verification of Galois field architecture circuits. The reduced ordered functional decision diagram has been explored. The proposed method has been found to lead to significant gains in time and space, depending on the resources that are available. The theoretical claims that were made have been supported by experiments.","Galois fields,
Circuits,
Boolean functions,
Arithmetic,
Data structures,
Formal verification,
Computer science,
Hardware,
Cryptography,
Digital signal processing"
An Abstract Workflow-Based Framework for Testing Composed Web Services,"Testing web services impose many challenges to existing testing methods, techniques, and tools; especially those available to traditional applications. Composed web services increase these challenges by requiring additional validation and verification efforts. Structural-based testing approaches have been thoroughly researched for traditional applications; however, they have not yet been examined, as a methodology, for testing composed web services. In this work, we introduce a formal model for an abstract-based workflow framework that can be used to capture a composed web service under test. We then define a set of applicable structural-based testing criteria to the framework. Finally we outline a promising line of testing criteria that can be applied to this framework.","Web services,
Simple object access protocol,
Automatic testing,
Uniform resource locators,
Collaborative work,
Semantic Web,
Computer science,
Application software,
Logic,
Packaging"
Towards An Automated Multiagent Taxi-Dispatch System,"This paper presents the study of a novel approach towards an automated taxi dispatch system that handles current bookings in a distributed fashion. Existing systems in use by taxi operators in Singapore attempt to increase customer satisfaction locally, by sequentially dispatching nearby taxis to service customers. The proposed dispatch system attempts to increase customer satisfaction more globally, by concurrently dispatching multiple taxis to the same number of customers in the same geographical region, and vis-a-vis human driver satisfaction. To realize the system, we propose a multiagent architecture, populated with software collaborative agents that can actively negotiate on behalf of taxi drivers in groups of size N for available customer bookings. The operational efficiency of the existing and proposed dispatch systems was evaluated through computer simulations on MITSIMLab, an existing simulation-based laboratory originally developed for evaluating traffic management system designs at the operational level. The empirical results, obtained for a 1000-strong taxi fleet over a discrete range of N, show that the proposed system can dispatch taxis with up to 33.1% and 26.3% reduction in customer waiting time and empty taxi cruising time, respectively.",
Using Neuro-Fuzzy Approach to Reduce False Positive Alerts,One of the major problems of Intrusion Detection Systems (IDS) at the present is the high rate of false alerts that the systems produce. These alerts cause problems to human analysts to repeatedly and intensively analyze the false alerts to initiate appropriate actions. We demonstrate the advantages of using a hybrid neuro-fuzzy approach to reduce the number of false alarms. The neuro-fuzzy approach was experimented with different background knowledge sets in DARPA 1999 network traffic dataset. The approach was evaluated and compared with RIPPER algorithm. The results shows that the neuro- fuzzy approach significantly reduces the number of false alarms more than the RIPPER algorithm and requires less background knowledge sets.,"Intrusion detection,
Neural networks,
Humans,
Telecommunication traffic,
Machine learning,
Multi-layer neural network,
Fuzzy systems,
Computer science,
Data security,
Learning systems"
Modeling End-to-End Delay Using Pareto Distribution,"Delay is one of the network performance parameters that are often measured using passive or active techniques along with packet loss, bandwidth, etc. If used appropriately, these parameters can indicate performance status of the network, and they can be used in fault and performance management, network provisioning, traffic engineering, and performance prediction. However, it is difficult to extract sufficient information from original measurement data to derive precise results. In this paper, we show that the PDF (probability distribution function) of delay data can indicate network load situation. We also show that Pareto distribution can model end-to-end delay appropriately in a statistical manner and demonstrate some statistical characteristics of end-to-end delay using Pareto distribution.",
estMax: Tracing Maximal Frequent Itemsets over Online Data Streams,"In general, the number of frequent itemsets in a data set is very large. In order to represent them in more compact notation, closed or maximal frequent itemsets (MFIs) are used. However, the characteristics of a data stream make such a task be more difficult. For this purpose, this paper proposes a method called estMax that can trace the set of MFIs over a data stream. The proposed method maintains the set of frequent itemsets by a prefix tree and extracts all of MFIs without any additional superset/subset checking mechanism. Upon processing a newly generated transaction, its longest matched frequent itemsets are marked in a prefix tree as candidates for MFIs. At the same time, if any subset of these newly marked itemsets has been already marked as a candidate MFI, it is cleared as well. By employing this additional step, it is possible to extract the set of MFIs at any moment. The performance of the proposed method is comparatively analyzed by a series of experiments to identify its various characteristics.","Itemsets,
Data mining,
Computer science,
Data analysis,
Performance analysis"
E-Government Evaluation: Reflections On Three Organisational Case Studies,The deployment of e-government continues at a significant cost and pace in the worldwide public sector. An important area of research is that of the evaluation of e-government. In this paper the authors report the findings from three interpretive in-depth organisational case studies that explore e-government evaluation within UK public sector settings. The paper elicits insights to organisational and managerial aspects with the aim of improving knowledge and understanding of e-government evaluation. The findings that are extrapolated from the analysis of the three case studies are classified and mapped onto a tentative e-government evaluation framework and presented in terms lessons learnt. These aim to inform theory and improve e-government evaluation practice. The paper concludes that e-government evaluation is an under developed area and calls for senior executives to engage more with the e-government agenda and commission e-government evaluation exercises to improve evaluation practice,"Electronic government,
Reflection,
Investments,
Economics,
Computer aided software engineering,
Councils,
Information systems,
Knowledge management,
Cost benefit analysis,
Performance analysis"
Locating Brain Tumors from MR Imagery Using Symmetry,"Tumor/abnormality segmentation from magnetic resonance imagery (MRI) can play a significant role in cancer research and clinical practice. Although accurate tumor segmentation by radiologists is ideal, it is extremely tedious. Experience shows that for MRI database indexing purposes approximate segmentations can be adequate. In this paper, we propose a straightforward, real-time technique to find a bounding box around the brain abnormality in an MR image. Our algorithm exploits left-to-right symmetry of the brain structure. The proposed detection algorithm can play a useful role in indexing and storage of bulk MRI data, as well as provide an initial step or seed to assist algorithms designed to find accurate tumor boundaries.","Neoplasms,
Image segmentation,
Magnetic resonance imaging,
Indexing,
Magnetic resonance,
Cancer,
Image databases,
Brain,
Detection algorithms,
Bulk storage"
Interscopic User Interface Concepts for Fish Tank Virtual Reality Systems,"In this paper we introduce new user interface concepts for fish tank virtual reality (VR) systems based on autostereoscopic (AS) display technologies. Such AS displays allow to view stereoscopic content without requiring special glasses. Unfortunately, until now simultaneous monoscopic and stereoscopic display was not possible. Hence prior work on fish tank VR systems focussed either on 2D or 3D interactions. In this paper we introduce so called interscopic interaction concepts providing an improved working experience, which enable great potentials in terms of the interaction between 2D elements, which may be displayed either in monoscopic or stereoscopic, e.g., GUI items, and the 3D virtual environment usually displayed stereoscopically. We present a framework which is based on a software layer between the operating system and its graphical user interface supporting the display of both mono- as well as stereoscopic content in arbitrary regions of an autostereoscopic display. The proposed concepts open up new vistas for the interaction in environments where essential parts of the GUI are displayed monoscopically and other parts are rendered stereoscopically. We address some essential issues of such fish tank VR systems and introduce intuitive interaction concepts which we have realized","User interfaces,
Marine animals,
Virtual reality,
Graphical user interfaces,
Computer displays,
Mice,
Visualization,
Glass,
Virtual environment,
Costs"
CA-RAM: A High-Performance Memory Substrate for Search-Intensive Applications,"This paper proposes a specialized memory structure called CA-RAM (content addressable random access memory) to accelerate search operations present in many important real-world applications. Search operations can occupy a significant portion of total execution time and energy consumption, while posing a difficult performance problem to tackle using traditional memory hierarchy concepts. In essence, CA-RAM is a direct hardware implementation of the well-known hashing technique. Searchable records are stored in CA-RAM at a location determined by a hash function, defined on their search key. After a database has been built, looking up a record in CA-RAM typically involves a single memory access followed by a parallel key matching operation. Compared with a conventional CAM (content addressable memory) solution, CA-RAM capitalizes on dense SRAM and DRAM designs, and achieves comparable search performance while occupying much smaller area and consuming significantly less power. This paper presents detailed design aspects of CA-RAM, to be integrated in future general-purpose and application-specific processors and systems. To further motivate and justify our approach, we present two real examples of using CA-RAM to build a high-performance search accelerator targeting: IP address lookup in core routers and trigram lookup in a large speech recognition system",
Improving the Fault Resilience of Overlay Multicast for Media Streaming,"A key technical challenge for overlay multicast is that the highly dynamic multicast members can make data delivery unreliable. In this paper, we address this issue in the context of live media streaming by exploring 1) how to construct a stable multicast tree that minimizes the negative impact of frequent member departures on an existing overlay and 2) how to efficiently recover from packet errors caused by end-system or network failures. For the first problem, we identify two layout schemes for the tree nodes, namely, the bandwidth-ordered tree and the time-ordered tree, which represent two typical approaches to improving tree reliability, and conduct a stochastic analysis on their properties regarding reliability and tree depth. Based on the findings, we propose a distributed reliability-oriented switching tree (ROST) algorithm that minimizes the failure correlation among tree nodes. Compared with some commonly used distributed algorithms, the ROST algorithm significantly improves tree reliability and reduces average service delay, while incurring only a small protocol overhead; furthermore, it features a mechanism that prevents cheating or malicious behaviors in the exchange of bandwidth/time information. For the second problem, we develop a simple cooperative error recovery (CER) protocol that helps recover from packet errors efficiently. Recognizing that a single recovery source is usually incapable of providing the timely delivery of the lost data, the protocol recovers from data outages using the residual bandwidths from multiple sources, which are identified using a minimum-loss-correlation algorithm. Extensive simulations demonstrate the effectiveness of the proposed schemes","Streaming media,
Resilience,
Peer to peer computing,
Bandwidth,
Stochastic processes,
Protocols,
Distributed algorithms,
Delay effects,
Multicast algorithms,
Shape"
Recursive Bayesian electromagnetic refractivity estimation from radar sea clutter,"Estimation of the range- and height-dependent index of refraction over the sea surface facilitates prediction of ducted microwave propagation loss. In this paper, refractivity estimation from radar clutter returns is performed using a Markov state space model for microwave propagation. Specifically, the parabolic approximation for numerical solution of the wave equation is used to formulate the refractivity from clutter (RFC) problem within a nonlinear recursive Bayesian state estimation framework. RFC under this nonlinear state space formulation is more efficient than global fitting of refractivity parameters when the total number of range-varying parameters exceeds the number of basis functions required to represent the height-dependent field at a given range. Moreover, the range-recursive nature of the estimator can be easily adapted to situations where the refractivity modeling changes at discrete ranges, such as at a shoreline. A fast range-recursive solution for obtaining range-varying refractivity is achieved by using sequential importance sampling extensions to state estimation techniques, namely, the forward and Viterbi algorithms. Simulation and real data results from radar clutter collected off Wallops Island, Virginia, are presented which demonstrate the ability of this method to produce propagation loss estimates that compare favorably with ground truth refractivity measurements.","Refractive index,
Ducts,
Clutter,
Sea surface,
Radar,
Estimation,
Mathematical model"
Any AND-OR Formula of Size N can be Evaluated in time N^{1/2 + o(1)} on a Quantum Computer,"For any AND-OR formula of size N, there exists a bounded-error N1/2+o(1)-time quantum algorithm, based on a discrete-time quantum walk, that evaluates this formula on a black-box input. Balanced, or ""approximately balanced,"" formulas can be evaluated in O(radicN) queries, which is optimal. It follows that the (2-o(1))th power of the quantum query complexity is a lower bound on the formula size, almost solving in the positive an open problem posed by Laplante, Lee and Szegedy.","Quantum computing,
Computer science,
Spectral analysis,
Combinatorial mathematics,
History,
Databases,
Algorithms"
A Novel Method for Segmentation of CT Head Images,"A novel method for CT image segmentation of brain is proposed in this paper, with which, several regions that are suspicious of having pathological changes can be segmented rapidly and effectively. To avoid the affection of extra-cranial highlight pixels in the later processing, row scanning approach is introduced firstly to search for the boundary of the skull. Secondly, combined with morphological operator, intra-cranial area is extracted accurately. Finally, 2D Otsu threshold segmentation is implemented to identify areas of interest and the threshold is obtained through PSO algorithm. The result of simulation demonstrates the efficiency of our method.","Image segmentation,
Computed tomography,
Head,
Image edge detection,
Morphology,
Image analysis,
Matrix converters,
Computer science,
Medical simulation,
Biomedical imaging"
Security Architecture for Third Generation (3G) using GMHS Cellular Network,"This paper presents the standardized effects of GMHS (Global Mobile Home Security) technology to achieve high security reasons using transmission of data (queries and data calls) in cellular networks. GMHS make use of the mobile's SMS and 3G services conducting the two-way communication between the primary user and the Security device. This operation is conducted by verifying the primary cellular number along with the security pin code and the query. The device has been made capable to alert user, of any change occurring intentionally and unintentionally, making it more secure and friendly. The results show the performance with respect to service conditions as the service is mainly depend upon the cellular network being used as the medium for queries.","Land mobile radio cellular systems,
Communication system security,
Data security,
Mobile communication,
Computer security,
Computer science,
Authentication,
Motion detection,
Wireless communication,
Computer architecture"
Hybrid predictive base station (HPBS) selection procedure in IEEE 802.16e-based WMAN,"According to the mobility framework of IEEE 802.16e, a mobile station (MS) should scan the neighbouring base stations (BSs), for selecting the best BS for a potential handover activity. However, the standard does not specify the number of BSs to be scanned leaving room for unnecessary scanning. Moreover, prolonged scanning also interrupts data transmissions thus degrading the QoS of an ongoing connection. Reducing unnecessary scanning is an important issue. This paper proposes a scheme to reduce the number of BSs to scan (thus improving the overall handover performance). Simulation results have shown that this hybrid predictive BS selection scheme for potential scanning activities is more effective than the conventional IEEE 802.16e handover scheme in terms of handover delay and resource wastages.","Base stations,
Data communication,
Computer science,
Software engineering,
Mobile computing,
Degradation,
Predictive models,
Delay effects,
Wireless LAN,
Metropolitan area networks"
Efficient Surface Reconstruction using Generalized Coulomb Potentials,"We propose a novel, geometrically adaptive method for surface reconstruction from noisy and sparse point clouds, without orientation information. The method employs a fast convection algorithm to attract the evolving surface towards the data points. The force field in which the surface is convected is based on generalized Coulomb potentials evaluated on an adaptive grid (i.e., an octree) using a fast, hierarchical algorithm. Formulating reconstruction as a convection problem in a velocity field generated by Coulomb potentials offers a number of advantages. Unlike methods which compute the distance from the data set to the implicit surface, which are sensitive to noise due to the very reliance on the distance transform, our method is highly resilient to shot noise since global, generalized Coulomb potentials can be used to disregard the presence of outliers due to noise. Coulomb potentials represent long-range interactions that consider all data points at once, and thus they convey global information which is crucial in the fitting process. Both the spatial and temporal complexities of our spatially-adaptive method are proportional to the size of the reconstructed object, which makes our method compare favorably with respect to previous approaches in terms of speed and flexibility. Experiments with sparse as well as noisy data sets show that the method is capable of delivering crisp and detailed yet smooth surfaces.","Surface reconstruction,
Surface contamination,
Surface fitting,
Clouds,
Topology,
Tagging,
Mathematics,
Noise robustness,
Noise level"
Mixture-of-Parts Pictorial Structures for Objects with Variable Part Sets,"For many multi-part object classes, the set of parts can vary not only in location but also in type. For example, player formations in American football involve various subsets of player types, and the spatial constraints among players depend largely upon which subset of player types constitutes the formation. In this work, we study the problem of localizing and classifying the parts of such objects. Pictorial structures provide an efficient and robust mechanism for localizing object parts. Unfortunately, these models assume that each object instance involves the same set of parts, making it difficult to apply them directly in our setting. With this motivation, we introduce the mixture-of-parts pictorial structure (MoPPS) model, which is characterized by three components: a set of available parts, a set of constraints that specify legal part subsets, and a function that returns a pictorial structure for any legal part subset. MoPPS inference corresponds to jointly computing the most likely subset of parts and their positions. We propose a restricted, but useful, representation for MoPPS models that facilitates inference via branch-and-bound optimization, which we show is efficient in practice. Experiments in the challenging domain of American football show the effectiveness of the model and inference procedure.",
Architectural Knowlege Management Strategies: Approaches in Research and Industry,"The software architecture community has recently gained an increasing interest in managing architectural knowledge. However, up until now there have been no attempts to obtain an overview of the work in the field. In this paper we present a preliminary review on current approaches to architectural knowledge management. To this end, we compare approaches known from literature and encountered in industry with knowledge management theory. We found that in reports from research and practice there appears to be a preference to use the codification strategy. However, our observations of the software architecture industry show that organizations in general tend to use a personalization strategy unintentionally. This paper serves as a call for awareness of this gap between intention and reality, and questions the biased focus on intentional codification alone. We suggest to close this gap through focusing on hybrid approaches.","Knowledge management,
Software architecture,
Computer industry,
Computer architecture,
Technology management,
Programming,
Artificial intelligence,
Pattern analysis"
Sufficient conditions for the existence of zeno behavior in a class of nonlinear hybrid systems via constant approximations,"The existence of Zeno behavior in hybrid systems is related to a certain type of equilibria, termed Zeno equilibria, that are invariant under the discrete, but not the continuous, dynamics of a hybrid system. In analogy to the standard procedure of linearizing a vector field at an equilibrium point to determine its stability, in this paper we study the local behavior of a hybrid system near a Zeno equilibrium point by considering the value of the vector field on each domain at this point, i.e., we consider constant approximations of nonlinear hybrid systems. By means of these constant approximations, we are able to derive conditions that simultaneously imply both the existence of Zeno behavior and the local exponential stability of a Zeno equilibrium point. Moreover, since these conditions are in terms of the value of the vector field on each domain at a point, they are remarkably easy to verify.","Sufficient conditions,
Stability,
Convergence,
Nonlinear dynamical systems,
Control systems,
Displays,
Nonlinear control systems,
USA Councils,
State-space methods,
Nonlinear systems"
Learning full-body motions from monocular vision: dynamic imitation in a humanoid robot,"In an effort to ease the burden of programming motor commands for humanoid robots, a computer vision technique is developed for converting a monocular video sequence of human poses into stabilized robot motor commands for a humanoid robot. The human teacher wears a multi-colored body suit while performing a desired set of actions. Leveraging the colors of the body suit, the system detects the most probable locations of the different body parts and joints in the image. Then, by exploiting the known dimensions of the body suit, a user specified number of candidate 3D poses are generated for each frame. Using human to robot joint correspondences, the estimated 3D poses for each frame are then mapped to corresponding robot motor commands. An initial set of kinematically valid motor commands is generated using an approximate best path search through the pose candidates for each frame. Finally a learning-based probabilistic dynamic balance model obtains a dynamically stable imitative sequence of motor commands. We demonstrate the viability of the approach by presenting results showing full-body imitation of human actions by a Fujitsu HOAP-2 humanoid robot.","Humanoid robots,
Humans,
Robot vision systems,
USA Councils,
Motion estimation,
Kinematics,
Cameras,
Intelligent robots,
Dynamic programming,
Robot programming"
A Unified Framework for Symbol Segmentation and Recognition of Handwritten Mathematical Expressions,"A symbol decoding and graph generation algorithm for online handwritten mathematical expression recognition is formulated. It differs from our previous system and most other systems in two aspects: (1) it embeds stroke grouping into symbol identification to form a unified probabilistic framework for symbol recognition; and (2) a symbol graph rather than a list of symbol sequence hypotheses is generated, which makes post-processing with new information possible. Experimental results show that high quality symbol graph can be generated by the proposed algorithm. Symbol sequence corresponding to the best path in the graph demonstrates much higher symbol recognition accuracy than before, especially after rescoring with trigram. Math formula recognition performance is significantly improved.","Handwriting recognition,
Decoding,
Engines,
Asia,
Computer science,
NP-hard problem,
Dynamic programming,
Heuristic algorithms"
An Adaptive Middleware to Overcome Service Discovery Heterogeneity in Mobile Ad Hoc Environments,"Ubiquitous computing research has progressed rapidly over the last few years, and real-world applications, especially mobile networking devices, are booming in consumer markets. With rising industry demand and career opportunities, more and more universities are offering graduates ubiquitous computing courses in addition to classical computer science curricula, such as mobile computing and wireless sensor networks. Here, I outline a graduate-level ubiquitous computing course I taught for final-year master's and first-year doctoral students in Finland at Abo Akademi University in 2006. Finland - the home of Nokia, the world's largest mobile phone maker - has more mobile devices per person than any other country, and its students are very interested in ubiquitous computing technologies. The experiences gained and lessons learned are a good source of information and reference for other educators in this field.","Middleware,
Ubiquitous computing,
Mobile computing,
Educational institutions,
Application software,
Computer industry,
Engineering profession,
Computer science,
Computer networks,
Pervasive computing"
Sensitivity in PET: Neural networks as an alternative to compton photons LOR analysis,"In high-resolution small-animal positron emission tomography (PET), sensitivity remains an active issue. Sensitivity can be increased by lowering the energy threshold to include more Compton-scattered events, but then computation of the correct annihilation line-of-response (LOR) proves problematic. The complexity of Compton-kinematics analysis, compounded with finite energy resolution and detection position quantization of finite-size detectors, yields unaffordable methods with rather poor success rates. As an alternative, this paper proposes an artificial neural network (ANN) approach, which forfeits all explicit handling of equations at the expense of a priori statistical training, and which has the potential to better handle the previous measurement impairments. The method first consists in a preprocessing step involving geometrical transformations, which simplifies the actual use of the neural network, in the second step. This paper presents the method's proof-of-concept. It focuses on a simple yet prevalent inter-crystal scatter scenario, where a 511-keV annihilation photon is detected coincidently with two inter-crystal-scattered photons whose energy sum accounts for the whole 511 keV annihilation energy. It shows, in preliminary simulations, a promising correct LOR computation rate in the range from 90 to 94%. Finally, it discusses the steps and requirements for the eventual implementation of the method, including further validation, hardware requirements, system- level issues and possible other applications.","Positron emission tomography,
Neural networks,
Artificial neural networks,
Energy resolution,
Quantization,
Detectors,
Equations,
Electromagnetic scattering,
Particle scattering,
Computational modeling"
Improving Software Based Self - Testing for Cache Memories,"Testing cache memories within the computer system environment is based on using processor instructions, which involve cache operations intermixed with RAM memory accesses. Applying test patterns to the cache and checking its behavior needs sophisticated instruction sequences. We simplify these sequences by means of the available on-chip performance monitors as well as built-in on-line error detectors. This new method is compared with classical approaches.","Automatic testing,
Software testing,
Cache memory,
System testing,
Computer aided instruction,
Read-write memory,
Random access memory,
Computer displays,
Computer errors,
Detectors"
Mathpad: A Fuzzy Logic-Based Recognition System for Handwritten Mathematics,"Currently available methods for inputting mathematical expressions are counter-intuitive and require knowledge of keywords. In this paper we present a recognition system for handwritten mathematics, with the goal of enabling users to enter mathematics in the customary fashion. Our approach centres around the use of fuzzy logic, with fuzzy rules being used to extract features, classify symbols, and assess spatial relationships. Taking advantage of the fuzzy information available, our structural analysis algorithm weighs up which candidate symbol identities and spatial relationships are worth exploring, and maintains numerous possible expression trees. The most likely tree is ultimately chosen as the result. The system has achieved high recognition rates on a database of expressions written by multiple users.",
An Analytical Estimation of the Failover Time in SCTP Multihoming Scenarios,"The motivation behind this paper is a need to have a more accurate estimation of the failover time in SCTP. The traditional one, commonly used in the literature, is based on the sum of the consecutive retransmission timeouts. This is not always appropriate, especially when using the SCTP multihoming feature as a basis for achieving transport layer mobility in wireless networking scenarios, where the transition time between available paths becomes a key aspect for the optimisation. Two new factors are introduced into the proposed estimation formula to reflect the influence of the network parameters and the behaviour of the most common protocol implementations. For the proposed model, we perform a best-worst case analysis, and then illustrate it with an example of a detailed estimation. Finally, we perform simulations comparing our proposal with the traditional estimation in a typical transport layer mobility scenario including long thin networks.","Failure analysis,
Transport protocols,
Communications Society,
Performance analysis,
Signal analysis,
Computer science,
Proposals,
Internet,
Availability,
Redundancy"
Detecting Arbitrary Stable Properties Using Efficient Snapshots,"A stable properly continues to hold in an execution once it becomes true. Detecting arbitrary stable properties efficiently in distributed executions is still an open problem. The known algorithms for detecting arbitrary stable properties and snapshot algorithms used to detect such stable properties suffer from drawbacks such as the following: They incur the overhead of a large number of messages per global snapshot, or alter application message headers, or use inhibition, or use the execution history, or assume a strong property such as causal delivery of messages in the system. We solve the problem of detecting an arbitrary stable property efficiently under the following assumptions: P1) the application messages should not be modified, not even by timestamps or message coloring. P2) no inhibition is allowed. P3) the algorithm should not use the message history. P4) any process can initiate the algorithm. This paper proposes a family of nonintrusive algorithms requiring 6(n - 1) control messages, where n is the number of processes. A three-phase strategy of uncoordinated observation of local states is used to give a consistent snapshot from which any stable property can be detected. A key feature of our algorithms is that they do not rely on the processes continually and pessimistically reporting their activity. Only the relevant activity that occurs in the thin slice during the algorithm execution needs to be examined.","System recovery,
History,
Computer vision,
Joining processes"
An Effective Iterative Metamorphic Testing Algorithm Based on Program Path Analysis,"Metamorphic testing (MT) is very practical and effective for programs with oracle problems, and many researches have been done in this field. In this article, we present a new iterative MT algorithm, APCEMSI, to avoid the blindness of existing MT methods. The test suites generated by APCEMSI satisfy a criterion which is defined upon program path analysis technique. Experiment result shows that APCEMSI could find errors effectively with much fewer test cases.","Iterative algorithms,
Algorithm design and analysis,
Software testing,
OFDM modulation,
Automatic testing,
Blindness,
Genetic mutations,
Computer science,
Iterative methods,
Error correction"
Cooperation of Cars and Formation of Cooperative Groups,"Cooperation of cars capable to communicate bears a high potential with respect to safety in critical situations. Following a top-down design, cars form cooperative groups, exchange information available to them and establish a common relevant picture upon which critical situations are detected, optimal decisions for the groups are derived and are distributed in form of individual action sequences. This paper focuses on the formation of cooperative groups. To this end, a graph-based spatiotemporal distance measure is developed, using the concept of virtual meeting points within the road infrastructure. The distance measure is analyzed and it serves to define cooperative groups of cognitive automobiles.","Computer science,
Data processing,
Remotely operated vehicles,
Road safety,
Data engineering,
Intelligent vehicles,
Vehicle safety,
Spatiotemporal phenomena,
Automobiles,
Adaptive systems"
Efficient feature selection based on information gain criterion for face recognition,"Feature selection based on information gain criterion is proposed for improvements in classification performance of face recognition tasks. A comparison of information gain with fisher criterion is presented for two different image database. Information gain criterion gives slight performance improvement when ICA based features are used for recognition of facial images with different illumination. For less number of selected features, information gain criterion gives superior performance compared to fisher criterion. However, the performance of information gain and fisher method for feature selection shows similar performance when the database has classes with different illusion, expressions, and occlusions.","Face recognition,
Independent component analysis,
Principal component analysis,
Feature extraction,
Performance gain,
Image databases,
Spatial databases,
Lighting,
Image recognition,
Cities and towns"
Region of Interest Video Coding for Low bit-rate Transmission of Carotid Ultrasound Videos over 3G Wireless Networks,"Efficient medical video transmission over 3G wireless is of great importance for fast diagnosis and on site medical staff training purposes. In this paper we present a region of interest based ultrasound video compression study which shows that significant reduction of the required, for transmission, bit rate can be achieved without altering the design of existing video codecs. Simple preprocessing of the original videos to define visually and clinically important areas is the only requirement.","Video coding,
Ultrasonic imaging,
Wireless networks,
Video compression,
Medical diagnostic imaging,
Biomedical imaging,
Humans,
Image coding,
Transform coding,
Videoconference"
Automatic Path Migration over InfiniBand: Early Experiences,"High computational power of commodity PCs combined with the emergence of low latency and high bandwidth interconnects has escalated the trends of cluster computing. Clusters with InfiniBand are being deployed, as reflected in the TOP 500 Supercomputer rankings. However, increasing scale of these clusters has reduced the mean time between failures (MTBF) of components. Network component is one such component of clusters, where failure of network interface cards (NICs), cables and/or switches breaks existing path(s) of communication. InfiniBand provides a hardware mechanism, automatic path migration (APM), which allows user transparent detection and recovery from network fault(s), without application restart. In this paper, we design a set of modules; which work together for providing network fault tolerance for user level applications leveraging the APM feature. Our performance evaluation at the MPI layer shows that APM incurs negligible overhead in the absence of faults in the system. In the presence of network faults, APM incurs negligible overhead for reasonably long running applications.","Personal communication networks,
Delay,
Bandwidth,
Supercomputers,
Network interfaces,
Communication cables,
Switches,
Communication switching,
Hardware,
Fault detection"
Brachytherapy Seed Localization Using Geometric and Linear Programming Techniques,We propose an optimization algorithm to solve the brachytherapy seed localization problem in prostate brachytherapy. Our algorithm is based on novel geometric approaches to exploit the special structure of the problem and relies on a number of key observations which help us formulate the optimization problem as a minimization integer program (IP). Our IP model precisely defines the feasibility polyhedron for this problem using a polynomial number of half-spaces; the solution to its corresponding linear program is rounded to yield an integral solution to our task of determining correspondences between seeds in multiple projection images. The algorithm is efficient in theory as well as in practice and performs well on simulation data (~98% accuracy) and real X-ray images (~95% accuracy). We present in detail the underlying ideas and an extensive set of performance evaluations based on our implementation.,"Brachytherapy,
Linear programming,
Implants,
X-ray imaging,
Cancer,
Neoplasms,
Computer science,
Medical treatment,
Minimization methods,
Polynomials"
QoS Provisioning Spectrum Management Based on Intelligent Matching and Reservation for Cognitive Radio System,"To improve spectrum utilization, cognitive radio attempts to use temporarily unoccupied spectrum which is referred to spectrum hole in cognitive radio. In this paper, we introduce novel spectrum management algorithms for cognitive radio. We introduce a heuristic matching algorithms which is named cost minimized matching. The heuristic algorithm proposed in this paper outperforms the general random matching algorithm with respect to the expected number of handovers and handover probability fairness. Also, the spectrum hole reservation algorithm minimizes service dropping while using optimum amount of resource, and it realizes seamless service by eliminating delay generated due to spectrum change. Performance analysis and computer simulations validate the efficiency of the proposed algorithms. The proposed algorithms can be efficiently used in cognitive radio systems.","Radio spectrum management,
Cognitive radio,
Interference,
Heuristic algorithms,
Wireless communication,
RF signals,
Frequency modulation,
Telephony,
Costs,
Chromium"
Adaptive Variable Switching Frequency Digital Controller Algorithm to Optimize Efficiency,"An adaptive digital controller with maximum efficiency point tracking to optimize DC-DC converter switching frequency is presented in this paper. The adaptive-frequency-optimization (AFO) method changes the DC-DC converter switching frequency while tracking the converter minimum input power (maximum efficiency) point under variable conditions including variable load and variable input voltage. The AFO digital controller continuously finds the optimum switching frequency that result in the minimum total loss while converter parameters and conditions vary. In this paper, the AFO method is discussed analyzed and its digital control algorithm and experimental implementation are presented.","Programmable control,
Adaptive control,
Switching frequency,
Digital control,
Switching converters,
DC-DC power converters,
Voltage control,
Switching loss,
Lighting control,
Centralized control"
Conformal Prediction with Neural Networks,"Conformal prediction (CP) is a method that can be used for complementing the bare predictions produced by any traditional machine learning algorithm with measures of confidence. CP gives good accuracy and confidence values, but unfortunately it is quite computationally inefficient. This computational inefficiency problem becomes huge when CP is coupled with a method that requires long training times, such as neural networks. In this paper we use a modification of the original CP method, called inductive conformal prediction (ICP), which allows us to a neural network confidence predictor without the massive computational overhead of CP The method we propose accompanies its predictions with confidence measures that are useful in practice, while still preserving the computational efficiency of its underlying neural network.","Neural networks,
Iterative closest point algorithm,
Bayesian methods,
Machine learning algorithms,
Computer networks,
Pattern recognition,
Machine learning,
Computer science,
Artificial intelligence,
Artificial neural networks"
A survey of information systems reaching small producers in global agricultural value chains,"Smallholder farmers face many challenges competing in the global marketplace. One major constraint is the lack of access to information and communications, which could be used to make decisions and reach new markets. In this paper, drawing from our experiences designing agricultural information systems in India and Central America, we provide a framework for understanding inter-stakeholder communications within agricultural value chains, focusing on the needs of small producers. First, we outline the major types of stakeholders - including farmers, consumers, intermediaries and various supporting organizations. Then, we survey the major categories of information systems supporting communication between stakeholders, focusing on those reaching small farmers. Based on this survey, we provide the following categorization of information flows within agricultural value chains: 1) link-to-link (L2L): those information flows required to coordinate the sale, movement, and distribution of produce along the value chain, 2) peer-to-peer (P2P): communications required to share knowledge and experiences between members of the same stakeholder group, and the expert community serving that stakeholder group and 3) end-to-end (E2E): communications between producers and consumers, for example, to facilitate exchange of non-economic values to be used as external inputs to market pricing (e.g, certification). We outline some reasons why current information systems have had difficulty in reaching small producers, and highlight a few technology trends that could contribute to increasing the fidelity and accessibility of communications, both between producers and consumers, as well as within their respective stakeholder communities.","Information systems,
Mexico Council,
Agriculture,
Costs,
Computer science,
Agricultural engineering,
Engineering drawings,
Marketing and sales,
Peer to peer computing,
Pricing"
An Oscillation Monitoring System for Real-time Detection of Small-Signal Instability in Large Electric Power Systems,"This paper proposes a real-time oscillation monitoring system for detecting the emergence of small-signal instability related events in large electric power systems. The oscillation monitoring system has been developed as a fully autonomous expert system for real-time extraction of modal information from the wide-area PMU measurements during the progress of power system disturbance events. The oscillation analysis uses three different algorithms, namely, the classical multiple-input Prony algorithm, the matrix Pencil algorithm as well as the recent Hankel total least square method. These algorithms were primarily used for off-line manual analysis of power system oscillations in the past. In our research, a specific set of rules has been designed to automate the modal analysis of oscillations by using the three algorithms above without any human intervention during the analysis. The monitor will thus issue operator alerts or control triggers whenever the damping levels of the electromechanical modes of oscillation go below preset thresholds.","Monitoring,
Real time systems,
Algorithm design and analysis,
Power system analysis computing,
Power system measurements,
Event detection,
Expert systems,
Data mining,
Phasor measurement units,
Power measurement"
A PDA-based Research Platform for Cochlear Implants,"Currently researchers interested in developing new signal processing algorithms for commercially available cochlear implants must rely on coding these algorithms in low-level assembly language. We propose a personal digital assistant (PDA) based research platform for developing and testing in real-time new signal processing strategies for cochlear implants. Software development can be done either in C or in LabVIEW. The C implementation can be further optimized using Intel's primitive routines. In this paper, we report on the real-time implementation of a 16-channel noise-band vocoder algorithm, which is a similar algorithm used in commercially available implant processors. We further report on EEG recordings on the PDA acquired through a compact-flash data acquisition card.","Cochlear implants,
Signal processing algorithms,
Personal digital assistants,
Assembly,
Testing,
Digital signal processing,
Programming,
Vocoders,
Electroencephalography,
Data acquisition"
Parallel Type-2 Fuzzy Logic Co-Processors for Engine Management,"Marine diesel engines operate in highly dynamic and uncertain environments, hence they require robust and accurate speed controllers that can handle the encountered uncertainties. Type-2 fuzzy logic controllers (FLCs) have shown that they can handle such uncertainties and give a superior performance to the existing commercial controllers. However, there are a number of computational bottlenecks that pose as significant barriers to the widespread deployment of type-2 FLCs in commercial embedded control systems. This paper explores the use of parallel hardware implementations of interval type-2 FLC as a means to eradicate these barriers thus producing bespoke co-processors for a soft core implementation of a FPGA based 32 bit RISC micro-processor. These coprocessors will perform functions such as fuzzification and type reduction and are currently utilised as part of a larger embedded interval type-2 fuzzy engine management system (T2FEMS). Numerous timing comparisons were undertaken between the co-processors and their sequential counterparts where the type-2 co-processors reduced significantly the computational cycles required by the type-2 FLC. This reduction in computational cycles allowed the T2FEMS to produce faster control responses whilst offering a superior control performance to the commercial engine management systems. Thus the proposed co-processors enable us to fully explore the potential of interval and possibly general type-2 FLCs in commercial embedded applications.","Fuzzy logic,
Coprocessors,
Diesel engines,
Control systems,
Uncertainty,
Robust control,
Embedded computing,
Hardware,
Job production systems,
Field programmable gate arrays"
Performance Analysis of Homing Pigeon based Delay Tolerant Networks,"This paper presents and analyzes a new type of delay tolerant network where each node owns a dedicated messenger (called a pigeon). The only form of inter-node communication is for a pigeon to periodically carry a batch of messages originated at the home node, deliver them to the corresponding destination nodes and return home. Clearly, given message expiration times, some messages may not reach their destinations by the deadline. Through theoretical analysis and simulations, we study the relationship between the arrival rate (at the home node), batch size, expiration time and delivery ratio (the percentage of messages reaching destinations before they expire) of messages. The simplistic assumptions we make render the problem tractable, and help us gather experience in this topic.","Performance analysis,
Disruption tolerant networking,
Mobile agents,
Routing,
Analytical models,
Communication system control,
Delay estimation,
Computer science,
Physics,
Astronomy"
Behavior Selection and Memory-based Learning for Artificial Creature Using Two-layered Confabulation,"Confabulations, where millions of items of relevant knowledge are applied in parallel in the human brain, are typically employed in thinking. This paper proposes a novel behavior selection architecture and memory-based learning method for an artificial creature based on two-layered confabulation. A behavior is selected by considering both internally generated will and the context of the external environment. Proposed behavior selection using a confabulation scheme is a parallel process in which a number of behaviors are considered simultaneously. An arbitration mechanism is employed to choose a proper behavior which is to be put into an action. Also memory-based behavior learning is proposed, w here the memory has the selection probabilities of behaviors based on will and context. The learning module updates the contents of memory according to the user-given reward or penalty signal. To demonstrate the effectiveness of the proposed scheme, an artificial creature is implemented in the 3D virtual environment such that it can interact with a human being considering its will and context.","Virtual environment,
Learning systems,
Service robots,
Human robot interaction,
Parallel robots,
Memory architecture,
Hardware,
Brain"
Localization of Unknown Networked Radio Sources Using a Mobile Robot with a Directional Antenna,"In this paper, we present algorithmic developments for a single mobile robot equipped with a directional antenna to localize unknown networked radio sources. We assume that the robot only senses radio transmissions at the physical layer and that the network has a carrier sense multiple access (CSMA)-type medium access control (MAC) layer. The total number of radio sources are assumed unknown. We first formulate the localization problem and then propose a particle filter-based localization algorithm. The algorithm combines a new CSMA model and a new directional antenna model. The new CSMA model provides network configuration data, such as the network size and the probability of collision, when listening to network traffic. The directional antenna model enhances the efficiency of robot motion. The new combined sensing model is capable of handling transmission collisions during localization. The overall algorithm runs in O(nm) time for n particles and m radio sources at each step. The numerical results show that the algorithm can localize unknown networked radio sources effectively.","Mobile robots,
Directional antennas,
Multiaccess communication,
Road accidents,
Robot sensing systems,
Physical layer,
Media Access Protocol,
Telecommunication traffic,
Traffic control,
Robot motion"
Soft Error Mitigation in Switch Modules of SRAM-based FPGAs,"In this paper, we propose two techniques to mitigate soft error effects on the switch modules of SRAM-based FPGAs: 1) The first technique tolerates SEU-caused open errors based on a new programming method for SRAM-bits of switch modules, and 2) The second technique mitigates SEU-cause short errors in the switch modules based on a mixed programmable and hard-wired switch module structure in the FPGAs. The effects of these two techniques on the delay, area and power consumption for 20 MCNC benchmark circuits are achieved using a minor modification in VPR and T-VPack FPGA CAD tools. The experimental results show that the first technique increase reliability of connections of switch module up to 30% while the second technique decreases the susceptibility of switch modules to SEUs about 50% compared to the traditional ones","Switches,
Field programmable gate arrays,
Single event upset,
Single event transient,
Redundancy,
Computer errors,
Fault tolerance,
Hardware,
Energy consumption,
Circuits"
A web-based learning management system with automatic assessment resources,"One of the desirable skills for engineering students is the ability of creating and understanding geometric objects. Most of this ability is acquired during elementary and high school courses, but some acquisition are left for undergraduate studies. The introduction of dynamic geometry (DG) brings dynamicity to the traditional geometry learning process. Some DG systems present web-based versions, allowing students to use them worldwide through web browsers. By using such systems, students may increase their performance on solving geometric problems, which increase the amount of work to be assessed by teachers or tutors. As a consequence, the delay between the time of submitting the student answer for a problem and its feedback is also increased. Nevertheless, if the learning process is mediated by a web- based learning system, the delay on providing feedback may cause student's disappointment or course abandonment. In this paper we present the free software SAW, a web-based learning management system that incorporates e-learning modules (e-LMs) for specific learning contents, such as geometry or programming. An e-LM is a Java applet that offer resources for client/server communication and may offer resources for problem authoring and automatic assessment.","Resource management,
Geometry,
Feedback,
Engineering students,
Educational institutions,
Delay effects,
Learning systems,
Delay systems,
Surface acoustic waves,
Content management"
Demand allocation without wavelength conversion under a sliding scheduled traffic model,"It has been shown that for the scheduled traffic model, connection holding time aware algorithms lead to more efficient resource allocation. The setup and teardown times of the scheduled demands may be fixed, or may be allowed to slide within a larger window. A number of optimal integer linear program (ILP) solutions for the first problem (fixed setup/teardown times) have been presented in the literature, for wavelength convertible networks. In this paper we present a new and complete ILP formulation for both fixed window model, and the more general sliding scheduled traffic model, where the setup and teardown times may vary within a specified range. We consider fault-free as well as survivable networks using path protection, and do not require any wavelength conversion. Our ILP can jointly optimize the problem of scheduling the demands (in time) and allocating resources for the scheduled lightpaths. We have shown that the complexity of our formulation for sliding scheduled traffic model, in terms of the number of integer variables, is less than existing ILP formulations for the simpler fixed window model. For very large networks, we have proposed a fast two-step optimization process. The first step schedules the demands optimally in time, such that the amount of overlap is minimized. The second step uses a connection holding time aware heuristic to perform routing and wavelength assignment for the scheduled demands.","Traffic control,
Protection,
Telecommunication traffic,
Resource management,
Optical wavelength conversion,
Optical fiber communication,
Processor scheduling,
WDM networks,
Wavelength division multiplexing,
Scheduling algorithm"
Optimal search performance in unstructured peer-to-peer networks with clustered demands,"This paper derives the optimal search time and the optimal search cost that can be achieved in unstructured peer-to-peer networks when the demand pattern exhibits clustering (i.e. file popularities vary across the set of nodes in the network). Clustering in file popularity patterns is evident from measurements on deployed peer-to-peer file sharing networks. In this paper, we provide mechanisms for modeling clustering in file popularity distributions and the consequent non-uniform distribution of file replicas. We derive relations that show the effect of the number of replicas of a file on the search time and on the search cost for a search for that file for the clustered demands case in such networks for both random walk and flooding search mechanisms. The derived relations are used to obtain the optimal search performance for the case of flooding search mechanisms. The potential performance benefit that clustering in demand patterns affords is captured by our results. Interestingly, the performance gains are shown to be independent of whether the search network topology reflects the clustering in file popularity (the optimal file replica distribution to obtain these performance gains, however, does depend on the search network topology).",
Hand Gesture Recognition To Understand Musical Conducting Action,"This paper deals with the understanding of four musical time patterns and three tempos that are generated by a human conductor of robot orchestra or an operator of computer-based music play system using the hand gesture recognition. We use only a stereo vision camera with no extra special devices such as sensor glove, 3D motion capture system, infra-red camera, electronic baton and so on. We propose a simple and reliable vision-based hand gesture recognition using the conducting feature point (CFP), the motion-direction code, and the motion history matching. The proposed hand gesture recognition system operates as follows: First, it extracts the human hand region by segmenting the depth information generated by stereo matching of image sequences. Next, it follows the motion of the center of the gravity(COG) of the extracted hand region and generates the gesture features such as CFP and the direction-code. Finally, we obtain the current timing pattern of the music's beat and tempo by the proposed hand gesture recognition using either CFP tracking or motion histogram matching. The experimental results show that the musical time pattern and tempo recognition rate are over 86% on the test data set when the motion histogram matching is used.","Pattern recognition,
Humans,
Robot vision systems,
Cameras,
Data mining,
Histograms,
Pattern matching,
Conductors,
Robot sensing systems,
Stereo vision"
Year,,
New Concepts Related to Non-Stationary Fuzzy Sets,"In this paper, formal definitions of the concepts relevant to non-stationary fuzzy sets are provided, as well as definitions of the basic non-stationary fuzzy set operators with proofs of selected properties of these operators. Among the novel terms introduced are the footprint of instantiations, the domain of instantiations and the temporal histogram. Further, we discuss the correspondence between non-stationary and type-2 fuzzy sets, and make the first attempt at proposing a set of comparable terms.",
Inter-operating grids through delegated matchmaking,"The grid vision of a single computing utility has yet to materíalize: while many grids with thousands of processors each exist, most work in isolation. An important obstacle for the effective and efficient inter-operation of grids is the problem of resource selection. In this paper we propose a solution to this problem that combines the hierarchical and decentralized approaches for interconnecting grids. In our solution, a hierarchy of grid sites is augmented with peer-to-peer connections between sites under the same administrative control. To operate this architecture, we employ the key concept of delegated matchmaking, which temporarily binds resources from remote sites to the local environment. With trace-based simulations we evaluate our solution under various infrastructural and load conditions, and we show that it outperforms other approaches to inter-operating grids. Specifically, we show that delegated matchmaking achieves up to 60% more goodput and completes 26% more jobs than its best alternative.","Grid computing,
Isolation technology,
Computer vision,
Permission,
Resource management,
Load management,
Computer architecture,
Production,
Laboratories,
Computer science education"
To Be or Not to Be: Predicting Soluble SecAs as Membrane Proteins,"SecA is an important component of protein translocation in bacteria, and exists in soluble and membrane-integrated forms. Most membrane prediction programs predict SecA as being a soluble protein, with the exception of TMpred and TopPred. However, the membrane associated predicted segments by TMpred and TopPred are inconsistent across bacterial species in spite of high sequence homology. In this paper we describe a new method for membrane protein prediction, PSSM_SVM, which provides consistent results for integral membrane domains of SecAs across bacterial species. This PSSM encoding scheme demonstrates the highest accuracy in terms of Q2 among the common prediction methods, and produces consistent results on blind test data. None of the previously described methods showed this kind of consistency when tested against the same blind test set. This scheme predicts traditional transmembrane segments and most of the soluble proteins accurately. The PSSM scheme applied to the membrane-associated protein SecA shows characteristic features. In the set of 223 known SecA sequences, the PSSM_SVM prediction scheme predicts eight to nine residue embedded membrane segments. This predicted region is part of a 12 residue helix from known X-ray crystal structures of SecAs. This information could be important for determining the structure of SecA proteins in the membrane which have different conformational properties from other transmembrane proteins, as well as other soluble proteins that may similarly integrate into lipid bi-layers.","Biomembranes,
Proteins,
Testing,
Microorganisms,
Computer science,
Prediction methods,
Amino acids,
Encoding,
Lipidomics"
Focal Pre-Correction of Projected Image for Deblurring Screen Image,"We propose a method for reducing out-of-focus blur caused by projector projection. In this method, we estimate the Point-Spread-Function (PSF) of the out-of-focus blur in the image projected onto the screen by comparing the screen image captured by a camera with the original image projected by the projector. According to the estimated PSF, the projected image is pre-corrected, so that the screen image can be deblurred. Experimental results show that our method can reduce out-of-focus projection blur.",
Bayesian Network Modeling of Acoustic Sensor Measurements,"Control and optimization of acoustic sensors can significantly impact the effectiveness of sonar deployment in variable and uncertain underwater environments. On the other hand, the design of optimal control systems requires tractable models of system dynamics, which in this case include acoustic-wave propagation phenomena. High-fidelity acoustic models that capture the influence of environmental conditions on wave propagation involve partial differential equations (PDEs), and are computationally intensive. Also, by relying on the numerical solution of PDEs for given boundary and initial conditions, they do not provide closed-form functional forms for the propagation loss or other output variables. In this paper, a simple Bayesian network (BN) model of acoustic propagation is presented for use in sonar control. The performance of the BN model compares favorably to that of a radial basis function neural network. Additionally, the sensor range dependency on spatial and temporal coordinates can be estimated and utilized to compute optimal sonar control strategies.","Bayesian methods,
Acoustic sensors,
Acoustic measurements,
Acoustic propagation,
Optimal control,
Sonar measurements,
Acoustic waves,
Underwater acoustics,
Partial differential equations,
Propagation losses"
Viscoelastic and Nonlinear Organ Model for Control of Needle Insertion Manipulator,"This paper shows the viscoelastic and nonlinear organ deformation model for organ model-based control of needle insertion, in which the deformation of an organ is calculated intraoperatively and the needle is manipulated with organ deformation taken into consideration. An organ model including such detailed material characteristics is important to achieve the control method in question. Firstly, the material properties of the liver are modeled from the measured data and its viscoelastic characteristics are represented by differential equations, including the term of the fractional derivative. Nonlinearity in terms of the fractional derivative was measured, and modeled using the quadratic function of strain. Next, a solution of an FE model using such material properties is shown. We use sampling time scaling property as the solution for the viscoelastic system. The solution for a nonlinear system using the Modified Newton-Raphson method is also shown. Finally, the organ deformation, assuming the needle is inserted, is simulated using an organ model and the overall deformation and distribution of the strain at each element is computed in these simulations.","Viscosity,
Elasticity,
Needles,
Deformable models,
Material properties,
Strain measurement,
Computational modeling,
Liver,
Differential equations,
Iron"
Automatic Phonetic Segmentation by Score Predictive Model for the Corpora of Mandarin Singing Voices,"This paper proposes the concept of a score predictive model (SPM) that can refine the phoneme boundaries obtained by a hidden Markov model (HMM) and dynamic time warping (DTW) for a Mandarin singing voice corpus. An SPM is constructed by using support vector regression. It predicts the score of a phoneme boundary according to the boundary's 58-dimensional feature vector. The correctly identified boundaries of a singing corpus can then be used for corpus-based singing voice synthesis. Several experiments with different settings, including the use of different initial estimates, different acoustic features, and various regression approaches, were designed to verify the feasibility of the proposed approach. Experimental results demonstrate that the proposed SPM is able to effectively refine the results of the HMM and DTW.","Predictive models,
Hidden Markov models,
Speech synthesis,
Scanning probe microscopy,
Humans,
Neural networks,
Viterbi algorithm,
Mel frequency cepstral coefficient,
Cepstral analysis,
Computer science"
A Sequential Monte Carlo Approach to Anomaly Detection in Tracking Visual Events,"In this paper we propose a technique to detect anomalies in individual and interactive event sequences. We categorize anomalies into two classes: abnormal event, and abnormal context, and model them in the Sequential Monte Carlo framework which is extended by Markov Random Field for tracking interactive events. Firstly, we propose a novel pixel-wise event representation method to construct feature images, in which each blob corresponds to a visual event. Then we transform the original blob-level features into subspaces to model probabilistic appearance manifolds for each event-class. With the probability of an observation associated with each event-class (or state) derived from probabilistic manifolds, and state transitional probability, the prior and posterior state distributions can be estimated. We demonstrate in experiments that the approach can reliably detect such anomalies with low false alarm rates.","Monte Carlo methods,
Event detection,
Sliding mode control,
Pixel,
Feature extraction,
Computer science,
Markov random fields,
State estimation,
State-space methods,
Sun"
Evolving Parameters for Xpilot Combat Agents,"In this paper we present a new method for evolving autonomous agents that are competitive in the space combat game Xpilot. A genetic algorithm is used to evolve the parameters related to the sensitivity of the agent to input stimuli and the agent's level of reaction to these stimuli. The resultant controllers are comparable to the best hand programmed artificial Xpilot bots, are competitive with human players, and display interesting behaviors that resemble human strategies.",
On Efficient Processing of Subspace Skyline Queries on High Dimensional Data,"Recent studies on efficiently answering subspace skyline queries can be separated into two approaches. The first focused on pre-materializing a set of skylines points in various subspaces while the second focus on dynamically answering the queries by using a set of anchors to prune off skyline points through spatial reasoning. Despite effort to compress the pre-materialized subspace skylines through removal of redundancy, the storage space for the first approach remain exponential in the number of dimensions. The query time for the second approach on the other hand also grow substantially for data with higher dimensionality where the pruning power of anchors become much weaker. In this paper, we propose methods for answering subspace skyline query on high dimensional data such that both prematerialization storage and query time can be moderated. We propose novel notions of maximal partial-dominating space, maximal partial-dominated space and the maximal equality space between pairs of skyline objects in the full space and use these concepts as the foundation for answering subspace skyline queries for high dimensional data. Query processing involves mostly simple pruning operations while skyline computation is done only on a small subset of candidate skyline points in the subspace. We also develop a random sampling method to compute the subspace skyline in an on-line fashion. Extensive experiments have been conducted and demonstrated the efficiency and effectiveness of our methods.","Airports,
Computer science,
Query processing,
Sampling methods,
Information systems,
Internet,
Relational databases"
Towards life-long learning in household robots: The Piagetian approach,"Learning is a core feature of future household robot systems. Nonetheless, present-day learning approaches fail to take into account that learning is never a finished process but an everyday task for biological systems. Additionally, humans always learn a various number of different tasks at the same time. This paper proposes an approach to these two problems by applying the concept of Piagetian learning to the problem of robot task learning. It proposes a method for the autonomous recognition of different task classes in the robots experiences and gives one possibility, how this task knowledge can be exploited for incremental learning of sequential reordering features of a task. This framework is evaluated using three different tasks from the household domain.","Humans,
Learning systems,
Robot programming,
Computer science,
Biological systems,
Mobile robots,
Service robots,
Robot vision systems,
Unsupervised learning,
Performance analysis"
Connectivity in vehicular ad hoc networks in presence wireless mobile base-stations,"Connectivity in vehicular ad hoc networks tends to be vulnerable. This is mostly because of the influence of road's traffic parameters like traffic flow and vehicle's speed. One possible way to improve the connectivity is to add some nodes with higher transmission range. These nodes also could give some commercial services to the vehicles on roads (i.e. audio/video service, traffic information, etc.). In this paper we study the connectivity in presence of these nodes which we call mobile base-stations. Our approach is based on the work of Miorandi and Altman that transformed the problem of connectivity distance distribution into that of the distribution of the busy period of an equivalent infinite server queue. We study the effects of mobile base-stations on the connectivity distance and number of nodes in a spatial cluster (platoon). In our investigation we use some publicly available statistical data and realistic traffic patterns. Our model can be used to obtain optimum values for number of base-stations and their transmission range in order to achieve intended degree of connectivity.","Ad hoc networks,
Traffic control,
Telecommunication traffic,
Vehicle safety,
Road vehicles,
FCC,
Road safety,
Transportation,
Chemical technology,
Electronic mail"
Standard Integration of Sensing and Opportunistic Diffusion for Urban Monitoring in Vehicular Sensor Networks: the MobEyes Architecture,"The emerging industrial relevance of vehicular sensor networks pushes towards their adoption for large-scale applications, from traffic routing and relief to environmental monitoring and distributed surveillance. With homeland security issues in mind, we have developed MobEyes, a fully distributed opportunistic harvesting system for urban monitoring. In MobEyes, regular vehicles equipped with sensors collect and locally store monitoring data while moving on the streets. Sensors may generate a sheer data amount, especially in the case of audio/video recording, thus making traditional reporting unfeasible. MobEyes originally adopts the guidelines of locally generating summaries of sensed data and of taking advantage of vehicle mobility and opportunistic one-hop communications to pump summaries towards mobile collectors, with minimal overhead, reasonable completeness, and limited latency. To that purpose, it carefully considers standard specifications to portably integrate with heterogeneous sensors, in particular by exploiting the Java Media Framework to interwork with cameras, the JSR179 Location API to interface with heterogeneous localization systems, and the Java Communications API to access lower-layer environmental sensors.",
A Spatio-Temporal Approach to Selective Data Dissemination in Mobile Peer-to-Peer Networks,"We examine data dissemination in mobile peer-to-peer networks, where moving objects communicate with each other via short-range wireless technologies such as IEEE 802.11 or Bluetooth. Given the memory and bandwidth/energy constraints at moving objects, the ideal mobile peer-to-peer dissemination method is for each moving object to store and transmit only the reports that are new to other objects encountered in the future. However, in practice no object can know the states of all the other objects due to the distributed and dynamic nature of mobile peer-to-peer networks. Thus, predicting the novelty probability of a report is important for efficient data dissemination in mobile peer-to-peer networks. In this paper we propose a decentralized spatio-temporal approach to selective data dissemination. In this approach, the novelty probability of a report is estimated based on both spatial and temporal attributes (AGE and DISTANCE) of the report and each moving object only stores and transmits the reports with the highest novelty probabilities. We study different strategies of using novelty factors to compute the novelty probability. Extensive experiments are conducted to test and analyze the performance of different strategies. The experimental results determine the best strategy and demonstrate its superiority against existing mobile peer-to-peer methods.","Peer to peer computing,
Bandwidth,
Mobile computing,
USA Councils,
Bluetooth,
Computer science,
Transportation,
Vehicle dynamics,
Testing,
Performance analysis"
Exploration of Link Structure and Community-Based Node Roles in Network Analysis,"Communities are nodes in a network that are grouped together based on a common set of properties. While the communities and link structures are often thought to be in alignment, it may not be the case when the communities are defined using other external criterion. In this paper we provide a new way to measure the alignment. We also provide a new metric that can be used to estimate the number of communities to which a node is attached. This metric, along with degree, is used to assign a community-based role to nodes. We demonstrate the usefulness of the community-based node roles by applying them to the influence maximization problem.",
Verifying Semantic Business Process Models in Inter-operation,"Process inter-operation is characterized as cooperative interactions among loosely coupled autonomous constituents to adaptively fulfill system-wide purpose. Issues of inconsistency can be anticipated in inter-operating processes given their independent management and design. To reduce inconsistency (that may contribute to failures) effective methods for statically verifying behavioral interoperability are required. This paper contributes a method for practical, semantic verification of interoperating processes (as represented with BPMN models). We provide methods to evaluate consistency during process design where annotation of the immediate effect of tasks and sub-processes has been provided. Furthermore, some guidelines are defined against common models of inter-operation for scoping traceability to possible causes of inconsistency. This supports subsequent resolution efforts.","Information analysis,
Unified modeling language,
Logic,
Laboratories,
Computer science,
Software engineering,
Process design,
Guidelines,
Proposals,
Environmental management"
Advanced Shortest Paths Algorithms on a Massively-Multithreaded Architecture,"We present a study of multithreaded implementations of Thorup's algorithm for solving the single source shortest path (SSSP) problem for undirected graphs. Our implementations leverage the fledgling multithreaded graph library (MTGL) to perform operations such as finding connected components and extracting induced subgraphs. To achieve good parallel performance from this algorithm, we deviate from several theoretically optimal algorithmic steps. In this paper, we present simplifications that perform better in practice, and we describe details of the multithreaded implementation that were necessary for scalability. We study synthetic graphs that model unstructured networks, such as social networks and economic transaction networks. Most of the recent progress in shortest path algorithms relies on structure that these networks do not have. In this work, we take a step back and explore the synergy between an elegant theoretical algorithm and an elegant computer architecture. Finally, we conclude with a prediction that this work will become relevant to shortest path computation on structured networks.","USA Councils,
Data structures,
Delay,
Computer architecture,
Roads,
Laboratories,
Computer science,
Educational institutions,
Libraries,
Scalability"
On Network Coding Based Multirate Video Streaming in Directed Networks,"This paper focuses on network coding based multirate multimedia streaming in directed networks and aims at maximizing the total layers received by all receivers, which directly determine the quality of video streaming. We consider the property of layered coding in video streaming and propose the layer separated network coding scheme (LSNC) for layered video streaming. Two algorithms OLSNC and SLSNC are proposed for LSNC based video streaming, where OLSNC achieves an optimal solution, while SLSNC is a polynomial time approximation algorithm. Simulation results show that LSNC is an efficient network coding scheme for multirate multimedia streaming, and the aggregated number of received layers of both OLSNC and SLSNC is very close to the theoretical upper bound in all configurations analyzed.","Network coding,
Streaming media,
Decoding,
Application software,
Approximation algorithms,
Bandwidth,
Polynomials,
Analytical models,
Upper bound,
Broadcasting"
On-Chip Bondwire Inductor with Ferrite-Epoxy Coating: A Cost-Effective Approach to Realize Power Systems on Chip,"A novel concept of on-chip bondwire inductors with ferrite epoxy coating is proposed to provide a cost effective approach realizing power systems on chip (SOC). A Q factor of 30-40 is experimentally demonstrated which represents an improvement by a factor of 3-30 over the state-of-the-art MEMS micromachined inductors. More importantly, the bondwire inductors can be easily integrated into power SOC manufacturing processes with minimal changes, and open enormous possibilities for realizing cost-effective, high current, high efficiency power SOC's.","System-on-a-chip,
Bonding,
Inductors,
Coatings,
Power systems,
Ferrites,
Costs,
Q factor,
Micromechanical devices,
Manufacturing processes"
Bipedal walking on rough terrain using manifold control,"This paper presents an algorithm for adapting periodic behavior to gradual shifts in task parameters. Since learning optimal control in high dimensional domains is subject to the 'curse of dimensionality', we parametrize the policy only along the limit cycle traversed by the gait, and thus focus the computational effort on a closed one-dimensional manifold, embedded in the high-dimensional state space. We take an initial gait as a departure point, and iterate between modifying the task slightly, and adapting the gait to this modification. This creates a sequence of gaits, each optimized for a different variant of the task. Since every two gaits in this sequence are very similar, the whole sequence spans a two-dimensional manifold, and combining all policies in this 2-manifold provides additional robustness to the system. We demonstrate our approach on two simulations of bipedal robots - the compass gait walker, which is a four-dimensional system, and RABBIT, which is ten-dimensional. The walkers' gaits are adapted to a sequence of changes in the ground slope, and when all policies in the sequence are combined, the walkers can safely traverse a rough terrain, where the incline changes at every step.","Legged locomotion,
State-space methods,
Control systems,
Learning,
Control theory,
Orbital robotics,
Limit-cycles,
Rabbits,
Intelligent robots,
USA Councils"
Optimal resource allocation for multicast flows in multihop wireless networks,"In this paper, we extend recent results on fair and stable resource allocation in wireless networks to include multicast flows, in particular multi-rate multicast. The solution for multi-rate multicast is based on scheduling virtual (shadow) ""traffic"" that ""moves"" in reverse direction from destinations to sources. This shadow scheduling algorithm can also be used to control delays in wireless networks.","Resource management,
Spread spectrum communication,
Wireless networks,
Unicast,
Communication system traffic control,
Telecommunication traffic,
Mobile ad hoc networks,
Broadcasting,
Delay,
Interference constraints"
Does Disturbance Discourage People from Communicating with a Robot?,"We suggest that people's responses to a robot of which attention starts to be distracted show whether they accept the robot as an intentional communication partner or not. Human-robot interaction (HRI) as well as human-human interaction (HHI) is sometimes interrupted by disturbing factors. However, in HHI people continue to communicate with a partner because they presuppose that the partner may shift his/her interactive orientation based on his/her internal state. We designed a communication robot equipped with a mechanism of saliency-based visual attention and evaluated it in an observational experiment of HRI. Our sociological analysis of people's responses to our robot showed that it was accepted as a proactive communication agent. When the robot shifted its attention to an irrelative target, the human partners, for example, followed the line of the robot's gaze and tried to regain its attention by exaggerating their actions and increasing their communication channels as they would do toward a human partner. Based on these results, we conclude that disturbance can be an encouraging factor for human activity in HRI. The results are discussed from both a sociological and an engineering point of view.","Human robot interaction,
TV,
Computer science,
Communication channels,
Telephony,
Watches,
Sociology"
Connected Giving: Ordinary People Coordinating Disaster Relief on the Internet,"The Internet is widely valued for distributing control over information to a lateral network of individuals, but it is not clear how these networks can most effectively organize themselves. This paper describes the distributed networks of volunteers that emerged online following Hurricane Katrina. Online communities responded to the disaster by facilitating the distribution of donated goods from ordinary people directly to hurricane survivors. These ""connected giving"" groups faced several challenges: establishing authority within the group, providing relevant information, developing trust in one another, and sustaining the group over time. Two forms of computer-mediated connected giving were observed: small blog communities and large forums. Small blog communities used a centralized authority structure that was more immediately successful in managing information and developing trust, but over time, blog communities were difficult to sustain. Larger and more decentralized forums had greater difficulties focusing the community's communication and developing trust but sustained themselves over a long period of time",
An Efficient Algorithm for Online Soft Real-Time Task Placement on Reconfigurable Hardware Devices,"Reconfigurable devices such as field programmable gate arrays (FPGAs) are very popular in today's embedded systems (design due to their low-cost, high-performance and flexibility. Partially runtime-reconfigurable (PRTR) FPGAs allow hardware tasks to be placed and removed dynamically at runtime. Hardware task scheduling on PRTR FPGAs brings many challenging issues to traditional real-time scheduling theory, which have not been adequately addressed by the real-time research community compared to software task scheduling on CPUs. In this paper, we present an efficient online task placement algorithm for minimizing fragmentation on PRTR FPGAs. First, we present a novel 2D area fragmentation metric that takes into account probability distribution of sizes of future task arrivals; second, we take into the time axis to obtain a 3D fragmentation metric. Simulation experiments indicate that our techniques result in low ratio of task rejection and high FPGA utilization compared to existing techniques",
Sparsity Promotion Models for the Choquet Integral,"In this paper, we present a novel algorithm for learning fuzzy measures for Choquet integration. There are two novel aspects of the algorithm: it seeks to explicitly reduce the number of nonzero parameters in the measure to eliminate noninformative or useless information sources and it uses a Bayesian model for parameter estimation which has not been previously applied to the fuzzy measure learning problem. The method uses a hierarchical model that implements a sparsity promotion algorithm through a Gibbs sampler. This approach builds on the methods proposed by Figueiredo et al which uses expectation maximization (EM) to maximize the least absolute shrinkage and selection operator (LASSO) criterion under a distribution that promotes sparsity. Additional constraints are needed to satisfy the requirements of fuzzy measures. Figueiredo's algorithm does not have a mechanism for imposing these constraints. The constraints are imposed by sequentially exploring the lattice tree of the power set and requiring that each fuzzy measure value assigned to a set lies in the domain of a truncated Gaussian determined by the fuzzy measures of supersets of the set under consideration","Fuzzy sets,
Information science,
Fuzzy logic,
USA Councils,
Power measurement,
Fuses,
Information resources,
Computational intelligence,
Gain measurement,
Bayesian methods"
Oblivious Routing with Mobile Fusion Centers over a Sensor Network,"We consider the problem of aggregating data at a mobile fusion center (fusor) (eg. a PDA or a cellular phone) moving within a spatial region over which a wireless sensor network (eg., fixed motes) has been deployed. Each sensor node generates packets destined to the fusor, and our objective is to develop strategies that can route the packets to the mobile fusor. For an arbitrary (possibly random) fusor mobility pattern over any connected subset of the sensor deployment area, we first derive upper bounds on the aggregation data rate (i.e., the uniform rate region from each sensor node to the mobile fusor), where we allow all sensor nodes to have complete knowledge of the mobility pattern of the fusor. We then consider aggregation data rates that can be achieved when the mobility pattern of the fusor is unknown to the sensor nodes. Surprisingly, we show that for a class of mobility patterns (random mobility over connected-compositions of convex sets of the deployment region, e.g. random walks over piece-wise linear sets), we can construct ""universal"" mobility-oblivious routing strategies that achieve aggregation data rates that are of the same order as the (mobility-aware) upper bound.","Routing,
Sensor fusion,
Wireless sensor networks,
Peer to peer computing,
Upper bound,
Mobile computing,
Communications Society,
Computer science,
Personal digital assistants,
Cellular phones"
Real-Time Projector Tracking on Complex Geometry Using Ordinary Imagery,"Using cameras to geometrically calibrate projector-based displays has been widely reported in the literature over the last decade. Most systems project structured-light patterns during a setup phase before starting the application in order to evaluate the geometric mapping from projector pixels to locations on the display surface. If this mapping changes e.g. the projector is moved, the application must be stopped and calibration re-initiated. Our pose estimation technique is based on detecting a set of image correspondences between the moving projector and a static camera using only the imagery displayed by the projector. We use an up-front calibration process to establish the geometry of the display surface, the pose of the camera, and the initial pose of the projector.",
Evaluation of ICA based fusion of hyperspectral images for color display,"Hyperspectral imaging is becoming increasingly important in a variety of applications. These images contain a large number of contiguous bands to provide information at a fine spectral resolution and, therefore, cannot be displayed directly using an RGB color display. There has been some recent work on the problem of fusing hyperspectral images to three-band images for color display purposes. In this paper, we evaluate the performance of our recently proposed approach based on independent component analysis, correlation coefficient and mutual information (ICA- CCMI) to fuse the information from a large number of bands to three images suitable for color display. Depending on whether the reference images are available or not, several image quality metrics such as entropy and edge correlation have been proposed and employed to evaluate the fusion performance via three widely used hyperspectral image datasets.","Independent component analysis,
Hyperspectral imaging,
Hyperspectral sensors,
Visualization,
Image quality,
Computer displays,
Image color analysis,
Application software,
Image resolution,
Mutual information"
A Biochip Microarray Fabrication System Using Inkjet Technology,"An automated biological fluid dispensing system for microarray fabrication using inkjet technology is presented in this paper. The hardware of the system consists of a high-precision motion system, a robotic material handling system, computer control systems, and extensive vision systems. The system software includes algorithms to handle control, communication, and manufacturing process information. This fluid dispensing system has been successfully used to produce cDNA microarrays and in the investigation of protein microarray manufacturing.","Fabrication,
Automatic control,
Communication system control,
Control systems,
Hardware,
Robot vision systems,
Robotics and automation,
Materials handling,
Biology computing,
Computer vision"
A Distributed Trust-based Reputation Model in P2P System,"The P2P system is an anonymous and dynamic system, thus, some malicious behaviour can't be punished. In order to restrict the malicious behaviour in the P2P system, researchers have focused on establishing effective reputation systems. However, the present reputation system can't avoid the trick of the false reputation feedback. We propose a distributed trust-based reputation model in p2p system (TBRM) to avoid it. The TBRM algorithm differentiated the node's capability of providing honest quality by the nodes reputation value, and the honest evaluation by the trust value. In our model, the reputation value represented the resource quality of the provider, thus, other nodes would like this node have low reputation value. At this time, the false reputation feedback happened. In this paper, we used the trust value to restrict the false reputation feedback For the nodes with low trust value was difficult to get the required resource, we punished the false reputation feedback by low their trust value. We show by both theoretical analysis and simulations that the proposed TBRM algorithm can get quick convergence which is 11 times, high equity for both low and high reputation nodes, and can get a high successful rate of file- downloading. When the malicious nodes' rate is 80%, the proposed TBRM is about 95% successful rate, compared to the algorithm without reputation who is only 27%.","Peer to peer computing,
Feedback,
Distributed computing,
Analytical models,
Resists,
Digital signatures,
Social network services,
Software engineering,
Artificial intelligence,
Computer science"
"NEAT Particles: Design, Representation, and Animation of Particle System Effects","Particle systems are a representation, computation, and rendering method for special effects such as fire, smoke, explosions, electricity, water, magic, and many other phenomena. This paper presents NEAT particles, a new design, representation, and animation method for particle systems tailored to real-time effects in video games and simulations. In NEAT particles, the neuroevolution of augmenting topologies (NEAT) method evolves artificial neural networks (ANN) that control the appearance and motion of particles. NEAT particles affords three primary advantages over traditional particle effect development methods. First, it decouples the creation of new particle effects from mathematics and programming, enabling users with little knowledge of either to produce complex effects. Second, it allows content designers to evolve a broader range of effects than typical development tools through a form of interactive evolutionary computation (IEC). And finally, it acts as a concept generator, allowing users to interactively explore the space of possible effects. In the future such a system may allow content to be evolved in the game itself, as it is played","Animation,
Artificial neural networks,
Space exploration,
Fires,
Explosions,
Real time systems,
Games,
Computational modeling,
Network topology,
Motion control"
Photometric Self-Calibration of a Projector-Camera System,"In this paper, we present a method for photometric self- calibration of a projector-camera system. In addition to the input transfer functions (commonly called gamma functions), we also reconstruct the spatial intensity fall-off from the center to fringe (commonly called the vignetting effect) for both the projector and camera. Projector-camera systems are becoming more popular in a large number of applications like scene capture, 3D reconstruction, and calibrating multi-projector displays. Our method enables the use of photometrically uncalibrated projectors and cameras in all such applications.","Photometry,
Cameras,
Transfer functions,
Computer displays,
Apertures,
Image reconstruction,
Calibration,
Dynamic range,
Computer science,
Layout"
Teleoperation System of the Internet-based Omnidirectional Mobile Robot with A Mounted Manipulator,"A platform of the Internet-based teleoperation system with an omni-directional mobile robot which has a five DOFs robot arm is constructed. Remote control of the robot through the Internet is implemented. The system is featured as low-cost and user interface friendly: remote users can control the robot through the Internet just by a client program in a general computer. The client computer can receive the live video and environment information measured by sensors. With the help of the remote video and the local simulation, users can easily communicate with the robot. Different modules are proposed and the implementation method of the system is presented. Related experiments are conducted to test the validity of the proposed system.",
Security and privacy protection for automated video surveillance,"In this paper, we present an automated video surveillance system designed to 1) ensure efficient selective storage of data, 2) provide means for enhancing privacy protection, and 3) secure visual data against malicious attacks. The proposed solution is a 3-module system processing captured video data before storage. Salient motion detection is used to retain relevant sequences and identify regions of interest with potential privacy-sensitive details. Then, an invertible and secure privacy preserving process is performed using a DCT-based scrambling technique on selected regions. To secure visual data and allow for data authentication, a self-embedding watermarking technique is applied on each image sequence. It offers the capability of proving authenticity as well as locating manipulated regions. Furthermore, this technique is also able to recover and reconstruct a good approximation of original lost content. In addition to a low computational complexity, simulation results show the effectiveness of the whole system in achieving its goals in terms of security and privacy enhancement of automated video surveillance data.","Protection,
Video surveillance,
Data security,
Data privacy,
Secure storage,
Storage automation,
Motion detection,
Authentication,
Watermarking,
Image sequences"
Influence of the network structure on robustness,"The classical connectivity is typically used to capture the robustness of networks. Robustness, however, encompasses more than this simple definition of being connected. A spectral metric, referred to as the algebraic connectivity, plays a special role for the robustness since it measures the extent to which it is difficult to cut the network into independent components. We rely on the algebraic connectivity to study the robustness to random node and link failures in three important network models: the random graph of Erdos-Renyi, the small-world graph of Watts and Strogatz and the scale-free graph of Barabasi-Albert. We show that the robustness to random node and link failures significantly differs between the three models. This points to explicit influence of the network structure on the robustness. The homogeneous structure of the random graph of Erdos-Renyi implies an invariant robustness under random node failures. The heterogeneous structure of the small-world graph of Watts and Strogatz and scale-free graph of Barabasi-Albert, on the other hand, implies a non-trivial robustness to random node and link failures.","Robustness,
Laplace equations,
Network topology,
Computer networks,
Eigenvalues and eigenfunctions,
Neural networks,
Mathematics,
Computer science,
Joining processes,
Computational modeling"
Separating Parts from 2D Shapes using Relatability,"It's often important to analyze shapes as made up of parts. But there are two ways to think of how parts fit together. We can characterize the remainder of a shape after apart is removed; here we want to cut the shape so what remains has the simplest possible structure. Alternatively, we can cut out the part so that the part itself takes on a simple shape. These cuts do not directly give rise to a segmentation of the shape; a point inside the shape may associate with the part, the remainder, neither, or both. We present a new model for reconstructing these cuts based on the differential geometry of smoothed local symmetries. The model takes into account relatability (which characterizes clean cuts) to determine part boundaries. Our approach complements and unifies existing work on part- based segmentation of shape, and can be used to construct interesting simplifications of shapes.","Shape,
Tail,
Computer vision,
Marine animals,
Application software,
Mathematical model,
Computer science,
Cognitive science,
Solid modeling,
Geometry"
Modeling of Advanced Multilayered Packages with Multiple Vias and Finite Ground Planes,"A system-level modeling approach, which combines the MoM (method of moments) and the SMM (scattering matrix method) has been presented for the modeling of advanced electronic packages (Oo et al., 2007). The focus of this paper is on addressing the problems of multilayered multiple via coupling and finite ground effects by the SMM method. Significant extensions of the SMM method facilitate the modeling of coupling among densely populated vias in multilayered packages with finite power/ground planes. The proposed approach is suitable for the signal and power integrity, and even EMI analysis of packages at system level.","Electronics packaging,
Power system modeling,
Electromagnetic scattering,
Electromagnetic waveguides,
Moment methods,
Signal analysis,
Electromagnetic interference,
Power systems,
Circuit simulation,
Transmission line matrix methods"
Secure k-Connectivity Properties of Wireless Sensor Networks,"A k-connected wireless sensor network (WSN) allows messages to be routed via one (or more) of at least k node-disjoint paths, so that even if some nodes along one of the paths fail, or are compromised, the other paths can still be used. This is a much desired feature in fault tolerance and security, k-connectivity in this context is largely a well-studied subject. When we apply the random key pre-distribution scheme to secure a WSN however, and only consider the paths consisting entirely of secure (encrypted and/or authenticated) links, we are concerned with the secure k-connectivity of the WSN. This notion of secure k-connectivity is relatively new and no results are yet available. The random key pre-distribution scheme has two important parameters: the key ring size and the key pool size. While it has been determined before the relation between these parameters and 1-connectivity, our work in k-connectivity is new. Using a recently introduced random graph model called kryptograph, we derive mathematical formulae to estimate the asymptotic probability of a WSN being securely k-connected, and the expected secure k-connectivity, as a function of the key ring size and the key pool size. Finally, our theoretical findings are supported by simulation results.","Wireless sensor networks,
Routing,
Computer networks,
Fault tolerance,
Cryptography,
Mathematical model,
Mission critical systems,
Security,
Telecommunication traffic,
Hardware"
Noise-Coupled Multi-Cell Delta-Sigma ADCs,"Noise-coupled multi-cell delta-sigma ADCs are presented. By providing quantization noise coupling between identical delta-sigma ADCs, one can effectively raise the order of the modulator. Cross-coupled noise injection also provides an efficient way to realize higher-order time-interleaved ADCs.","Quantization,
Delta modulation,
Transfer functions,
Noise shaping,
Filters,
Integrated circuit noise,
Virtual colonoscopy,
Stability,
Hydrogen,
Computer science"
EDA Approach for Model Based Localization and Recognition of Vehicles,"We address the problem of model based recognition. Our aim is to localize and recognize road vehicles from monocular images in calibrated scenes. A deformable 3D geometric vehicle model with 12 parameters is set up as prior information and Bayesian Classification Error is adopted for evaluation of fitness between the model and images. Using a novel evolutionary computing method called EDA (Estimation of Distribution Algorithm), we can not only determine the 3D pose of the vehicle, but also obtain a 12 dimensional vector which corresponds to the 12 shape parameters of the model. By clustering obtained vectors in the parameter space, we can recognize different types of vehicles. Experimental results demonstrate the effectiveness of the approach to vehicles of different types and poses. Thanks to EDA, we can not only localize and recognize vehicles, but also show the whole evolution procedure of the deformable model which gradually fits the image better and better.",
Classification of EEG signals using different feature extraction techniques for mental-task BCI,"The use of electroencephalogram (EEG) or ""brain waves"" for human-computer interaction is a new and challenging field that has gained momentum in the past few years. If several mental states can be reliably distinguished by recognizing patterns in EEG, then a paralyzed person could communicate to a device like a wheelchair by composing sequences of these mental states. In this research, EEG from one subject who performed three mental tasks have been classified using radial basis function (RBF) support vector machines (SVM) to control overfitting. A method for EEG preprocessing based on independent component analysis (ICA) was proposed and three different feature extraction techniques were compared: parametric autoregressive (AR) modeling, AR spectral analysis and power differences between four frequency bands. The best classification accuracy was approximately 70% using the parametric AR model representation with almost 5% improvement of accuracy over unprocessed data.","Electroencephalography,
Feature extraction,
Support vector machines,
Support vector machine classification,
Independent component analysis,
Brain modeling,
Pattern recognition,
Wheelchairs,
Spectral analysis,
Frequency"
Exploiting Prior Knowledge in the Recovery of Non-Sparse Signals from Noisy Random Projections,"This paper illustrates that exploiting the source statistics in the recovery process results in significant performance gains, even if the signal is reconstructed in a basis in which it does not admit a sparse representation. Successful recovery will depend on the capability of exploiting all available a priori information in the basis where reconstruction is performed. The proposed framework is similar to joint source-channel coding schemes in digital communications, but applies on the analog domain.","Hidden Markov models,
Compressed sensing,
Signal processing,
Stochastic processes,
State estimation,
Linear approximation,
Vectors,
Statistics,
Performance gain,
Digital communication"
Clone Detection via Structural Abstraction,"This paper describes the design, implementation, and application of a new algorithm to detect cloned code. It operates on the abstract syntax trees formed by many compilers as an intermediate representation. It extends prior work by identifying clones even when arbitrary subtrees have been changed. On a 440,000-line code corpus, 20-50% of the clones it detected were missed by previous methods. The method also identifies cloning in declarations, so it is somewhat more general than conventional procedural abstraction.",
Sequential Architecture for Efficient Car Detection,"Based on multi-cue integration and hierarchical SVM, we present a sequential architecture for efficient car detection under complex outdoor scene in this paper. On the low level, two novel area templates based on edge and interest-point cues respectively are first constructed, which can be applied to forming the identities of visual perception to some extent and thus utilized to reject rapidly most of the negative non-car objects at the cost of missing few of the true ones. Moreover on the high level, both global structure and local texture cues are exploited to characterize the car objects precisely. To improve the computational efficiency of general SVM, a solution approximating based two-level hierarchical SVM is proposed. The experimental results show that the integration of global structure and local texture properties provides more powerful ability in discrimination of car objects from non-car ones. The final high detection performance also contributes to the utilizing of two novel low level visual cues and the hierarchical SVM.","Object detection,
Support vector machines,
Face detection,
Support vector machine classification,
Vehicle detection,
Computational efficiency,
Information science,
Laboratories,
Layout,
Pattern recognition"
Face Tracing Based Geographic Routing in Nonplanar Wireless Networks,"Scalable and efficient routing is a main challenge in the deployment of large ad hoc wireless networks. An essential element of practical routing protocols is their accommodation of realistic network topologies. In this paper, we study geographic routing in general large wireless networks. Geographic routing is a celebrated idea that uses the locations of nodes to effectively support routing. However, to guarantee delivery, recent geographic routing algorithms usually resort to perimeter routing, which requires the removal of communication links to get a planar sub-network on which perimeter routing is performed. Localized network planarization requires the wireless network to be a unit-disk graph (UDG) or its close approximation. For networks that significantly deviate from the UDG model, a common case in practice, substantially more expensive and non-localized network planarization methods have to be used. How to make such methods efficiently adaptable to network dynamics, and how to avoid the removal of an excessive number of links that leads to lowered routing performance, are still open problems. To enable efficient geographic routing in general wireless networks, we present face-tracing based routing, a novel approach that routes the message in the faces of the network that are virtually embedded in a topological surface. Such faces are easily recognizable and constructible, and adaptively capture the important geometric features in wireless networks - in particular, holes, -thus leading to very efficient routing. We show by both analysis and si mulations that the face-tracing based routing is a highly scalable routing protocol that generates short routes, incurs low overhead, adapts quickly to network dynamics, and is very robust to variations in network models.","Wireless networks,
Planarization,
Peer to peer computing,
Semiconductor device modeling,
Partitioning algorithms,
Routing protocols,
Communications Society,
Computer science,
Floods,
Euclidean distance"
Protecting Cryptographic Keys from Memory Disclosure Attacks,"Cryptography has become an indispensable mechanism for securing systems, communications and applications. While offering strong protection, cryptography makes the assumption that cryptographic keys are kept absolutely secret. In general this assumption is very difficult to guarantee in real life because computers may be compromised relatively easily. In this paper we investigate a class of attacks, which exploit memory disclosure vulnerabilities to expose cryptographic keys. We demonstrate that the threat is real by formulating an attack that exposed the private key of an OpenSSH server within 1 minute, and exposed the private key of an Apache HTTP server within 5 minutes. We propose a set of techniques to address such attacks. Experimental results show that our techniques are efficient (i.e., imposing no performance penalty) and effective - unless a large portion of allocated memory is disclosed.","Protection,
Cryptography,
Web server,
Operating systems,
Software tools,
Application software,
Concrete,
Memory management,
Hardware,
Computer science"
The best fingerprint compression standard yet,"Modern fingerprint compression and reconstruction standards, such as those used by the US Federal Bureau of Investigation FBI), are based upon the 9/7 discrete wavelet transform. This paper describes how a genetic algorithm was used to evolve wavelet and scaling numbers for each level of a multiresolution analysis (MRA) transform that consistently outperforms the 9/7 wavelet for fingerprint compression and reconstruction tasks. Our evolved transforms also improve upon wavelets optimized by a genetic algorithm via the lifting scheme, and thus establish a new state-of-the-art in this important application area.","Fingerprint recognition,
Discrete wavelet transforms,
Multiresolution analysis,
Quantization,
Image reconstruction,
Genetic algorithms,
Military computing,
Wavelet analysis,
History,
Hydrogen"
Synthesizing goal-directed actions from a library of example movements,"We present a new learning framework for synthesizing goal-directed actions from example movements. The approach is based on the memorization of training data and locally weighted regression to compute suitable movements for a large range of situations. The proposed method avoids making specific assumptions about an adequate representation of the task. Instead, we use a general representation based on fifth order splines. The data used for learning comes either from the observation of events in the Cartesian space or from the actual movement execution on the robot. Thus it informs us about the appropriate motion in the example situations. We show that by applying locally weighted regression to such data, we can generate actions having proper dynamics to solve the given task. To test the validity of the approach, we present simulation results under various conditions as well as experiments on a real robot.","Libraries,
Humanoid robots,
Physics computing,
Hidden Markov models,
Orbital robotics,
Robotics and automation,
Cybernetics,
Laboratories,
Computer science,
Data engineering"
Enhancing Edge Computing with Database Replication,"As the use of the Internet continues to grow explosively, edge computing has emerged as an important technique for delivering Web content over the Internet. Edge computing moves data and computation closer to end-users for fast local access and better load distribution. Current approaches use caching, which does not work well with highly dynamic data. In this paper, we propose a different approach to enhance edge computing. Our approach lies in a wide area data replication protocol that enables the delivery of dynamic content with full consistency guarantees and with all the benefits of edge computing, such as low latency and high scalability. What is more, the proposed solution is fully transparent to the applications that are brought to the edge. Our extensive evaluations in a real wide area network using TPC-W show promising results.","Delay,
Wide area networks,
Scalability,
Distributed computing,
Network servers,
Transaction databases,
Internet,
Web server,
Computer networks,
Middleware"
Concurrent Multiple Instance Learning for Image Categorization,"We propose a new multiple instance learning (MIL) algorithm to learn image categories. Unlike existing MIL algorithms, in which the individual instances in a bag are assumed to be independent with each other, we develop concurrent tensors to explicitly model the inter-dependency between the instances to better capture image's inherent semantics. Rank-1 tensor factorization is then applied to obtain the label of each instance. Furthermore, we formulate the classification problem in the reproducing kernel Hilbert space (RKHS) to extend instance label prediction to the whole feature space. Finally, a regularizer is introduced, which avoids overfitting and significantly improves learning machine's generalization capability, similar to that in SVMs. We report superior categorization performances compared with key existing approaches on both the COREL and the Caltech datasets.","Tensile stress,
Layout,
Machine learning,
Labeling,
Asia,
Kernel,
Hilbert space,
Couplings,
Automation,
Digital photography"
An AM-FM model for Motion Estimation in Atherosclerotic Plaque Videos,We present new multidimensional amplitude-modulation frequency-modulation (AM-FM) methods for motion estimation. For a single AM-FM component we show that the optical flow constraint leads to separate equations for amplitude modulation (AM) and frequency modulation (FM). We compare our approach with phase-based estimation developed by Fleet and Jepson and also the original optical flow method by Horn and Schunck. An advantage of the proposed method is that it provides for dense estimates that remain accurate over the entire video. We also present preliminary results on atherosclerotic plaques videos where the AM method appears to work best.,"Motion estimation,
Videos,
Optical modulation,
Image motion analysis,
Frequency modulation,
Multidimensional systems,
Frequency estimation,
Optical devices,
Equations,
Amplitude modulation"
Reconsidering power management,"Power-management approaches have been widely studied in an attempt to conserve idling energy by allowing nodes to switch to a low-power sleep mode. However, due to the inherent inability of current approaches to match sleep schedules to different traffic patterns, energy is wasted switching needlessly from sleep to idle or large delays in traffic delivery are incurred due to being in the sleep state too long. In this paper, we explore such effects of various traffic patterns on current power management protocols. Our results show the importance of traffic information to obtain larger benefits from power management. While some proposals that exploit traffic information exist, they rely primarily on individual sender traffic patterns to develop sleep schedules, ignoring aggregate traffic observed by receivers. This deficiency motivates the design of a new power management protocol that use traffic information at the receivers to adapt sleep schedules.","Energy management,
Protocols,
Traffic control,
Telecommunication traffic,
Energy consumption,
Costs,
Switches,
Energy conservation,
Scheduling,
Computer science"
High performances supercapacitor recovery system including Power Factor Correction (PFC) for elevators,"In this paper a novel and high efficiency conversion system based on a Supercapacitor Recover System (SRS) and a Power Factor Correction circuit (PFC) is proposed and tested. It allows to greatly improve the performances of motion systems for residential application, such as elevators, both in terms of energy efficiency and power demand, allowing the compliance with the recents international standards on power quality.","Supercapacitors,
Power factor correction,
Elevators,
Power quality,
Power demand,
Circuit testing,
Inverters,
Resistors,
Energy storage,
Batteries"
Characteristics of Power-Line Channels in Cargo Ships,"The broadband PLC in cargo ships is a long-awaited technology. However the characteristics of power-line channels in the cargo ships have not been studied yet. The most unique point of the channels is that power-line cables used in most Asian cargo ships are shielded. In order to characterize the channel and investigate a suitable PLC system to the cargo ships, some parameters were measured in three typical ships. The parameters were the transfer functions (LCTR, LTR and TTR), LCL and stationary noise spectrum. From these measurements, (1) the required dynamic range of receivers, (2) average attenuation at distribution boards, (3) phase coupling loss and (4) average LCL values were derived in this paper. Since the cables are shielded and the hull provides the good ground, the usage of the common-mode transmission in addition to the conventional differential-mode, which is referred to as a dual-mode transmission system, is also proposed. Therefore the measured parameters above were included the common-mode and mode-conversion properties. As the evidence to support our proposal, some results in which the LCTR values were superior to the TTR values are shown.","Marine vehicles,
Programmable control,
Cables,
Transfer functions,
Attenuation measurement,
Loss measurement,
Phase measurement,
Dynamic range,
Couplings,
Proposals"
Visualizing the Analysis of Dynamically Adaptive Systems Using i* and DSLs,"Self-adaptation is emerging as a crucial enabling capability for many applications, particularly those deployed in dynamically changing environments. One key challenge posed by dynamically adaptive systems (DASs) is the need to handle changes to the requirements and corresponding behavior of a DAS in response to varying environmental conditions. In this paper we propose a visual model-driven approach that uses the i* modeling language to represent goal models for the DAS requirements. Our approach applies a rigorous separation of concerns between the requirements for the DAS to operate in stable conditions and those that enable it to adapt at run-time to enable it to cope with changes in its environment. We further show how requirements derived from the i* modeling can be used by a domain-specific language to achieve requirements model-driven development. We describe our experiences with applying this approach to GridStix, an adaptive flood warning system, deployed on the River Ribble in North Yorkshire, England.",
A Uniformly Distributed Adaptive Clustering Hierarchy Routing Protocol,"Energy efficiency is one of the most important considerations in wireless sensor networks. In a clustered sensor network, some of the nodes become cluster heads. They aggregate the data from their cluster members and transmit it to the base station. In this paper, we propose a uniformly distributed adaptive clustering hierarchy routing protocol (UDACH). Unlike LEACH and LEACH-C, our protocol is based on energy equilibrium instead of choosing cluster heads randomly. We divide the protocol into three stages: cluster construction, building a cluster head tree, and sending data. In cluster construction stage, cluster heads are elected and each node identifies its cluster head. In the stage of building a cluster head tree, the tree is created based on the weight of each cluster head. And the weight is inversely proportional to the distance from the base station to the node. In the last stage, each cluster member collects the data and sends it to the base station by its cluster head. Our proposed protocol is simulated and compared with LEACH and LEACH-C. The simulation result shows that UDACH has better performance in network lifetime and energy expense than LEACH and LEACH-C.","Routing protocols,
Wireless sensor networks,
Energy efficiency,
Base stations,
Buildings,
Radio transmitters,
Educational institutions,
Computer networks,
Distributed computing,
Aggregates"
Enabling Web Services Policy Negotiation with Privacy preserved using XACML,"In recent Web services research, there are increasing demands and discussions about negotiation technologies for different Web services applications. One of the important topics is the policy negotiation. As many business activities become automated, policy compliance negotiation between human agents can be a bottleneck. In this paper, we focus on the policy negotiation research issues in privacy policy. We adopt the extensible Access Control Markup Language (XACML) as a policy description language and explore its potential in privacy policy negotiation. We first formalize the negotiation process in the context of Web services. Then, we illustrate the policy negotiation model by introducing a policy negotiation point (PNP) between the policy enforcement point (PEP) and policy decision point (PDP) in the XACML policy management architecture. We discuss different phases in a privacy policy negotiation and finally we illustrate how PNP can help on negotiating policies through an example scenario","Web services,
Privacy,
Business,
Humans,
Security,
Computer science,
Information technology,
Access control,
Markup languages,
Context-aware services"
Architecture and Algorithm for a Laboratory Vehicle Collision Avoidance System,"In this paper we describe the application architecture for a collision avoidance system developed for a fleet of sensorless mobile vehicles. The system has been deployed in the IT Convergence Lab in the Coordinated Science Laboratory at the University of Illinois at Urbana-Champaign, which is a testbed for studying system architecture for networked embedded control systems. We describe several factors that a well designed collision avoidance algorithm needs to address, and discuss some of the tradeoffs and design decisions that need to be made. The solution that we have developed has a minimal effect on existing components for higher level functionality, as well their interfaces. The architecture and algorithm provide a low level safety guarantee regardless of higher level objectives. The architecture exploits the infrastructure and services provided by the control domain middleware, called Etherware, which has been developed in the laboratory. Indeed, the development of the collision avoidance system shows the usefulness of the specific services and abstractions that Etherware provides to the application designer in facilitating rapid system design and deployment.","Laboratories,
Vehicles,
Collision avoidance,
Algorithm design and analysis,
Convergence,
System testing,
Sensorless control,
Control systems,
Safety,
Middleware"
Local path planning in image space for autonomous robot navigation in unstructured environments,"An approach to stereo based local path planning in unstructured environments is presented. The approach differs from previous stereo based and image based planning systems (e.g. top-down occupancy grid planners, autonomous highway driving algorithms, and view-sequenced route representation), in that it uses specialized cost functions to find paths through an occupancy grid representation of the world directly in the image plane and forgoes a projection of cost information from the image plane down onto a top-down 2D Cartesian cost map. We discuss three cost metrics for path selection in image space. We present a basic image based planning system, discuss its susceptibility to rotational and translational oscillation, and present and implement two extensions to the basic system that overcome these limitations - a cylindrical based image system and a hierarchical planning system. All three systems are implemented in an autonomous robot and are tested against a standard top-down 2D Cartesian planning system on three outdoor courses of varying difficulty. We find that the basic image based planning system fails under certain conditions; however, the cylindrical based system is well suited to the task of local path planning and for use as a high resolution local planning component of a hierarchical planning system.","Path planning,
Orbital robotics,
Navigation,
Cost function,
Robot sensing systems,
Layout,
Intelligent robots,
Road transportation,
Production facilities,
Mobile robots"
Automatic Test Case Generation from UML Models,"This paper presents a novel approach of generating test cases from UML design diagrams. We consider use case and sequence diagram in our test case generation scheme. Our approach consists of transforming a UML use case diagram into a graph called use case diagram graph (UDG) and sequence diagram into a graph called the sequence diagram graph (SDG) and then integrating UDG and SDG to form the system testing graph (STG). The STG is then traversed to generate test cases. The test cases thus generated are suitable for system testing and to detect operational, use case dependency, interaction and scenario faults.","Automatic testing,
Unified modeling language,
Object oriented modeling,
System testing,
Computer science,
Design engineering,
Software testing,
Information technology,
Fault detection,
Application software"
Space charge in LLDPE loaded with nanoparticles,"Fillers are widely used in insulation systems to achieve specific electrical, mechanical and thermal properties. Recently, polymer nanocomposites have attracted significant attention as a means of improving their performance and widening their utility. To better use the nanocomposites, a thorough understanding of interaction between nanoparticles and their matrix is important. The interaction can be studied by various methods. In the present study we focus our attention on space charge dynamics of linear low density polyethylene (LLDPE) loaded with nanometric size aluminium oxide (alumina) under dc electric fields. To obtain a good dispersion of nano fillers in LLDPE, a Brabender mixer has been utilised. Four types of LLDPE nanocomposite films were prepared with concentrations of alumina of 0, 1, 5 and 10 wt% for space charge measurements. They were electrically stressed at three electric field levels of 10, 50 and 100 kV/mm and space charge dynamics during stressing period and decay after the removal of the applied electric field were observed. The results show that space charge dynamics in nanocomposites are dependent on both content of filler and the applied electric field. Generally, the amount of space charge in the sample with 1% of alumina is less while there is a significant change in characteristics in the sample loaded with 5% alumina or more. In particular, the electrical performance in the sample with 10% alumina deteriorates, indicating there is an optimal load of alumina.","Space charge,
Nanoparticles,
Nanocomposites,
Dielectrics and electrical insulation,
Mechanical factors,
Polymers,
Polyethylene,
Aluminum oxide,
Current measurement,
Charge measurement"
A New Method to Measure the Similarity Between Interval-Valued Fuzzy Numbers,"In this paper, we propose a new similarity measure between interval-valued fuzzy numbers. The proposed similarity measure considers the similarity of the gravities on the X-axis between upper fuzzy numbers, the difference of the spreads between upper fuzzy numbers, the heights of the upper fuzzy numbers, the degree of similarity on the X-axis between interval-valued fuzzy numbers, and the gravities on the Y-axis between interval-valued fuzzy numbers. We also prove three properties of the proposed similarity measure between interval-valued fuzzy numbers. We also use some data sets to compare the proposed similarity measure with Chen's method [1]. The proposed method gets more reasonable results than the method presented in [1] to calculate the degree of similarity between interval-valued fuzzy numbers.","Gold,
Gravity,
Fuzzy sets,
Machine learning,
Cybernetics,
Computer science,
Information filtering"
Cell Cluster Image Segmentation on Form Analysis,"In this paper, a segmentation algorithm based on form information is proposed for separation of touching and overlapping cells. The method integrating morphological smoothing with holes filling is employed in a pre-processing stage. Then the overlapping cell clusters are identified through a polygon approximation. Later the separation is implemented, which consists of the detecting concave points on the contours and determining the separation lines. The algorithm can split complicated large cell clusters. In addition, this algorithm can be adopted for other applications, where separation between touching and overlapping particles is needed.","Image segmentation,
Image analysis,
Clustering algorithms,
Skeleton,
Fires,
Computer science,
Smoothing methods,
Prototypes,
Image recognition,
Approximation algorithms"
The intelligent STB - implementation of next generation of residential gateway in digital home,"As the core of digital home, RG(residential gateway) distributes services to diverse electronic appliances through different mediums. In this paper, we propose a new solution using a more intelligent STB(set top box) to fulfill tasks of RG. On the one hand, it integrates the two equipments together to cut the costs; on the other hand, it provides much more functions with good quality. We adopt the two-CPU architecture to satisfy powerful computing demands. Upon it the middleware is indispensable due to the heterogeneity of home network. Then, abundant embedded applications at the top accomplished the ultimate tasks successfully. By the way, this intelligent STB is nominated RG-STB.","US Department of Transportation,
Roentgenium,
Home appliances,
Home automation,
Consumer electronics,
Internet,
Multimedia systems,
TV,
Access protocols,
Ethernet networks"
Real-time acoustic source localization in noisy environments for human-robot multimodal interaction,"Interaction between humans involves a plethora of sensory information, both in the form of explicit communication as well as more subtle unconsciously perceived signals. In order to enable natural human-robot interaction, robots will have to acquire the skills to detect and meaningfully integrate information from multiple modalities. In this article, we focus on sound localization in the context of a multi-sensory humanoid robot that combines audio and video information to yield natural and intuitive responses to human behavior, such as directed eye-head movements towards natural stimuli. We highlight four common sound source localization algorithms and compare their performance and advantages for real-time interaction. We also briefly introduce an integrated distributed control framework called DVC, where additional modalities such as speech recognition, visual tracking, or object recognition can easily be integrated. We further describe the way the sound localization module has been integrated in our humanoid robot, CB.","Acoustic noise,
Working environment noise,
Humanoid robots,
Robot sensing systems,
Human robot interaction,
Acoustic signal detection,
Context,
Distributed control,
Speech recognition,
Object recognition"
Assessing the scalability of a multiple robot interface,"As multiple robot systems become more common, it is necessary to develop scalable human-robot interfaces that permit the inclusion of additional robots without reducing the overall system performance. Workload and situational awareness play key roles in determining the ratio of m operators to n robots. A scalable interface, where m is much smaller than n, will have to manage the operator's workload and promote a high level of situation awareness. This work focused on the development of a scalable interface for a single human-multiple robot system. This interface introduces a relational “halo” display that augments a camera view to promote situational awareness and the management of multiple robots by providing information regarding the robots' relative locations with respect to a selected robot. An evaluation was conducted to determine the scalability of the interface focusing on the effects of increasing the number of robots on workload, situation awareness, and robot usage. Twenty participants completed two bomb defusing tasks: one employing six robots, the other nine. The results indicated that increasing the number of robots increased overall workload and the operator's situation awareness.","Weapons,
Cameras,
Robot vision systems,
Color,
Collision avoidance"
A Unified Streaming Architecture for Real Time Face Detection and Gender Classification,"An integral part of interactive computing environments are systems that have the ability to process information about their users in real-time. In many cases it is desirable to not only recognize a human user but also to extract as much information about the user as possible, such as gender, ethnicity, age, etc. In this paper we present an FPGA implementation of a neural network configured specifically for performing face detection and gender classification in real-time video streams. Our streaming architecture performs the face and gender classification tasks at 30 frames per second on a small sized Virtex-4 FPGA, at accuracy comparable to that of a leading commercial software implementation.","Face detection,
Computer architecture,
Field programmable gate arrays,
Streaming media,
Artificial neural networks,
Real time systems,
Neural networks,
Hardware,
Logic,
Computer science"
Connecting Scientific Data to Scientific Experiments with Provenance,"As scientific workflows and the data they operate on, grow in size and complexity, the task of defining how those workflows should execute (which resources to use, where the resources must be in readiness for processing etc.) becomes proportionally more difficult. While ""workflow compilers"", such as Pegasus, reduce this burden, a further problem arises: since specifying details of execution is now automatic, a workflow's results are harder to interpret, as they are partly due to specifics of execution. By automating steps between the experiment design and its results, we lose the connection between them, hindering interpretation of results. To reconnect the scientific data with the original experiment, we argue that scientists should have access to the full provenance of their data, including not only parameters, inputs and intermediary data, but also the abstract experiment, refined into a concrete execution by the ""workflow compiler"". In this paper, we describe preliminary work on adapting Pegasus to capture the process of workflow refinement in the PASOA provenance system.","Joining processes,
Computer science,
Concrete,
Distributed computing,
Astronomy,
Grid computing,
Educational institutions,
Image retrieval,
Error correction,
Costs"
Real Time Forward Kinematics Solutions for General Stewart Platforms,"A new paradigm is introduced for solving the forward kinematics of general Stewart platforms in real-time. It consists of an off-line preprocessing phase and an online realtime evaluation phase. In the preprocessing phase, the platform leg (link) space is decomposed into cells, and a large set of data is generated for the platform position/orientation (pose), and their corresponding link lengths are computed using the known inverse kinematics. Due to the existence of multiple solutions (poses) for a particular link vector, a data classification technique is employed to identify various solutions. The classified data are used to find the parameters of a simple model that represents the forward kinematics within a cell. These parameters are stored in a lookup table. During the online phase, given the link lengths, the appropriate cell is identified, the model parameters are retrieved from the lookup table and the poses are computed. The proposed method is tested on a Stewart platform, and the accuracy and online times are presented to show the effectiveness of the proposed method for real-time applications.","Nonlinear equations,
Leg,
Table lookup,
Robot kinematics,
Polynomials,
Robotics and automation,
Computer science,
Testing,
Aerospace simulation,
Machining"
A Multi-Gate MOSFET Compact Model Featuring Independent-Gate Operation,A compact model for multi-gate MOSFETs with two independently-biased gates is presented. The core model is verified against TCAD simulations without the use of any fitting parameters. Real device effects such as short channel effects and body doping effects are captured. The use of the model is demonstrated through two simulation examples: (1) Back-gate dynamic feedback of FinFET SRAM cells and (2) Tuning of device variations through back gate biasing.,"MOSFET circuits,
FinFETs,
Semiconductor process modeling,
Capacitance-voltage characteristics,
Feedback,
Random access memory,
Circuit synthesis,
Iterative algorithms,
Analytical models,
Electronic mail"
Polylogarithmic Independence Can Fool DNF Formulas,"We show that any k-wise independent probability measure on {0, 1}n can O(m2ldr2ldr2-radick/10)-fool any boolean function computable by an rn-clauses DNF (or CNF) formula on n variables. Thus, for each constant c > 0. there is a constant e > 0 such that any boolean function computable by an m-clauses DNF (or CNF) formula can be in m-e-fooled by any clog in-wise probability measure. This resolves, asymptotically and up to a logm factor, the depth-2 circuits case of a conjecture due to Linial and Nisan (1990). The result is equivalent to a new characterization of DNF (or CNF) formulas by low degree polynomials. It implies a similar statement for probability measures with the small bias property. Using known explicit constructions of small probability spaces having the limited independence property or the small bias property, we. directly obtain a large class of explicit PRG's ofO(log2 m log n)-seed length for m-clauses DNF (or CNF) formulas on n variables, improving previously known seed lengths.","Boolean functions,
Probability,
Machinery,
Computer science,
Polynomials,
Harmonic analysis,
Size measurement,
Counting circuits,
Random variables"
Knowledge-Based Reconfiguration of Automation Systems,"This article describes the work in progress on knowledge-based reconfiguration of a class of automation systems. The knowledge about manufacturing is represented in a number of formalisms and gathered around an ontology expressed in OWL, that allows generic reasoning in description logic. In the same time multiple representations facilitate efficient processing by a number of special-purpose reasoning modules, specific for the application domain. At the final stage of reconfiguration we exploit ontology-based rewriting, simplifying creation of the final configuration files.","Ontologies,
Artificial intelligence,
Manufacturing automation,
Knowledge representation,
Web sites,
Service robots,
Inspection,
Assembly systems,
Knowledge engineering,
USA Councils"
Impact of Variability on Clock Skew in H-tree Clock Networks,"Clock distribution networks play a key role in determining overall system performance. In this paper, the authors investigate the effect of parameter variations on the performance of a commonly used clock distribution structure, a H-tree clock network. The design of robust high performance clock networks face significant challenges due to increasing parameter variations in sub-65nm technologies. As shown in the results, the contribution of interconnect variations to clock skew has risen by upto 3 times from 180nm to 45nm technology. It also suggests that the effect of variability is most prominent at the second and third stages of the 5-stage H-tree clock network. This analysis will help develop mitigation techniques that focus on addressing specific failure mechanisms caused by variability in clock networks","Clocks,
Circuits,
Semiconductor device manufacture,
Failure analysis,
Design for manufacture,
Robustness,
Manufacturing processes,
Error correction codes,
Very large scale integration,
Computer science"
Utilization-Bound Based Schedulability Analysis of Weighted Round Robin Schedulers,"Schedulability analysis is a cornerstone of modern real-time scheduling theory development. Utilization- bound based schedulability test is considered one of most efficient and effective schedulability tests, because of its extremely low runtime overhead. Deriving the utilization bound for a given real-time system has, nevertheless, been a challenging task as it usually requires comprehensive modeling and understanding of the system and payload tasks. This paper is focused on deriving utilization bounds for weighted round robin schedulers. We demonstrate how to establish a unified modeling framework and then use it to derive utilization bounds. We obtain the optimal parameter selection that maximizes the utilization bound, then compare the new bound with those of fixed priority schedulers, and timed token ring schedulers. The new bound is further extended to systems with sporadic job requests. We argue that our modeling framework is highly versatile, and hence, can be easily tailored for analysis of other types of real-time systems.","Round robin,
Real time systems,
Processor scheduling,
Scheduling algorithm,
Payloads,
System testing,
Runtime,
Token networks,
Load management,
Computer science"
Modeling Vague Data with Genetic Fuzzy Systems under a Combination of Crisp and Imprecise Criteria,"Multicriteria genetic algorithms can produce fuzzy models with a good balance between their precision and their complexity. The accuracy of a model is usually measured by the mean squared error of its residual. When vague training data is used, the residual becomes a fuzzy number, and it is needed to optimize a combination of crisp and fuzzy objectives in order to learn balanced models. In this paper, we will extend the NSGA-II algorithm to this last case, and test it over a practical problem of causal modeling in marketing. Different setups of this algorithm are compared, and it is shown that the algorithm proposed here is able to improve the generalization properties of those models obtained from the defuzzified training data.","Fuzzy systems,
Training data,
Genetic algorithms,
Additive noise,
Stochastic resonance,
Position measurement,
Noise measurement,
Global Positioning System,
Probability distribution,
Computer science"
An Empirical Study of Web-Based Knowledge Community Success,"Web-based knowledge communities (WKCs) allow individuals with similar interests to collectively engage in knowledge acquisition and exchange. In spite of increased research interest in the topic of online knowledge exchange and social interaction, few studies have identified factors that contribute to WKC success. In this study we apply Preece's community success framework of usability and sociability and the information system (IS) success model as conceptual foundations for an investigation into the factors that lead to WKC success. We present and empirically test a research model for WKC success using survey data from an IT-related WKC. We find that both usability and sociability factors have a significant effect on user satisfaction which leads to increased sense of community and a greater frequency of WKC usage. This research is one of the first attempts to empirically examine Web-based knowledge community success. The implications for research and practice are also discussed",
Synergy of Lip-Motion and Acoustic Features in Biometric Speech and Speaker Recognition,"This paper presents the scheme and evaluation of a robust audio-visual digit-and-speaker-recognition system using lip motion and speech biometrics. Moreover, a liveness verification barrier based on a person's lip movement is added to the system to guard against advanced spoofing attempts such as replayed videos. The acoustic and visual features are integrated at the feature level and evaluated first by a support vector machine for digit and speaker identification and, then, by a Gaussian mixture model for speaker verification. Based on ap300 different personal identities, this paper represents, to our knowledge, the first extensive study investigating the added value of lip motion features for speaker and speech-recognition applications. Digit recognition and person-identification and verification experiments are conducted on the publicly available XM2VTS database showing favorable results (speaker verification is 98 percent, speaker identification is 100 percent, and digit identification is 83 percent to 100 percent).",
Enhanced Image Trans-coding Using Reversible Data Hiding,"The primary application of watermarking and data hiding is for authentication, or to prove the ownership of digital media. In this paper, a new trans-coding system with the help of the technique of watermarking and data hiding is proposed. Side information is extracted before the image transcoding, such as resizing. In this paper, we focus on the problem of resizing in ""thin edge"" region. ""Thin edge"" structure normally cannot preserve after resizing process and become discrete. As the ""thin edge"" region is difficult to analyze real time and with a high degree of accuracy, data hiding can be used. The information of the ""thin edge"" region is generated and embedding into the multimedia content in encoder. Experimental results shown that there is a great improvement in the visual quality by using side information.","Data encapsulation,
Watermarking,
Data mining,
Displays,
Image resolution,
PSNR,
Gold,
Data engineering,
Application software,
Authentication"
Storage@home: Petascale Distributed Storage,"Storage@home is a distributed storage infrastructure developed to solve the problem of backing up and sharing petabytes of scientific results using a distributed model of volunteer managed hosts. Data is maintained by a mixture of replication and monitoring, with repairs done as needed. By the time of publication, the system should be out of testing, in use, and available for volunteer participation.","Bandwidth,
Internet,
Distributed computing,
Search engines,
IP networks,
Petascale computing,
Computer science,
Computerized monitoring,
System testing,
Chemistry"
Improving lecture speech summarization using rhetorical information,"We propose a novel method of extractive summarization of lecture speech based on unsupervised learning of its rhetorical structure. We present empirical evidence showing that rhetorical structure is the underlying semantics which is then rendered in linguistic and acoustic/prosodic forms in lecture speech. We present a first thorough investigation of the relative contribution of linguistic versus acoustic features and show that, at least for lecture speech, what is said is more important than how it is said. We base our experiments on conference speeches and corresponding presentation slides as the latter is a faithful description of the rhetorical structure of the former. We find that discourse features from broadcast news are not applicable to lecture speech. By using rhetorical structure information in our summarizer, its performance reaches 67.87% ROUGE-L F-measure at 30% compression, surpassing all previously reported results. The performance is also superior to the 66.47% ROUGE-L F-measure of baseline summarization performance without rhetorical information. We also show that, despite a 29.7% character error rate in speech recognition, extractive summarization performs relatively well, underlining the fact that spontaneity in lecture speech does not affect the central meaning of lecture speech.","Broadcasting,
Data mining,
Principal component analysis,
Humans,
Speech recognition,
Hidden Markov models,
Natural languages,
Unsupervised learning,
Underwater acoustics,
Error analysis"
PKTown: A Peer-to-Peer Middleware to Support Multiplayer Online Games,"Peer-to-peer (P2P) system can support service for lots of end users with little hardware investment, which suits for the efforts to lower down the service cost for multiplayer online games (MOG). In this paper, we introduce PKTown, a P2P middleware inserted into Star Craft and the network layer. PKTown captures all packets generated by Star Craft. We apply application layer multicast (ALM) to transmit the broadcast packets through overlay network. PKTown extends the LAN game experiences to players belong to different LAN. We design a k-regular random overlay network based on game specific requirements analysis. No change is made on binary code of Star Craft. Preliminary test results show that PKTown works well and supports other MOG in the same way. Our contribution is to provide a new game experience method with little hardware investment.","Peer to peer computing,
Middleware,
Local area networks,
Hardware,
Investments,
Web server,
Costs,
Broadcasting,
Binary codes,
Testing"
SOPS: Stock Prediction Using Web Sentiment,"Recently, the web has rapidly emerged as a great source of financial information ranging from news articles to per- sonal opinions. Data mining and analysis of such financial information can aid stock market predictions. Traditional approaches have usually relied on predictions based on past performance of the stocks. In this paper, we introduce a novel way to do stock market prediction based on sentiments of web users. Our method involves scanning for financial message boards and extracting sentiments expressed by in- dividual authors. The system then learns the correlation between the sentiments and the stock values. The learned model can then be used to make future predictions about stock values. In our experiments, we show that our method is able to predict the sentiment with high precision and we also show that the stock performance and its recent web sentiments are also closely correlated.","Data mining,
Discussion forums,
Stock markets,
Information analysis,
Predictive models,
Blogs,
Conferences,
Computer science,
Educational institutions,
Data analysis"
Rotation Invariant Shape Contexts based on Feature-space Fourier Transformation,"We propose a new pixel-level shape descriptor. First, shape contexts are computed. Then, 2D FFT is performed on each 2D histogram from shape contexts. Such a scheme solves the rotation-invariance problem of shape contexts based on the shift theorem of Fourier transformation while does not increase the computational complexity. Theoretical proof and experimental validation are provided.",
A study on software reliability prediction based on support vector machines,"Support vector machines (SVMs) have been successfully used in many domains, while their application in software reliability prediction is still quite rare. A few SVM- based software reliability prediction models have been proposed in the literature; however, the accuracy of prediction can still be improved. In this paper, we propose an SVM-based model for software reliability prediction and we study issues that affect the prediction accuracy. These issues include: 1. Whether all historical failure data should be used; 2. What type of failure data is more appropriate to use in terms of prediction accuracy. We also compare the prediction accuracy of software reliability prediction models based on SVM and artificial neural network (ANN). Experimental results show that our proposed SVM-based software reliability prediction model could achieve a higher prediction accuracy compared with ANN-based and existing SVM-based models.","Software reliability,
Support vector machines,
Predictive models,
Accuracy,
Artificial neural networks,
Data analysis,
Software testing,
Industrial engineering,
Industrial electronics,
Computer industry"
On the prediction of the evolution of libre software projects,"Libre (free / open source) software development is a complex phenomenon. Many actors (core developers, casual contributors, bug reporters, patch submitters, users, etc.), in many cases volunteers, interact in complex patterns without the constrains of formal hierarchical structures or organizational ties. Understanding this complex behavior with enough detail to build explanatory models suitable for prediction is an open challenge, and few results have been published to date in this area. Therefore statistical, non-explanatory models (such as the traditional regression model) have a clear role, and have been used in some evolution studies. Our proposal goes in this direction, but using a model that we have found more useful: time series analysis. Data available from the source code management repository is used to compute the size of the software over its past life, using this information to estimate the future evolution of the project. In this paper we present this methodology and apply it to three large projects, showing how in these cases predictions are more accurate than regression models, and precise enough to estimate with little error their near future evolutions.","Open source software,
Predictive models,
Programming,
Project management,
Software engineering,
Proposals,
Time series analysis,
Life estimation,
Computer errors,
Computer science"
Design Considerations for a Personalized Wheelchair Navigation System,"Individuals with mobility impairments such as wheelchair users are often at a disadvantage when traveling to a new place, as their mobility can be easily affected by environmental barriers, and as such, even short trips can be difficult and perhaps impossible. We envision a personalized wheelchair navigation system based on a PDA equipped with wireless Internet access and GPS that can provide adaptive navigation support to wheelchair users in any geographic environment. Requirements, architectures and components of such a system are described in this paper.","Wheelchairs,
Navigation,
Global Positioning System,
Geographic Information Systems,
Internet,
Intelligent sensors,
Buildings,
Roads,
Wireless communication,
Communications technology"
A Quality Model for Open Source Software Selection,"Although the need for developing software using Open Source Software(OSS) has been increasing, there is no measurement to select a suitable and qualified OSS which is proper property to develop.In this study, it is suggested whom wants to develop software by using OSS that a quality model to choose the best quality which is suitable characteristics of development.To conduct the proposal quality model: 1.Select 23 development companies which have experience in developing software with OSS. 2.Sample among the 23 companies which has ISO/IEC 9126 certification and additional characteristics by the order of priority3.From top-15 sub-characteristics, group, combine and classify by characteristics.Finally, we arrive at the quality model abstracted from 4 main-characteristics and 10 sub- characteristics.The quality model provides the criteria for measuring quality to select OSS and fundamental researching metrics and indicates the applicable method for practical and basic information.","Open source software,
ISO standards,
IEC standards,
Software quality,
Software measurement,
Software standards,
Measurement standards,
Information technology,
Computer science,
Proposals"
Coordinated control and operation of DFIG and FSIG based Wind Farms,"This paper considers a coordinated control strategy for adjacent FSIG and DFIG based wind farms whereby the reactive power output of the DFIG is prioritised over real power after a fault on the network, in order to boost the FSIG terminal voltage and improve its fault ride through capability. The issue of protection for a DFIG based wind turbine during network fault is also considered. It is seen that the large rotor currents can generally be ascribed to the small converter rating. Different protection arrangements for the converter are examined. The simulation results show that the stability margin of a FSIG can be significantly greater when there is extra reactive power compensation available from a DFIG in the vicinity.","Wind farms,
Wind turbines,
Reactive power,
Reactive power control,
Induction generators,
Protection,
Rotors,
Stability,
Voltage control,
Stators"
A Novel Algorithm of Image Gaussian Noise Filtering based on PCNN Time Matrix,"The problem of image Gaussian noise filtering in the framework of Pulse Coupled Neural Network (PCNN) time matrix is addressed. The time matrix, generated by PCNN, contains useful information related to spatial structure of the image under processing. It is a mapping from image spatial information to time sequence. Through time matrix, Gaussian noisy pixels can be detected and then processed by using five methods respectively. Computer simulations show that Gaussian noise can be reduced efficiently, and visual effect of restored images by using the proposed algorithm is much better than those by using traditional noise reduction methods, such as Median Filter, Mean Filter and even Wiener Filter. The proposed algorithm presents higher Peak Signal-to-Noise Ratio, better capability to reduce noise and better protection to edges and details of images. It is a novel Gaussian noise filtering method, which is comparable to Wiener Filter.",
Indoor Marker-based Localization Using Coded Seamless Pattern for Interior Decoration,"Because marker-based position tracking system is inexpensive and easy to use, it has the potential to make the system more feasible for homes or businesses and broaden the current suite of augmented reality (AR) techniques. To apply marker-based systems to homes or businesses, blending markers with environments naturally is important. Our innovative approach to marker-based 3D position tracking uses seamless patterns encrypted with positional data. Although users can obtain 3D positional data by processing marker images similar to many existing marker-based systems, our markers are designed with interior decoration in mind. That way they can be installed in walls, floors, or ceilings. Unlike existing systems whose fiducial markers were designed first and foremost to be processed by computers, ours are visually attractive. By integrating positional information within the interior design, our system enables users to enjoy the benefits of position tracking without being constantly aware of the system's presence. We developed a method for making patterns in which positional information is encrypted and a method for calculating the 3D position of a user by decoding those patterns. We then constructed a system using three patterns that were made by the proposed method and evaluated that system","Information science,
Cameras,
Image processing,
Augmented reality,
Cryptography,
Floors,
Navigation,
Wearable computers,
Costs,
Global Positioning System"
Fault-Tolerant Topology Control for Heterogeneous Wireless Sensor Networks,This paper addresses fault-tolerant topology control in a heterogeneous wireless sensor network consisting of several resource-rich supernodes used for data relaying and a large number of energy constrained wireless sensor nodes. We introduce the fc-degree Anycast Topology Control (fc-ATC) problem with the objective of selecting each sensor's transmission range such that each sensor is fc-vertex supernode connected and the maximum sensor transmission power is minimized. Such topologies are needed for applications that support sensor data reporting even in the event of failures of up to k-1 sensor nodes. We propose two solutions for the k-ATC problem: a greedy centralized algorithm that produces the optimal solution and a distributed and localized algorithm that incrementally adjusts sensors' transmission range such that the k-vertex supernode connectivity requirement is met. Simulation results are presented to verify our approaches.,"Fault tolerance,
Network topology,
Wireless sensor networks,
Relays,
Energy consumption,
Delay,
Approximation algorithms,
Computer science,
Data engineering,
Power engineering and energy"
An Empirical Study of Object Category Recognition: Sequential Testing with Generalized Samples,"In this paper we present an empirical study of object category recognition using generalized samples and a set of sequential tests. We study 33 categories, each consisting of a small data set of 30 instances. To increase the amount of training data we have, we use a compositional object model to learn a representation for each category from which we select 30 additional templates with varied appearance from the training set. These samples better span the appearance space and form an augmented training set OmegaT of 1980 (60times33) training templates. To perform recognition on a testing image, we use a set of sequential tests to project OmegaT into different representation spaces to narrow the number of candidate matches in OmegaT. We use""graphlets""(structural elements), as our local features and model OmegaT at each stage using histograms of graphlets over categories, histograms of graphlets over object instances, histograms of pairs of graphlets over objects, shape context. Each test is increasingly computationally expensive, and by the end of the cascade we have a small candidate set remaining to use with our most powerful test, a top-down graph matching algorithm. We achieve an 81.4 % classification rate on classifying 800 testing images in 33 categories, 15.2% more accurate than a method without generalized samples.","Sequential analysis,
Testing,
Histograms,
Training data,
Image recognition,
Statistical analysis,
Computer science,
Performance evaluation,
Context modeling,
Shape"
Game Relations and Metrics,"We consider two-player games played over finite state spaces for an infinite number of rounds. At each state, the players simultaneously choose moves; the moves determine a successor state. It is often advantageous for players to choose probability distributions over moves, rather than single moves. Given a goal (e.g., ""reach a target state""), the question of winning is thus a probabilistic one: ""what is the maximal probability of winning from a given state?"". On these game structures, two fundamental notions are those of equivalences and metrics. Given a set of winning conditions, two states are equivalent if the players can win the same games with the same probability from both states. Metrics provide a bound on the difference in the probabilities of winning across states, capturing a quantitative notion of state ""similarity"". We introduce equivalences and metrics for two-player game structures, and we show that they characterize the difference in probability of winning games whose goals are expressed in the quantitative mu-calculus. The quantitative mu- calculus can express a large set of goals, including reachability, safety, and omega-regular properties. Thus, we claim that our relations and metrics provide the canonical extensions to games, of the classical notion of bisimulation for transition systems. We develop our results both for equivalences and metrics, which generalize bisimulation, and for asymmetrical versions, which generalize simulation.","Probability distribution,
Computer science,
State-space methods,
Heart,
Safety,
Stochastic systems,
Minimax techniques,
Logic,
Kernel,
Cost accounting"
"Exploring the interplay of yield, area, and performance in processor caches","The deployment of future deep submicron technology calls for a careful review of existing cache organizations and design practices in terms of yield and performance. This paper presents a cache design flow that enables processor architects to consider yield, area, and performance (YAP) together in a unified framework. Since there is a complex, changing trade-off between these metrics depending on the technology, the cache organization, and the yield enhancement scheme employed, such a design flow becomes invaluable to processor architects when they assess a design and explore the design space quickly at an early stage. We develop a complete set of tools supporting the proposed design flow, from injecting defects into a wafer to evaluating program performance of individual processors in the wafer. A case study is presented to demonstrate the effectiveness of the proposed design flow and developed tools.","microprocessor chips,
cache storage,
integrated circuit design,
logic design"
Multievent Crisis Management Using Noncooperative Multistep Games,"The optimal allocation of resources to emergency locations in the event of multiple crises in an urban environment is an intricate problem, especially when the available resources are limited. In such a scenario, it is important to allocate emergency response units in a fair manner based on the criticality of the crisis events and their requests. In this research, a crisis management tool is developed which incorporates a resource allocation algorithm. The problem is formulated as a game-theoretic framework in which the crisis events are modeled as the players, the emergency response centers as the resource locations with emergency units to be scheduled, and the possible allocations as strategies. The payoff is modeled as a function of the criticality of the event and the anticipated response times. The game is played assuming a specific region within a certain locality of the crisis events to derive an optimal allocation. If a solution is not feasible, the perimeter of the locality in consideration is increased and the game is repeated until convergence. Experimental results are presented to illustrate the efficacy of the proposed methodology and metrics are derived to quantify the fairness of the solution. A regression analysis is performed to establish the statistical significance of the results",
Coordinated Multilevel Buffer Cache Management with Consistent Access Locality Quantification,"This paper proposes a protocol for effective coordinated buffer cache management in a multilevel cache hierarchy typical of a client/server system. Currently, such cache hierarchies are managed suboptimally-decisions about block placement and replacement are made locally at each level of the hierarchy without coordination between levels. Though straightforward, this approach has several weaknesses: 1) Blocks may be redundantly cached, reducing the effective total cache size, 2) weakened locality at lower-level caches makes recency-based replacement algorithms such as LRU less effective, and 3) high-level caches cannot effectively identify blocks with strong locality and may place them in low-level caches, The fundamental reason for these weaknesses is that the locality information embedded in the streams of access requests from clients is not consistently analyzed and exploited, resulting in globally nonsystematic, and therefore suboptimal, placement and replacement of cached blocks across the hierarchy. To address this problem, we propose a coordinated multilevel cache management protocol based on consistent access-locality quantification. In this protocol, locality is dynamically quantified at the client level to direct servers to place or replace blocks appropriately at each level of the cache hierarchy. The result is that the block layout in the entirely hierarchy dynamically matches the locality of block accesses. Our simulation experiments on both synthetic and real-life traces show that the protocol effectively ameliorates these caching problems. As anecdotal evidence, our protocol achieves a reduction of block accesses of 11 percent to 71 percent, with an average of 35 percent, over uniLRU, a unified multilevel cache scheme","cache storage,
client-server systems,
network operating systems,
protocols"
Toward Models for Forensic Analysis,"The existing solutions in the field of computer forensics are largely ad hoc. This paper discusses the need for a rigorous model of forensics and outlines qualities that such a model should possess. It presents an overview of a forensic model and an example of how to apply the model to a real-world, multi-stage attack. We show how using the model can result in forensic analysis requiring a much smaller amount of carefully selected, highly useful data than without the model",
High-Performance Millimeter-Wave SOP Technology with Flip-Chip Interconnection,"In this paper, we demonstrate the development of the system-on-package (SOP) technology using the SNU's deposited multi-chip module (MCM-D) technology for compact and high-performance millimeter-wave (mm-wave) modules. A distinctive feature of our MCM-D technology is the existence of Si-bumps and ground-bumps. The Si-bumps having a low coefficient of thermal expansion (CTE) and a high thermal conductivity can solve thermal and thermo-mechanical problems of the flip-chip structure. And the ground-bumps can make easy ground connection without deep-via process. From thermal analysis using a three-dimensional (3-D) finite element method (FEM) simulator, we confirmed that the proposed substrate has the significantly improved thermal performance. And the integrated passives such as the SiNx capacitor, the NiCr resistor, and the broadside Lange coupler were fabricated and characterized for SOP technology. Especially port terminator was optimized to provide a good match so that the reflection of microwave power is minimized. The flip-chip transition between the thin-film microstrip (TFMS) line and the coplanar waveguide (CPW) line on the flipped chip is also optimized for mm-wave range. As an illustration of this implementation methodology, a W-band transmitter was realized on the SNU's MCM-D substrate by means of the flip-chip technology. And the MCM-D substrate was fabricated for a mm-wave power amplifier (PA) module.","Millimeter wave technology,
Thermal conductivity,
Thermal expansion,
Substrates,
Coplanar waveguides,
Thermomechanical processes,
Performance analysis,
Finite element methods,
Analytical models,
Silicon compounds"
Minimum Expected Distortion in Gaussian Source Coding with Uncertain Side Information,"We consider a layered approach to source coding with side information received over an uncertain channel that minimizes expected distortion. Specifically, we assume a Gaussian source encoder whereby the decoder receives a compressed version of the symbol at a given rate, as well as an uncompressed version over a separate side-information channel with slow fading and noise. The decoder knows the realization of the slow fading but the encoder knows only its distribution. We consider a layered encoding strategy with a base layer describing the source assuming worst-case fading on the side-information channel, and subsequent layers describing the source under better fading conditions. Optimization of the layering scheme utilizes the Heegard-Berger rate-distortion function that describes the rate required to meet a different distortion constraint for each fading state. When the side-information channel has two discrete fading states, we obtain closed-form expressions for the optimal rate allocation between the fading states and the resulting minimum expected distortion. For multiple fading states, the minimum expected distortion is formulated as the solution of a convex optimization problem. Under discretized Rayleigh fading, we show that the optimal rate allocation puts almost all rate into the base layer associated with the worst-case fading. This implies that uncertain side information yields little performance benefit over no side information. Moreover, as the source coding rate",
A New Receiving System of Visible Light Communication for ITS,"This paper discusses a new receiving system for visible light communication applied to intelligent transport systems (ITS). We use a light emitting diode (LED) traffic signal as a transmitter that sends information to vehicles that feature a receiver. There are, however, several problems associated with applying visible light communication to the field of ITS. First, we have to transmit information over long distances; second, the relationship between the transmitter and the receiver position changes with time; and third, the communication is affected by a lot of optical noise. Here, we propose a new receiving system to solve these problems and construct a prototype. Evaluation experiments on the system demonstrate its validity.","Light emitting diodes,
Optical transmitters,
Optical receivers,
Communication system traffic control,
Intelligent systems,
Radio transmitters,
Intelligent vehicles,
Optical noise,
Prototypes,
Roads"
Design and Implementation of Network Management System for Power Line Communcation Network,"Power Line Communication (PLC) is an evolving communication network technology using the existing power lines and enables to provide the automatic meter reading service, high-speed Internet service as well as home networking service. As the use of PLC network and their applications increase, we need to manage the resources of PLC networks efficiently. Major PLC chipset and modem vendors are trying to provide network management solutions, but they are only specific solutions. It is necessary to provide a general PLC network management solution for PLC networks comprised of PLC devices from heterogeneous vendors. In this paper, we propose a PLC network management system called i-NetMSuite4PLC. We also propose a common PLC MIB (Management Information Base) based on some existing PLC MIBs. We present the design and implementation of the system.","Energy management,
Power system management,
Programmable control,
Power line communications,
Communication networks,
Communications technology,
Automatic meter reading,
Web and internet services,
Resource management,
Modems"
Characterization of Sample Entropy in the Context of Biomedical Signal Analysis,"Sample entropy (SampEn) has been proposed as a method to overcome limitations associated with approximate entropy (ApEn). The initial paper describing the SampEn metric included a characterization study comparing both ApEn and SampEn against theoretical results and concluded that SampEn is both more consistent and agrees more closely with theory for known random processes than ApEn. SampEn has been used in several studies to analyze the regularity of clinical and experimental time series. However, questions regarding how to interpret SampEn in certain clinical situations and its relationship to classical signal parameters remain unanswered. In this paper we report the results of a characterization study intended to provide additional insights regarding the interpretability of SampEn in the context of biomedical signal analysis.","Entropy,
Signal analysis,
Time series analysis,
Biomedical measurements,
Bismuth,
Heart rate variability,
Random processes,
Hypertension,
Brain injuries,
Temperature"
Design Patterns for Teaching Type Checking in a Compiler Construction Course,"A course in compiler construction seeks to develop an understanding of well-defined fundamental theory and typically involves the production of a language processor. In a graduate degree in software engineering, the development of a compiler contributes significantly to the developer's comprehension of the practical application of theoretical concepts. Different formal notations are commonly used to define type systems, and some of them are used to teach the semantic analysis phase of language processing. In the traditional approach, attribute grammars are probably the most widely used ones. This paper shows how object-oriented design patterns represented in unified modeling language (UML) can be used to both teach type systems and develop the semantic analysis phase of a compiler. The main benefit of this approach is two-fold: better comprehension of theoretical concepts because of the use of notations known by the students (UML diagrams), and improvement of software engineering skills for the development of a complete language processor.","Computer languages,
Construction industry,
Software engineering,
Unified modeling language,
Object oriented modeling,
Compounds"
Computation of minimal counterexamples by using black box techniques and symbolic methods,"Computing counterexamples is a crucial task for error diagnosis and debugging of sequential systems. If an implementation does not fulfill its specification, counterexamples are used to explain the error effect to the designer. In order to be understood by the designer, counterexamples should be simple, i.e. they should be as general as possible and assign values to a minimal number of input signals. Here we use the concept of Black Boxes - parts of the design with unknown behavior - to mask out components for counterexample computation. By doing so, the resulting counterexample will argue about a reduced number of components in the system to facilitate the task of understanding and correcting the error. We introduce the notion of 'uniform counterexamples' to provide an exact formalization of simplified counterexamples arguing only about components which were not masked out. Our computation of counterexamples is based on symbolic methods using AIGs (And-Inverter-Graphs). Experimental results using a VLIW processor as a case study clearly demonstrate our capability of providing simplified counterexamples.","Error correction,
Computer errors,
Sequential circuits,
Data structures,
Boolean functions,
Safety,
Computer science,
Debugging,
Signal design,
VLIW"
Voltage island-driven floorplanning,"Energy efficiency has become one of the most important issues to be addressed in today's system-on-a-chip (SoC) designs. One way to lower the power consumption is to reduce the supply voltage. Multi-supply voltage (MSV) is thus introduced to provide higher flexibility in controlling the power and performance trade-off. In region-based MSV, circuits are partitioned into ""voltage islands"" where each island occupies a contiguous physical space and operates at one supply voltage. These tasks of island partitioning and voltage level assignments should be done simultaneously in the floorplanning process in order to take those important physical information into consideration. In this paper, we consider this core-based voltage island driven floorplanning problem including islands with power down mode, and propose a method to solve it. Given a candidate floorplan solution represented by a normalized Polish expression, we are able to obtain optimal voltage assignment and island partitioning (including islands with power down mode) simultaneously to minimize the total power consumption. Simulated annealing is used as the basic searching engine. By using this approach, we can achieve significant power savings (up to 50%) for all data sets, without any significant increase in area and wire length. Our floorplanner can also be extended to minimize the number of level shifters between different voltage islands and to simplify the power routing step by placing the islands in proximity to the corresponding power pins.","System-on-a-chip,
Energy consumption,
Energy efficiency,
Voltage control,
Circuits,
Simulated annealing,
Search engines,
Wire,
Routing,
Pins"
Distributed Social-based Overlay Adaptation for Unstructured P2P Networks,"The widespread use of peer-to-peer (P2P) systems has made multimedia content sharing more efficient. Users in a P2P network can query and download objects based on their preference for specific types of multimedia content. However, most P2P systems only construct the overlay architecture according to physical network constraints and do not take user preferences into account. In this paper, we investigate a social-based overlay that can cluster peers that have similar preferences. To construct a semantic social-based overlay, we model a quantifiable measure of similarity between peers so that those with a higher degree of similarity can be connected by shorter paths. Hence, peers can locate objects of interest from their overlay neighbors, i.e., peers who have common interests. In addition, we propose an overlay adaptation algorithm that allows the overlay to adapt to P2P churn and preference changes in a distributed manner. We use simulations and a real database called Audioscrobbler, which tracks users' listening habits, to evaluate the proposed social-based overlay. The results show that social-based overlay adaptation enables users to locate content of interest with a higher success ratio and with less message overhead.",
A Caching-Based Approach to Routing in Delay-Tolerant Networks,"Delay-tolerant networks (DTNs), where no connected path generally exists between a source and a destination at any given time, present significant challenges from a routing perspective. A plethora of routing approaches have been previously introduced that make use of node mobility models or probabilistic models of network topology. In this paper, we offer a new perspective by making an analogy between routing table construction in DTNs and caching in program execution. In this approach, each node uses a simple caching heuristic to choose a subset of previously encountered nodes to be its routing neighbors. The approach is evaluated and shows good performance while keeping the implementation very simple.","Routing,
Disruption tolerant networking,
Counting circuits,
Mobile agents,
Delay effects,
Predictive models,
Computer science,
Network topology,
Engines,
Space exploration"
Enhanced MVDR Beamforming for Arrays of Directional Microphones,"Microphone arrays based on the minimum variance distortionless response (MVDR) beamformer are among the most popular for speech enhancement applications. The original MVDR is excessively sensitive to source location and microphone gains. Previous research has made MVDR practical by successfully increasing the robustness of MVDR to source location, and MVDR-based microphone arrays are already commercially available. Nevertheless, MVDR performance is still weak in cases where microphone gain variations are too large, e.g., for circular arrays of directional microphones. In this paper we propose an improved MVDR beamformer which takes into account the effect of sensors (e.g. microphones) with arbitrary, potentially directional responses. Specifically, we form estimates of the relative magnitude responses of the sensors based on the data received at the array and include those in the original formulation of the MVDR beamforming problem. Experimental results on real-world audio data show an average 2.4 dB improvement over conventional MVDR beamforming, which does not account for the magnitude responses of the sensors.","Microphone arrays,
Array signal processing,
Direction of arrival estimation,
Sensor arrays,
Microwave integrated circuits,
Position measurement,
Acoustic sensors,
Cameras,
Interference,
Computer science"
Investigating Key Factors of Deciding RFID's Adoption in Logistics Service Providers,"This study aims at examining critical factors of RFID technology adoption in logistics service providers (LSP) in Taiwan. The study adopts two phased expert questionnaire. First phase questionnaire applies fuzzy Delphi method to identity decision dimensions and factors affecting RFID's adoption. Second phase questionnaire applies fuzzy AHP method to calculate weighted values of decision dimensions and factors. Results indicate globalization trend, RFID introduction cost, technical interoperability and intention of upstream and downstream partner industry cooperation as four most critical decision factors. Empirical result could provide as reference for government, RFID's system developer and logistics service providers in promoting the RFID's commercial application.","Radiofrequency identification,
Logistics,
Costs,
Supply chains,
Manufacturing industries,
Government,
Conference management,
Technology management,
Globalization,
Application software"
"Pervasive computing: Past, present and future","The paradigm of pervasive computing describes ubiquitous computing environments that provide anytime and anywhere access to information services while making the presence of the system invisible to the user. Pervasive computing envisioned by Mark Weiser emerged at the conjunction of research and development in a number of areas which include embedded and devices and systems, wireless communications, and distributed, mobile and context-aware computing. This paper provides an overview of constituent components of pervasive computing and outlines the current progress made as a result of convergence of these areas of research.","Pervasive computing,
Mobile computing,
Embedded computing,
Wireless communication,
Distributed computing,
Context awareness,
Computer industry,
Grid computing,
Telecommunication computing,
Communication industry"
Arabic Speech Recognition System Based on CMUSphinx,"In this paper we present the creation of an Arabic version of Automated Speech Recognition System (ASR). This system is based on the open source Sphinx-4, from the Carnegie Mellon University. Which is a speech recognition system based on discrete hidden Markov models (HMMs). We investigate the changes that must be made to the model to adapt Arabic voice recognition.","Speech recognition,
Automatic speech recognition,
Hidden Markov models,
Java,
Robustness,
Application software,
Packaging,
Computational intelligence,
Sun,
Computer languages"
Towards a Framework for Differential Unit Testing of Object-Oriented Programs,"Software developers often face the task of determining how the behaviors of one version of a program unit differ from (or are the same as) the behaviors of a (slightly) different version of the same program unit. In such situations, developers would like to generate tests that exhibit the behavioral differences between the two versions, if any differences exist. We call this type of testing differential unit testing. Some examples of differential unit testing include regression testing, N-version testing, and mutation testing. We propose a framework, called Diffut, that enables differential unit testing of object-oriented programs. Diffut enables ""simultaneous"" execution of the pairs of corresponding methods from the two versions: methods can receive the same inputs (consisting of the object graph reachable from the receiver and method arguments), and Diffut compares their outputs (consisting of the object graph reachable from the receiver and method return values). Given two versions of a Java class, Diffut automatically synthesizes annotations (in the form of preconditions and postconditions) in the Java Modeling Language (JML) and inserts them into the unit under test to allow the simultaneous execution of the corresponding methods.","Software testing,
Java,
Genetic mutations,
Automatic testing,
Software systems,
Computer science,
Object oriented modeling,
System testing,
File systems,
Instruments"
Efficient feature representation employing PCA and VQ in the transform domain for facial recognition,"In this paper a new fast facial recognition system employing the principal component analysis, in the transform domain, and in conjunction with vector quantization, TD2DPCA/VQ, is presented. A transform domain two dimensional principal component analysis algorithm (TD2DPCA) was recently reported which possesses high recognition accuracy and low storage and computational requirements. The TD2DPCA/VQ presented here, maintains the recognition accuracy of the TD2DPCA while considerably improving the storage and computational properties. Employing the TD2DPCA/VQ, the storage and computational requirements are reduced by a factor P, where P is the number of training images (poses) per individual, used in the training mode. Experimental results employing the ORL and Yale databases confirm these excellent properties, where it is shown that the storage requirements and the computational complexity, for P=5, are reduced by 80% compared to the, high-performance, TD2DPCA algorithm.",
Discrete-Time Sliding Mode Control of Nonlinear Systems,"The control problem of nonlinear discrete-time systems based on the Takagi-Sugeno (T-S) fuzzy model was addressed. A nonlinear system dynamic model is represented by a T-S fuzzy model. The global T-S fuzzy model of nonlinear system was transformed into linear uncertain system model. So the stabilization problem of nonlinear systems becomes the robust stabilization problem of linear uncertain systems. Discrete-time sliding mode control approach was employed to guarantee robust stabilization of linear uncertain systems. The stable sliding surface was designed by using linear matrix inequalities to reduce the influence of mismatched uncertainties. The sufficient condition for the existence of stable sliding surface was derived in terms of LMI. Moreover the design of sliding mode control law was presented also. The system robust stabilization can be guaranteed and the chattering around the sliding surface in sliding mode control was obviously reduced by the proposed design approach. At last, an illustrative example of truck-trailer was presented to show the feasibility and effectiveness of the proposed method.","Sliding mode control,
Nonlinear systems,
Fuzzy systems,
Uncertain systems,
Robust control,
Control system synthesis,
Nonlinear control systems,
Takagi-Sugeno model,
Fuzzy control,
Nonlinear dynamical systems"
Distributed Information Storage and Collection for WSNs,"Distributed data storage is an important component of wireless sensor networks, which protects the mission critical information from unexpected node failures or malicious destruction of parts of the network. In this paper we present DISC, a protocol for distributed information storage and collection. The two major mechanisms in DISC which make our solution distinct from the related approaches are probabilistic choice of storing nodes and a search engine based on the usage of Bloom filters. In comparison to the deterministic choice of the backup node, the random selection strategy makes it virtually impossible for an attacker to determine and destroy the exact node keeping a particular piece of information. The usage of Bloom filters in the information search engine makes the navigation to a specific data fast and efficient. We show that with DISC the amount of recovered information is more than two times higher than that in deterministic storage schemes.","Wireless sensor networks,
Protocols,
Memory,
Mission critical systems,
Search engines,
Information filtering,
Information filters,
Information retrieval,
Area measurement,
Particle measurements"
Energy-Based Target Numeration in Wireless Sensor Networks,"Target numeration is of great importance for activity monitoring applications in wireless sensor networks (WSNs); however it is also a challenging problem in a WSN only equipped with simple amplitude sensors. Only a few algorithms have been proposed to solve the problem of target counting, and their accuracy and computational complexity is not satisfactory. This paper provides a two- step energy-based target numeration (EBTN) algorithm that firstly groups the sensor nodes that detect a target into separate clusters, and then calculates the number of targets covered by each cluster based on the total signal energy collected over the cluster. A polynomial regression function is used to approximate the signal strength over a cluster, and the total energy is estimated by taking the integral of the function over the area. By combining with preliminary clustering step, energy-based target counting greatly improves the counting accuracy. Experiments also show that EBTN requires lower node density and computational complexity compared with other algorithms.",
A Configurable and Extensible Transport Protocol,"The ability to configure transport protocols from collections of smaller software modules allows the characteristics of the protocol to be customized for a specific application or network technology. This paper describes a configurable transport protocol system called CTP in which microprotocols implementing individual attributes of transport can be combined into a composite protocol that realizes the desired overall functionality. In addition to describing the overall architecture of CTP and its microprotocols, this paper also presents experiments on both local area and wide area platforms that illustrate the flexibility of CTP and how its ability to match more closely application needs can result in better application performance. The prototype implementation of CTP has been built using the C version of the Cactus microprotocol composition framework running on Linux.","Transport protocols,
Bridges,
Application software,
Linux,
Prototypes,
Computer science,
Clustering algorithms,
Wireless networks,
Standards development,
Bandwidth"
MIST: Cellular data network measurement for mobile applications,"The rapid growth in the popularity of cellular networks has led to aggressive deployment and a rapid expansion of mobile services. Services based on the integration of cellular networks into the Internet have only recently become available, but are expected to become very popular. One current limitation to the deployment of many of these services is poor or unknown network performance, particularly in the cellular portion of the network. Our goal in this paper is to motivate and present the Mobile Internet Services Test (MIST) platform, a new distributed architecture to measure and characterize cellular network performance as experienced by mobile devices. We have used MIST to conduct preliminary measurements; evaluate MIST’s effectiveness; and motivate further measurement research.","Cellular networks,
Land mobile radio cellular systems,
Web and internet services,
Java,
Transport protocols,
Mobile computing,
Testing,
Space exploration,
Application software,
Computer science"
Finding Collisions in Interactive Protocols - A Tight Lower Bound on the Round Complexity of Statistically-Hiding Commitments,"We study the round complexity of various cryptographic protocols. Our main result is a tight lower bound on the round complexity of any fully-black-box construction of a statistically-hiding commitment scheme from oneway permutations, and even front trapdoor permutations. This lower bound matches the round complexity of the statistically-hiding commitment scheme due to Naor, Ostrovsky, Venkatesan and Yung (CRYPTO '92). As a corollary, we derive similar tight lower bounds for several other ctyptographicprotocols, such as single-server private information retrieval, interactive hashing, and oblivious transfer that guarantees statistical security for one of the parties. Our techniques extend the collision-finding oracle due to Simon (EUROCRYPT '98) to the setting of interactive protocols (our extension also implies an alternative proof for the main property of the original oracle). In addition, we substantially extend the reconstruction paradigm of Gennaro and Trevisan (FOCS '00). In both cases, our extensions are quite delicate and may be found useful in proving additional black-box separation results.","Cryptography,
Cryptographic protocols,
Computer science,
Information retrieval,
Information security,
Polynomials,
Gas insulated transmission lines,
Mathematics,
Upper bound,
Algorithms"
Image fusion algorithm based on neighbors and cousins information in nonsubsampled contourlet transform domain,"Nonsubsampled contourlet transform (NSCT) provides flexible multiresolution, anisotropy and directional expansion for images. Compared with the foremost contourlet transform, it is shift-invariant and can overcome the pseudo-Gibbs phenomena around singularities. In addition, coefficients of NSCT are dependent on their neighborhood coefficients in the local window and cousin coefficients in directional subbands. In this paper, region energy and cousin correlation are defined to represent the neighbors and cousins information, respectively. Salience measure, as the combination of region energy and cousin correlation, is defined to obtain fused coefficients in the high-frequency NSCT domain. First, source images are decomposed into subimages via NSCT. Secondly, salience measure is computed. Thirdly, salience measure-maximum-based rule and average rule are employed to obtain high-frequency and low-frequency coefficients, respectively. Finally, fused image is reconstructed by inverse NSCT. Experimental results show that the proposed algorithm outperforms wavelet-based fusion algorithms and contourlet transform-based fusion algorithms.","Image fusion,
Wavelet transforms,
Filter bank,
Wavelet analysis,
Wavelet domain,
Image analysis,
Pattern analysis,
Pattern recognition,
Laboratories"
Improving Knowledge Discovery in Document Collections through Combining Text Retrieval and Link Analysis Techniques,"In this paper, we present Concept Chain Queries (CCQ), a special case of text mining in document collections focusing on detecting links between two topics across text documents. We interpret such a query as finding the most meaningful evidence trails across documents that connect these two topics. We propose to use link-analysis techniques over the extracted features provided by Information Extraction Engine for finding new knowledge. A graphical text representation and mining model is proposed which combines information retrieval, association mining and link analysis techniques. We present experiments on different datasets that demonstrate the effectiveness of our algorithm. Specifically, the algorithm generates ranked concept chains and evidence trails where the key terms representing significant relationships between topics are ranked high.","Data mining,
Information retrieval,
Text mining,
Information analysis,
Computer science,
Feature extraction,
Engines,
Data engineering,
Knowledge engineering,
USA Councils"
Design Considerations and Dynamic Technique for Digitally Controlled Variable Frequency DC-DC Converter,"A dynamic algorithm to avoid limit cycle oscillation problems and to improve stability in digitally controlled power converters with variable switching frequency operation is presented in this paper. Employing Variable switching frequency at light loads is one way to improve the efficiency of DC-DC converters. However, varying the switching frequency digitally introduces additional considerations that do not exist in variable frequency analog controllers. In this paper a dynamic algorithm to maintain system stability and dynamics at different switching frequencies and to avoid limit cycle oscillation problems is discussed and experimentally verified.","Digital control,
Frequency conversion,
DC-DC power converters,
Digital-to-frequency converters,
Switching frequency,
Switching converters,
Limit-cycles,
Pulse width modulation,
Heuristic algorithms,
Stability"
Query Expansion Based on a Personalized Web Search Model,"A novel query expansion algorithm is proposed in this paper. It is based on a model of personalized web search system. The new system, as a middleware between a user and a Web search engine, is set up on the client machine. It can learn a user's preference implicitly and then generate the user profile automatically. When the user inputs query keywords, more personalized expansion words are generated by the proposed algorithm, and then these words together with the query keywords are submitted to a popular search engine such as Baidu or Google. These expansion words can help a search engine retrieval information for a user according to his/her implicit search intentions. The new Web search model can make a common search engine personalized, that is, throughout personalized query expansion the search engine can return different search results to different users who input the same keywords. The experimental results show the effect and applicability of the presented work for personalized information service of a search engine.","Web search,
Search engines,
Information retrieval,
Marine vehicles,
Motorcycles,
Educational institutions,
Computer science,
Middleware,
Service oriented architecture,
Internet"
Towards a European Master Programme on Global Software Engineering,"This paper presents a European Master programme on global software engineering (SE), being put forward by four leading institutions from Sweden, UK, Netherlands and Italy. The Global SE European Master (GSEEM) programme aims to provide students with an excellence in SE based on sound theoretical foundations and practical experience, as well as prepare them to participate in global development of complex and large software systems. GSEEM has been designed with three noteworthy aspects: 1) Three specialization profiles in which the consortium excels: Software Architecting, Real-time Embedded Systems Engineering, and Web Systems and Services Engineering. 2) Two market-driven routes: ""professional"" to work as professionals, and ""scientific"" to continue the education towards research degrees. 3) An innovative concept of ""shared modules"", delivered together by multiple institutions. Four types of shared modules are foreseen: ""parallel"" twin modules which run remotely between universities, ""shifted"" modules which teach SE concepts incrementally with shifts in study locations and timeline ,""complementary"" modules in which complementary SE concepts are taught in parallel through shared projects, and ""common"" modules which share the presentations and the project. The profiles realize ""integrated knowledge"" by complementing partial knowledge available at partner institutions. The paper explains how GSEEM achieves the objectives of educating global software engineers.","Software engineering,
Acoustical engineering,
Design engineering,
Systems engineering and theory,
Software systems,
Embedded software,
Real time systems,
Embedded system,
Continuing education,
Systems engineering education"
Conceptual Modeling of Temporal Clinical Workflows,"The diffusion of clinical guidelines to describe the proper way to deal with patients' situations is spreading out and opens new issues in the context of modeling and managing (temporal) information about medical activities. Guidelines can be seen as processes describing the sequence of activities to be executed, and thus approaches proposed in the business context can be used to model them. In this paper, we propose a general conceptual workflow model, considering both activities and their temporal properties, and focus on the representation of clinical guidelines by the proposed model.","Guidelines,
Context modeling,
Computer science,
Scheduling,
Drugs,
Delay,
Performance evaluation,
Information management,
Engineering management,
Natural languages"
Ukrainian Grid Infrastructure: Practical Experience,"In 2006 National Academy of Sciences of Ukraine approved the project for development of grid technologies in Ukraine. Since then an extensive work is carried out in order to build a full-scale grid infrastructure for scientific and educational institutions. This article deals with the aspects of building such infrastructure, its organization and peculiarities of implementation in Ukraine, shows the current status of project, describes scientific tasks which benefit from using grid.",
A DSP based real-time simulation of Dual-Bridge Matrix Converters,"This paper investigates the state-space modeling and real-time simulation of dual-bridge matrix converters (DBMC) based on a DS1104 digital signal processor. This single board controller of dSPACE uses the matlab-simulink software as a front-end for the implementation of graphic models in block- diagram format. First, the state-space model of the converter and the SVM control system are designed with simulink in the form of user-friendly block-diagram. Next, C-codes are generated from the block diagrams with the real-time workshop. After that the executable code is automatically downloaded to the target DSP board. Finally, real-time simulation results are presented and compared to those obtained using off-line computer simulations. Good agreement is observed between both simulation results which shows the effectiveness and accuracy of the proposed realtime simulator.","Digital signal processing,
Matrix converters,
Mathematical model,
Computational modeling,
Computer simulation,
Digital signal processors,
Computer languages,
Graphics,
Support vector machines,
Control system synthesis"
Regression Test Selection for Black-box Dynamic Link Library Components,"Software products are often configured with commercial-off-the-shelf (COTS) components. When new releases of these components are made available for integration and testing, source code is usually not provided. Various regression test selection processes have been developed and have been shown to be cost effective. However, the majority of these test selection techniques rely on access to source code for change identification. Based on our prior work, we are studying the solution to regression testing COTS-based applications that incorporate components of dynamic link library (DLL) files. We evolved the Integrated - Black-box Approach for Component Change Identification (I-BACCI) process that selects regression tests for applications based upon static binary code analysis to Version 4 to support DLL components. A feasibility case study was conducted at ABB on products written in C/C++ to determine the effectiveness of the I-BACCI process. The results of the case study indicate this process can reduce the required number of regression tests by as much as 100% if our analysis indicates the changes to the component are not called by the glue code of the application using the COTS component. Similar to other regression test selection techniques, when there are many changes in the new component I-BACCI suggests a retest-all regression test strategy.","System testing,
Software testing,
Binary codes,
Software systems,
Software libraries,
Computer science,
Costs,
Hardware,
Software tools,
Documentation"
Ontology Engineering to Model Clinical Pathways: Towards the Computerization and Execution of Clinical Pathways,"Clinical pathways translate evidence-based recommendations into locally practicable, process-specific algorithms that reduce practice variations and optimize quality of care. Our objective was to abstract practice-oriented knowledge from a cohort of real clinical pathways and represent this knowledge as a clinical pathway ontology. We employed a four step methodology: (1) knowledge source identification and classification of clinical pathways according to variations in setting, stage of care, patient type, outcome and specialty; (2) iterative knowledge abstraction using grounded theory; (3) ontology engineering as adapted from the Model-based Incremental Knowledge Engineering approach; and, (4) ontology evaluation through encoding a sample of real clinical pathways. We present our clinical pathway ontology that offers a detailed ontological model describing the structure and function of clinical pathways. Our ontology can potentially integrate with a healthcare semantic web, and ontologies for clinical practice guidelines, patients and institutions to form the foundational knowledge for generating patient-specific CarePlans.","Ontologies,
Knowledge engineering,
Medical services,
Biomedical engineering,
Iterative methods,
Guidelines,
Medical diagnostic imaging,
Knowledge management,
Design engineering,
Computer science"
Privacy in the Semantic Web: What Policy Languages Have to Offer,"Uncontrolled disclosure of sensitive information during electronic transactions may expose users to threats like loss of privacy and identity theft. The means envisioned for addressing protection of security and privacy in the context of the Semantic Web are policy languages for trust establishment and management. Although a number of policy languages have been proposed, it is unclear how well each language can address users' privacy concerns. The contribution of this work is an independent, scenario-based comparison of six prominent policy languages, namely Protune, Rei, Ponder, Trust-X, KeyNote and P3P-APPEL, with respect to the needs that users have in protecting their personal, sensitive data. We present how each language addresses access control for objects, such as user credentials and sensitive policies. We evaluate how each language defines or imports hierarchies of resources, whether the language supports protection of user information after it has been released, whether the language supports the principle of least privilege and more. The evaluation is not only an analytical literature study but also rich in actual implementations in all six languages.","Privacy,
Semantic Web,
Protection,
Credit cards,
Computer crime,
Information science,
Information security,
Access control,
Collaboration,
Lead"
Towards Competitive Web Service Market,"By fully exploiting service oriented architecture (SOA), Web service provides an efficient way to connect the existing on-line assets to support business enterprises. Most of the current Web service research inherently assumes that there exists only one service provider that can invoke those element services and compose them in a specific solution scenario. This paper aims to extend Web services research beyond the current limitations, allowing the existence of parallel and competitive service providers that compete for serving the customers. This paper intends to formally describe the concept of competitive Web service market, or CWSM in short. Web service composition problems pertinent to CWSM are also discussed. Two different models for CWSM are investigated, and the heuristics to be considered for the design of revenue oriented composition approaches are also proposed","Web services,
Service oriented architecture,
Application specific processors,
Atomic layer deposition,
Tin,
Computer science,
Software design,
Software systems,
Simple object access protocol,
Access protocols"
Efficient Message Passing Architecture for High Throughput LDPC Decoder,"In this paper, we propose an efficient message passing architecture for permutation matrices based LDPC code decoders. Min-sum algorithm is reformulated to facilitate significant reduction of routing complexity and memory usage. For a (2048, 1723) (6, 32) LDPC code with 4-bit quantization, 54% outgoing wires per variable node unit and 90% outgoing wires per check node unit can be saved. To further reduce hardware complexity, an optimized nonuniform quantization scheme using only 3 bits to represent each message has been investigated. The simulation result shows that it has only 0.25dB performance loss from the floating-point SPA",
Error Detection via Online Checking of Cache Coherence with Token Coherence Signatures,"To provide high dependability in a multithreaded system despite hardware faults, the system must detect and correct errors in its shared memory system. Recent research has explored dynamic checking of cache coherence as a comprehensive approach to memory system error detection. However, existing coherence checkers are costly to implement, incur high interconnection network traffic overhead, and do not scale well. In this paper, we describe the token coherence signature checker (TCSC), which provides comprehensive, low-cost, scalable coherence checking by maintaining signatures that represent recent histories of coherence events at all nodes (cache and memory controllers). Periodically, these signatures are sent to a verifier to determine if an error occurred. TCSC has a small constant hardware cost per node, independent of cache and memory size and the number of nodes. TCSC's interconnect bandwidth overhead has a constant upper bound and never exceeds 7% in our experiments. TCSC has negligible impact on system performance",
CT image reconstruction using hexagonal grids,"In the transversal plane CT exhibits a nearly rotational symmetric point spread function. Pixel sampling is typically done on Cartesian grids which are not ideal from a signal processing point of view. It is advantageous to use a hexagonal grid which can capture the same signal components with 13% fewer sampling points. In 3D one can even save 29%. We developed an efficient scheme to allow for arbitrarily shaped field of views and a hierarchical memory layout. The latter divides the images into small hexagonal subimages, similar to honeycombs, whose size is small enough to avoid cache misses on CPU-based algorithms or to be used on dedicated signal processing hardware such as GPUs (graphics processing units) or CBEs (cell broadband engines) that may be memory limited. As the final step a resampling algorithm converts from the hex domain to the Cartesian domain before storing images. We implemented a hyperfast CBE-based Feldkamp algorithm for the hexagonal lattice and compared its performance and its image quality to reconstructions using the standard Feldkamp algorithm. Both algorithms were run on the Dual Cell Based System (DCBS, Mercury Computer Systems, Berlin, Germany) on two CBEs with 3.2 GHz each. A 5123 volume was reconstructed from 720 projections of 5122 detector pixels. Image quality of the hexagonal approach was identical to the direct approach using the Cartesian lattice: the maximum relative difference between the MTFs was 1% and image noise differed by not more than 3%. The improvement in reconstruction speed was approximately 12% for the hexagonal grid which is slightly lower than the expectation. The complete reconstruction finished in about 10 s. With identical image quality reconstruction on the hexagonal grid is a simple and effective approach to significantly reduce memory consumption and reconstruction time. Existing reconstruction algorithms can be easily modified to operate in the hexagonal domain.","Computed tomography,
Image reconstruction,
Signal processing algorithms,
Image quality,
Image sampling,
Signal sampling,
Lattices,
Signal processing,
Layout,
Hardware"
Payoff based dynamics for multi-player weakly acyclic games,"We consider repeated multi-player games in which players repeatedly and simultaneously choose strategies from a finite set of available strategies according to some strategy adjustment process. We focus on the specific class of weakly acyclic games, which is particularly relevant for multi-agent cooperative control problems. A strategy adjustment process determines how players select their strategies at any stage as a function of the information gathered over previous stages. Of particular interest are ""payoff based"" processes, in which at any stage, players only know their own actions and (noise corrupted) payoffs from previous stages. In particular, players do not know the actions taken by other players and do not know the structural form of payoff functions. We introduce three different payoff based processes for increasingly general scenarios and prove that after a sufficiently large number of stages, player actions constitute a Nash equilibrium at any stage with arbitrarily high probability. We also show how to modify player utility functions through tolls and incentives in so-called congestion games, a special class of weakly acyclic games, to guarantee that a centralized objective can be realized as a Nash equilibrium. We illustrate the methods with a simulation of distributed routing over a network.","Routing,
Nash equilibrium,
Multiagent systems,
Environmental economics,
Delay,
USA Councils,
Distributed control,
Control systems,
Motion control,
Laboratories"
Anomaly localization in large-scale clusters,"A critical problem facing by managing large-scale clusters is to identify the location of problems in a system in case of unusual events. As the scale of high performance computing (HPC) grows, systems are getting bigger. When a system fails to function properly, health-related data are collected for troubleshooting. However, due to the massive quantities of information obtained from a large number of components, the root causes of anomalies are often buried like needles in a haystack. In this paper, we present a localization method to automatically find out the potential root causes (i.e. a subset of nodes) of the problem from the overwhelming amount of data collected system-wide. System managers can focus on examining these potential locations, thereby significantly reducing human efforts required for anomaly localization. Our method consists of three interrelated steps: (1) feature collection to assemble a feature space for the system; (2) feature extraction to obtain the most significant features for efficient data analysis by applying the principal component analysis (PCA) algorithm; and (3) outlier detection to quickly identify the nodes that are ldquofar awayrdquo from the majority by using the cell-based detection algorithm. Preliminary studies are presented to demonstrate the potential of our method for localizing anomalies in a computing environment where the nodes perform comparable tasks.","Feature extraction,
Principal component analysis,
Data mining,
Algorithm design and analysis,
Data analysis,
Data models,
Complexity theory"
"Optimizing center performance through coordinated data staging, scheduling and recovery","Procurement and the optimized utilization of Petascale supercomputers and centers is a renewed national priority. Sustained performance and availability of such large centers is a key technical challenge significantly impacting their usability. Storage systems are known to be the primary fault source leading to data unavailability and job resubmissions. This results in reduced center performance, partially due to the lack of coordination between I/O activities and job scheduling. In this work, we propose the coordination of job scheduling with data staging/offloading and on-demand staged data reconstruction to address the availability of job input data and to improve center-wide performance. Fundamental to both mechanisms is the efficient management of transient data: in the way it is scheduled and recovered. Collectively, from a center's standpoint, these techniques optimize resource usage and increase its data/service availability. From a user's standpoint, they reduce the job turnaround time and optimize the allocated time usage.","Processor scheduling,
Scheduling algorithm,
Information science,
Licenses,
Hardware,
Software algorithms,
Permission,
Computer science,
Concurrent computing,
Workstations"
A Grid-Based Stable Routing Algorithm in Mobile Ad Hoc Networks,"Since nodes in the mobile ad hoc networks (MANETs) network move freely and randomly, routes often get disconnected. The major challenge for MANETs is therefore to implement routing protocols that must respond to changes in the network topology in order to maintain and reconstruct the routes in a timely manner as well as to establish reliable routes. In this paper, we propose a new routing algorithm called grid-based stable routing algorithm (GSRA), GSRA exploits the concept of a routing protocol called Grid. The main difference, between these two protocols is that GSRA considers grid head stability and route stability but the Grid does not. In GSRA, grid partitioning is the same as in the Grid routing protocol. Each grid zone selects a grid head to route. Also, GSRA defines a new parameter called grid head stability to select stable grid head. In routing discovery, each node receives the RREQ packet and uses the link stability metric to evaluate link stability. The destination node collects several feasible routes and then selects the most stable route according to end-to-end reliability. Simulation results indicate that GSRA has a higher packet delivery ratio, lower end-to-end delay and lower routing load than Grid and AODV","Mobile ad hoc networks,
Routing protocols,
Stability,
Network topology,
Ad hoc networks,
Load management,
Computer science,
Electronic mail,
Maintenance,
Telecommunication network reliability"
An Area-Efficient Approach to Improving Register File Reliability against Transient Errors,"This paper studies approaches to exploiting the space both within or across registers efficiently for improving the register file reliability against transient errors. The idea of our approach is based on the fact that a large number of register values are narrow (i.e., less than or equal to 16 bits for a 32-bit architecture); therefore, the upper 16 bits of the registers can be used to replicate the short operands for enhancing register integrity. This paper also adapts a prior register replication approach by selectively copying register values (i.e., long operands only) to the unused physical registers for enhancing reliability without incurring significant hardware cost. Our experiments indicate that on average, 993% register reads (regardless of short or long operands) can find their replicas available, implying significant improvement of register file integrity against transient errors.","Registers,
Error correction codes,
Protection,
Error correction,
Energy consumption,
Microprocessors,
Computer errors,
Cache memory,
Reliability engineering,
Computer science"
An Investigation on the Compression Quality of aiNet,"AiNet is an immune-inspired algorithm for data compression, i.e. the reduction of redundancy in data sets. In this paper we investigate the compression quality of aiNet. Therefore, a similarity measure between input set and reduced output set is presented which is based on the Parzen window estimation and the Kullback-Leibler divergence. Four different artificially generated data sets are created and the compression quality is investigated. Experiments reveal that aiNet produced reasonable results on an uniformly distributed data set, but poor results on non-uniformly distributed data sets, i.e. data sets which contain dense point regions. This effect is caused by the optimization criterion of aiNet","Clustering algorithms,
Immune system,
Computer science,
Computational intelligence,
Density measurement,
Continuous production,
Mirrors,
Helium,
Data compression,
Pattern recognition"
Auditory and visual integration based localization and tracking of humans in daily-life environments,"The purpose of this research is to develop techniques that enable robots to choose and track a desired person for interaction in daily-life environments. Therefore, localizing multiple moving sounds and human faces is necessary so that robots can locate a desired person. For sound source localization, we used a cross-power spectrum phase analysis (CSP) method and showed that CSP can localize sound sources only using two microphones and does not need impulse response data. An expectation-maximization (EM) algorithm was shown to enable a robot to cope with multiple moving sound sources. For face localization, we developed a method that can reliably detect several faces using the skin color classification obtained by using the EM algorithm. To deal with a change in color state according to illumination condition and various skin colors, the robot can obtain new skin color features of faces detected by OpenCV, an open vision library, for detecting human faces. Finally, we developed a probability based method to integrate auditory and visual information and to produce a reliable tracking path in real time. Furthermore, the developed system chose and tracked people while dealing with various background noises that are considered loud, even in the daily-life environments.","Humans,
Face detection,
Skin,
Change detection algorithms,
Microphones,
Lighting,
Robot vision systems,
Computer vision,
Libraries,
Background noise"
Toward Reducing Fault Fix Time: Understanding Developer Behavior for the Design of Automated Fault Detection Tools,"The longer a fault remains in the code from the time it was injected, the more time it will take to fix the fault. Increasingly, automated fault detection (AFD) tools are providing developers with prompt feedback on recently-introduced faults to reduce fault fix time. If however, the frequency and content of this feedback does not match the developer's goals and/or workflow, the developer may ignore the information. We conducted a controlled study with 18 developers to explore what factors are used by developers to decide whether or not to address a fault when notified of the error. The findings of our study lead to several conjectures about the design of AFD tools to effectively notify developers of faults in the coding phase. The AFD tools should present fault information that is relevant to the primary programming task with accurate and precise descriptions. The fault severity and the specific timing of fault notification should be customizable. Finally, the AFD tool must be accurate and reliable to build trust with the developer.","Fault detection,
Feedback,
Software engineering,
Software measurement,
Time measurement,
Computer science,
Frequency,
Automatic control,
Error correction,
Timing"
Standard and Genetic k-means Clustering Techniques in Image Segmentation,"Clustering or data grouping is a key initial procedure in image processing. This paper deals with the application of standard and genetic k-means clustering algorithms in the area of image segmentation. In order to assess and compare both versions of k-means algorithm and its variants, appropriate procedures and software have been designed and implemented. Experimental results point that genetically optimized k-means algorithms proved their usefulness in the area of image analysis, yielding comparable and even better segmentation results.",
Learning Ontologies to Improve the Quality of Automatic Web Service Matching,"Automatically finding suitable Web services given a request is a difficult problem because the interface descriptions of Web services are often terse and cryptic. Dictionary and information retrieval based techniques have proven useful in disambiguating the semantics of service descriptions, but they are limited in their capability to consider the relationships between the words describing the Web services. Current ontology-based approaches typically require a user to explicitly create domain ontologies. This paper presents a novel technique that significantly improves the quality of semantic Web service matching by (1) automatically generating ontologies based on Web service descriptions and (2) using these ontologies to guide the mapping between Web services. Our approach differs from earlier work on service matching by considering the relationship between words rather than treating them as a bag of unrelated words. The experimental results indicate that with our unsupervised approach we can eliminate up to 70% of incorrect matches that are made by dictionary-based approaches.","Ontologies,
Web services,
Dictionaries,
Semantic Web,
Computer science,
Middleware,
Information retrieval,
Wrapping,
Impedance matching"
Conditional Random Fields for Intrusion Detection,"An intrusion detection system is now an inevitable part of any computer network. With the ever increasing number and diverse type of attacks, including new and previously unseen attacks, the effectiveness of an intrusion detection system is often subjected to testing. The use of such systems have greatly reduced the threat level, however, the networks and hence the data and services offered by them are far away from the state when they can be considered as secure. In this paper we propose and experimentally validate the use and robustness of 'conditional random fields,' for the task of intrusion detection. We show, experimentally, that the conditional random fields, can be very effective in detecting intrusions when compared with the previously known techniques.","Intrusion detection,
Engines,
Telecommunication traffic,
Application software,
Computer network management,
Computer science,
Software engineering,
Computer networks,
System testing,
Robustness"
An Optical Fiber Proximity Sensor for Haptic Exploration,This paper presents the design of an optical fiber proximity sensor for haptic exploration with a robotic finger. The sensor uses emitter and receiver optical fiber pairs to measure the intensity of light reflected off surrounding objects in a 2-D workspace. We present the design and construction a 32-point sensor array mounted within a 36 mm diameter finger and describe software techniques to process data acquired by an inexpensive Web cam. We experimentally characterize the sensor performance and demonstrate applications for haptic exploration such as pre-contact velocity reduction and non-contact contour following based on object curvature.,
